<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-07-21T02:01:37.697Z</updated>
    <generator>osmosfeed 1.11.0</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Towards Domain-Agnostic Contrastive Learning. (arXiv:2011.04419v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04419</id>
        <link href="http://arxiv.org/abs/2011.04419"/>
        <updated>2021-07-21T02:01:37.638Z</updated>
        <summary type="html"><![CDATA[Despite recent success, most contrastive self-supervised learning methods are
domain-specific, relying heavily on data augmentation techniques that require
knowledge about a particular domain, such as image cropping and rotation. To
overcome such limitation, we propose a novel domain-agnostic approach to
contrastive learning, named DACL, that is applicable to domains where
invariances, and thus, data augmentation techniques, are not readily available.
Key to our approach is the use of Mixup noise to create similar and dissimilar
examples by mixing data samples differently either at the input or hidden-state
levels. To demonstrate the effectiveness of DACL, we conduct experiments across
various domains such as tabular data, images, and graphs. Our results show that
DACL not only outperforms other domain-agnostic noising methods, such as
Gaussian-noise, but also combines well with domain-specific methods, such as
SimCLR, to improve self-supervised visual representation learning. Finally, we
theoretically analyze our method and show advantages over the Gaussian-noise
based contrastive learning approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1"&gt;Vikas Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1"&gt;Minh-Thang Luong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Canonical Polyadic Decomposition and Deep Learning for Machine Fault Detection. (arXiv:2107.09519v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.09519</id>
        <link href="http://arxiv.org/abs/2107.09519"/>
        <updated>2021-07-21T02:01:37.625Z</updated>
        <summary type="html"><![CDATA[Acoustic monitoring for machine fault detection is a recent and expanding
research path that has already provided promising results for industries.
However, it is impossible to collect enough data to learn all types of faults
from a machine. Thus, new algorithms, trained using data from healthy
conditions only, were developed to perform unsupervised anomaly detection. A
key issue in the development of these algorithms is the noise in the signals,
as it impacts the anomaly detection performance. In this work, we propose a
powerful data-driven and quasi non-parametric denoising strategy for spectral
data based on a tensor decomposition: the Non-negative Canonical Polyadic (CP)
decomposition. This method is particularly adapted for machine emitting
stationary sound. We demonstrate in a case study, the Malfunctioning Industrial
Machine Investigation and Inspection (MIMII) baseline, how the use of our
denoising strategy leads to a sensible improvement of the unsupervised anomaly
detection. Such approaches are capable to make sound-based monitoring of
industrial processes more reliable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gaetan_F/0/1/0/all/0/1"&gt;Frusque Gaetan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gabriel_M/0/1/0/all/0/1"&gt;Michau Gabriel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Olga_F/0/1/0/all/0/1"&gt;Fink Olga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08285</id>
        <link href="http://arxiv.org/abs/2106.08285"/>
        <updated>2021-07-21T02:01:37.619Z</updated>
        <summary type="html"><![CDATA[Time-lapse fluorescent microscopy (TLFM) combined with predictive
mathematical modelling is a powerful tool to study the inherently dynamic
processes of life on the single-cell level. Such experiments are costly,
complex and labour intensive. A complimentary approach and a step towards in
silico experimentation, is to synthesise the imagery itself. Here, we propose
Multi-StyleGAN as a descriptive approach to simulate time-lapse fluorescence
microscopy imagery of living cells, based on a past experiment. This novel
generative adversarial network synthesises a multi-domain sequence of
consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live
yeast cells in microstructured environments and train on a dataset recorded in
our laboratory. The simulation captures underlying biophysical factors and time
dependencies, such as cell morphology, growth, physical interactions, as well
as the intensity of a fluorescent reporter protein. An immediate application is
to generate additional training and validation data for feature extraction
algorithms or to aid and expedite development of advanced experimental
techniques such as online monitoring or control of cells.

Code and dataset is available at
https://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1"&gt;Christoph Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1"&gt;Tim Prangemeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1"&gt;Christian Wildner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1"&gt;Heinz Koeppl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse within Sparse Gaussian Processes using Neighbor Information. (arXiv:2011.05041v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05041</id>
        <link href="http://arxiv.org/abs/2011.05041"/>
        <updated>2021-07-21T02:01:37.612Z</updated>
        <summary type="html"><![CDATA[Approximations to Gaussian processes based on inducing variables, combined
with variational inference techniques, enable state-of-the-art sparse
approaches to infer GPs at scale through mini batch-based learning. In this
work, we address one limitation of sparse GPs, which is due to the challenge in
dealing with a large number of inducing variables without imposing a special
structure on the inducing inputs. In particular, we introduce a novel
hierarchical prior, which imposes sparsity on the set of inducing variables. We
treat our model variationally, and we experimentally show considerable
computational gains compared to standard sparse GPs when sparsity on the
inducing variables is realized considering the nearest inducing inputs of a
random mini-batch of the data. We perform an extensive experimental validation
that demonstrates the effectiveness of our approach compared to the
state-of-the-art. Our approach enables the possibility to use sparse GPs using
a large number of inducing points without incurring a prohibitive computational
cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tran_G/0/1/0/all/0/1"&gt;Gia-Lac Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Milios_D/0/1/0/all/0/1"&gt;Dimitrios Milios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Michiardi_P/0/1/0/all/0/1"&gt;Pietro Michiardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Filippone_M/0/1/0/all/0/1"&gt;Maurizio Filippone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries. (arXiv:2107.09609v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09609</id>
        <link href="http://arxiv.org/abs/2107.09609"/>
        <updated>2021-07-21T02:01:37.296Z</updated>
        <summary type="html"><![CDATA[Detecting customized moments and highlights from videos given natural
language (NL) user queries is an important but under-studied topic. One of the
challenges in pursuing this direction is the lack of annotated data. To address
this issue, we present the Query-based Video Highlights (QVHighlights) dataset.
It consists of over 10,000 YouTube videos, covering a wide range of topics,
from everyday activities and travel in lifestyle vlog videos to social and
political activities in news videos. Each video in the dataset is annotated
with: (1) a human-written free-form NL query, (2) relevant moments in the video
w.r.t. the query, and (3) five-point scale saliency scores for all
query-relevant clips. This comprehensive annotation enables us to develop and
evaluate systems that detect relevant moments as well as salient highlights for
diverse, flexible user queries. We also present a strong baseline for this
task, Moment-DETR, a transformer encoder-decoder model that views moment
retrieval as a direct set prediction problem, taking extracted video and query
representations as inputs and predicting moment coordinates and saliency scores
end-to-end. While our model does not utilize any human prior, we show that it
performs competitively when compared to well-engineered architectures. With
weakly supervised pretraining using ASR captions, Moment-DETR substantially
outperforms previous methods. Lastly, we present several ablations and
visualizations of Moment-DETR. Data and code is publicly available at
https://github.com/jayleicn/moment_detr]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1"&gt;Tamara L. Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09391</id>
        <link href="http://arxiv.org/abs/2107.09391"/>
        <updated>2021-07-21T02:01:37.289Z</updated>
        <summary type="html"><![CDATA[We focus on building robustness in the convolutions of neural visual
classifiers, especially against natural perturbations like elastic
deformations, occlusions and Gaussian noise. Existing CNNs show outstanding
performance on clean images, but fail to tackle naturally occurring
perturbations. In this paper, we start from elastic perturbations, which
approximate (local) view-point changes of the object. We present
elastically-augmented convolutions (EAConv) by parameterizing filters as a
combination of fixed elastically-perturbed bases functions and trainable
weights for the purpose of integrating unseen viewpoints in the CNN. We show on
CIFAR-10 and STL-10 datasets that the general robustness of our method on
unseen occlusion and Gaussian perturbations improves, while even improving the
performance on clean images slightly without performing any data augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gulshad_S/0/1/0/all/0/1"&gt;Sadaf Gulshad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1"&gt;Ivan Sosnovik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1"&gt;Arnold Smeulders&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. (arXiv:2106.06054v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06054</id>
        <link href="http://arxiv.org/abs/2106.06054"/>
        <updated>2021-07-21T02:01:37.283Z</updated>
        <summary type="html"><![CDATA[In recent years, many incidents have been reported where machine learning
models exhibited discrimination among people based on race, sex, age, etc.
Research has been conducted to measure and mitigate unfairness in machine
learning models. For a machine learning task, it is a common practice to build
a pipeline that includes an ordered set of data preprocessing stages followed
by a classifier. However, most of the research on fairness has considered a
single classifier based prediction task. What are the fairness impacts of the
preprocessing stages in machine learning pipeline? Furthermore, studies showed
that often the root cause of unfairness is ingrained in the data itself, rather
than the model. But no research has been conducted to measure the unfairness
caused by a specific transformation made in the data preprocessing stage. In
this paper, we introduced the causal method of fairness to reason about the
fairness impact of data preprocessing stages in ML pipeline. We leveraged
existing metrics to define the fairness measures of the stages. Then we
conducted a detailed fairness evaluation of the preprocessing stages in 37
pipelines collected from three different sources. Our results show that certain
data transformers are causing the model to exhibit unfairness. We identified a
number of fairness patterns in several categories of data transformers.
Finally, we showed how the local fairness of a preprocessing stage composes in
the global fairness of the pipeline. We used the fairness composition to choose
appropriate downstream transformer that mitigates unfairness in the machine
learning pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1"&gt;Sumon Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1"&gt;Hridesh Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedProto: Federated Prototype Learning over Heterogeneous Devices. (arXiv:2105.00243v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00243</id>
        <link href="http://arxiv.org/abs/2105.00243"/>
        <updated>2021-07-21T02:01:37.276Z</updated>
        <summary type="html"><![CDATA[The heterogeneity across devices usually hinders the optimization convergence
and generalization performance of federated learning (FL) when the aggregation
of devices' knowledge occurs in the gradient space. For example, devices may
differ in terms of data distribution, network latency, input/output space,
and/or model architecture, which can easily lead to the misalignment of their
local gradients. To improve the tolerance to heterogeneity, we propose a novel
federated prototype learning (FedProto) framework in which the devices and
server communicate the class prototypes instead of the gradients. FedProto
aggregates the local prototypes collected from different devices, and then
sends the global prototypes back to all devices to regularize the training of
local models. The training on each device aims to minimize the classification
error on the local data while keeping the resulting local prototypes
sufficiently close to the corresponding global ones. Through experiments, we
propose a benchmark setting tailored for heterogeneous FL, with FedProto
outperforming several recent FL approaches on multiple datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;Yue Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1"&gt;Guodong Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1"&gt;Qinghua Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chengqi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ps and Qs: Quantization-aware pruning for efficient low latency neural network inference. (arXiv:2102.11289v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11289</id>
        <link href="http://arxiv.org/abs/2102.11289"/>
        <updated>2021-07-21T02:01:37.257Z</updated>
        <summary type="html"><![CDATA[Efficient machine learning implementations optimized for inference in
hardware have wide-ranging benefits, depending on the application, from lower
inference latency to higher data throughput and reduced energy consumption. Two
popular techniques for reducing computation in neural networks are pruning,
removing insignificant synapses, and quantization, reducing the precision of
the calculations. In this work, we explore the interplay between pruning and
quantization during the training of neural networks for ultra low latency
applications targeting high energy physics use cases. Techniques developed for
this study have potential applications across many other domains. We study
various configurations of pruning during quantization-aware training, which we
term quantization-aware pruning, and the effect of techniques like
regularization, batch normalization, and different pruning schemes on
performance, computational complexity, and information content metrics. We find
that quantization-aware pruning yields more computationally efficient models
than either pruning or quantization alone for our task. Further,
quantization-aware pruning typically performs similar to or better in terms of
computational efficiency compared to other neural architecture search
techniques like Bayesian optimization. Surprisingly, while networks with
different training configurations can have similar performance for the
benchmark application, the information content in the network can vary
significantly, affecting its generalizability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hawks_B/0/1/0/all/0/1"&gt;Benjamin Hawks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1"&gt;Javier Duarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fraser_N/0/1/0/all/0/1"&gt;Nicholas J. Fraser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pappalardo_A/0/1/0/all/0/1"&gt;Alessandro Pappalardo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1"&gt;Nhan Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Umuroglu_Y/0/1/0/all/0/1"&gt;Yaman Umuroglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning with Prototypical Representations. (arXiv:2102.11271v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11271</id>
        <link href="http://arxiv.org/abs/2102.11271"/>
        <updated>2021-07-21T02:01:37.250Z</updated>
        <summary type="html"><![CDATA[Learning effective representations in image-based environments is crucial for
sample efficient Reinforcement Learning (RL). Unfortunately, in RL,
representation learning is confounded with the exploratory experience of the
agent -- learning a useful representation requires diverse data, while
effective exploration is only possible with coherent representations.
Furthermore, we would like to learn representations that not only generalize
across tasks but also accelerate downstream exploration for efficient
task-specific training. To address these challenges we propose Proto-RL, a
self-supervised framework that ties representation learning with exploration
through prototypical representations. These prototypes simultaneously serve as
a summarization of the exploratory experience of an agent as well as a basis
for representing observations. We pre-train these task-agnostic representations
and prototypes on environments without downstream task information. This
enables state-of-the-art downstream policy learning on a set of difficult
continuous control tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yarats_D/0/1/0/all/0/1"&gt;Denis Yarats&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1"&gt;Rob Fergus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1"&gt;Alessandro Lazaric&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1"&gt;Lerrel Pinto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum-Inspired Algorithms from Randomized Numerical Linear Algebra. (arXiv:2011.04125v5 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04125</id>
        <link href="http://arxiv.org/abs/2011.04125"/>
        <updated>2021-07-21T02:01:37.244Z</updated>
        <summary type="html"><![CDATA[We create classical (non-quantum) dynamic data structures supporting queries
for recommender systems and least-squares regression that are comparable to
their quantum analogues. De-quantizing such algorithms has received a flurry of
attention in recent years; we obtain sharper bounds for these problems. More
significantly, we achieve these improvements by arguing that the previous
quantum-inspired algorithms for these problems are doing leverage or
ridge-leverage score sampling in disguise; these are powerful and standard
techniques in randomized numerical linear algebra. With this recognition, we
are able to employ the large body of work in numerical linear algebra to obtain
algorithms for these problems that are simpler or faster (or both) than
existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chepurko_N/0/1/0/all/0/1"&gt;Nadiia Chepurko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clarkson_K/0/1/0/all/0/1"&gt;Kenneth L. Clarkson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horesh_L/0/1/0/all/0/1"&gt;Lior Horesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the 2020 US Presidential Election with Twitter. (arXiv:2107.09640v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2107.09640</id>
        <link href="http://arxiv.org/abs/2107.09640"/>
        <updated>2021-07-21T02:01:37.237Z</updated>
        <summary type="html"><![CDATA[One major sub-domain in the subject of polling public opinion with social
media data is electoral prediction. Electoral prediction utilizing social media
data potentially would significantly affect campaign strategies, complementing
traditional polling methods and providing cheaper polling in real-time. First,
this paper explores past successful methods from research for analysis and
prediction of the 2020 US Presidential Election using Twitter data. Then, this
research proposes a new method for electoral prediction which combines
sentiment, from NLP on the text of tweets, and structural data with aggregate
polling, a time series analysis, and a special focus on Twitter users critical
to the election. Though this method performed worse than its baseline of
polling predictions, it is inconclusive whether this is an accurate method for
predicting elections due to scarcity of data. More research and more data are
needed to accurately measure this method's overall effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caballero_M/0/1/0/all/0/1"&gt;Michael Caballero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Hyperbolic Neural Networks. (arXiv:2105.14686v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14686</id>
        <link href="http://arxiv.org/abs/2105.14686"/>
        <updated>2021-07-21T02:01:37.231Z</updated>
        <summary type="html"><![CDATA[Hyperbolic neural networks have shown great potential for modeling complex
data. However, existing hyperbolic networks are not completely hyperbolic, as
they encode features in a hyperbolic space yet formalize most of their
operations in the tangent space (a Euclidean subspace) at the origin of the
hyperbolic space. This hybrid method greatly limits the modeling ability of
networks. In this paper, we propose a fully hyperbolic framework to build
hyperbolic networks based on the Lorentz model by adapting the Lorentz
transformations (including boost and rotation) to formalize essential
operations of neural networks. Moreover, we also prove that linear
transformation in tangent spaces used by existing hyperbolic networks is a
relaxation of the Lorentz rotation and does not include the boost, implicitly
limiting the capabilities of existing hyperbolic networks. The experimental
results on four NLP tasks show that our method has better performance for
building both shallow and deep networks. Our code will be released to
facilitate follow-up research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weize Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yankai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hexu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Buffered Asynchronous Aggregation. (arXiv:2106.06639v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06639</id>
        <link href="http://arxiv.org/abs/2106.06639"/>
        <updated>2021-07-21T02:01:37.215Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) trains a shared model across distributed devices
while keeping the training data on the devices. Most FL schemes are
synchronous: they perform a synchronized aggregation of model updates from
individual devices. Synchronous training can be slow because of late-arriving
devices (stragglers). On the other hand, completely asynchronous training makes
FL less private because of incompatibility with secure aggregation. In this
work, we propose a model aggregation scheme, FedBuff, that combines the best
properties of synchronous and asynchronous FL. Similar to synchronous FL,
FedBuff is compatible with secure aggregation. Similar to asynchronous FL,
FedBuff is robust to stragglers. In FedBuff, clients trains asynchronously and
send updates to the server. The server aggregates client updates in a private
buffer until updates have been received, at which point a server model update
is immediately performed. We provide theoretical convergence guarantees for
FedBuff in a non-convex setting. Empirically, FedBuff converges up to 3.8x
faster than previous proposals for synchronous FL (e.g., FedAvgM), and up to
2.5x faster than previous proposals for asynchronous FL (e.g., FedAsync). We
show that FedBuff is robust to different staleness distributions and is more
scalable than synchronous FL techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_J/0/1/0/all/0/1"&gt;John Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1"&gt;Kshitiz Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1"&gt;Hongyuan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yousefpour_A/0/1/0/all/0/1"&gt;Ashkan Yousefpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1"&gt;Michael Rabbat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malek_M/0/1/0/all/0/1"&gt;Mani Malek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huba_D/0/1/0/all/0/1"&gt;Dzmitry Huba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hash Layers For Large Sparse Models. (arXiv:2106.04426v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04426</id>
        <link href="http://arxiv.org/abs/2106.04426"/>
        <updated>2021-07-21T02:01:37.209Z</updated>
        <summary type="html"><![CDATA[We investigate the training of sparse layers that use different parameters
for different inputs based on hashing in large Transformer models.
Specifically, we modify the feedforward layer to hash to different sets of
weights depending on the current token, over all tokens in the sequence. We
show that this procedure either outperforms or is competitive with
learning-to-route mixture-of-expert methods such as Switch Transformers and
BASE Layers, while requiring no routing parameters or extra terms in the
objective function such as a load balancing loss, and no sophisticated
assignment algorithm. We study the performance of different hashing techniques,
hash sizes and input features, and show that balanced and random hashes focused
on the most local features work best, compared to either learning clusters or
using longer-range context. We show our approach works well both on large
language modeling and dialogue tasks, and on downstream fine-tuning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated data-driven approach for gap filling in the time series using evolutionary learning. (arXiv:2103.01124v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01124</id>
        <link href="http://arxiv.org/abs/2103.01124"/>
        <updated>2021-07-21T02:01:37.203Z</updated>
        <summary type="html"><![CDATA[In the paper, we propose an adaptive data-driven model-based approach for
filling the gaps in time series. The approach is based on the automated
evolutionary identification of the optimal structure for a composite
data-driven model. It allows adapting the model for the effective gap-filling
in a specific dataset without the involvement of the data scientist. As a case
study, both synthetic and real datasets from different fields (environmental,
economic, etc) are used. The experiments confirm that the proposed approach
allows achieving the higher quality of the gap restoration and improve the
effectiveness of forecasting models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarafanov_M/0/1/0/all/0/1"&gt;Mikhail Sarafanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikitin_N/0/1/0/all/0/1"&gt;Nikolay O. Nikitin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1"&gt;Anna V. Kalyuzhnaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Precision-Weighted Federated Learning. (arXiv:2107.09627v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09627</id>
        <link href="http://arxiv.org/abs/2107.09627"/>
        <updated>2021-07-21T02:01:37.196Z</updated>
        <summary type="html"><![CDATA[Federated Learning using the Federated Averaging algorithm has shown great
advantages for large-scale applications that rely on collaborative learning,
especially when the training data is either unbalanced or inaccessible due to
privacy constraints. We hypothesize that Federated Averaging underestimates the
full extent of heterogeneity of data when the aggregation is performed. We
propose Precision-weighted Federated Learning a novel algorithm that takes into
account the variance of the stochastic gradients when computing the weighted
average of the parameters of models trained in a Federated Learning setting.
With Precision-weighted Federated Learning, we provide an alternate averaging
scheme that leverages the heterogeneity of the data when it has a large
diversity of features in its composition. Our method was evaluated using
standard image classification datasets with two different data partitioning
strategies (IID/non-IID) to measure the performance and speed of our method in
resource-constrained environments, such as mobile and IoT devices. We obtained
a good balance between computational efficiency and convergence rates with
Precision-weighted Federated Learning. Our performance evaluations show 9%
better predictions with MNIST, 18% with Fashion-MNIST, and 5% with CIFAR-10 in
the non-IID setting. Further reliability evaluations ratify the stability in
our method by reaching a 99% reliability index with IID partitions and 96% with
non-IID partitions. In addition, we obtained a 20x speedup on Fashion-MNIST
with only 10 clients and up to 37x with 100 clients participating in the
aggregation concurrently per communication round. The results indicate that
Precision-weighted Federated Learning is an effective and faster alternative
approach for aggregating private data, especially in domains where data is
highly heterogeneous.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reyes_J/0/1/0/all/0/1"&gt;Jonatan Reyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jorio_L/0/1/0/all/0/1"&gt;Lisa Di Jorio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Low_Kam_C/0/1/0/all/0/1"&gt;Cecile Low-Kam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersten_Oertel_M/0/1/0/all/0/1"&gt;Marta Kersten-Oertel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpookyNet: Learning Force Fields with Electronic Degrees of Freedom and Nonlocal Effects. (arXiv:2105.00304v2 [physics.chem-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00304</id>
        <link href="http://arxiv.org/abs/2105.00304"/>
        <updated>2021-07-21T02:01:37.189Z</updated>
        <summary type="html"><![CDATA[Machine-learned force fields (ML-FFs) combine the accuracy of ab initio
methods with the efficiency of conventional force fields. However, current
ML-FFs typically ignore electronic degrees of freedom, such as the total charge
or spin state, and assume chemical locality, which is problematic when
molecules have inconsistent electronic states, or when nonlocal effects play a
significant role. This work introduces SpookyNet, a deep neural network for
constructing ML-FFs with explicit treatment of electronic degrees of freedom
and quantum nonlocality. Chemically meaningful inductive biases and analytical
corrections built into the network architecture allow it to properly model
physical limits. SpookyNet improves upon the current state-of-the-art (or
achieves similar performance) on popular quantum chemistry data sets. Notably,
it is able to generalize across chemical and conformational space and can
leverage the learned chemical insights, e.g. by predicting unknown spin states,
thus helping to close a further important remaining gap for today's machine
learning models in quantum chemistry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Unke_O/0/1/0/all/0/1"&gt;Oliver T. Unke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chmiela_S/0/1/0/all/0/1"&gt;Stefan Chmiela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gastegger_M/0/1/0/all/0/1"&gt;Michael Gastegger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schutt_K/0/1/0/all/0/1"&gt;Kristof T. Sch&amp;#xfc;tt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sauceda_H/0/1/0/all/0/1"&gt;Huziel E. Sauceda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Muller_K/0/1/0/all/0/1"&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bayesian Federated Learning Framework with Online Laplace Approximation. (arXiv:2102.01936v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01936</id>
        <link href="http://arxiv.org/abs/2102.01936"/>
        <updated>2021-07-21T02:01:37.170Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) allows multiple clients to collaboratively learn a
globally shared model through cycles of model aggregation and local model
training, without the need to share data. Most existing FL methods train local
models separately on different clients, and then simply average their
parameters to obtain a centralized model on the server side. However, these
approaches generally suffer from large aggregation errors and severe local
forgetting, which are particularly bad in heterogeneous data settings. To
tackle these issues, in this paper, we propose a novel FL framework that uses
online Laplace approximation to approximate posteriors on both the client and
server side. On the server side, a multivariate Gaussian product mechanism is
employed to construct and maximize a global posterior, largely reducing the
aggregation errors induced by large discrepancies between local models. On the
client side, a prior loss that uses the global posterior probabilistic
parameters delivered from the server is designed to guide the local training.
Binding such learning constraints from other clients enables our method to
mitigate local forgetting. Finally, we achieve state-of-the-art results on
several benchmarks, clearly demonstrating the advantages of the proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liangxi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Feng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1"&gt;Guo-Jun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Bandit Learning Through Heterogeneous Feedback Aggregation. (arXiv:2010.15390v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15390</id>
        <link href="http://arxiv.org/abs/2010.15390"/>
        <updated>2021-07-21T02:01:37.164Z</updated>
        <summary type="html"><![CDATA[In many real-world applications, multiple agents seek to learn how to perform
highly related yet slightly different tasks in an online bandit learning
protocol. We formulate this problem as the $\epsilon$-multi-player multi-armed
bandit problem, in which a set of players concurrently interact with a set of
arms, and for each arm, the reward distributions for all players are similar
but not necessarily identical. We develop an upper confidence bound-based
algorithm, RobustAgg$(\epsilon)$, that adaptively aggregates rewards collected
by different players. In the setting where an upper bound on the pairwise
similarities of reward distributions between players is known, we achieve
instance-dependent regret guarantees that depend on the amenability of
information sharing across players. We complement these upper bounds with
nearly matching lower bounds. In the setting where pairwise similarities are
unknown, we provide a lower bound, as well as an algorithm that trades off
minimax regret guarantees for adaptivity to unknown similarity structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chicheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Manish Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riek_L/0/1/0/all/0/1"&gt;Laurel D. Riek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1"&gt;Kamalika Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11858</id>
        <link href="http://arxiv.org/abs/2106.11858"/>
        <updated>2021-07-21T02:01:37.157Z</updated>
        <summary type="html"><![CDATA[Image segmentation is a common and challenging task in autonomous driving.
Availability of sufficient pixel-level annotations for the training data is a
hurdle. Active learning helps learning from small amounts of data by suggesting
the most promising samples for labeling. In this work, we propose a new
pool-based method for active learning, which proposes promising patches
extracted from full image, in each acquisition step. The problem is framed in
an exploration-exploitation framework by combining an embedding based on
Uniform Manifold Approximation to model representativeness with entropy as
uncertainty measure to model informativeness. We applied our proposed method to
the autonomous driving datasets CamVid and Cityscapes and performed a
quantitative comparison with state-of-the-art baselines. We find that our
active learning method achieves better performance compared to previous
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1"&gt;Deepthi Sreenivasaiah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otterbach_J/0/1/0/all/0/1"&gt;Johannes Otterbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1"&gt;Thomas Wollmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Discovery with Multi-Domain LiNGAM for Latent Factors. (arXiv:2009.09176v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09176</id>
        <link href="http://arxiv.org/abs/2009.09176"/>
        <updated>2021-07-21T02:01:37.150Z</updated>
        <summary type="html"><![CDATA[Discovering causal structures among latent factors from observed data is a
particularly challenging problem. Despite some efforts for this problem,
existing methods focus on the single-domain data only. In this paper, we
propose Multi-Domain Linear Non-Gaussian Acyclic Models for Latent Factors
(MD-LiNA), where the causal structure among latent factors of interest is
shared for all domains, and we provide its identification results. The model
enriches the causal representation for multi-domain data. We propose an
integrated two-phase algorithm to estimate the model. In particular, we first
locate the latent factors and estimate the factor loading matrix. Then to
uncover the causal structure among shared latent factors of interest, we derive
a score function based on the characterization of independence relations
between external influences and the dependence relations between multi-domain
latent factors and latent factors of interest. We show that the proposed method
provides locally consistent estimators. Experimental results on both synthetic
and real-world data demonstrate the efficacy and robustness of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1"&gt;Yan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shimizu_S/0/1/0/all/0/1"&gt;Shohei Shimizu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1"&gt;Ruichu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1"&gt;Feng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamamoto_M/0/1/0/all/0/1"&gt;Michio Yamamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1"&gt;Zhifeng Hao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scan Specific Artifact Reduction in K-space (SPARK) Neural Networks Synergize with Physics-based Reconstruction to Accelerate MRI. (arXiv:2104.01188v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01188</id>
        <link href="http://arxiv.org/abs/2104.01188"/>
        <updated>2021-07-21T02:01:37.134Z</updated>
        <summary type="html"><![CDATA[Purpose: To develop a scan-specific model that estimates and corrects k-space
errors made when reconstructing accelerated Magnetic Resonance Imaging (MRI)
data.

Methods: Scan-Specific Artifact Reduction in k-space (SPARK) trains a
convolutional-neural-network to estimate and correct k-space errors made by an
input reconstruction technique by back-propagating from the mean-squared-error
loss between an auto-calibration signal (ACS) and the input technique's
reconstructed ACS. First, SPARK is applied to GRAPPA and demonstrates improved
robustness over other scan-specific models, such as RAKI and residual-RAKI.
Subsequent experiments demonstrate that SPARK synergizes with residual-RAKI to
improve reconstruction performance. SPARK also improves reconstruction quality
when applied to advanced acquisition and reconstruction techniques like 2D
virtual coil (VC-) GRAPPA, 2D LORAKS, 3D GRAPPA without an integrated ACS
region, and 2D/3D wave-encoded images.

Results: SPARK yields 1.5x - 2x RMSE reduction when applied to GRAPPA and
improves robustness to ACS size for various acceleration rates in comparison to
other scan-specific techniques. When applied to advanced reconstruction
techniques such as residual-RAKI, 2D VC-GRAPPA and LORAKS, SPARK achieves up to
20% RMSE improvement. SPARK with 3D GRAPPA also improves performance by ~2x and
perceived image quality without a fully sampled ACS region. Finally, SPARK
synergizes with non-cartesian 2D and 3D wave-encoding imaging by reducing RMSE
between 20-25% and providing qualitative improvements.

Conclusion: SPARK synergizes with physics-based acquisition and
reconstruction techniques to improve accelerated MRI by training scan-specific
models to estimate and correct reconstruction errors in k-space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Arefeen_Y/0/1/0/all/0/1"&gt;Yamin Arefeen&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Beker_O/0/1/0/all/0/1"&gt;Onur Beker&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/eess/1/au:+Cho_J/0/1/0/all/0/1"&gt;Jaejin Cho&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1"&gt;Heng Yu&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1"&gt;Elfar Adalsteinsson&lt;/a&gt; (1 and 5 and 6), &lt;a href="http://arxiv.org/find/eess/1/au:+Bilgic_B/0/1/0/all/0/1"&gt;Berkin Bilgic&lt;/a&gt; (3 and 5 and 7) ((1) Massachusetts Institute of Technology, (2) &amp;#xc9;cole Polytechnique F&amp;#xe9;d&amp;#xe9;rale de Lausanne, (3) Athinoula A. Martinos Center for Biomedical Imaging (4) Tsinghua University, (5) Harvard-MIT Health Sciences and Technology, (6) Institute for Medical Engineering and Science, (7) Harvard Medical School)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution-free calibration guarantees for histogram binning without sample splitting. (arXiv:2105.04656v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04656</id>
        <link href="http://arxiv.org/abs/2105.04656"/>
        <updated>2021-07-21T02:01:37.126Z</updated>
        <summary type="html"><![CDATA[We prove calibration guarantees for the popular histogram binning (also
called uniform-mass binning) method of Zadrozny and Elkan [2001]. Histogram
binning has displayed strong practical performance, but theoretical guarantees
have only been shown for sample split versions that avoid 'double dipping' the
data. We demonstrate that the statistical cost of sample splitting is
practically significant on a credit default dataset. We then prove calibration
guarantees for the original method that double dips the data, using a certain
Markov property of order statistics. Based on our results, we make practical
recommendations for choosing the number of bins in histogram binning. In our
illustrative simulations, we propose a new tool for assessing calibration --
validity plots -- which provide more information than an ECE estimate. Code for
this work will be made publicly available at
https://github.com/aigen/df-posthoc-calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gupta_C/0/1/0/all/0/1"&gt;Chirag Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya K. Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proxy Convexity: A Unified Framework for the Analysis of Neural Networks Trained by Gradient Descent. (arXiv:2106.13792v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13792</id>
        <link href="http://arxiv.org/abs/2106.13792"/>
        <updated>2021-07-21T02:01:37.120Z</updated>
        <summary type="html"><![CDATA[Although the optimization objectives for learning neural networks are highly
non-convex, gradient-based methods have been wildly successful at learning
neural networks in practice. This juxtaposition has led to a number of recent
studies on provable guarantees for neural networks trained by gradient descent.
Unfortunately, the techniques in these works are often highly specific to the
problem studied in each setting, relying on different assumptions on the
distribution, optimization parameters, and network architectures, making it
difficult to generalize across different settings. In this work, we propose a
unified non-convex optimization framework for the analysis of neural network
training. We introduce the notions of proxy convexity and proxy
Polyak-Lojasiewicz (PL) inequalities, which are satisfied if the original
objective function induces a proxy objective function that is implicitly
minimized when using gradient methods. We show that stochastic gradient descent
(SGD) on objectives satisfying proxy convexity or the proxy PL inequality leads
to efficient guarantees for proxy objective functions. We further show that
many existing guarantees for neural networks trained by gradient descent can be
unified through proxy convexity and proxy PL inequalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1"&gt;Spencer Frei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Topological Dependencies of Recurrent Congestion in Road Networks. (arXiv:2107.09554v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09554</id>
        <link href="http://arxiv.org/abs/2107.09554"/>
        <updated>2021-07-21T02:01:37.112Z</updated>
        <summary type="html"><![CDATA[The discovery of spatio-temporal dependencies within urban road networks that
cause Recurrent Congestion (RC) patterns is crucial for numerous real-world
applications, including urban planning and scheduling of public transportation
services. While most existing studies investigate temporal patterns of RC
phenomena, the influence of the road network topology on RC is often
overlooked. This article proposes the ST-Discovery algorithm, a novel
unsupervised spatio-temporal data mining algorithm that facilitates the
effective data-driven discovery of RC dependencies induced by the road network
topology using real-world traffic data. We factor out regularly reoccurring
traffic phenomena, such as rush hours, mainly induced by the daytime, by
modelling and systematically exploiting temporal traffic load outliers. We
present an algorithm that first constructs connected subgraphs of the road
network based on the traffic speed outliers. Second, the algorithm identifies
pairs of subgraphs that indicate spatio-temporal correlations in their traffic
load behaviour to identify topological dependencies within the road network.
Finally, we rank the identified subgraph pairs based on the dependency score
determined by our algorithm. Our experimental results demonstrate that
ST-Discovery can effectively reveal topological dependencies in urban road
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tempelmeier_N/0/1/0/all/0/1"&gt;Nicolas Tempelmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feuerhake_U/0/1/0/all/0/1"&gt;Udo Feuerhake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wage_O/0/1/0/all/0/1"&gt;Oskar Wage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demidova_E/0/1/0/all/0/1"&gt;Elena Demidova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Parametric Estimation of Manifolds from Noisy Data. (arXiv:2105.04754v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04754</id>
        <link href="http://arxiv.org/abs/2105.04754"/>
        <updated>2021-07-21T02:01:37.106Z</updated>
        <summary type="html"><![CDATA[A common observation in data-driven applications is that high dimensional
data has a low intrinsic dimension, at least locally. In this work, we consider
the problem of estimating a $d$ dimensional sub-manifold of $\mathbb{R}^D$ from
a finite set of noisy samples. Assuming that the data was sampled uniformly
from a tubular neighborhood of $\mathcal{M}\in \mathcal{C}^k$, a compact
manifold without boundary, we present an algorithm that takes a point $r$ from
the tubular neighborhood and outputs $\hat p_n\in \mathbb{R}^D$, and
$\widehat{T_{\hat p_n}\mathcal{M}}$ an element in the Grassmanian $Gr(d, D)$.
We prove that as the number of samples $n\to\infty$ the point $\hat p_n$
converges to $p\in \mathcal{M}$ and $\widehat{T_{\hat p_n}\mathcal{M}}$
converges to $T_p\mathcal{M}$ (the tangent space at that point) with high
probability. Furthermore, we show that the estimation yields asymptotic rates
of convergence of $n^{-\frac{k}{2k + d}}$ for the point estimation and
$n^{-\frac{k-1}{2k + d}}$ for the estimation of the tangent space. These rates
are known to be optimal for the case of function estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Aizenbud_Y/0/1/0/all/0/1"&gt;Yariv Aizenbud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sober_B/0/1/0/all/0/1"&gt;Barak Sober&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Imaging with Deep Learning for COVID- 19 Diagnosis: A Comprehensive Review. (arXiv:2107.09602v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09602</id>
        <link href="http://arxiv.org/abs/2107.09602"/>
        <updated>2021-07-21T02:01:37.098Z</updated>
        <summary type="html"><![CDATA[The outbreak of novel coronavirus disease (COVID- 19) has claimed millions of
lives and has affected all aspects of human life. This paper focuses on the
application of deep learning (DL) models to medical imaging and drug discovery
for managing COVID-19 disease. In this article, we detail various medical
imaging-based studies such as X-rays and computed tomography (CT) images along
with DL methods for classifying COVID-19 affected versus pneumonia. The
applications of DL techniques to medical images are further described in terms
of image localization, segmentation, registration, and classification leading
to COVID-19 detection. The reviews of recent papers indicate that the highest
classification accuracy of 99.80% is obtained when InstaCovNet-19 DL method is
applied to an X-ray dataset of 361 COVID-19 patients, 362 pneumonia patients
and 365 normal people. Furthermore, it can be seen that the best classification
accuracy of 99.054% can be achieved when EDL_COVID DL method is applied to a CT
image dataset of 7500 samples where COVID-19 patients, lung tumor patients and
normal people are equal in number. Moreover, we illustrate the potential DL
techniques in drug or vaccine discovery in combating the coronavirus. Finally,
we address a number of problems, concerns and future research directions
relevant to DL applications for COVID-19.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bharati_S/0/1/0/all/0/1"&gt;Subrato Bharati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Podder_P/0/1/0/all/0/1"&gt;Prajoy Podder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mondal_M/0/1/0/all/0/1"&gt;M. Rubaiyat Hossain Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prasath_V/0/1/0/all/0/1"&gt;V.B. Surya Prasath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Laplace for Bayesian neural networks. (arXiv:2103.00222v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00222</id>
        <link href="http://arxiv.org/abs/2103.00222"/>
        <updated>2021-07-21T02:01:37.091Z</updated>
        <summary type="html"><![CDATA[We develop variational Laplace for Bayesian neural networks (BNNs) which
exploits a local approximation of the curvature of the likelihood to estimate
the ELBO without the need for stochastic sampling of the neural-network
weights. The Variational Laplace objective is simple to evaluate, as it is (in
essence) the log-likelihood, plus weight-decay, plus a squared-gradient
regularizer. Variational Laplace gave better test performance and expected
calibration errors than maximum a-posteriori inference and standard
sampling-based variational inference, despite using the same variational
approximate posterior. Finally, we emphasise care needed in benchmarking
standard VI as there is a risk of stopping before the variance parameters have
converged. We show that early-stopping can be avoided by increasing the
learning rate for the variance parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Unlu_A/0/1/0/all/0/1"&gt;Ali Unlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1"&gt;Laurence Aitchison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stein Latent Optimization for GANs. (arXiv:2106.05319v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05319</id>
        <link href="http://arxiv.org/abs/2106.05319"/>
        <updated>2021-07-21T02:01:37.085Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) with clustered latent spaces can
perform conditional generation in a completely unsupervised manner. However,
the salient attributes of unlabeled data in the real-world are mostly
imbalanced. Existing unsupervised conditional GANs cannot properly cluster the
attributes in their latent spaces because they assume uniform distributions of
the attributes. To address this problem, we theoretically derive Stein latent
optimization that provides reparameterizable gradient estimations of the latent
distribution parameters assuming a Gaussian mixture prior in a continuous
latent space. Structurally, we introduce an encoder network and a novel
contrastive loss to help generated data from a single mixture component to
represent a single attribute. We confirm that the proposed method, named Stein
Latent Optimization for GANs (SLOGAN), successfully learns the balanced or
imbalanced attributes and performs unsupervised tasks such as unsupervised
conditional generation, unconditional generation, and cluster assignment even
in the absence of information of the attributes (e.g. the imbalance ratio).
Moreover, we demonstrate that the attributes to be learned can be manipulated
using a small amount of probe data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_U/0/1/0/all/0/1"&gt;Uiwon Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Heeseung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1"&gt;Dahuin Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1"&gt;Hyemi Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hyungyu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing Generalization under Out-Of-Distribution Shifts in Deep Metric Learning. (arXiv:2107.09562v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09562</id>
        <link href="http://arxiv.org/abs/2107.09562"/>
        <updated>2021-07-21T02:01:37.052Z</updated>
        <summary type="html"><![CDATA[Deep Metric Learning (DML) aims to find representations suitable for
zero-shot transfer to a priori unknown test distributions. However, common
evaluation protocols only test a single, fixed data split in which train and
test classes are assigned randomly. More realistic evaluations should consider
a broad spectrum of distribution shifts with potentially varying degree and
difficulty. In this work, we systematically construct train-test splits of
increasing difficulty and present the ooDML benchmark to characterize
generalization under out-of-distribution shifts in DML. ooDML is designed to
probe the generalization performance on much more challenging, diverse
train-to-test distribution shifts. Based on our new benchmark, we conduct a
thorough empirical analysis of state-of-the-art DML methods. We find that while
generalization tends to consistently degrade with difficulty, some methods are
better at retaining performance as the distribution shift increases. Finally,
we propose few-shot DML as an efficient way to consistently improve
generalization in response to unknown test shifts presented in ooDML. Code
available here:
https://github.com/Confusezius/Characterizing_Generalization_in_DeepMetricLearning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1"&gt;Timo Milbich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1"&gt;Karsten Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1"&gt;Samarth Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;Marzyeh Ghassemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04570</id>
        <link href="http://arxiv.org/abs/2107.04570"/>
        <updated>2021-07-21T02:01:37.040Z</updated>
        <summary type="html"><![CDATA[Randomized smoothing has recently emerged as an effective tool that enables
certification of deep neural network classifiers at scale. All prior art on
randomized smoothing has focused on isotropic $\ell_p$ certification, which has
the advantage of yielding certificates that can be easily compared among
isotropic methods via $\ell_p$-norm radius. However, isotropic certification
limits the region that can be certified around an input to worst-case
adversaries, i.e., it cannot reason about other "close", potentially large,
constant prediction safe regions. To alleviate this issue, (i) we theoretically
extend the isotropic randomized smoothing $\ell_1$ and $\ell_2$ certificates to
their generalized anisotropic counterparts following a simplified analysis.
Moreover, (ii) we propose evaluation metrics allowing for the comparison of
general certificates - a certificate is superior to another if it certifies a
superset region - with the quantification of each certificate through the
volume of the certified region. We introduce ANCER, a practical framework for
obtaining anisotropic certificates for a given test set sample via volume
maximization. Our empirical results demonstrate that ANCER achieves
state-of-the-art $\ell_1$ and $\ell_2$ certified accuracy on both CIFAR-10 and
ImageNet at multiple radii, while certifying substantially larger regions in
terms of volume, thus highlighting the benefits of moving away from isotropic
analysis. Code used in our experiments is available in
https://github.com/MotasemAlfarra/ANCER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1"&gt;Francisco Eiras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1"&gt;Motasem Alfarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M. Pawan Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H. S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1"&gt;Puneet K. Dokania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1"&gt;Adel Bibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying identifiability to choose and audit $\epsilon$ in differentially private deep learning. (arXiv:2103.02913v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02913</id>
        <link href="http://arxiv.org/abs/2103.02913"/>
        <updated>2021-07-21T02:01:37.032Z</updated>
        <summary type="html"><![CDATA[Differential privacy allows bounding the influence that training data records
have on a machine learning model. To use differential privacy in machine
learning, data scientists must choose privacy parameters $(\epsilon,\delta)$.
Choosing meaningful privacy parameters is key, since models trained with weak
privacy parameters might result in excessive privacy leakage, while strong
privacy parameters might overly degrade model utility. However, privacy
parameter values are difficult to choose for two main reasons. First, the
theoretical upper bound on privacy loss $(\epsilon,\delta)$ might be loose,
depending on the chosen sensitivity and data distribution of practical
datasets. Second, legal requirements and societal norms for anonymization often
refer to individual identifiability, to which $(\epsilon,\delta)$ are only
indirectly related.

We transform $(\epsilon,\delta)$ to a bound on the Bayesian posterior belief
of the adversary assumed by differential privacy concerning the presence of any
record in the training dataset. The bound holds for multidimensional queries
under composition, and we show that it can be tight in practice. Furthermore,
we derive an identifiability bound, which relates the adversary assumed in
differential privacy to previous work on membership inference adversaries. We
formulate an implementation of this differential privacy adversary that allows
data scientists to audit model training and compute empirical identifiability
scores and empirical $(\epsilon,\delta)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bernau_D/0/1/0/all/0/1"&gt;Daniel Bernau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eibl_G/0/1/0/all/0/1"&gt;G&amp;#xfc;nther Eibl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grassal_P/0/1/0/all/0/1"&gt;Philip W. Grassal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_H/0/1/0/all/0/1"&gt;Hannah Keller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerschbaum_F/0/1/0/all/0/1"&gt;Florian Kerschbaum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Theory of Label Propagation for Subpopulation Shift. (arXiv:2102.11203v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11203</id>
        <link href="http://arxiv.org/abs/2102.11203"/>
        <updated>2021-07-21T02:01:37.021Z</updated>
        <summary type="html"><![CDATA[One of the central problems in machine learning is domain adaptation. Unlike
past theoretical work, we consider a new model for subpopulation shift in the
input or representation space. In this work, we propose a provably effective
framework for domain adaptation based on label propagation. In our analysis, we
use a simple but realistic expansion assumption, proposed in
\citet{wei2021theoretical}. Using a teacher classifier trained on the source
domain, our algorithm not only propagates to the target domain but also
improves upon the teacher. By leveraging existing generalization bounds, we
also obtain end-to-end finite-sample guarantees on the entire algorithm. In
addition, we extend our theoretical framework to a more general setting of
source-to-target transfer based on a third unlabeled dataset, which can be
easily applied in various learning scenarios. Inspired by our theory, we adapt
consistency-based semi-supervised learning methods to domain adaptation
settings and gain significant improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruiqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1"&gt;Qi Lei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Agnostic Learning to Meta-Learn. (arXiv:2012.02684v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02684</id>
        <link href="http://arxiv.org/abs/2012.02684"/>
        <updated>2021-07-21T02:01:37.004Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a learning algorithm that enables a model to
quickly exploit commonalities among related tasks from an unseen task
distribution, before quickly adapting to specific tasks from that same
distribution. We investigate how learning with different task distributions can
first improve adaptability by meta-finetuning on related tasks before improving
goal task generalization with finetuning. Synthetic regression experiments
validate the intuition that learning to meta-learn improves adaptability and
consecutively generalization. Experiments on more complex image classification,
continual regression, and reinforcement learning tasks demonstrate that
learning to meta-learn generally improves task-specific adaptation. The
methodology, setup, and hypotheses in this proposal were positively evaluated
by peer review before conclusive experiments were carried out.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devos_A/0/1/0/all/0/1"&gt;Arnout Devos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dandi_Y/0/1/0/all/0/1"&gt;Yatin Dandi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture of Robust Experts (MoRE):A Robust Denoising Method towards multiple perturbations. (arXiv:2104.10586v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10586</id>
        <link href="http://arxiv.org/abs/2104.10586"/>
        <updated>2021-07-21T02:01:36.979Z</updated>
        <summary type="html"><![CDATA[To tackle the susceptibility of deep neural networks to examples, the
adversarial training has been proposed which provides a notion of robust
through an inner maximization problem presenting the first-order embedded
within the outer minimization of the training loss. To generalize the
adversarial robustness over different perturbation types, the adversarial
training method has been augmented with the improved inner maximization
presenting a union of multiple perturbations e.g., various $\ell_p$
norm-bounded perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1"&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldhahn_R/0/1/0/all/0/1"&gt;Ryan Goldhahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Low-rank plus Sparse Network for Dynamic MR Imaging. (arXiv:2010.13677v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13677</id>
        <link href="http://arxiv.org/abs/2010.13677"/>
        <updated>2021-07-21T02:01:36.971Z</updated>
        <summary type="html"><![CDATA[In dynamic magnetic resonance (MR) imaging, low-rank plus sparse (L+S)
decomposition, or robust principal component analysis (PCA), has achieved
stunning performance. However, the selection of the parameters of L+S is
empirical, and the acceleration rate is limited, which are common failings of
iterative compressed sensing MR imaging (CS-MRI) reconstruction methods. Many
deep learning approaches have been proposed to address these issues, but few of
them use a low-rank prior. In this paper, a model-based low-rank plus sparse
network, dubbed L+S-Net, is proposed for dynamic MR reconstruction. In
particular, we use an alternating linearized minimization method to solve the
optimization problem with low-rank and sparse regularization. Learned soft
singular value thresholding is introduced to ensure the clear separation of the
L component and S component. Then, the iterative steps are unrolled into a
network in which the regularization parameters are learnable. We prove that the
proposed L+S-Net achieves global convergence under two standard assumptions.
Experiments on retrospective and prospective cardiac cine datasets show that
the proposed model outperforms state-of-the-art CS and existing deep learning
methods and has great potential for extremely high acceleration factors (up to
24x).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wenqi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ke_Z/0/1/0/all/0/1"&gt;Ziwen Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhuo-Xu Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jing Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1"&gt;Zhilang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Sen Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ying_L/0/1/0/all/0/1"&gt;Leslie Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanjie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1"&gt;Dong Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Believe The HiPe: Hierarchical Perturbation for Fast, Robust and Model-Agnostic Explanations. (arXiv:2103.05108v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05108</id>
        <link href="http://arxiv.org/abs/2103.05108"/>
        <updated>2021-07-21T02:01:36.929Z</updated>
        <summary type="html"><![CDATA[Understanding the predictions made by Artificial Intelligence (AI) systems is
becoming more and more important as deep learning models are used for
increasingly complex and high-stakes tasks. Saliency mapping - an easily
interpretable visual attribution method - is one important tool for this, but
existing formulations are limited by either computational cost or architectural
constraints. We therefore propose Hierarchical Perturbation, a very fast and
completely model-agnostic method for explaining model predictions with robust
saliency maps. Using standard benchmarks and datasets, we show that our
saliency maps are of competitive or superior quality to those generated by
existing model-agnostic methods - and are over 20X faster to compute.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1"&gt;Jessica Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1"&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1"&gt;David J Harrison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning. (arXiv:2107.09645v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.09645</id>
        <link href="http://arxiv.org/abs/2107.09645"/>
        <updated>2021-07-21T02:01:36.923Z</updated>
        <summary type="html"><![CDATA[We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for
visual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic
approach that uses data augmentation to learn directly from pixels. We
introduce several improvements that yield state-of-the-art results on the
DeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid
locomotion tasks directly from pixel observations, previously unattained by
model-free RL. DrQ-v2 is conceptually simple, easy to implement, and provides
significantly better computational footprint compared to prior work, with the
majority of tasks taking just 8 hours to train on a single GPU. Finally, we
publicly release DrQ-v2's implementation to provide RL practitioners with a
strong and computationally efficient baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yarats_D/0/1/0/all/0/1"&gt;Denis Yarats&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1"&gt;Rob Fergus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1"&gt;Alessandro Lazaric&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1"&gt;Lerrel Pinto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quality Evolvability ES: Evolving Individuals With a Distribution of Well Performing and Diverse Offspring. (arXiv:2103.10790v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10790</id>
        <link href="http://arxiv.org/abs/2103.10790"/>
        <updated>2021-07-21T02:01:36.890Z</updated>
        <summary type="html"><![CDATA[One of the most important lessons from the success of deep learning is that
learned representations tend to perform much better at any task compared to
representations we design by hand. Yet evolution of evolvability algorithms,
which aim to automatically learn good genetic representations, have received
relatively little attention, perhaps because of the large amount of
computational power they require. The recent method Evolvability ES allows
direct selection for evolvability with little computation. However, it can only
be used to solve problems where evolvability and task performance are aligned.
We propose Quality Evolvability ES, a method that simultaneously optimizes for
task performance and evolvability and without this restriction. Our proposed
approach Quality Evolvability has similar motivation to Quality Diversity
algorithms, but with some important differences. While Quality Diversity aims
to find an archive of diverse and well-performing, but potentially genetically
distant individuals, Quality Evolvability aims to find a single individual with
a diverse and well-performing distribution of offspring. By doing so Quality
Evolvability is forced to discover more evolvable representations. We
demonstrate on robotic locomotion control tasks that Quality Evolvability ES,
similarly to Quality Diversity methods, can learn faster than objective-based
methods and can handle deceptive problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Katona_A/0/1/0/all/0/1"&gt;Adam Katona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franks_D/0/1/0/all/0/1"&gt;Daniel W. Franks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1"&gt;James Alfred Walker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Conformal Prediction with Auxiliary Tasks. (arXiv:2102.08898v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08898</id>
        <link href="http://arxiv.org/abs/2102.08898"/>
        <updated>2021-07-21T02:01:36.863Z</updated>
        <summary type="html"><![CDATA[We develop a novel approach to conformal prediction when the target task has
limited data available for training. Conformal prediction identifies a small
set of promising output candidates in place of a single prediction, with
guarantees that the set contains the correct answer with high probability. When
training data is limited, however, the predicted set can easily become unusably
large. In this work, we obtain substantially tighter prediction sets while
maintaining desirable marginal guarantees by casting conformal prediction as a
meta-learning paradigm over exchangeable collections of auxiliary tasks. Our
conformalization algorithm is simple, fast, and agnostic to the choice of
underlying model, learning algorithm, or dataset. We demonstrate the
effectiveness of this approach across a number of few-shot classification and
regression tasks in natural language processing, computer vision, and
computational chemistry for drug discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1"&gt;Adam Fisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1"&gt;Tal Schuster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1"&gt;Tommi Jaakkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Billion Audio Sounds from GPU-enabled Modular Synthesis. (arXiv:2104.12922v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12922</id>
        <link href="http://arxiv.org/abs/2104.12922"/>
        <updated>2021-07-21T02:01:36.832Z</updated>
        <summary type="html"><![CDATA[We release synth1B1, a multi-modal audio corpus consisting of 1 billion
4-second synthesized sounds, paired with the synthesis parameters used to
generate them. The dataset is 100x larger than any audio dataset in the
literature. We also introduce torchsynth, an open source modular synthesizer
that generates the synth1B1 samples on-the-fly at 16200x faster than real-time
(714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth
timbre and subtractive synth pitch. Using these datasets, we demonstrate new
rank-based evaluation criteria for existing audio representations. Finally, we
propose a novel approach to synthesizer hyperparameter optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Turian_J/0/1/0/all/0/1"&gt;Joseph Turian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shier_J/0/1/0/all/0/1"&gt;Jordie Shier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzanetakis_G/0/1/0/all/0/1"&gt;George Tzanetakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McNally_K/0/1/0/all/0/1"&gt;Kirk McNally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henry_M/0/1/0/all/0/1"&gt;Max Henry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental Sampling Without Replacement for Sequence Models. (arXiv:2002.09067v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.09067</id>
        <link href="http://arxiv.org/abs/2002.09067"/>
        <updated>2021-07-21T02:01:36.825Z</updated>
        <summary type="html"><![CDATA[Sampling is a fundamental technique, and sampling without replacement is
often desirable when duplicate samples are not beneficial. Within machine
learning, sampling is useful for generating diverse outputs from a trained
model. We present an elegant procedure for sampling without replacement from a
broad class of randomized programs, including generative neural models that
construct outputs sequentially. Our procedure is efficient even for
exponentially-large output spaces. Unlike prior work, our approach is
incremental, i.e., samples can be drawn one at a time, allowing for increased
flexibility. We also present a new estimator for computing expectations from
samples drawn without replacement. We show that incremental sampling without
replacement is applicable to many domains, e.g., program synthesis and
combinatorial optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1"&gt;Kensen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bieber_D/0/1/0/all/0/1"&gt;David Bieber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1"&gt;Charles Sutton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positively Weighted Kernel Quadrature via Subsampling. (arXiv:2107.09597v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2107.09597</id>
        <link href="http://arxiv.org/abs/2107.09597"/>
        <updated>2021-07-21T02:01:36.719Z</updated>
        <summary type="html"><![CDATA[We study kernel quadrature rules with positive weights for probability
measures on general domains. Our theoretical analysis combines the spectral
properties of the kernel with random sampling of points. This results in
effective algorithms to construct kernel quadrature rules with positive weights
and small worst-case error. Besides additional robustness, our numerical
experiments indicate that this can achieve fast convergence rates that compete
with the optimal bounds in well-known examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hayakawa_S/0/1/0/all/0/1"&gt;Satoshi Hayakawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Oberhauser_H/0/1/0/all/0/1"&gt;Harald Oberhauser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lyons_T/0/1/0/all/0/1"&gt;Terry Lyons&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous network-based drug repurposing for COVID-19. (arXiv:2107.09217v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09217</id>
        <link href="http://arxiv.org/abs/2107.09217"/>
        <updated>2021-07-21T02:01:36.701Z</updated>
        <summary type="html"><![CDATA[The Corona Virus Disease 2019 (COVID-19) belongs to human coronaviruses
(HCoVs), which spreads rapidly around the world. Compared with new drug
development, drug repurposing may be the best shortcut for treating COVID-19.
Therefore, we constructed a comprehensive heterogeneous network based on the
HCoVs-related target proteins and use the previously proposed deepDTnet, to
discover potential drug candidates for COVID-19. We obtain high performance in
predicting the possible drugs effective for COVID-19 related proteins. In
summary, this work utilizes a powerful heterogeneous network-based deep
learning method, which may be beneficial to quickly identify candidate
repurposable drugs toward future clinical trials for COVID-19. The code and
data are available at https://github.com/stjin-XMU/HnDR-COVID.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1"&gt;Shuting Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xiangxiang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Feng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Changzhi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangrong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shaoliang Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Riemannian Manifold Optimization for Discriminant Subspace Learning. (arXiv:2101.08032v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08032</id>
        <link href="http://arxiv.org/abs/2101.08032"/>
        <updated>2021-07-21T02:01:36.638Z</updated>
        <summary type="html"><![CDATA[Linear discriminant analysis (LDA) is a widely used algorithm in machine
learning to extract a low-dimensional representation of high-dimensional data,
it features to find the orthogonal discriminant projection subspace by using
the Fisher discriminant criterion. However, the traditional Euclidean-based
methods for solving LDA are easily convergent to spurious local minima and
hardly obtain an optimal solution. To address such a problem, in this paper, we
propose a novel algorithm namely Riemannian-based discriminant analysis (RDA)
for subspace learning. In order to obtain an explicit solution, we transform
the traditional Euclidean-based methods to the Riemannian manifold space and
use the trust-region method to learn the discriminant projection subspace. We
compare the proposed algorithm to existing variants of LDA, as well as the
unsupervised tensor decomposition methods on image classification tasks. The
numerical results suggest that RDA achieves state-of-the-art performance in
classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wanguang Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhengming Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quanying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Sampling for Minimax Fair Classification. (arXiv:2103.00755v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00755</id>
        <link href="http://arxiv.org/abs/2103.00755"/>
        <updated>2021-07-21T02:01:36.628Z</updated>
        <summary type="html"><![CDATA[Machine learning models trained on uncurated datasets can often end up
adversely affecting inputs belonging to underrepresented groups. To address
this issue, we consider the problem of adaptively constructing training sets
which allow us to learn classifiers that are fair in a minimax sense. We first
propose an adaptive sampling algorithm based on the principle of optimism, and
derive theoretical bounds on its performance. We also propose heuristic
extensions of this algorithm suitable for application to large scale, practical
problems. Next, by deriving algorithm independent lower-bounds for a specific
class of problems, we show that the performance achieved by our adaptive scheme
cannot be improved in general. We then validate the benefits of adaptively
constructing training sets via experiments on synthetic tasks with logistic
regression classifiers, as well as on several real-world tasks using
convolutional neural networks (CNNs).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1"&gt;Shubhanshu Shekhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fields_G/0/1/0/all/0/1"&gt;Greg Fields&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1"&gt;Mohammad Ghavamzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1"&gt;Tara Javidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Technical Challenges and Solutions. (arXiv:2107.09546v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09546</id>
        <link href="http://arxiv.org/abs/2107.09546"/>
        <updated>2021-07-21T02:01:36.619Z</updated>
        <summary type="html"><![CDATA[Machine learning is expected to fuel significant improvements in medical
care. To ensure that fundamental principles such as beneficence, respect for
human autonomy, prevention of harm, justice, privacy, and transparency are
respected, medical machine learning applications must be developed responsibly.
In this paper, we survey the technical challenges involved in creating medical
machine learning systems responsibly and in conformity with existing
regulations, as well as possible solutions to address these challenges. We
begin by providing a brief overview of existing regulations affecting medical
machine learning, showing that properties such as safety, robustness,
reliability, privacy, security, transparency, explainability, and
nondiscrimination are all demanded already by existing law and regulations -
albeit, in many cases, to an uncertain degree. Next, we discuss the underlying
technical challenges, possible ways for addressing them, and their respective
merits and drawbacks. We notice that distribution shift, spurious correlations,
model underspecification, and data scarcity represent severe challenges in the
medical context (and others) that are very difficult to solve with classical
black-box deep neural networks. Important measures that may help to address
these challenges include the use of large and representative datasets and
federated learning as a means to that end, the careful exploitation of domain
knowledge wherever feasible, the use of inherently transparent models,
comprehensive model testing and verification, as well as stakeholder inclusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_E/0/1/0/all/0/1"&gt;Eike Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potdevin_Y/0/1/0/all/0/1"&gt;Yannik Potdevin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadi_E/0/1/0/all/0/1"&gt;Esfandiar Mohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zidowitz_S/0/1/0/all/0/1"&gt;Stephan Zidowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breyer_S/0/1/0/all/0/1"&gt;Sabrina Breyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowotka_D/0/1/0/all/0/1"&gt;Dirk Nowotka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henn_S/0/1/0/all/0/1"&gt;Sandra Henn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechmann_L/0/1/0/all/0/1"&gt;Ludwig Pechmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leucker_M/0/1/0/all/0/1"&gt;Martin Leucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostalski_P/0/1/0/all/0/1"&gt;Philipp Rostalski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herzog_C/0/1/0/all/0/1"&gt;Christian Herzog&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DanHAR: Dual Attention Network For Multimodal Human Activity Recognition Using Wearable Sensors. (arXiv:2006.14435v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14435</id>
        <link href="http://arxiv.org/abs/2006.14435"/>
        <updated>2021-07-21T02:01:36.612Z</updated>
        <summary type="html"><![CDATA[Human activity recognition (HAR) in ubiquitous computing has been beginning
to incorporate attention into the context of deep neural networks (DNNs), in
which the rich sensing data from multimodal sensors such as accelerometer and
gyroscope is used to infer human activities. Recently, two attention methods
are proposed via combining with Gated Recurrent Units (GRU) and Long Short-Term
Memory (LSTM) network, which can capture the dependencies of sensing signals in
both spatial and temporal domains simultaneously. However, recurrent networks
often have a weak feature representing power compared with convolutional neural
networks (CNNs). On the other hand, two attention, i.e., hard attention and
soft attention, are applied in temporal domains via combining with CNN, which
pay more attention to the target activity from a long sequence. However, they
can only tell where to focus and miss channel information, which plays an
important role in deciding what to focus. As a result, they fail to address the
spatial-temporal dependencies of multimodal sensing signals, compared with
attention-based GRU or LSTM. In the paper, we propose a novel dual attention
method called DanHAR, which introduces the framework of blending channel
attention and temporal attention on a CNN, demonstrating superiority in
improving the comprehensibility for multimodal HAR. Extensive experiments on
four public HAR datasets and weakly labeled dataset show that DanHAR achieves
state-of-the-art performance with negligible overhead of parameters.
Furthermore, visualizing analysis is provided to show that our attention can
amplifies more important sensor modalities and timesteps during classification,
which agrees well with human common intuition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wenbin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_Q/0/1/0/all/0/1"&gt;Qi Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jun He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPUTreeShap: Massively Parallel Exact Calculation of SHAP Scores for Tree Ensembles. (arXiv:2010.13972v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13972</id>
        <link href="http://arxiv.org/abs/2010.13972"/>
        <updated>2021-07-21T02:01:36.593Z</updated>
        <summary type="html"><![CDATA[SHAP (SHapley Additive exPlanation) values provide a game theoretic
interpretation of the predictions of machine learning models based on Shapley
values. While exact calculation of SHAP values is computationally intractable
in general, a recursive polynomial-time algorithm called TreeShap is available
for decision tree models. However, despite its polynomial time complexity,
TreeShap can become a significant bottleneck in practical machine learning
pipelines when applied to large decision tree ensembles. We present
GPUTreeShap, a modified TreeShap algorithm suitable for massively parallel
computation on graphics processing units. Our approach first preprocesses each
decision tree to isolate variable sized sub-problems from the original
recursive algorithm, then solves a bin packing problem, and finally maps
sub-problems to single-instruction, multiple-thread (SIMT) tasks for parallel
execution with specialised hardware instructions. With a single NVIDIA Tesla
V100-32 GPU, we achieve speedups of up to 19x for SHAP values, and speedups of
up to 340x for SHAP interaction values, over a state-of-the-art multi-core CPU
implementation executed on two 20-core Xeon E5-2698 v4 2.2 GHz CPUs. We also
experiment with multi-GPU computing using eight V100 GPUs, demonstrating
throughput of 1.2M rows per second -- equivalent CPU-based performance is
estimated to require 6850 CPU cores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_R/0/1/0/all/0/1"&gt;Rory Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1"&gt;Eibe Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holmes_G/0/1/0/all/0/1"&gt;Geoffrey Holmes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Privacy-preserving Explanations in Medical Image Analysis. (arXiv:2107.09652v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09652</id>
        <link href="http://arxiv.org/abs/2107.09652"/>
        <updated>2021-07-21T02:01:36.586Z</updated>
        <summary type="html"><![CDATA[The use of Deep Learning in the medical field is hindered by the lack of
interpretability. Case-based interpretability strategies can provide intuitive
explanations for deep learning models' decisions, thus, enhancing trust.
However, the resulting explanations threaten patient privacy, motivating the
development of privacy-preserving methods compatible with the specifics of
medical data. In this work, we analyze existing privacy-preserving methods and
their respective capacity to anonymize medical data while preserving
disease-related semantic features. We find that the PPRL-VGAN deep learning
method was the best at preserving the disease-related semantic features while
guaranteeing a high level of privacy among the compared state-of-the-art
methods. Nevertheless, we emphasize the need to improve privacy-preserving
methods for medical imaging, as we identified relevant drawbacks in all
existing privacy-preserving approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Montenegro_H/0/1/0/all/0/1"&gt;H. Montenegro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_W/0/1/0/all/0/1"&gt;W. Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1"&gt;J. S. Cardoso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine Pseudo-Labeling with Visual-Semantic Meta-Embedding. (arXiv:2007.05675v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05675</id>
        <link href="http://arxiv.org/abs/2007.05675"/>
        <updated>2021-07-21T02:01:36.578Z</updated>
        <summary type="html"><![CDATA[Few-shot learning aims at rapidly adapting to novel categories with only a
handful of samples at test time, which has been predominantly tackled with the
idea of meta-learning. However, meta-learning approaches essentially learn
across a variety of few-shot tasks and thus still require large-scale training
data with fine-grained supervision to derive a generalized model, thereby
involving prohibitive annotation cost. In this paper, we advance the few-shot
classification paradigm towards a more challenging scenario, i.e.,
cross-granularity few-shot classification, where the model observes only coarse
labels during training while is expected to perform fine-grained classification
during testing. This task largely relieves the annotation cost since
fine-grained labeling usually requires strong domain-specific expertise. To
bridge the cross-granularity gap, we approximate the fine-grained data
distribution by greedy clustering of each coarse-class into pseudo-fine-classes
according to the similarity of image embeddings. We then propose a
meta-embedder that jointly optimizes the visual- and semantic-discrimination,
in both instance-wise and coarse class-wise, to obtain a good feature space for
this coarse-to-fine pseudo-labeling process. Extensive experiments and ablation
studies are conducted to demonstrate the effectiveness and robustness of our
approach on three representative datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinhai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hua Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Assisted Calibrated Beam Training for Millimeter-Wave Communication Systems. (arXiv:2101.05206v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05206</id>
        <link href="http://arxiv.org/abs/2101.05206"/>
        <updated>2021-07-21T02:01:36.571Z</updated>
        <summary type="html"><![CDATA[Huge overhead of beam training imposes a significant challenge in
millimeter-wave (mmWave) wireless communications. To address this issue, in
this paper, we propose a wide beam based training approach to calibrate the
narrow beam direction according to the channel power leakage. To handle the
complex nonlinear properties of the channel power leakage, deep learning is
utilized to predict the optimal narrow beam directly. Specifically, three deep
learning assisted calibrated beam training schemes are proposed. The first
scheme adopts convolution neural network to implement the prediction based on
the instantaneous received signals of wide beam training. We also perform the
additional narrow beam training based on the predicted probabilities for
further beam direction calibrations. However, the first scheme only depends on
one wide beam training, which lacks the robustness to noise. To tackle this
problem, the second scheme adopts long-short term memory (LSTM) network for
tracking the movement of users and calibrating the beam direction according to
the received signals of prior beam training, in order to enhance the robustness
to noise. To further reduce the overhead of wide beam training, our third
scheme, an adaptive beam training strategy, selects partial wide beams to be
trained based on the prior received signals. Two criteria, namely, optimal
neighboring criterion and maximum probability criterion, are designed for the
selection. Furthermore, to handle mobile scenarios, auxiliary LSTM is
introduced to calibrate the directions of the selected wide beams more
precisely. Simulation results demonstrate that our proposed schemes achieve
significantly higher beamforming gain with smaller beam training overhead
compared with the conventional and existing deep-learning based counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1"&gt;Ke Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1"&gt;Dongxuan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hancun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaocheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sheng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy Disaggregation using Variational Autoencoders. (arXiv:2103.12177v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12177</id>
        <link href="http://arxiv.org/abs/2103.12177"/>
        <updated>2021-07-21T02:01:36.564Z</updated>
        <summary type="html"><![CDATA[Non-intrusive load monitoring (NILM) is a technique that uses a single sensor
to measure the total power consumption of a building. Using an energy
disaggregation method, the consumption of individual appliances can be
estimated from the aggregate measurement. Recent disaggregation algorithms have
significantly improved the performance of NILM systems. However, the
generalization capability of these methods to different houses as well as the
disaggregation of multi-state appliances are still major challenges. In this
paper we address these issues and propose an energy disaggregation approach
based on the variational autoencoders framework. The probabilistic encoder
makes this approach an efficient model for encoding information relevant to the
reconstruction of the target appliance consumption. In particular, the proposed
model accurately generates more complex load profiles, thus improving the power
signal reconstruction of multi-state appliances. Moreover, its regularized
latent space improves the generalization capabilities of the model across
different houses. The proposed model is compared to state-of-the-art NILM
approaches on the UK-DALE and REFIT datasets, and yields competitive results.
The mean absolute error reduces by 18% on average across all appliances
compared to the state-of-the-art. The F1-Score increases by more than 11%,
showing improvements for the detection of the target appliance in the aggregate
measurement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Langevin_A/0/1/0/all/0/1"&gt;Antoine Langevin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carbonneau_M/0/1/0/all/0/1"&gt;Marc-Andr&amp;#xe9; Carbonneau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheriet_M/0/1/0/all/0/1"&gt;Mohamed Cheriet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gagnon_G/0/1/0/all/0/1"&gt;Ghyslain Gagnon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Claim Verification using a Multi-GAN based Model. (arXiv:2103.08001v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08001</id>
        <link href="http://arxiv.org/abs/2103.08001"/>
        <updated>2021-07-21T02:01:36.542Z</updated>
        <summary type="html"><![CDATA[This article describes research on claim verification carried out using a
multiple GAN-based model. The proposed model consists of three pairs of
generators and discriminators. The generator and discriminator pairs are
responsible for generating synthetic data for supported and refuted claims and
claim labels. A theoretical discussion about the proposed model is provided to
validate the equilibrium state of the model. The proposed model is applied to
the FEVER dataset, and a pre-trained language model is used for the input text
data. The synthetically generated data helps to gain information which helps
the model to perform better than state of the art models and other standard
classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hatua_A/0/1/0/all/0/1"&gt;Amartya Hatua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1"&gt;Arjun Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1"&gt;Rakesh M. Verma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Altruistic Behaviours in Reinforcement Learning without External Rewards. (arXiv:2107.09598v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.09598</id>
        <link href="http://arxiv.org/abs/2107.09598"/>
        <updated>2021-07-21T02:01:36.535Z</updated>
        <summary type="html"><![CDATA[Can artificial agents learn to assist others in achieving their goals without
knowing what those goals are? Generic reinforcement learning agents could be
trained to behave altruistically towards others by rewarding them for
altruistic behaviour, i.e., rewarding them for benefiting other agents in a
given situation. Such an approach assumes that other agents' goals are known so
that the altruistic agent can cooperate in achieving those goals. However,
explicit knowledge of other agents' goals is often difficult to acquire. Even
assuming such knowledge to be given, training of altruistic agents would
require manually-tuned external rewards for each new environment. Thus, it is
beneficial to develop agents that do not depend on external supervision and can
learn altruistic behaviour in a task-agnostic manner. Assuming that other
agents rationally pursue their goals, we hypothesize that giving them more
choices will allow them to pursue those goals better. Some concrete examples
include opening a door for others or safeguarding them to pursue their
objectives without interference. We formalize this concept and propose an
altruistic agent that learns to increase the choices another agent has by
maximizing the number of states that the other agent can reach in its future.
We evaluate our approach on three different multi-agent environments where
another agent's success depends on the altruistic agent's behaviour. Finally,
we show that our unsupervised agents can perform comparably to agents
explicitly trained to work cooperatively. In some cases, our agents can even
outperform the supervised ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Franzmeyer_T/0/1/0/all/0/1"&gt;Tim Franzmeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o F. Henriques&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline RL Without Off-Policy Evaluation. (arXiv:2106.08909v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08909</id>
        <link href="http://arxiv.org/abs/2106.08909"/>
        <updated>2021-07-21T02:01:36.527Z</updated>
        <summary type="html"><![CDATA[Most prior approaches to offline reinforcement learning (RL) have taken an
iterative actor-critic approach involving off-policy evaluation. In this paper
we show that simply doing one step of constrained/regularized policy
improvement using an on-policy Q estimate of the behavior policy performs
surprisingly well. This one-step algorithm beats the previously reported
results of iterative algorithms on a large portion of the D4RL benchmark. The
simple one-step baseline achieves this strong performance without many of the
tricks used by previously proposed iterative algorithms and is more robust to
hyperparameters. We argue that the relatively poor performance of iterative
approaches is a result of the high variance inherent in doing off-policy
evaluation and magnified by the repeated optimization of policies against those
high-variance estimates. In addition, we hypothesize that the strong
performance of the one-step algorithm is due to a combination of favorable
structure in the environment and behavior policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandfonbrener_D/0/1/0/all/0/1"&gt;David Brandfonbrener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1"&gt;William F. Whitney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1"&gt;Joan Bruna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09543</id>
        <link href="http://arxiv.org/abs/2107.09543"/>
        <updated>2021-07-21T02:01:36.520Z</updated>
        <summary type="html"><![CDATA[Despite technological and medical advances, the detection, interpretation,
and treatment of cancer based on imaging data continue to pose significant
challenges. These include high inter-observer variability, difficulty of
small-sized lesion detection, nodule interpretation and malignancy
determination, inter- and intra-tumour heterogeneity, class imbalance,
segmentation inaccuracies, and treatment effect uncertainty. The recent
advancements in Generative Adversarial Networks (GANs) in computer vision as
well as in medical imaging may provide a basis for enhanced capabilities in
cancer detection and analysis. In this review, we assess the potential of GANs
to address a number of key challenges of cancer imaging, including data
scarcity and imbalance, domain and dataset shifts, data access and privacy,
data annotation and quantification, as well as cancer detection, tumour
profiling and treatment planning. We provide a critical appraisal of the
existing literature of GANs applied to cancer imagery, together with
suggestions on future research directions to address these challenges. We
analyse and discuss 163 papers that apply adversarial training techniques in
the context of cancer imaging and elaborate their methodologies, advantages and
limitations. With this work, we strive to bridge the gap between the needs of
the clinical cancer imaging community and the current and prospective research
on GANs in the artificial intelligence community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1"&gt;Richard Osuala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1"&gt;Kaisar Kushibar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1"&gt;Lidia Garrucho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1"&gt;Akis Linardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1"&gt;Zuzanna Szafranowska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1"&gt;Stefan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1"&gt;Oliver Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proximal Policy Optimization for Tracking Control Exploiting Future Reference Information. (arXiv:2107.09647v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09647</id>
        <link href="http://arxiv.org/abs/2107.09647"/>
        <updated>2021-07-21T02:01:36.513Z</updated>
        <summary type="html"><![CDATA[In recent years, reinforcement learning (RL) has gained increasing attention
in control engineering. Especially, policy gradient methods are widely used. In
this work, we improve the tracking performance of proximal policy optimization
(PPO) for arbitrary reference signals by incorporating information about future
reference values. Two variants of extending the argument of the actor and the
critic taking future reference values into account are presented. In the first
variant, global future reference values are added to the argument. For the
second variant, a novel kind of residual space with future reference values
applicable to model-free reinforcement learning is introduced. Our approach is
evaluated against a PI controller on a simple drive train model. We expect our
method to generalize to arbitrary references better than previous approaches,
pointing towards the applicability of RL to control real systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mayer_J/0/1/0/all/0/1"&gt;Jana Mayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Westermann_J/0/1/0/all/0/1"&gt;Johannes Westermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muriedas_J/0/1/0/all/0/1"&gt;Juan Pedro Guti&amp;#xe9;rrez H. Muriedas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mettin_U/0/1/0/all/0/1"&gt;Uwe Mettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampe_A/0/1/0/all/0/1"&gt;Alexander Lampe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification. (arXiv:2102.07856v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07856</id>
        <link href="http://arxiv.org/abs/2102.07856"/>
        <updated>2021-07-21T02:01:36.497Z</updated>
        <summary type="html"><![CDATA[Modern machine learning models with high accuracy are often miscalibrated --
the predicted top probability does not reflect the actual accuracy, and tends
to be over-confident. It is commonly believed that such over-confidence is
mainly due to over-parametrization, in particular when the model is large
enough to memorize the training data and maximize the confidence.

In this paper, we show theoretically that over-parametrization is not the
only reason for over-confidence. We prove that logistic regression is
inherently over-confident, in the realizable, under-parametrized setting where
the data is generated from the logistic model, and the sample size is much
larger than the number of parameters. Further, this over-confidence happens for
general well-specified binary classification problems as long as the activation
is symmetric and concave on the positive part. Perhaps surprisingly, we also
show that over-confidence is not always the case -- there exists another
activation function (and a suitable loss function) under which the learned
classifier is under-confident at some probability values. Overall, our theory
provides a precise characterization of calibration in realizable binary
classification, which we verify on simulations and real data experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yu Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1"&gt;Song Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture Density Network Estimation of Continuous Variable Maximum Likelihood Using Discrete Training Samples. (arXiv:2103.13416v2 [physics.data-an] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13416</id>
        <link href="http://arxiv.org/abs/2103.13416"/>
        <updated>2021-07-21T02:01:36.490Z</updated>
        <summary type="html"><![CDATA[Mixture Density Networks (MDNs) can be used to generate probability density
functions of model parameters $\boldsymbol{\theta}$ given a set of observables
$\mathbf{x}$. In some applications, training data are available only for
discrete values of a continuous parameter $\boldsymbol{\theta}$. In such
situations a number of performance-limiting issues arise which can result in
biased estimates. We demonstrate the usage of MDNs for parameter estimation,
discuss the origins of the biases, and propose a corrective method for each
issue.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Burton_C/0/1/0/all/0/1"&gt;Charles Burton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Stubbs_S/0/1/0/all/0/1"&gt;Spencer Stubbs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Onyisi_P/0/1/0/all/0/1"&gt;Peter Onyisi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Sparse Coding using Hierarchical Riemannian Pursuit. (arXiv:2104.10314v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10314</id>
        <link href="http://arxiv.org/abs/2104.10314"/>
        <updated>2021-07-21T02:01:36.482Z</updated>
        <summary type="html"><![CDATA[Sparse coding is a class of unsupervised methods for learning a sparse
representation of the input data in the form of a linear combination of a
dictionary and a sparse code. This learning framework has led to
state-of-the-art results in various image and video processing tasks. However,
classical methods learn the dictionary and the sparse code based on alternating
optimizations, usually without theoretical guarantees for either optimality or
convergence due to non-convexity of the problem. Recent works on sparse coding
with a complete dictionary provide strong theoretical guarantees thanks to the
development of the non-convex optimization. However, initial non-convex
approaches learn the dictionary in the sparse coding problem sequentially in an
atom-by-atom manner, which leads to a long execution time. More recent works
seek to directly learn the entire dictionary at once, which substantially
reduces the execution time. However, the associated recovery performance is
degraded with a finite number of data samples. In this paper, we propose an
efficient sparse coding scheme with a two-stage optimization. The proposed
scheme leverages the global and local Riemannian geometry of the two-stage
optimization problem and facilitates fast implementation for superb dictionary
recovery performance by a finite number of samples without atom-by-atom
calculation. We further prove that, with high probability, the proposed scheme
can exactly recover any atom in the target dictionary with a finite number of
samples if it is adopted to recover one atom of the dictionary. An application
on wireless sensor data compression is also proposed. Experiments on both
synthetic and real-world data verify the efficiency and effectiveness of the
proposed scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Ye Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_V/0/1/0/all/0/1"&gt;Vincent Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1"&gt;Songfu Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Schr\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02081</id>
        <link href="http://arxiv.org/abs/2106.02081"/>
        <updated>2021-07-21T02:01:36.474Z</updated>
        <summary type="html"><![CDATA[The Schr\"odinger bridge problem (SBP) finds the most likely stochastic
evolution between two probability distributions given a prior stochastic
evolution. As well as applications in the natural sciences, problems of this
kind have important applications in machine learning such as dataset alignment
and hypothesis testing. Whilst the theory behind this problem is relatively
mature, scalable numerical recipes to estimate the Schr\"odinger bridge remain
an active area of research. We prove an equivalence between the SBP and maximum
likelihood estimation enabling direct application of successful machine
learning techniques. We propose a numerical procedure to estimate SBPs using
Gaussian process and demonstrate the practical usage of our approach in
numerical simulations and experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1"&gt;Francisco Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1"&gt;Pierre Thodoroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1"&gt;Neil D. Lawrence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1"&gt;Austen Lamacraft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Tradeoff in Integrated Sensing and Communication: Recognition Accuracy versus Communication Rate. (arXiv:2107.09621v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.09621</id>
        <link href="http://arxiv.org/abs/2107.09621"/>
        <updated>2021-07-21T02:01:36.468Z</updated>
        <summary type="html"><![CDATA[Integrated sensing and communication (ISAC) is a promising technology to
improve the band-utilization efficiency via spectrum sharing or hardware
sharing between radar and communication systems. Since a common radio resource
budget is shared by both functionalities, there exists a tradeoff between the
sensing and communication performance. However, this tradeoff curve is
currently unknown in ISAC systems with human motion recognition tasks based on
deep learning. To fill this gap, this paper formulates and solves a
multi-objective optimization problem which simultaneously maximizes the
recognition accuracy and the communication data rate. The key ingredient of
this new formulation is a nonlinear recognition accuracy model with respect to
the wireless resources, where the model is derived from power function
regression of the system performance of the deep spectrogram network. To avoid
cost-expensive data collection procedures, a primitive-based autoregressive
hybrid (PBAH) channel model is developed, which facilitates efficient training
and testing dataset generation for human motion recognition in a virtual
environment. Extensive results demonstrate that the proposed wireless
recognition accuracy and PBAH channel models match the actual experimental data
very well. Moreover, it is found that the accuracy-rate region consists of a
communication saturation zone, a sensing saturation zone, and a
communication-sensing adversarial zone, of which the third zone achieves the
desirable balanced performance for ISAC systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guoliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Meihong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xiaohui Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1"&gt;Tony Xiao Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Meta-Reinforcement Learning with Online Self-Supervision. (arXiv:2107.03974v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03974</id>
        <link href="http://arxiv.org/abs/2107.03974"/>
        <updated>2021-07-21T02:01:36.449Z</updated>
        <summary type="html"><![CDATA[Meta-reinforcement learning (RL) can meta-train policies that adapt to new
tasks with orders of magnitude less data than standard RL, but meta-training
itself is costly and time-consuming. If we can meta-train on offline data, then
we can reuse the same static dataset, labeled once with rewards for different
tasks, to meta-train policies that adapt to a variety of new tasks at meta-test
time. Although this capability would make meta-RL a practical tool for
real-world use, offline meta-RL presents additional challenges beyond online
meta-RL or standard offline RL settings. Meta-RL learns an exploration strategy
that collects data for adapting, and also meta-trains a policy that quickly
adapts to data from a new task. Since this policy was meta-trained on a fixed,
offline dataset, it might behave unpredictably when adapting to data collected
by the learned exploration strategy, which differs systematically from the
offline data and thus induces distributional shift. We do not want to remove
this distributional shift by simply adopting a conservative exploration
strategy, because learning an exploration strategy enables an agent to collect
better data for faster adaptation. Instead, we propose a hybrid offline meta-RL
algorithm, which uses offline data with rewards to meta-train an adaptive
policy, and then collects additional unsupervised online data, without any
reward labels to bridge this distribution shift. By not requiring reward labels
for online collection, this data can be much cheaper to collect. We compare our
method to prior work on offline meta-RL on simulated robot locomotion and
manipulation tasks and find that using additional unsupervised online data
collection leads to a dramatic improvement in the adaptive capabilities of the
meta-trained policies, matching the performance of fully online meta-RL on a
range of challenging domains that require generalization to new tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1"&gt;Vitchyr H. Pong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1"&gt;Ashvin Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1"&gt;Laura Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Catherine Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Representations and Neural Network Estimation of R\'enyi Divergences. (arXiv:2007.03814v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03814</id>
        <link href="http://arxiv.org/abs/2007.03814"/>
        <updated>2021-07-21T02:01:36.442Z</updated>
        <summary type="html"><![CDATA[We derive a new variational formula for the R\'enyi family of divergences,
$R_\alpha(Q\|P)$, between probability measures $Q$ and $P$. Our result
generalizes the classical Donsker-Varadhan variational formula for the
Kullback-Leibler divergence. We further show that this R\'enyi variational
formula holds over a range of function spaces; this leads to a formula for the
optimizer under very weak assumptions and is also key in our development of a
consistency theory for R\'enyi divergence estimators. By applying this theory
to neural-network estimators, we show that if a neural network family satisfies
one of several strengthened versions of the universal approximation property
then the corresponding R\'enyi divergence estimator is consistent. In contrast
to density-estimator based methods, our estimators involve only expectations
under $Q$ and $P$ and hence are more effective in high dimensional systems. We
illustrate this via several numerical examples of neural network estimation in
systems of up to 5000 dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Birrell_J/0/1/0/all/0/1"&gt;Jeremiah Birrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dupuis_P/0/1/0/all/0/1"&gt;Paul Dupuis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Katsoulakis_M/0/1/0/all/0/1"&gt;Markos A. Katsoulakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rey_Bellet_L/0/1/0/all/0/1"&gt;Luc Rey-Bellet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jie Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best-of-All-Worlds Bounds for Online Learning with Feedback Graphs. (arXiv:2107.09572v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09572</id>
        <link href="http://arxiv.org/abs/2107.09572"/>
        <updated>2021-07-21T02:01:36.436Z</updated>
        <summary type="html"><![CDATA[We study the online learning with feedback graphs framework introduced by
Mannor and Shamir (2011), in which the feedback received by the online learner
is specified by a graph $G$ over the available actions. We develop an algorithm
that simultaneously achieves regret bounds of the form:
$\smash{\mathcal{O}(\sqrt{\theta(G) T})}$ with adversarial losses;
$\mathcal{O}(\theta(G)\operatorname{polylog}{T})$ with stochastic losses; and
$\mathcal{O}(\theta(G)\operatorname{polylog}{T} + \smash{\sqrt{\theta(G) C})}$
with stochastic losses subject to $C$ adversarial corruptions. Here,
$\theta(G)$ is the clique covering number of the graph $G$. Our algorithm is an
instantiation of Follow-the-Regularized-Leader with a novel regularization that
can be seen as a product of a Tsallis entropy component (inspired by Zimmert
and Seldin (2019)) and a Shannon entropy component (analyzed in the corrupted
stochastic case by Amir et al. (2020)), thus subtly interpolating between the
two forms of entropies. One of our key technical contributions is in
establishing the convexity of this regularizer and controlling its inverse
Hessian, despite its complex product structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Erez_L/0/1/0/all/0/1"&gt;Liad Erez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1"&gt;Tomer Koren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation Theory of Convolutional Architectures for Time Series Modelling. (arXiv:2107.09355v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09355</id>
        <link href="http://arxiv.org/abs/2107.09355"/>
        <updated>2021-07-21T02:01:36.429Z</updated>
        <summary type="html"><![CDATA[We study the approximation properties of convolutional architectures applied
to time series modelling, which can be formulated mathematically as a
functional approximation problem. In the recurrent setting, recent results
reveal an intricate connection between approximation efficiency and memory
structures in the data generation process. In this paper, we derive parallel
results for convolutional architectures, with WaveNet being a prime example.
Our results reveal that in this new setting, approximation efficiency is not
only characterised by memory, but also additional fine structures in the target
relationship. This leads to a novel definition of spectrum-based regularity
that measures the complexity of temporal relationships under the convolutional
approximation scheme. These analyses provide a foundation to understand the
differences between architectural choices for time series modelling and can
give theoretically grounded guidance for practical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haotian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qianxiao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open Problem: Is There an Online Learning Algorithm That Learns Whenever Online Learning Is Possible?. (arXiv:2107.09542v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09542</id>
        <link href="http://arxiv.org/abs/2107.09542"/>
        <updated>2021-07-21T02:01:36.422Z</updated>
        <summary type="html"><![CDATA[This open problem asks whether there exists an online learning algorithm for
binary classification that guarantees, for all target concepts, to make a
sublinear number of mistakes, under only the assumption that the (possibly
random) sequence of points X allows that such a learning algorithm can exist
for that sequence. As a secondary problem, it also asks whether a specific
concise condition completely determines whether a given (possibly random)
sequence of points X admits the existence of online learning algorithms
guaranteeing a sublinear number of mistakes for all target concepts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1"&gt;Steve Hanneke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA['CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06212</id>
        <link href="http://arxiv.org/abs/2107.06212"/>
        <updated>2021-07-21T02:01:36.416Z</updated>
        <summary type="html"><![CDATA[Ongoing advancements in the fields of 3D modelling and digital archiving have
led to an outburst in the amount of data stored digitally. Consequently,
several retrieval systems have been developed depending on the type of data
stored in these databases. However, unlike text data or images, performing a
search for 3D models is non-trivial. Among 3D models, retrieving 3D
Engineering/CAD models or mechanical components is even more challenging due to
the presence of holes, volumetric features, presence of sharp edges etc., which
make CAD a domain unto itself. The research work presented in this paper aims
at developing a dataset suitable for building a retrieval system for 3D CAD
models based on deep learning. 3D CAD models from the available CAD databases
are collected, and a dataset of computer-generated sketch data, termed
'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the
components are also added to CADSketchNet. Using the sketch images from this
dataset, the paper also aims at evaluating the performance of various retrieval
system or a search engine for 3D CAD models that accepts a sketch image as the
input query. Many experimental models are constructed and tested on
CADSketchNet. These experiments, along with the model architecture, choice of
similarity metrics are reported along with the search results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1"&gt;Bharadwaj Manda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1"&gt;Shubham Dhayarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1"&gt;Sai Mitheran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1"&gt;V.K. Viekash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1"&gt;Ramanathan Muthuganapathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text Spotting. (arXiv:2105.03620v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03620</id>
        <link href="http://arxiv.org/abs/2105.03620"/>
        <updated>2021-07-21T02:01:36.398Z</updated>
        <summary type="html"><![CDATA[End-to-end text-spotting, which aims to integrate detection and recognition
in a unified framework, has attracted increasing attention due to its
simplicity of the two complimentary tasks. It remains an open problem
especially when processing arbitrarily-shaped text instances. Previous methods
can be roughly categorized into two groups: character-based and
segmentation-based, which often require character-level annotations and/or
complex post-processing due to the unstructured output. Here, we tackle
end-to-end text spotting by presenting Adaptive Bezier Curve Network v2 (ABCNet
v2). Our main contributions are four-fold: 1) For the first time, we adaptively
fit arbitrarily-shaped text by a parameterized Bezier curve, which, compared
with segmentation-based methods, can not only provide structured output but
also controllable representation. 2) We design a novel BezierAlign layer for
extracting accurate convolution features of a text instance of arbitrary
shapes, significantly improving the precision of recognition over previous
methods. 3) Different from previous methods, which often suffer from complex
post-processing and sensitive hyper-parameters, our ABCNet v2 maintains a
simple pipeline with the only post-processing non-maximum suppression (NMS). 4)
As the performance of text recognition closely depends on feature alignment,
ABCNet v2 further adopts a simple yet effective coordinate convolution to
encode the position of the convolutional filters, which leads to a considerable
improvement with negligible computation overhead. Comprehensive experiments
conducted on various bilingual (English and Chinese) benchmark datasets
demonstrate that ABCNet v2 can achieve state-of-the-art performance while
maintaining very high efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Lianwen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Peng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chongyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parametric Scattering Networks. (arXiv:2107.09539v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09539</id>
        <link href="http://arxiv.org/abs/2107.09539"/>
        <updated>2021-07-21T02:01:36.391Z</updated>
        <summary type="html"><![CDATA[The wavelet scattering transform creates geometric invariants and deformation
stability from an initial structured signal. In multiple signal domains it has
been shown to yield more discriminative representations compared to other
non-learned representations, and to outperform learned representations in
certain tasks, particularly on limited labeled data and highly structured
signals. The wavelet filters used in the scattering transform are typically
selected to create a tight frame via a parameterized mother wavelet. Focusing
on Morlet wavelets, we propose to instead adapt the scales, orientations, and
slants of the filters to produce problem-specific parametrizations of the
scattering transform. We show that our learned versions of the scattering
transform yield significant performance gains over the standard scattering
transform in the small sample classification settings, and our empirical
results suggest that tight frames may not always be necessary for scattering
transforms to extract effective representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gauthier_S/0/1/0/all/0/1"&gt;Shanel Gauthier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Therien_B/0/1/0/all/0/1"&gt;Benjamin Th&amp;#xe9;rien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alsene_Racicot_L/0/1/0/all/0/1"&gt;Laurent Als&amp;#xe8;ne-Racicot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1"&gt;Irina Rish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1"&gt;Eugene Belilovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eickenberg_M/0/1/0/all/0/1"&gt;Michael Eickenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1"&gt;Guy Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Does Cell-Free Massive MIMO Support Multiple Federated Learning Groups?. (arXiv:2107.09577v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.09577</id>
        <link href="http://arxiv.org/abs/2107.09577"/>
        <updated>2021-07-21T02:01:36.385Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has been considered as a promising learning framework
for future machine learning systems due to its privacy preservation and
communication efficiency. In beyond-5G/6G systems, it is likely to have
multiple FL groups with different learning purposes. This scenario leads to a
question: How does a wireless network support multiple FL groups? As an answer,
we first propose to use a cell-free massive multiple-input multiple-output
(MIMO) network to guarantee the stable operation of multiple FL processes by
letting the iterations of these FL processes be executed together within a
large-scale coherence time. We then develop a novel scheme that asynchronously
executes the iterations of FL processes under multicasting downlink and
conventional uplink transmission protocols. Finally, we propose a
simple/low-complexity resource allocation algorithm which optimally chooses the
power and computation resources to minimize the execution time of each
iteration of each FL process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1"&gt;Tung T. Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1"&gt;Hien Quoc Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marzetta_T/0/1/0/all/0/1"&gt;Thomas L. Marzetta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matthaiou_M/0/1/0/all/0/1"&gt;Michail Matthaiou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature-Filter: Detecting Adversarial Examples through Filtering off Recessive Features. (arXiv:2107.09502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09502</id>
        <link href="http://arxiv.org/abs/2107.09502"/>
        <updated>2021-07-21T02:01:36.368Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are under threat from adversarial example
attacks. The adversary can easily change the outputs of DNNs by adding small
well-designed perturbations to inputs. Adversarial example detection is a
fundamental work for robust DNNs-based service. Adversarial examples show the
difference between humans and DNNs in image recognition. From a human-centric
perspective, image features could be divided into dominant features that are
comprehensible to humans, and recessive features that are incomprehensible to
humans, yet are exploited by DNNs. In this paper, we reveal that imperceptible
adversarial examples are the product of recessive features misleading neural
networks, and an adversarial attack is essentially a kind of method to enrich
these recessive features in the image. The imperceptibility of the adversarial
examples indicates that the perturbations enrich recessive features, yet hardly
affect dominant features. Therefore, adversarial examples are sensitive to
filtering off recessive features, while benign examples are immune to such
operation. Inspired by this idea, we propose a label-only adversarial detection
approach that is referred to as feature-filter. Feature-filter utilizes
discrete cosine transform to approximately separate recessive features from
dominant features, and gets a mutant image that is filtered off recessive
features. By only comparing DNN's prediction labels on the input and its
mutant, feature-filter can real-time detect imperceptible adversarial examples
at high accuracy and few false positives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1"&gt;Bo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yuefeng Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiabao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CheXbreak: Misclassification Identification for Deep Learning Models Interpreting Chest X-rays. (arXiv:2103.09957v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09957</id>
        <link href="http://arxiv.org/abs/2103.09957"/>
        <updated>2021-07-21T02:01:36.362Z</updated>
        <summary type="html"><![CDATA[A major obstacle to the integration of deep learning models for chest x-ray
interpretation into clinical settings is the lack of understanding of their
failure modes. In this work, we first investigate whether there are patient
subgroups that chest x-ray models are likely to misclassify. We find that
patient age and the radiographic finding of lung lesion, pneumothorax or
support devices are statistically relevant features for predicting
misclassification for some chest x-ray models. Second, we develop
misclassification predictors on chest x-ray models using their outputs and
clinical features. We find that our best performing misclassification
identifier achieves an AUROC close to 0.9 for most diseases. Third, employing
our misclassification identifiers, we develop a corrective algorithm to
selectively flip model predictions that have high likelihood of
misclassification at inference time. We observe F1 improvement on the
prediction of Consolidation (0.008 [95% CI 0.005, 0.010]) and Edema (0.003,
[95% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and
high-performing chest x-ray models, we are able to derive insights across model
architectures and offer a generalizable framework applicable to other medical
imaging tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Emma Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1"&gt;Andy Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1"&gt;Rayan Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1"&gt;Jin Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Protecting Semantic Segmentation Models by Using Block-wise Image Encryption with Secret Key from Unauthorized Access. (arXiv:2107.09362v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09362</id>
        <link href="http://arxiv.org/abs/2107.09362"/>
        <updated>2021-07-21T02:01:36.354Z</updated>
        <summary type="html"><![CDATA[Since production-level trained deep neural networks (DNNs) are of a great
business value, protecting such DNN models against copyright infringement and
unauthorized access is in a rising demand. However, conventional model
protection methods focused only the image classification task, and these
protection methods were never applied to semantic segmentation although it has
an increasing number of applications. In this paper, we propose to protect
semantic segmentation models from unauthorized access by utilizing block-wise
transformation with a secret key for the first time. Protected models are
trained by using transformed images. Experiment results show that the proposed
protection method allows rightful users with the correct key to access the
model to full capacity and deteriorate the performance for unauthorized users.
However, protected models slightly drop the segmentation performance compared
to non-protected models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ito_H/0/1/0/all/0/1"&gt;Hiroki Ito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+AprilPyone_M/0/1/0/all/0/1"&gt;MaungMaung AprilPyone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GNN4IP: Graph Neural Network for Hardware Intellectual Property Piracy Detection. (arXiv:2107.09130v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.09130</id>
        <link href="http://arxiv.org/abs/2107.09130"/>
        <updated>2021-07-21T02:01:36.345Z</updated>
        <summary type="html"><![CDATA[Aggressive time-to-market constraints and enormous hardware design and
fabrication costs have pushed the semiconductor industry toward hardware
Intellectual Properties (IP) core design. However, the globalization of the
integrated circuits (IC) supply chain exposes IP providers to theft and illegal
redistribution of IPs. Watermarking and fingerprinting are proposed to detect
IP piracy. Nevertheless, they come with additional hardware overhead and cannot
guarantee IP security as advanced attacks are reported to remove the watermark,
forge, or bypass it. In this work, we propose a novel methodology, GNN4IP, to
assess similarities between circuits and detect IP piracy. We model the
hardware design as a graph and construct a graph neural network model to learn
its behavior using the comprehensive dataset of register transfer level codes
and gate-level netlists that we have gathered. GNN4IP detects IP piracy with
96% accuracy in our dataset and recognizes the original IP in its obfuscated
version with 100% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yasaei_R/0/1/0/all/0/1"&gt;Rozhin Yasaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shih-Yuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeini_E/0/1/0/all/0/1"&gt;Emad Kasaeyan Naeini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1"&gt;Mohammad Abdullah Al Faruque&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modality Fusion Network and Personalized Attention in Momentary Stress Detection in the Wild. (arXiv:2107.09510v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.09510</id>
        <link href="http://arxiv.org/abs/2107.09510"/>
        <updated>2021-07-21T02:01:36.339Z</updated>
        <summary type="html"><![CDATA[Multimodal wearable physiological data in daily life settings have been used
to estimate self-reported stress labels.However, missing data modalities in
data collection make it challenging to leverage all the collected samples.
Besides, heterogeneous sensor data and labels among individuals add challenges
in building robust stress detection models. In this paper, we proposed a
modality fusion network (MFN) to train models and infer self-reported binary
stress labels under both complete and incomplete modality condition. In
addition, we applied a personalized attention (PA) strategy to leverage
personalized representation along with the generalized one-size-fits-all model.
We evaluated our methods on a multimodal wearable sensor dataset (N=41)
including galvanic skin response (GSR) and electrocardiogram (ECG). Compared to
the baseline method using the samples with complete modalities, the performance
of the MFN improved by 1.6\% in f1-scores. On the other hand, the proposed PA
strategy showed a 2.3\% higher stress detection f1-score and approximately up
to 70\% reduction in personalized model parameter size (9.1 MB) compared to the
previous state-of-the-art transfer learning strategy (29.3 MB).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vaessen_T/0/1/0/all/0/1"&gt;Thomas Vaessen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Myin_Germeys_I/0/1/0/all/0/1"&gt;Inez Myin-Germeys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sano_A/0/1/0/all/0/1"&gt;Akane Sano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithm Selection on a Meta Level. (arXiv:2107.09414v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09414</id>
        <link href="http://arxiv.org/abs/2107.09414"/>
        <updated>2021-07-21T02:01:36.271Z</updated>
        <summary type="html"><![CDATA[The problem of selecting an algorithm that appears most suitable for a
specific instance of an algorithmic problem class, such as the Boolean
satisfiability problem, is called instance-specific algorithm selection. Over
the past decade, the problem has received considerable attention, resulting in
a number of different methods for algorithm selection. Although most of these
methods are based on machine learning, surprisingly little work has been done
on meta learning, that is, on taking advantage of the complementarity of
existing algorithm selection methods in order to combine them into a single
superior algorithm selector. In this paper, we introduce the problem of meta
algorithm selection, which essentially asks for the best way to combine a given
set of algorithm selectors. We present a general methodological framework for
meta algorithm selection as well as several concrete learning methods as
instantiations of this framework, essentially combining ideas of meta learning
and ensemble learning. In an extensive experimental evaluation, we demonstrate
that ensembles of algorithm selectors can significantly outperform single
algorithm selectors and have the potential to form the new state of the art in
algorithm selection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tornede_A/0/1/0/all/0/1"&gt;Alexander Tornede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehring_L/0/1/0/all/0/1"&gt;Lukas Gehring&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tornede_T/0/1/0/all/0/1"&gt;Tanja Tornede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1"&gt;Marcel Wever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Real-time Speaker Diarization System Based on Spatial Spectrum. (arXiv:2107.09321v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.09321</id>
        <link href="http://arxiv.org/abs/2107.09321"/>
        <updated>2021-07-21T02:01:36.264Z</updated>
        <summary type="html"><![CDATA[In this paper we describe a speaker diarization system that enables
localization and identification of all speakers present in a conversation or
meeting. We propose a novel systematic approach to tackle several long-standing
challenges in speaker diarization tasks: (1) to segment and separate
overlapping speech from two speakers; (2) to estimate the number of speakers
when participants may enter or leave the conversation at any time; (3) to
provide accurate speaker identification on short text-independent utterances;
(4) to track down speakers movement during the conversation; (5) to detect
speaker change incidence real-time. First, a differential directional
microphone array-based approach is exploited to capture the target speakers'
voice in far-field adverse environment. Second, an online speaker-location
joint clustering approach is proposed to keep track of speaker location. Third,
an instant speaker number detector is developed to trigger the mechanism that
separates overlapped speech. The results suggest that our system effectively
incorporates spatial information and achieves significant gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Siqi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Weilong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xianliang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suo_H/0/1/0/all/0/1"&gt;Hongbin Suo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jinwei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhijie Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Token-Level Supervised Contrastive Learning for Punctuation Restoration. (arXiv:2107.09099v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09099</id>
        <link href="http://arxiv.org/abs/2107.09099"/>
        <updated>2021-07-21T02:01:36.257Z</updated>
        <summary type="html"><![CDATA[Punctuation is critical in understanding natural language text. Currently,
most automatic speech recognition (ASR) systems do not generate punctuation,
which affects the performance of downstream tasks, such as intent detection and
slot filling. This gives rise to the need for punctuation restoration. Recent
work in punctuation restoration heavily utilizes pre-trained language models
without considering data imbalance when predicting punctuation classes. In this
work, we address this problem by proposing a token-level supervised contrastive
learning method that aims at maximizing the distance of representation of
different punctuation marks in the embedding space. The result shows that
training with token-level supervised contrastive learning obtains up to 3.2%
absolute F1 improvement on the test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qiushi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1"&gt;Tom Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;H Lilian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xubo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bo Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InsPose: Instance-Aware Networks for Single-Stage Multi-Person Pose Estimation. (arXiv:2107.08982v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08982</id>
        <link href="http://arxiv.org/abs/2107.08982"/>
        <updated>2021-07-21T02:01:36.213Z</updated>
        <summary type="html"><![CDATA[Multi-person pose estimation is an attractive and challenging task. Existing
methods are mostly based on two-stage frameworks, which include top-down and
bottom-up methods. Two-stage methods either suffer from high computational
redundancy for additional person detectors or they need to group keypoints
heuristically after predicting all the instance-agnostic keypoints. The
single-stage paradigm aims to simplify the multi-person pose estimation
pipeline and receives a lot of attention. However, recent single-stage methods
have the limitation of low performance due to the difficulty of regressing
various full-body poses from a single feature vector. Different from previous
solutions that involve complex heuristic designs, we present a simple yet
effective solution by employing instance-aware dynamic networks. Specifically,
we propose an instance-aware module to adaptively adjust (part of) the network
parameters for each instance. Our solution can significantly increase the
capacity and adaptive-ability of the network for recognizing various poses,
while maintaining a compact end-to-end trainable pipeline. Extensive
experiments on the MS-COCO dataset demonstrate that our method achieves
significant improvement over existing single-stage methods, and makes a better
balance of accuracy and efficiency compared to the state-of-the-art two-stage
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Dahu Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xing Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xiaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1"&gt;Wenming Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Ye Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1"&gt;Shiliang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive wavelet distillation from neural networks through interpretations. (arXiv:2107.09145v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.09145</id>
        <link href="http://arxiv.org/abs/2107.09145"/>
        <updated>2021-07-21T02:01:36.204Z</updated>
        <summary type="html"><![CDATA[Recent deep-learning models have achieved impressive prediction performance,
but often sacrifice interpretability and computational efficiency.
Interpretability is crucial in many disciplines, such as science and medicine,
where models must be carefully vetted or where interpretation is the goal
itself. Moreover, interpretable models are concise and often yield
computational efficiency. Here, we propose adaptive wavelet distillation (AWD),
a method which aims to distill information from a trained neural network into a
wavelet transform. Specifically, AWD penalizes feature attributions of a neural
network in the wavelet domain to learn an effective multi-resolution wavelet
transform. The resulting model is highly predictive, concise, computationally
efficient, and has properties (such as a multi-scale structure) which make it
easy to interpret. In close collaboration with domain experts, we showcase how
AWD addresses challenges in two real-world settings: cosmological parameter
inference and molecular-partner prediction. In both cases, AWD yields a
scientifically interpretable and concise model which gives predictive
performance better than state-of-the-art neural networks. Moreover, AWD
identifies predictive features that are scientifically meaningful in the
context of respective domains. All code and models are released in a
full-fledged package available on Github
(https://github.com/Yu-Group/adaptive-wavelets).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ha_W/0/1/0/all/0/1"&gt;Wooseok Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Singh_C/0/1/0/all/0/1"&gt;Chandan Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lanusse_F/0/1/0/all/0/1"&gt;Francois Lanusse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Song_E/0/1/0/all/0/1"&gt;Eli Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dang_S/0/1/0/all/0/1"&gt;Song Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+He_K/0/1/0/all/0/1"&gt;Kangmin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Upadhyayula_S/0/1/0/all/0/1"&gt;Srigokul Upadhyayula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bin Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Clustering-Based Technique for the Acceleration of Deep Convolutional Networks. (arXiv:2107.09095v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09095</id>
        <link href="http://arxiv.org/abs/2107.09095"/>
        <updated>2021-07-21T02:01:36.197Z</updated>
        <summary type="html"><![CDATA[Deep learning and especially the use of Deep Neural Networks (DNNs) provides
impressive results in various regression and classification tasks. However, to
achieve these results, there is a high demand for computing and storing
resources. This becomes problematic when, for instance, real-time, mobile
applications are considered, in which the involved (embedded) devices have
limited resources. A common way of addressing this problem is to transform the
original large pre-trained networks into new smaller models, by utilizing Model
Compression and Acceleration (MCA) techniques. Within the MCA framework, we
propose a clustering-based approach that is able to increase the number of
employed centroids/representatives, while at the same time, have an
acceleration gain compared to conventional, $k$-means based approaches. This is
achieved by imposing a special structure to the employed representatives, which
is enabled by the particularities of the problem at hand. Moreover, the
theoretical acceleration gains are presented and the key system
hyper-parameters that affect that gain, are identified. Extensive evaluation
studies carried out using various state-of-the-art DNN models trained in image
classification, validate the superiority of the proposed method as compared for
its use in MCA tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pikoulis_E/0/1/0/all/0/1"&gt;Erion-Vasilis Pikoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mavrokefalidis_C/0/1/0/all/0/1"&gt;Christos Mavrokefalidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1"&gt;Aris S. Lalos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Driver Takeover Time in Conditionally Automated Driving. (arXiv:2107.09545v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09545</id>
        <link href="http://arxiv.org/abs/2107.09545"/>
        <updated>2021-07-21T02:01:36.176Z</updated>
        <summary type="html"><![CDATA[It is extremely important to ensure a safe takeover transition in
conditionally automated driving. One of the critical factors that quantifies
the safe takeover transition is takeover time. Previous studies identified the
effects of many factors on takeover time, such as takeover lead time,
non-driving tasks, modalities of the takeover requests (TORs), and scenario
urgency. However, there is a lack of research to predict takeover time by
considering these factors all at the same time. Toward this end, we used
eXtreme Gradient Boosting (XGBoost) to predict the takeover time using a
dataset from a meta-analysis study [1]. In addition, we used SHAP (SHapley
Additive exPlanation) to analyze and explain the effects of the predictors on
takeover time. We identified seven most critical predictors that resulted in
the best prediction performance. Their main effects and interaction effects on
takeover time were examined. The results showed that the proposed approach
provided both good performance and explainability. Our findings have
implications on the design of in-vehicle monitoring and alert systems to
facilitate the interaction between the drivers and the automated vehicle.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ayoub_J/0/1/0/all/0/1"&gt;Jackie Ayoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1"&gt;Na Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;X. Jessie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"&gt;Feng Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unity Perception: Generate Synthetic Data for Computer Vision. (arXiv:2107.04259v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04259</id>
        <link href="http://arxiv.org/abs/2107.04259"/>
        <updated>2021-07-21T02:01:36.164Z</updated>
        <summary type="html"><![CDATA[We introduce the Unity Perception package which aims to simplify and
accelerate the process of generating synthetic datasets for computer vision
tasks by offering an easy-to-use and highly customizable toolset. This
open-source package extends the Unity Editor and engine components to generate
perfectly annotated examples for several common computer vision tasks.
Additionally, it offers an extensible Randomization framework that lets the
user quickly construct and configure randomized simulation parameters in order
to introduce variation into the generated datasets. We provide an overview of
the provided tools and how they work, and demonstrate the value of the
generated synthetic datasets by training a 2D object detection model. The model
trained with mostly synthetic data outperforms the model trained using only
real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borkman_S/0/1/0/all/0/1"&gt;Steve Borkman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crespi_A/0/1/0/all/0/1"&gt;Adam Crespi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhakad_S/0/1/0/all/0/1"&gt;Saurav Dhakad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguly_S/0/1/0/all/0/1"&gt;Sujoy Ganguly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hogins_J/0/1/0/all/0/1"&gt;Jonathan Hogins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jhang_Y/0/1/0/all/0/1"&gt;You-Cyuan Jhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamalzadeh_M/0/1/0/all/0/1"&gt;Mohsen Kamalzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_S/0/1/0/all/0/1"&gt;Steven Leal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parisi_P/0/1/0/all/0/1"&gt;Pete Parisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romero_C/0/1/0/all/0/1"&gt;Cesar Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1"&gt;Wesley Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thaman_A/0/1/0/all/0/1"&gt;Alex Thaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warren_S/0/1/0/all/0/1"&gt;Samuel Warren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1"&gt;Nupur Yadav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison of Supervised and Unsupervised Deep Learning Methods for Anomaly Detection in Images. (arXiv:2107.09204v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09204</id>
        <link href="http://arxiv.org/abs/2107.09204"/>
        <updated>2021-07-21T02:01:36.145Z</updated>
        <summary type="html"><![CDATA[Anomaly detection in images plays a significant role for many applications
across all industries, such as disease diagnosis in healthcare or quality
assurance in manufacturing. Manual inspection of images, when extended over a
monotonously repetitive period of time is very time consuming and can lead to
anomalies being overlooked.Artificial neural networks have proven themselves
very successful on simple, repetitive tasks, in some cases even outperforming
humans. Therefore, in this paper we investigate different methods of deep
learning, including supervised and unsupervised learning, for anomaly detection
applied to a quality assurance use case. We utilize the MVTec anomaly dataset
and develop three different models, a CNN for supervised anomaly detection,
KD-CAE for autoencoder anomaly detection, NI-CAE for noise induced anomaly
detection and a DCGAN for generating reconstructed images. By experiments, we
found that KD-CAE performs better on the anomaly datasets compared to CNN and
NI-CAE, with NI-CAE performing the best on the Transistor dataset. We also
implemented a DCGAN for the creation of new training data but due to
computational limitation and lack of extrapolating the mechanics of AnoGAN, we
restricted ourselves just to the generation of GAN based images. We conclude
that unsupervised methods are more powerful for anomaly detection in images,
especially in a setting where only a small amount of anomalous data is
available, or the data is unlabeled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilmet_V/0/1/0/all/0/1"&gt;Vincent Wilmet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1"&gt;Sauraj Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Redl_T/0/1/0/all/0/1"&gt;Tabea Redl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandaker_H/0/1/0/all/0/1"&gt;H&amp;#xe5;kon Sandaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenning Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04570</id>
        <link href="http://arxiv.org/abs/2107.04570"/>
        <updated>2021-07-21T02:01:36.138Z</updated>
        <summary type="html"><![CDATA[Randomized smoothing has recently emerged as an effective tool that enables
certification of deep neural network classifiers at scale. All prior art on
randomized smoothing has focused on isotropic $\ell_p$ certification, which has
the advantage of yielding certificates that can be easily compared among
isotropic methods via $\ell_p$-norm radius. However, isotropic certification
limits the region that can be certified around an input to worst-case
adversaries, i.e., it cannot reason about other "close", potentially large,
constant prediction safe regions. To alleviate this issue, (i) we theoretically
extend the isotropic randomized smoothing $\ell_1$ and $\ell_2$ certificates to
their generalized anisotropic counterparts following a simplified analysis.
Moreover, (ii) we propose evaluation metrics allowing for the comparison of
general certificates - a certificate is superior to another if it certifies a
superset region - with the quantification of each certificate through the
volume of the certified region. We introduce ANCER, a practical framework for
obtaining anisotropic certificates for a given test set sample via volume
maximization. Our empirical results demonstrate that ANCER achieves
state-of-the-art $\ell_1$ and $\ell_2$ certified accuracy on both CIFAR-10 and
ImageNet at multiple radii, while certifying substantially larger regions in
terms of volume, thus highlighting the benefits of moving away from isotropic
analysis. Code used in our experiments is available in
https://github.com/MotasemAlfarra/ANCER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1"&gt;Francisco Eiras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1"&gt;Motasem Alfarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M. Pawan Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H. S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1"&gt;Puneet K. Dokania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1"&gt;Adel Bibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EEG-based Cross-Subject Driver Drowsiness Recognition with Interpretable CNN. (arXiv:2107.09507v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.09507</id>
        <link href="http://arxiv.org/abs/2107.09507"/>
        <updated>2021-07-21T02:01:36.039Z</updated>
        <summary type="html"><![CDATA[In the context of electroencephalogram (EEG)-based driver drowsiness
recognition, it is still a challenging task to design a calibration-free
system, since there exists a significant variability of EEG signals among
different subjects and recording sessions. As deep learning has received much
research attention in recent years, many efforts have been made to use deep
learning methods for EEG signal recognition. However, existing works mostly
treat deep learning models as blackbox classifiers, while what have been
learned by the models and to which extent they are affected by the noise from
EEG data are still underexplored. In this paper, we develop a novel
convolutional neural network that can explain its decision by highlighting the
local areas of the input sample that contain important information for the
classification. The network has a compact structure for ease of interpretation
and takes advantage of separable convolutions to process the EEG signals in a
spatial-temporal sequence. Results show that the model achieves an average
accuracy of 78.35% on 11 subjects for leave-one-out cross-subject drowsiness
recognition, which is higher than the conventional baseline methods of
53.4%-72.68% and state-of-art deep learning methods of 63.90%-65.61%.
Visualization results show that the model has learned to recognize biologically
explainable features from EEG signals, e.g., Alpha spindles, as strong
indicators of drowsiness across different subjects. In addition, we also
explore reasons behind some wrongly classified samples and how the model is
affected by artifacts and noise in the data. Our work illustrates a promising
direction on using interpretable deep learning models to discover meaning
patterns related to different mental states from complex EEG signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cui_J/0/1/0/all/0/1"&gt;Jian Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yisi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lan_Z/0/1/0/all/0/1"&gt;Zirui Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sourina_O/0/1/0/all/0/1"&gt;Olga Sourina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Muller_Wittig_W/0/1/0/all/0/1"&gt;Wolfgang M&amp;#xfc;ller-Wittig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relay-Assisted Cooperative Federated Learning. (arXiv:2107.09518v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.09518</id>
        <link href="http://arxiv.org/abs/2107.09518"/>
        <updated>2021-07-21T02:01:36.023Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has recently emerged as a promising technology to
enable artificial intelligence (AI) at the network edge, where distributed
mobile devices collaboratively train a shared AI model under the coordination
of an edge server. To significantly improve the communication efficiency of FL,
over-the-air computation allows a large number of mobile devices to
concurrently upload their local models by exploiting the superposition property
of wireless multi-access channels. Due to wireless channel fading, the model
aggregation error at the edge server is dominated by the weakest channel among
all devices, causing severe straggler issues. In this paper, we propose a
relay-assisted cooperative FL scheme to effectively address the straggler
issue. In particular, we deploy multiple half-duplex relays to cooperatively
assist the devices in uploading the local model updates to the edge server. The
nature of the over-the-air computation poses system objectives and constraints
that are distinct from those in traditional relay communication systems.
Moreover, the strong coupling between the design variables renders the
optimization of such a system challenging. To tackle the issue, we propose an
alternating-optimization-based algorithm to optimize the transceiver and relay
operation with low complexity. Then, we analyze the model aggregation error in
a single-relay case and show that our relay-assisted scheme achieves a smaller
error than the one without relays provided that the relay transmit power and
the relay channel gains are sufficiently large. The analysis provides critical
insights on relay deployment in the implementation of cooperative FL. Extensive
numerical results show that our design achieves faster convergence compared
with state-of-the-art schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zehong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ying-Jun Angela Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wearable Health Monitoring System for Older Adults in a Smart Home Environment. (arXiv:2107.09509v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.09509</id>
        <link href="http://arxiv.org/abs/2107.09509"/>
        <updated>2021-07-21T02:01:36.014Z</updated>
        <summary type="html"><![CDATA[The advent of IoT has enabled the design of connected and integrated smart
health monitoring systems. These smart health monitoring systems could be
realized in a smart home context to render long-term care to the elderly
population. In this paper, we present the design of a wearable health
monitoring system suitable for older adults in a smart home context. The
proposed system offers solutions to monitor the stress, blood pressure, and
location of an individual within a smart home environment. The stress detection
model proposed in this work uses Electrodermal Activity (EDA),
Photoplethysmogram (PPG), and Skin Temperature (ST) sensors embedded in a smart
wristband for detecting physiological stress. The stress detection model is
trained and tested using stress labels obtained from salivary cortisol which is
a clinically established biomarker for physiological stress. A voice-based
prototype is also implemented and the feasibility of the proposed system for
integration in a smart home environment is analyzed by simulating a data
acquisition and streaming scenario. We have also proposed a blood pressure
estimation model using PPG signal and advanced regression techniques for
integration with the stress detection model in the wearable health monitoring
system. Finally, the design of a voice-assisted indoor location system is
proposed for integration with the proposed system within a smart home
environment. The proposed wearable health monitoring system is an important
direction to realize a smart home environment with extensive diagnostic
capabilities so that such a system could be useful for rendering long-term and
personalized care to the aging population in the comfort of their home.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nath_R/0/1/0/all/0/1"&gt;Rajdeep Kumar Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thapliyal_H/0/1/0/all/0/1"&gt;Himanshu Thapliyal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Object Detectors from Few Weakly-Labeled and Many Unlabeled Images. (arXiv:1912.00384v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.00384</id>
        <link href="http://arxiv.org/abs/1912.00384"/>
        <updated>2021-07-21T02:01:35.994Z</updated>
        <summary type="html"><![CDATA[Weakly-supervised object detection attempts to limit the amount of
supervision by dispensing the need for bounding boxes, but still assumes
image-level labels on the entire training set. In this work, we study the
problem of training an object detector from one or few images with image-level
labels and a larger set of completely unlabeled images. This is an extreme case
of semi-supervised learning where the labeled data are not enough to bootstrap
the learning of a detector. Our solution is to train a weakly-supervised
student detector model from image-level pseudo-labels generated on the
unlabeled set by a teacher classifier model, bootstrapped by region-level
similarities to labeled images. Building upon the recent representative
weakly-supervised pipeline PCL, our method can use more unlabeled images to
achieve performance competitive or superior to many recent weakly-supervised
detection solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhaohui Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1"&gt;Miaojing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1"&gt;Vittorio Ferrari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1"&gt;Yannis Avrithis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Different kinds of cognitive plausibility: why are transformers better than RNNs at predicting N400 amplitude?. (arXiv:2107.09648v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09648</id>
        <link href="http://arxiv.org/abs/2107.09648"/>
        <updated>2021-07-21T02:01:35.987Z</updated>
        <summary type="html"><![CDATA[Despite being designed for performance rather than cognitive plausibility,
transformer language models have been found to be better at predicting metrics
used to assess human language comprehension than language models with other
architectures, such as recurrent neural networks. Based on how well they
predict the N400, a neural signal associated with processing difficulty, we
propose and provide evidence for one possible explanation - their predictions
are affected by the preceding context in a way analogous to the effect of
semantic facilitation in humans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1"&gt;James A. Michaelov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bardolph_M/0/1/0/all/0/1"&gt;Megan D. Bardolph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coulson_S/0/1/0/all/0/1"&gt;Seana Coulson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1"&gt;Benjamin K. Bergen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn2Hop: Learned Optimization on Rough Landscapes. (arXiv:2107.09661v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09661</id>
        <link href="http://arxiv.org/abs/2107.09661"/>
        <updated>2021-07-21T02:01:35.981Z</updated>
        <summary type="html"><![CDATA[Optimization of non-convex loss surfaces containing many local minima remains
a critical problem in a variety of domains, including operations research,
informatics, and material design. Yet, current techniques either require
extremely high iteration counts or a large number of random restarts for good
performance. In this work, we propose adapting recent developments in
meta-learning to these many-minima problems by learning the optimization
algorithm for various loss landscapes. We focus on problems from atomic
structural optimization--finding low energy configurations of many-atom
systems--including widely studied models such as bimetallic clusters and
disordered silicon. We find that our optimizer learns a 'hopping' behavior
which enables efficient exploration and improves the rate of low energy minima
discovery. Finally, our learned optimizers show promising generalization with
efficiency gains on never before seen tasks (e.g. new elements or
compositions). Code will be made available shortly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Merchant_A/0/1/0/all/0/1"&gt;Amil Merchant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metz_L/0/1/0/all/0/1"&gt;Luke Metz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schoenholz_S/0/1/0/all/0/1"&gt;Sam Schoenholz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1"&gt;Ekin Dogus Cubuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReSSL: Relational Self-Supervised Learning with Weak Augmentation. (arXiv:2107.09282v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09282</id>
        <link href="http://arxiv.org/abs/2107.09282"/>
        <updated>2021-07-21T02:01:35.973Z</updated>
        <summary type="html"><![CDATA[Self-supervised Learning (SSL) including the mainstream contrastive learning
has achieved great success in learning visual representations without data
annotations. However, most of methods mainly focus on the instance level
information (\ie, the different augmented images of the same instance should
have the same feature or cluster into the same class), but there is a lack of
attention on the relationships between different instances. In this paper, we
introduced a novel SSL paradigm, which we term as relational self-supervised
learning (ReSSL) framework that learns representations by modeling the
relationship between different instances. Specifically, our proposed method
employs sharpened distribution of pairwise similarities among different
instances as \textit{relation} metric, which is thus utilized to match the
feature embeddings of different augmentations. Moreover, to boost the
performance, we argue that weak augmentations matter to represent a more
reliable relation, and leverage momentum strategy for practical efficiency.
Experimental results show that our proposed ReSSL significantly outperforms the
previous state-of-the-art algorithms in terms of both performance and training
efficiency. Code is available at \url{https://github.com/KyleZheng1997/ReSSL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge of chaos as a guiding principle for modern neural network training. (arXiv:2107.09437v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09437</id>
        <link href="http://arxiv.org/abs/2107.09437"/>
        <updated>2021-07-21T02:01:35.966Z</updated>
        <summary type="html"><![CDATA[The success of deep neural networks in real-world problems has prompted many
attempts to explain their training dynamics and generalization performance, but
more guiding principles for the training of neural networks are still needed.
Motivated by the edge of chaos principle behind the optimal performance of
neural networks, we study the role of various hyperparameters in modern neural
network training algorithms in terms of the order-chaos phase diagram. In
particular, we study a fully analytical feedforward neural network trained on
the widely adopted Fashion-MNIST dataset, and study the dynamics associated
with the hyperparameters in back-propagation during the training process. We
find that for the basic algorithm of stochastic gradient descent with momentum,
in the range around the commonly used hyperparameter values, clear scaling
relations are present with respect to the training time during the ordered
phase in the phase diagram, and the model's optimal generalization power at the
edge of chaos is similar across different training parameter combinations. In
the chaotic phase, the same scaling no longer exists. The scaling allows us to
choose the training parameters to achieve faster training without sacrificing
performance. In addition, we find that the commonly used model regularization
method - weight decay - effectively pushes the model towards the ordered phase
to achieve better performance. Leveraging on this fact and the scaling
relations in the other hyperparameters, we derived a principled guideline for
hyperparameter determination, such that the model can achieve optimal
performance by saturating it at the edge of chaos. Demonstrated on this simple
neural network model and training algorithm, our work improves the
understanding of neural network training dynamics, and can potentially be
extended to guiding principles of more complex model architectures and
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1"&gt;Ling Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1"&gt;Choy Heng Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Video Transformer: Can Objects be the Words?. (arXiv:2107.09240v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09240</id>
        <link href="http://arxiv.org/abs/2107.09240"/>
        <updated>2021-07-21T02:01:35.947Z</updated>
        <summary type="html"><![CDATA[Transformers have been successful for many natural language processing tasks.
However, applying transformers to the video domain for tasks such as long-term
video generation and scene understanding has remained elusive due to the high
computational complexity and the lack of natural tokenization. In this paper,
we propose the Object-Centric Video Transformer (OCVT) which utilizes an
object-centric approach for decomposing scenes into tokens suitable for use in
a generative video transformer. By factoring the video into objects, our fully
unsupervised model is able to learn complex spatio-temporal dynamics of
multiple interacting objects in a scene and generate future frames of the
video. Our model is also significantly more memory-efficient than pixel-based
models and thus able to train on videos of length up to 70 frames with a single
48GB GPU. We compare our model with previous RNN-based approaches as well as
other possible video transformer baselines. We demonstrate OCVT performs well
when compared to baselines in generating future frames. OCVT also develops
useful representations for video reasoning, achieving start-of-the-art
performance on the CATER task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi-Fu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jaesik Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sungjin Ahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Probabilistic Inference in Deep Learning: Beyond Marginal Predictions. (arXiv:2107.09224v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09224</id>
        <link href="http://arxiv.org/abs/2107.09224"/>
        <updated>2021-07-21T02:01:35.941Z</updated>
        <summary type="html"><![CDATA[A fundamental challenge for any intelligent system is prediction: given some
inputs $X_1,..,X_\tau$ can you predict outcomes $Y_1,.., Y_\tau$. The KL
divergence $\mathbf{d}_{\mathrm{KL}}$ provides a natural measure of prediction
quality, but the majority of deep learning research looks only at the marginal
predictions per input $X_t$. In this technical report we propose a scoring rule
$\mathbf{d}_{\mathrm{KL}}^\tau$, parameterized by $\tau \in \mathcal{N}$ that
evaluates the joint predictions at $\tau$ inputs simultaneously. We show that
the commonly-used $\tau=1$ can be insufficient to drive good decisions in many
settings of interest. We also show that, as $\tau$ grows, performing well
according to $\mathbf{d}_{\mathrm{KL}}^\tau$ recovers universal guarantees for
any possible decision. Finally, we provide problem-dependent guidance on the
scale of $\tau$ for which our score provides sufficient guarantees for good
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiuyuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1"&gt;Ian Osband&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1"&gt;Zheng Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Establishing process-structure linkages using Generative Adversarial Networks. (arXiv:2107.09402v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2107.09402</id>
        <link href="http://arxiv.org/abs/2107.09402"/>
        <updated>2021-07-21T02:01:35.934Z</updated>
        <summary type="html"><![CDATA[The microstructure of material strongly influences its mechanical properties
and the microstructure itself is influenced by the processing conditions. Thus,
establishing a Process-Structure-Property relationship is a crucial task in
material design and is of interest in many engineering applications. We develop
a GAN (Generative Adversarial Network) to synthesize microstructures based on
given processing conditions. This approach is devoid of feature engineering,
needs little domain awareness, and can be applied to a wide variety of material
systems. Results show that our GAN model can produce high-fidelity multi-phase
microstructures which have a good correlation with the given processing
conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Safiuddin_M/0/1/0/all/0/1"&gt;Mohammad Safiuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Reddy_C/0/1/0/all/0/1"&gt;CH Likith Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Vasantada_G/0/1/0/all/0/1"&gt;Ganesh Vasantada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Harsha_C/0/1/0/all/0/1"&gt;CHJNS Harsha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Gangolu_S/0/1/0/all/0/1"&gt;Srinu Gangolu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bayesian Approach to Invariant Deep Neural Networks. (arXiv:2107.09301v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.09301</id>
        <link href="http://arxiv.org/abs/2107.09301"/>
        <updated>2021-07-21T02:01:35.929Z</updated>
        <summary type="html"><![CDATA[We propose a novel Bayesian neural network architecture that can learn
invariances from data alone by inferring a posterior distribution over
different weight-sharing schemes. We show that our model outperforms other
non-invariant architectures, when trained on datasets that contain specific
invariances. The same holds true when no data augmentation is performed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mourdoukoutas_N/0/1/0/all/0/1"&gt;Nikolaos Mourdoukoutas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Federici_M/0/1/0/all/0/1"&gt;Marco Federici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pantalos_G/0/1/0/all/0/1"&gt;Georges Pantalos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wilk_M/0/1/0/all/0/1"&gt;Mark van der Wilk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SVSNet: An End-to-end Speaker Voice Similarity Assessment Model. (arXiv:2107.09392v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.09392</id>
        <link href="http://arxiv.org/abs/2107.09392"/>
        <updated>2021-07-21T02:01:35.922Z</updated>
        <summary type="html"><![CDATA[Neural evaluation metrics derived for numerous speech generation tasks have
recently attracted great attention. In this paper, we propose SVSNet, the first
end-to-end neural network model to assess the speaker voice similarity between
natural speech and synthesized speech. Unlike most neural evaluation metrics
that use hand-crafted features, SVSNet directly takes the raw waveform as input
to more completely utilize speech information for prediction. SVSNet consists
of encoder, co-attention, distance calculation, and prediction modules and is
trained in an end-to-end manner. The experimental results on the Voice
Conversion Challenge 2018 and 2020 (VCC2018 and VCC2020) datasets show that
SVSNet notably outperforms well-known baseline systems in the assessment of
speaker similarity at the utterance and system levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hu_C/0/1/0/all/0/1"&gt;Cheng-Hung Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yu-Huai Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1"&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1"&gt;Yu Tsao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hsin-Min Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CANITA: Faster Rates for Distributed Convex Optimization with Communication Compression. (arXiv:2107.09461v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09461</id>
        <link href="http://arxiv.org/abs/2107.09461"/>
        <updated>2021-07-21T02:01:35.903Z</updated>
        <summary type="html"><![CDATA[Due to the high communication cost in distributed and federated learning,
methods relying on compressed communication are becoming increasingly popular.
Besides, the best theoretically and practically performing gradient-type
methods invariably rely on some form of acceleration/momentum to reduce the
number of communications (faster convergence), e.g., Nesterov's accelerated
gradient descent (Nesterov, 2004) and Adam (Kingma and Ba, 2014). In order to
combine the benefits of communication compression and convergence acceleration,
we propose a \emph{compressed and accelerated} gradient method for distributed
optimization, which we call CANITA. Our CANITA achieves the \emph{first
accelerated rate}
$O\bigg(\sqrt{\Big(1+\sqrt{\frac{\omega^3}{n}}\Big)\frac{L}{\epsilon}} +
\omega\big(\frac{1}{\epsilon}\big)^{\frac{1}{3}}\bigg)$, which improves upon
the state-of-the-art non-accelerated rate
$O\left((1+\frac{\omega}{n})\frac{L}{\epsilon} +
\frac{\omega^2+n}{\omega+n}\frac{1}{\epsilon}\right)$ of DIANA (Khaled et al.,
2020b) for distributed general convex problems, where $\epsilon$ is the target
error, $L$ is the smooth parameter of the objective, $n$ is the number of
machines/devices, and $\omega$ is the compression parameter (larger $\omega$
means more compression can be applied, and no compression implies $\omega=0$).
Our results show that as long as the number of devices $n$ is large (often true
in distributed/federated learning), or the compression $\omega$ is not very
high, CANITA achieves the faster convergence rate
$O\Big(\sqrt{\frac{L}{\epsilon}}\Big)$, i.e., the number of communication
rounds is $O\Big(\sqrt{\frac{L}{\epsilon}}\Big)$ (vs.
$O\big(\frac{L}{\epsilon}\big)$ achieved by previous works). As a result,
CANITA enjoys the advantages of both compression (compressed communication in
each round) and acceleration (much fewer communication rounds).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhize Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CREW: Computation Reuse and Efficient Weight Storage for Hardware-accelerated MLPs and RNNs. (arXiv:2107.09408v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2107.09408</id>
        <link href="http://arxiv.org/abs/2107.09408"/>
        <updated>2021-07-21T02:01:35.897Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have achieved tremendous success for cognitive
applications. The core operation in a DNN is the dot product between quantized
inputs and weights. Prior works exploit the weight/input repetition that arises
due to quantization to avoid redundant computations in Convolutional Neural
Networks (CNNs). However, in this paper we show that their effectiveness is
severely limited when applied to Fully-Connected (FC) layers, which are
commonly used in state-of-the-art DNNs, as it is the case of modern Recurrent
Neural Networks (RNNs) and Transformer models.

To improve energy-efficiency of FC computation we present CREW, a hardware
accelerator that implements Computation Reuse and an Efficient Weight Storage
mechanism to exploit the large number of repeated weights in FC layers. CREW
first performs the multiplications of the unique weights by their respective
inputs and stores the results in an on-chip buffer. The storage requirements
are modest due to the small number of unique weights and the relatively small
size of the input compared to convolutional layers. Next, CREW computes each
output by fetching and adding its required products. To this end, each weight
is replaced offline by an index in the buffer of unique products. Indices are
typically smaller than the quantized weights, since the number of unique
weights for each input tends to be much lower than the range of quantized
weights, which reduces storage and memory bandwidth requirements.

Overall, CREW greatly reduces the number of multiplications and provides
significant savings in model memory footprint and memory bandwidth usage. We
evaluate CREW on a diverse set of modern DNNs. On average, CREW provides 2.61x
speedup and 2.42x energy savings over a TPU-like accelerator. Compared to UCNN,
a state-of-art computation reuse technique, CREW achieves 2.10x speedup and
2.08x energy savings on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riera_M/0/1/0/all/0/1"&gt;Marc Riera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnau_J/0/1/0/all/0/1"&gt;Jose-Maria Arnau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_A/0/1/0/all/0/1"&gt;Antonio Gonzalez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An induction proof of the backpropagation algorithm in matrix notation. (arXiv:2107.09384v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.09384</id>
        <link href="http://arxiv.org/abs/2107.09384"/>
        <updated>2021-07-21T02:01:35.890Z</updated>
        <summary type="html"><![CDATA[Backpropagation (BP) is a core component of the contemporary deep learning
incarnation of neural networks. Briefly, BP is an algorithm that exploits the
computational architecture of neural networks to efficiently evaluate the
gradient of a cost function during neural network parameter optimization. The
validity of BP rests on the application of a multivariate chain rule to the
computational architecture of neural networks and their associated objective
functions. Introductions to deep learning theory commonly present the
computational architecture of neural networks in matrix form, but eschew a
parallel formulation and justification of BP in the framework of matrix
differential calculus. This entails several drawbacks for the theory and
didactics of deep learning. In this work, we overcome these limitations by
providing a full induction proof of the BP algorithm in matrix notation.
Specifically, we situate the BP algorithm in the framework of matrix
differential calculus, encompass affine-linear potential functions, prove the
validity of the BP algorithm in inductive form, and exemplify the
implementation of the matrix form BP algorithm in computer code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ostwald_D/0/1/0/all/0/1"&gt;Dirk Ostwald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Usee_F/0/1/0/all/0/1"&gt;Franziska Us&amp;#xe9;e&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Significant Wave Height Prediction based on Wavelet Graph Neural Network. (arXiv:2107.09483v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09483</id>
        <link href="http://arxiv.org/abs/2107.09483"/>
        <updated>2021-07-21T02:01:35.883Z</updated>
        <summary type="html"><![CDATA[Computational intelligence-based ocean characteristics forecasting
applications, such as Significant Wave Height (SWH) prediction, are crucial for
avoiding social and economic loss in coastal cities. Compared to the
traditional empirical-based or numerical-based forecasting models, "soft
computing" approaches, including machine learning and deep learning models,
have shown numerous success in recent years. In this paper, we focus on
enabling the deep learning model to learn both short-term and long-term
spatial-temporal dependencies for SWH prediction. A Wavelet Graph Neural
Network (WGNN) approach is proposed to integrate the advantages of wavelet
transform and graph neural network. Several parallel graph neural networks are
separately trained on wavelet decomposed data, and the reconstruction of each
model's prediction forms the final SWH prediction. Experimental results show
that the proposed WGNN approach outperforms other models, including the
numerical models, the machine learning models, and several deep learning
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Delong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaomin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zewen Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wave-Informed Matrix Factorization withGlobal Optimality Guarantees. (arXiv:2107.09144v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09144</id>
        <link href="http://arxiv.org/abs/2107.09144"/>
        <updated>2021-07-21T02:01:35.877Z</updated>
        <summary type="html"><![CDATA[With the recent success of representation learning methods, which includes
deep learning as a special case, there has been considerable interest in
developing representation learning techniques that can incorporate known
physical constraints into the learned representation. As one example, in many
applications that involve a signal propagating through physical media (e.g.,
optics, acoustics, fluid dynamics, etc), it is known that the dynamics of the
signal must satisfy constraints imposed by the wave equation. Here we propose a
matrix factorization technique that decomposes such signals into a sum of
components, where each component is regularized to ensure that it satisfies
wave equation constraints. Although our proposed formulation is non-convex, we
prove that our model can be efficiently solved to global optimality in
polynomial time. We demonstrate the benefits of our work by applications in
structural health monitoring, where prior work has attempted to solve this
problem using sparse dictionary learning approaches that do not come with any
theoretical guarantees regarding convergence to global optimality and employ
heuristics to capture desired physical constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tetali_H/0/1/0/all/0/1"&gt;Harsha Vardhan Tetali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harley_J/0/1/0/all/0/1"&gt;Joel B. Harley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1"&gt;Benjamin D. Haeffele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Follow Your Path: a Progressive Method for Knowledge Distillation. (arXiv:2107.09305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09305</id>
        <link href="http://arxiv.org/abs/2107.09305"/>
        <updated>2021-07-21T02:01:35.858Z</updated>
        <summary type="html"><![CDATA[Deep neural networks often have a huge number of parameters, which posts
challenges in deployment in application scenarios with limited memory and
computation capacity. Knowledge distillation is one approach to derive compact
models from bigger ones. However, it has been observed that a converged heavy
teacher model is strongly constrained for learning a compact student network
and could make the optimization subject to poor local optima. In this paper, we
propose ProKT, a new model-agnostic method by projecting the supervision
signals of a teacher model into the student's parameter space. Such projection
is implemented by decomposing the training objective into local intermediate
targets with an approximate mirror descent technique. The proposed method could
be less sensitive with the quirks during optimization which could result in a
better local optimum. Experiments on both image and text datasets show that our
proposed ProKT consistently achieves superior performance compared to other
existing knowledge distillation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Wenxian Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yuxuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bohan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A quantum algorithm for training wide and deep classical neural networks. (arXiv:2107.09200v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.09200</id>
        <link href="http://arxiv.org/abs/2107.09200"/>
        <updated>2021-07-21T02:01:35.851Z</updated>
        <summary type="html"><![CDATA[Given the success of deep learning in classical machine learning, quantum
algorithms for traditional neural network architectures may provide one of the
most promising settings for quantum machine learning. Considering a
fully-connected feedforward neural network, we show that conditions amenable to
classical trainability via gradient descent coincide with those necessary for
efficiently solving quantum linear systems. We propose a quantum algorithm to
approximately train a wide and deep neural network up to $O(1/n)$ error for a
training set of size $n$ by performing sparse matrix inversion in $O(\log n)$
time. To achieve an end-to-end exponential speedup over gradient descent, the
data distribution must permit efficient state preparation and readout. We
numerically demonstrate that the MNIST image dataset satisfies such conditions;
moreover, the quantum algorithm matches the accuracy of the fully-connected
network. Beyond the proven architecture, we provide empirical evidence for
$O(\log n)$ training of a convolutional neural network with pooling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zlokapa_A/0/1/0/all/0/1"&gt;Alexander Zlokapa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Neven_H/0/1/0/all/0/1"&gt;Hartmut Neven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lloyd_S/0/1/0/all/0/1"&gt;Seth Lloyd&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can we globally optimize cross-validation loss? Quasiconvexity in ridge regression. (arXiv:2107.09194v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.09194</id>
        <link href="http://arxiv.org/abs/2107.09194"/>
        <updated>2021-07-21T02:01:35.844Z</updated>
        <summary type="html"><![CDATA[Models like LASSO and ridge regression are extensively used in practice due
to their interpretability, ease of use, and strong theoretical guarantees.
Cross-validation (CV) is widely used for hyperparameter tuning in these models,
but do practical optimization methods minimize the true out-of-sample loss? A
recent line of research promises to show that the optimum of the CV loss
matches the optimum of the out-of-sample loss (possibly after simple
corrections). It remains to show how tractable it is to minimize the CV loss.
In the present paper, we show that, in the case of ridge regression, the CV
loss may fail to be quasiconvex and thus may have multiple local optima. We can
guarantee that the CV loss is quasiconvex in at least one case: when the
spectrum of the covariate matrix is nearly flat and the noise in the observed
responses is not too high. More generally, we show that quasiconvexity status
is independent of many properties of the observed data (response norm,
covariate-matrix right singular vectors and singular-value scaling) and has a
complex dependence on the few that remain. We empirically confirm our theory
using simulated experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Stephenson_W/0/1/0/all/0/1"&gt;William T. Stephenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Frangella_Z/0/1/0/all/0/1"&gt;Zachary Frangella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Udell_M/0/1/0/all/0/1"&gt;Madeleine Udell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Broderick_T/0/1/0/all/0/1"&gt;Tamara Broderick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning for Credit Card Fraud Detection: A Journey from Research to Production. (arXiv:2107.09323v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09323</id>
        <link href="http://arxiv.org/abs/2107.09323"/>
        <updated>2021-07-21T02:01:35.837Z</updated>
        <summary type="html"><![CDATA[The dark face of digital commerce generalization is the increase of fraud
attempts. To prevent any type of attacks, state of the art fraud detection
systems are now embedding Machine Learning (ML) modules. The conception of such
modules is only communicated at the level of research and papers mostly focus
on results for isolated benchmark datasets and metrics. But research is only a
part of the journey, preceded by the right formulation of the business problem
and collection of data, and followed by a practical integration. In this paper,
we give a wider vision of the process, on a case study of transfer learning for
fraud detection, from business to research, and back to business.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siblini_W/0/1/0/all/0/1"&gt;Wissam Siblini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coter_G/0/1/0/all/0/1"&gt;Guillaume Coter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fabry_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Fabry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Guelton_L/0/1/0/all/0/1"&gt;Liyun He-Guelton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oble_F/0/1/0/all/0/1"&gt;Fr&amp;#xe9;d&amp;#xe9;ric Obl&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lebichot_B/0/1/0/all/0/1"&gt;Bertrand Lebichot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgne_Y/0/1/0/all/0/1"&gt;Yann-A&amp;#xeb;l Le Borgne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bontempi_G/0/1/0/all/0/1"&gt;Gianluca Bontempi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reward-Weighted Regression Converges to a Global Optimum. (arXiv:2107.09088v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.09088</id>
        <link href="http://arxiv.org/abs/2107.09088"/>
        <updated>2021-07-21T02:01:35.829Z</updated>
        <summary type="html"><![CDATA[Reward-Weighted Regression (RWR) belongs to a family of widely known
iterative Reinforcement Learning algorithms based on the
Expectation-Maximization framework. In this family, learning at each iteration
consists of sampling a batch of trajectories using the current policy and
fitting a new policy to maximize a return-weighted log-likelihood of actions.
Although RWR is known to yield monotonic improvement of the policy under
certain circumstances, whether and under which conditions RWR converges to the
optimal policy have remained open questions. In this paper, we provide for the
first time a proof that RWR converges to a global optimum when no function
approximation is used.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Strupl_M/0/1/0/all/0/1"&gt;Miroslav &amp;#x160;trupl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Faccio_F/0/1/0/all/0/1"&gt;Francesco Faccio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ashley_D/0/1/0/all/0/1"&gt;Dylan R. Ashley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Srivastava_R/0/1/0/all/0/1"&gt;Rupesh Kumar Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidhuber_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Music Tempo Estimation via Neural Networks -- A Comparative Analysis. (arXiv:2107.09208v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.09208</id>
        <link href="http://arxiv.org/abs/2107.09208"/>
        <updated>2021-07-21T02:01:35.810Z</updated>
        <summary type="html"><![CDATA[This paper presents a comparative analysis on two artificial neural networks
(with different architectures) for the task of tempo estimation. For this
purpose, it also proposes the modeling, training and evaluation of a B-RNN
(Bidirectional Recurrent Neural Network) model capable of estimating tempo in
bpm (beats per minutes) of musical pieces, without using external auxiliary
modules. An extensive database (12,550 pieces in total) was curated to conduct
a quantitative and qualitative analysis over the experiment. Percussion-only
tracks were also included in the dataset. The performance of the B-RNN is
compared to that of state-of-the-art models. For further comparison, a
state-of-the-art CNN was also retrained with the same datasets used for the
B-RNN training. Evaluation results for each model and datasets are presented
and discussed, as well as observations and ideas for future research. Tempo
estimation was more accurate for the percussion only dataset, suggesting that
the estimation can be more accurate for percussion-only tracks, although
further experiments (with more of such datasets) should be made to gather
stronger evidence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Souza_M/0/1/0/all/0/1"&gt;Mila Soares de Oliveira de Souza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moura_P/0/1/0/all/0/1"&gt;Pedro Nuno de Souza Moura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Briot_J/0/1/0/all/0/1"&gt;Jean-Pierre Briot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Support Recovery in Universal One-bit Compressed Sensing. (arXiv:2107.09091v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.09091</id>
        <link href="http://arxiv.org/abs/2107.09091"/>
        <updated>2021-07-21T02:01:35.804Z</updated>
        <summary type="html"><![CDATA[One-bit compressed sensing (1bCS) is an extreme-quantized signal acquisition
method that has been widely studied in the past decade. In 1bCS, linear samples
of a high dimensional signal are quantized to only one bit per sample (sign of
the measurement). Assuming the original signal vector to be sparse, existing
results either aim to find the support of the vector, or approximate the signal
within an $\epsilon$-ball. The focus of this paper is support recovery, which
often also computationally facilitates approximate signal recovery. A universal
measurement matrix for 1bCS refers to one set of measurements that work for all
sparse signals. With universality, it is known that $\tilde{\Theta}(k^2)$ 1bCS
measurements are necessary and sufficient for support recovery (where $k$
denotes the sparsity). In this work, we show that it is possible to universally
recover the support with a small number of false positives with
$\tilde{O}(k^{3/2})$ measurements. If the dynamic range of the signal vector is
known, then with a different technique, this result can be improved to only
$\tilde{O}(k)$ measurements. Further results on support recovery are also
provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1"&gt;Arya Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1"&gt;Soumyabrata Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Collaborative Reinforcement Learning Agents that Communicate Through Text-Based Natural Language. (arXiv:2107.09356v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09356</id>
        <link href="http://arxiv.org/abs/2107.09356"/>
        <updated>2021-07-21T02:01:35.797Z</updated>
        <summary type="html"><![CDATA[Communication between agents in collaborative multi-agent settings is in
general implicit or a direct data stream. This paper considers text-based
natural language as a novel form of communication between multiple agents
trained with reinforcement learning. This could be considered first steps
toward a truly autonomous communication without the need to define a limited
set of instructions, and natural collaboration between humans and robots.
Inspired by the game of Blind Leads, we propose an environment where one agent
uses natural language instructions to guide another through a maze. We test the
ability of reinforcement learning agents to effectively communicate through
discrete word-level symbols and show that the agents are able to sufficiently
communicate through natural language with a limited vocabulary. Although the
communication is not always perfect English, the agents are still able to
navigate the maze. We achieve a BLEU score of 0.85, which is an improvement of
0.61 over randomly generated sequences while maintaining a 100% maze completion
rate. This is a 3.5 times the performance of the random baseline using our
reference set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eloff_K/0/1/0/all/0/1"&gt;Kevin Eloff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engelbrecht_H/0/1/0/all/0/1"&gt;Herman Engelbrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positive/Negative Approximate Multipliers for DNN Accelerators. (arXiv:2107.09366v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2107.09366</id>
        <link href="http://arxiv.org/abs/2107.09366"/>
        <updated>2021-07-21T02:01:35.789Z</updated>
        <summary type="html"><![CDATA[Recent Deep Neural Networks (DNNs) managed to deliver superhuman accuracy
levels on many AI tasks. Several applications rely more and more on DNNs to
deliver sophisticated services and DNN accelerators are becoming integral
components of modern systems-on-chips. DNNs perform millions of arithmetic
operations per inference and DNN accelerators integrate thousands of
multiply-accumulate units leading to increased energy requirements. Approximate
computing principles are employed to significantly lower the energy consumption
of DNN accelerators at the cost of some accuracy loss. Nevertheless, recent
research demonstrated that complex DNNs are increasingly sensitive to
approximation. Hence, the obtained energy savings are often limited when
targeting tight accuracy constraints. In this work, we present a dynamically
configurable approximate multiplier that supports three operation modes, i.e.,
exact, positive error, and negative error. In addition, we propose a
filter-oriented approximation method to map the weights to the appropriate
modes of the approximate multiplier. Our mapping algorithm balances the
positive with the negative errors due to the approximate multiplications,
aiming at maximizing the energy reduction while minimizing the overall
convolution error. We evaluate our approach on multiple DNNs and datasets
against state-of-the-art approaches, where our method achieves 18.33% energy
gains on average across 7 NNs on 4 different datasets for a maximum accuracy
drop of only 1%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Spantidi_O/0/1/0/all/0/1"&gt;Ourania Spantidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zervakis_G/0/1/0/all/0/1"&gt;Georgios Zervakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anagnostopoulos_I/0/1/0/all/0/1"&gt;Iraklis Anagnostopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amrouch_H/0/1/0/all/0/1"&gt;Hussam Amrouch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henkel_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg Henkel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos. (arXiv:2107.09262v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09262</id>
        <link href="http://arxiv.org/abs/2107.09262"/>
        <updated>2021-07-21T02:01:35.782Z</updated>
        <summary type="html"><![CDATA[Deep learning based visual to sound generation systems essentially need to be
developed particularly considering the synchronicity aspects of visual and
audio features with time. In this research we introduce a novel task of guiding
a class conditioned generative adversarial network with the temporal visual
information of a video input for visual to sound generation task adapting the
synchronicity traits between audio-visual modalities. Our proposed FoleyGAN
model is capable of conditioning action sequences of visual events leading
towards generating visually aligned realistic sound tracks. We expand our
previously proposed Automatic Foley dataset to train with FoleyGAN and evaluate
our synthesized sound through human survey that shows noteworthy (on average
81\%) audio-visual synchronicity performance. Our approach also outperforms in
statistical experiments compared with other baseline models and audio-visual
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1"&gt;Sanchita Ghose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prevost_J/0/1/0/all/0/1"&gt;John J. Prevost&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-scale graph representation learning with very deep GNNs and self-supervision. (arXiv:2107.09422v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09422</id>
        <link href="http://arxiv.org/abs/2107.09422"/>
        <updated>2021-07-21T02:01:35.764Z</updated>
        <summary type="html"><![CDATA[Effectively and efficiently deploying graph neural networks (GNNs) at scale
remains one of the most challenging aspects of graph representation learning.
Many powerful solutions have only ever been validated on comparatively small
datasets, often with counter-intuitive outcomes -- a barrier which has been
broken by the Open Graph Benchmark Large-Scale Challenge (OGB-LSC). We entered
the OGB-LSC with two large-scale GNNs: a deep transductive node classifier
powered by bootstrapping, and a very deep (up to 50-layer) inductive graph
regressor regularised by denoising objectives. Our models achieved an
award-level (top-3) performance on both the MAG240M and PCQM4M benchmarks. In
doing so, we demonstrate evidence of scalable self-supervised graph
representation learning, and utility of very deep GNNs -- both very important
open issues. Our code is publicly available at:
https://github.com/deepmind/deepmind-research/tree/master/ogb_lsc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Addanki_R/0/1/0/all/0/1"&gt;Ravichandra Addanki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1"&gt;Peter W. Battaglia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Budden_D/0/1/0/all/0/1"&gt;David Budden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deac_A/0/1/0/all/0/1"&gt;Andreea Deac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Godwin_J/0/1/0/all/0/1"&gt;Jonathan Godwin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keck_T/0/1/0/all/0/1"&gt;Thomas Keck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wai Lok Sibon Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1"&gt;Alvaro Sanchez-Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stott_J/0/1/0/all/0/1"&gt;Jacklynn Stott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakoor_S/0/1/0/all/0/1"&gt;Shantanu Thakoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1"&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating domain knowledge into neural-guided search. (arXiv:2107.09182v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09182</id>
        <link href="http://arxiv.org/abs/2107.09182"/>
        <updated>2021-07-21T02:01:35.757Z</updated>
        <summary type="html"><![CDATA[Many AutoML problems involve optimizing discrete objects under a black-box
reward. Neural-guided search provides a flexible means of searching these
combinatorial spaces using an autoregressive recurrent neural network. A major
benefit of this approach is that builds up objects sequentially--this provides
an opportunity to incorporate domain knowledge into the search by directly
modifying the logits emitted during sampling. In this work, we formalize a
framework for incorporating such in situ priors and constraints into
neural-guided search, and provide sufficient conditions for enforcing
constraints. We integrate several priors and constraints from existing works
into this framework, propose several new ones, and demonstrate their efficacy
in informing the task of symbolic regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_B/0/1/0/all/0/1"&gt;Brenden K. Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santiago_C/0/1/0/all/0/1"&gt;Claudio P. Santiago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larma_M/0/1/0/all/0/1"&gt;Mikel Landajuela Larma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Streaming End-to-End ASR based on Blockwise Non-Autoregressive Models. (arXiv:2107.09428v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.09428</id>
        <link href="http://arxiv.org/abs/2107.09428"/>
        <updated>2021-07-21T02:01:35.750Z</updated>
        <summary type="html"><![CDATA[Non-autoregressive (NAR) modeling has gained more and more attention in
speech processing. With recent state-of-the-art attention-based automatic
speech recognition (ASR) structure, NAR can realize promising real-time factor
(RTF) improvement with only small degradation of accuracy compared to the
autoregressive (AR) models. However, the recognition inference needs to wait
for the completion of a full speech utterance, which limits their applications
on low latency scenarios. To address this issue, we propose a novel end-to-end
streaming NAR speech recognition system by combining blockwise-attention and
connectionist temporal classification with mask-predict (Mask-CTC) NAR. During
inference, the input audio is separated into small blocks and then processed in
a blockwise streaming way. To address the insertion and deletion error at the
edge of the output of each block, we apply an overlapping decoding strategy
with a dynamic mapping trick that can produce more coherent sentences.
Experimental results show that the proposed method improves online ASR
recognition in low latency conditions compared to vanilla Mask-CTC. Moreover,
it can achieve a much faster inference speed compared to the AR attention-based
models. All of our codes will be publicly available at
https://github.com/espnet/espnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianzi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fujita_Y/0/1/0/all/0/1"&gt;Yuya Fujita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xuankai Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OPAL: Offline Preference-Based Apprenticeship Learning. (arXiv:2107.09251v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09251</id>
        <link href="http://arxiv.org/abs/2107.09251"/>
        <updated>2021-07-21T02:01:35.743Z</updated>
        <summary type="html"><![CDATA[We study how an offline dataset of prior (possibly random) experience can be
used to address two challenges that autonomous systems face when they endeavor
to learn from, adapt to, and collaborate with humans : (1) identifying the
human's intent and (2) safely optimizing the autonomous system's behavior to
achieve this inferred intent. First, we use the offline dataset to efficiently
infer the human's reward function via pool-based active preference learning.
Second, given this learned reward function, we perform offline reinforcement
learning to optimize a policy based on the inferred human intent. Crucially,
our proposed approach does not require actual physical rollouts or an accurate
simulator for either the reward learning or policy optimization steps, enabling
both safe and efficient apprenticeship learning. We identify and evaluate our
approach on a subset of existing offline RL benchmarks that are well suited for
offline reward learning and also evaluate extensions of these benchmarks which
allow more open-ended behaviors. Our experiments show that offline
preference-based reward learning followed by offline reinforcement learning
enables efficient and high-performing policies, while only requiring small
numbers of preference queries. Videos available at
https://sites.google.com/view/offline-prefs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1"&gt;Daniel Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1"&gt;Daniel S. Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LENS: Layer Distribution Enabled Neural Architecture Search in Edge-Cloud Hierarchies. (arXiv:2107.09309v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09309</id>
        <link href="http://arxiv.org/abs/2107.09309"/>
        <updated>2021-07-21T02:01:35.736Z</updated>
        <summary type="html"><![CDATA[Edge-Cloud hierarchical systems employing intelligence through Deep Neural
Networks (DNNs) endure the dilemma of workload distribution within them.
Previous solutions proposed to distribute workloads at runtime according to the
state of the surroundings, like the wireless conditions. However, such
conditions are usually overlooked at design time. This paper addresses this
issue for DNN architectural design by presenting a novel methodology, LENS,
which administers multi-objective Neural Architecture Search (NAS) for
two-tiered systems, where the performance objectives are refashioned to
consider the wireless communication parameters. From our experimental search
space, we demonstrate that LENS improves upon the traditional solution's Pareto
set by 76.47% and 75% with respect to the energy and latency metrics,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Odema_M/0/1/0/all/0/1"&gt;Mohanad Odema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashid_N/0/1/0/all/0/1"&gt;Nafiul Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demirel_B/0/1/0/all/0/1"&gt;Berken Utku Demirel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1"&gt;Mohammad Abdullah Al Faruque&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Embedding of ReLU Networks and an Analysis of their Identifiability. (arXiv:2107.09370v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09370</id>
        <link href="http://arxiv.org/abs/2107.09370"/>
        <updated>2021-07-21T02:01:35.729Z</updated>
        <summary type="html"><![CDATA[Neural networks with the Rectified Linear Unit (ReLU) nonlinearity are
described by a vector of parameters $\theta$, and realized as a piecewise
linear continuous function $R_{\theta}: x \in \mathbb R^{d} \mapsto
R_{\theta}(x) \in \mathbb R^{k}$. Natural scalings and permutations operations
on the parameters $\theta$ leave the realization unchanged, leading to
equivalence classes of parameters that yield the same realization. These
considerations in turn lead to the notion of identifiability -- the ability to
recover (the equivalence class of) $\theta$ from the sole knowledge of its
realization $R_{\theta}$. The overall objective of this paper is to introduce
an embedding for ReLU neural networks of any depth, $\Phi(\theta)$, that is
invariant to scalings and that provides a locally linear parameterization of
the realization of the network. Leveraging these two key properties, we derive
some conditions under which a deep ReLU network is indeed locally identifiable
from the knowledge of the realization on a finite set of samples $x_{i} \in
\mathbb R^{d}$. We study the shallow case in more depth, establishing necessary
and sufficient conditions for the network to be identifiable from a bounded
subset $\mathcal X \subseteq \mathbb R^{d}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stock_P/0/1/0/all/0/1"&gt;Pierre Stock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gribonval_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Gribonval&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separating Skills and Concepts for Novel Visual Question Answering. (arXiv:2107.09106v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09106</id>
        <link href="http://arxiv.org/abs/2107.09106"/>
        <updated>2021-07-21T02:01:35.711Z</updated>
        <summary type="html"><![CDATA[Generalization to out-of-distribution data has been a problem for Visual
Question Answering (VQA) models. To measure generalization to novel questions,
we propose to separate them into "skills" and "concepts". "Skills" are visual
tasks, such as counting or attribute recognition, and are applied to "concepts"
mentioned in the question, such as objects and people. VQA methods should be
able to compose skills and concepts in novel ways, regardless of whether the
specific composition has been seen in training, yet we demonstrate that
existing models have much to improve upon towards handling new compositions. We
present a novel method for learning to compose skills and concepts that
separates these two factors implicitly within a model by learning grounded
concept representations and disentangling the encoding of skills from that of
concepts. We enforce these properties with a novel contrastive learning
procedure that does not rely on external annotations and can be learned from
unlabeled image-question pairs. Experiments demonstrate the effectiveness of
our approach for improving compositional and grounding performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1"&gt;Spencer Whitehead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shared Interest: Large-Scale Visual Analysis of Model Behavior by Measuring Human-AI Alignment. (arXiv:2107.09234v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09234</id>
        <link href="http://arxiv.org/abs/2107.09234"/>
        <updated>2021-07-21T02:01:35.704Z</updated>
        <summary type="html"><![CDATA[Saliency methods -- techniques to identify the importance of input features
on a model's output -- are a common first step in understanding neural network
behavior. However, interpreting saliency requires tedious manual inspection to
identify and aggregate patterns in model behavior, resulting in ad hoc or
cherry-picked analysis. To address these concerns, we present Shared Interest:
a set of metrics for comparing saliency with human annotated ground truths. By
providing quantitative descriptors, Shared Interest allows ranking, sorting,
and aggregation of inputs thereby facilitating large-scale systematic analysis
of model behavior. We use Shared Interest to identify eight recurring patterns
in model behavior including focusing on a sufficient subset of ground truth
features or being distracted by contextual features. Working with
representative real-world users, we show how Shared Interest can be used to
rapidly develop or lose trust in a model's reliability, uncover issues that are
missed in manual analyses, and enable interactive probing of model behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1"&gt;Angie Boggust&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1"&gt;Benjamin Hoover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1"&gt;Arvind Satyanarayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1"&gt;Hendrik Strobelt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Friction System Performance with Symbolic Regression and Genetic Programming with Factor Variables. (arXiv:2107.09484v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09484</id>
        <link href="http://arxiv.org/abs/2107.09484"/>
        <updated>2021-07-21T02:01:35.697Z</updated>
        <summary type="html"><![CDATA[Friction systems are mechanical systems wherein friction is used for force
transmission (e.g. mechanical braking systems or automatic gearboxes). For
finding optimal and safe design parameters, engineers have to predict friction
system performance. This is especially difficult in real-world applications,
because it is affected by many parameters. We have used symbolic regression and
genetic programming for finding accurate and trustworthy prediction models for
this task. However, it is not straight-forward how nominal variables can be
included. In particular, a one-hot-encoding is unsatisfactory because genetic
programming tends to remove such indicator variables. We have therefore used
so-called factor variables for representing nominal variables in symbolic
regression models. Our results show that GP is able to produce symbolic
regression models for predicting friction performance with predictive accuracy
that is comparable to artificial neural networks. The symbolic regression
models with factor variables are less complex than models using a one-hot
encoding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1"&gt;Gabriel Kronberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kommenda_M/0/1/0/all/0/1"&gt;Michael Kommenda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Promberger_A/0/1/0/all/0/1"&gt;Andreas Promberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nickel_F/0/1/0/all/0/1"&gt;Falk Nickel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Analysis of Measure-Valued Derivatives for Policy Gradients. (arXiv:2107.09359v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09359</id>
        <link href="http://arxiv.org/abs/2107.09359"/>
        <updated>2021-07-21T02:01:35.690Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning methods for robotics are increasingly successful due
to the constant development of better policy gradient techniques. A precise
(low variance) and accurate (low bias) gradient estimator is crucial to face
increasingly complex tasks. Traditional policy gradient algorithms use the
likelihood-ratio trick, which is known to produce unbiased but high variance
estimates. More modern approaches exploit the reparametrization trick, which
gives lower variance gradient estimates but requires differentiable value
function approximators. In this work, we study a different type of stochastic
gradient estimator: the Measure-Valued Derivative. This estimator is unbiased,
has low variance, and can be used with differentiable and non-differentiable
function approximators. We empirically evaluate this estimator in the
actor-critic policy gradient setting and show that it can reach comparable
performance with methods based on the likelihood-ratio or reparametrization
tricks, both in low and high-dimensional action spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tateo_D/0/1/0/all/0/1"&gt;Davide Tateo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muratore_F/0/1/0/all/0/1"&gt;Fabio Muratore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jan Peters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09391</id>
        <link href="http://arxiv.org/abs/2107.09391"/>
        <updated>2021-07-21T02:01:35.682Z</updated>
        <summary type="html"><![CDATA[We focus on building robustness in the convolutions of neural visual
classifiers, especially against natural perturbations like elastic
deformations, occlusions and Gaussian noise. Existing CNNs show outstanding
performance on clean images, but fail to tackle naturally occurring
perturbations. In this paper, we start from elastic perturbations, which
approximate (local) view-point changes of the object. We present
elastically-augmented convolutions (EAConv) by parameterizing filters as a
combination of fixed elastically-perturbed bases functions and trainable
weights for the purpose of integrating unseen viewpoints in the CNN. We show on
CIFAR-10 and STL-10 datasets that the general robustness of our method on
unseen occlusion and Gaussian perturbations improves, while even improving the
performance on clean images slightly without performing any data augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gulshad_S/0/1/0/all/0/1"&gt;Sadaf Gulshad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1"&gt;Ivan Sosnovik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1"&gt;Arnold Smeulders&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ByPE-VAE: Bayesian Pseudocoresets Exemplar VAE. (arXiv:2107.09286v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09286</id>
        <link href="http://arxiv.org/abs/2107.09286"/>
        <updated>2021-07-21T02:01:35.665Z</updated>
        <summary type="html"><![CDATA[Recent studies show that advanced priors play a major role in deep generative
models. Exemplar VAE, as a variant of VAE with an exemplar-based prior, has
achieved impressive results. However, due to the nature of model design, an
exemplar-based model usually requires vast amounts of data to participate in
training, which leads to huge computational complexity. To address this issue,
we propose Bayesian Pseudocoresets Exemplar VAE (ByPE-VAE), a new variant of
VAE with a prior based on Bayesian pseudocoreset. The proposed prior is
conditioned on a small-scale pseudocoreset rather than the whole dataset for
reducing the computational cost and avoiding overfitting. Simultaneously, we
obtain the optimal pseudocoreset via a stochastic optimization algorithm during
VAE training aiming to minimize the Kullback-Leibler divergence between the
prior based on the pseudocoreset and that based on the whole dataset.
Experimental results show that ByPE-VAE can achieve competitive improvements
over the state-of-the-art VAEs in the tasks of density estimation,
representation learning, and generative data augmentation. Particularly, on a
basic VAE architecture, ByPE-VAE is up to 3 times faster than Exemplar VAE
while almost holding the performance. Code is available at our supplementary
materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingzhong Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lirong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Sorted Table Search and Static Indexes in Small Space: Methodological and Practical Insights via an Experimental Study. (arXiv:2107.09480v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.09480</id>
        <link href="http://arxiv.org/abs/2107.09480"/>
        <updated>2021-07-21T02:01:35.658Z</updated>
        <summary type="html"><![CDATA[Sorted Table Search Procedures are the quintessential query-answering tool,
still very useful, e.g, Search Engines (Google Chrome). Speeding them up, in
small additional space with respect to the table being searched into, is still
a quite significant achievement. Static Learned Indexes have been very
successful in achieving such a speed-up, but leave open a major question: To
what extent one can enjoy the speed-up of Learned Indexes while using constant
or nearly constant additional space. By generalizing the experimental
methodology of a recent benchmarking study on Learned Indexes, we shed light on
this question, by considering two scenarios. The first, quite elementary, i.e.,
textbook code, and the second using advanced Learned Indexing algorithms and
the supporting sophisticated software platforms. Although in both cases one
would expect a positive answer, its achievement is not as simple as it seems.
Indeed, our extensive set of experiments reveal a complex relationship between
query time and model space. The findings regarding this relationship and the
corresponding quantitative estimates, across memory levels, can be of interest
to algorithm designers and of use to practitioners as well. As an essential
part of our research, we introduce two new models that are of interest in their
own right. The first is a constant space model that can be seen as a
generalization of $k$-ary search, while the second is a synoptic {\bf RMI}, in
which we can control model space usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amato_D/0/1/0/all/0/1"&gt;Domenico Amato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giancarlo_R/0/1/0/all/0/1"&gt;Raffaele Giancarlo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosco_G/0/1/0/all/0/1"&gt;Giosu&amp;#xe8; Lo Bosco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wide and Deep Graph Neural Network with Distributed Online Learning. (arXiv:2107.09203v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09203</id>
        <link href="http://arxiv.org/abs/2107.09203"/>
        <updated>2021-07-21T02:01:35.651Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are naturally distributed architectures for
learning representations from network data. This renders them suitable
candidates for decentralized tasks. In these scenarios, the underlying graph
often changes with time due to link failures or topology variations, creating a
mismatch between the graphs on which GNNs were trained and the ones on which
they are tested. Online learning can be leveraged to retrain GNNs at testing
time to overcome this issue. However, most online algorithms are centralized
and usually offer guarantees only on convex problems, which GNNs rarely lead
to. This paper develops the Wide and Deep GNN (WD-GNN), a novel architecture
that can be updated with distributed online learning mechanisms. The WD-GNN
consists of two components: the wide part is a linear graph filter and the deep
part is a nonlinear GNN. At training time, the joint wide and deep architecture
learns nonlinear representations from data. At testing time, the wide, linear
part is retrained, while the deep, nonlinear one remains fixed. This often
leads to a convex formulation. We further propose a distributed online learning
algorithm that can be implemented in a decentralized setting. We also show the
stability of the WD-GNN to changes of the underlying graph and analyze the
convergence of the proposed online learning procedure. Experiments on movie
recommendation, source localization and robot swarm control corroborate
theoretical findings and show the potential of the WD-GNN for distributed
online learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1"&gt;Fernando Gama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active operator inference for learning low-dimensional dynamical-system models from noisy data. (arXiv:2107.09256v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09256</id>
        <link href="http://arxiv.org/abs/2107.09256"/>
        <updated>2021-07-21T02:01:35.603Z</updated>
        <summary type="html"><![CDATA[Noise poses a challenge for learning dynamical-system models because already
small variations can distort the dynamics described by trajectory data. This
work builds on operator inference from scientific machine learning to infer
low-dimensional models from high-dimensional state trajectories polluted with
noise. The presented analysis shows that, under certain conditions, the
inferred operators are unbiased estimators of the well-studied projection-based
reduced operators from traditional model reduction. Furthermore, the connection
between operator inference and projection-based model reduction enables
bounding the mean-squared errors of predictions made with the learned models
with respect to traditional reduced models. The analysis also motivates an
active operator inference approach that judiciously samples high-dimensional
trajectories with the aim of achieving a low mean-squared error by reducing the
effect of noise. Numerical experiments with high-dimensional linear and
nonlinear state dynamics demonstrate that predictions obtained with active
operator inference have orders of magnitude lower mean-squared errors than
operator inference with traditional, equidistantly sampled trajectory data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uy_W/0/1/0/all/0/1"&gt;Wayne Isaac Tan Uy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuepeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yuxiao Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peherstorfer_B/0/1/0/all/0/1"&gt;Benjamin Peherstorfer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the limiting dynamics of SGD: modified loss, phase space oscillations, and anomalous diffusion. (arXiv:2107.09133v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09133</id>
        <link href="http://arxiv.org/abs/2107.09133"/>
        <updated>2021-07-21T02:01:35.596Z</updated>
        <summary type="html"><![CDATA[In this work we explore the limiting dynamics of deep neural networks trained
with stochastic gradient descent (SGD). We find empirically that long after
performance has converged, networks continue to move through parameter space by
a process of anomalous diffusion in which distance travelled grows as a power
law in the number of gradient updates with a nontrivial exponent. We reveal an
intricate interaction between the hyperparameters of optimization, the
structure in the gradient noise, and the Hessian matrix at the end of training
that explains this anomalous diffusion. To build this understanding, we first
derive a continuous-time model for SGD with finite learning rates and batch
sizes as an underdamped Langevin equation. We study this equation in the
setting of linear regression, where we can derive exact, analytic expressions
for the phase space dynamics of the parameters and their instantaneous
velocities from initialization to stationarity. Using the Fokker-Planck
equation, we show that the key ingredient driving these dynamics is not the
original training loss, but rather the combination of a modified loss, which
implicitly regularizes the velocity, and probability currents, which cause
oscillations in phase space. We identify qualitative and quantitative
predictions of this theory in the dynamics of a ResNet-18 model trained on
ImageNet. Through the lens of statistical physics, we uncover a mechanistic
origin for the anomalous limiting dynamics of deep neural networks trained with
SGD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kunin_D/0/1/0/all/0/1"&gt;Daniel Kunin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sagastuy_Brena_J/0/1/0/all/0/1"&gt;Javier Sagastuy-Brena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gillespie_L/0/1/0/all/0/1"&gt;Lauren Gillespie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Margalit_E/0/1/0/all/0/1"&gt;Eshed Margalit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1"&gt;Hidenori Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1"&gt;Surya Ganguli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1"&gt;Daniel L. K. Yamins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement learning autonomously identifying the source of errors for agents in a group mission. (arXiv:2107.09232v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.09232</id>
        <link href="http://arxiv.org/abs/2107.09232"/>
        <updated>2021-07-21T02:01:35.577Z</updated>
        <summary type="html"><![CDATA[When agents are swarmed to carry out a mission, there is often a sudden
failure of some of the agents observed from the command base. It is generally
difficult to distinguish whether the failure is caused by actuators
(hypothesis, $h_a$) or sensors (hypothesis, $h_s$) solely by the communication
between the command base and the concerning agent. By making a collision to the
agent by another, we would be able to distinguish which hypothesis is likely:
For $h_a$, we expect to detect corresponding displacements while for $h_a$ we
do not. Such swarm strategies to grasp the situation are preferably to be
generated autonomously by artificial intelligence (AI). Preferable actions
($e.g.$, the collision) for the distinction would be those maximizing the
difference between the expected behaviors for each hypothesis, as a value
function. Such actions exist, however, only very sparsely in the whole
possibilities, for which the conventional search based on gradient methods does
not make sense. Instead, we have successfully applied the reinforcement
learning technique, achieving the maximization of such a sparse value function.
The machine learning actually concluded autonomously the colliding action to
distinguish the hypothesises. Getting recognized an agent with actuator error
by the action, the agents behave as if other ones want to assist the
malfunctioning one to achieve a given mission.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Utimula_K/0/1/0/all/0/1"&gt;Keishu Utimula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayaschi_K/0/1/0/all/0/1"&gt;Ken-taro Hayaschi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakano_K/0/1/0/all/0/1"&gt;Kousuke Nakano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hongo_K/0/1/0/all/0/1"&gt;Kenta Hongo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maezono_R/0/1/0/all/0/1"&gt;Ryo Maezono&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Policy Gradient Method for Safe and Fast Reinforcement Learning: a Neural Tangent Kernel Based Approach. (arXiv:2107.09139v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09139</id>
        <link href="http://arxiv.org/abs/2107.09139"/>
        <updated>2021-07-21T02:01:35.570Z</updated>
        <summary type="html"><![CDATA[This paper presents a constrained policy gradient algorithm. We introduce
constraints for safe learning with the following steps. First, learning is
slowed down (lazy learning) so that the episodic policy change can be computed
with the help of the policy gradient theorem and the neural tangent kernel.
Then, this enables us the evaluation of the policy at arbitrary states too. In
the same spirit, learning can be guided, ensuring safety via augmenting episode
batches with states where the desired action probabilities are prescribed.
Finally, exogenous discounted sum of future rewards (returns) can be computed
at these specific state-action pairs such that the policy network satisfies
constraints. Computing the returns is based on solving a system of linear
equations (equality constraints) or a constrained quadratic program (inequality
constraints). Simulation results suggest that adding constraints (external
information) to the learning can improve learning in terms of speed and safety
reasonably if constraints are appropriately selected. The efficiency of the
constrained learning was demonstrated with a shallow and wide ReLU network in
the Cartpole and Lunar Lander OpenAI gym environments. The main novelty of the
paper is giving a practical use of the neural tangent kernel in reinforcement
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Varga_B/0/1/0/all/0/1"&gt;Bal&amp;#xe1;zs Varga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulcsar_B/0/1/0/all/0/1"&gt;Bal&amp;#xe1;zs Kulcs&amp;#xe1;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1"&gt;Morteza Haghir Chehreghani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequence-to-Sequence Piano Transcription with Transformers. (arXiv:2107.09142v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.09142</id>
        <link href="http://arxiv.org/abs/2107.09142"/>
        <updated>2021-07-21T02:01:35.563Z</updated>
        <summary type="html"><![CDATA[Automatic Music Transcription has seen significant progress in recent years
by training custom deep neural networks on large datasets. However, these
models have required extensive domain-specific design of network architectures,
input/output representations, and complex decoding schemes. In this work, we
show that equivalent performance can be achieved using a generic
encoder-decoder Transformer with standard decoding methods. We demonstrate that
the model can learn to translate spectrogram inputs directly to MIDI-like
output events for several transcription tasks. This sequence-to-sequence
approach simplifies transcription by jointly modeling audio features and
language-like output dependencies, thus removing the need for task-specific
architectures. These results point toward possibilities for creating new Music
Information Retrieval models by focusing on dataset creation and labeling
rather than custom model design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hawthorne_C/0/1/0/all/0/1"&gt;Curtis Hawthorne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simon_I/0/1/0/all/0/1"&gt;Ian Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swavely_R/0/1/0/all/0/1"&gt;Rigel Swavely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manilow_E/0/1/0/all/0/1"&gt;Ethan Manilow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engel_J/0/1/0/all/0/1"&gt;Jesse Engel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymptotic Escape of Spurious Critical Points on the Low-rank Matrix Manifold. (arXiv:2107.09207v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.09207</id>
        <link href="http://arxiv.org/abs/2107.09207"/>
        <updated>2021-07-21T02:01:35.534Z</updated>
        <summary type="html"><![CDATA[We show that the Riemannian gradient descent algorithm on the low-rank matrix
manifold almost surely escapes some spurious critical points on the boundary of
the manifold. Given that the low-rank matrix manifold is an incomplete set,
this result is the first to overcome this difficulty and partially justify the
global use of the Riemannian gradient descent on the manifold. The spurious
critical points are some rank-deficient matrices that capture only part of the
SVD components of the ground truth. They exhibit very singular behavior and
evade the classical analysis of strict saddle points. We show that using the
dynamical low-rank approximation and a rescaled gradient flow, some of the
spurious critical points can be converted to classical strict saddle points,
which leads to the desired result. Numerical experiments are provided to
support our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hou_T/0/1/0/all/0/1"&gt;Thomas Y. Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenzhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziyun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Selection for Stein Variational Gradient Descent. (arXiv:2107.09338v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09338</id>
        <link href="http://arxiv.org/abs/2107.09338"/>
        <updated>2021-07-21T02:01:35.518Z</updated>
        <summary type="html"><![CDATA[Stein variational gradient descent (SVGD) and its variants have shown
promising successes in approximate inference for complex distributions.
However, their empirical performance depends crucially on the choice of optimal
kernel. Unfortunately, RBF kernel with median heuristics is a common choice in
previous approaches which has been proved sub-optimal. Inspired by the paradigm
of multiple kernel learning, our solution to this issue is using a combination
of multiple kernels to approximate the optimal kernel instead of a single one
which may limit the performance and flexibility. To do so, we extend Kernelized
Stein Discrepancy (KSD) to its multiple kernel view called Multiple Kernelized
Stein Discrepancy (MKSD). Further, we leverage MKSD to construct a general
algorithm based on SVGD, which be called Multiple Kernel SVGD (MK-SVGD).
Besides, we automatically assign a weight to each kernel without any other
parameters. The proposed method not only gets rid of optimal kernel dependence
but also maintains computational effectiveness. Experiments on various tasks
and models show the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingzhong Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressing Multisets with Large Alphabets. (arXiv:2107.09202v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.09202</id>
        <link href="http://arxiv.org/abs/2107.09202"/>
        <updated>2021-07-21T02:01:35.511Z</updated>
        <summary type="html"><![CDATA[Current methods that optimally compress multisets are not suitable for
high-dimensional symbols, as their compute time scales linearly with alphabet
size. Compressing a multiset as an ordered sequence with off-the-shelf codecs
is computationally more efficient, but has a sub-optimal compression rate, as
bits are wasted encoding the order between symbols. We present a method that
can recover those bits, assuming symbols are i.i.d., at the cost of an
additional $\mathcal{O}(|\mathcal{M}|\log M)$ in average time complexity, where
$|\mathcal{M}|$ and $M$ are the total and unique number of symbols in the
multiset. Our method is compatible with any prefix-free code. Experiments show
that, when paired with efficient coders, our method can efficiently compress
high-dimensional sources such as multisets of images and collections of JSON
files.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Severo_D/0/1/0/all/0/1"&gt;Daniel Severo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Townsend_J/0/1/0/all/0/1"&gt;James Townsend&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khisti_A/0/1/0/all/0/1"&gt;Ashish Khisti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makhzani_A/0/1/0/all/0/1"&gt;Alireza Makhzani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1"&gt;Karen Ullrich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LAPNet: Non-rigid Registration derived in k-space for Magnetic Resonance Imaging. (arXiv:2107.09060v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09060</id>
        <link href="http://arxiv.org/abs/2107.09060"/>
        <updated>2021-07-21T02:01:35.437Z</updated>
        <summary type="html"><![CDATA[Physiological motion, such as cardiac and respiratory motion, during Magnetic
Resonance (MR) image acquisition can cause image artifacts. Motion correction
techniques have been proposed to compensate for these types of motion during
thoracic scans, relying on accurate motion estimation from undersampled
motion-resolved reconstruction. A particular interest and challenge lie in the
derivation of reliable non-rigid motion fields from the undersampled
motion-resolved data. Motion estimation is usually formulated in image space
via diffusion, parametric-spline, or optical flow methods. However, image-based
registration can be impaired by remaining aliasing artifacts due to the
undersampled motion-resolved reconstruction. In this work, we describe a
formalism to perform non-rigid registration directly in the sampled Fourier
space, i.e. k-space. We propose a deep-learning based approach to perform fast
and accurate non-rigid registration from the undersampled k-space data. The
basic working principle originates from the Local All-Pass (LAP) technique, a
recently introduced optical flow-based registration. The proposed LAPNet is
compared against traditional and deep learning image-based registrations and
tested on fully-sampled and highly-accelerated (with two undersampling
strategies) 3D respiratory motion-resolved MR images in a cohort of 40 patients
with suspected liver or lung metastases and 25 healthy subjects. The proposed
LAPNet provided consistent and superior performance to image-based approaches
throughout different sampling trajectories and acceleration factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kustner_T/0/1/0/all/0/1"&gt;Thomas K&amp;#xfc;stner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1"&gt;Jiazhen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haikun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cruz_G/0/1/0/all/0/1"&gt;Gastao Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gilliam_C/0/1/0/all/0/1"&gt;Christopher Gilliam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blu_T/0/1/0/all/0/1"&gt;Thierry Blu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1"&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Botnar_R/0/1/0/all/0/1"&gt;Ren&amp;#xe9; Botnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prieto_C/0/1/0/all/0/1"&gt;Claudia Prieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating deep neural networks for efficient scene understanding in automotive cyber-physical systems. (arXiv:2107.09101v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09101</id>
        <link href="http://arxiv.org/abs/2107.09101"/>
        <updated>2021-07-21T02:01:35.427Z</updated>
        <summary type="html"><![CDATA[Automotive Cyber-Physical Systems (ACPS) have attracted a significant amount
of interest in the past few decades, while one of the most critical operations
in these systems is the perception of the environment. Deep learning and,
especially, the use of Deep Neural Networks (DNNs) provides impressive results
in analyzing and understanding complex and dynamic scenes from visual data. The
prediction horizons for those perception systems are very short and inference
must often be performed in real time, stressing the need of transforming the
original large pre-trained networks into new smaller models, by utilizing Model
Compression and Acceleration (MCA) techniques. Our goal in this work is to
investigate best practices for appropriately applying novel weight sharing
techniques, optimizing the available variables and the training procedures
towards the significant acceleration of widely adopted DNNs. Extensive
evaluation studies carried out using various state-of-the-art DNN models in
object detection and tracking experiments, provide details about the type of
errors that manifest after the application of weight sharing techniques,
resulting in significant acceleration gains with negligible accuracy losses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1"&gt;Stavros Nousias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pikoulis_E/0/1/0/all/0/1"&gt;Erion-Vasilis Pikoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mavrokefalidis_C/0/1/0/all/0/1"&gt;Christos Mavrokefalidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1"&gt;Aris S. Lalos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepSocNav: Social Navigation by Imitating Human Behaviors. (arXiv:2107.09170v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09170</id>
        <link href="http://arxiv.org/abs/2107.09170"/>
        <updated>2021-07-21T02:01:35.400Z</updated>
        <summary type="html"><![CDATA[Current datasets to train social behaviors are usually borrowed from
surveillance applications that capture visual data from a bird's-eye
perspective. This leaves aside precious relationships and visual cues that
could be captured through a first-person view of a scene. In this work, we
propose a strategy to exploit the power of current game engines, such as Unity,
to transform pre-existing bird's-eye view datasets into a first-person view, in
particular, a depth view. Using this strategy, we are able to generate large
volumes of synthetic data that can be used to pre-train a social navigation
model. To test our ideas, we present DeepSocNav, a deep learning based model
that takes advantage of the proposed approach to generate synthetic data.
Furthermore, DeepSocNav includes a self-supervised strategy that is included as
an auxiliary task. This consists of predicting the next depth frame that the
agent will face. Our experiments show the benefits of the proposed model that
is able to outperform relevant baselines in terms of social navigation scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vicente_J/0/1/0/all/0/1"&gt;Juan Pablo de Vicente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1"&gt;Alvaro Soto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity of Learning Quantum Circuits. (arXiv:2107.09078v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.09078</id>
        <link href="http://arxiv.org/abs/2107.09078"/>
        <updated>2021-07-21T02:01:35.387Z</updated>
        <summary type="html"><![CDATA[Quantum computers hold unprecedented potentials for machine learning
applications. Here, we prove that physical quantum circuits are PAC (probably
approximately correct) learnable on a quantum computer via empirical risk
minimization: to learn a quantum circuit with at most $n^c$ gates and each gate
acting on a constant number of qubits, the sample complexity is bounded by
$\tilde{O}(n^{c+1})$. In particular, we explicitly construct a family of
variational quantum circuits with $O(n^{c+1})$ elementary gates arranged in a
fixed pattern, which can represent all physical quantum circuits consisting of
at most $n^c$ elementary gates. Our results provide a valuable guide for
quantum machine learning in both theory and experiment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cai_H/0/1/0/all/0/1"&gt;Haoyuan Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qi Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Deng_D/0/1/0/all/0/1"&gt;Dong-Ling Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OnlineSTL: Scaling Time Series Decomposition by 100x. (arXiv:2107.09110v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09110</id>
        <link href="http://arxiv.org/abs/2107.09110"/>
        <updated>2021-07-21T02:01:35.365Z</updated>
        <summary type="html"><![CDATA[Decomposing a complex time series into trend, seasonality, and remainder
components is an important primitive that facilitates time series anomaly
detection, change point detection and forecasting. Although numerous batch
algorithms are known for time series decomposition, none operate well in an
online scalable setting where high throughput and real-time response are
paramount. In this paper, we propose OnlineSTL, a novel online algorithm for
time series decomposition which solves the scalability problem and is deployed
for real-time metrics monitoring on high resolution, high ingest rate data.
Experiments on different synthetic and real world time series datasets
demonstrate that OnlineSTL achieves orders of magnitude speedups while
maintaining quality of decomposition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1"&gt;Abhinav Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sriharsha_R/0/1/0/all/0/1"&gt;Ram Sriharsha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1"&gt;Sichen Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving exploration in policy gradient search: Application to symbolic optimization. (arXiv:2107.09158v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09158</id>
        <link href="http://arxiv.org/abs/2107.09158"/>
        <updated>2021-07-21T02:01:35.334Z</updated>
        <summary type="html"><![CDATA[Many machine learning strategies designed to automate mathematical tasks
leverage neural networks to search large combinatorial spaces of mathematical
symbols. In contrast to traditional evolutionary approaches, using a neural
network at the core of the search allows learning higher-level symbolic
patterns, providing an informed direction to guide the search. When no labeled
data is available, such networks can still be trained using reinforcement
learning. However, we demonstrate that this approach can suffer from an early
commitment phenomenon and from initialization bias, both of which limit
exploration. We present two exploration methods to tackle these issues,
building upon ideas of entropy regularization and distribution initialization.
We show that these techniques can improve the performance, increase sample
efficiency, and lower the complexity of solutions for the task of symbolic
regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Larma_M/0/1/0/all/0/1"&gt;Mikel Landajuela Larma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_B/0/1/0/all/0/1"&gt;Brenden K. Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Soo K. Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santiago_C/0/1/0/all/0/1"&gt;Claudio P. Santiago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glatt_R/0/1/0/all/0/1"&gt;Ruben Glatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mundhenk_T/0/1/0/all/0/1"&gt;T. Nathan Mundhenk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pettit_J/0/1/0/all/0/1"&gt;Jacob F. Pettit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faissol_D/0/1/0/all/0/1"&gt;Daniel M. Faissol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI in Finance: Challenges, Techniques and Opportunities. (arXiv:2107.09051v1 [q-fin.CP])]]></title>
        <id>http://arxiv.org/abs/2107.09051</id>
        <link href="http://arxiv.org/abs/2107.09051"/>
        <updated>2021-07-21T02:01:35.316Z</updated>
        <summary type="html"><![CDATA[AI in finance broadly refers to the applications of AI techniques in
financial businesses. This area has been lasting for decades with both classic
and modern AI techniques applied to increasingly broader areas of finance,
economy and society. In contrast to either discussing the problems, aspects and
opportunities of finance that have benefited from specific AI techniques and in
particular some new-generation AI and data science (AIDS) areas or reviewing
the progress of applying specific techniques to resolving certain financial
problems, this review offers a comprehensive and dense roadmap of the
overwhelming challenges, techniques and opportunities of AI research in finance
over the past decades. The landscapes and challenges of financial businesses
and data are firstly outlined, followed by a comprehensive categorization and a
dense overview of the decades of AI research in finance. We then structure and
illustrate the data-driven analytics and learning of financial businesses and
data. The comparison, criticism and discussion of classic vs. modern AI
techniques for finance are followed. Lastly, open issues and opportunities
address future AI-empowered finance and finance-motivated AI research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Cao_L/0/1/0/all/0/1"&gt;Longbing Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latency-Memory Optimized Splitting of Convolution Neural Networks for Resource Constrained Edge Devices. (arXiv:2107.09123v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09123</id>
        <link href="http://arxiv.org/abs/2107.09123"/>
        <updated>2021-07-21T02:01:35.299Z</updated>
        <summary type="html"><![CDATA[With the increasing reliance of users on smart devices, bringing essential
computation at the edge has become a crucial requirement for any type of
business. Many such computations utilize Convolution Neural Networks (CNNs) to
perform AI tasks, having high resource and computation requirements, that are
infeasible for edge devices. Splitting the CNN architecture to perform part of
the computation on edge and remaining on the cloud is an area of research that
has seen increasing interest in the field. In this paper, we assert that
running CNNs between an edge device and the cloud is synonymous to solving a
resource-constrained optimization problem that minimizes the latency and
maximizes resource utilization at the edge. We formulate a multi-objective
optimization problem and propose the LMOS algorithm to achieve a Pareto
efficient solution. Experiments done on real-world edge devices show that, LMOS
ensures feasible execution of different CNN models at the edge and also
improves upon existing state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1"&gt;Tanmay Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avaneesh/0/1/0/all/0/1"&gt;Avaneesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1"&gt;Rohit Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shorey_R/0/1/0/all/0/1"&gt;Rajeev Shorey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOGAN: 3D-Aware Shadow and Occlusion Robust GAN for Makeup Transfer. (arXiv:2104.10567v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10567</id>
        <link href="http://arxiv.org/abs/2104.10567"/>
        <updated>2021-07-21T02:01:35.279Z</updated>
        <summary type="html"><![CDATA[In recent years, virtual makeup applications have become more and more
popular. However, it is still challenging to propose a robust makeup transfer
method in the real-world environment. Current makeup transfer methods mostly
work well on good-conditioned clean makeup images, but transferring makeup that
exhibits shadow and occlusion is not satisfying. To alleviate it, we propose a
novel makeup transfer method, called 3D-Aware Shadow and Occlusion Robust GAN
(SOGAN). Given the source and the reference faces, we first fit a 3D face model
and then disentangle the faces into shape and texture. In the texture branch,
we map the texture to the UV space and design a UV texture generator to
transfer the makeup. Since human faces are symmetrical in the UV space, we can
conveniently remove the undesired shadow and occlusion from the reference image
by carefully designing a Flip Attention Module (FAM). After obtaining cleaner
makeup features from the reference image, a Makeup Transfer Module (MTM) is
introduced to perform accurate makeup transfer. The qualitative and
quantitative experiments demonstrate that our SOGAN not only achieves superior
results in shadow and occlusion situations but also performs well in large pose
and expression variations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yueming Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1"&gt;Bo Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tieniu Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Aware Neural Networks for Skin Cancer Detection. (arXiv:2107.09118v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09118</id>
        <link href="http://arxiv.org/abs/2107.09118"/>
        <updated>2021-07-21T02:01:35.272Z</updated>
        <summary type="html"><![CDATA[Deep learning (DL) models have received particular attention in medical
imaging due to their promising pattern recognition capabilities. However, Deep
Neural Networks (DNNs) require a huge amount of data, and because of the lack
of sufficient data in this field, transfer learning can be a great solution.
DNNs used for disease diagnosis meticulously concentrate on improving the
accuracy of predictions without providing a figure about their confidence of
predictions. Knowing how much a DNN model is confident in a computer-aided
diagnosis model is necessary for gaining clinicians' confidence and trust in
DL-based solutions. To address this issue, this work presents three different
methods for quantifying uncertainties for skin cancer detection from images. It
also comprehensively evaluates and compares performance of these DNNs using
novel uncertainty-related metrics. The obtained results reveal that the
predictive uncertainty estimation methods are capable of flagging risky and
erroneous predictions with a high uncertainty estimate. We also demonstrate
that ensemble approaches are more reliable in capturing uncertainties through
inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khaledyan_D/0/1/0/all/0/1"&gt;Donya Khaledyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tajally_A/0/1/0/all/0/1"&gt;AmirReza Tajally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sarkhosh_R/0/1/0/all/0/1"&gt;Reza Sarkhosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shamsi_A/0/1/0/all/0/1"&gt;Afshar Shamsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asgharnezhad_H/0/1/0/all/0/1"&gt;Hamzeh Asgharnezhad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1"&gt;Abbas Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1"&gt;Saeid Nahavandi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-supervised Semantic Segmentation in Cityscape via Hyperspectral Image. (arXiv:2012.10122v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10122</id>
        <link href="http://arxiv.org/abs/2012.10122"/>
        <updated>2021-07-21T02:01:35.265Z</updated>
        <summary type="html"><![CDATA[High-resolution hyperspectral images (HSIs) contain the response of each
pixel in different spectral bands, which can be used to effectively distinguish
various objects in complex scenes. While HSI cameras have become low cost,
algorithms based on it have not been well exploited. In this paper, we focus on
a novel topic, weakly-supervised semantic segmentation in cityscape via HSIs.
It is based on the idea that high-resolution HSIs in city scenes contain rich
spectral information, which can be easily associated to semantics without
manual labeling. Therefore, it enables low cost, highly reliable semantic
segmentation in complex scenes. Specifically, in this paper, we theoretically
analyze the HSIs and introduce a weakly-supervised HSI semantic segmentation
framework, which utilizes spectral information to improve the coarse labels to
a finer degree. The experimental results show that our method can obtain highly
competitive labels and even have higher edge fineness than artificial fine
labels in some classes. At the same time, the results also show that the
refined labels can effectively improve the effect of semantic segmentation. The
combination of HSIs and semantic segmentation proves that HSIs have great
potential in high-level visual tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shaodi You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Ying Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1"&gt;Qiu Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Facial Affect Analysis: A Review of Methods, Applications and Challenges. (arXiv:2103.15599v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15599</id>
        <link href="http://arxiv.org/abs/2103.15599"/>
        <updated>2021-07-21T02:01:35.259Z</updated>
        <summary type="html"><![CDATA[Facial affect analysis (FAA) using visual signals is important in
human-computer interaction. Early methods focus on extracting appearance and
geometry features associated with human affects, while ignoring the latent
semantic information among individual facial changes, leading to limited
performance and generalization. Recent work attempts to establish a graph-based
representation to model these semantic relationships and develop frameworks to
leverage them for various FAA tasks. In this paper, we provide a comprehensive
review of graph-based FAA, including the evolution of algorithms and their
applications. First, the FAA background knowledge is introduced, especially on
the role of the graph. We then discuss approaches that are widely used for
graph-based affective representation in literature and show a trend towards
graph construction. For the relational reasoning in graph-based FAA, existing
studies are categorized according to their usage of traditional methods or deep
models, with a special emphasis on the latest graph neural networks.
Performance comparisons of the state-of-the-art graph-based FAA methods are
also summarized. Finally, we discuss the challenges and potential directions.
As far as we know, this is the first survey of graph-based FAA methods. Our
findings can serve as a reference for future research in this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xingming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinzhao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yante Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guoying Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dim but not entirely dark: Extracting the Galactic Center Excess' source-count distribution with neural nets. (arXiv:2107.09070v1 [astro-ph.HE])]]></title>
        <id>http://arxiv.org/abs/2107.09070</id>
        <link href="http://arxiv.org/abs/2107.09070"/>
        <updated>2021-07-21T02:01:35.252Z</updated>
        <summary type="html"><![CDATA[The two leading hypotheses for the Galactic Center Excess (GCE) in the
$\textit{Fermi}$ data are an unresolved population of faint millisecond pulsars
(MSPs) and dark-matter (DM) annihilation. The dichotomy between these
explanations is typically reflected by modeling them as two separate emission
components. However, point-sources (PSs) such as MSPs become statistically
degenerate with smooth Poisson emission in the ultra-faint limit (formally
where each source is expected to contribute much less than one photon on
average), leading to an ambiguity that can render questions such as whether the
emission is PS-like or Poissonian in nature ill-defined. We present a
conceptually new approach that describes the PS and Poisson emission in a
unified manner and only afterwards derives constraints on the Poissonian
component from the so obtained results. For the implementation of this
approach, we leverage deep learning techniques, centered around a neural
network-based method for histogram regression that expresses uncertainties in
terms of quantiles. We demonstrate that our method is robust against a number
of systematics that have plagued previous approaches, in particular DM / PS
misattribution. In the $\textit{Fermi}$ data, we find a faint GCE described by
a median source-count distribution (SCD) peaked at a flux of $\sim4 \times
10^{-11} \ \text{counts} \ \text{cm}^{-2} \ \text{s}^{-1}$ (corresponding to
$\sim3 - 4$ expected counts per PS), which would require $N \sim
\mathcal{O}(10^4)$ sources to explain the entire excess (median value $N =
\text{29,300}$ across the sky). Although faint, this SCD allows us to derive
the constraint $\eta_P \leq 66\%$ for the Poissonian fraction of the GCE flux
$\eta_P$ at 95% confidence, suggesting that a substantial amount of the GCE
flux is due to PSs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+List_F/0/1/0/all/0/1"&gt;Florian List&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Rodd_N/0/1/0/all/0/1"&gt;Nicholas L. Rodd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lewis_G/0/1/0/all/0/1"&gt;Geraint F. Lewis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08285</id>
        <link href="http://arxiv.org/abs/2106.08285"/>
        <updated>2021-07-21T02:01:35.229Z</updated>
        <summary type="html"><![CDATA[Time-lapse fluorescent microscopy (TLFM) combined with predictive
mathematical modelling is a powerful tool to study the inherently dynamic
processes of life on the single-cell level. Such experiments are costly,
complex and labour intensive. A complimentary approach and a step towards in
silico experimentation, is to synthesise the imagery itself. Here, we propose
Multi-StyleGAN as a descriptive approach to simulate time-lapse fluorescence
microscopy imagery of living cells, based on a past experiment. This novel
generative adversarial network synthesises a multi-domain sequence of
consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live
yeast cells in microstructured environments and train on a dataset recorded in
our laboratory. The simulation captures underlying biophysical factors and time
dependencies, such as cell morphology, growth, physical interactions, as well
as the intensity of a fluorescent reporter protein. An immediate application is
to generate additional training and validation data for feature extraction
algorithms or to aid and expedite development of advanced experimental
techniques such as online monitoring or control of cells.

Code and dataset is available at
https://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1"&gt;Christoph Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1"&gt;Tim Prangemeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1"&gt;Christian Wildner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1"&gt;Heinz Koeppl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11858</id>
        <link href="http://arxiv.org/abs/2106.11858"/>
        <updated>2021-07-21T02:01:35.220Z</updated>
        <summary type="html"><![CDATA[Image segmentation is a common and challenging task in autonomous driving.
Availability of sufficient pixel-level annotations for the training data is a
hurdle. Active learning helps learning from small amounts of data by suggesting
the most promising samples for labeling. In this work, we propose a new
pool-based method for active learning, which proposes promising patches
extracted from full image, in each acquisition step. The problem is framed in
an exploration-exploitation framework by combining an embedding based on
Uniform Manifold Approximation to model representativeness with entropy as
uncertainty measure to model informativeness. We applied our proposed method to
the autonomous driving datasets CamVid and Cityscapes and performed a
quantitative comparison with state-of-the-art baselines. We find that our
active learning method achieves better performance compared to previous
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1"&gt;Deepthi Sreenivasaiah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otterbach_J/0/1/0/all/0/1"&gt;Johannes Otterbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1"&gt;Thomas Wollmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconstruction of the Density Power Spectrum from Quasar Spectra using Machine Learning. (arXiv:2107.09082v1 [astro-ph.CO])]]></title>
        <id>http://arxiv.org/abs/2107.09082</id>
        <link href="http://arxiv.org/abs/2107.09082"/>
        <updated>2021-07-21T02:01:35.213Z</updated>
        <summary type="html"><![CDATA[We describe a novel end-to-end approach using Machine Learning to reconstruct
the power spectrum of cosmological density perturbations at high redshift from
observed quasar spectra. State-of-the-art cosmological simulations of structure
formation are used to generate a large synthetic dataset of line-of-sight
absorption spectra paired with 1-dimensional fluid quantities along the same
line-of-sight, such as the total density of matter and the density of neutral
atomic hydrogen. With this dataset, we build a series of data-driven models to
predict the power spectrum of total matter density. We are able to produce
models which yield reconstruction to accuracy of about 1% for wavelengths $k
\leq 2 h Mpc^{-1}$, while the error increases at larger $k$. We show the size
of data sample required to reach a particular error rate, giving a sense of how
much data is necessary to reach a desired accuracy. This work provides a
foundation for developing methods to analyse very large upcoming datasets with
the next-generation observational facilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Veiga_M/0/1/0/all/0/1"&gt;Maria Han Veiga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xi Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Gnedin_O/0/1/0/all/0/1"&gt;Oleg Y. Gnedin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Gnedin_N/0/1/0/all/0/1"&gt;Nickolay Y. Gnedin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Huan_X/0/1/0/all/0/1"&gt;Xun Huan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[iGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02924</id>
        <link href="http://arxiv.org/abs/2012.02924"/>
        <updated>2021-07-21T02:01:35.206Z</updated>
        <summary type="html"><![CDATA[We present iGibson 1.0, a novel simulation environment to develop robotic
solutions for interactive tasks in large-scale realistic scenes. Our
environment contains 15 fully interactive home-sized scenes with 108 rooms
populated with rigid and articulated objects. The scenes are replicas of
real-world homes, with distribution and the layout of objects aligned to those
of the real world. iGibson 1.0 integrates several key features to facilitate
the study of interactive tasks: i) generation of high-quality virtual sensor
signals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain
randomization to change the materials of the objects (both visual and physical)
and/or their shapes, iii) integrated sampling-based motion planners to generate
collision-free trajectories for robot bases and arms, and iv) intuitive
human-iGibson interface that enables efficient collection of human
demonstrations. Through experiments, we show that the full interactivity of the
scenes enables agents to learn useful visual representations that accelerate
the training of downstream manipulation tasks. We also show that iGibson 1.0
features enable the generalization of navigation agents, and that the
human-iGibson interface and integrated motion planners facilitate efficient
imitation learning of human demonstrated (mobile) manipulation behaviors.
iGibson 1.0 is open-source, equipped with comprehensive examples and
documentation. For more information, visit our project website:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bokui Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengshu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Linxi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1"&gt;Claudia P&amp;#xe9;rez-D&amp;#x27;Arpino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1"&gt;Shyamal Buch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Sanjana Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1"&gt;Lyne P. Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1"&gt;Micael E. Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1"&gt;Kent Vainio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vessel-CAPTCHA: an efficient learning framework for vessel annotation and segmentation. (arXiv:2101.09321v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09321</id>
        <link href="http://arxiv.org/abs/2101.09321"/>
        <updated>2021-07-21T02:01:35.199Z</updated>
        <summary type="html"><![CDATA[Deep learning techniques for 3D brain vessel image segmentation have not been
as successful as in the segmentation of other organs and tissues. This can be
explained by two factors. First, deep learning techniques tend to show poor
performances at the segmentation of relatively small objects compared to the
size of the full image. Second, due to the complexity of vascular trees and the
small size of vessels, it is challenging to obtain the amount of annotated
training data typically needed by deep learning methods. To address these
problems, we propose a novel annotation-efficient deep learning vessel
segmentation framework. The framework avoids pixel-wise annotations, only
requiring weak patch-level labels to discriminate between vessel and non-vessel
2D patches in the training set, in a setup similar to the CAPTCHAs used to
differentiate humans from bots in web applications. The user-provided weak
annotations are used for two tasks: 1) to synthesize pixel-wise pseudo-labels
for vessels and background in each patch, which are used to train a
segmentation network, and 2) to train a classifier network. The classifier
network allows to generate additional weak patch labels, further reducing the
annotation burden, and it acts as a noise filter for poor quality images. We
use this framework for the segmentation of the cerebrovascular tree in
Time-of-Flight angiography (TOF) and Susceptibility-Weighted Images (SWI). The
results show that the framework achieves state-of-the-art accuracy, while
reducing the annotation time by ~77% w.r.t. learning-based segmentation methods
using pixel-wise labels for training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dang_V/0/1/0/all/0/1"&gt;Vien Ngoc Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galati_F/0/1/0/all/0/1"&gt;Francesco Galati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cortese_R/0/1/0/all/0/1"&gt;Rosa Cortese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giacomo_G/0/1/0/all/0/1"&gt;Giuseppe Di Giacomo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marconetto_V/0/1/0/all/0/1"&gt;Viola Marconetto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1"&gt;Prateek Mathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1"&gt;Marco Lorenzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prados_F/0/1/0/all/0/1"&gt;Ferran Prados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuluaga_M/0/1/0/all/0/1"&gt;Maria A. Zuluaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DPNNet-2.0 Part I: Finding hidden planets from simulated images of protoplanetary disk gaps. (arXiv:2107.09086v1 [astro-ph.EP])]]></title>
        <id>http://arxiv.org/abs/2107.09086</id>
        <link href="http://arxiv.org/abs/2107.09086"/>
        <updated>2021-07-21T02:01:35.180Z</updated>
        <summary type="html"><![CDATA[The observed sub-structures, like annular gaps, in dust emissions from
protoplanetary disk, are often interpreted as signatures of embedded planets.
Fitting a model of planetary gaps to these observed features using customized
simulations or empirical relations can reveal the characteristics of the hidden
planets. However, customized fitting is often impractical owing to the
increasing sample size and the complexity of disk-planet interaction. In this
paper we introduce the architecture of DPNNet-2.0, second in the series after
DPNNet \citep{aud20}, designed using a Convolutional Neural Network ( CNN, here
specifically ResNet50) for predicting exoplanet masses directly from simulated
images of protoplanetary disks hosting a single planet. DPNNet-2.0 additionally
consists of a multi-input framework that uses both a CNN and multi-layer
perceptron (a class of artificial neural network) for processing image and disk
parameters simultaneously. This enables DPNNet-2.0 to be trained using images
directly, with the added option of considering disk parameters (disk
viscosities, disk temperatures, disk surface density profiles, dust abundances,
and particle Stokes numbers) generated from disk-planet hydrodynamic
simulations as inputs. This work provides the required framework and is the
first step towards the use of computer vision (implementing CNN) to directly
extract mass of an exoplanet from planetary gaps observed in dust-surface
density maps by telescopes such as the Atacama Large (sub-)Millimeter Array.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Auddy_S/0/1/0/all/0/1"&gt;Sayantan Auddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Dey_R/0/1/0/all/0/1"&gt;Ramit Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lin_M/0/1/0/all/0/1"&gt;Min-Kai Lin&lt;/a&gt; (ASIAA, NCTS Physics Division), &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Hall_C/0/1/0/all/0/1"&gt;Cassandra Hall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active 3D Shape Reconstruction from Vision and Touch. (arXiv:2107.09584v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09584</id>
        <link href="http://arxiv.org/abs/2107.09584"/>
        <updated>2021-07-21T02:01:35.174Z</updated>
        <summary type="html"><![CDATA[Humans build 3D understandings of the world through active object
exploration, using jointly their senses of vision and touch. However, in 3D
shape reconstruction, most recent progress has relied on static datasets of
limited sensory data such as RGB images, depth maps or haptic readings, leaving
the active exploration of the shape largely unexplored. In active touch sensing
for 3D reconstruction, the goal is to actively select the tactile readings that
maximize the improvement in shape reconstruction accuracy. However, the
development of deep learning-based active touch models is largely limited by
the lack of frameworks for shape exploration. In this paper, we focus on this
problem and introduce a system composed of: 1) a haptic simulator leveraging
high spatial resolution vision-based tactile sensors for active touching of 3D
objects; 2) a mesh-based 3D shape reconstruction model that relies on tactile
or visuotactile signals; and 3) a set of data-driven solutions with either
tactile or visuotactile priors to guide the shape exploration. Our framework
enables the development of the first fully data-driven solutions to active
touch on top of learned models for object understanding. Our experiments show
the benefits of such solutions in the task of 3D shape understanding where our
models consistently outperform natural baselines. We provide our framework as a
tool to foster future research in this direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1"&gt;Edward J. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1"&gt;David Meger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1"&gt;Luis Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1"&gt;Roberto Calandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1"&gt;Jitendra Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romero_A/0/1/0/all/0/1"&gt;Adriana Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drozdzal_M/0/1/0/all/0/1"&gt;Michal Drozdzal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CheXbreak: Misclassification Identification for Deep Learning Models Interpreting Chest X-rays. (arXiv:2103.09957v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09957</id>
        <link href="http://arxiv.org/abs/2103.09957"/>
        <updated>2021-07-21T02:01:35.167Z</updated>
        <summary type="html"><![CDATA[A major obstacle to the integration of deep learning models for chest x-ray
interpretation into clinical settings is the lack of understanding of their
failure modes. In this work, we first investigate whether there are patient
subgroups that chest x-ray models are likely to misclassify. We find that
patient age and the radiographic finding of lung lesion, pneumothorax or
support devices are statistically relevant features for predicting
misclassification for some chest x-ray models. Second, we develop
misclassification predictors on chest x-ray models using their outputs and
clinical features. We find that our best performing misclassification
identifier achieves an AUROC close to 0.9 for most diseases. Third, employing
our misclassification identifiers, we develop a corrective algorithm to
selectively flip model predictions that have high likelihood of
misclassification at inference time. We observe F1 improvement on the
prediction of Consolidation (0.008 [95% CI 0.005, 0.010]) and Edema (0.003,
[95% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and
high-performing chest x-ray models, we are able to derive insights across model
architectures and offer a generalizable framework applicable to other medical
imaging tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Emma Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1"&gt;Andy Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1"&gt;Rayan Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1"&gt;Jin Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantification of Carbon Sequestration in Urban Forests. (arXiv:2106.00182v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00182</id>
        <link href="http://arxiv.org/abs/2106.00182"/>
        <updated>2021-07-21T02:01:35.159Z</updated>
        <summary type="html"><![CDATA[Vegetation, trees in particular, sequester carbon by absorbing carbon dioxide
from the atmosphere. However, the lack of efficient quantification methods of
carbon stored in trees renders it difficult to track the process. We present an
approach to estimate the carbon storage in trees based on fusing multi-spectral
aerial imagery and LiDAR data to identify tree coverage, geometric shape, and
tree species -- key attributes to carbon storage quantification. We demonstrate
that tree species information and their three-dimensional geometric shapes can
be estimated from aerial imagery in order to determine the tree's biomass.
Specifically, we estimate a total of $52,000$ tons of carbon sequestered in
trees for New York City's borough Manhattan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klein_L/0/1/0/all/0/1"&gt;Levente J. Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_C/0/1/0/all/0/1"&gt;Conrad M. Albrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Believe The HiPe: Hierarchical Perturbation for Fast, Robust and Model-Agnostic Explanations. (arXiv:2103.05108v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05108</id>
        <link href="http://arxiv.org/abs/2103.05108"/>
        <updated>2021-07-21T02:01:35.152Z</updated>
        <summary type="html"><![CDATA[Understanding the predictions made by Artificial Intelligence (AI) systems is
becoming more and more important as deep learning models are used for
increasingly complex and high-stakes tasks. Saliency mapping - an easily
interpretable visual attribution method - is one important tool for this, but
existing formulations are limited by either computational cost or architectural
constraints. We therefore propose Hierarchical Perturbation, a very fast and
completely model-agnostic method for explaining model predictions with robust
saliency maps. Using standard benchmarks and datasets, we show that our
saliency maps are of competitive or superior quality to those generated by
existing model-agnostic methods - and are over 20X faster to compute.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1"&gt;Jessica Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1"&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1"&gt;David J Harrison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Efficient Scene Understanding via Squeeze Reasoning. (arXiv:2011.03308v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03308</id>
        <link href="http://arxiv.org/abs/2011.03308"/>
        <updated>2021-07-21T02:01:35.132Z</updated>
        <summary type="html"><![CDATA[Graph-based convolutional model such as non-local block has shown to be
effective for strengthening the context modeling ability in convolutional
neural networks (CNNs). However, its pixel-wise computational overhead is
prohibitive which renders it unsuitable for high resolution imagery. In this
paper, we explore the efficiency of context graph reasoning and propose a novel
framework called Squeeze Reasoning. Instead of propagating information on the
spatial map, we first learn to squeeze the input feature into a channel-wise
global vector and perform reasoning within the single vector where the
computation cost can be significantly reduced. Specifically, we build the node
graph in the vector where each node represents an abstract semantic concept.
The refined feature within the same semantic category results to be consistent,
which is thus beneficial for downstream tasks. We show that our approach can be
modularized as an end-to-end trained block and can be easily plugged into
existing networks. {Despite its simplicity and being lightweight, the proposed
strategy allows us to establish the considerable results on different semantic
segmentation datasets and shows significant improvements with respect to strong
baselines on various other scene understanding tasks including object
detection, instance segmentation and panoptic segmentation.} Code is available
at \url{https://github.com/lxtGH/SFSegNets}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiangtai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_A/0/1/0/all/0/1"&gt;Ansheng You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guangliang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kuiyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yunhai Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stock price prediction using BERT and GAN. (arXiv:2107.09055v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2107.09055</id>
        <link href="http://arxiv.org/abs/2107.09055"/>
        <updated>2021-07-21T02:01:35.125Z</updated>
        <summary type="html"><![CDATA[The stock market has been a popular topic of interest in the recent past. The
growth in the inflation rate has compelled people to invest in the stock and
commodity markets and other areas rather than saving. Further, the ability of
Deep Learning models to make predictions on the time series data has been
proven time and again. Technical analysis on the stock market with the help of
technical indicators has been the most common practice among traders and
investors. One more aspect is the sentiment analysis - the emotion of the
investors that shows the willingness to invest. A variety of techniques have
been used by people around the globe involving basic Machine Learning and
Neural Networks. Ranging from the basic linear regression to the advanced
neural networks people have experimented with all possible techniques to
predict the stock market. It's evident from recent events how news and
headlines affect the stock markets and cryptocurrencies. This paper proposes an
ensemble of state-of-the-art methods for predicting stock prices. Firstly
sentiment analysis of the news and the headlines for the company Apple Inc,
listed on the NASDAQ is performed using a version of BERT, which is a
pre-trained transformer model by Google for Natural Language Processing (NLP).
Afterward, a Generative Adversarial Network (GAN) predicts the stock price for
Apple Inc using the technical indicators, stock indexes of various countries,
some commodities, and historical prices along with the sentiment scores.
Comparison is done with baseline models like - Long Short Term Memory (LSTM),
Gated Recurrent Units (GRU), vanilla GAN, and Auto-Regressive Integrated Moving
Average (ARIMA) model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Sonkiya_P/0/1/0/all/0/1"&gt;Priyank Sonkiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Bajpai_V/0/1/0/all/0/1"&gt;Vikas Bajpai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Bansal_A/0/1/0/all/0/1"&gt;Anukriti Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisQA: X-raying Vision and Language Reasoning in Transformers. (arXiv:2104.00926v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00926</id>
        <link href="http://arxiv.org/abs/2104.00926"/>
        <updated>2021-07-21T02:01:35.119Z</updated>
        <summary type="html"><![CDATA[Visual Question Answering systems target answering open-ended textual
questions given input images. They are a testbed for learning high-level
reasoning with a primary use in HCI, for instance assistance for the visually
impaired. Recent research has shown that state-of-the-art models tend to
produce answers exploiting biases and shortcuts in the training data, and
sometimes do not even look at the input image, instead of performing the
required reasoning steps. We present VisQA, a visual analytics tool that
explores this question of reasoning vs. bias exploitation. It exposes the key
element of state-of-the-art neural models -- attention maps in transformers.
Our working hypothesis is that reasoning steps leading to model predictions are
observable from attention distributions, which are particularly useful for
visualization. The design process of VisQA was motivated by well-known bias
examples from the fields of deep learning and vision-language reasoning and
evaluated in two ways. First, as a result of a collaboration of three fields,
machine learning, vision and language reasoning, and data analytics, the work
lead to a better understanding of bias exploitation of neural models for VQA,
which eventually resulted in an impact on its design and training through the
proposition of a method for the transfer of reasoning patterns from an oracle
model. Second, we also report on the design of VisQA, and a goal-oriented
evaluation of VisQA targeting the analysis of a model decision process from
multiple experts, providing evidence that it makes the inner workings of models
accessible to users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaunet_T/0/1/0/all/0/1"&gt;Theo Jaunet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kervadec_C/0/1/0/all/0/1"&gt;Corentin Kervadec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vuillemot_R/0/1/0/all/0/1"&gt;Romain Vuillemot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antipov_G/0/1/0/all/0/1"&gt;Grigory Antipov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baccouche_M/0/1/0/all/0/1"&gt;Moez Baccouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1"&gt;Christian Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Determining Sequence of Image Processing Technique (IPT) to Detect Adversarial Attacks. (arXiv:2007.00337v2 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2007.00337</id>
        <link href="http://arxiv.org/abs/2007.00337"/>
        <updated>2021-07-21T02:01:35.112Z</updated>
        <summary type="html"><![CDATA[Developing secure machine learning models from adversarial examples is
challenging as various methods are continually being developed to generate
adversarial attacks. In this work, we propose an evolutionary approach to
automatically determine Image Processing Techniques Sequence (IPTS) for
detecting malicious inputs. Accordingly, we first used a diverse set of attack
methods including adaptive attack methods (on our defense) to generate
adversarial samples from the clean dataset. A detection framework based on a
genetic algorithm (GA) is developed to find the optimal IPTS, where the
optimality is estimated by different fitness measures such as Euclidean
distance, entropy loss, average histogram, local binary pattern and loss
functions. The "image difference" between the original and processed images is
used to extract the features, which are then fed to a classification scheme in
order to determine whether the input sample is adversarial or clean. This paper
described our methodology and performed experiments using multiple data-sets
tested with several adversarial attacks. For each attack-type and dataset, it
generates unique IPTS. A set of IPTS selected dynamically in testing time which
works as a filter for the adversarial attack. Our empirical experiments
exhibited promising results indicating the approach can efficiently be used as
processing for any AI model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kishor Datta Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1"&gt;Zahid Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dasgupta_D/0/1/0/all/0/1"&gt;Dipankar Dasgupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Follow Your Path: a Progressive Method for Knowledge Distillation. (arXiv:2107.09305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09305</id>
        <link href="http://arxiv.org/abs/2107.09305"/>
        <updated>2021-07-21T02:01:35.104Z</updated>
        <summary type="html"><![CDATA[Deep neural networks often have a huge number of parameters, which posts
challenges in deployment in application scenarios with limited memory and
computation capacity. Knowledge distillation is one approach to derive compact
models from bigger ones. However, it has been observed that a converged heavy
teacher model is strongly constrained for learning a compact student network
and could make the optimization subject to poor local optima. In this paper, we
propose ProKT, a new model-agnostic method by projecting the supervision
signals of a teacher model into the student's parameter space. Such projection
is implemented by decomposing the training objective into local intermediate
targets with an approximate mirror descent technique. The proposed method could
be less sensitive with the quirks during optimization which could result in a
better local optimum. Experiments on both image and text datasets show that our
proposed ProKT consistently achieves superior performance compared to other
existing knowledge distillation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Wenxian Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yuxuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bohan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Low-rank plus Sparse Network for Dynamic MR Imaging. (arXiv:2010.13677v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13677</id>
        <link href="http://arxiv.org/abs/2010.13677"/>
        <updated>2021-07-21T02:01:35.085Z</updated>
        <summary type="html"><![CDATA[In dynamic magnetic resonance (MR) imaging, low-rank plus sparse (L+S)
decomposition, or robust principal component analysis (PCA), has achieved
stunning performance. However, the selection of the parameters of L+S is
empirical, and the acceleration rate is limited, which are common failings of
iterative compressed sensing MR imaging (CS-MRI) reconstruction methods. Many
deep learning approaches have been proposed to address these issues, but few of
them use a low-rank prior. In this paper, a model-based low-rank plus sparse
network, dubbed L+S-Net, is proposed for dynamic MR reconstruction. In
particular, we use an alternating linearized minimization method to solve the
optimization problem with low-rank and sparse regularization. Learned soft
singular value thresholding is introduced to ensure the clear separation of the
L component and S component. Then, the iterative steps are unrolled into a
network in which the regularization parameters are learnable. We prove that the
proposed L+S-Net achieves global convergence under two standard assumptions.
Experiments on retrospective and prospective cardiac cine datasets show that
the proposed model outperforms state-of-the-art CS and existing deep learning
methods and has great potential for extremely high acceleration factors (up to
24x).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wenqi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ke_Z/0/1/0/all/0/1"&gt;Ziwen Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhuo-Xu Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jing Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1"&gt;Zhilang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Sen Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ying_L/0/1/0/all/0/1"&gt;Leslie Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanjie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1"&gt;Dong Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing Generalization under Out-Of-Distribution Shifts in Deep Metric Learning. (arXiv:2107.09562v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09562</id>
        <link href="http://arxiv.org/abs/2107.09562"/>
        <updated>2021-07-21T02:01:35.078Z</updated>
        <summary type="html"><![CDATA[Deep Metric Learning (DML) aims to find representations suitable for
zero-shot transfer to a priori unknown test distributions. However, common
evaluation protocols only test a single, fixed data split in which train and
test classes are assigned randomly. More realistic evaluations should consider
a broad spectrum of distribution shifts with potentially varying degree and
difficulty. In this work, we systematically construct train-test splits of
increasing difficulty and present the ooDML benchmark to characterize
generalization under out-of-distribution shifts in DML. ooDML is designed to
probe the generalization performance on much more challenging, diverse
train-to-test distribution shifts. Based on our new benchmark, we conduct a
thorough empirical analysis of state-of-the-art DML methods. We find that while
generalization tends to consistently degrade with difficulty, some methods are
better at retaining performance as the distribution shift increases. Finally,
we propose few-shot DML as an efficient way to consistently improve
generalization in response to unknown test shifts presented in ooDML. Code
available here:
https://github.com/Confusezius/Characterizing_Generalization_in_DeepMetricLearning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1"&gt;Timo Milbich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1"&gt;Karsten Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1"&gt;Samarth Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;Marzyeh Ghassemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Visual Navigation. (arXiv:2107.01151v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01151</id>
        <link href="http://arxiv.org/abs/2107.01151"/>
        <updated>2021-07-21T02:01:35.071Z</updated>
        <summary type="html"><![CDATA[As a fundamental problem for Artificial Intelligence, multi-agent system
(MAS) is making rapid progress, mainly driven by multi-agent reinforcement
learning (MARL) techniques. However, previous MARL methods largely focused on
grid-world like or game environments; MAS in visually rich environments has
remained less explored. To narrow this gap and emphasize the crucial role of
perception in MAS, we propose a large-scale 3D dataset, CollaVN, for
multi-agent visual navigation (MAVN). In CollaVN, multiple agents are entailed
to cooperatively navigate across photo-realistic environments to reach target
locations. Diverse MAVN variants are explored to make our problem more general.
Moreover, a memory-augmented communication framework is proposed. Each agent is
equipped with a private, external memory to persistently store communication
information. This allows agents to make better use of their past communication
information, enabling more efficient collaboration and robust long-term
planning. In our experiments, several baselines and evaluation metrics are
designed. We also empirically verify the efficacy of our proposed MARL approach
across different MAVN task settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenguan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xizhou Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jifeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anchor Pruning for Object Detection. (arXiv:2104.00432v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00432</id>
        <link href="http://arxiv.org/abs/2104.00432"/>
        <updated>2021-07-21T02:01:35.063Z</updated>
        <summary type="html"><![CDATA[This paper proposes anchor pruning for object detection in one-stage
anchor-based detectors. While pruning techniques are widely used to reduce the
computational cost of convolutional neural networks, they tend to focus on
optimizing the backbone networks where often most computations are. In this
work we demonstrate an additional pruning technique, specifically for object
detection: anchor pruning. With more efficient backbone networks and a growing
trend of deploying object detectors on embedded systems where post-processing
steps such as non-maximum suppression can be a bottleneck, the impact of the
anchors used in the detection head is becoming increasingly more important. In
this work, we show that many anchors in the object detection head can be
removed without any loss in accuracy. With additional retraining, anchor
pruning can even lead to improved accuracy. Extensive experiments on SSD and MS
COCO show that the detection head can be made up to 44% more efficient while
simultaneously increasing accuracy. Further experiments on RetinaNet and PASCAL
VOC show the general effectiveness of our approach. We also introduce
`overanchorized' models that can be used together with anchor pruning to
eliminate hyperparameters related to the initial shape of anchors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bonnaerens_M/0/1/0/all/0/1"&gt;Maxim Bonnaerens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freiberger_M/0/1/0/all/0/1"&gt;Matthias Freiberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dambre_J/0/1/0/all/0/1"&gt;Joni Dambre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistent Two-Flow Network for Tele-Registration of Point Clouds. (arXiv:2106.00329v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00329</id>
        <link href="http://arxiv.org/abs/2106.00329"/>
        <updated>2021-07-21T02:01:35.054Z</updated>
        <summary type="html"><![CDATA[Rigid registration of partial observations is a fundamental problem in
various applied fields. In computer graphics, special attention has been given
to the registration between two partial point clouds generated by scanning
devices. State-of-the-art registration techniques still struggle when the
overlap region between the two point clouds is small, and completely fail if
there is no overlap between the scan pairs. In this paper, we present a
learning-based technique that alleviates this problem, and allows registration
between point clouds, presented in arbitrary poses, and having little or even
no overlap, a setting that has been referred to as tele-registration. Our
technique is based on a novel neural network design that learns a prior of a
class of shapes and can complete a partial shape. The key idea is combining the
registration and completion tasks in a way that reinforces each other. In
particular, we simultaneously train the registration network and completion
network using two coupled flows, one that register-and-complete, and one that
complete-and-register, and encourage the two flows to produce a consistent
result. We show that, compared with each separate flow, this two-flow training
leads to robust and reliable tele-registration, and hence to a better point
cloud prediction that completes the registered scans. It is also worth
mentioning that each of the components in our neural network outperforms
state-of-the-art methods in both completion and registration. We further
analyze our network with several ablation studies and demonstrate its
performance on a large number of partial point clouds, both synthetic and
real-world, that have only small or no overlap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zihao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1"&gt;Zimu Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Ruizhen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hui Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Cloud Distortion Quantification based on Potential Energy for Human and Machine Perception. (arXiv:2103.02850v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02850</id>
        <link href="http://arxiv.org/abs/2103.02850"/>
        <updated>2021-07-21T02:01:35.036Z</updated>
        <summary type="html"><![CDATA[Distortion quantification of point clouds plays a stealth, yet vital role in
a wide range of human and machine perception tasks. For human perception tasks,
a distortion quantification can substitute subjective experiments to guide 3D
visualization; while for machine perception tasks, a distortion quantification
can work as a loss function to guide the training of deep neural networks for
unsupervised learning tasks. To handle a variety of demands in many
applications, a distortion quantification needs to be distortion discriminable,
differentiable, and have a low computational complexity. Currently, however,
there is a lack of a general distortion quantification that can satisfy all
three conditions. To fill this gap, this work proposes multiscale potential
energy discrepancy (MPED), a distortion quantification to measure point cloud
geometry and color difference. By evaluating at various neighborhood sizes, the
proposed MPED achieves global-local tradeoffs, capturing distortion in a
multiscale fashion. Extensive experimental studies validate MPED's superiority
for both human and machine perception tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yiling Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1"&gt;M. Salman Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhan Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation. (arXiv:2107.09600v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09600</id>
        <link href="http://arxiv.org/abs/2107.09600"/>
        <updated>2021-07-21T02:01:35.029Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) for semantic segmentation aims to adapt
a segmentation model trained on the labeled source domain to the unlabeled
target domain. Existing methods try to learn domain invariant features while
suffering from large domain gaps that make it difficult to correctly align
discrepant features, especially in the initial training phase. To address this
issue, we propose a novel Dual Soft-Paste (DSP) method in this paper.
Specifically, DSP selects some classes from a source domain image using a
long-tail class first sampling strategy and softly pastes the corresponding
image patch on both the source and target training images with a fusion weight.
Technically, we adopt the mean teacher framework for domain adaptation, where
the pasted source and target images go through the student network while the
original target image goes through the teacher network. Output-level alignment
is carried out by aligning the probability maps of the target fused image from
both networks using a weighted cross-entropy loss. In addition, feature-level
alignment is carried out by aligning the feature maps of the source and target
images from student network using a weighted maximum mean discrepancy loss. DSP
facilitates the model learning domain-invariant features from the intermediate
domains, leading to faster convergence and better performance. Experiments on
two challenging benchmarks demonstrate the superiority of DSP over
state-of-the-art methods. Code is available at
\url{https://github.com/GaoLii/DSP}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Li Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lefei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Fusion Transformer. (arXiv:2107.09011v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09011</id>
        <link href="http://arxiv.org/abs/2107.09011"/>
        <updated>2021-07-21T02:01:35.018Z</updated>
        <summary type="html"><![CDATA[In image fusion, images obtained from different sensors are fused to generate
a single image with enhanced information. In recent years, state-of-the-art
methods have adopted Convolution Neural Networks (CNNs) to encode meaningful
features for image fusion. Specifically, CNN-based methods perform image fusion
by fusing local features. However, they do not consider long-range dependencies
that are present in the image. Transformer-based models are designed to
overcome this by modeling the long-range dependencies with the help of
self-attention mechanism. This motivates us to propose a novel Image Fusion
Transformer (IFT) where we develop a transformer-based multi-scale fusion
strategy that attends to both local and long-range information (or global
context). The proposed method follows a two-stage training approach. In the
first stage, we train an auto-encoder to extract deep features at multiple
scales. In the second stage, multi-scale features are fused using a
Spatio-Transformer (ST) fusion strategy. The ST fusion blocks are comprised of
a CNN and a transformer branch which capture local and long-range features,
respectively. Extensive experiments on multiple benchmark datasets show that
the proposed method performs better than many competitive fusion algorithms.
Furthermore, we show the effectiveness of the proposed ST fusion strategy with
an ablation analysis. The source code is available at:
https://github.com/Vibashan/Image-Fusion-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1"&gt;Vibashan VS&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1"&gt;Jeya Maria Jose Valanarasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1"&gt;Poojan Oza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Imaging with Deep Learning for COVID- 19 Diagnosis: A Comprehensive Review. (arXiv:2107.09602v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09602</id>
        <link href="http://arxiv.org/abs/2107.09602"/>
        <updated>2021-07-21T02:01:35.002Z</updated>
        <summary type="html"><![CDATA[The outbreak of novel coronavirus disease (COVID- 19) has claimed millions of
lives and has affected all aspects of human life. This paper focuses on the
application of deep learning (DL) models to medical imaging and drug discovery
for managing COVID-19 disease. In this article, we detail various medical
imaging-based studies such as X-rays and computed tomography (CT) images along
with DL methods for classifying COVID-19 affected versus pneumonia. The
applications of DL techniques to medical images are further described in terms
of image localization, segmentation, registration, and classification leading
to COVID-19 detection. The reviews of recent papers indicate that the highest
classification accuracy of 99.80% is obtained when InstaCovNet-19 DL method is
applied to an X-ray dataset of 361 COVID-19 patients, 362 pneumonia patients
and 365 normal people. Furthermore, it can be seen that the best classification
accuracy of 99.054% can be achieved when EDL_COVID DL method is applied to a CT
image dataset of 7500 samples where COVID-19 patients, lung tumor patients and
normal people are equal in number. Moreover, we illustrate the potential DL
techniques in drug or vaccine discovery in combating the coronavirus. Finally,
we address a number of problems, concerns and future research directions
relevant to DL applications for COVID-19.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bharati_S/0/1/0/all/0/1"&gt;Subrato Bharati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Podder_P/0/1/0/all/0/1"&gt;Prajoy Podder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mondal_M/0/1/0/all/0/1"&gt;M. Rubaiyat Hossain Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prasath_V/0/1/0/all/0/1"&gt;V.B. Surya Prasath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Invariant Representation with Consistency and Diversity for Semi-supervised Source Hypothesis Transfer. (arXiv:2107.03008v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03008</id>
        <link href="http://arxiv.org/abs/2107.03008"/>
        <updated>2021-07-21T02:01:34.994Z</updated>
        <summary type="html"><![CDATA[Semi-supervised domain adaptation (SSDA) aims to solve tasks in target domain
by utilizing transferable information learned from the available source domain
and a few labeled target data. However, source data is not always accessible in
practical scenarios, which restricts the application of SSDA in real world
circumstances. In this paper, we propose a novel task named Semi-supervised
Source Hypothesis Transfer (SSHT), which performs domain adaptation based on
source trained model, to generalize well in target domain with a few
supervisions. In SSHT, we are facing two challenges: (1) The insufficient
labeled target data may result in target features near the decision boundary,
with the increased risk of mis-classification; (2) The data are usually
imbalanced in source domain, so the model trained with these data is biased.
The biased model is prone to categorize samples of minority categories into
majority ones, resulting in low prediction diversity. To tackle the above
issues, we propose Consistency and Diversity Learning (CDL), a simple but
effective framework for SSHT by facilitating prediction consistency between two
randomly augmented unlabeled data and maintaining the prediction diversity when
adapting model to target domain. Encouraging consistency regularization brings
difficulty to memorize the few labeled target data and thus enhances the
generalization ability of the learned model. We further integrate Batch
Nuclear-norm Maximization into our method to enhance the discriminability and
diversity. Experimental results show that our method outperforms existing SSDA
methods and unsupervised model adaptation methods on DomainNet, Office-Home and
Office-31 datasets. The code is available at
https://github.com/Wang-xd1899/SSHT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaodong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1"&gt;Junbao Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuhao Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Co-Heterogeneous and Adaptive Segmentation from Multi-Source and Multi-Phase CT Imaging Data: A Study on Pathological Liver and Lesion Segmentation. (arXiv:2005.13201v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.13201</id>
        <link href="http://arxiv.org/abs/2005.13201"/>
        <updated>2021-07-21T02:01:34.976Z</updated>
        <summary type="html"><![CDATA[In medical imaging, organ/pathology segmentation models trained on current
publicly available and fully-annotated datasets usually do not well-represent
the heterogeneous modalities, phases, pathologies, and clinical scenarios
encountered in real environments. On the other hand, there are tremendous
amounts of unlabelled patient imaging scans stored by many modern clinical
centers. In this work, we present a novel segmentation strategy,
co-heterogenous and adaptive segmentation (CHASe), which only requires a small
labeled cohort of single phase imaging data to adapt to any unlabeled cohort of
heterogenous multi-phase data with possibly new clinical scenarios and
pathologies. To do this, we propose a versatile framework that fuses appearance
based semi-supervision, mask based adversarial domain adaptation, and
pseudo-labeling. We also introduce co-heterogeneous training, which is a novel
integration of co-training and hetero modality learning. We have evaluated
CHASe using a clinically comprehensive and challenging dataset of multi-phase
computed tomography (CT) imaging studies (1147 patients and 4577 3D volumes).
Compared to previous state-of-the-art baselines, CHASe can further improve
pathological liver mask Dice-Sorensen coefficients by ranges of $4.2\% \sim
9.4\%$, depending on the phase combinations: e.g., from $84.6\%$ to $94.0\%$ on
non-contrast CTs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Raju_A/0/1/0/all/0/1"&gt;Ashwin Raju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Chi-Tung Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yunakai Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jinzheng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jing Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1"&gt;Le Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liao_C/0/1/0/all/0/1"&gt;ChienHuang Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Harrison_A/0/1/0/all/0/1"&gt;Adam P Harrison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReSSL: Relational Self-Supervised Learning with Weak Augmentation. (arXiv:2107.09282v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09282</id>
        <link href="http://arxiv.org/abs/2107.09282"/>
        <updated>2021-07-21T02:01:34.957Z</updated>
        <summary type="html"><![CDATA[Self-supervised Learning (SSL) including the mainstream contrastive learning
has achieved great success in learning visual representations without data
annotations. However, most of methods mainly focus on the instance level
information (\ie, the different augmented images of the same instance should
have the same feature or cluster into the same class), but there is a lack of
attention on the relationships between different instances. In this paper, we
introduced a novel SSL paradigm, which we term as relational self-supervised
learning (ReSSL) framework that learns representations by modeling the
relationship between different instances. Specifically, our proposed method
employs sharpened distribution of pairwise similarities among different
instances as \textit{relation} metric, which is thus utilized to match the
feature embeddings of different augmentations. Moreover, to boost the
performance, we argue that weak augmentations matter to represent a more
reliable relation, and leverage momentum strategy for practical efficiency.
Experimental results show that our proposed ReSSL significantly outperforms the
previous state-of-the-art algorithms in terms of both performance and training
efficiency. Code is available at \url{https://github.com/KyleZheng1997/ReSSL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine Pseudo-Labeling with Visual-Semantic Meta-Embedding. (arXiv:2007.05675v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05675</id>
        <link href="http://arxiv.org/abs/2007.05675"/>
        <updated>2021-07-21T02:01:34.947Z</updated>
        <summary type="html"><![CDATA[Few-shot learning aims at rapidly adapting to novel categories with only a
handful of samples at test time, which has been predominantly tackled with the
idea of meta-learning. However, meta-learning approaches essentially learn
across a variety of few-shot tasks and thus still require large-scale training
data with fine-grained supervision to derive a generalized model, thereby
involving prohibitive annotation cost. In this paper, we advance the few-shot
classification paradigm towards a more challenging scenario, i.e.,
cross-granularity few-shot classification, where the model observes only coarse
labels during training while is expected to perform fine-grained classification
during testing. This task largely relieves the annotation cost since
fine-grained labeling usually requires strong domain-specific expertise. To
bridge the cross-granularity gap, we approximate the fine-grained data
distribution by greedy clustering of each coarse-class into pseudo-fine-classes
according to the similarity of image embeddings. We then propose a
meta-embedder that jointly optimizes the visual- and semantic-discrimination,
in both instance-wise and coarse class-wise, to obtain a good feature space for
this coarse-to-fine pseudo-labeling process. Extensive experiments and ablation
studies are conducted to demonstrate the effectiveness and robustness of our
approach on three representative datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinhai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hua Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Saliency for free: Saliency prediction as a side-effect of object recognition. (arXiv:2107.09628v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09628</id>
        <link href="http://arxiv.org/abs/2107.09628"/>
        <updated>2021-07-21T02:01:34.925Z</updated>
        <summary type="html"><![CDATA[Saliency is the perceptual capacity of our visual system to focus our
attention (i.e. gaze) on relevant objects. Neural networks for saliency
estimation require ground truth saliency maps for training which are usually
achieved via eyetracking experiments. In the current paper, we demonstrate that
saliency maps can be generated as a side-effect of training an object
recognition deep neural network that is endowed with a saliency branch. Such a
network does not require any ground-truth saliency maps for training.Extensive
experiments carried out on both real and synthetic saliency datasets
demonstrate that our approach is able to generate accurate saliency maps,
achieving competitive results on both synthetic and real datasets when compared
to methods that do require ground truth data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Figueroa_Flores_C/0/1/0/all/0/1"&gt;Carola Figueroa-Flores&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berga_D/0/1/0/all/0/1"&gt;David Berga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1"&gt;Joost van der Weijer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1"&gt;Bogdan Raducanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Split, embed and merge: An accurate table structure recognizer. (arXiv:2107.05214v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05214</id>
        <link href="http://arxiv.org/abs/2107.05214"/>
        <updated>2021-07-21T02:01:34.907Z</updated>
        <summary type="html"><![CDATA[Table structure recognition is an essential part for making machines
understand tables. Its main task is to recognize the internal structure of a
table. However, due to the complexity and diversity in their structure and
style, it is very difficult to parse the tabular data into the structured
format which machines can understand easily, especially for complex tables. In
this paper, we introduce Split, Embed and Merge (SEM), an accurate table
structure recognizer. Our model takes table images as input and can correctly
recognize the structure of tables, whether they are simple or a complex tables.
SEM is mainly composed of three parts, splitter, embedder and merger. In the
first stage, we apply the splitter to predict the potential regions of the
table row (column) separators, and obtain the fine grid structure of the table.
In the second stage, by taking a full consideration of the textual information
in the table, we fuse the output features for each table grid from both vision
and language modalities. Moreover, we achieve a higher precision in our
experiments through adding additional semantic features. Finally, we process
the merging of these basic table grids in a self-regression manner. The
correspondent merging results is learned through the attention mechanism. In
our experiments, SEM achieves an average F1-Measure of 97.11% on the SciTSR
dataset which outperforms other methods by a large margin. We also won the
first place in the complex table and third place in all tables in ICDAR 2021
Competition on Scientific Literature Parsing, Task-B. Extensive experiments on
other publicly available datasets demonstrate that our model achieves
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhenrong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianshu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1"&gt;Jun Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARID: A Comprehensive Study on Recognizing Actions in the Dark and A New Benchmark Dataset. (arXiv:2006.03876v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03876</id>
        <link href="http://arxiv.org/abs/2006.03876"/>
        <updated>2021-07-21T02:01:34.849Z</updated>
        <summary type="html"><![CDATA[The task of action recognition in dark videos is useful in various scenarios,
e.g., night surveillance and self-driving at night. Though progress has been
made in the action recognition task for videos in normal illumination, few have
studied action recognition in the dark. This is partly due to the lack of
sufficient datasets for such a task. In this paper, we explored the task of
action recognition in dark videos. We bridge the gap of the lack of data for
this task by collecting a new dataset: the Action Recognition in the Dark
(ARID) dataset. It consists of over 3,780 video clips with 11 action
categories. To the best of our knowledge, it is the first dataset focused on
human actions in dark videos. To gain further understandings of our ARID
dataset, we analyze the ARID dataset in detail and exhibited its necessity over
synthetic dark videos. Additionally, we benchmarked the performance of several
current action recognition models on our dataset and explored potential methods
for increasing their performances. Our results show that current action
recognition models and frame enhancement methods may not be effective solutions
for the task of action recognition in dark videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yuecong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1"&gt;Haozhi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1"&gt;Kezhi Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianxiong Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1"&gt;Simon See&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA['CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06212</id>
        <link href="http://arxiv.org/abs/2107.06212"/>
        <updated>2021-07-21T02:01:34.842Z</updated>
        <summary type="html"><![CDATA[Ongoing advancements in the fields of 3D modelling and digital archiving have
led to an outburst in the amount of data stored digitally. Consequently,
several retrieval systems have been developed depending on the type of data
stored in these databases. However, unlike text data or images, performing a
search for 3D models is non-trivial. Among 3D models, retrieving 3D
Engineering/CAD models or mechanical components is even more challenging due to
the presence of holes, volumetric features, presence of sharp edges etc., which
make CAD a domain unto itself. The research work presented in this paper aims
at developing a dataset suitable for building a retrieval system for 3D CAD
models based on deep learning. 3D CAD models from the available CAD databases
are collected, and a dataset of computer-generated sketch data, termed
'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the
components are also added to CADSketchNet. Using the sketch images from this
dataset, the paper also aims at evaluating the performance of various retrieval
system or a search engine for 3D CAD models that accepts a sketch image as the
input query. Many experimental models are constructed and tested on
CADSketchNet. These experiments, along with the model architecture, choice of
similarity metrics are reported along with the search results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1"&gt;Bharadwaj Manda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1"&gt;Shubham Dhayarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1"&gt;Sai Mitheran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1"&gt;V.K. Viekash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1"&gt;Ramanathan Muthuganapathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Segmentation and Volume Measurement of Intracranial Carotid Artery Calcification on Non-Contrast CT. (arXiv:2107.09442v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09442</id>
        <link href="http://arxiv.org/abs/2107.09442"/>
        <updated>2021-07-21T02:01:34.809Z</updated>
        <summary type="html"><![CDATA[Purpose: To evaluate a fully-automated deep-learning-based method for
assessment of intracranial carotid artery calcification (ICAC). Methods: Two
observers manually delineated ICAC in non-contrast CT scans of 2,319
participants (mean age 69 (SD 7) years; 1154 women) of the Rotterdam Study,
prospectively collected between 2003 and 2006. These data were used to
retrospectively develop and validate a deep-learning-based method for automated
ICAC delineation and volume measurement. To evaluate the method, we compared
manual and automatic assessment (computed using ten-fold cross-validation) with
respect to 1) the agreement with an independent observer's assessment
(available in a random subset of 47 scans); 2) the accuracy in delineating ICAC
as judged via blinded visual comparison by an expert; 3) the association with
first stroke incidence from the scan date until 2012. All method performance
metrics were computed using 10-fold cross-validation. Results: The automated
delineation of ICAC reached sensitivity of 83.8% and positive predictive value
(PPV) of 88%. The intraclass correlation between automatic and manual ICAC
volume measures was 0.98 (95% CI: 0.97, 0.98; computed in the entire dataset).
Measured between the assessments of independent observers, sensitivity was
73.9%, PPV was 89.5%, and intraclass correlation was 0.91 (95% CI: 0.84, 0.95;
computed in the 47-scan subset). In the blinded visual comparisons, automatic
delineations were more accurate than manual ones (p-value = 0.01). The
association of ICAC volume with incident stroke was similarly strong for both
automated (hazard ratio, 1.38 (95% CI: 1.12, 1.75) and manually measured
volumes (hazard ratio, 1.48 (95% CI: 1.20, 1.87)). Conclusions: The developed
model was capable of automated segmentation and volume quantification of ICAC
with accuracy comparable to human experts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bortsova_G/0/1/0/all/0/1"&gt;Gerda Bortsova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bos_D/0/1/0/all/0/1"&gt;Daniel Bos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dubost_F/0/1/0/all/0/1"&gt;Florian Dubost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vernooij_M/0/1/0/all/0/1"&gt;Meike W. Vernooij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ikram_M/0/1/0/all/0/1"&gt;M. Kamran Ikram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tulder_G/0/1/0/all/0/1"&gt;Gijs van Tulder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bruijne_M/0/1/0/all/0/1"&gt;Marleen de Bruijne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cell Detection from Imperfect Annotation by Pseudo Label Selection Using P-classification. (arXiv:2107.09289v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09289</id>
        <link href="http://arxiv.org/abs/2107.09289"/>
        <updated>2021-07-21T02:01:34.802Z</updated>
        <summary type="html"><![CDATA[Cell detection is an essential task in cell image analysis. Recent deep
learning-based detection methods have achieved very promising results. In
general, these methods require exhaustively annotating the cells in an entire
image. If some of the cells are not annotated (imperfect annotation), the
detection performance significantly degrades due to noisy labels. This often
occurs in real collaborations with biologists and even in public data-sets. Our
proposed method takes a pseudo labeling approach for cell detection from
imperfect annotated data. A detection convolutional neural network (CNN)
trained using such missing labeled data often produces over-detection. We treat
partially labeled cells as positive samples and the detected positions except
for the labeled cell as unlabeled samples. Then we select reliable pseudo
labels from unlabeled data using recent machine learning techniques;
positive-and-unlabeled (PU) learning and P-classification. Experiments using
microscopy images for five different conditions demonstrate the effectiveness
of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1"&gt;Kazuma Fujii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daiki_S/0/1/0/all/0/1"&gt;Suehiro Daiki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazuya_N/0/1/0/all/0/1"&gt;Nishimura Kazuya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryoma_B/0/1/0/all/0/1"&gt;Bise Ryoma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Examining the Human Perceptibility of Black-Box Adversarial Attacks on Face Recognition. (arXiv:2107.09126v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09126</id>
        <link href="http://arxiv.org/abs/2107.09126"/>
        <updated>2021-07-21T02:01:34.606Z</updated>
        <summary type="html"><![CDATA[The modern open internet contains billions of public images of human faces
across the web, especially on social media websites used by half the world's
population. In this context, Face Recognition (FR) systems have the potential
to match faces to specific names and identities, creating glaring privacy
concerns. Adversarial attacks are a promising way to grant users privacy from
FR systems by disrupting their capability to recognize faces. Yet, such attacks
can be perceptible to human observers, especially under the more challenging
black-box threat model. In the literature, the justification for the
imperceptibility of such attacks hinges on bounding metrics such as $\ell_p$
norms. However, there is not much research on how these norms match up with
human perception. Through examining and measuring both the effectiveness of
recent black-box attacks in the face recognition setting and their
corresponding human perceptibility through survey data, we demonstrate the
trade-offs in perceptibility that occur as attacks become more aggressive. We
also show how the $\ell_2$ norm and other metrics do not correlate with human
perceptibility in a linear fashion, thus making these norms suboptimal at
measuring adversarial attack perceptibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Spetter_Goldstein_B/0/1/0/all/0/1"&gt;Benjamin Spetter-Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1"&gt;Nataniel Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1"&gt;Sarah Adel Bargal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminator-Free Generative Adversarial Attack. (arXiv:2107.09225v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09225</id>
        <link href="http://arxiv.org/abs/2107.09225"/>
        <updated>2021-07-21T02:01:34.599Z</updated>
        <summary type="html"><![CDATA[The Deep Neural Networks are vulnerable toadversarial exam-ples(Figure 1),
making the DNNs-based systems collapsed byadding the inconspicuous
perturbations to the images. Most of the existing works for adversarial attack
are gradient-based and suf-fer from the latency efficiencies and the load on
GPU memory. Thegenerative-based adversarial attacks can get rid of this
limitation,and some relative works propose the approaches based on GAN.However,
suffering from the difficulty of the convergence of train-ing a GAN, the
adversarial examples have either bad attack abilityor bad visual quality. In
this work, we find that the discriminatorcould be not necessary for
generative-based adversarial attack, andpropose theSymmetric Saliency-based
Auto-Encoder (SSAE)to generate the perturbations, which is composed of the
saliencymap module and the angle-norm disentanglement of the featuresmodule.
The advantage of our proposed method lies in that it is notdepending on
discriminator, and uses the generative saliency map to pay more attention to
label-relevant regions. The extensive exper-iments among the various tasks,
datasets, and models demonstratethat the adversarial examples generated by SSAE
not only make thewidely-used models collapse, but also achieves good visual
quality.The code is available at https://github.com/BravoLu/SSAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shaohao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1"&gt;Yuqiao Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1"&gt;Ke Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xing Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaowei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wei-Shi Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos. (arXiv:2107.09262v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09262</id>
        <link href="http://arxiv.org/abs/2107.09262"/>
        <updated>2021-07-21T02:01:34.592Z</updated>
        <summary type="html"><![CDATA[Deep learning based visual to sound generation systems essentially need to be
developed particularly considering the synchronicity aspects of visual and
audio features with time. In this research we introduce a novel task of guiding
a class conditioned generative adversarial network with the temporal visual
information of a video input for visual to sound generation task adapting the
synchronicity traits between audio-visual modalities. Our proposed FoleyGAN
model is capable of conditioning action sequences of visual events leading
towards generating visually aligned realistic sound tracks. We expand our
previously proposed Automatic Foley dataset to train with FoleyGAN and evaluate
our synthesized sound through human survey that shows noteworthy (on average
81\%) audio-visual synchronicity performance. Our approach also outperforms in
statistical experiments compared with other baseline models and audio-visual
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1"&gt;Sanchita Ghose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prevost_J/0/1/0/all/0/1"&gt;John J. Prevost&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image-Hashing-Based Anomaly Detection for Privacy-Preserving Online Proctoring. (arXiv:2107.09373v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.09373</id>
        <link href="http://arxiv.org/abs/2107.09373"/>
        <updated>2021-07-21T02:01:34.586Z</updated>
        <summary type="html"><![CDATA[Online proctoring has become a necessity in online teaching. Video-based
crowd-sourced online proctoring solutions are being used, where an exam-taking
student's video is monitored by third parties, leading to privacy concerns. In
this paper, we propose a privacy-preserving online proctoring system. The
proposed image-hashing-based system can detect the student's excessive face and
body movement (i.e., anomalies) that is resulted when the student tries to
cheat in the exam. The detection can be done even if the student's face is
blurred or masked in video frames. Experiment with an in-house dataset shows
the usability of the proposed system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yaqub_W/0/1/0/all/0/1"&gt;Waheeb Yaqub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohanty_M/0/1/0/all/0/1"&gt;Manoranjan Mohanty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suleiman_B/0/1/0/all/0/1"&gt;Basem Suleiman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Domain Adaptation for Diabetic Retinopathy Grading using Vessel Image Reconstruction. (arXiv:2107.09372v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09372</id>
        <link href="http://arxiv.org/abs/2107.09372"/>
        <updated>2021-07-21T02:01:34.567Z</updated>
        <summary type="html"><![CDATA[This paper investigates the problem of domain adaptation for diabetic
retinopathy (DR) grading. We learn invariant target-domain features by defining
a novel self-supervised task based on retinal vessel image reconstructions,
inspired by medical domain knowledge. Then, a benchmark of current
state-of-the-art unsupervised domain adaptation methods on the DR problem is
provided. It can be shown that our approach outperforms existing domain
adaption strategies. Furthermore, when utilizing entire training data in the
target domain, we are able to compete with several state-of-the-art approaches
in final classification accuracy just by applying standard network
architectures and using image-level labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Duy M. H. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1"&gt;Truong T. N. Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Than_N/0/1/0/all/0/1"&gt;Ngoc T. T. Than&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prange_A/0/1/0/all/0/1"&gt;Alexander Prange&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1"&gt;Daniel Sonntag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facial Landmarks Localization using Cascaded Neural Networks. (arXiv:1805.01760v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1805.01760</id>
        <link href="http://arxiv.org/abs/1805.01760"/>
        <updated>2021-07-21T02:01:34.559Z</updated>
        <summary type="html"><![CDATA[The accurate localization of facial landmarks is at the core of face analysis
tasks, such as face recognition and facial expression analysis, to name a few.
In this work, we propose a novel localization approach based on a deep learning
architecture that utilizes cascaded subnetworks with convolutional neural
network units. The cascaded units of the first subnetwork estimate
heatmap-based encodings of the landmarks locations, while the cascaded units of
the second subnetwork receive as input the output of the corresponding heatmap
estimation units, and refine them through regression. The proposed scheme is
experimentally shown to compare favorably with contemporary state-of-the-art
schemes, especially when applied to images depicting challenging localization
conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahpod_S/0/1/0/all/0/1"&gt;Shahar Mahpod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1"&gt;Rig Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maiorana_E/0/1/0/all/0/1"&gt;Emanuele Maiorana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1"&gt;Yosi Keller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campisi_P/0/1/0/all/0/1"&gt;Patrizio Campisi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZstGAN: An Adversarial Approach for Unsupervised Zero-Shot Image-to-Image Translation. (arXiv:1906.00184v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.00184</id>
        <link href="http://arxiv.org/abs/1906.00184"/>
        <updated>2021-07-21T02:01:34.552Z</updated>
        <summary type="html"><![CDATA[Image-to-image translation models have shown remarkable ability on
transferring images among different domains. Most of existing work follows the
setting that the source domain and target domain keep the same at training and
inference phases, which cannot be generalized to the scenarios for translating
an image from an unseen domain to another unseen domain. In this work, we
propose the Unsupervised Zero-Shot Image-to-image Translation (UZSIT) problem,
which aims to learn a model that can translate samples from image domains that
are not observed during training. Accordingly, we propose a framework called
ZstGAN: By introducing an adversarial training scheme, ZstGAN learns to model
each domain with domain-specific feature distribution that is semantically
consistent on vision and attribute modalities. Then the domain-invariant
features are disentangled with an shared encoder for image generation. We carry
out extensive experiments on CUB and FLO datasets, and the results demonstrate
the effectiveness of proposed method on UZSIT task. Moreover, ZstGAN shows
significant accuracy improvements over state-of-the-art zero-shot learning
methods on CUB and FLO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jianxin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yingce Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuqin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhibo Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Gender and Racial Disparities in Image Recognition Models. (arXiv:2107.09211v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09211</id>
        <link href="http://arxiv.org/abs/2107.09211"/>
        <updated>2021-07-21T02:01:34.545Z</updated>
        <summary type="html"><![CDATA[Large scale image classification models trained on top of popular datasets
such as Imagenet have shown to have a distributional skew which leads to
disparities in prediction accuracies across different subsections of population
demographics. A lot of approaches have been made to solve for this
distributional skew using methods that alter the model pre, post and during
training. We investigate one such approach - which uses a multi-label softmax
loss with cross-entropy as the loss function instead of a binary cross-entropy
on a multi-label classification problem on the Inclusive Images dataset which
is a subset of the OpenImages V6 dataset. We use the MR2 dataset, which
contains images of people with self-identified gender and race attributes to
evaluate the fairness in the model outcomes and try to interpret the mistakes
by looking at model activations and suggest possible fixes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahadev_R/0/1/0/all/0/1"&gt;Rohan Mahadev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarti_A/0/1/0/all/0/1"&gt;Anindya Chakravarti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monocular Visual Analysis for Electronic Line Calling of Tennis Games. (arXiv:2107.09255v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09255</id>
        <link href="http://arxiv.org/abs/2107.09255"/>
        <updated>2021-07-21T02:01:34.539Z</updated>
        <summary type="html"><![CDATA[Electronic Line Calling is an auxiliary referee system used for tennis
matches based on binocular vision technology. While ELC has been widely used,
there are still many problems, such as complex installation and maintenance,
high cost and etc. We propose a monocular vision technology based ELC method.
The method has the following steps. First, locate the tennis ball's trajectory.
We propose a multistage tennis ball positioning approach combining background
subtraction and color area filtering. Then we propose a bouncing point
prediction method by minimizing the fitting loss of the uncertain point.
Finally, we find out whether the bouncing point of the ball is out of bounds or
not according to the relative position between the bouncing point and the court
side line in the two dimensional image. We collected and tagged 394 samples
with an accuracy rate of 99.4%, and 81.8% of the 11 samples with bouncing
points.The experimental results show that our method is feasible to judge if a
ball is out of the court with monocular vision and significantly reduce complex
installation and costs of ELC system with binocular vision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuanzhou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shaobo Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynthTIGER: Synthetic Text Image GEneratoR Towards Better Text Recognition Models. (arXiv:2107.09313v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09313</id>
        <link href="http://arxiv.org/abs/2107.09313"/>
        <updated>2021-07-21T02:01:34.521Z</updated>
        <summary type="html"><![CDATA[For successful scene text recognition (STR) models, synthetic text image
generators have alleviated the lack of annotated text images from the real
world. Specifically, they generate multiple text images with diverse
backgrounds, font styles, and text shapes and enable STR models to learn visual
patterns that might not be accessible from manually annotated data. In this
paper, we introduce a new synthetic text image generator, SynthTIGER, by
analyzing techniques used for text image synthesis and integrating effective
ones under a single algorithm. Moreover, we propose two techniques that
alleviate the long-tail problem in length and character distributions of
training data. In our experiments, SynthTIGER achieves better STR performance
than the combination of synthetic datasets, MJSynth (MJ) and SynthText (ST).
Our ablation study demonstrates the benefits of using sub-components of
SynthTIGER and the guideline on generating synthetic text images for STR
models. Our implementation is publicly available at
https://github.com/clovaai/synthtiger.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yim_M/0/1/0/all/0/1"&gt;Moonbin Yim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yoonsik Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Han-Cheol Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sungrae Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Test-Agnostic Long-Tailed Recognition by Test-Time Aggregating Diverse Experts with Self-Supervision. (arXiv:2107.09249v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09249</id>
        <link href="http://arxiv.org/abs/2107.09249"/>
        <updated>2021-07-21T02:01:34.514Z</updated>
        <summary type="html"><![CDATA[Existing long-tailed recognition methods, aiming to train class-balance
models from long-tailed data, generally assume the models would be evaluated on
the uniform test class distribution. However, the practical test class
distribution often violates such an assumption (e.g., being long-tailed or even
inversely long-tailed), which would lead existing methods to fail in real-world
applications. In this work, we study a more practical task setting, called
test-agnostic long-tailed recognition, where the training class distribution is
long-tailed while the test class distribution is unknown and can be skewed
arbitrarily. In addition to the issue of class imbalance, this task poses
another challenge: the class distribution shift between the training and test
samples is unidentified. To address this task, we propose a new method, called
Test-time Aggregating Diverse Experts (TADE), that presents two solution
strategies: (1) a novel skill-diverse expert learning strategy that trains
diverse experts to excel at handling different test distributions from a single
long-tailed training distribution; (2) a novel test-time expert aggregation
strategy that leverages self-supervision to aggregate multiple experts for
handling various test distributions. Moreover, we theoretically show that our
method has provable ability to simulate unknown test class distributions.
Promising results on both vanilla and test-agnostic long-tailed recognition
verify the effectiveness of TADE. Code is available at
https://github.com/Vanint/TADE-AgnosticLT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yifan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1"&gt;Bryan Hooi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lanqing Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepSMILE: Self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&E whole-slide images. (arXiv:2107.09405v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09405</id>
        <link href="http://arxiv.org/abs/2107.09405"/>
        <updated>2021-07-21T02:01:34.507Z</updated>
        <summary type="html"><![CDATA[We propose a Deep learning-based weak label learning method for analysing
whole slide images (WSIs) of Hematoxylin and Eosin (H&E) stained tumorcells not
requiring pixel-level or tile-level annotations using Self-supervised
pre-training and heterogeneity-aware deep Multiple Instance LEarning
(DeepSMILE). We apply DeepSMILE to the task of Homologous recombination
deficiency (HRD) and microsatellite instability (MSI) prediction. We utilize
contrastive self-supervised learning to pre-train a feature extractor on
histopathology tiles of cancer tissue. Additionally, we use variability-aware
deep multiple instance learning to learn the tile feature aggregation function
while modeling tumor heterogeneity. Compared to state-of-the-art genomic label
classification methods, DeepSMILE improves classification performance for HRD
from $70.43\pm4.10\%$ to $83.79\pm1.25\%$ AUC and MSI from $78.56\pm6.24\%$ to
$90.32\pm3.58\%$ AUC in a multi-center breast and colorectal cancer dataset,
respectively. These improvements suggest we can improve genomic label
classification performance without collecting larger datasets. In the future,
this may reduce the need for expensive genome sequencing techniques, provide
personalized therapy recommendations based on widely available WSIs of cancer
tissue, and improve patient care with quicker treatment decisions - also in
medical centers without access to genome sequencing resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schirris_Y/0/1/0/all/0/1"&gt;Yoni Schirris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gavves_E/0/1/0/all/0/1"&gt;Efstratios Gavves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nederlof_I/0/1/0/all/0/1"&gt;Iris Nederlof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Horlings_H/0/1/0/all/0/1"&gt;Hugo Mark Horlings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1"&gt;Jonas Teuwen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Protecting Semantic Segmentation Models by Using Block-wise Image Encryption with Secret Key from Unauthorized Access. (arXiv:2107.09362v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09362</id>
        <link href="http://arxiv.org/abs/2107.09362"/>
        <updated>2021-07-21T02:01:34.500Z</updated>
        <summary type="html"><![CDATA[Since production-level trained deep neural networks (DNNs) are of a great
business value, protecting such DNN models against copyright infringement and
unauthorized access is in a rising demand. However, conventional model
protection methods focused only the image classification task, and these
protection methods were never applied to semantic segmentation although it has
an increasing number of applications. In this paper, we propose to protect
semantic segmentation models from unauthorized access by utilizing block-wise
transformation with a secret key for the first time. Protected models are
trained by using transformed images. Experiment results show that the proposed
protection method allows rightful users with the correct key to access the
model to full capacity and deteriorate the performance for unauthorized users.
However, protected models slightly drop the segmentation performance compared
to non-protected models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ito_H/0/1/0/all/0/1"&gt;Hiroki Ito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+AprilPyone_M/0/1/0/all/0/1"&gt;MaungMaung AprilPyone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RankSRGAN: Super Resolution Generative Adversarial Networks with Learning to Rank. (arXiv:2107.09427v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09427</id>
        <link href="http://arxiv.org/abs/2107.09427"/>
        <updated>2021-07-21T02:01:34.493Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GAN) have demonstrated the potential to
recover realistic details for single image super-resolution (SISR). To further
improve the visual quality of super-resolved results, PIRM2018-SR Challenge
employed perceptual metrics to assess the perceptual quality, such as PI, NIQE,
and Ma. However, existing methods cannot directly optimize these
indifferentiable perceptual metrics, which are shown to be highly correlated
with human ratings. To address the problem, we propose Super-Resolution
Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator
in the direction of different perceptual metrics. Specifically, we first train
a Ranker which can learn the behaviour of perceptual metrics and then introduce
a novel rank-content loss to optimize the perceptual quality. The most
appealing part is that the proposed method can combine the strengths of
different SR methods to generate better results. Furthermore, we extend our
method to multiple Rankers to provide multi-dimension constraints for the
generator. Extensive experiments show that RankSRGAN achieves visually pleasing
results and reaches state-of-the-art performance in perceptual metrics and
quality. Project page: https://wenlongzhang0517.github.io/Projects/RankSRGAN]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenlong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S2Looking: A Satellite Side-Looking Dataset for Building Change Detection. (arXiv:2107.09244v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09244</id>
        <link href="http://arxiv.org/abs/2107.09244"/>
        <updated>2021-07-21T02:01:34.485Z</updated>
        <summary type="html"><![CDATA[Collecting large-scale annotated satellite imagery datasets is essential for
deep-learning-based global building change surveillance. In particular, the
scroll imaging mode of optical satellites enables larger observation ranges and
shorter revisit periods, facilitating efficient global surveillance. However,
the images in recent satellite change detection datasets are mainly captured at
near-nadir viewing angles. In this paper, we introduce S2Looking, a building
change detection dataset that contains large-scale side-looking satellite
images captured at varying off-nadir angles. Our S2Looking dataset consists of
5000 registered bitemporal image pairs (size of 1024*1024, 0.5 ~ 0.8 m/pixel)
of rural areas throughout the world and more than 65,920 annotated change
instances. We provide two label maps to separately indicate the newly built and
demolished building regions for each sample in the dataset. We establish a
benchmark task based on this dataset, i.e., identifying the pixel-level
building changes in the bi-temporal images. We test several state-of-the-art
methods on both the S2Looking dataset and the (near-nadir) LEVIR-CD+ dataset.
The experimental results show that recent change detection methods exhibit much
poorer performance on the S2Looking than on LEVIR-CD+. The proposed S2Looking
dataset presents three main challenges: 1) large viewing angle changes, 2)
large illumination variances and 3) various complex scene characteristics
encountered in rural areas. Our proposed dataset may promote the development of
algorithms for satellite image change detection and registration under
conditions of large off-nadir angles. The dataset is available at
https://github.com/AnonymousForACMMM/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Hao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1"&gt;Donghai Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1"&gt;Jiabao Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Rui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Ao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1"&gt;Shouye Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bitao Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locality-aware Channel-wise Dropout for Occluded Face Recognition. (arXiv:2107.09270v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09270</id>
        <link href="http://arxiv.org/abs/2107.09270"/>
        <updated>2021-07-21T02:01:34.462Z</updated>
        <summary type="html"><![CDATA[Face recognition remains a challenging task in unconstrained scenarios,
especially when faces are partially occluded. To improve the robustness against
occlusion, augmenting the training images with artificial occlusions has been
proved as a useful approach. However, these artificial occlusions are commonly
generated by adding a black rectangle or several object templates including
sunglasses, scarfs and phones, which cannot well simulate the realistic
occlusions. In this paper, based on the argument that the occlusion essentially
damages a group of neurons, we propose a novel and elegant occlusion-simulation
method via dropping the activations of a group of neurons in some elaborately
selected channel. Specifically, we first employ a spatial regularization to
encourage each feature channel to respond to local and different face regions.
In this way, the activations affected by an occlusion in a local region are
more likely to be located in a single feature channel. Then, the locality-aware
channel-wise dropout (LCD) is designed to simulate the occlusion by dropping
out the entire feature channel. Furthermore, by randomly dropping out several
feature channels, our method can well simulate the occlusion of larger area.
The proposed LCD can encourage its succeeding layers to minimize the
intra-class feature variance caused by occlusions, thus leading to improved
robustness against occlusion. In addition, we design an auxiliary spatial
attention module by learning a channel-wise attention vector to reweight the
feature channels, which improves the contributions of non-occluded regions.
Extensive experiments on various benchmarks show that the proposed method
outperforms state-of-the-art methods with a remarkable improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1"&gt;Mingjie He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shiguang Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xilin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Video Transformer: Can Objects be the Words?. (arXiv:2107.09240v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09240</id>
        <link href="http://arxiv.org/abs/2107.09240"/>
        <updated>2021-07-21T02:01:34.453Z</updated>
        <summary type="html"><![CDATA[Transformers have been successful for many natural language processing tasks.
However, applying transformers to the video domain for tasks such as long-term
video generation and scene understanding has remained elusive due to the high
computational complexity and the lack of natural tokenization. In this paper,
we propose the Object-Centric Video Transformer (OCVT) which utilizes an
object-centric approach for decomposing scenes into tokens suitable for use in
a generative video transformer. By factoring the video into objects, our fully
unsupervised model is able to learn complex spatio-temporal dynamics of
multiple interacting objects in a scene and generate future frames of the
video. Our model is also significantly more memory-efficient than pixel-based
models and thus able to train on videos of length up to 70 frames with a single
48GB GPU. We compare our model with previous RNN-based approaches as well as
other possible video transformer baselines. We demonstrate OCVT performs well
when compared to baselines in generating future frames. OCVT also develops
useful representations for video reasoning, achieving start-of-the-art
performance on the CATER task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi-Fu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jaesik Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sungjin Ahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09543</id>
        <link href="http://arxiv.org/abs/2107.09543"/>
        <updated>2021-07-21T02:01:34.445Z</updated>
        <summary type="html"><![CDATA[Despite technological and medical advances, the detection, interpretation,
and treatment of cancer based on imaging data continue to pose significant
challenges. These include high inter-observer variability, difficulty of
small-sized lesion detection, nodule interpretation and malignancy
determination, inter- and intra-tumour heterogeneity, class imbalance,
segmentation inaccuracies, and treatment effect uncertainty. The recent
advancements in Generative Adversarial Networks (GANs) in computer vision as
well as in medical imaging may provide a basis for enhanced capabilities in
cancer detection and analysis. In this review, we assess the potential of GANs
to address a number of key challenges of cancer imaging, including data
scarcity and imbalance, domain and dataset shifts, data access and privacy,
data annotation and quantification, as well as cancer detection, tumour
profiling and treatment planning. We provide a critical appraisal of the
existing literature of GANs applied to cancer imagery, together with
suggestions on future research directions to address these challenges. We
analyse and discuss 163 papers that apply adversarial training techniques in
the context of cancer imaging and elaborate their methodologies, advantages and
limitations. With this work, we strive to bridge the gap between the needs of
the clinical cancer imaging community and the current and prospective research
on GANs in the artificial intelligence community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1"&gt;Richard Osuala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1"&gt;Kaisar Kushibar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1"&gt;Lidia Garrucho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1"&gt;Akis Linardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1"&gt;Zuzanna Szafranowska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1"&gt;Stefan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1"&gt;Oliver Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography. (arXiv:2107.09287v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09287</id>
        <link href="http://arxiv.org/abs/2107.09287"/>
        <updated>2021-07-21T02:01:34.418Z</updated>
        <summary type="html"><![CDATA[Data hiding is the process of embedding information into a noise-tolerant
signal such as a piece of audio, video, or image. Digital watermarking is a
form of data hiding where identifying data is robustly embedded so that it can
resist tampering and be used to identify the original owners of the media.
Steganography, another form of data hiding, embeds data for the purpose of
secure and secret communication. This survey summarises recent developments in
deep learning techniques for data hiding for the purposes of watermarking and
steganography, categorising them based on model architectures and noise
injection methods. The objective functions, evaluation metrics, and datasets
used for training these data hiding models are comprehensively summarised.
Finally, we propose and discuss possible future directions for research into
deep data hiding techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Byrnes_O/0/1/0/all/0/1"&gt;Olivia Byrnes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+La_W/0/1/0/all/0/1"&gt;Wendy La&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Congbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1"&gt;Minhui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating deep neural networks for efficient scene understanding in automotive cyber-physical systems. (arXiv:2107.09101v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09101</id>
        <link href="http://arxiv.org/abs/2107.09101"/>
        <updated>2021-07-21T02:01:34.411Z</updated>
        <summary type="html"><![CDATA[Automotive Cyber-Physical Systems (ACPS) have attracted a significant amount
of interest in the past few decades, while one of the most critical operations
in these systems is the perception of the environment. Deep learning and,
especially, the use of Deep Neural Networks (DNNs) provides impressive results
in analyzing and understanding complex and dynamic scenes from visual data. The
prediction horizons for those perception systems are very short and inference
must often be performed in real time, stressing the need of transforming the
original large pre-trained networks into new smaller models, by utilizing Model
Compression and Acceleration (MCA) techniques. Our goal in this work is to
investigate best practices for appropriately applying novel weight sharing
techniques, optimizing the available variables and the training procedures
towards the significant acceleration of widely adopted DNNs. Extensive
evaluation studies carried out using various state-of-the-art DNN models in
object detection and tracking experiments, provide details about the type of
errors that manifest after the application of weight sharing techniques,
resulting in significant acceleration gains with negligible accuracy losses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1"&gt;Stavros Nousias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pikoulis_E/0/1/0/all/0/1"&gt;Erion-Vasilis Pikoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mavrokefalidis_C/0/1/0/all/0/1"&gt;Christos Mavrokefalidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1"&gt;Aris S. Lalos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynthSeg: Domain Randomisation for Segmentation of Brain MRI Scans of any Contrast and Resolution. (arXiv:2107.09559v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09559</id>
        <link href="http://arxiv.org/abs/2107.09559"/>
        <updated>2021-07-21T02:01:34.386Z</updated>
        <summary type="html"><![CDATA[Despite advances in data augmentation and transfer learning, convolutional
neural networks (CNNs) have difficulties generalising to unseen target domains.
When applied to segmentation of brain MRI scans, CNNs are highly sensitive to
changes in resolution and contrast: even within the same MR modality, decreases
in performance can be observed across datasets. We introduce SynthSeg, the
first segmentation CNN agnostic to brain MRI scans of any contrast and
resolution. SynthSeg is trained with synthetic data sampled from a generative
model inspired by Bayesian segmentation. Crucially, we adopt a \textit{domain
randomisation} strategy where we fully randomise the generation parameters to
maximise the variability of the training data. Consequently, SynthSeg can
segment preprocessed and unpreprocessed real scans of any target domain,
without retraining or fine-tuning. Because SynthSeg only requires segmentations
to be trained (no images), it can learn from label maps obtained automatically
from existing datasets of different populations (e.g., with atrophy and
lesions), thus achieving robustness to a wide range of morphological
variability. We demonstrate SynthSeg on 5,500 scans of 6 modalities and 10
resolutions, where it exhibits unparalleled generalisation compared to
supervised CNNs, test time adaptation, and Bayesian segmentation. The code and
trained model are available at https://github.com/BBillot/SynthSeg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Billot_B/0/1/0/all/0/1"&gt;Benjamin Billot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Greve_D/0/1/0/all/0/1"&gt;Douglas N. Greve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Puonti_O/0/1/0/all/0/1"&gt;Oula Puonti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thielscher_A/0/1/0/all/0/1"&gt;Axel Thielscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leemput_K/0/1/0/all/0/1"&gt;Koen Van Leemput&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1"&gt;Bruce Fischl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1"&gt;Adrian V. Dalca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1"&gt;Juan Eugenio Iglesias&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison of Supervised and Unsupervised Deep Learning Methods for Anomaly Detection in Images. (arXiv:2107.09204v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09204</id>
        <link href="http://arxiv.org/abs/2107.09204"/>
        <updated>2021-07-21T02:01:34.379Z</updated>
        <summary type="html"><![CDATA[Anomaly detection in images plays a significant role for many applications
across all industries, such as disease diagnosis in healthcare or quality
assurance in manufacturing. Manual inspection of images, when extended over a
monotonously repetitive period of time is very time consuming and can lead to
anomalies being overlooked.Artificial neural networks have proven themselves
very successful on simple, repetitive tasks, in some cases even outperforming
humans. Therefore, in this paper we investigate different methods of deep
learning, including supervised and unsupervised learning, for anomaly detection
applied to a quality assurance use case. We utilize the MVTec anomaly dataset
and develop three different models, a CNN for supervised anomaly detection,
KD-CAE for autoencoder anomaly detection, NI-CAE for noise induced anomaly
detection and a DCGAN for generating reconstructed images. By experiments, we
found that KD-CAE performs better on the anomaly datasets compared to CNN and
NI-CAE, with NI-CAE performing the best on the Transistor dataset. We also
implemented a DCGAN for the creation of new training data but due to
computational limitation and lack of extrapolating the mechanics of AnoGAN, we
restricted ourselves just to the generation of GAN based images. We conclude
that unsupervised methods are more powerful for anomaly detection in images,
especially in a setting where only a small amount of anomalous data is
available, or the data is unlabeled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilmet_V/0/1/0/all/0/1"&gt;Vincent Wilmet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1"&gt;Sauraj Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Redl_T/0/1/0/all/0/1"&gt;Tabea Redl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandaker_H/0/1/0/all/0/1"&gt;H&amp;#xe5;kon Sandaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenning Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Large-scale Dataset for Hate Speech Detection on Vietnamese Social Media Texts. (arXiv:2103.11528v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11528</id>
        <link href="http://arxiv.org/abs/2103.11528"/>
        <updated>2021-07-21T02:01:34.365Z</updated>
        <summary type="html"><![CDATA[In recent years, Vietnam witnesses the mass development of social network
users on different social platforms such as Facebook, Youtube, Instagram, and
Tiktok. On social medias, hate speech has become a critical problem for social
network users. To solve this problem, we introduce the ViHSD - a
human-annotated dataset for automatically detecting hate speech on the social
network. This dataset contains over 30,000 comments, each comment in the
dataset has one of three labels: CLEAN, OFFENSIVE, or HATE. Besides, we
introduce the data creation process for annotating and evaluating the quality
of the dataset. Finally, we evaluated the dataset by deep learning models and
transformer models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1"&gt;Son T. Luu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Kiet Van Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Ngan Luu-Thuy Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Temporal Convolutional Network for Anticipating Actions in Egocentric Videos. (arXiv:2107.09504v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09504</id>
        <link href="http://arxiv.org/abs/2107.09504"/>
        <updated>2021-07-21T02:01:34.357Z</updated>
        <summary type="html"><![CDATA[Anticipating human actions is an important task that needs to be addressed
for the development of reliable intelligent agents, such as self-driving cars
or robot assistants. While the ability to make future predictions with high
accuracy is crucial for designing the anticipation approaches, the speed at
which the inference is performed is not less important. Methods that are
accurate but not sufficiently fast would introduce a high latency into the
decision process. Thus, this will increase the reaction time of the underlying
system. This poses a problem for domains such as autonomous driving, where the
reaction time is crucial. In this work, we propose a simple and effective
multi-modal architecture based on temporal convolutions. Our approach stacks a
hierarchy of temporal convolutional layers and does not rely on recurrent
layers to ensure a fast prediction. We further introduce a multi-modal fusion
mechanism that captures the pairwise interactions between RGB, flow, and object
modalities. Results on two large-scale datasets of egocentric videos,
EPIC-Kitchens-55 and EPIC-Kitchens-100, show that our approach achieves
comparable performance to the state-of-the-art approaches while being
significantly faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zatsarynna_O/0/1/0/all/0/1"&gt;Olga Zatsarynna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farha_Y/0/1/0/all/0/1"&gt;Yazan Abu Farha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1"&gt;Juergen Gall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion. (arXiv:2107.09293v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09293</id>
        <link href="http://arxiv.org/abs/2107.09293"/>
        <updated>2021-07-21T02:01:34.351Z</updated>
        <summary type="html"><![CDATA[We propose an audio-driven talking-head method to generate photo-realistic
talking-head videos from a single reference image. In this work, we tackle two
key challenges: (i) producing natural head motions that match speech prosody,
and (ii) maintaining the appearance of a speaker in a large head motion while
stabilizing the non-face regions. We first design a head pose predictor by
modeling rigid 6D head movements with a motion-aware recurrent neural network
(RNN). In this way, the predicted head poses act as the low-frequency holistic
movements of a talking head, thus allowing our latter network to focus on
detailed facial movement generation. To depict the entire image motions arising
from audio, we exploit a keypoint based dense motion field representation.
Then, we develop a motion field generator to produce the dense motion fields
from input audio, head poses, and a reference image. As this keypoint based
representation models the motions of facial regions, head, and backgrounds
integrally, our method can better constrain the spatial and temporal
consistency of the generated videos. Finally, an image generation network is
employed to render photo-realistic talking-head videos from the estimated
keypoint based motion fields and the input reference image. Extensive
experiments demonstrate that our method produces videos with plausible head
motions, synchronized facial expressions, and stable backgrounds and
outperforms the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Suzhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lincheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yu Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting few-shot classification with view-learnable contrastive learning. (arXiv:2107.09242v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09242</id>
        <link href="http://arxiv.org/abs/2107.09242"/>
        <updated>2021-07-21T02:01:34.331Z</updated>
        <summary type="html"><![CDATA[The goal of few-shot classification is to classify new categories with few
labeled examples within each class. Nowadays, the excellent performance in
handling few-shot classification problems is shown by metric-based
meta-learning methods. However, it is very hard for previous methods to
discriminate the fine-grained sub-categories in the embedding space without
fine-grained labels. This may lead to unsatisfactory generalization to
fine-grained subcategories, and thus affects model interpretation. To tackle
this problem, we introduce the contrastive loss into few-shot classification
for learning latent fine-grained structure in the embedding space. Furthermore,
to overcome the drawbacks of random image transformation used in current
contrastive learning in producing noisy and inaccurate image pairs (i.e.,
views), we develop a learning-to-learn algorithm to automatically generate
different views of the same image. Extensive experiments on standard few-shot
learning benchmarks demonstrate the superiority of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xu Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1"&gt;Liangjian Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1"&gt;Lili Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Critic Guided Segmentation of Rewarding Objects in First-Person Views. (arXiv:2107.09540v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09540</id>
        <link href="http://arxiv.org/abs/2107.09540"/>
        <updated>2021-07-21T02:01:34.324Z</updated>
        <summary type="html"><![CDATA[This work discusses a learning approach to mask rewarding objects in images
using sparse reward signals from an imitation learning dataset. For that, we
train an Hourglass network using only feedback from a critic model. The
Hourglass network learns to produce a mask to decrease the critic's score of a
high score image and increase the critic's score of a low score image by
swapping the masked areas between these two images. We trained the model on an
imitation learning dataset from the NeurIPS 2020 MineRL Competition Track,
where our model learned to mask rewarding objects in a complex interactive 3D
environment with a sparse reward signal. This approach was part of the 1st
place winning solution in this competition. Video demonstration and code:
https://rebrand.ly/critic-guided-segmentation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Melnik_A/0/1/0/all/0/1"&gt;Andrew Melnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harter_A/0/1/0/all/0/1"&gt;Augustin Harter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Limberg_C/0/1/0/all/0/1"&gt;Christian Limberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rana_K/0/1/0/all/0/1"&gt;Krishan Rana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suenderhauf_N/0/1/0/all/0/1"&gt;Niko Suenderhauf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritter_H/0/1/0/all/0/1"&gt;Helge Ritter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-Guided NIR Image Colorization via Adaptive Fusion of Semantic and Texture Clues. (arXiv:2107.09237v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09237</id>
        <link href="http://arxiv.org/abs/2107.09237"/>
        <updated>2021-07-21T02:01:34.317Z</updated>
        <summary type="html"><![CDATA[Near infrared (NIR) imaging has been widely applied in low-light imaging
scenarios; however, it is difficult for human and algorithms to perceive the
real scene in the colorless NIR domain. While Generative Adversarial Network
(GAN) has been widely employed in various image colorization tasks, it is
challenging for a direct mapping mechanism, such as a conventional GAN, to
transform an image from the NIR to the RGB domain with correct semantic
reasoning, well-preserved textures, and vivid color combinations concurrently.
In this work, we propose a novel Attention-based NIR image colorization
framework via Adaptive Fusion of Semantic and Texture clues, aiming at
achieving these goals within the same framework. The tasks of texture transfer
and semantic reasoning are carried out in two separate network blocks.
Specifically, the Texture Transfer Block (TTB) aims at extracting texture
features from the NIR image's Laplacian component and transferring them for
subsequent color fusion. The Semantic Reasoning Block (SRB) extracts semantic
clues and maps the NIR pixel values to the RGB domain. Finally, a Fusion
Attention Block (FAB) is proposed to adaptively fuse the features from the two
branches and generate an optimized colorization result. In order to enhance the
network's learning capacity in semantic reasoning as well as mapping precision
in texture transfer, we have proposed the Residual Coordinate Attention Block
(RCAB), which incorporates coordinate attention into a residual learning
framework, enabling the network to capture long-range dependencies along the
channel direction and meanwhile precise positional information can be preserved
along spatial directions. RCAB is also incorporated into FAB to facilitate
accurate texture alignment during fusion. Both quantitative and qualitative
evaluations show that the proposed method outperforms state-of-the-art NIR
image colorization methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xingxing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zaifeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenghua Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Spoken Language Understanding for Generalized Voice Assistants. (arXiv:2106.09009v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09009</id>
        <link href="http://arxiv.org/abs/2106.09009"/>
        <updated>2021-07-21T02:01:34.309Z</updated>
        <summary type="html"><![CDATA[End-to-end (E2E) spoken language understanding (SLU) systems predict
utterance semantics directly from speech using a single model. Previous work in
this area has focused on targeted tasks in fixed domains, where the output
semantic structure is assumed a priori and the input speech is of limited
complexity. In this work we present our approach to developing an E2E model for
generalized SLU in commercial voice assistants (VAs). We propose a fully
differentiable, transformer-based, hierarchical system that can be pretrained
at both the ASR and NLU levels. This is then fine-tuned on both transcription
and semantic classification losses to handle a diverse set of intent and
argument combinations. This leads to an SLU system that achieves significant
improvements over baselines on a complex internal generalized VA dataset with a
43% improvement in accuracy, while still meeting the 99% accuracy benchmark on
the popular Fluent Speech Commands dataset. We further evaluate our model on a
hard test set, exclusively containing slot arguments unseen in training, and
demonstrate a nearly 20% improvement, showing the efficacy of our approach in
truly demanding VA scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1"&gt;Michael Saxon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1"&gt;Samridhi Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKenna_J/0/1/0/all/0/1"&gt;Joseph P. McKenna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1"&gt;Athanasios Mouchtaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepSocNav: Social Navigation by Imitating Human Behaviors. (arXiv:2107.09170v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09170</id>
        <link href="http://arxiv.org/abs/2107.09170"/>
        <updated>2021-07-21T02:01:34.291Z</updated>
        <summary type="html"><![CDATA[Current datasets to train social behaviors are usually borrowed from
surveillance applications that capture visual data from a bird's-eye
perspective. This leaves aside precious relationships and visual cues that
could be captured through a first-person view of a scene. In this work, we
propose a strategy to exploit the power of current game engines, such as Unity,
to transform pre-existing bird's-eye view datasets into a first-person view, in
particular, a depth view. Using this strategy, we are able to generate large
volumes of synthetic data that can be used to pre-train a social navigation
model. To test our ideas, we present DeepSocNav, a deep learning based model
that takes advantage of the proposed approach to generate synthetic data.
Furthermore, DeepSocNav includes a self-supervised strategy that is included as
an auxiliary task. This consists of predicting the next depth frame that the
agent will face. Our experiments show the benefits of the proposed model that
is able to outperform relevant baselines in terms of social navigation scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vicente_J/0/1/0/all/0/1"&gt;Juan Pablo de Vicente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1"&gt;Alvaro Soto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OSLO: On-the-Sphere Learning for Omnidirectional images and its application to 360-degree image compression. (arXiv:2107.09179v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09179</id>
        <link href="http://arxiv.org/abs/2107.09179"/>
        <updated>2021-07-21T02:01:34.082Z</updated>
        <summary type="html"><![CDATA[State-of-the-art 2D image compression schemes rely on the power of
convolutional neural networks (CNNs). Although CNNs offer promising
perspectives for 2D image compression, extending such models to omnidirectional
images is not straightforward. First, omnidirectional images have specific
spatial and statistical properties that can not be fully captured by current
CNN models. Second, basic mathematical operations composing a CNN architecture,
e.g., translation and sampling, are not well-defined on the sphere. In this
paper, we study the learning of representation models for omnidirectional
images and propose to use the properties of HEALPix uniform sampling of the
sphere to redefine the mathematical tools used in deep learning models for
omnidirectional images. In particular, we: i) propose the definition of a new
convolution operation on the sphere that keeps the high expressiveness and the
low complexity of a classical 2D convolution; ii) adapt standard CNN techniques
such as stride, iterative aggregation, and pixel shuffling to the spherical
domain; and then iii) apply our new framework to the task of omnidirectional
image compression. Our experiments show that our proposed on-the-sphere
solution leads to a better compression gain that can save 13.7% of the bit rate
compared to similar learned models applied to equirectangular images. Also,
compared to learning models based on graph convolutional networks, our solution
supports more expressive filters that can preserve high frequencies and provide
a better perceptual quality of the compressed images. Such results demonstrate
the efficiency of the proposed framework, which opens new research venues for
other omnidirectional vision tasks to be effectively implemented on the sphere
manifold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bidgoli_N/0/1/0/all/0/1"&gt;Navid Mahmoudian Bidgoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Azevedo_R/0/1/0/all/0/1"&gt;Roberto G. de A. Azevedo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maugey_T/0/1/0/all/0/1"&gt;Thomas Maugey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Roumy_A/0/1/0/all/0/1"&gt;Aline Roumy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Frossard_P/0/1/0/all/0/1"&gt;Pascal Frossard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quality and Complexity Assessment of Learning-Based Image Compression Solutions. (arXiv:2107.09136v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09136</id>
        <link href="http://arxiv.org/abs/2107.09136"/>
        <updated>2021-07-21T02:01:33.936Z</updated>
        <summary type="html"><![CDATA[This work presents an analysis of state-of-the-art learning-based image
compression techniques. We compare 8 models available in the Tensorflow
Compression package in terms of visual quality metrics and processing time,
using the KODAK data set. The results are compared with the Better Portable
Graphics (BPG) and the JPEG2000 codecs. Results show that JPEG2000 has the
lowest execution times compared with the fastest learning-based model, with a
speedup of 1.46x in compression and 30x in decompression. However, the
learning-based models achieved improvements over JPEG2000 in terms of quality,
specially for lower bitrates. Our findings also show that BPG is more efficient
in terms of PSNR, but the learning models are better for other quality metrics,
and sometimes even faster. The results indicate that learning-based techniques
are promising solutions towards a future mainstream compression method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Dick_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Dick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abreu_B/0/1/0/all/0/1"&gt;Brunno Abreu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Grellert_M/0/1/0/all/0/1"&gt;Mateus Grellert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bampi_S/0/1/0/all/0/1"&gt;Sergio Bampi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Claim Verification using a Multi-GAN based Model. (arXiv:2103.08001v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08001</id>
        <link href="http://arxiv.org/abs/2103.08001"/>
        <updated>2021-07-21T02:01:33.929Z</updated>
        <summary type="html"><![CDATA[This article describes research on claim verification carried out using a
multiple GAN-based model. The proposed model consists of three pairs of
generators and discriminators. The generator and discriminator pairs are
responsible for generating synthetic data for supported and refuted claims and
claim labels. A theoretical discussion about the proposed model is provided to
validate the equilibrium state of the model. The proposed model is applied to
the FEVER dataset, and a pre-trained language model is used for the input text
data. The synthetically generated data helps to gain information which helps
the model to perform better than state of the art models and other standard
classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hatua_A/0/1/0/all/0/1"&gt;Amartya Hatua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1"&gt;Arjun Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1"&gt;Rakesh M. Verma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separating Skills and Concepts for Novel Visual Question Answering. (arXiv:2107.09106v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09106</id>
        <link href="http://arxiv.org/abs/2107.09106"/>
        <updated>2021-07-21T02:01:33.922Z</updated>
        <summary type="html"><![CDATA[Generalization to out-of-distribution data has been a problem for Visual
Question Answering (VQA) models. To measure generalization to novel questions,
we propose to separate them into "skills" and "concepts". "Skills" are visual
tasks, such as counting or attribute recognition, and are applied to "concepts"
mentioned in the question, such as objects and people. VQA methods should be
able to compose skills and concepts in novel ways, regardless of whether the
specific composition has been seen in training, yet we demonstrate that
existing models have much to improve upon towards handling new compositions. We
present a novel method for learning to compose skills and concepts that
separates these two factors implicitly within a model by learning grounded
concept representations and disentangling the encoding of skills from that of
concepts. We enforce these properties with a novel contrastive learning
procedure that does not rely on external annotations and can be learned from
unlabeled image-question pairs. Experiments demonstrate the effectiveness of
our approach for improving compositional and grounding performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1"&gt;Spencer Whitehead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward a Knowledge Discovery Framework for Data Science Job Market in the United States. (arXiv:2106.11077v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11077</id>
        <link href="http://arxiv.org/abs/2106.11077"/>
        <updated>2021-07-21T02:01:33.913Z</updated>
        <summary type="html"><![CDATA[The growth of the data science field requires better tools to understand such
a fast-paced growing domain. Moreover, individuals from different backgrounds
became interested in following a career as data scientists. Therefore,
providing a quantitative guide for individuals and organizations to understand
the skills required in the job market would be crucial. This paper introduces a
framework to analyze the job market for data science-related jobs within the US
while providing an interface to access insights in this market. The proposed
framework includes three sub-modules allowing continuous data collection,
information extraction, and a web-based dashboard visualization to investigate
the spatial and temporal distribution of data science-related jobs and skills.
The result of this work shows important skills for the main branches of data
science jobs and attempts to provide a skill-based definition of these data
science branches. The current version of this application is deployed on the
web and allows individuals and institutes to investigate skills required for
data science positions through the industry lens.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidarysafa_M/0/1/0/all/0/1"&gt;Mojtaba Heidarysafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowsari_K/0/1/0/all/0/1"&gt;Kamran Kowsari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bashiri_M/0/1/0/all/0/1"&gt;Masoud Bashiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1"&gt;Donald E. Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional module for heart localization and segmentation in MRI. (arXiv:2107.09134v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09134</id>
        <link href="http://arxiv.org/abs/2107.09134"/>
        <updated>2021-07-21T02:01:33.906Z</updated>
        <summary type="html"><![CDATA[Magnetic resonance imaging (MRI) is a widely known medical imaging technique
used to assess the heart function. Deep learning (DL) models perform several
tasks in cardiac MRI (CMR) images with good efficacy, such as segmentation,
estimation, and detection of diseases. Many DL models based on convolutional
neural networks (CNN) were improved by detecting regions-of-interest (ROI)
either automatically or by hand. In this paper we describe Visual-Motion-Focus
(VMF), a module that detects the heart motion in the 4D MRI sequence, and
highlights ROIs by focusing a Radial Basis Function (RBF) on the estimated
motion field. We experimented and evaluated VMF on three CMR datasets,
observing that the proposed ROIs cover 99.7% of data labels (Recall score),
improved the CNN segmentation (mean Dice score) by 1.7 (p < .001) after the ROI
extraction, and improved the overall training speed by 2.5 times (+150%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lima_D/0/1/0/all/0/1"&gt;Daniel Lima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Graves_C/0/1/0/all/0/1"&gt;Catharine Graves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gutierrez_M/0/1/0/all/0/1"&gt;Marco Gutierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brandoli_B/0/1/0/all/0/1"&gt;Bruno Brandoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jose_J/0/1/0/all/0/1"&gt;Jose Rodrigues-Jr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LAPNet: Non-rigid Registration derived in k-space for Magnetic Resonance Imaging. (arXiv:2107.09060v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09060</id>
        <link href="http://arxiv.org/abs/2107.09060"/>
        <updated>2021-07-21T02:01:33.895Z</updated>
        <summary type="html"><![CDATA[Physiological motion, such as cardiac and respiratory motion, during Magnetic
Resonance (MR) image acquisition can cause image artifacts. Motion correction
techniques have been proposed to compensate for these types of motion during
thoracic scans, relying on accurate motion estimation from undersampled
motion-resolved reconstruction. A particular interest and challenge lie in the
derivation of reliable non-rigid motion fields from the undersampled
motion-resolved data. Motion estimation is usually formulated in image space
via diffusion, parametric-spline, or optical flow methods. However, image-based
registration can be impaired by remaining aliasing artifacts due to the
undersampled motion-resolved reconstruction. In this work, we describe a
formalism to perform non-rigid registration directly in the sampled Fourier
space, i.e. k-space. We propose a deep-learning based approach to perform fast
and accurate non-rigid registration from the undersampled k-space data. The
basic working principle originates from the Local All-Pass (LAP) technique, a
recently introduced optical flow-based registration. The proposed LAPNet is
compared against traditional and deep learning image-based registrations and
tested on fully-sampled and highly-accelerated (with two undersampling
strategies) 3D respiratory motion-resolved MR images in a cohort of 40 patients
with suspected liver or lung metastases and 25 healthy subjects. The proposed
LAPNet provided consistent and superior performance to image-based approaches
throughout different sampling trajectories and acceleration factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kustner_T/0/1/0/all/0/1"&gt;Thomas K&amp;#xfc;stner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1"&gt;Jiazhen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haikun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cruz_G/0/1/0/all/0/1"&gt;Gastao Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gilliam_C/0/1/0/all/0/1"&gt;Christopher Gilliam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blu_T/0/1/0/all/0/1"&gt;Thierry Blu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1"&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Botnar_R/0/1/0/all/0/1"&gt;Ren&amp;#xe9; Botnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prieto_C/0/1/0/all/0/1"&gt;Claudia Prieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Collaborative Reinforcement Learning Agents that Communicate Through Text-Based Natural Language. (arXiv:2107.09356v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09356</id>
        <link href="http://arxiv.org/abs/2107.09356"/>
        <updated>2021-07-21T02:01:33.868Z</updated>
        <summary type="html"><![CDATA[Communication between agents in collaborative multi-agent settings is in
general implicit or a direct data stream. This paper considers text-based
natural language as a novel form of communication between multiple agents
trained with reinforcement learning. This could be considered first steps
toward a truly autonomous communication without the need to define a limited
set of instructions, and natural collaboration between humans and robots.
Inspired by the game of Blind Leads, we propose an environment where one agent
uses natural language instructions to guide another through a maze. We test the
ability of reinforcement learning agents to effectively communicate through
discrete word-level symbols and show that the agents are able to sufficiently
communicate through natural language with a limited vocabulary. Although the
communication is not always perfect English, the agents are still able to
navigate the maze. We achieve a BLEU score of 0.85, which is an improvement of
0.61 over randomly generated sequences while maintaining a 100% maze completion
rate. This is a 3.5 times the performance of the random baseline using our
reference set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eloff_K/0/1/0/all/0/1"&gt;Kevin Eloff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engelbrecht_H/0/1/0/all/0/1"&gt;Herman Engelbrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Aware Neural Networks for Skin Cancer Detection. (arXiv:2107.09118v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.09118</id>
        <link href="http://arxiv.org/abs/2107.09118"/>
        <updated>2021-07-21T02:01:33.852Z</updated>
        <summary type="html"><![CDATA[Deep learning (DL) models have received particular attention in medical
imaging due to their promising pattern recognition capabilities. However, Deep
Neural Networks (DNNs) require a huge amount of data, and because of the lack
of sufficient data in this field, transfer learning can be a great solution.
DNNs used for disease diagnosis meticulously concentrate on improving the
accuracy of predictions without providing a figure about their confidence of
predictions. Knowing how much a DNN model is confident in a computer-aided
diagnosis model is necessary for gaining clinicians' confidence and trust in
DL-based solutions. To address this issue, this work presents three different
methods for quantifying uncertainties for skin cancer detection from images. It
also comprehensively evaluates and compares performance of these DNNs using
novel uncertainty-related metrics. The obtained results reveal that the
predictive uncertainty estimation methods are capable of flagging risky and
erroneous predictions with a high uncertainty estimate. We also demonstrate
that ensemble approaches are more reliable in capturing uncertainties through
inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khaledyan_D/0/1/0/all/0/1"&gt;Donya Khaledyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tajally_A/0/1/0/all/0/1"&gt;AmirReza Tajally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sarkhosh_R/0/1/0/all/0/1"&gt;Reza Sarkhosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shamsi_A/0/1/0/all/0/1"&gt;Afshar Shamsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asgharnezhad_H/0/1/0/all/0/1"&gt;Hamzeh Asgharnezhad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1"&gt;Abbas Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1"&gt;Saeid Nahavandi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Parameters? No Thanks!. (arXiv:2107.09622v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09622</id>
        <link href="http://arxiv.org/abs/2107.09622"/>
        <updated>2021-07-21T02:01:33.837Z</updated>
        <summary type="html"><![CDATA[This work studies the long-standing problems of model capacity and negative
interference in multilingual neural machine translation MNMT. We use network
pruning techniques and observe that pruning 50-70% of the parameters from a
trained MNMT model results only in a 0.29-1.98 drop in the BLEU score.
Suggesting that there exist large redundancies even in MNMT models. These
observations motivate us to use the redundant parameters and counter the
interference problem efficiently. We propose a novel adaptation strategy, where
we iteratively prune and retrain the redundant parameters of an MNMT to improve
bilingual representations while retaining the multilinguality. Negative
interference severely affects high resource languages, and our method
alleviates it without any additional adapter modules. Hence, we call it
parameter-free adaptation strategy, paving way for the efficient adaptation of
MNMT. We demonstrate the effectiveness of our method on a 9 language MNMT
trained on TED talks, and report an average improvement of +1.36 BLEU on high
resource pairs. Code will be released here.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1"&gt;Zeeshan Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akella_K/0/1/0/all/0/1"&gt;Kartheek Akella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay P. Namboodiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1"&gt;C V Jawahar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Sensor-invariant Embedding of Satellite Data: A Case Study for Lake Ice Monitoring. (arXiv:2107.09092v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09092</id>
        <link href="http://arxiv.org/abs/2107.09092"/>
        <updated>2021-07-21T02:01:33.823Z</updated>
        <summary type="html"><![CDATA[Fusing satellite imagery acquired with different sensors has been a
long-standing challenge of Earth observation, particularly across different
modalities such as optical and Synthetic Aperture Radar (SAR) images. Here, we
explore the joint analysis of imagery from different sensors in the light of
representation learning: we propose to learn a joint, sensor-invariant
embedding (feature representation) within a deep neural network. Our
application problem is the monitoring of lake ice on Alpine lakes. To reach the
temporal resolution requirement of the Swiss Global Climate Observing System
(GCOS) office, we combine three image sources: Sentinel-1 SAR (S1-SAR), Terra
MODIS and Suomi-NPP VIIRS. The large gaps between the optical and SAR domains
and between the sensor resolutions make this a challenging instance of the
sensor fusion problem. Our approach can be classified as a feature-level fusion
that is learnt in a data-driven manner. The proposed network architecture has
separate encoding branches for each image sensor, which feed into a single
latent embedding. I.e., a common feature representation shared by all inputs,
such that subsequent processing steps deliver comparable output irrespective of
which sort of input image was used. By fusing satellite data, we map lake ice
at a temporal resolution of <1.5 days. The network produces spatially explicit
lake ice maps with pixel-wise accuracies >91.3% (respectively, mIoU scores
>60.7%) and generalises well across different lakes and winters. Moreover, it
sets a new state-of-the-art for determining the important ice-on and ice-off
dates for the target lakes, in many cases meeting the GCOS requirement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tom_M/0/1/0/all/0/1"&gt;Manu Tom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuchang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baltsavias_E/0/1/0/all/0/1"&gt;Emmanuel Baltsavias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A keyword-driven approach to science. (arXiv:2106.14610v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14610</id>
        <link href="http://arxiv.org/abs/2106.14610"/>
        <updated>2021-07-21T02:01:33.805Z</updated>
        <summary type="html"><![CDATA[To a good extent, words can be understood as corresponding to patterns or
categories that appeared in order to represent concepts and structures that are
particularly important or useful in a given time and space. Words are
characterized by not being completely general nor specific, in the sense that
the same word can be instantiated or related to several different contexts,
depending on specific situations. Indeed, the way in which words are
instantiated and associated represents a particularly interesting aspect that
can substantially help to better understand the context in which they are
employed. Scientific words are no exception to that. In the present work, we
approach the associations between a set of particularly relevant words in the
sense of being not only frequently used in several areas, but also representing
concepts that are currently related to some of the main standing challenges in
science. More specifically, the study reported here takes into account the
words "prediction", "model", "optimization", "complex", "entropy", "random",
"deterministic", "pattern", and "database". In order to complement the
analysis, we also obtain a network representing the relationship between the
adopted areas. Many interesting results were found. First and foremost, several
of the words were observed to have markedly distinct associations in different
areas. Biology was found to be related to computer science, sharing
associations with databases. Furthermore, for most of the cases, the words
"complex", "model", and "prediction" were observed to have several strong
associations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arruda_H/0/1/0/all/0/1"&gt;Henrique Ferraz de Arruda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1"&gt;Luciano da Fontoura Costa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Conformal Prediction with Auxiliary Tasks. (arXiv:2102.08898v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08898</id>
        <link href="http://arxiv.org/abs/2102.08898"/>
        <updated>2021-07-21T02:01:33.797Z</updated>
        <summary type="html"><![CDATA[We develop a novel approach to conformal prediction when the target task has
limited data available for training. Conformal prediction identifies a small
set of promising output candidates in place of a single prediction, with
guarantees that the set contains the correct answer with high probability. When
training data is limited, however, the predicted set can easily become unusably
large. In this work, we obtain substantially tighter prediction sets while
maintaining desirable marginal guarantees by casting conformal prediction as a
meta-learning paradigm over exchangeable collections of auxiliary tasks. Our
conformalization algorithm is simple, fast, and agnostic to the choice of
underlying model, learning algorithm, or dataset. We demonstrate the
effectiveness of this approach across a number of few-shot classification and
regression tasks in natural language processing, computer vision, and
computational chemistry for drug discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1"&gt;Adam Fisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1"&gt;Tal Schuster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1"&gt;Tommi Jaakkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recognizing Emotion Cause in Conversations. (arXiv:2012.11820v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11820</id>
        <link href="http://arxiv.org/abs/2012.11820"/>
        <updated>2021-07-21T02:01:33.787Z</updated>
        <summary type="html"><![CDATA[We address the problem of recognizing emotion cause in conversations, define
two novel sub-tasks of this problem, and provide a corresponding dialogue-level
dataset, along with strong Transformer-based baselines. The dataset is
available at https://github.com/declare-lab/RECCON.

Introduction: Recognizing the cause behind emotions in text is a fundamental
yet under-explored area of research in NLP. Advances in this area hold the
potential to improve interpretability and performance in affect-based models.
Identifying emotion causes at the utterance level in conversations is
particularly challenging due to the intermingling dynamics among the
interlocutors.

Method: We introduce the task of Recognizing Emotion Cause in CONversations
with an accompanying dataset named \RECCONDA, containing over 1,000 dialogues
and 10,000 utterance cause-effect pairs. Furthermore, we define different cause
types based on the source of the causes, and establish strong Transformer-based
baselines to address two different sub-tasks on this dataset: causal span
extraction and causal emotion entailment.

Result: Our Transformer-based baselines, which leverage contextual
pre-trained embeddings, such as RoBERTa, outperform the state-of-the-art
emotion cause extraction approaches

Conclusion: We introduce a new task highly relevant for (explainable)
emotion-aware artificial intelligence: recognizing emotion cause in
conversations, provide a new highly challenging publicly available
dialogue-level dataset for this task, and give strong baseline results on this
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1"&gt;Devamanyu Hazarika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1"&gt;Deepanway Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1"&gt;Rishabh Bhardwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1"&gt;Samson Yu Bai Jian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1"&gt;Pengfei Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1"&gt;Romila Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1"&gt;Abhinaba Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhaya_N/0/1/0/all/0/1"&gt;Niyati Chhaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1"&gt;Alexander Gelbukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering. (arXiv:2012.14610v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14610</id>
        <link href="http://arxiv.org/abs/2012.14610"/>
        <updated>2021-07-21T02:01:33.780Z</updated>
        <summary type="html"><![CDATA[We study open-domain question answering with structured, unstructured and
semi-structured knowledge sources, including text, tables, lists and knowledge
bases. Departing from prior work, we propose a unifying approach that
homogenizes all sources by reducing them to text and applies the
retriever-reader model which has so far been limited to text sources only. Our
approach greatly improves the results on knowledge-base QA tasks by 11 points,
compared to latest graph-based methods. More importantly, we demonstrate that
our unified knowledge (UniK-QA) model is a simple and yet effective way to
combine heterogeneous sources of knowledge, advancing the state-of-the-art
results on two popular question answering benchmarks, NaturalQuestions and
WebQuestions, by 3.5 and 2.6 points, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1"&gt;Barlas Oguz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xilun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1"&gt;Vladimir Karpukhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1"&gt;Stan Peshterliev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1"&gt;Dmytro Okhonko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1"&gt;Michael Schlichtkrull&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sonal Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1"&gt;Yashar Mehdad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1"&gt;Scott Yih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Trends of COVID-19 Vaccine Beliefs on Twitter with Lexical Embeddings. (arXiv:2104.01131v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01131</id>
        <link href="http://arxiv.org/abs/2104.01131"/>
        <updated>2021-07-21T02:01:33.773Z</updated>
        <summary type="html"><![CDATA[Social media plays a pivotal role in disseminating news globally and acts as
a platform for people to express their opinions on various topics. A wide
variety of views accompanies COVID-19 vaccination drives across the globe,
often colored by emotions, which change along with rising cases, approval of
vaccines, and multiple factors discussed online. This study aims at analyzing
the temporal evolution of different Emotion categories: Hesitation, Rage,
Sorrow, Anticipation, Faith, and Contentment with Influencing Factors: Vaccine
Rollout, Misinformation, Health Effects, and Inequities as lexical categories
created from Tweets belonging to five countries with vital vaccine roll-out
programs, namely, India, United States of America, Brazil, United Kingdom, and
Australia. We extracted a corpus of nearly 1.8 million Twitter posts related to
COVID-19 vaccination. Using cosine distance from selected seed words, we
expanded the vocabulary of each category and tracked the longitudinal change in
their strength from June 2020 to April 2021. We used community detection
algorithms to find modules in positive correlation networks. Our findings
suggest that tweets expressing hesitancy towards vaccines contain the highest
mentions of health-related effects in all countries. Our results indicated that
the patterns of hesitancy were variable across geographies and can help us
learn targeted interventions. We also observed a significant change in the
linear trends of categories like hesitation and contentment before and after
approval of vaccines. Negative emotions like rage and sorrow gained the highest
importance in the alluvial diagram. They formed a significant module with all
the influencing factors in April 2021, when India observed the second wave of
COVID-19 cases. The relationship between Emotions and Influencing Factors was
found to be variable across the countries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chopra_H/0/1/0/all/0/1"&gt;Harshita Chopra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vashishtha_A/0/1/0/all/0/1"&gt;Aniket Vashishtha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_R/0/1/0/all/0/1"&gt;Ridam Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashima/0/1/0/all/0/1"&gt;Ashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1"&gt;Ananya Tyagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1"&gt;Tavpritesh Sethi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hash Layers For Large Sparse Models. (arXiv:2106.04426v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04426</id>
        <link href="http://arxiv.org/abs/2106.04426"/>
        <updated>2021-07-21T02:01:33.752Z</updated>
        <summary type="html"><![CDATA[We investigate the training of sparse layers that use different parameters
for different inputs based on hashing in large Transformer models.
Specifically, we modify the feedforward layer to hash to different sets of
weights depending on the current token, over all tokens in the sequence. We
show that this procedure either outperforms or is competitive with
learning-to-route mixture-of-expert methods such as Switch Transformers and
BASE Layers, while requiring no routing parameters or extra terms in the
objective function such as a load balancing loss, and no sophisticated
assignment algorithm. We study the performance of different hashing techniques,
hash sizes and input features, and show that balanced and random hashes focused
on the most local features work best, compared to either learning clusters or
using longer-range context. We show our approach works well both on large
language modeling and dialogue tasks, and on downstream fine-tuning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The advent and fall of a vocabulary learning bias from communicative efficiency. (arXiv:2105.11519v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11519</id>
        <link href="http://arxiv.org/abs/2105.11519"/>
        <updated>2021-07-21T02:01:33.745Z</updated>
        <summary type="html"><![CDATA[Biosemiosis is a process of choice-making between simultaneously alternative
options. It is well-known that, when sufficiently young children encounter a
new word, they tend to interpret it as pointing to a meaning that does not have
a word yet in their lexicon rather than to a meaning that already has a word
attached. In previous research, the strategy was shown to be optimal from an
information theoretic standpoint. In that framework, interpretation is
hypothesized to be driven by the minimization of a cost function: the option of
least communication cost is chosen. However, the information theoretic model
employed in that research neither explains the weakening of that vocabulary
learning bias in older children or polylinguals nor reproduces Zipf's
meaning-frequency law, namely the non-linear relationship between the number of
meanings of a word and its frequency. Here we consider a generalization of the
model that is channeled to reproduce that law. The analysis of the new model
reveals regions of the phase space where the bias disappears consistently with
the weakening or loss of the bias in older children or polylinguals. The model
is abstract enough to support future research on other levels of life that are
relevant to biosemiotics. In the deep learning era, the model is a transparent
low-dimensional tool for future experimental research and illustrates the
predictive power of a theoretical framework originally designed to shed light
on the origins of Zipf's rank-frequency law.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carrera_Casado_D/0/1/0/all/0/1"&gt;David Carrera-Casado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1"&gt;Ramon Ferrer-i-Cancho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Hyperbolic Neural Networks. (arXiv:2105.14686v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14686</id>
        <link href="http://arxiv.org/abs/2105.14686"/>
        <updated>2021-07-21T02:01:33.737Z</updated>
        <summary type="html"><![CDATA[Hyperbolic neural networks have shown great potential for modeling complex
data. However, existing hyperbolic networks are not completely hyperbolic, as
they encode features in a hyperbolic space yet formalize most of their
operations in the tangent space (a Euclidean subspace) at the origin of the
hyperbolic space. This hybrid method greatly limits the modeling ability of
networks. In this paper, we propose a fully hyperbolic framework to build
hyperbolic networks based on the Lorentz model by adapting the Lorentz
transformations (including boost and rotation) to formalize essential
operations of neural networks. Moreover, we also prove that linear
transformation in tangent spaces used by existing hyperbolic networks is a
relaxation of the Lorentz rotation and does not include the boost, implicitly
limiting the capabilities of existing hyperbolic networks. The experimental
results on four NLP tasks show that our method has better performance for
building both shallow and deep networks. Our code will be released to
facilitate follow-up research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weize Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yankai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hexu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Different kinds of cognitive plausibility: why are transformers better than RNNs at predicting N400 amplitude?. (arXiv:2107.09648v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09648</id>
        <link href="http://arxiv.org/abs/2107.09648"/>
        <updated>2021-07-21T02:01:33.727Z</updated>
        <summary type="html"><![CDATA[Despite being designed for performance rather than cognitive plausibility,
transformer language models have been found to be better at predicting metrics
used to assess human language comprehension than language models with other
architectures, such as recurrent neural networks. Based on how well they
predict the N400, a neural signal associated with processing difficulty, we
propose and provide evidence for one possible explanation - their predictions
are affected by the preceding context in a way analogous to the effect of
semantic facilitation in humans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1"&gt;James A. Michaelov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bardolph_M/0/1/0/all/0/1"&gt;Megan D. Bardolph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coulson_S/0/1/0/all/0/1"&gt;Seana Coulson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1"&gt;Benjamin K. Bergen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning ULMFiT and Self-Distillation with Calibration for Medical Dialogue System. (arXiv:2107.09625v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09625</id>
        <link href="http://arxiv.org/abs/2107.09625"/>
        <updated>2021-07-21T02:01:33.710Z</updated>
        <summary type="html"><![CDATA[A medical dialogue system is essential for healthcare service as providing
primary clinical advice and diagnoses. It has been gradually adopted and
practiced in medical organizations in the form of a conversational bot, largely
due to the advancement of NLP. In recent years, the introduction of
state-of-the-art deep learning models and transfer learning techniques like
Universal Language Model Fine Tuning (ULMFiT) and Knowledge Distillation (KD)
largely contributes to the performance of NLP tasks. However, some deep neural
networks are poorly calibrated and wrongly estimate the uncertainty. Hence the
model is not trustworthy, especially in sensitive medical decision-making
systems and safety tasks. In this paper, we investigate the well-calibrated
model for ULMFiT and self-distillation (SD) in a medical dialogue system. The
calibrated ULMFiT (CULMFiT) is obtained by incorporating label smoothing (LS),
a commonly used regularization technique to achieve a well-calibrated model.
Moreover, we apply the technique to recalibrate the confidence score called
temperature scaling (TS) with KD to observe its correlation with network
calibration. To further understand the relation between SD and calibration, we
use both fixed and optimal temperatures to fine-tune the whole model. All
experiments are conducted on the consultation backpain dataset collected by
experts then further validated using a large publicly medial dialogue corpus.
We empirically show that our proposed methodologies outperform conventional
methods in terms of accuracy and robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ao_S/0/1/0/all/0/1"&gt;Shuang Ao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_X/0/1/0/all/0/1"&gt;Xeno Acharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StreamBlocks: A compiler for heterogeneous dataflow computing (technical report). (arXiv:2107.09333v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2107.09333</id>
        <link href="http://arxiv.org/abs/2107.09333"/>
        <updated>2021-07-21T02:01:33.687Z</updated>
        <summary type="html"><![CDATA[To increase performance and efficiency, systems use FPGAs as reconfigurable
accelerators. A key challenge in designing these systems is partitioning
computation between processors and an FPGA. An appropriate division of labor
may be difficult to predict in advance and require experiments and
measurements. When an investigation requires rewriting part of the system in a
new language or with a new programming model, its high cost can retard the
study of different configurations. A single-language system with an appropriate
programming model and compiler that targets both platforms simplifies this
exploration to a simple recompile with new compiler directives.

This work introduces StreamBlocks, an open-source compiler and runtime that
uses the CAL dataflow programming language to partition computations across
heterogeneous (CPU/accelerator) platforms. Because of the dataflow model's
semantics and the CAL language, StreamBlocks can exploit both thread
parallelism in multi-core CPUs and the inherent parallelism of FPGAs.
StreamBlocks supports exploring the design space with a profile-guided tool
that helps identify the best hardware-software partitions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bezati_E/0/1/0/all/0/1"&gt;Endri Bezati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emami_M/0/1/0/all/0/1"&gt;Mahyar Emami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janneck_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rn Janneck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larus_J/0/1/0/all/0/1"&gt;James Larus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries. (arXiv:2107.09609v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09609</id>
        <link href="http://arxiv.org/abs/2107.09609"/>
        <updated>2021-07-21T02:01:33.677Z</updated>
        <summary type="html"><![CDATA[Detecting customized moments and highlights from videos given natural
language (NL) user queries is an important but under-studied topic. One of the
challenges in pursuing this direction is the lack of annotated data. To address
this issue, we present the Query-based Video Highlights (QVHighlights) dataset.
It consists of over 10,000 YouTube videos, covering a wide range of topics,
from everyday activities and travel in lifestyle vlog videos to social and
political activities in news videos. Each video in the dataset is annotated
with: (1) a human-written free-form NL query, (2) relevant moments in the video
w.r.t. the query, and (3) five-point scale saliency scores for all
query-relevant clips. This comprehensive annotation enables us to develop and
evaluate systems that detect relevant moments as well as salient highlights for
diverse, flexible user queries. We also present a strong baseline for this
task, Moment-DETR, a transformer encoder-decoder model that views moment
retrieval as a direct set prediction problem, taking extracted video and query
representations as inputs and predicting moment coordinates and saliency scores
end-to-end. While our model does not utilize any human prior, we show that it
performs competitively when compared to well-engineered architectures. With
weakly supervised pretraining using ASR captions, Moment-DETR substantially
outperforms previous methods. Lastly, we present several ablations and
visualizations of Moment-DETR. Data and code is publicly available at
https://github.com/jayleicn/moment_detr]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1"&gt;Tamara L. Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Into Summarization Techniques for IoT Data Discovery Routing. (arXiv:2107.09558v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2107.09558</id>
        <link href="http://arxiv.org/abs/2107.09558"/>
        <updated>2021-07-21T02:01:33.669Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider the IoT data discovery data objects to specific
nodes in the network. They are very problem in very large and growing scale
networks. Specifically, we investigate in depth the routing table summarization
techniques to support effective and space-efficient IoT data discovery routing.
Novel summarization algorithms, including alphabetical based, hash based, and
meaning based summarization and their corresponding coding schemes are
proposed. The issue of potentially misleading routing due to summarization is
also investigated. Subsequently, we analyze the strategy of when to summarize
in order to balance the tradeoff especially in handling MAA based lookups.
between the routing table compression rate and the chance of Unstructured
discovery routing approaches, such as [4] [5], causing misleading routing. For
experimental study, we have collected 100K IoT data streams from various IoT
databases as the input dataset. Experimental results show that our
summarization solution can reduce the routing table size by 20 to 30 folds with
2-5% increase in latency when compared with similar peer-to-peer discovery
routing algorithms without summarization. Also, our approach outperforms DHT
based approaches by 2 to 6 folds in terms of latency and traffic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1"&gt;Hieu Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1"&gt;Son Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yen_I/0/1/0/all/0/1"&gt;I-Ling Yen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastani_F/0/1/0/all/0/1"&gt;Farokh Bastani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoningKnife: Joint Entity Mention Detection and Typing for Nested NER via prior Boundary Knowledge. (arXiv:2107.09429v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09429</id>
        <link href="http://arxiv.org/abs/2107.09429"/>
        <updated>2021-07-21T02:01:33.660Z</updated>
        <summary type="html"><![CDATA[While named entity recognition (NER) is a key task in natural language
processing, most approaches only target flat entities, ignoring nested
structures which are common in many scenarios. Most existing nested NER methods
traverse all sub-sequences which is both expensive and inefficient, and also
don't well consider boundary knowledge which is significant for nested
entities. In this paper, we propose a joint entity mention detection and typing
model via prior boundary knowledge (BoningKnife) to better handle nested NER
extraction and recognition tasks. BoningKnife consists of two modules,
MentionTagger and TypeClassifier. MentionTagger better leverages boundary
knowledge beyond just entity start/end to improve the handling of nesting
levels and longer spans, while generating high quality mention candidates.
TypeClassifier utilizes a two-level attention mechanism to decouple different
nested level representations and better distinguish entity types. We jointly
train both modules sharing a common representation and a new dual-info
attention layer, which leads to improved representation focus on entity-related
information. Experiments over different datasets show that our approach
outperforms previous state of the art methods and achieves 86.41, 85.46, and
94.2 F1 scores on ACE2004, ACE2005, and NNE, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Huiqiang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guoxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weile Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chengxi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1"&gt;B&amp;#xf6;rje F. Karlsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seed Words Based Data Selection for Language Model Adaptation. (arXiv:2107.09433v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09433</id>
        <link href="http://arxiv.org/abs/2107.09433"/>
        <updated>2021-07-21T02:01:33.651Z</updated>
        <summary type="html"><![CDATA[We address the problem of language model customization in applications where
the ASR component needs to manage domain-specific terminology; although current
state-of-the-art speech recognition technology provides excellent results for
generic domains, the adaptation to specialized dictionaries or glossaries is
still an open issue. In this work we present an approach for automatically
selecting sentences, from a text corpus, that match, both semantically and
morphologically, a glossary of terms (words or composite words) furnished by
the user. The final goal is to rapidly adapt the language model of an hybrid
ASR system with a limited amount of in-domain text data in order to
successfully cope with the linguistic domain at hand; the vocabulary of the
baseline model is expanded and tailored, reducing the resulting OOV rate. Data
selection strategies based on shallow morphological seeds and semantic
similarity viaword2vec are introduced and discussed; the experimental setting
consists in a simultaneous interpreting scenario, where ASRs in three languages
are designed to recognize the domain-specific terms (i.e. dentistry). Results
using different metrics (OOV rate, WER, precision and recall) show the
effectiveness of the proposed techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gretter_R/0/1/0/all/0/1"&gt;Roberto Gretter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matassoni_M/0/1/0/all/0/1"&gt;Marco Matassoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falavigna_D/0/1/0/all/0/1"&gt;Daniele Falavigna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Prosody Modeling for ASR+TTS based Voice Conversion. (arXiv:2107.09477v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.09477</id>
        <link href="http://arxiv.org/abs/2107.09477"/>
        <updated>2021-07-21T02:01:33.628Z</updated>
        <summary type="html"><![CDATA[In voice conversion (VC), an approach showing promising results in the latest
voice conversion challenge (VCC) 2020 is to first use an automatic speech
recognition (ASR) model to transcribe the source speech into the underlying
linguistic contents; these are then used as input by a text-to-speech (TTS)
system to generate the converted speech. Such a paradigm, referred to as
ASR+TTS, overlooks the modeling of prosody, which plays an important role in
speech naturalness and conversion similarity. Although some researchers have
considered transferring prosodic clues from the source speech, there arises a
speaker mismatch during training and conversion. To address this issue, in this
work, we propose to directly predict prosody from the linguistic representation
in a target-speaker-dependent manner, referred to as target text prediction
(TTP). We evaluate both methods on the VCC2020 benchmark and consider different
linguistic representations. The results demonstrate the effectiveness of TTP in
both objective and subjective evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wen-Chin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1"&gt;Tomoki Hayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinjian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset. (arXiv:2107.09556v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09556</id>
        <link href="http://arxiv.org/abs/2107.09556"/>
        <updated>2021-07-21T02:01:33.596Z</updated>
        <summary type="html"><![CDATA[We present a new dataset of Wikipedia articles each paired with a knowledge
graph, to facilitate the research in conditional text generation, graph
generation and graph representation learning. Existing graph-text paired
datasets typically contain small graphs and short text (1 or few sentences),
thus limiting the capabilities of the models that can be learned on the data.
Our new dataset WikiGraphs is collected by pairing each Wikipedia article from
the established WikiText-103 benchmark (Merity et al., 2016) with a subgraph
from the Freebase knowledge graph (Bollacker et al., 2008). This makes it easy
to benchmark against other state-of-the-art text generative models that are
capable of generating long paragraphs of coherent text. Both the graphs and the
text data are of significantly larger scale compared to prior graph-text paired
datasets. We present baseline graph neural network and transformer model
results on our dataset for 3 tasks: graph -> text generation, graph -> text
retrieval and text -> graph retrieval. We show that better conditioning on the
graph provides gains in generation and retrieval quality but there is still
large room for improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Luyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yujia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aslan_O/0/1/0/all/0/1"&gt;Ozlem Aslan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Sentence-Level Relation Extraction through Curriculum Learning. (arXiv:2107.09332v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09332</id>
        <link href="http://arxiv.org/abs/2107.09332"/>
        <updated>2021-07-21T02:01:33.587Z</updated>
        <summary type="html"><![CDATA[The sentence-level relation extraction mainly aims to classify the relation
between two entities in a sentence. The sentence-level relation extraction
corpus is often containing data of difficulty for the model to infer or noise
data. In this paper, we propose a curriculum learning-based relation extraction
model that split data by difficulty and utilize it for learning. In the
experiments with the representative sentence-level relation extraction
datasets, TACRED and Re-TACRED, the proposed method showed good performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seongsik Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Harksoo Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Paraphrasing via Ranking Many Candidates. (arXiv:2107.09274v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09274</id>
        <link href="http://arxiv.org/abs/2107.09274"/>
        <updated>2021-07-21T02:01:33.251Z</updated>
        <summary type="html"><![CDATA[We present a simple and effective way to generate a variety of paraphrases
and find a good quality paraphrase among them. As in previous studies, it is
difficult to ensure that one generation method always generates the best
paraphrase in various domains. Therefore, we focus on finding the best
candidate from multiple candidates, rather than assuming that there is only one
combination of generative models and decoding options. Our approach shows that
it is easy to apply in various domains and has sufficiently good performance
compared to previous methods. In addition, our approach can be used for data
agumentation that extends the downstream corpus, showing that it can help
improve performance in English and Korean datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joosung Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos. (arXiv:2107.09262v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09262</id>
        <link href="http://arxiv.org/abs/2107.09262"/>
        <updated>2021-07-21T02:01:33.208Z</updated>
        <summary type="html"><![CDATA[Deep learning based visual to sound generation systems essentially need to be
developed particularly considering the synchronicity aspects of visual and
audio features with time. In this research we introduce a novel task of guiding
a class conditioned generative adversarial network with the temporal visual
information of a video input for visual to sound generation task adapting the
synchronicity traits between audio-visual modalities. Our proposed FoleyGAN
model is capable of conditioning action sequences of visual events leading
towards generating visually aligned realistic sound tracks. We expand our
previously proposed Automatic Foley dataset to train with FoleyGAN and evaluate
our synthesized sound through human survey that shows noteworthy (on average
81\%) audio-visual synchronicity performance. Our approach also outperforms in
statistical experiments compared with other baseline models and audio-visual
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1"&gt;Sanchita Ghose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prevost_J/0/1/0/all/0/1"&gt;John J. Prevost&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Streaming of 360 Videos with Perfect, Imperfect, and Unknown FoV Viewing Probabilities in Wireless Networks. (arXiv:2107.09491v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.09491</id>
        <link href="http://arxiv.org/abs/2107.09491"/>
        <updated>2021-07-21T02:01:33.193Z</updated>
        <summary type="html"><![CDATA[This paper investigates adaptive streaming of one or multiple tiled 360
videos from a multi-antenna base station (BS) to one or multiple single-antenna
users, respectively, in a multi-carrier wireless system. We aim to maximize the
video quality while keeping rebuffering time small via encoding rate adaptation
at each group of pictures (GOP) and transmission adaptation at each
(transmission) slot. To capture the impact of field-of-view (FoV) prediction,
we consider three cases of FoV viewing probability distributions, i.e.,
perfect, imperfect, and unknown FoV viewing probability distributions, and use
the average total utility, worst average total utility, and worst total utility
as the respective performance metrics. In the single-user scenario, we optimize
the encoding rates of the tiles, encoding rates of the FoVs, and transmission
beamforming vectors for all subcarriers to maximize the total utility in each
case. In the multi-user scenario, we adopt rate splitting with successive
decoding and optimize the encoding rates of the tiles, encoding rates of the
FoVs, rates of the common and private messages, and transmission beamforming
vectors for all subcarriers to maximize the total utility in each case. Then,
we separate the challenging optimization problem into multiple tractable
problems in each scenario. In the single-user scenario, we obtain a globally
optimal solution of each problem using transformation techniques and the
Karush-Kuhn-Tucker (KKT) conditions. In the multi-user scenario, we obtain a
KKT point of each problem using the concave-convex procedure (CCCP). Finally,
numerical results demonstrate that the proposed solutions achieve notable gains
over existing schemes in all three cases. To the best of our knowledge, this is
the first work revealing the impact of FoV prediction on the performance of
adaptive streaming of tiled 360 videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Lingzhi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Ying Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunfei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sheng Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion. (arXiv:2107.09293v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09293</id>
        <link href="http://arxiv.org/abs/2107.09293"/>
        <updated>2021-07-21T02:01:33.170Z</updated>
        <summary type="html"><![CDATA[We propose an audio-driven talking-head method to generate photo-realistic
talking-head videos from a single reference image. In this work, we tackle two
key challenges: (i) producing natural head motions that match speech prosody,
and (ii) maintaining the appearance of a speaker in a large head motion while
stabilizing the non-face regions. We first design a head pose predictor by
modeling rigid 6D head movements with a motion-aware recurrent neural network
(RNN). In this way, the predicted head poses act as the low-frequency holistic
movements of a talking head, thus allowing our latter network to focus on
detailed facial movement generation. To depict the entire image motions arising
from audio, we exploit a keypoint based dense motion field representation.
Then, we develop a motion field generator to produce the dense motion fields
from input audio, head poses, and a reference image. As this keypoint based
representation models the motions of facial regions, head, and backgrounds
integrally, our method can better constrain the spatial and temporal
consistency of the generated videos. Finally, an image generation network is
employed to render photo-realistic talking-head videos from the estimated
keypoint based motion fields and the input reference image. Extensive
experiments demonstrate that our method produces videos with plausible head
motions, synchronized facial expressions, and stable backgrounds and
outperforms the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Suzhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lincheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yu Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequence Model with Self-Adaptive Sliding Window for Efficient Spoken Document Segmentation. (arXiv:2107.09278v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09278</id>
        <link href="http://arxiv.org/abs/2107.09278"/>
        <updated>2021-07-21T02:01:33.158Z</updated>
        <summary type="html"><![CDATA[Transcripts generated by automatic speech recognition (ASR) systems for
spoken documents lack structural annotations such as paragraphs, significantly
reducing their readability. Automatically predicting paragraph segmentation for
spoken documents may both improve readability and downstream NLP performance
such as summarization and machine reading comprehension. We propose a sequence
model with self-adaptive sliding window for accurate and efficient paragraph
segmentation. We also propose an approach to exploit phonetic information,
which significantly improves robustness of spoken document segmentation to ASR
errors. Evaluations are conducted on the English Wiki-727K document
segmentation benchmark, a Chinese Wikipedia-based document segmentation dataset
we created, and an in-house Chinese spoken document dataset. Our proposed model
outperforms the state-of-the-art (SOTA) model based on the same BERT-Base,
increasing segmentation F1 on the English benchmark by 4.2 points and on
Chinese datasets by 4.3-10.1 points, while reducing inference time to less than
1/6 of inference time of the current SOTA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinglin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yali Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stock price prediction using BERT and GAN. (arXiv:2107.09055v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2107.09055</id>
        <link href="http://arxiv.org/abs/2107.09055"/>
        <updated>2021-07-21T02:01:33.143Z</updated>
        <summary type="html"><![CDATA[The stock market has been a popular topic of interest in the recent past. The
growth in the inflation rate has compelled people to invest in the stock and
commodity markets and other areas rather than saving. Further, the ability of
Deep Learning models to make predictions on the time series data has been
proven time and again. Technical analysis on the stock market with the help of
technical indicators has been the most common practice among traders and
investors. One more aspect is the sentiment analysis - the emotion of the
investors that shows the willingness to invest. A variety of techniques have
been used by people around the globe involving basic Machine Learning and
Neural Networks. Ranging from the basic linear regression to the advanced
neural networks people have experimented with all possible techniques to
predict the stock market. It's evident from recent events how news and
headlines affect the stock markets and cryptocurrencies. This paper proposes an
ensemble of state-of-the-art methods for predicting stock prices. Firstly
sentiment analysis of the news and the headlines for the company Apple Inc,
listed on the NASDAQ is performed using a version of BERT, which is a
pre-trained transformer model by Google for Natural Language Processing (NLP).
Afterward, a Generative Adversarial Network (GAN) predicts the stock price for
Apple Inc using the technical indicators, stock indexes of various countries,
some commodities, and historical prices along with the sentiment scores.
Comparison is done with baseline models like - Long Short Term Memory (LSTM),
Gated Recurrent Units (GRU), vanilla GAN, and Auto-Regressive Integrated Moving
Average (ARIMA) model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Sonkiya_P/0/1/0/all/0/1"&gt;Priyank Sonkiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Bajpai_V/0/1/0/all/0/1"&gt;Vikas Bajpai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Bansal_A/0/1/0/all/0/1"&gt;Anukriti Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separating Skills and Concepts for Novel Visual Question Answering. (arXiv:2107.09106v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.09106</id>
        <link href="http://arxiv.org/abs/2107.09106"/>
        <updated>2021-07-21T02:01:33.123Z</updated>
        <summary type="html"><![CDATA[Generalization to out-of-distribution data has been a problem for Visual
Question Answering (VQA) models. To measure generalization to novel questions,
we propose to separate them into "skills" and "concepts". "Skills" are visual
tasks, such as counting or attribute recognition, and are applied to "concepts"
mentioned in the question, such as objects and people. VQA methods should be
able to compose skills and concepts in novel ways, regardless of whether the
specific composition has been seen in training, yet we demonstrate that
existing models have much to improve upon towards handling new compositions. We
present a novel method for learning to compose skills and concepts that
separates these two factors implicitly within a model by learning grounded
concept representations and disentangling the encoding of skills from that of
concepts. We enforce these properties with a novel contrastive learning
procedure that does not rely on external annotations and can be learned from
unlabeled image-question pairs. Experiments demonstrate the effectiveness of
our approach for improving compositional and grounding performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1"&gt;Spencer Whitehead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Lingual BERT Contextual Embedding Space Mapping with Isotropic and Isometric Conditions. (arXiv:2107.09186v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09186</id>
        <link href="http://arxiv.org/abs/2107.09186"/>
        <updated>2021-07-21T02:01:33.080Z</updated>
        <summary type="html"><![CDATA[Typically, a linearly orthogonal transformation mapping is learned by
aligning static type-level embeddings to build a shared semantic space. In view
of the analysis that contextual embeddings contain richer semantic features, we
investigate a context-aware and dictionary-free mapping approach by leveraging
parallel corpora. We illustrate that our contextual embedding space mapping
significantly outperforms previous multilingual word embedding methods on the
bilingual dictionary induction (BDI) task by providing a higher degree of
isomorphism. To improve the quality of mapping, we also explore sense-level
embeddings that are split from type-level representations, which can align
spaces in a finer resolution and yield more precise mapping. Moreover, we
reveal that contextual embedding spaces suffer from their natural properties --
anisotropy and anisometry. To mitigate these two problems, we introduce the
iterative normalization algorithm as an imperative preprocessing step. Our
findings unfold the tight relationship between isotropy, isometry, and
isomorphism in normalized contextual embedding spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haoran Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1"&gt;Philipp Koehn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Video Transformer: Can Objects be the Words?. (arXiv:2107.09240v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.09240</id>
        <link href="http://arxiv.org/abs/2107.09240"/>
        <updated>2021-07-21T02:01:33.069Z</updated>
        <summary type="html"><![CDATA[Transformers have been successful for many natural language processing tasks.
However, applying transformers to the video domain for tasks such as long-term
video generation and scene understanding has remained elusive due to the high
computational complexity and the lack of natural tokenization. In this paper,
we propose the Object-Centric Video Transformer (OCVT) which utilizes an
object-centric approach for decomposing scenes into tokens suitable for use in
a generative video transformer. By factoring the video into objects, our fully
unsupervised model is able to learn complex spatio-temporal dynamics of
multiple interacting objects in a scene and generate future frames of the
video. Our model is also significantly more memory-efficient than pixel-based
models and thus able to train on videos of length up to 70 frames with a single
48GB GPU. We compare our model with previous RNN-based approaches as well as
other possible video transformer baselines. We demonstrate OCVT performs well
when compared to baselines in generating future frames. OCVT also develops
useful representations for video reasoning, achieving start-of-the-art
performance on the CATER task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi-Fu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jaesik Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sungjin Ahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dance2Music: Automatic Dance-driven Music Generation. (arXiv:2107.06252v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06252</id>
        <link href="http://arxiv.org/abs/2107.06252"/>
        <updated>2021-07-21T02:01:33.048Z</updated>
        <summary type="html"><![CDATA[Dance and music typically go hand in hand. The complexities in dance, music,
and their synchronisation make them fascinating to study from a computational
creativity perspective. While several works have looked at generating dance for
a given music, automatically generating music for a given dance remains
under-explored. This capability could have several creative expression and
entertainment applications. We present some early explorations in this
direction. We present a search-based offline approach that generates music
after processing the entire dance video and an online approach that uses a deep
neural network to generate music on-the-fly as the video proceeds. We compare
these approaches to a strong heuristic baseline via human studies and present
our findings. We have integrated our online approach in a live demo! A video of
the demo can be found here:
https://sites.google.com/view/dance2music/live-demo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1"&gt;Gunjan Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Abstructions: Abstractions that Support Construction for Grounded Language Learning. (arXiv:2107.09285v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09285</id>
        <link href="http://arxiv.org/abs/2107.09285"/>
        <updated>2021-07-21T02:01:33.033Z</updated>
        <summary type="html"><![CDATA[Although virtual agents are increasingly situated in environments where
natural language is the most effective mode of interaction with humans, these
exchanges are rarely used as an opportunity for learning. Leveraging language
interactions effectively requires addressing limitations in the two most common
approaches to language grounding: semantic parsers built on top of fixed object
categories are precise but inflexible and end-to-end models are maximally
expressive, but fickle and opaque. Our goal is to develop a system that
balances the strengths of each approach so that users can teach agents new
instructions that generalize broadly from a single example. We introduce the
idea of neural abstructions: a set of constraints on the inference procedure of
a label-conditioned generative model that can affect the meaning of the label
in context. Starting from a core programming language that operates over
abstructions, users can define increasingly complex mappings from natural
language to actions. We show that with this method a user population is able to
build a semantic parser for an open-ended house modification task in Minecraft.
The semantic parser that results is both flexible and expressive: the
percentage of utterances sourced from redefinitions increases steadily over the
course of 191 total exchanges, achieving a final value of 28%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burns_K/0/1/0/all/0/1"&gt;Kaylee Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1"&gt;Christopher D. Manning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Token-Level Supervised Contrastive Learning for Punctuation Restoration. (arXiv:2107.09099v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.09099</id>
        <link href="http://arxiv.org/abs/2107.09099"/>
        <updated>2021-07-21T02:01:33.001Z</updated>
        <summary type="html"><![CDATA[Punctuation is critical in understanding natural language text. Currently,
most automatic speech recognition (ASR) systems do not generate punctuation,
which affects the performance of downstream tasks, such as intent detection and
slot filling. This gives rise to the need for punctuation restoration. Recent
work in punctuation restoration heavily utilizes pre-trained language models
without considering data imbalance when predicting punctuation classes. In this
work, we address this problem by proposing a token-level supervised contrastive
learning method that aims at maximizing the distance of representation of
different punctuation marks in the embedding space. The result shows that
training with token-level supervised contrastive learning obtains up to 3.2%
absolute F1 improvement on the test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qiushi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1"&gt;Tom Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;H Lilian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xubo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bo Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TUTA: Tree-based Transformers for Generally Structured Table Pre-training. (arXiv:2010.12537v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12537</id>
        <link href="http://arxiv.org/abs/2010.12537"/>
        <updated>2021-07-21T02:01:32.983Z</updated>
        <summary type="html"><![CDATA[Tables are widely used with various structures to organize and present data.
Recent attempts on table understanding mainly focus on relational tables, yet
overlook to other common table structures. In this paper, we propose TUTA, a
unified pre-training architecture for understanding generally structured
tables. Noticing that understanding a table requires spatial, hierarchical, and
semantic information, we enhance transformers with three novel structure-aware
mechanisms. First, we devise a unified tree-based structure, called a
bi-dimensional coordinate tree, to describe both the spatial and hierarchical
information of generally structured tables. Upon this, we propose tree-based
attention and position embedding to better capture the spatial and hierarchical
information. Moreover, we devise three progressive pre-training objectives to
enable representations at the token, cell, and table levels. We pre-train TUTA
on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two
critical tasks in the field of table structure understanding: cell type
classification and table type classification. Experiments show that TUTA is
highly effective, achieving state-of-the-art on five widely-studied datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiruo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Haoyu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ran Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhiyi Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Sorted Table Search and Static Indexes in Small Space: Methodological and Practical Insights via an Experimental Study. (arXiv:2107.09480v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.09480</id>
        <link href="http://arxiv.org/abs/2107.09480"/>
        <updated>2021-07-21T02:01:32.669Z</updated>
        <summary type="html"><![CDATA[Sorted Table Search Procedures are the quintessential query-answering tool,
still very useful, e.g, Search Engines (Google Chrome). Speeding them up, in
small additional space with respect to the table being searched into, is still
a quite significant achievement. Static Learned Indexes have been very
successful in achieving such a speed-up, but leave open a major question: To
what extent one can enjoy the speed-up of Learned Indexes while using constant
or nearly constant additional space. By generalizing the experimental
methodology of a recent benchmarking study on Learned Indexes, we shed light on
this question, by considering two scenarios. The first, quite elementary, i.e.,
textbook code, and the second using advanced Learned Indexing algorithms and
the supporting sophisticated software platforms. Although in both cases one
would expect a positive answer, its achievement is not as simple as it seems.
Indeed, our extensive set of experiments reveal a complex relationship between
query time and model space. The findings regarding this relationship and the
corresponding quantitative estimates, across memory levels, can be of interest
to algorithm designers and of use to practitioners as well. As an essential
part of our research, we introduce two new models that are of interest in their
own right. The first is a constant space model that can be seen as a
generalization of $k$-ary search, while the second is a synoptic {\bf RMI}, in
which we can control model space usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amato_D/0/1/0/all/0/1"&gt;Domenico Amato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giancarlo_R/0/1/0/all/0/1"&gt;Raffaele Giancarlo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosco_G/0/1/0/all/0/1"&gt;Giosu&amp;#xe8; Lo Bosco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Design of Grating Couplers Using the Policy Gradient Method from Reinforcement Learning. (arXiv:2107.00088v2 [physics.comp-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00088</id>
        <link href="http://arxiv.org/abs/2107.00088"/>
        <updated>2021-07-20T02:04:49.165Z</updated>
        <summary type="html"><![CDATA[We present a proof-of-concept technique for the inverse design of
electromagnetic devices motivated by the policy gradient method in
reinforcement learning, named PHORCED (PHotonic Optimization using REINFORCE
Criteria for Enhanced Design). This technique uses a probabilistic generative
neural network interfaced with an electromagnetic solver to assist in the
design of photonic devices, such as grating couplers. We show that PHORCED
obtains better performing grating coupler designs than local gradient-based
inverse design via the adjoint method, while potentially providing faster
convergence over competing state-of-the-art generative methods. Furthermore, we
implement transfer learning with PHORCED, demonstrating that a neural network
trained to optimize 8$^\circ$ grating couplers can then be re-trained on
grating couplers with alternate scattering angles while requiring >$10\times$
fewer simulations than control cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Hooten_S/0/1/0/all/0/1"&gt;Sean Hooten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Beausoleil_R/0/1/0/all/0/1"&gt;Raymond G. Beausoleil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Vaerenbergh_T/0/1/0/all/0/1"&gt;Thomas Van Vaerenbergh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Likelihood-Free Frequentist Inference: Bridging Classical Statistics and Machine Learning in Simulation and Uncertainty Quantification. (arXiv:2107.03920v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03920</id>
        <link href="http://arxiv.org/abs/2107.03920"/>
        <updated>2021-07-20T02:04:49.148Z</updated>
        <summary type="html"><![CDATA[Many areas of science make extensive use of computer simulators that
implicitly encode likelihood functions of complex systems. Classical
statistical methods are poorly suited for these so-called likelihood-free
inference (LFI) settings, outside the asymptotic and low-dimensional regimes.
Although new machine learning methods, such as normalizing flows, have
revolutionized the sample efficiency and capacity of LFI methods, it remains an
open question whether they produce reliable measures of uncertainty. This paper
presents a statistical framework for LFI that unifies classical statistics with
modern machine learning to: (1) efficiently construct frequentist confidence
sets and hypothesis tests with finite-sample guarantees of nominal coverage
(type I error control) and power; (2) provide practical diagnostics for
assessing empirical coverage over the entire parameter space. We refer to our
framework as likelihood-free frequentist inference (LF2I). Any method that
estimates a test statistic, like the likelihood ratio, can be plugged into our
framework to create valid confidence sets and compute diagnostics, without
costly Monte Carlo samples at fixed parameter settings. In this work, we
specifically study the power of two test statistics (ACORE and BFF), which,
respectively, maximize versus integrate an odds function over the parameter
space. Our study offers multifaceted perspectives on the challenges in LF2I.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dalmasso_N/0/1/0/all/0/1"&gt;Niccol&amp;#xf2; Dalmasso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1"&gt;David Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1"&gt;Rafael Izbicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1"&gt;Ann B. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Reinforcement Learning Using Advantage-Based Intervention. (arXiv:2106.09110v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09110</id>
        <link href="http://arxiv.org/abs/2106.09110"/>
        <updated>2021-07-20T02:04:49.130Z</updated>
        <summary type="html"><![CDATA[Many sequential decision problems involve finding a policy that maximizes
total reward while obeying safety constraints. Although much recent research
has focused on the development of safe reinforcement learning (RL) algorithms
that produce a safe policy after training, ensuring safety during training as
well remains an open problem. A fundamental challenge is performing exploration
while still satisfying constraints in an unknown Markov decision process (MDP).
In this work, we address this problem for the chance-constrained setting. We
propose a new algorithm, SAILR, that uses an intervention mechanism based on
advantage functions to keep the agent safe throughout training and optimizes
the agent's policy using off-the-shelf RL algorithms designed for unconstrained
MDPs. Our method comes with strong guarantees on safety during both training
and deployment (i.e., after training and without the intervention mechanism)
and policy performance compared to the optimal safety-constrained policy. In
our experiments, we show that SAILR violates constraints far less during
training than standard safe RL and constrained MDP approaches and converges to
a well-performing policy that can be deployed safely without intervention. Our
code is available at https://github.com/nolanwagener/safe_rl.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagener_N/0/1/0/all/0/1"&gt;Nolan Wagener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1"&gt;Byron Boots&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Ching-An Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Path Length Bounds for Gradient Descent and Flow. (arXiv:1908.01089v4 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/1908.01089</id>
        <link href="http://arxiv.org/abs/1908.01089"/>
        <updated>2021-07-20T02:04:49.113Z</updated>
        <summary type="html"><![CDATA[We derive bounds on the path length $\zeta$ of gradient descent (GD) and
gradient flow (GF) curves for various classes of smooth convex and nonconvex
functions. Among other results, we prove that: (a) if the iterates are linearly
convergent with factor $(1-c)$, then $\zeta$ is at most $\mathcal{O}(1/c)$; (b)
under the Polyak-Kurdyka-Lojasiewicz (PKL) condition, $\zeta$ is at most
$\mathcal{O}(\sqrt{\kappa})$, where $\kappa$ is the condition number, and at
least $\widetilde\Omega(\sqrt{d} \wedge \kappa^{1/4})$; (c) for quadratics,
$\zeta$ is $\Theta(\min\{\sqrt{d},\sqrt{\log \kappa}\})$ and in some cases can
be independent of $\kappa$; (d) assuming just convexity, $\zeta$ can be at most
$2^{4d\log d}$; (e) for separable quasiconvex functions, $\zeta$ is
${\Theta}(\sqrt{d})$. Thus, we advance current understanding of the properties
of GD and GF curves beyond rates of convergence. We expect our techniques to
facilitate future studies for other algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1"&gt;Chirag Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_S/0/1/0/all/0/1"&gt;Sivaraman Balakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Counterfactual Explanations on Graph Neural Networks. (arXiv:2107.04086v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04086</id>
        <link href="http://arxiv.org/abs/2107.04086"/>
        <updated>2021-07-20T02:04:49.068Z</updated>
        <summary type="html"><![CDATA[Massive deployment of Graph Neural Networks (GNNs) in high-stake applications
generates a strong demand for explanations that are robust to noise and align
well with human intuition. Most existing methods generate explanations by
identifying a subgraph of an input graph that has a strong correlation with the
prediction. These explanations are not robust to noise because independently
optimizing the correlation for a single input can easily overfit noise.
Moreover, they do not align well with human intuition because removing an
identified subgraph from an input graph does not necessarily change the
prediction result. In this paper, we propose a novel method to generate robust
counterfactual explanations on GNNs by explicitly modelling the common decision
logic of GNNs on similar input graphs. Our explanations are naturally robust to
noise because they are produced from the common decision boundaries of a GNN
that govern the predictions of many similar input graphs. The explanations also
align well with human intuition because removing the set of edges identified by
an explanation from the input graph changes the prediction significantly.
Exhaustive experiments on many public datasets demonstrate the superior
performance of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bajaj_M/0/1/0/all/0/1"&gt;Mohit Bajaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1"&gt;Lingyang Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zi Yu Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1"&gt;Jian Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lanjun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_P/0/1/0/all/0/1"&gt;Peter Cho-Ho Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ill-posed Surface Emissivity Retrieval from Multi-Geometry Hyperspectral Images using a Hybrid Deep Neural Network. (arXiv:2107.04631v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04631</id>
        <link href="http://arxiv.org/abs/2107.04631"/>
        <updated>2021-07-20T02:04:49.049Z</updated>
        <summary type="html"><![CDATA[Atmospheric correction is a fundamental task in remote sensing because
observations are taken either of the atmosphere or looking through the
atmosphere. Atmospheric correction errors can significantly alter the spectral
signature of the observations, and lead to invalid classifications or target
detection. This is even more crucial when working with hyperspectral data,
where a precise measurement of spectral properties is required.
State-of-the-art physics-based atmospheric correction approaches require
extensive prior knowledge about sensor characteristics, collection geometry,
and environmental characteristics of the scene being collected. These
approaches are computationally expensive, prone to inaccuracy due to lack of
sufficient environmental and collection information, and often impossible for
real-time applications. In this paper, a geometry-dependent hybrid neural
network is proposed for automatic atmospheric correction using multi-scan
hyperspectral data collected from different geometries. The proposed network
can characterize the atmosphere without any additional meteorological data. A
grid-search method is also proposed to solve the temperature emissivity
separation problem. Results show that the proposed network has the capacity to
accurately characterize the atmosphere and estimate target emissivity spectra
with a Mean Absolute Error (MAE) under 0.02 for 29 different materials. This
solution can lead to accurate atmospheric correction to improve target
detection for real time applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_F/0/1/0/all/0/1"&gt;Fangcao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cervone_G/0/1/0/all/0/1"&gt;Guido Cervone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salvador_M/0/1/0/all/0/1"&gt;Mark Salvador&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANs for Medical Image Synthesis: An Empirical Study. (arXiv:2105.05318v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05318</id>
        <link href="http://arxiv.org/abs/2105.05318"/>
        <updated>2021-07-20T02:04:49.029Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have become increasingly powerful,
generating mind-blowing photorealistic images that mimic the content of
datasets they were trained to replicate. One recurrent theme in medical imaging
is whether GANs can also be effective at generating workable medical data as
they are for generating realistic RGB images. In this paper, we perform a
multi-GAN and multi-application study to gauge the benefits of GANs in medical
imaging. We tested various GAN architectures from basic DCGAN to more
sophisticated style-based GANs on three medical imaging modalities and organs
namely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on
well-known and widely utilized datasets from which their FID score were
computed to measure the visual acuity of their generated images. We further
tested their usefulness by measuring the segmentation accuracy of a U-Net
trained on these generated images.

Results reveal that GANs are far from being equal as some are ill-suited for
medical imaging applications while others are much better off. The
top-performing GANs are capable of generating realistic-looking medical images
by FID standards that can fool trained experts in a visual Turing test and
comply to some metrics. However, segmentation results suggests that no GAN is
capable of reproducing the full richness of a medical datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Skandarani_Y/0/1/0/all/0/1"&gt;Youssef Skandarani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1"&gt;Pierre-Marc Jodoin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lalande_A/0/1/0/all/0/1"&gt;Alain Lalande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Hierarchical Graph Neural Networks for Image Clustering. (arXiv:2107.01319v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01319</id>
        <link href="http://arxiv.org/abs/2107.01319"/>
        <updated>2021-07-20T02:04:49.013Z</updated>
        <summary type="html"><![CDATA[We propose a hierarchical graph neural network (GNN) model that learns how to
cluster a set of images into an unknown number of identities using a training
set of images annotated with labels belonging to a disjoint set of identities.
Our hierarchical GNN uses a novel approach to merge connected components
predicted at each level of the hierarchy to form a new graph at the next level.
Unlike fully unsupervised hierarchical clustering, the choice of grouping and
complexity criteria stems naturally from supervision in the training set. The
resulting method, Hi-LANDER, achieves an average of 54% improvement in F-score
and 8% increase in Normalized Mutual Information (NMI) relative to current
GNN-based clustering algorithms. Additionally, state-of-the-art GNN-based
methods rely on separate models to predict linkage probabilities and node
densities as intermediate steps of the clustering process. In contrast, our
unified framework achieves a seven-fold decrease in computational cost. We
release our training and inference code at
https://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1"&gt;Yifan Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1"&gt;Tianjun Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yuanjun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1"&gt;Wei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1"&gt;David Wipf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution-free binary classification: prediction sets, confidence intervals and calibration. (arXiv:2006.10564v3 [stat.ML] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2006.10564</id>
        <link href="http://arxiv.org/abs/2006.10564"/>
        <updated>2021-07-20T02:04:48.995Z</updated>
        <summary type="html"><![CDATA[We study three notions of uncertainty quantification -- calibration,
confidence intervals and prediction sets -- for binary classification in the
distribution-free setting, that is without making any distributional
assumptions on the data. With a focus towards calibration, we establish a
'tripod' of theorems that connect these three notions for score-based
classifiers. A direct implication is that distribution-free calibration is only
possible, even asymptotically, using a scoring function whose level sets
partition the feature space into at most countably many sets. Parametric
calibration schemes such as variants of Platt scaling do not satisfy this
requirement, while nonparametric schemes based on binning do. To close the
loop, we derive distribution-free confidence intervals for binned probabilities
for both fixed-width and uniform-mass binning. As a consequence of our 'tripod'
theorems, these confidence intervals for binned probabilities lead to
distribution-free calibration. We also derive extensions to settings with
streaming data and covariate shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gupta_C/0/1/0/all/0/1"&gt;Chirag Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Podkopaev_A/0/1/0/all/0/1"&gt;Aleksandr Podkopaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Data Balancing for Unlabeled Satellite Imagery. (arXiv:2107.03227v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.03227</id>
        <link href="http://arxiv.org/abs/2107.03227"/>
        <updated>2021-07-20T02:04:48.978Z</updated>
        <summary type="html"><![CDATA[Data imbalance is a ubiquitous problem in machine learning. In large scale
collected and annotated datasets, data imbalance is either mitigated manually
by undersampling frequent classes and oversampling rare classes, or planned for
with imputation and augmentation techniques. In both cases balancing data
requires labels. In other words, only annotated data can be balanced.
Collecting fully annotated datasets is challenging, especially for large scale
satellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.
Although the NASA Earth Imagery dataset is unlabeled, there are implicit
properties of the data source that we can rely on to hypothesize about its
imbalance, such as distribution of land and water in the case of the Earth's
imagery. We present a new iterative method to balance unlabeled data. Our
method utilizes image embeddings as a proxy for image labels that can be used
to balance data, and ultimately when trained increases overall accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1"&gt;Deep Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1"&gt;Erin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1"&gt;Anirudh Koul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1"&gt;Meher Anand Kasam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RoFL: Attestable Robustness for Secure Federated Learning. (arXiv:2107.03311v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03311</id>
        <link href="http://arxiv.org/abs/2107.03311"/>
        <updated>2021-07-20T02:04:48.935Z</updated>
        <summary type="html"><![CDATA[Federated Learning is an emerging decentralized machine learning paradigm
that allows a large number of clients to train a joint model without the need
to share their private data. Participants instead only share ephemeral updates
necessary to train the model. To ensure the confidentiality of the client
updates, Federated Learning systems employ secure aggregation; clients encrypt
their gradient updates, and only the aggregated model is revealed to the
server. Achieving this level of data protection, however, presents new
challenges to the robustness of Federated Learning, i.e., the ability to
tolerate failures and attacks. Unfortunately, in this setting, a malicious
client can now easily exert influence on the model behavior without being
detected. As Federated Learning is being deployed in practice in a range of
sensitive applications, its robustness is growing in importance. In this paper,
we take a step towards understanding and improving the robustness of secure
Federated Learning. We start this paper with a systematic study that evaluates
and analyzes existing attack vectors and discusses potential defenses and
assesses their effectiveness. We then present RoFL, a secure Federated Learning
system that improves robustness against malicious clients through input checks
on the encrypted model updates. RoFL extends Federated Learning's secure
aggregation protocol to allow expressing a variety of properties and
constraints on model updates using zero-knowledge proofs. To enable RoFL to
scale to typical Federated Learning settings, we introduce several ML and
cryptographic optimizations specific to Federated Learning. We implement and
evaluate a prototype of RoFL and show that realistic ML models can be trained
in a reasonable time while improving robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burkhalter_L/0/1/0/all/0/1"&gt;Lukas Burkhalter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lycklama_H/0/1/0/all/0/1"&gt;Hidde Lycklama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viand_A/0/1/0/all/0/1"&gt;Alexander Viand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuchler_N/0/1/0/all/0/1"&gt;Nicolas K&amp;#xfc;chler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hithnawi_A/0/1/0/all/0/1"&gt;Anwar Hithnawi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably efficient machine learning for quantum many-body problems. (arXiv:2106.12627v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12627</id>
        <link href="http://arxiv.org/abs/2106.12627"/>
        <updated>2021-07-20T02:04:48.915Z</updated>
        <summary type="html"><![CDATA[Classical machine learning (ML) provides a potentially powerful approach to
solving challenging quantum many-body problems in physics and chemistry.
However, the advantages of ML over more traditional methods have not been
firmly established. In this work, we prove that classical ML algorithms can
efficiently predict ground state properties of gapped Hamiltonians in finite
spatial dimensions, after learning from data obtained by measuring other
Hamiltonians in the same quantum phase of matter. In contrast, under widely
accepted complexity theory assumptions, classical algorithms that do not learn
from data cannot achieve the same guarantee. We also prove that classical ML
algorithms can efficiently classify a wide range of quantum phases of matter.
Our arguments are based on the concept of a classical shadow, a succinct
classical description of a many-body quantum state that can be constructed in
feasible quantum experiments and be used to predict many properties of the
state. Extensive numerical experiments corroborate our theoretical results in a
variety of scenarios, including Rydberg atom systems, 2D random Heisenberg
models, symmetry-protected topological phases, and topologically ordered
phases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hsin-Yuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kueng_R/0/1/0/all/0/1"&gt;Richard Kueng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Torlai_G/0/1/0/all/0/1"&gt;Giacomo Torlai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Albert_V/0/1/0/all/0/1"&gt;Victor V. Albert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Preskill_J/0/1/0/all/0/1"&gt;John Preskill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code Integrity Attestation for PLCs using Black Box Neural Network Predictions. (arXiv:2106.07851v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07851</id>
        <link href="http://arxiv.org/abs/2106.07851"/>
        <updated>2021-07-20T02:04:48.896Z</updated>
        <summary type="html"><![CDATA[Cyber-physical systems (CPSs) are widespread in critical domains, and
significant damage can be caused if an attacker is able to modify the code of
their programmable logic controllers (PLCs). Unfortunately, traditional
techniques for attesting code integrity (i.e. verifying that it has not been
modified) rely on firmware access or roots-of-trust, neither of which
proprietary or legacy PLCs are likely to provide. In this paper, we propose a
practical code integrity checking solution based on privacy-preserving black
box models that instead attest the input/output behaviour of PLC programs.
Using faithful offline copies of the PLC programs, we identify their most
important inputs through an information flow analysis, execute them on multiple
combinations to collect data, then train neural networks able to predict PLC
outputs (i.e. actuator commands) from their inputs. By exploiting the black box
nature of the model, our solution maintains the privacy of the original PLC
code and does not assume that attackers are unaware of its presence. The trust
instead comes from the fact that it is extremely hard to attack the PLC code
and neural networks at the same time and with consistent outcomes. We evaluated
our approach on a modern six-stage water treatment plant testbed, finding that
it could predict actuator states from PLC inputs with near-100% accuracy, and
thus could detect all 120 effective code mutations that we subjected the PLCs
to. Finally, we found that it is not practically possible to simultaneously
modify the PLC code and apply discreet adversarial noise to our attesters in a
way that leads to consistent (mis-)predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poskitt_C/0/1/0/all/0/1"&gt;Christopher M. Poskitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jun Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hack The Box: Fooling Deep Learning Abstraction-Based Monitors. (arXiv:2107.04764v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04764</id>
        <link href="http://arxiv.org/abs/2107.04764"/>
        <updated>2021-07-20T02:04:48.877Z</updated>
        <summary type="html"><![CDATA[Deep learning is a type of machine learning that adapts a deep hierarchy of
concepts. Deep learning classifiers link the most basic version of concepts at
the input layer to the most abstract version of concepts at the output layer,
also known as a class or label. However, once trained over a finite set of
classes, some deep learning models do not have the power to say that a given
input does not belong to any of the classes and simply cannot be linked.
Correctly invalidating the prediction of unrelated classes is a challenging
problem that has been tackled in many ways in the literature. Novelty detection
gives deep learning the ability to output "do not know" for novel/unseen
classes. Still, no attention has been given to the security aspects of novelty
detection. In this paper, we consider the case study of abstraction-based
novelty detection and show that it is not robust against adversarial samples.
Moreover, we show the feasibility of crafting adversarial samples that fool the
deep learning classifier and bypass the novelty detection monitoring at the
same time. In other words, these monitoring boxes are hackable. We demonstrate
that novelty detection itself ends up as an attack surface.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_S/0/1/0/all/0/1"&gt;Sara Hajj Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nassar_M/0/1/0/all/0/1"&gt;Mohamed Nassar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thinking Like Transformers. (arXiv:2106.06981v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06981</id>
        <link href="http://arxiv.org/abs/2106.06981"/>
        <updated>2021-07-20T02:04:48.833Z</updated>
        <summary type="html"><![CDATA[What is the computational model behind a Transformer? Where recurrent neural
networks have direct parallels in finite state machines, allowing clear
discussion and thought around architecture variants or trained models,
Transformers have no such familiar parallel. In this paper we aim to change
that, proposing a computational model for the transformer-encoder in the form
of a programming language. We map the basic components of a transformer-encoder
-- attention and feed-forward computation -- into simple primitives, around
which we form a programming language: the Restricted Access Sequence Processing
Language (RASP). We show how RASP can be used to program solutions to tasks
that could conceivably be learned by a Transformer, and how a Transformer can
be trained to mimic a RASP solution. In particular, we provide RASP programs
for histograms, sorting, and Dyck-languages. We further use our model to relate
their difficulty in terms of the number of required layers and attention heads:
analyzing a RASP program implies a maximum number of heads and layers necessary
to encode a task in a transformer. Finally, we see how insights gained from our
abstraction might be used to explain phenomena seen in recent works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1"&gt;Gail Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1"&gt;Eran Yahav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Optimal Transport Algorithm by Accelerated Gradient descent. (arXiv:2104.05802v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05802</id>
        <link href="http://arxiv.org/abs/2104.05802"/>
        <updated>2021-07-20T02:04:48.814Z</updated>
        <summary type="html"><![CDATA[Optimal transport (OT) plays an essential role in various areas like machine
learning and deep learning. However, computing discrete optimal transport plan
for large scale problems with adequate accuracy and efficiency is still highly
challenging. Recently, methods based on the Sinkhorn algorithm add an entropy
regularizer to the prime problem and get a trade off between efficiency and
accuracy. In this paper, we propose a novel algorithm to further improve the
efficiency and accuracy based on Nesterov's smoothing technique. Basically, the
non-smooth c-transform of the Kantorovich potential is approximated by the
smooth Log-Sum-Exp function, which finally smooths the original non-smooth
Kantorovich dual functional (energy). The smooth Kantorovich functional can be
optimized by the fast proximal gradient algorithm (FISTA) efficiently.
Theoretically, the computational complexity of the proposed method is given by
$O(n^{\frac{5}{2}} \sqrt{\log n} /\epsilon)$, which is lower than that of the
Sinkhorn algorithm. Empirically, compared with the Sinkhorn algorithm, our
experimental results demonstrate that the proposed method achieves faster
convergence and better accuracy with the same parameter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1"&gt;Dongsheng An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_N/0/1/0/all/0/1"&gt;Na Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xianfeng Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization via Inference-time Label-Preserving Target Projections. (arXiv:2103.01134v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01134</id>
        <link href="http://arxiv.org/abs/2103.01134"/>
        <updated>2021-07-20T02:04:48.795Z</updated>
        <summary type="html"><![CDATA[Generalization of machine learning models trained on a set of source domains
on unseen target domains with different statistics, is a challenging problem.
While many approaches have been proposed to solve this problem, they only
utilize source data during training but do not take advantage of the fact that
a single target example is available at the time of inference. Motivated by
this, we propose a method that effectively uses the target sample during
inference beyond mere classification. Our method has three components - (i) A
label-preserving feature or metric transformation on source data such that the
source samples are clustered in accordance with their class irrespective of
their domain (ii) A generative model trained on the these features (iii) A
label-preserving projection of the target point on the source-feature manifold
during inference via solving an optimization problem on the input space of the
generative model using the learned metric. Finally, the projected target is
used in the classifier. Since the projected target feature comes from the
source manifold and has the same label as the real target by design, the
classifier is expected to perform better on it than the true target. We
demonstrate that our method outperforms the state-of-the-art Domain
Generalization methods on multiple datasets and tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1"&gt;Prashant Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1"&gt;Mrigank Raman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varambally_S/0/1/0/all/0/1"&gt;Sumanth Varambally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1"&gt;Prathosh AP&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14399</id>
        <link href="http://arxiv.org/abs/2105.14399"/>
        <updated>2021-07-20T02:04:48.643Z</updated>
        <summary type="html"><![CDATA[Current out-of-distribution detection approaches usually present special
requirements (e.g., collecting outlier data and hyperparameter validation) and
produce side effects (classification accuracy drop and slow/inefficient
inferences). Recently, entropic out-of-distribution detection has been proposed
as a seamless approach (i.e., a solution that avoids all the previously
mentioned drawbacks). The entropic out-of-distribution detection solution
comprises the IsoMax loss for training and the entropic score for
out-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in
replacement because swapping the SoftMax loss with the IsoMax loss requires no
changes in the model's architecture or training procedures/hyperparameters. In
this paper, we propose to perform what we call an isometrization of the
distances used in the IsoMax loss. Additionally, we propose to replace the
entropic score with the minimum distance score. Our experiments showed that
these simple modifications increase out-of-distribution detection performance
while keeping the solution seamless. Code available at
$\href{https://github.com/dlmacedo/entropic-out-of-distribution-detection}{\text{entropic
out-of-distribution detection}}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1"&gt;David Mac&amp;#xea;do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI. (arXiv:2107.08821v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08821</id>
        <link href="http://arxiv.org/abs/2107.08821"/>
        <updated>2021-07-20T02:04:48.623Z</updated>
        <summary type="html"><![CDATA[This is the Proceedings of ICML 2021 Workshop on Theoretic Foundation,
Criticism, and Application Trend of Explainable AI. Deep neural networks (DNNs)
have undoubtedly brought great success to a wide range of applications in
computer vision, computational linguistics, and AI. However, foundational
principles underlying the DNNs' success and their resilience to adversarial
attacks are still largely missing. Interpreting and theorizing the internal
mechanisms of DNNs becomes a compelling yet controversial topic. This workshop
pays a special interest in theoretic foundations, limitations, and new
application trends in the scope of XAI. These issues reflect new bottlenecks in
the future development of XAI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanshi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1"&gt;Tian Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lixin Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhanxing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Ying Nian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jie Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Dynamic Transformer for Text to Speech. (arXiv:2107.08795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08795</id>
        <link href="http://arxiv.org/abs/2107.08795"/>
        <updated>2021-07-20T02:04:48.580Z</updated>
        <summary type="html"><![CDATA[Text to speech (TTS) is a crucial task for user interaction, but TTS model
training relies on a sizable set of high-quality original datasets. Due to
privacy and security issues, the original datasets are usually unavailable
directly. Recently, federated learning proposes a popular distributed machine
learning paradigm with an enhanced privacy protection mechanism. It offers a
practical and secure framework for data owners to collaborate with others, thus
obtaining a better global model trained on the larger dataset. However, due to
the high complexity of transformer models, the convergence process becomes slow
and unstable in the federated learning setting. Besides, the transformer model
trained in federated learning is costly communication and limited computational
speed on clients, impeding its popularity. To deal with these challenges, we
propose the federated dynamic transformer. On the one hand, the performance is
greatly improved comparing with the federated transformer, approaching
centralize-trained Transformer-TTS when increasing clients number. On the other
hand, it achieves faster and more stable convergence in the training phase and
significantly reduces communication time. Experiments on the LJSpeech dataset
also strongly prove our method's advantage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1"&gt;Zhenhou Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianzong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1"&gt;Xiaoyang Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Chendong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jing Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self Training with Ensemble of Teacher Models. (arXiv:2107.08211v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08211</id>
        <link href="http://arxiv.org/abs/2107.08211"/>
        <updated>2021-07-20T02:04:48.562Z</updated>
        <summary type="html"><![CDATA[In order to train robust deep learning models, large amounts of labelled data
is required. However, in the absence of such large repositories of labelled
data, unlabeled data can be exploited for the same. Semi-Supervised learning
aims to utilize such unlabeled data for training classification models. Recent
progress of self-training based approaches have shown promise in this area,
which leads to this study where we utilize an ensemble approach for the same. A
by-product of any semi-supervised approach may be loss of calibration of the
trained model especially in scenarios where unlabeled data may contain
out-of-distribution samples, which leads to this investigation on how to adapt
to such effects. Our proposed algorithm carefully avoids common pitfalls in
utilizing unlabeled data and leads to a more accurate and calibrated supervised
model compared to vanilla self-training based student-teacher algorithms. We
perform several experiments on the popular STL-10 database followed by an
extensive analysis of our approach and study its effects on model accuracy and
calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Soumyadeep Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjay Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_J/0/1/0/all/0/1"&gt;Janu Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Awanish Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time Series Anomaly Detection for Smart Grids: A Survey. (arXiv:2107.08835v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08835</id>
        <link href="http://arxiv.org/abs/2107.08835"/>
        <updated>2021-07-20T02:04:48.542Z</updated>
        <summary type="html"><![CDATA[With the rapid increase in the integration of renewable energy generation and
the wide adoption of various electric appliances, power grids are now faced
with more and more challenges. One prominent challenge is to implement
efficient anomaly detection for different types of anomalous behaviors within
power grids. These anomalous behaviors might be induced by unusual consumption
patterns of the users, faulty grid infrastructures, outages, external
cyberattacks, or energy fraud. Identifying such anomalies is of critical
importance for the reliable and efficient operation of modern power grids.
Various methods have been proposed for anomaly detection on power grid
time-series data. This paper presents a short survey of the recent advances in
anomaly detection for power grid time-series data. Specifically, we first
outline current research challenges in the power grid anomaly detection domain
and further review the major anomaly detection approaches. Finally, we conclude
the survey by identifying the potential directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiuqi/0/1/0/all/0/1"&gt;Jiuqi&lt;/a&gt; (Elise) &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang/0/1/0/all/0/1"&gt;Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Di Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boulet_B/0/1/0/all/0/1"&gt;Benoit Boulet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VolcanoML: Speeding up End-to-End AutoML via Scalable Search Space Decomposition. (arXiv:2107.08861v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08861</id>
        <link href="http://arxiv.org/abs/2107.08861"/>
        <updated>2021-07-20T02:04:48.525Z</updated>
        <summary type="html"><![CDATA[End-to-end AutoML has attracted intensive interests from both academia and
industry, which automatically searches for ML pipelines in a space induced by
feature engineering, algorithm/model selection, and hyper-parameter tuning.
Existing AutoML systems, however, suffer from scalability issues when applying
to application domains with large, high-dimensional search spaces. We present
VolcanoML, a scalable and extensible framework that facilitates systematic
exploration of large AutoML search spaces. VolcanoML introduces and implements
basic building blocks that decompose a large search space into smaller ones,
and allows users to utilize these building blocks to compose an execution plan
for the AutoML problem at hand. VolcanoML further supports a Volcano-style
execution model - akin to the one supported by modern database systems - to
execute the plan constructed. Our evaluation demonstrates that, not only does
VolcanoML raise the level of expressiveness for search space decomposition in
AutoML, it also leads to actual findings of decomposition strategies that are
significantly more efficient than the ones employed by state-of-the-art AutoML
systems such as auto-sklearn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wentao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiawei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1"&gt;Bolin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wentao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Bin Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Reward Shaping for Efficient Exploration in Reinforcement Learning. (arXiv:2107.08888v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08888</id>
        <link href="http://arxiv.org/abs/2107.08888"/>
        <updated>2021-07-20T02:04:48.508Z</updated>
        <summary type="html"><![CDATA[Maintaining long-term exploration ability remains one of the challenges of
deep reinforcement learning (DRL). In practice, the reward shaping-based
approaches are leveraged to provide intrinsic rewards for the agent to
incentivize motivation. However, most existing IRS modules rely on attendant
models or additional memory to record and analyze learning procedures, which
leads to high computational complexity and low robustness. Moreover, they
overemphasize the influence of a single state on exploration, which cannot
evaluate the exploration performance from a global perspective. To tackle the
problem, state entropy-based methods are proposed to encourage the agent to
visit the state space more equitably. However, the estimation error and sample
complexity are prohibitive when handling environments with high-dimensional
observation. In this paper, we introduce a novel metric entitled Jain's
fairness index (JFI) to replace the entropy regularizer, which requires no
additional models or memory. In particular, JFI overcomes the vanishing
intrinsic rewards problem and can be generalized into arbitrary tasks.
Furthermore, we use a variational auto-encoder (VAE) model to capture the
life-long novelty of states. Finally, the global JFI score and local state
novelty are combined to form a multimodal intrinsic reward, controlling the
exploration extent more precisely. Finally, extensive simulation results
demonstrate that our multimodal reward shaping (MMRS) method can achieve higher
performance in contrast to other benchmark schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Mingqi Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pun_M/0/1/0/all/0/1"&gt;Mon-on Pun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haojun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-Parameterization and Generalization in Audio Classification. (arXiv:2107.08933v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.08933</id>
        <link href="http://arxiv.org/abs/2107.08933"/>
        <updated>2021-07-20T02:04:48.490Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) have been dominating classification
tasks in various domains, such as machine vision, machine listening, and
natural language processing. In machine listening, while generally exhibiting
very good generalization capabilities, CNNs are sensitive to the specific audio
recording device used, which has been recognized as a substantial problem in
the acoustic scene classification (DCASE) community. In this study, we
investigate the relationship between over-parameterization of acoustic scene
classification models, and their resulting generalization abilities.
Specifically, we test scaling CNNs in width and depth, under different
conditions. Our results indicate that increasing width improves generalization
to unseen devices, even without an increase in the number of parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koutini_K/0/1/0/all/0/1"&gt;Khaled Koutini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eghbal_zadeh_H/0/1/0/all/0/1"&gt;Hamid Eghbal-zadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henkel_F/0/1/0/all/0/1"&gt;Florian Henkel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_J/0/1/0/all/0/1"&gt;Jan Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1"&gt;Gerhard Widmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Epistemic Neural Networks. (arXiv:2107.08924v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08924</id>
        <link href="http://arxiv.org/abs/2107.08924"/>
        <updated>2021-07-20T02:04:48.445Z</updated>
        <summary type="html"><![CDATA[We introduce the \textit{epistemic neural network} (ENN) as an interface for
uncertainty modeling in deep learning. All existing approaches to uncertainty
modeling can be expressed as ENNs, and any ENN can be identified with a
Bayesian neural network. However, this new perspective provides several
promising directions for future research. Where prior work has developed
probabilistic inference tools for neural networks; we ask instead, `which
neural networks are suitable as tools for probabilistic inference?'. We propose
a clear and simple metric for progress in ENNs: the KL-divergence with respect
to a target distribution. We develop a computational testbed based on inference
in a neural network Gaussian process and release our code as a benchmark at
\url{https://github.com/deepmind/enn}. We evaluate several canonical approaches
to uncertainty modeling in deep learning, and find they vary greatly in their
performance. We provide insight to the sensitivity of these results and show
that our metric is highly correlated with performance in sequential decision
problems. Finally, we provide indications that new ENN architectures can
improve performance in both the statistical quality and computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1"&gt;Ian Osband&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1"&gt;Zheng Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asghari_M/0/1/0/all/0/1"&gt;Mohammad Asghari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahimi_M/0/1/0/all/0/1"&gt;Morteza Ibrahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiyuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Mining: A Novel Training Strategy for Convolutional Neural Network. (arXiv:2107.08421v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08421</id>
        <link href="http://arxiv.org/abs/2107.08421"/>
        <updated>2021-07-20T02:04:48.428Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel training strategy for convolutional neural
network(CNN) named Feature Mining, that aims to strengthen the network's
learning of the local feature. Through experiments, we find that semantic
contained in different parts of the feature is different, while the network
will inevitably lose the local information during feedforward propagation. In
order to enhance the learning of local feature, Feature Mining divides the
complete feature into two complementary parts and reuse these divided feature
to make the network learn more local information, we call the two steps as
feature segmentation and feature reusing. Feature Mining is a parameter-free
method and has plug-and-play nature, and can be applied to any CNN models.
Extensive experiments demonstrate the wide applicability, versatility, and
compatibility of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tianshu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaomin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Minghui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jiali Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the efficacy of neural networks for trajectory compression and the inverse problem. (arXiv:2107.08849v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08849</id>
        <link href="http://arxiv.org/abs/2107.08849"/>
        <updated>2021-07-20T02:04:48.411Z</updated>
        <summary type="html"><![CDATA[In this document, a neural network is employed in order to estimate the
solution of the initial value problem in the context of non linear
trajectories. Such trajectories can be subject to gravity, thrust, drag,
centrifugal force, temperature, ambient air density and pressure. First, we
generate a grid of trajectory points given a specified uniform density as a
design parameter and then we investigate the performance of a neural network in
a compression and inverse problem task: the network is trained to predict the
initial conditions of the dynamics model we used in the simulation, given a
target point in space. We investigate this as a regression task, with error
propagation in consideration. For target points, up to a radius of 2
kilometers, the model is able to accurately predict the initial conditions of
the trajectories, with sub-meter deviation. This simulation-based training
process and novel real-world evaluation method is capable of computing
trajectories of arbitrary dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ntakouris_T/0/1/0/all/0/1"&gt;Theodoros Ntakouris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysing Cyberbullying using Natural Language Processing by Understanding Jargon in Social Media. (arXiv:2107.08902v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08902</id>
        <link href="http://arxiv.org/abs/2107.08902"/>
        <updated>2021-07-20T02:04:48.393Z</updated>
        <summary type="html"><![CDATA[Cyberbullying is of extreme prevalence today. Online-hate comments, toxicity,
cyberbullying amongst children and other vulnerable groups are only growing
over online classes, and increased access to social platforms, especially post
COVID-19. It is paramount to detect and ensure minors' safety across social
platforms so that any violence or hate-crime is automatically detected and
strict action is taken against it. In our work, we explore binary
classification by using a combination of datasets from various social media
platforms that cover a wide range of cyberbullying such as sexism, racism,
abusive, and hate-speech. We experiment through multiple models such as
Bi-LSTM, GloVe, state-of-the-art models like BERT, and apply a unique
preprocessing technique by introducing a slang-abusive corpus, achieving a
higher precision in comparison to models without slang preprocessing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_B/0/1/0/all/0/1"&gt;Bhumika Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1"&gt;Anuj Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anjum/0/1/0/all/0/1"&gt;Anjum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katarya_R/0/1/0/all/0/1"&gt;Rahul Katarya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Face Frontalization via Domain Agnostic Learning. (arXiv:2107.08311v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08311</id>
        <link href="http://arxiv.org/abs/2107.08311"/>
        <updated>2021-07-20T02:04:48.375Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep convolutional neural networks (DCNNs) have shown
impressive performance improvements on thermal to visible face synthesis and
matching problems. However, current DCNN-based synthesis models do not perform
well on thermal faces with large pose variations. In order to deal with this
problem, heterogeneous face frontalization methods are needed in which a model
takes a thermal profile face image and generates a frontal visible face. This
is an extremely difficult problem due to the large domain as well as large pose
discrepancies between the two modalities. Despite its applications in
biometrics and surveillance, this problem is relatively unexplored in the
literature. We propose a domain agnostic learning-based generative adversarial
network (DAL-GAN) which can synthesize frontal views in the visible domain from
thermal faces with pose variations. DAL-GAN consists of a generator with an
auxiliary classifier and two discriminators which capture both local and global
texture discriminations for better synthesis. A contrastive constraint is
enforced in the latent space of the generator with the help of a dual-path
training strategy, which improves the feature vector discrimination. Finally, a
multi-purpose loss function is utilized to guide the network in synthesizing
identity preserving cross-domain frontalization. Extensive experimental results
demonstrate that DAL-GAN can generate better quality frontal views compared to
the other baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1"&gt;Xing Di&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shuowen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RingFed: Reducing Communication Costs in Federated Learning on Non-IID Data. (arXiv:2107.08873v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08873</id>
        <link href="http://arxiv.org/abs/2107.08873"/>
        <updated>2021-07-20T02:04:48.329Z</updated>
        <summary type="html"><![CDATA[Federated learning is a widely used distributed deep learning framework that
protects the privacy of each client by exchanging model parameters rather than
raw data. However, federated learning suffers from high communication costs, as
a considerable number of model parameters need to be transmitted many times
during the training process, making the approach inefficient, especially when
the communication network bandwidth is limited. This article proposes RingFed,
a novel framework to reduce communication overhead during the training process
of federated learning. Rather than transmitting parameters between the center
server and each client, as in original federated learning, in the proposed
RingFed, the updated parameters are transmitted between each client in turn,
and only the final result is transmitted to the central server, thereby
reducing the communication overhead substantially. After several local updates,
clients first send their parameters to another proximal client, not to the
center server directly, to preaggregate. Experiments on two different public
datasets show that RingFed has fast convergence, high model accuracy, and low
communication cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_K/0/1/0/all/0/1"&gt;Ke Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chunhe Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhijia Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1"&gt;Tierui Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Education: Opportunities and Challenges. (arXiv:2107.08828v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08828</id>
        <link href="http://arxiv.org/abs/2107.08828"/>
        <updated>2021-07-20T02:04:48.308Z</updated>
        <summary type="html"><![CDATA[This survey article has grown out of the RL4ED workshop organized by the
authors at the Educational Data Mining (EDM) 2021 conference. We organized this
workshop as part of a community-building effort to bring together researchers
and practitioners interested in the broad areas of reinforcement learning (RL)
and education (ED). This article aims to provide an overview of the workshop
activities and summarize the main research directions in the area of RL for ED.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1"&gt;Adish Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rafferty_A/0/1/0/all/0/1"&gt;Anna N. Rafferty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radanovic_G/0/1/0/all/0/1"&gt;Goran Radanovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heffernan_N/0/1/0/all/0/1"&gt;Neil T. Heffernan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Future will be Different than Today: Model Evaluation Considerations when Developing Translational Clinical Biomarker. (arXiv:2107.08787v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2107.08787</id>
        <link href="http://arxiv.org/abs/2107.08787"/>
        <updated>2021-07-20T02:04:48.291Z</updated>
        <summary type="html"><![CDATA[Finding translational biomarkers stands center stage of the future of
personalized medicine in healthcare. We observed notable challenges in
identifying robust biomarkers as some with great performance in one scenario
often fail to perform well in new trials (e.g. different population,
indications). With rapid development in the clinical trial world (e.g. assay,
disease definition), new trials very likely differ from legacy ones in many
perspectives and in development of biomarkers this heterogeneity should be
considered. In response, we recommend considering building in the heterogeneity
when evaluating biomarkers. In this paper, we present one evaluation strategy
by using leave-one-study-out (LOSO) in place of conventional cross-validation
(cv) methods to account for the potential heterogeneity across trials used for
building and testing the biomarkers. To demonstrate the performance of K-fold
vs LOSO cv in estimating the effect size of biomarkers, we leveraged data from
clinical trials and simulation studies. In our assessment, LOSO cv provided a
more objective estimate of the future performance. This conclusion remained
true across different evaluation metrics and different statistical methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yichen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fridlyand_J/0/1/0/all/0/1"&gt;Jane Fridlyand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tiffany Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Qi_T/0/1/0/all/0/1"&gt;Ting Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Simon_N/0/1/0/all/0/1"&gt;Noah Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Leng_N/0/1/0/all/0/1"&gt;Ning Leng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bayesian Hierarchical Score for Structure Learning from Related Data Sets. (arXiv:2008.01683v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01683</id>
        <link href="http://arxiv.org/abs/2008.01683"/>
        <updated>2021-07-20T02:04:47.806Z</updated>
        <summary type="html"><![CDATA[Score functions for learning the structure of Bayesian networks in the
literature assume that data are a homogeneous set of observations; whereas it
is often the case that they comprise different related, but not homogeneous,
data sets collected in different ways. In this paper we propose a new Bayesian
Dirichlet score, which we call Bayesian Hierarchical Dirichlet (BHD). The
proposed score is based on a hierarchical model that pools information across
data sets to learn a single encompassing network structure, while taking into
account the differences in their probabilistic structures. We derive a
closed-form expression for BHD using a variational approximation of the
marginal likelihood, we study the associated computational cost and we evaluate
its performance using simulated data. We find that, when data comprise multiple
related data sets, BHD outperforms the Bayesian Dirichlet equivalent uniform
(BDeu) score in terms of reconstruction accuracy as measured by the Structural
Hamming distance, and that it is as accurate as BDeu when data are homogeneous.
This improvement is particularly clear when either the number of variables in
the network or the number of observations is large. Moreover, the estimated
networks are sparser and therefore more interpretable than those obtained with
BDeu thanks to a lower number of false positive arcs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Azzimonti_L/0/1/0/all/0/1"&gt;Laura Azzimonti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Corani_G/0/1/0/all/0/1"&gt;Giorgio Corani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scutari_M/0/1/0/all/0/1"&gt;Marco Scutari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grab the Reins of Crowds: Estimating the Effects of Crowd Movement Guidance Using Causal Inference. (arXiv:2102.03980v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03980</id>
        <link href="http://arxiv.org/abs/2102.03980"/>
        <updated>2021-07-20T02:04:47.788Z</updated>
        <summary type="html"><![CDATA[Crowd movement guidance has been a fascinating problem in various fields,
such as easing traffic congestion in unusual events and evacuating people from
an emergency-affected area. To grab the reins of crowds, there has been
considerable demand for a decision support system that can answer a typical
question: ``what will be the outcomes of each of the possible options in the
current situation. In this paper, we consider the problem of estimating the
effects of crowd movement guidance from past data. To cope with limited amount
of available data biased by past decision-makers, we leverage two recent
techniques in deep representation learning for spatial data analysis and causal
inference. We use a spatial convolutional operator to extract effective spatial
features of crowds from a small amount of data and use balanced representation
learning based on the integral probability metrics to mitigate the selection
bias and missing counterfactual outcomes. To evaluate the performance on
estimating the treatment effects of possible guidance, we use a multi-agent
simulator to generate realistic data on evacuation scenarios in a crowded
theater, since there are no available datasets recording outcomes of all
possible crowd movement guidance. The results of three experiments demonstrate
that our proposed method reduces the estimation error by at most 56% from
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1"&gt;Koh Takeuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishida_R/0/1/0/all/0/1"&gt;Ryo Nishida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1"&gt;Hisashi Kashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onishi_M/0/1/0/all/0/1"&gt;Masaki Onishi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Document Visual Question Answering Challenge 2020. (arXiv:2008.08899v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08899</id>
        <link href="http://arxiv.org/abs/2008.08899"/>
        <updated>2021-07-20T02:04:47.750Z</updated>
        <summary type="html"><![CDATA[This paper presents results of Document Visual Question Answering Challenge
organized as part of "Text and Documents in the Deep Learning Era" workshop, in
CVPR 2020. The challenge introduces a new problem - Visual Question Answering
on document images. The challenge comprised two tasks. The first task concerns
with asking questions on a single document image. On the other hand, the second
task is set as a retrieval task where the question is posed over a collection
of images. For the task 1 a new dataset is introduced comprising 50,000
questions-answer(s) pairs defined over 12,767 document images. For task 2
another dataset has been created comprising 20 questions over 14,362 document
images which share the same document template.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1"&gt;Minesh Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1"&gt;Ruben Tito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1"&gt;Dimosthenis Karatzas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1"&gt;R. Manmatha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1"&gt;C.V. Jawahar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Connection Between Approximation, Depth Separation and Learnability in Neural Networks. (arXiv:2102.00434v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00434</id>
        <link href="http://arxiv.org/abs/2102.00434"/>
        <updated>2021-07-20T02:04:47.732Z</updated>
        <summary type="html"><![CDATA[Several recent works have shown separation results between deep neural
networks, and hypothesis classes with inferior approximation capacity such as
shallow networks or kernel classes. On the other hand, the fact that deep
networks can efficiently express a target function does not mean that this
target function can be learned efficiently by deep neural networks. In this
work we study the intricate connection between learnability and approximation
capacity. We show that learnability with deep networks of a target function
depends on the ability of simpler classes to approximate the target.
Specifically, we show that a necessary condition for a function to be learnable
by gradient descent on deep neural networks is to be able to approximate the
function, at least in a weak sense, with shallow neural networks. We also show
that a class of functions can be learned by an efficient statistical query
algorithm if and only if it can be approximated in a weak sense by some kernel
class. We give several examples of functions which demonstrate depth
separation, and conclude that they cannot be efficiently learned, even by a
hypothesis class that can efficiently approximate them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malach_E/0/1/0/all/0/1"&gt;Eran Malach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1"&gt;Gilad Yehudai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1"&gt;Shai Shalev-Shwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1"&gt;Ohad Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-based clustering of partial records. (arXiv:2103.16336v3 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16336</id>
        <link href="http://arxiv.org/abs/2103.16336"/>
        <updated>2021-07-20T02:04:47.479Z</updated>
        <summary type="html"><![CDATA[Partially recorded data are frequently encountered in many applications and
usually clustered by first removing incomplete cases or features with missing
values, or by imputing missing values, followed by application of a clustering
algorithm to the resulting altered dataset. Here, we develop clustering
methodology through a model-based approach using the marginal density for the
observed values, assuming a finite mixture model of multivariate $t$
distributions. We compare our approximate algorithm to the corresponding full
expectation-maximization (EM) approach that considers the missing values in the
incomplete data set and makes a missing at random (MAR) assumption, as well as
case deletion and imputation methods. Since only the observed values are
utilized, our approach is computationally more efficient than imputation or
full EM. Simulation studies demonstrate that our approach has favorable
recovery of the true cluster partition compared to case deletion and imputation
under various missingness mechanisms, and is at least competitive with the full
EM approach, even when MAR assumptions are violated. Our methodology is
demonstrated on a problem of clustering gamma-ray bursts and is implemented at
https://github.com/emilygoren/MixtClust.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Goren_E/0/1/0/all/0/1"&gt;Emily M. Goren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1"&gt;Ranjan Maitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Learning of Classifiers from a Statistical Perspective: A Brief Review. (arXiv:2104.04046v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04046</id>
        <link href="http://arxiv.org/abs/2104.04046"/>
        <updated>2021-07-20T02:04:47.476Z</updated>
        <summary type="html"><![CDATA[There has been increasing attention to semi-supervised learning (SSL)
approaches in machine learning to forming a classifier in situations where the
training data for a classifier consists of a limited number of classified
observations but a much larger number of unclassified observations. This is
because the procurement of classified data can be quite costly due to high
acquisition costs and subsequent financial, time, and ethical issues that can
arise in attempts to provide the true class labels for the unclassified data
that have been acquired. We provide here a review of statistical SSL approaches
to this problem, focussing on the recent result that a classifier formed from a
partially classified sample can actually have smaller expected error rate than
that if the sample were completely classified.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ahfock_D/0/1/0/all/0/1"&gt;Daniel Ahfock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+McLachlan_G/0/1/0/all/0/1"&gt;Geoffrey J. McLachlan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voting for the right answer: Adversarial defense for speaker verification. (arXiv:2106.07868v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07868</id>
        <link href="http://arxiv.org/abs/2106.07868"/>
        <updated>2021-07-20T02:04:47.463Z</updated>
        <summary type="html"><![CDATA[Automatic speaker verification (ASV) is a well developed technology for
biometric identification, and has been ubiquitous implemented in
security-critic applications, such as banking and access control. However,
previous works have shown that ASV is under the radar of adversarial attacks,
which are very similar to their original counterparts from human's perception,
yet will manipulate the ASV render wrong prediction. Due to the very late
emergence of adversarial attacks for ASV, effective countermeasures against
them are limited. Given that the security of ASV is of high priority, in this
work, we propose the idea of "voting for the right answer" to prevent risky
decisions of ASV in blind spot areas, by employing random sampling and voting.
Experimental results show that our proposed method improves the robustness
against both the limited-knowledge attackers by pulling the adversarial samples
out of the blind spots, and the perfect-knowledge attackers by introducing
randomness and increasing the attackers' budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haibin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Graph Learning with Proximity-based Views and Channel Contrast. (arXiv:2106.03723v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03723</id>
        <link href="http://arxiv.org/abs/2106.03723"/>
        <updated>2021-07-20T02:04:47.461Z</updated>
        <summary type="html"><![CDATA[We consider graph representation learning in a self-supervised manner. Graph
neural networks (GNNs) use neighborhood aggregation as a core component that
results in feature smoothing among nodes in proximity. While successful in
various prediction tasks, such a paradigm falls short of capturing nodes'
similarities over a long distance, which proves to be important for
high-quality learning. To tackle this problem, we strengthen the graph with two
additional graph views, in which nodes are directly linked to those with the
most similar features or local structures. Not restricted by connectivity in
the original graph, the generated views allow the model to enhance its
expressive power with new and complementary perspectives from which to look at
the relationship between nodes. Following a contrastive learning approach, we
propose a method that aims to maximize the agreement between representations
across generated views and the original graph. We also propose a channel-level
contrast approach that greatly reduces computation cost, compared to the
commonly used node level contrast, which requires computation cost quadratic in
the number of nodes. Extensive experiments on seven assortative graphs and four
disassortative graphs demonstrate the effectiveness of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1"&gt;Wei Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_G/0/1/0/all/0/1"&gt;Guang Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction-Free, Real-Time Flexible Control of Tidal Lagoons through Proximal Policy Optimisation: A Case Study for the Swansea Lagoon. (arXiv:2106.10360v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10360</id>
        <link href="http://arxiv.org/abs/2106.10360"/>
        <updated>2021-07-20T02:04:47.421Z</updated>
        <summary type="html"><![CDATA[Tidal range structures have been considered for large scale electricity
generation for their potential ability to produce reasonable predictable energy
without the emission of greenhouse gases. Once the main forcing components for
driving the tides have deterministic dynamics, the available energy in a given
tidal power plant has been estimated, through analytical and numerical
optimisation routines, as a mostly predictable event. This constraint imposes
state-of-art flexible operation methods to rely on tidal predictions
(concurrent with measured data and up to a multiple of half-tidal cycles into
the future) to infer best operational strategies for tidal lagoons, with the
additional cost of requiring to run optimisation routines for every new tide.
In this paper, we propose a novel optimised operation of tidal lagoons with
proximal policy optimisation through Unity ML-Agents. We compare this technique
with 6 different operation optimisation approaches (baselines) devised from the
literature, utilising the Swansea Bay Tidal Lagoon as a case study. We show
that our approach is successful in maximising energy generation through an
optimised operational policy of turbines and sluices, yielding competitive
results with state-of-the-art methods of optimisation, regardless of test data
used, requiring training once and performing real-time flexible control with
measured ocean data only.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moreira_T/0/1/0/all/0/1"&gt;T&amp;#xfa;lio Marcondes Moreira&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Faria_J/0/1/0/all/0/1"&gt;Jackson Geraldo de Faria Jr&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Melo_P/0/1/0/all/0/1"&gt;Pedro O.S. Vaz de Melo&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chaimowicz_L/0/1/0/all/0/1"&gt;Luiz Chaimowicz&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Medeiros_Ribeiro_G/0/1/0/all/0/1"&gt;Gilberto Medeiros-Ribeiro&lt;/a&gt; (1) ((1) Universidade Federal de Minas Gerais, Belo Horizonte, Brazil)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rate-Distortion Analysis of Minimum Excess Risk in Bayesian Learning. (arXiv:2105.04180v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04180</id>
        <link href="http://arxiv.org/abs/2105.04180"/>
        <updated>2021-07-20T02:04:47.413Z</updated>
        <summary type="html"><![CDATA[In parametric Bayesian learning, a prior is assumed on the parameter $W$
which determines the distribution of samples. In this setting, Minimum Excess
Risk (MER) is defined as the difference between the minimum expected loss
achievable when learning from data and the minimum expected loss that could be
achieved if $W$ was observed. In this paper, we build upon and extend the
recent results of (Xu & Raginsky, 2020) to analyze the MER in Bayesian learning
and derive information-theoretic bounds on it. We formulate the problem as a
(constrained) rate-distortion optimization and show how the solution can be
bounded above and below by two other rate-distortion functions that are easier
to study. The lower bound represents the minimum possible excess risk
achievable by any process using $R$ bits of information from the parameter $W$.
For the upper bound, the optimization is further constrained to use $R$ bits
from the training set, a setting which relates MER to information-theoretic
bounds on the generalization gap in frequentist learning. We derive
information-theoretic bounds on the difference between these upper and lower
bounds and show that they can provide order-wise tight rates for MER under
certain conditions. This analysis gives more insight into the
information-theoretic nature of Bayesian learning as well as providing novel
bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hafez_Kolahi_H/0/1/0/all/0/1"&gt;Hassan Hafez-Kolahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moniri_B/0/1/0/all/0/1"&gt;Behrad Moniri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1"&gt;Shohreh Kasaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baghshah_M/0/1/0/all/0/1"&gt;Mahdieh Soleymani Baghshah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DebiasedDTA: Model Debiasing to Boost Drug-Target Affinity Prediction. (arXiv:2107.05556v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05556</id>
        <link href="http://arxiv.org/abs/2107.05556"/>
        <updated>2021-07-20T02:04:47.411Z</updated>
        <summary type="html"><![CDATA[Motivation: Computational models that accurately identify high-affinity
protein-compound pairs can accelerate drug discovery pipelines. These models
aim to learn binding mechanics through drug-target interaction datasets and use
the learned knowledge for predicting the affinity of an input protein-compound
pair. However, the datasets they rely on bear misleading patterns that bias
models towards memorizing dataset-specific biomolecule properties, instead of
learning binding mechanics. This results in models that struggle while
predicting drug-target affinities (DTA), especially between de novo
biomolecules. Here we present DebiasedDTA, the first DTA model debiasing
approach that avoids dataset biases in order to boost affinity prediction for
novel biomolecules. DebiasedDTA uses ensemble learning and sample weight
adaptation for bias identification and avoidance and is applicable to almost
all existing DTA prediction models. Results: The results show that DebiasedDTA
can boost models while predicting the interactions between novel biomolecules.
Known biomolecules also benefit from the performance improvement, especially
when the test biomolecules are dissimilar to the training set. The experiments
also show that DebiasedDTA can augment DTA prediction models of different input
and model structures and is able to avoid biases of different sources.
Availability and Implementation: The source code, the models, and the datasets
are freely available for download at
https://github.com/boun-tabi/debiaseddta-reproduce, implementation in Python3,
and supported for Linux, MacOS and MS Windows. Contact:
arzucan.ozgur@boun.edu.tr, elif.ozkirimli@roche.com]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ozcelik_R/0/1/0/all/0/1"&gt;R&amp;#x131;za &amp;#xd6;z&amp;#xe7;elik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bag_A/0/1/0/all/0/1"&gt;Alperen Ba&amp;#x11f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Atil_B/0/1/0/all/0/1"&gt;Berk At&amp;#x131;l&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ozgur_A/0/1/0/all/0/1"&gt;Arzucan &amp;#xd6;zg&amp;#xfc;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ozkirimli_E/0/1/0/all/0/1"&gt;Elif &amp;#xd6;zk&amp;#x131;r&amp;#x131;ml&amp;#x131;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fea2Fea: Exploring Structural Feature Correlations via Graph Neural Networks. (arXiv:2106.13061v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13061</id>
        <link href="http://arxiv.org/abs/2106.13061"/>
        <updated>2021-07-20T02:04:47.406Z</updated>
        <summary type="html"><![CDATA[Structural features are important features in graph datasets. However,
although there are some correlation analysis of features based on covariance,
there is no relevant research on exploring structural feature correlation on
graphs with graph neural network based models. In this paper, we introduce
graph feature to feature (Fea2Fea) prediction pipelines in a low dimensional
space to explore some preliminary results on structural feature correlation,
which is based on graph neural network. The results show that there exists high
correlation between some of the structural features. A non-redundant feature
combination with initial node features, which is filtered by graph neural
network has improved its classification accuracy in some graph datasets. We
compare the difference between concatenation methods on connecting embeddings
between features and show that the simplest is the best. We generalize on the
synthetic geometric graphs and certify the results on prediction difficulty
between two structural features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiaqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1"&gt;Rex Ying&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06419</id>
        <link href="http://arxiv.org/abs/2103.06419"/>
        <updated>2021-07-20T02:04:47.391Z</updated>
        <summary type="html"><![CDATA[Background and objective: In this paper, a modified U-Net based framework is
presented, which leverages techniques from Squeeze-and-Excitation (SE) block,
Atrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and
robust liver CT segmentation, and the effectiveness of the proposed method was
tested on two public datasets LiTS17 and SLiver07.

Methods: A new network architecture called SAR-U-Net was designed. Firstly,
the SE block is introduced to adaptively extract image features after each
convolution in the U-Net encoder, while suppressing irrelevant regions, and
highlighting features of specific segmentation task; Secondly, ASPP was
employed to replace the transition layer and the output layer, and acquire
multi-scale image information via different receptive fields. Thirdly, to
alleviate the degradation problem, the traditional convolution block was
replaced with the residual block and thus prompt the network to gain accuracy
from considerably increased depth.

Results: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and
MSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other
closely related 2D-based models, the proposed method achieved the highest
accuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,
ASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared
with other closely related models, the proposed method achieved the highest
segmentation accuracy except for the RVD.

Conclusion: The proposed model enables a great improvement on the accuracy
compared to 2D-based models, and its robustness in circumvent challenging
problems, such as small liver regions, discontinuous liver regions, and fuzzy
liver boundaries, is also well demonstrated and validated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jinke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1"&gt;Peiqing Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Missing Cone Artifacts Removal in ODT using Unsupervised Deep Learning in Projection Domain. (arXiv:2103.09022v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09022</id>
        <link href="http://arxiv.org/abs/2103.09022"/>
        <updated>2021-07-20T02:04:47.260Z</updated>
        <summary type="html"><![CDATA[Optical diffraction tomography (ODT) produces three dimensional distribution
of refractive index (RI) by measuring scattering fields at various angles.
Although the distribution of RI index is highly informative, due to the missing
cone problem stemming from the limited-angle acquisition of holograms,
reconstructions have very poor resolution along axial direction compared to the
horizontal imaging plane. To solve this issue, here we present a novel
unsupervised deep learning framework, which learns the probability distribution
of missing projection views through optimal transport driven cycleGAN.
Experimental results show that missing cone artifact in ODT can be
significantly resolved by the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1"&gt;Hyungjin Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huh_J/0/1/0/all/0/1"&gt;Jaeyoung Huh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_G/0/1/0/all/0/1"&gt;Geon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1"&gt;Yong Keun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence of Deep Learning with Differential Privacy. (arXiv:2106.07830v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07830</id>
        <link href="http://arxiv.org/abs/2106.07830"/>
        <updated>2021-07-20T02:04:47.255Z</updated>
        <summary type="html"><![CDATA[In deep learning with differential privacy (DP), the neural network achieves
the privacy usually at the cost of slower convergence (and thus lower
performance) than its non-private counterpart. This work gives the first
convergence analysis of the DP deep learning, through the lens of training
dynamics and the neural tangent kernel (NTK). Our convergence theory
successfully characterizes the effects of two key components in the DP
training: the per-sample clipping (flat or layerwise) and the noise addition.
Our analysis not only initiates a general principled framework to understand
the DP deep learning with any network architecture and loss function, but also
motivates a new clipping method -- the global clipping, that significantly
improves the convergence while preserving the same privacy guarantee as the
existing local clipping.

In terms of theoretical results, we establish the precise connection between
the per-sample clipping and NTK matrix. We show that in the gradient flow,
i.e., with infinitesimal learning rate, the noise level of DP optimizers does
not affect the convergence. We prove that DP gradient descent (GD) with global
clipping guarantees the monotone convergence to zero loss, which can be
violated by the existing DP-GD with local clipping. Notably, our analysis
framework easily extends to other optimizers, e.g., DP-Adam. Empirically
speaking, DP optimizers equipped with global clipping perform strongly on a
wide range of classification and regression tasks. In particular, our global
clipping is surprisingly effective at learning calibrated classifiers, in
contrast to the existing DP classifiers which are oftentimes over-confident and
unreliable. Implementation-wise, the new clipping can be realized by adding one
line of code into the Opacus library.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1"&gt;Zhiqi Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1"&gt;Qi Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weijie J. Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09540</id>
        <link href="http://arxiv.org/abs/2105.09540"/>
        <updated>2021-07-20T02:04:47.253Z</updated>
        <summary type="html"><![CDATA[The increasing concerns about data privacy and security drive an emerging
field of studying privacy-preserving machine learning from isolated data
sources, i.e., federated learning. A class of federated learning, vertical
federated learning, where different parties hold different features for common
users, has a great potential of driving a more variety of business cooperation
among enterprises in many fields. In machine learning, decision tree ensembles
such as gradient boosting decision tree (GBDT) and random forest are widely
applied powerful models with high interpretability and modeling efficiency.
However, the interpretability is compromised in state-of-the-art vertical
federated learning frameworks such as SecureBoost with anonymous features to
avoid possible data breaches. To address this issue in the inference process,
in this paper, we propose Fed-EINI to protect data privacy and allow the
disclosure of feature meaning by concealing decision paths with a
communication-efficient secure computation method for inference outputs. The
advantages of Fed-EINI will be demonstrated through both theoretical analysis
and extensive numerical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaolin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuai Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Hao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zejin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongji Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepOPF-V: Solving AC-OPF Problems Efficiently. (arXiv:2103.11793v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11793</id>
        <link href="http://arxiv.org/abs/2103.11793"/>
        <updated>2021-07-20T02:04:47.241Z</updated>
        <summary type="html"><![CDATA[AC optimal power flow (AC-OPF) problems need to be solved more frequently in
the future to maintain stable and economic power system operation. To tackle
this challenge, a deep neural network-based voltage-constrained approach
(DeepOPF-V) is proposed to solve AC-OPF problems with high computational
efficiency. Its unique design predicts voltages of all buses and then uses them
to reconstruct the remaining variables without solving non-linear AC power flow
equations. A fast post-processing process is developed to enforce the box
constraints. The effectiveness of DeepOPF-V is validated by simulations on IEEE
118/300-bus systems and a 2000-bus test system. Compared with existing studies,
DeepOPF-V achieves decent computation speedup up to four orders of magnitude
and comparable performance in optimality gap and preserving the feasibility of
the solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wanjun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xiang Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minghua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Low_S/0/1/0/all/0/1"&gt;Steven H. Low&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smart Feasibility Pump: Reinforcement Learning for (Mixed) Integer Programming. (arXiv:2102.09663v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09663</id>
        <link href="http://arxiv.org/abs/2102.09663"/>
        <updated>2021-07-20T02:04:47.225Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a deep reinforcement learning (DRL) model for
finding a feasible solution for (mixed) integer programming (MIP) problems.
Finding a feasible solution for MIP problems is critical because many
successful heuristics rely on a known initial feasible solution. However, it is
in general NP-hard. Inspired by the feasibility pump (FP), a well-known
heuristic for searching feasible MIP solutions, we develop a smart feasibility
pump (SFP) method using DRL. In addition to multi-layer perception (MLP), we
propose a novel convolution neural network (CNN) structure for the policy
network to capture the hidden information of the constraint matrix of the MIP
problem. Numerical experiments on various problem instances show that SFP
significantly outperforms the classic FP in terms of the number of steps
required to reach the first feasible solution. Moreover, the CNN structure
works without the projection of the current solution as the input, which saves
the computational effort at each step of the FP algorithms to find projections.
This highlights the representational power of the CNN structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1"&gt;Meng Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zuo-Jun Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XAI-N: Sensor-based Robot Navigation using Expert Policies and Decision Trees. (arXiv:2104.10818v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10818</id>
        <link href="http://arxiv.org/abs/2104.10818"/>
        <updated>2021-07-20T02:04:47.223Z</updated>
        <summary type="html"><![CDATA[We present a novel sensor-based learning navigation algorithm to compute a
collision-free trajectory for a robot in dense and dynamic environments with
moving obstacles or targets. Our approach uses deep reinforcement
learning-based expert policy that is trained using a sim2real paradigm. In
order to increase the reliability and handle the failure cases of the expert
policy, we combine with a policy extraction technique to transform the
resulting policy into a decision tree format. The resulting decision tree has
properties which we use to analyze and modify the policy and improve
performance on navigation metrics including smoothness, frequency of
oscillation, frequency of immobilization, and obstruction of target. We are
able to modify the policy to address these imperfections without retraining,
combining the learning power of deep learning with the control of
domain-specific algorithms. We highlight the benefits of our algorithm in
simulated environments and navigating a Clearpath Jackal robot among moving
pedestrians. (Videos at this url:
https://gamma.umd.edu/researchdirections/xrl/navviper)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1"&gt;Aaron M. Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jing Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptive Robotic Gesture Recognition with Unsupervised Kinematic-Visual Data Alignment. (arXiv:2103.04075v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04075</id>
        <link href="http://arxiv.org/abs/2103.04075"/>
        <updated>2021-07-20T02:04:47.219Z</updated>
        <summary type="html"><![CDATA[Automated surgical gesture recognition is of great importance in
robot-assisted minimally invasive surgery. However, existing methods assume
that training and testing data are from the same domain, which suffers from
severe performance degradation when a domain gap exists, such as the simulator
and real robot. In this paper, we propose a novel unsupervised domain
adaptation framework which can simultaneously transfer multi-modality
knowledge, i.e., both kinematic and visual data, from simulator to real robot.
It remedies the domain gap with enhanced transferable features by using
temporal cues in videos, and inherent correlations in multi-modal towards
recognizing gesture. Specifically, we first propose an MDO-K to align
kinematics, which exploits temporal continuity to transfer motion directions
with smaller gap rather than position values, relieving the adaptation burden.
Moreover, we propose a KV-Relation-ATT to transfer the co-occurrence signals of
kinematics and vision. Such features attended by correlation similarity are
more informative for enhancing domain-invariance of the model. Two feature
alignment strategies benefit the model mutually during the end-to-end learning
process. We extensively evaluate our method for gesture recognition using DESK
dataset with peg transfer procedure. Results show that our approach recovers
the performance with great improvement gains, up to 12.91% in ACC and 20.16% in
F1score without using any annotations in real robot.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xueying Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yueming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1"&gt;Qi Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jing Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng-Ann Heng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Contextual Learners for Protein Networks. (arXiv:2106.02246v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02246</id>
        <link href="http://arxiv.org/abs/2106.02246"/>
        <updated>2021-07-20T02:04:47.216Z</updated>
        <summary type="html"><![CDATA[Spatial context is central to understanding health and disease. Yet reference
protein interaction networks lack such contextualization, thereby limiting the
study of where protein interactions likely occur in the human body and how they
may be altered in disease. Contextualized protein interactions could better
characterize genes with disease-specific interactions and elucidate diseases'
manifestation in specific cell types. Here, we introduce AWARE, a graph neural
message passing approach to inject cellular and tissue context into protein
embeddings. AWARE optimizes for a multi-scale embedding space, whose structure
reflects network topology at a single-cell resolution. We construct a
multi-scale network of the Human Cell Atlas and apply AWARE to learn protein,
cell type, and tissue embeddings that uphold cell type and tissue hierarchies.
We demonstrate AWARE's utility on the novel task of predicting whether a
protein is altered in disease and where that association most likely manifests
in the human body. To this end, AWARE outperforms generic embeddings without
contextual information by at least 12.5%, showing AWARE's potential to reveal
context-dependent roles of proteins in disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Michelle M. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1"&gt;Marinka Zitnik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Training of Social Media Image Classification Models for Rapid Disaster Response. (arXiv:2104.04184v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04184</id>
        <link href="http://arxiv.org/abs/2104.04184"/>
        <updated>2021-07-20T02:04:47.190Z</updated>
        <summary type="html"><![CDATA[Images shared on social media help crisis managers gain situational awareness
and assess incurred damages, among other response tasks. As the volume and
velocity of such content are typically high, real-time image classification has
become an urgent need for a faster disaster response. Recent advances in
computer vision and deep neural networks have enabled the development of models
for real-time image classification for a number of tasks, including detecting
crisis incidents, filtering irrelevant images, classifying images into specific
humanitarian categories, and assessing the severity of the damage. To develop
robust real-time models, it is necessary to understand the capability of the
publicly available pre-trained models for these tasks, which remains to be
under-explored in the crisis informatics literature. In this study, we address
such limitations by investigating ten different network architectures for four
different tasks using the largest publicly available datasets for these tasks.
We also explore various data augmentation strategies, semi-supervised
techniques, and a multitask learning setup. In our extensive experiments, we
achieve promising results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1"&gt;Firoj Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1"&gt;Tanvirul Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1"&gt;Muhammad Imran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1"&gt;Ferda Ofli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Approximation Rate of ReLU Networks in terms of Width and Depth. (arXiv:2103.00502v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00502</id>
        <link href="http://arxiv.org/abs/2103.00502"/>
        <updated>2021-07-20T02:04:47.189Z</updated>
        <summary type="html"><![CDATA[This paper concentrates on the approximation power of deep feed-forward
neural networks in terms of width and depth. It is proved by construction that
ReLU networks with width $\mathcal{O}\big(\max\{d\lfloor N^{1/d}\rfloor,\,
N+2\}\big)$ and depth $\mathcal{O}(L)$ can approximate a H\"older continuous
function on $[0,1]^d$ with an approximation rate
$\mathcal{O}\big(\lambda\sqrt{d} (N^2L^2\ln N)^{-\alpha/d}\big)$, where
$\alpha\in (0,1]$ and $\lambda>0$ are H\"older order and constant,
respectively. Such a rate is optimal up to a constant in terms of width and
depth separately, while existing results are only nearly optimal without the
logarithmic factor in the approximation rate. More generally, for an arbitrary
continuous function $f$ on $[0,1]^d$, the approximation rate becomes
$\mathcal{O}\big(\,\sqrt{d}\,\omega_f\big( (N^2L^2\ln N)^{-1/d}\big)\,\big)$,
where $\omega_f(\cdot)$ is the modulus of continuity. We also extend our
analysis to any continuous function $f$ on a bounded set. Particularly, if ReLU
networks with depth $31$ and width $\mathcal{O}(N)$ are used to approximate
one-dimensional Lipschitz continuous functions on $[0,1]$ with a Lipschitz
constant $\lambda>0$, the approximation rate in terms of the total number of
parameters, $W=\mathcal{O}(N^2)$, becomes $\mathcal{O}(\tfrac{\lambda}{W\ln
W})$, which has not been discovered in the literature for fixed-depth ReLU
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zuowei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entanglement Diagnostics for Efficient Quantum Computation. (arXiv:2102.12534v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12534</id>
        <link href="http://arxiv.org/abs/2102.12534"/>
        <updated>2021-07-20T02:04:47.181Z</updated>
        <summary type="html"><![CDATA[We consider information spreading measures in randomly initialized
variational quantum circuits and introduce entanglement diagnostics for
efficient variational quantum/classical computations. We establish a robust
connection between entanglement measures and optimization accuracy by solving
two eigensolver problems for Ising Hamiltonians with nearest-neighbor and
long-range spin interactions. As the circuit depth affects the average
entanglement of random circuit states, the entanglement diagnostics can
identify a high-performing depth range for optimization tasks encoded in local
Hamiltonians. We argue, based on an eigensolver problem for the
Sachdev-Ye-Kitaev model, that entanglement alone is insufficient as a
diagnostic to the approximation of volume-law entangled target states and that
a large number of circuit parameters is needed for such an optimization task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kim_J/0/1/0/all/0/1"&gt;Joonho Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Oz_Y/0/1/0/all/0/1"&gt;Yaron Oz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization in Vision: A Survey. (arXiv:2103.02503v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02503</id>
        <link href="http://arxiv.org/abs/2103.02503"/>
        <updated>2021-07-20T02:04:47.181Z</updated>
        <summary type="html"><![CDATA[Generalization to out-of-distribution (OOD) data is a capability natural to
humans yet challenging for machines to reproduce. This is because most learning
algorithms strongly rely on the i.i.d.~assumption on source/target data, which
is often violated in practice due to domain shift. Domain generalization (DG)
aims to achieve OOD generalization by using only source data for model
learning. Since first introduced in 2011, research in DG has made great
progresses. In particular, intensive research in this topic has led to a broad
spectrum of methodologies, e.g., those based on domain alignment,
meta-learning, data augmentation, or ensemble learning, just to name a few; and
has covered various vision applications such as object recognition,
segmentation, action recognition, and person re-identification. In this paper,
for the first time a comprehensive literature review is provided to summarize
the developments in DG for computer vision over the past decade. Specifically,
we first cover the background by formally defining DG and relating it to other
research fields like domain adaptation and transfer learning. Second, we
conduct a thorough review into existing methods and present a categorization
based on their methodologies and motivations. Finally, we conclude this survey
with insights and discussions on future research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kaiyang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tao Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Complexity of Labeled Datasets. (arXiv:1911.05461v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.05461</id>
        <link href="http://arxiv.org/abs/1911.05461"/>
        <updated>2021-07-20T02:04:47.162Z</updated>
        <summary type="html"><![CDATA[The Statistical Learning Theory (SLT) provides the foundation to ensure that
a supervised algorithm generalizes the mapping $f: \mathcal{X} \to \mathcal{Y}$
given $f$ is selected from its search space bias $\mathcal{F}$. SLT depends on
the Shattering coefficient function $\mathcal{N}(\mathcal{F},n)$ to upper bound
the empirical risk minimization principle, from which one can estimate the
necessary training sample size to ensure the probabilistic learning convergence
and, most importantly, the characterization of the capacity of $\mathcal{F}$,
including its underfitting and overfitting abilities while addressing specific
target problems. However, the analytical solution of the Shattering coefficient
is still an open problem since the first studies by Vapnik and Chervonenkis in
$1962$, which we address on specific datasets, in this paper, by employing
equivalence relations from Topology, data separability results by Har-Peled and
Jones, and combinatorics. Our approach computes the Shattering coefficient for
both binary and multi-class datasets, leading to the following additional
contributions: (i) the estimation of the required number of hyperplanes in the
worst and best-case classification scenarios and the respective $\Omega$ and
$O$ complexities; (ii) the estimation of the training sample sizes required to
ensure supervised learning; and (iii) the comparison of dataset embeddings,
once they (re)organize samples into some new space configuration. All results
introduced and discussed along this paper are supported by the R package
shattering (https://cran.r-project.org/web/packages/shattering).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mello_R/0/1/0/all/0/1"&gt;Rodrigo Fernandes de Mello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation. (arXiv:2010.15728v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15728</id>
        <link href="http://arxiv.org/abs/2010.15728"/>
        <updated>2021-07-20T02:04:47.162Z</updated>
        <summary type="html"><![CDATA[Diagnostic or procedural coding of clinical notes aims to derive a coded
summary of disease-related information about patients. Such coding is usually
done manually in hospitals but could potentially be automated to improve the
efficiency and accuracy of medical coding. Recent studies on deep learning for
automated medical coding achieved promising performances. However, the
explainability of these models is usually poor, preventing them to be used
confidently in supporting clinical practice. Another limitation is that these
models mostly assume independence among labels, ignoring the complex
correlation among medical codes which can potentially be exploited to improve
the performance. We propose a Hierarchical Label-wise Attention Network (HLAN),
which aimed to interpret the model by quantifying importance (as attention
weights) of words and sentences related to each of the labels. Secondly, we
propose to enhance the major deep learning models with a label embedding (LE)
initialisation approach, which learns a dense, continuous vector representation
and then injects the representation into the final layers and the label-wise
attention layers in the models. We evaluated the methods using three settings
on the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS
COVID-19 shielding codes. Experiments were conducted to compare HLAN and LE
initialisation to the state-of-the-art neural network based methods. HLAN
achieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and
comparable results on the NHS COVID-19 shielding code prediction to other
models. By highlighting the most salient words and sentences for each label,
HLAN showed more meaningful and comprehensive model interpretation compared to
its downgraded baselines and the CNN-based models. LE initialisation
consistently boosted most deep learning models for automated medical coding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor Su&amp;#xe1;rez-Paniagua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteley_W/0/1/0/all/0/1"&gt;William Whiteley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Honghan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Whitening and second order optimization both make information in the dataset unusable during training, and can reduce or prevent generalization. (arXiv:2008.07545v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07545</id>
        <link href="http://arxiv.org/abs/2008.07545"/>
        <updated>2021-07-20T02:04:47.161Z</updated>
        <summary type="html"><![CDATA[Machine learning is predicated on the concept of generalization: a model
achieving low error on a sufficiently large training set should also perform
well on novel samples from the same distribution. We show that both data
whitening and second order optimization can harm or entirely prevent
generalization. In general, model training harnesses information contained in
the sample-sample second moment matrix of a dataset. For a general class of
models, namely models with a fully connected first layer, we prove that the
information contained in this matrix is the only information which can be used
to generalize. Models trained using whitened data, or with certain second order
optimization schemes, have less access to this information, resulting in
reduced or nonexistent generalization ability. We experimentally verify these
predictions for several architectures, and further demonstrate that
generalization continues to be harmed even when theoretical requirements are
relaxed. However, we also show experimentally that regularized second order
optimization can provide a practical tradeoff, where training is accelerated
but less information is lost, and generalization can in some circumstances even
improve.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wadia_N/0/1/0/all/0/1"&gt;Neha S. Wadia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duckworth_D/0/1/0/all/0/1"&gt;Daniel Duckworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schoenholz_S/0/1/0/all/0/1"&gt;Samuel S. Schoenholz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1"&gt;Ethan Dyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1"&gt;Jascha Sohl-Dickstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey. (arXiv:2012.09830v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09830</id>
        <link href="http://arxiv.org/abs/2012.09830"/>
        <updated>2021-07-20T02:04:47.161Z</updated>
        <summary type="html"><![CDATA[Building autonomous machines that can explore open-ended environments,
discover possible interactions and autonomously build repertoires of skills is
a general objective of artificial intelligence. Developmental approaches argue
that this can only be achieved by autonomous and intrinsically motivated
learning agents that can generate, select and learn to solve their own
problems. In recent years, we have seen a convergence of developmental
approaches, and developmental robotics in particular, with deep reinforcement
learning (RL) methods, forming the new domain of developmental machine
learning. Within this new domain, we review here a set of methods where deep RL
algorithms are trained to tackle the developmental robotics problem of the
autonomous acquisition of open-ended repertoires of skills. Intrinsically
motivated goal-conditioned RL algorithms train agents to learn to represent,
generate and pursue their own goals. The self-generation of goals requires the
learning of compact goal encodings as well as their associated goal-achievement
functions, which results in new challenges compared to traditional RL
algorithms designed to tackle pre-defined sets of goals using external reward
signals. This paper proposes a typology of these methods at the intersection of
deep RL and developmental approaches, surveys recent approaches and discusses
future avenues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Colas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1"&gt;Tristan Karch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1"&gt;Olivier Sigaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1"&gt;Pierre-Yves Oudeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Implicit Soft-Body Physics. (arXiv:2102.05791v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05791</id>
        <link href="http://arxiv.org/abs/2102.05791"/>
        <updated>2021-07-20T02:04:47.160Z</updated>
        <summary type="html"><![CDATA[We present a differentiable soft-body physics simulator that can be composed
with neural networks as a differentiable layer. In contrast to other
differentiable physics approaches that use explicit forward models to define
state transitions, we focus on implicit state transitions defined via function
minimization. Implicit state transitions appear in implicit numerical
integration methods, which offer the benefits of large time steps and excellent
numerical stability, but require a special treatment to achieve
differentiability due to the absence of an explicit differentiable forward
pass. In contrast to other implicit differentiation approaches that require
explicit formulas for the force function and the force Jacobian matrix, we
present an energy-based approach that allows us to compute these derivatives
automatically and in a matrix-free fashion via reverse-mode automatic
differentiation. This allows for more flexibility and productivity when
defining physical models and is particularly important in the context of neural
network training, which often relies on reverse-mode automatic differentiation
(backpropagation). We demonstrate the effectiveness of our differentiable
simulator in policy optimization for locomotion tasks and show that it achieves
better sample efficiency than model-free reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rojas_J/0/1/0/all/0/1"&gt;Junior Rojas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sifakis_E/0/1/0/all/0/1"&gt;Eftychios Sifakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kavan_L/0/1/0/all/0/1"&gt;Ladislav Kavan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating covariant Lyapunov vectors from data. (arXiv:2107.08925v1 [physics.data-an])]]></title>
        <id>http://arxiv.org/abs/2107.08925</id>
        <link href="http://arxiv.org/abs/2107.08925"/>
        <updated>2021-07-20T02:04:47.152Z</updated>
        <summary type="html"><![CDATA[Covariant Lyapunov vectors (CLVs) characterize the directions along which
perturbations in dynamical systems grow. They have also been studied as
potential predictors of critical transitions and extreme events. For many
applications, it is, however, necessary to estimate the vectors from data since
model equations are unknown for many interesting phenomena. We propose a novel
method for estimating CLVs based on data records without knowing the underlying
equations of the system which is suitable also for high-dimensional data and
computationally inexpensive. We demonstrate that this purely data-driven
approach can accurately estimate CLVs from data records generated by chaotic
dynamical systems of dimension 128 and multiple lower-dimensional systems and
thus provides the foundation for numerous future applications in data-analysis
and data-based predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Martin_C/0/1/0/all/0/1"&gt;Christoph Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sharafi_N/0/1/0/all/0/1"&gt;Nahal Sharafi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hallerberg_S/0/1/0/all/0/1"&gt;Sarah Hallerberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selective Focusing Learning in Conditional GANs. (arXiv:2107.08792v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08792</id>
        <link href="http://arxiv.org/abs/2107.08792"/>
        <updated>2021-07-20T02:04:47.039Z</updated>
        <summary type="html"><![CDATA[Conditional generative adversarial networks (cGANs) have demonstrated
remarkable success due to their class-wise controllability and superior quality
for complex generation tasks. Typical cGANs solve the joint distribution
matching problem by decomposing two easier sub-problems: marginal matching and
conditional matching. From our toy experiments, we found that it is the best to
apply only conditional matching to certain samples due to the content-aware
optimization of the discriminator. This paper proposes a simple (a few lines of
code) but effective training methodology, selective focusing learning, which
enforces the discriminator and generator to learn easy samples of each class
rapidly while maintaining diversity. Our key idea is to selectively apply
conditional and joint matching for the data in each mini-batch. We conducted
experiments on recent cGAN variants in ImageNet (64x64 and 128x128), CIFAR-10,
and CIFAR-100 datasets, and improved the performance significantly (up to
35.18% in terms of FID) without sacrificing diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1"&gt;Kyeongbo Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyunghun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1"&gt;Woo-Jin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;Suk-Ju Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisDrone-CC2020: The Vision Meets Drone Crowd Counting Challenge Results. (arXiv:2107.08766v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08766</id>
        <link href="http://arxiv.org/abs/2107.08766"/>
        <updated>2021-07-20T02:04:47.020Z</updated>
        <summary type="html"><![CDATA[Crowd counting on the drone platform is an interesting topic in computer
vision, which brings new challenges such as small object inference, background
clutter and wide viewpoint. However, there are few algorithms focusing on crowd
counting on the drone-captured data due to the lack of comprehensive datasets.
To this end, we collect a large-scale dataset and organize the Vision Meets
Drone Crowd Counting Challenge (VisDrone-CC2020) in conjunction with the 16th
European Conference on Computer Vision (ECCV 2020) to promote the developments
in the related fields. The collected dataset is formed by $3,360$ images,
including $2,460$ images for training, and $900$ images for testing.
Specifically, we manually annotate persons with points in each video frame.
There are $14$ algorithms from $15$ institutes submitted to the VisDrone-CC2020
Challenge. We provide a detailed analysis of the evaluation results and
conclude the challenge. More information can be found at the website:
\url{this http URL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1"&gt;Dawei Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1"&gt;Longyin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1"&gt;Pengfei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Heng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Qinghua Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1"&gt;Haibin Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1"&gt;Junwen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Ali_A/0/1/0/all/0/1"&gt;Ali Al-Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Amr Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imene_B/0/1/0/all/0/1"&gt;Bakour Imene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1"&gt;Bin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Binyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nesma_B/0/1/0/all/0/1"&gt;Bouchali Hadia Nesma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1"&gt;Chenzhen Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castiello_C/0/1/0/all/0/1"&gt;Ciro Castiello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mencar_C/0/1/0/all/0/1"&gt;Corrado Mencar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1"&gt;Dingkang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kruger_F/0/1/0/all/0/1"&gt;Florian Kr&amp;#xfc;ger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vessio_G/0/1/0/all/0/1"&gt;Gennaro Vessio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castellano_G/0/1/0/all/0/1"&gt;Giovanna Castellano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jieru Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Junyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abualsaud_K/0/1/0/all/0/1"&gt;Khalid Abualsaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"&gt;Laihui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Lei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cianciotta_M/0/1/0/all/0/1"&gt;Marco Cianciotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saqib_M/0/1/0/all/0/1"&gt;Muhammad Saqib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Almaadeed_N/0/1/0/all/0/1"&gt;Noor Almaadeed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elharrouss_O/0/1/0/all/0/1"&gt;Omar Elharrouss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_P/0/1/0/all/0/1"&gt;Pei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shidong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1"&gt;Shuang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Siyang Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Maadeed_S/0/1/0/all/0/1"&gt;Somaya Al-Maadeed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Sultan Daud Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khattab_T/0/1/0/all/0/1"&gt;Tamer Khattab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1"&gt;Tao Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golda_T/0/1/0/all/0/1"&gt;Thomas Golda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiang Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaoqing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuelong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yanyun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Ye Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yingnan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yongchao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuehan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhenyu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhijian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhipeng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhiwei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhiyuan Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Dermatological Lesion Classification and Confidence Modeling with Uncertainty Estimation. (arXiv:2107.08770v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08770</id>
        <link href="http://arxiv.org/abs/2107.08770"/>
        <updated>2021-07-20T02:04:47.020Z</updated>
        <summary type="html"><![CDATA[Deep learning has played a major role in the interpretation of dermoscopic
images for detecting skin defects and abnormalities. However, current deep
learning solutions for dermatological lesion analysis are typically limited in
providing probabilistic predictions which highlights the importance of
concerning uncertainties. This concept of uncertainty can provide a confidence
level for each feature which prevents overconfident predictions with poor
generalization on unseen data. In this paper, we propose an overall framework
that jointly considers dermatological classification and uncertainty estimation
together. The estimated confidence of each feature to avoid uncertain feature
and undesirable shift, which are caused by environmental difference of input
image, in the latent space is pooled from confidence network. Our qualitative
results show that modeling uncertainties not only helps to quantify model
confidence for each prediction but also helps classification layers to focus on
confident features, therefore, improving the accuracy for dermatological lesion
classification. We demonstrate the potential of the proposed approach in two
state-of-the-art dermoscopic datasets (ISIC 2018 and ISIC 2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gun-Hee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1"&gt;Han-Bin Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MEGEX: Data-Free Model Extraction Attack against Gradient-Based Explainable AI. (arXiv:2107.08909v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.08909</id>
        <link href="http://arxiv.org/abs/2107.08909"/>
        <updated>2021-07-20T02:04:47.019Z</updated>
        <summary type="html"><![CDATA[The advance of explainable artificial intelligence, which provides reasons
for its predictions, is expected to accelerate the use of deep neural networks
in the real world like Machine Learning as a Service (MLaaS) that returns
predictions on queried data with the trained model. Deep neural networks
deployed in MLaaS face the threat of model extraction attacks. A model
extraction attack is an attack to violate intellectual property and privacy in
which an adversary steals trained models in a cloud using only their
predictions. In particular, a data-free model extraction attack has been
proposed recently and is more critical. In this attack, an adversary uses a
generative model instead of preparing input data. The feasibility of this
attack, however, needs to be studied since it requires more queries than that
with surrogate datasets. In this paper, we propose MEGEX, a data-free model
extraction attack against a gradient-based explainable AI. In this method, an
adversary uses the explanations to train the generative model and reduces the
number of queries to steal the model. Our experiments show that our proposed
method reconstructs high-accuracy models -- 0.97$\times$ and 0.98$\times$ the
victim model accuracy on SVHN and CIFAR-10 datasets given 2M and 20M queries,
respectively. This implies that there is a trade-off between the
interpretability of models and the difficulty of stealing them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miura_T/0/1/0/all/0/1"&gt;Takayuki Miura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasegawa_S/0/1/0/all/0/1"&gt;Satoshi Hasegawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shibahara_T/0/1/0/all/0/1"&gt;Toshiki Shibahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reasoning-Modulated Representations. (arXiv:2107.08881v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08881</id>
        <link href="http://arxiv.org/abs/2107.08881"/>
        <updated>2021-07-20T02:04:47.018Z</updated>
        <summary type="html"><![CDATA[Neural networks leverage robust internal representations in order to
generalise. Learning them is difficult, and often requires a large training set
that covers the data distribution densely. We study a common setting where our
task is not purely opaque. Indeed, very often we may have access to information
about the underlying system (e.g. that observations must obey certain laws of
physics) that any "tabula rasa" neural network would need to re-learn from
scratch, penalising data efficiency. We incorporate this information into a
pre-trained reasoning module, and investigate its role in shaping the
discovered representations in diverse self-supervised learning settings from
pixels. Our approach paves the way for a new class of data-efficient
representation learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1"&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosnjak_M/0/1/0/all/0/1"&gt;Matko Bo&amp;#x161;njak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1"&gt;Thomas Kipf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerchner_A/0/1/0/all/0/1"&gt;Alexander Lerchner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1"&gt;Raia Hadsell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1"&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1"&gt;Charles Blundell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Effects of Learning in Morphologically Evolving Robot Systems. (arXiv:2107.08249v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.08249</id>
        <link href="http://arxiv.org/abs/2107.08249"/>
        <updated>2021-07-20T02:04:46.996Z</updated>
        <summary type="html"><![CDATA[When controllers (brains) and morphologies (bodies) of robots simultaneously
evolve, this can lead to a problem, namely the brain & body mismatch problem.
In this research, we propose a solution of lifetime learning. We set up a
system where modular robots can create offspring that inherit the bodies of
parents by recombination and mutation. With regards to the brains of the
offspring, we use two methods to create them. The first one entails solely
evolution which means the brain of a robot child is inherited from its parents.
The second approach is evolution plus learning which means the brain of a child
is inherited as well, but additionally is developed by a learning algorithm -
RevDEknn. We compare these two methods by running experiments in a simulator
called Revolve and use efficiency, efficacy, and the morphology intelligence of
the robots for the comparison. The experiments show that the evolution plus
learning method does not only lead to a higher fitness level, but also to more
morphologically evolving robots. This constitutes a quantitative demonstration
that changes in the brain can induce changes in the body, leading to the
concept of morphological intelligence, which is quantified by the learning
delta, meaning the ability of a morphology to facilitate learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1"&gt;Jakub M. Tomczak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eiben_A/0/1/0/all/0/1"&gt;Agoston E. Eiben&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Fairness Testing of Neural Classifiers through Adversarial Sampling. (arXiv:2107.08176v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08176</id>
        <link href="http://arxiv.org/abs/2107.08176"/>
        <updated>2021-07-20T02:04:46.989Z</updated>
        <summary type="html"><![CDATA[Although deep learning has demonstrated astonishing performance in many
applications, there are still concerns on their dependability. One desirable
property of deep learning applications with societal impact is fairness (i.e.,
non-discrimination). Unfortunately, discrimination might be intrinsically
embedded into the models due to discrimination in the training data. As a
countermeasure, fairness testing systemically identifies discriminative
samples, which can be used to retrain the model and improve its fairness.
Existing fairness testing approaches however have two major limitations. First,
they only work well on traditional machine learning models and have poor
performance (e.g., effectiveness and efficiency) on deep learning models.
Second, they only work on simple tabular data and are not applicable for
domains such as text. In this work, we bridge the gap by proposing a scalable
and effective approach for systematically searching for discriminative samples
while extending fairness testing to address a challenging domain, i.e., text
classification. Compared with state-of-the-art methods, our approach only
employs lightweight procedures like gradient computation and clustering, which
makes it significantly more scalable. Experimental results show that on
average, our approach explores the search space more effectively (9.62 and 2.38
times more than the state-of-art methods respectively on tabular and text
datasets) and generates much more individual discriminatory instances (24.95
and 2.68 times) within reasonable time. The retrained models reduce
discrimination by 57.2% and 60.2% respectively on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Peixin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1"&gt;Guoliang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1"&gt;Ting Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jin Song Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Representation Learning Does Not Generalize Strongly Within the Same Domain. (arXiv:2107.08221v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08221</id>
        <link href="http://arxiv.org/abs/2107.08221"/>
        <updated>2021-07-20T02:04:46.986Z</updated>
        <summary type="html"><![CDATA[An important component for generalization in machine learning is to uncover
underlying latent factors of variation as well as the mechanism through which
each factor acts in the world. In this paper, we test whether 17 unsupervised,
weakly supervised, and fully supervised representation learning approaches
correctly infer the generative factors of variation in simple datasets
(dSprites, Shapes3D, MPI3D). In contrast to prior robustness work that
introduces novel factors of variation during test time, such as blur or other
(un)structured noise, we here recompose, interpolate, or extrapolate only
existing factors of variation from the training data set (e.g., small and
medium-sized objects during training and large objects during testing). Models
that learn the correct mechanism should be able to generalize to this
benchmark. In total, we train and test 2000+ models and observe that all of
them struggle to learn the underlying mechanism regardless of supervision
signal and architectural bias. Moreover, the generalization capabilities of all
tested models drop significantly as we move from artificial datasets towards
more realistic real-world datasets. Despite their inability to identify the
correct mechanism, the models are quite modular as their ability to infer other
in-distribution factors remains fairly stable, providing only a single factor
is out-of-distribution. These results point to an important yet understudied
problem of learning mechanistic models of observations that can facilitate
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1"&gt;Lukas Schott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1"&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1"&gt;Frederik Tr&amp;#xe4;uble&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1"&gt;Peter Gehler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1"&gt;Chris Russell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1"&gt;Francesco Locatello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Constraints in First-Order Optimization: A View from Non-Smooth Dynamical Systems. (arXiv:2107.08225v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.08225</id>
        <link href="http://arxiv.org/abs/2107.08225"/>
        <updated>2021-07-20T02:04:46.985Z</updated>
        <summary type="html"><![CDATA[We introduce a class of first-order methods for smooth constrained
optimization that are based on an analogy to non-smooth dynamical systems. Two
distinctive features of our approach are that (i) projections or optimizations
over the entire feasible set are avoided, in stark contrast to projected
gradient methods or the Frank-Wolfe method, and (ii) iterates are allowed to
become infeasible, which differs from active set or feasible direction methods,
where the descent motion stops as soon as a new constraint is encountered. The
resulting algorithmic procedure is simple to implement even when constraints
are nonlinear, and is suitable for large-scale constrained optimization
problems in which the feasible set fails to have a simple structure. The key
underlying idea is that constraints are expressed in terms of velocities
instead of positions, which has the algorithmic consequence that optimizations
over feasible sets at each iteration are replaced with optimizations over
local, sparse convex approximations. The result is a simplified suite of
algorithms and an expanded range of possible applications in machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Muehlebach_M/0/1/0/all/0/1"&gt;Michael Muehlebach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoFL: Enabling Heterogeneity-Aware Energy Efficient Federated Learning. (arXiv:2107.08147v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08147</id>
        <link href="http://arxiv.org/abs/2107.08147"/>
        <updated>2021-07-20T02:04:46.984Z</updated>
        <summary type="html"><![CDATA[Federated learning enables a cluster of decentralized mobile devices at the
edge to collaboratively train a shared machine learning model, while keeping
all the raw training samples on device. This decentralized training approach is
demonstrated as a practical solution to mitigate the risk of privacy leakage.
However, enabling efficient FL deployment at the edge is challenging because of
non-IID training data distribution, wide system heterogeneity and
stochastic-varying runtime effects in the field. This paper jointly optimizes
time-to-convergence and energy efficiency of state-of-the-art FL use cases by
taking into account the stochastic nature of edge execution. We propose AutoFL
by tailor-designing a reinforcement learning algorithm that learns and
determines which K participant devices and per-device execution targets for
each FL model aggregation round in the presence of stochastic runtime variance,
system and data heterogeneity. By considering the unique characteristics of FL
edge deployment judiciously, AutoFL achieves 3.6 times faster model convergence
time and 4.7 and 5.2 times higher energy efficiency for local clients and
globally over the cluster of K participants, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young Geun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Carole-Jean Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEDS-Bench: Behavior of EHR-models under Distributional Shift--A Benchmark. (arXiv:2107.08189v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08189</id>
        <link href="http://arxiv.org/abs/2107.08189"/>
        <updated>2021-07-20T02:04:46.984Z</updated>
        <summary type="html"><![CDATA[Machine learning has recently demonstrated impressive progress in predictive
accuracy across a wide array of tasks. Most ML approaches focus on
generalization performance on unseen data that are similar to the training data
(In-Distribution, or IND). However, real world applications and deployments of
ML rarely enjoy the comfort of encountering examples that are always IND. In
such situations, most ML models commonly display erratic behavior on
Out-of-Distribution (OOD) examples, such as assigning high confidence to wrong
predictions, or vice-versa. Implications of such unusual model behavior are
further exacerbated in the healthcare setting, where patient health can
potentially be put at risk. It is crucial to study the behavior and robustness
properties of models under distributional shift, understand common failure
modes, and take mitigation steps before the model is deployed. Having a
benchmark that shines light upon these aspects of a model is a first and
necessary step in addressing the issue. Recent work and interest in increasing
model robustness in OOD settings have focused more on image modality, while the
Electronic Health Record (EHR) modality is still largely under-explored. We aim
to bridge this gap by releasing BEDS-Bench, a benchmark for quantifying the
behavior of ML models over EHR data under OOD settings. We use two open access,
de-identified EHR datasets to construct several OOD data settings to run tests
on, and measure relevant metrics that characterize crucial aspects of a model's
OOD behavior. We evaluate several learning algorithms under BEDS-Bench and find
that all of them show poor generalization performance under distributional
shift in general. Our results highlight the need and the potential to improve
robustness of EHR models under distributional shift, and BEDS-Bench provides
one way to measure progress towards that goal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avati_A/0/1/0/all/0/1"&gt;Anand Avati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seneviratne_M/0/1/0/all/0/1"&gt;Martin Seneviratne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_E/0/1/0/all/0/1"&gt;Emily Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1"&gt;Andrew M. Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Markov Blanket Discovery using Minimum Message Length. (arXiv:2107.08140v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08140</id>
        <link href="http://arxiv.org/abs/2107.08140"/>
        <updated>2021-07-20T02:04:46.983Z</updated>
        <summary type="html"><![CDATA[Causal discovery automates the learning of causal Bayesian networks from data
and has been of active interest from their beginning. With the sourcing of
large data sets off the internet, interest in scaling up to very large data
sets has grown. One approach to this is to parallelize search using Markov
Blanket (MB) discovery as a first step, followed by a process of combining MBs
in a global causal model. We develop and explore three new methods of MB
discovery using Minimum Message Length (MML) and compare them empirically to
the best existing methods, whether developed specifically as MB discovery or as
feature selection. Our best MML method is consistently competitive and has some
advantageous features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korb_K/0/1/0/all/0/1"&gt;Kevin B Korb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allison_L/0/1/0/all/0/1"&gt;Lloyd Allison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Megaverse: Simulating Embodied Agents at One Million Experiences per Second. (arXiv:2107.08170v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08170</id>
        <link href="http://arxiv.org/abs/2107.08170"/>
        <updated>2021-07-20T02:04:46.983Z</updated>
        <summary type="html"><![CDATA[We present Megaverse, a new 3D simulation platform for reinforcement learning
and embodied AI research. The efficient design of our engine enables
physics-based simulation with high-dimensional egocentric observations at more
than 1,000,000 actions per second on a single 8-GPU node. Megaverse is up to
70x faster than DeepMind Lab in fully-shaded 3D scenes with interactive
objects. We achieve this high simulation performance by leveraging batched
simulation, thereby taking full advantage of the massive parallelism of modern
GPUs. We use Megaverse to build a new benchmark that consists of several
single-agent and multi-agent tasks covering a variety of cognitive challenges.
We evaluate model-free RL on this benchmark to provide baselines and facilitate
future research. The source code is available at https://www.megaverse.info]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petrenko_A/0/1/0/all/0/1"&gt;Aleksei Petrenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1"&gt;Erik Wijmans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shacklett_B/0/1/0/all/0/1"&gt;Brennan Shacklett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1"&gt;Vladlen Koltun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Declarative Machine Learning Systems. (arXiv:2107.08148v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08148</id>
        <link href="http://arxiv.org/abs/2107.08148"/>
        <updated>2021-07-20T02:04:46.981Z</updated>
        <summary type="html"><![CDATA[In the last years machine learning (ML) has moved from a academic endeavor to
a pervasive technology adopted in almost every aspect of computing. ML-powered
products are now embedded in our digital lives: from recommendations of what to
watch, to divining our search intent, to powering virtual assistants in
consumer and enterprise settings. Recent successes in applying ML in natural
sciences revealed that ML can be used to tackle some of the hardest real-world
problems humanity faces today. For these reasons ML has become central in the
strategy of tech companies and has gathered even more attention from academia
than ever before. Despite these successes, what we have witnessed so far is
just the beginning. Right now the people training and using ML models are
expert developers working within large organizations, but we believe the next
wave of ML systems will allow a larger amount of people, potentially without
coding skills, to perform the same tasks. These new ML systems will not require
users to fully understand all the details of how models are trained and
utilized for obtaining predictions. Declarative interfaces are well suited for
this goal, by hiding complexity and favouring separation of interests, and can
lead to increased productivity. We worked on such abstract interfaces by
developing two declarative ML systems, Overton and Ludwig, that require users
to declare only their data schema (names and types of inputs) and tasks rather
then writing low level ML code. In this article we will describe how ML systems
are currently structured, highlight important factors for their success and
adoption, what are the issues current ML systems are facing and how the systems
we developed addressed them. Finally we will talk about learnings from the
development of ML systems throughout the years and how we believe the next
generation of ML systems will look like.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Molino_P/0/1/0/all/0/1"&gt;Piero Molino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1"&gt;Christopher R&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Renyi Differential Privacy of the Subsampled Shuffle Model in Distributed Learning. (arXiv:2107.08763v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08763</id>
        <link href="http://arxiv.org/abs/2107.08763"/>
        <updated>2021-07-20T02:04:46.980Z</updated>
        <summary type="html"><![CDATA[We study privacy in a distributed learning framework, where clients
collaboratively build a learning model iteratively through interactions with a
server from whom we need privacy. Motivated by stochastic optimization and the
federated learning (FL) paradigm, we focus on the case where a small fraction
of data samples are randomly sub-sampled in each round to participate in the
learning process, which also enables privacy amplification. To obtain even
stronger local privacy guarantees, we study this in the shuffle privacy model,
where each client randomizes its response using a local differentially private
(LDP) mechanism and the server only receives a random permutation (shuffle) of
the clients' responses without their association to each client. The principal
result of this paper is a privacy-optimization performance trade-off for
discrete randomization mechanisms in this sub-sampled shuffle privacy model.
This is enabled through a new theoretical technique to analyze the Renyi
Differential Privacy (RDP) of the sub-sampled shuffle model. We numerically
demonstrate that, for important regimes, with composition our bound yields
significant improvement in privacy guarantee over the state-of-the-art
approximate Differential Privacy (DP) guarantee (with strong composition) for
sub-sampled shuffled models. We also demonstrate numerically significant
improvement in privacy-learning performance operating point using real data
sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girgis_A/0/1/0/all/0/1"&gt;Antonious M. Girgis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Data_D/0/1/0/all/0/1"&gt;Deepesh Data&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1"&gt;Suhas Diggavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interventional Sum-Product Networks: Causal Inference with Tractable Probabilistic Models. (arXiv:2102.10440v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10440</id>
        <link href="http://arxiv.org/abs/2102.10440"/>
        <updated>2021-07-20T02:04:46.980Z</updated>
        <summary type="html"><![CDATA[While probabilistic models are an important tool for studying causality,
doing so suffers from the intractability of inference. As a step towards
tractable causal models, we consider the problem of learning interventional
distributions using sum-product networks (SPNs) that are over-parameterized by
gate functions, e.g., neural networks. Providing an arbitrarily intervened
causal graph as input, effectively subsuming Pearl's do-operator, the gate
function predicts the parameters of the SPN. The resulting interventional SPNs
are motivated and illustrated by a structural causal model themed around
personal health. Our empirical evaluation on three benchmark data sets as well
as a synthetic health data set clearly demonstrates that interventional SPNs
indeed are both expressive in modelling and flexible in adapting to the
interventions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1"&gt;Matej Ze&amp;#x10d;evi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1"&gt;Devendra Singh Dhami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karanam_A/0/1/0/all/0/1"&gt;Athresh Karanam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natarajan_S/0/1/0/all/0/1"&gt;Sriraam Natarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Path Integrals for the Attribution of Model Uncertainties. (arXiv:2107.08756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08756</id>
        <link href="http://arxiv.org/abs/2107.08756"/>
        <updated>2021-07-20T02:04:46.979Z</updated>
        <summary type="html"><![CDATA[Enabling interpretations of model uncertainties is of key importance in
Bayesian machine learning applications. Often, this requires to meaningfully
attribute predictive uncertainties to source features in an image, text or
categorical array. However, popular attribution methods are particularly
designed for classification and regression scores. In order to explain
uncertainties, state of the art alternatives commonly procure counterfactual
feature vectors, and proceed by making direct comparisons. In this paper, we
leverage path integrals to attribute uncertainties in Bayesian differentiable
models. We present a novel algorithm that relies on in-distribution curves
connecting a feature vector to some counterfactual counterpart, and we retain
desirable properties of interpretability methods. We validate our approach on
benchmark image data sets with varying resolution, and show that it
significantly simplifies interpretability over the existing alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1"&gt;Iker Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skalski_P/0/1/0/all/0/1"&gt;Piotr Skalski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barns_Graham_A/0/1/0/all/0/1"&gt;Alec Barns-Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1"&gt;Jason Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_D/0/1/0/all/0/1"&gt;David Sutton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Nonconvex Framework for Structured Dynamic Covariance Recovery. (arXiv:2011.05601v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05601</id>
        <link href="http://arxiv.org/abs/2011.05601"/>
        <updated>2021-07-20T02:04:46.979Z</updated>
        <summary type="html"><![CDATA[We propose a flexible yet interpretable model for high-dimensional data with
time-varying second order statistics, motivated and applied to functional
neuroimaging data. Motivated by the neuroscience literature, we factorize the
covariances into sparse spatial and smooth temporal components. While this
factorization results in both parsimony and domain interpretability, the
resulting estimation problem is nonconvex. To this end, we design a two-stage
optimization scheme with a carefully tailored spectral initialization, combined
with iteratively refined alternating projected gradient descent. We prove a
linear convergence rate up to a nontrivial statistical error for the proposed
descent scheme and establish sample complexity guarantees for the estimator. We
further quantify the statistical error for the multivariate Gaussian case.
Empirical results using simulated and real brain imaging data illustrate that
our approach outperforms existing baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tsai_K/0/1/0/all/0/1"&gt;Katherine Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kolar_M/0/1/0/all/0/1"&gt;Mladen Kolar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Koyejo_O/0/1/0/all/0/1"&gt;Oluwasanmi Koyejo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CETransformer: Casual Effect Estimation via Transformer Based Representation Learning. (arXiv:2107.08714v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08714</id>
        <link href="http://arxiv.org/abs/2107.08714"/>
        <updated>2021-07-20T02:04:46.978Z</updated>
        <summary type="html"><![CDATA[Treatment effect estimation, which refers to the estimation of causal effects
and aims to measure the strength of the causal relationship, is of great
importance in many fields but is a challenging problem in practice. As present,
data-driven causal effect estimation faces two main challenges, i.e., selection
bias and the missing of counterfactual. To address these two issues, most of
the existing approaches tend to reduce the selection bias by learning a
balanced representation, and then to estimate the counterfactual through the
representation. However, they heavily rely on the finely hand-crafted metric
functions when learning balanced representations, which generally doesn't work
well for the situations where the original distribution is complicated. In this
paper, we propose a CETransformer model for casual effect estimation via
transformer based representation learning. To learn the representation of
covariates(features) robustly, a self-supervised transformer is proposed, by
which the correlation between covariates can be well exploited through
self-attention mechanism. In addition, an adversarial network is adopted to
balance the distribution of the treated and control groups in the
representation space. Experimental results on three real-world datasets
demonstrate the advantages of the proposed CETransformer, compared with the
state-of-the-art treatment effect estimation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zhenyu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhizhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1"&gt;Kun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhenfeng Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Transfer Learning on Graph Neural Networks. (arXiv:2107.08765v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08765</id>
        <link href="http://arxiv.org/abs/2107.08765"/>
        <updated>2021-07-20T02:04:46.977Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) is widely used to learn a powerful
representation of graph-structured data. Recent work demonstrates that
transferring knowledge from self-supervised tasks to downstream tasks could
further improve graph representation. However, there is an inherent gap between
self-supervised tasks and downstream tasks in terms of optimization objective
and training data. Conventional pre-training methods may be not effective
enough on knowledge transfer since they do not make any adaptation for
downstream tasks. To solve such problems, we propose a new transfer learning
paradigm on GNNs which could effectively leverage self-supervised tasks as
auxiliary tasks to help the target task. Our methods would adaptively select
and combine different auxiliary tasks with the target task in the fine-tuning
stage. We design an adaptive auxiliary loss weighting model to learn the
weights of auxiliary tasks by quantifying the consistency between auxiliary
tasks and the target task. In addition, we learn the weighting model through
meta-learning. Our methods can be applied to various transfer learning
approaches, it performs well not only in multi-task learning but also in
pre-training and fine-tuning. Comprehensive experiments on multiple downstream
tasks demonstrate that the proposed methods can effectively combine auxiliary
tasks with the target task and significantly improve the performance compared
to state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xueting Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhenhuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1"&gt;Bang An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1"&gt;Jing Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stock Movement Prediction with Financial News using Contextualized Embedding from BERT. (arXiv:2107.08721v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2107.08721</id>
        <link href="http://arxiv.org/abs/2107.08721"/>
        <updated>2021-07-20T02:04:46.976Z</updated>
        <summary type="html"><![CDATA[News events can greatly influence equity markets. In this paper, we are
interested in predicting the short-term movement of stock prices after
financial news events using only the headlines of the news. To achieve this
goal, we introduce a new text mining method called Fine-Tuned
Contextualized-Embedding Recurrent Neural Network (FT-CE-RNN). Compared with
previous approaches which use static vector representations of the news (static
embedding), our model uses contextualized vector representations of the
headlines (contextualized embeddings) generated from Bidirectional Encoder
Representations from Transformers (BERT). Our model obtains the
state-of-the-art result on this stock movement prediction task. It shows
significant improvement compared with other baseline models, in both accuracy
and trading simulations. Through various trading simulations based on millions
of headlines from Bloomberg News, we demonstrate the ability of this model in
real scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qinkai Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Attributed Graph Representations with Communicative Message Passing Transformer. (arXiv:2107.08773v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08773</id>
        <link href="http://arxiv.org/abs/2107.08773"/>
        <updated>2021-07-20T02:04:46.975Z</updated>
        <summary type="html"><![CDATA[Constructing appropriate representations of molecules lies at the core of
numerous tasks such as material science, chemistry and drug designs. Recent
researches abstract molecules as attributed graphs and employ graph neural
networks (GNN) for molecular representation learning, which have made
remarkable achievements in molecular graph modeling. Albeit powerful, current
models either are based on local aggregation operations and thus miss
higher-order graph properties or focus on only node information without fully
using the edge information. For this sake, we propose a Communicative Message
Passing Transformer (CoMPT) neural network to improve the molecular graph
representation by reinforcing message interactions between nodes and edges
based on the Transformer architecture. Unlike the previous transformer-style
GNNs that treat molecules as fully connected graphs, we introduce a message
diffusion mechanism to leverage the graph connectivity inductive bias and
reduce the message enrichment explosion. Extensive experiments demonstrated
that the proposed model obtained superior performances (around 4$\%$ on
average) against state-of-the-art baselines on seven chemical property datasets
(graph-level tasks) and two chemical shift datasets (node-level tasks). Further
visualization studies also indicated a better representation capacity achieved
by our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jianwen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuangjia Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Ying Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1"&gt;Jiahua Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuedong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MARC: Mining Association Rules from datasets by using Clustering models. (arXiv:2107.08814v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2107.08814</id>
        <link href="http://arxiv.org/abs/2107.08814"/>
        <updated>2021-07-20T02:04:46.975Z</updated>
        <summary type="html"><![CDATA[Association rules are useful to discover relationships, which are mostly
hidden, between the different items in large datasets. Symbolic models are the
principal tools to extract association rules. This basic technique is
time-consuming, and it generates a big number of associated rules. To overcome
this drawback, we suggest a new method, called MARC, to extract the more
important association rules of two important levels: Type I, and Type II. This
approach relies on a multi-topographic unsupervised neural network model as
well as clustering quality measures that evaluate the success of a given
numerical classification model to behave as a natural symbolic model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shehabi_S/0/1/0/all/0/1"&gt;Shadi Al Shehabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baba_A/0/1/0/all/0/1"&gt;Abdullatif Baba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Class Classification for Wafer Map using Adversarial Autoencoder with DSVDD Prior. (arXiv:2107.08823v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08823</id>
        <link href="http://arxiv.org/abs/2107.08823"/>
        <updated>2021-07-20T02:04:46.974Z</updated>
        <summary type="html"><![CDATA[Recently, semiconductors' demand has exploded in virtual reality,
smartphones, wearable devices, the internet of things, robotics, and
automobiles. Semiconductor manufacturers want to make semiconductors with high
yields. To do this, manufacturers conduct many quality assurance activities.
Wafer map pattern classification is a typical way of quality assurance. The
defect pattern on the wafer map can tell us which process has a problem. Most
of the existing wafer map classification methods are based on supervised
methods. The supervised methods tend to have high performance, but they require
extensive labor and expert knowledge to produce labeled datasets with a
balanced distribution in mind. In the semiconductor manufacturing process, it
is challenging to get defect data with balanced distribution. In this paper, we
propose a one-class classification method using an Adversarial Autoencoder
(AAE) with Deep Support Vector Data Description (DSVDD) prior, which generates
random vectors within the hypersphere of DSVDD. We use the WM-811k dataset,
which consists of a real-world wafer map. We compare the F1 score performance
of our model with DSVDD and AAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jo_H/0/1/0/all/0/1"&gt;Ha Young Jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Adversarial Imitation Learning using Variational Models. (arXiv:2107.08829v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08829</id>
        <link href="http://arxiv.org/abs/2107.08829"/>
        <updated>2021-07-20T02:04:46.973Z</updated>
        <summary type="html"><![CDATA[Reward function specification, which requires considerable human effort and
iteration, remains a major impediment for learning behaviors through deep
reinforcement learning. In contrast, providing visual demonstrations of desired
behaviors often presents an easier and more natural way to teach agents. We
consider a setting where an agent is provided a fixed dataset of visual
demonstrations illustrating how to perform a task, and must learn to solve the
task using the provided demonstrations and unsupervised environment
interactions. This setting presents a number of challenges including
representation learning for visual observations, sample complexity due to high
dimensional spaces, and learning instability due to the lack of a fixed reward
or learning signal. Towards addressing these challenges, we develop a
variational model-based adversarial imitation learning (V-MAIL) algorithm. The
model-based approach provides a strong signal for representation learning,
enables sample efficiency, and improves the stability of adversarial training
by enabling on-policy learning. Through experiments involving several
vision-based locomotion and manipulation tasks, we find that V-MAIL learns
successful visuomotor policies in a sample-efficient manner, has better
stability compared to prior work, and also achieves higher asymptotic
performance. We further find that by transferring the learned models, V-MAIL
can learn new tasks from visual demonstrations without any additional
environment interactions. All results including videos can be found online at
\url{https://sites.google.com/view/variational-mail}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1"&gt;Rafael Rafailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tianhe Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1"&gt;Aravind Rajeswaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-UAV System for Exploration and Target Finding in Cluttered and GPS-Denied Environments. (arXiv:2107.08834v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.08834</id>
        <link href="http://arxiv.org/abs/2107.08834"/>
        <updated>2021-07-20T02:04:46.973Z</updated>
        <summary type="html"><![CDATA[The use of multi-rotor Unmanned Aerial Vehicles (UAVs) for search and rescue
as well as remote sensing is rapidly increasing. Multi-rotor UAVs, however,
have limited endurance. The range of UAV applications can be widened if teams
of multiple UAVs are used. We propose a framework for a team of UAVs to
cooperatively explore and find a target in complex GPS-denied environments with
obstacles. The team of UAVs autonomously navigates, explores, detects, and
finds the target in a cluttered environment with a known map. Examples of such
environments include indoor scenarios, urban or natural canyons, caves, and
tunnels, where the GPS signal is limited or blocked. The framework is based on
a probabilistic decentralised Partially Observable Markov Decision Process
which accounts for the uncertainties in sensing and the environment. The team
can cooperate efficiently, with each UAV sharing only limited processed
observations and their locations during the mission. The system is simulated
using the Robotic Operating System and Gazebo. Performance of the system with
an increasing number of UAVs in several indoor scenarios with obstacles is
tested. Results indicate that the proposed multi-UAV system has improvements in
terms of time-cost, the proportion of search area surveyed, as well as
successful rates for search and rescue missions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaolong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vanegas_F/0/1/0/all/0/1"&gt;Fernando Vanegas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1"&gt;Felipe Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanderson_C/0/1/0/all/0/1"&gt;Conrad Sanderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities. (arXiv:2103.01488v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01488</id>
        <link href="http://arxiv.org/abs/2103.01488"/>
        <updated>2021-07-20T02:04:46.972Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have been widely used to learn vector
representation of graph-structured data and achieved better task performance
than conventional methods. The foundation of GNNs is the message passing
procedure, which propagates the information in a node to its neighbors. Since
this procedure proceeds one step per layer, the range of the information
propagation among nodes is small in the lower layers, and it expands toward the
higher layers. Therefore, a GNN model has to be deep enough to capture global
structural information in a graph. On the other hand, it is known that deep GNN
models suffer from performance degradation because they lose nodes' local
information, which would be essential for good model performance, through many
message passing steps. In this study, we propose a multi-level attention
pooling (MLAP) for graph-level classification tasks, which can adapt to both
local and global structural information in a graph. It has an attention pooling
layer for each message passing step and computes the final graph representation
by unifying the layer-wise graph representations. The MLAP architecture allows
models to utilize the structural information of graphs with multiple levels of
localities because it preserves layer-wise information before losing them due
to oversmoothing. Results of our experiments show that the MLAP architecture
improves deeper models' performance in graph classification tasks compared to
the baseline architectures. In addition, analyses on the layer-wise graph
representations suggest that aggregating information from multiple levels of
localities indeed has the potential to improve the discriminability of learned
graph representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Itoh_T/0/1/0/all/0/1"&gt;Takeshi D. Itoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kubo_T/0/1/0/all/0/1"&gt;Takatomi Kubo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ikeda_K/0/1/0/all/0/1"&gt;Kazushi Ikeda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolutionary Generative Adversarial Networks with Crossover Based Knowledge Distillation. (arXiv:2101.11186v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11186</id>
        <link href="http://arxiv.org/abs/2101.11186"/>
        <updated>2021-07-20T02:04:46.971Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GAN) is an adversarial model, and it has
been demonstrated to be effective for various generative tasks. However, GAN
and its variants also suffer from many training problems, such as mode collapse
and gradient vanish. In this paper, we firstly propose a general crossover
operator, which can be widely applied to GANs using evolutionary strategies.
Then we design an evolutionary GAN framework C-GAN based on it. And we combine
the crossover operator with evolutionary generative adversarial networks (EGAN)
to implement the evolutionary generative adversarial networks with crossover
(CE-GAN). Under the premise that a variety of loss functions are used as
mutation operators to generate mutation individuals, we evaluate the generated
samples and allow the mutation individuals to learn experiences from the output
in a knowledge distillation manner, imitating the best output outcome,
resulting in better offspring. Then, we greedily selected the best offspring as
parents for subsequent training using discriminator as evaluator. Experiments
on real datasets demonstrate the effectiveness of CE-GAN and show that our
method is competitive in terms of generated images quality and time efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1"&gt;Xiaoyu Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shuai L&amp;#xfc;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Accuracy Model-Based Reinforcement Learning, a Survey. (arXiv:2107.08241v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08241</id>
        <link href="http://arxiv.org/abs/2107.08241"/>
        <updated>2021-07-20T02:04:46.880Z</updated>
        <summary type="html"><![CDATA[Deep reinforcement learning has shown remarkable success in the past few
years. Highly complex sequential decision making problems from game playing and
robotics have been solved with deep model-free methods. Unfortunately, the
sample complexity of model-free methods is often high. To reduce the number of
environment samples, model-based reinforcement learning creates an explicit
model of the environment dynamics. Achieving high model accuracy is a challenge
in high-dimensional problems. In recent years, a diverse landscape of
model-based methods has been introduced to improve model accuracy, using
methods such as uncertainty modeling, model-predictive control, latent models,
and end-to-end learning and planning. Some of these methods succeed in
achieving high accuracy at low sample complexity, most do so either in a
robotics or in a games context. In this paper, we survey these methods; we
explain in detail how they work and what their strengths and weaknesses are. We
conclude with a research agenda for future work to make the methods more robust
and more widely applicable to other applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1"&gt;Aske Plaat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kosters_W/0/1/0/all/0/1"&gt;Walter Kosters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preuss_M/0/1/0/all/0/1"&gt;Mike Preuss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subset-of-Data Variational Inference for Deep Gaussian-Processes Regression. (arXiv:2107.08265v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.08265</id>
        <link href="http://arxiv.org/abs/2107.08265"/>
        <updated>2021-07-20T02:04:46.879Z</updated>
        <summary type="html"><![CDATA[Deep Gaussian Processes (DGPs) are multi-layer, flexible extensions of
Gaussian processes but their training remains challenging. Sparse
approximations simplify the training but often require optimization over a
large number of inducing inputs and their locations across layers. In this
paper, we simplify the training by setting the locations to a fixed subset of
data and sampling the inducing inputs from a variational distribution. This
reduces the trainable parameters and computation cost without significant
performance degradations, as demonstrated by our empirical results on
regression problems. Our modifications simplify and stabilize DGP training
while making it amenable to sampling schemes for setting the inducing inputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jain_A/0/1/0/all/0/1"&gt;Ayush Jain&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/stat/1/au:+Srijith_P/0/1/0/all/0/1"&gt;P. K. Srijith&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt; (2) ((1) Department of Computer Science and Engineering, Indian Institute of Technology Hyderabad, India, (2) RIKEN Center for AI Project, Tokyo, Japan)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Optimizing Engagement to Measuring Value. (arXiv:2008.12623v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12623</id>
        <link href="http://arxiv.org/abs/2008.12623"/>
        <updated>2021-07-20T02:04:46.878Z</updated>
        <summary type="html"><![CDATA[Most recommendation engines today are based on predicting user engagement,
e.g. predicting whether a user will click on an item or not. However, there is
potentially a large gap between engagement signals and a desired notion of
"value" that is worth optimizing for. We use the framework of measurement
theory to (a) confront the designer with a normative question about what the
designer values, (b) provide a general latent variable model approach that can
be used to operationalize the target construct and directly optimize for it,
and (c) guide the designer in evaluating and revising their operationalization.
We implement our approach on the Twitter platform on millions of users. In line
with established approaches to assessing the validity of measurements, we
perform a qualitative evaluation of how well our model captures a desired
notion of "value".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milli_S/0/1/0/all/0/1"&gt;Smitha Milli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belli_L/0/1/0/all/0/1"&gt;Luca Belli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1"&gt;Moritz Hardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08199</id>
        <link href="http://arxiv.org/abs/2107.08199"/>
        <updated>2021-07-20T02:04:46.868Z</updated>
        <summary type="html"><![CDATA[The Transformer architecture is widely used for machine translation tasks.
However, its resource-intensive nature makes it challenging to implement on
constrained embedded devices, particularly where available hardware resources
can vary at run-time. We propose a dynamic machine translation model that
scales the Transformer architecture based on the available resources at any
particular time. The proposed approach, 'Dynamic-HAT', uses a HAT
SuperTransformer as the backbone to search for SubTransformers with different
accuracy-latency trade-offs at design time. The optimal SubTransformers are
sampled from the SuperTransformer at run-time, depending on latency
constraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses
inherited SubTransformers sampled directly from the SuperTransformer with a
switching time of <1s. Using inherited SubTransformers results in a BLEU score
loss of <1.5% because the SubTransformer configuration is not retrained from
scratch after sampling. However, to recover this loss in performance, the
dimensions of the design space can be reduced to tailor it to a family of
target hardware. The new reduced design space results in a BLEU score increase
of approximately 1% for sub-optimal models from the original design space, with
a wide range for performance scaling between 0.356s - 1.526s for the GPU and
2.9s - 7.31s for the CPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1"&gt;Hishan Parry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1"&gt;Lei Xun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1"&gt;Amin Sabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1"&gt;Jia Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1"&gt;Geoff V. Merrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic Decision Making with Conditional Fairness. (arXiv:2006.10483v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10483</id>
        <link href="http://arxiv.org/abs/2006.10483"/>
        <updated>2021-07-20T02:04:46.867Z</updated>
        <summary type="html"><![CDATA[Nowadays fairness issues have raised great concerns in decision-making
systems. Various fairness notions have been proposed to measure the degree to
which an algorithm is unfair. In practice, there frequently exist a certain set
of variables we term as fair variables, which are pre-decision covariates such
as users' choices. The effects of fair variables are irrelevant in assessing
the fairness of the decision support algorithm. We thus define conditional
fairness as a more sound fairness metric by conditioning on the fairness
variables. Given different prior knowledge of fair variables, we demonstrate
that traditional fairness notations, such as demographic parity and equalized
odds, are special cases of our conditional fairness notations. Moreover, we
propose a Derivable Conditional Fairness Regularizer (DCFR), which can be
integrated into any decision-making model, to track the trade-off between
precision and fairness of algorithmic decision making. Specifically, an
adversarial representation based conditional independence loss is proposed in
our DCFR to measure the degree of unfairness. With extensive experiments on
three real-world datasets, we demonstrate the advantages of our conditional
fairness notation and DCFR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Renzhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1"&gt;Peng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1"&gt;Kun Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Linjun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zheyan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1"&gt;Wei Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relaxed Wasserstein with Applications to GANs. (arXiv:1705.07164v8 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1705.07164</id>
        <link href="http://arxiv.org/abs/1705.07164"/>
        <updated>2021-07-20T02:04:46.866Z</updated>
        <summary type="html"><![CDATA[Wasserstein Generative Adversarial Networks (WGANs) provide a versatile class
of models, which have attracted great attention in various applications.
However, this framework has two main drawbacks: (i) Wasserstein-1 (or
Earth-Mover) distance is restrictive such that WGANs cannot always fit data
geometry well; (ii) It is difficult to achieve fast training of WGANs. In this
paper, we propose a new class of \textit{Relaxed Wasserstein} (RW) distances by
generalizing Wasserstein-1 distance with Bregman cost functions. We show that
RW distances achieve nice statistical properties while not sacrificing the
computational tractability. Combined with the GANs framework, we develop
Relaxed WGANs (RWGANs) which are not only statistically flexible but can be
approximated efficiently using heuristic approaches. Experiments on real images
demonstrate that the RWGAN with Kullback-Leibler (KL) cost function outperforms
other competing approaches, e.g., WGANs, even with gradient penalty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hong_J/0/1/0/all/0/1"&gt;Johnny Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_N/0/1/0/all/0/1"&gt;Nan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08369</id>
        <link href="http://arxiv.org/abs/2107.08369"/>
        <updated>2021-07-20T02:04:46.865Z</updated>
        <summary type="html"><![CDATA[Floods wreak havoc throughout the world, causing billions of dollars in
damages, and uprooting communities, ecosystems and economies. Accurate and
robust flood detection including delineating open water flood areas and
identifying flood levels can aid in disaster response and mitigation. However,
estimating flood levels remotely is of essence as physical access to flooded
areas is limited and the ability to deploy instruments in potential flood zones
can be dangerous. Aligning flood extent mapping with local topography can
provide a plan-of-action that the disaster response team can consider. Thus,
remote flood level estimation via satellites like Sentinel-1 can prove to be
remedial. The Emerging Techniques in Computational Intelligence (ETCI)
competition on Flood Detection tasked participants with predicting flooded
pixels after training with synthetic aperture radar (SAR) images in a
supervised setting. We use a cyclical approach involving two stages (1)
training an ensemble model of multiple UNet architectures with available high
and low confidence labeled data and, (2) generating pseudo labels or low
confidence labels on the unlabeled test dataset, and then, combining the
generated labels with the previously available high confidence labeled dataset.
This assimilated dataset is used for the next round of training ensemble
models. This cyclical process is repeated until the performance improvement
plateaus. Additionally, we post process our results with Conditional Random
Fields. Our approach sets a high score on the public leaderboard for the ETCI
competition with 0.7654 IoU. Our method, which we release with all the code
including trained models, can also be used as an open science benchmark for the
Sentinel-1 released dataset on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Sayak Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composite Neural Network: Theory and Application to PM2.5 Prediction. (arXiv:1910.09739v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.09739</id>
        <link href="http://arxiv.org/abs/1910.09739"/>
        <updated>2021-07-20T02:04:46.865Z</updated>
        <summary type="html"><![CDATA[This work investigates the framework and performance issues of the composite
neural network, which is composed of a collection of pre-trained and
non-instantiated neural network models connected as a rooted directed acyclic
graph for solving complicated applications. A pre-trained neural network model
is generally well trained, targeted to approximate a specific function. Despite
a general belief that a composite neural network may perform better than a
single component, the overall performance characteristics are not clear. In
this work, we construct the framework of a composite network, and prove that a
composite neural network performs better than any of its pre-trained components
with a high probability bound. In addition, if an extra pre-trained component
is added to a composite network, with high probability, the overall performance
will not be degraded. In the study, we explore a complicated application --
PM2.5 prediction -- to illustrate the correctness of the proposed composite
network theory. In the empirical evaluations of PM2.5 prediction, the
constructed composite neural network models support the proposed theory and
perform better than other machine learning models, demonstrate the advantages
of the proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Chuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Meng Chang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards autonomic orchestration of machine learning pipelines in future networks. (arXiv:2107.08194v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2107.08194</id>
        <link href="http://arxiv.org/abs/2107.08194"/>
        <updated>2021-07-20T02:04:46.864Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) techniques are being increasingly used in mobile
networks for network planning, operation, management, optimisation and much
more. These techniques are realised using a set of logical nodes known as ML
pipeline. A single network operator might have thousands of such ML pipelines
distributed across its network. These pipelines need to be managed and
orchestrated across network domains. Thus it is essential to have autonomic
multi-domain orchestration of ML pipelines in mobile networks. International
Telecommunications Union (ITU) has provided an architectural framework for
management and orchestration of ML pipelines in future networks. We extend this
framework to enable autonomic orchestration of ML pipelines across multiple
network domains. We present our system architecture and describe its
application using a smart factory use case. Our work allows autonomic
orchestration of multi-domain ML pipelines in a standardised, technology
agnostic, privacy preserving fashion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dandekar_A/0/1/0/all/0/1"&gt;Abhishek Dandekar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning. (arXiv:2002.06470v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.06470</id>
        <link href="http://arxiv.org/abs/2002.06470"/>
        <updated>2021-07-20T02:04:46.864Z</updated>
        <summary type="html"><![CDATA[Uncertainty estimation and ensembling methods go hand-in-hand. Uncertainty
estimation is one of the main benchmarks for assessment of ensembling
performance. At the same time, deep learning ensembles have provided
state-of-the-art results in uncertainty estimation. In this work, we focus on
in-domain uncertainty for image classification. We explore the standards for
its quantification and point out pitfalls of existing metrics. Avoiding these
pitfalls, we perform a broad study of different ensembling techniques. To
provide more insight in this study, we introduce the deep ensemble equivalent
score (DEE) and show that many sophisticated ensembling techniques are
equivalent to an ensemble of only few independently trained networks in terms
of test performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ashukha_A/0/1/0/all/0/1"&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lyzhov_A/0/1/0/all/0/1"&gt;Alexander Lyzhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Molchanov_D/0/1/0/all/0/1"&gt;Dmitry Molchanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration. (arXiv:2006.08170v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08170</id>
        <link href="http://arxiv.org/abs/2006.08170"/>
        <updated>2021-07-20T02:04:46.863Z</updated>
        <summary type="html"><![CDATA[Meta reinforcement learning (meta-RL) extracts knowledge from previous tasks
and achieves fast adaptation to new tasks. Despite recent progress, efficient
exploration in meta-RL remains a key challenge in sparse-reward tasks, as it
requires quickly finding informative task-relevant experiences in both
meta-training and adaptation. To address this challenge, we explicitly model an
exploration policy learning problem for meta-RL, which is separated from
exploitation policy learning, and introduce a novel empowerment-driven
exploration objective, which aims to maximize information gain for task
identification. We derive a corresponding intrinsic reward and develop a new
off-policy meta-RL framework, which efficiently learns separate context-aware
exploration and exploitation policies by sharing the knowledge of task
inference. Experimental evaluation shows that our meta-RL method significantly
outperforms state-of-the-art baselines on various sparse-reward MuJoCo
locomotion tasks and more complex sparse-reward Meta-World tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingfeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Copying Behaviors of Pre-Training for Neural Machine Translation. (arXiv:2107.08212v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08212</id>
        <link href="http://arxiv.org/abs/2107.08212"/>
        <updated>2021-07-20T02:04:46.862Z</updated>
        <summary type="html"><![CDATA[Previous studies have shown that initializing neural machine translation
(NMT) models with the pre-trained language models (LM) can speed up the model
training and boost the model performance. In this work, we identify a critical
side-effect of pre-training for NMT, which is due to the discrepancy between
the training objectives of LM-based pre-training and NMT. Since the LM
objective learns to reconstruct a few source tokens and copy most of them, the
pre-training initialization would affect the copying behaviors of NMT models.
We provide a quantitative analysis of copying behaviors by introducing a metric
called copying ratio, which empirically shows that pre-training based NMT
models have a larger copying ratio than the standard one. In response to this
problem, we propose a simple and effective method named copying penalty to
control the copying behaviors in decoding. Extensive experiments on both
in-domain and out-of-domain benchmarks show that the copying penalty method
consistently improves translation performance by controlling copying behaviors
for pre-training based NMT models. Source code is freely available at
https://github.com/SunbowLiu/CopyingPenalty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuebo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Longyue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1"&gt;Derek F. Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"&gt;Liang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1"&gt;Lidia S. Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuming Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhaopeng Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient and Visualizable Convolutional Neural Networks for COVID-19 Classification Using Chest CT. (arXiv:2012.11860v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11860</id>
        <link href="http://arxiv.org/abs/2012.11860"/>
        <updated>2021-07-20T02:04:46.862Z</updated>
        <summary type="html"><![CDATA[With COVID-19 cases rising rapidly, deep learning has emerged as a promising
diagnosis technique. However, identifying the most accurate models to
characterize COVID-19 patients is challenging because comparing results
obtained with different types of data and acquisition processes is non-trivial.
In this paper we designed, evaluated, and compared the performance of 20
convolutional neutral networks in classifying patients as COVID-19 positive,
healthy, or suffering from other pulmonary lung infections based on Chest CT
scans, serving as the first to consider the EfficientNet family for COVID-19
diagnosis and employ intermediate activation maps for visualizing model
performance. All models are trained and evaluated in Python using 4173 Chest CT
images from the dataset entitled "A COVID multiclass dataset of CT scans," with
2168, 758, and 1247 images of patients that are COVID-19 positive, healthy, or
suffering from other pulmonary infections, respectively. EfficientNet-B5 was
identified as the best model with an F1 score of 0.9769+/-0.0046, accuracy of
0.9759+/-0.0048, sensitivity of 0.9788+/-0.0055, specificity of
0.9730+/-0.0057, and precision of 0.9751 +/- 0.0051. On an alternate 2-class
dataset, EfficientNetB5 obtained an accuracy of 0.9845+/-0.0109, F1 score of
0.9599+/-0.0251, sensitivity of 0.9682+/-0.0099, specificity of
0.9883+/-0.0150, and precision of 0.9526 +/- 0.0523. Intermediate activation
maps and Gradient-weighted Class Activation Mappings offered
human-interpretable evidence of the model's perception of ground-class
opacities and consolidations, hinting towards a promising use-case of
artificial intelligence-assisted radiology tools. With a prediction speed of
under 0.1 seconds on GPUs and 0.5 seconds on CPUs, our proposed model offers a
rapid, scalable, and accurate diagnostic for COVID-19.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Garg_A/0/1/0/all/0/1"&gt;Aksh Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salehi_S/0/1/0/all/0/1"&gt;Sana Salehi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rocca_M/0/1/0/all/0/1"&gt;Marianna La Rocca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garner_R/0/1/0/all/0/1"&gt;Rachael Garner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duncan_D/0/1/0/all/0/1"&gt;Dominique Duncan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mediated Uncoupled Learning: Learning Functions without Direct Input-output Correspondences. (arXiv:2107.08135v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.08135</id>
        <link href="http://arxiv.org/abs/2107.08135"/>
        <updated>2021-07-20T02:04:46.861Z</updated>
        <summary type="html"><![CDATA[Ordinary supervised learning is useful when we have paired training data of
input $X$ and output $Y$. However, such paired data can be difficult to collect
in practice. In this paper, we consider the task of predicting $Y$ from $X$
when we have no paired data of them, but we have two separate, independent
datasets of $X$ and $Y$ each observed with some mediating variable $U$, that
is, we have two datasets $S_X = \{(X_i, U_i)\}$ and $S_Y = \{(U'_j, Y'_j)\}$. A
naive approach is to predict $U$ from $X$ using $S_X$ and then $Y$ from $U$
using $S_Y$, but we show that this is not statistically consistent. Moreover,
predicting $U$ can be more difficult than predicting $Y$ in practice, e.g.,
when $U$ has higher dimensionality. To circumvent the difficulty, we propose a
new method that avoids predicting $U$ but directly learns $Y = f(X)$ by
training $f(X)$ with $S_{X}$ to predict $h(U)$ which is trained with $S_{Y}$ to
approximate $Y$. We prove statistical consistency and error bounds of our
method and experimentally confirm its practical usefulness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yamane_I/0/1/0/all/0/1"&gt;Ikko Yamane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Honda_J/0/1/0/all/0/1"&gt;Junya Honda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yger_F/0/1/0/all/0/1"&gt;Florian Yger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection Based on Multiple-Hypothesis Autoencoder. (arXiv:2107.08790v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08790</id>
        <link href="http://arxiv.org/abs/2107.08790"/>
        <updated>2021-07-20T02:04:46.860Z</updated>
        <summary type="html"><![CDATA[Recently Autoencoder(AE) based models are widely used in the field of anomaly
detection. A model trained with normal data generates a larger restoration
error for abnormal data. Whether or not abnormal data is determined by
observing the restoration error. It takes a lot of cost and time to obtain
abnormal data in the industrial field. Therefore the model trains only normal
data and detects abnormal data in the inference phase. However, the restoration
area for the input data of AE is limited in the latent space. To solve this
problem, we propose Multiple-hypothesis Autoencoder(MH-AE) model composed of
several decoders. MH-AE model increases the restoration area through contention
between decoders. The proposed method shows that the anomaly detection
performance is improved compared to the traditional AE for various input
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;JoonSung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1"&gt;YeongHyeon Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Bayesian Learning with Diagonal Quasi-Newton Method For Large Scale Classification. (arXiv:2107.08195v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08195</id>
        <link href="http://arxiv.org/abs/2107.08195"/>
        <updated>2021-07-20T02:04:46.829Z</updated>
        <summary type="html"><![CDATA[Sparse Bayesian Learning (SBL) constructs an extremely sparse probabilistic
model with very competitive generalization. However, SBL needs to invert a big
covariance matrix with complexity O(M^3 ) (M: feature size) for updating the
regularization priors, making it difficult for practical use. There are three
issues in SBL: 1) Inverting the covariance matrix may obtain singular solutions
in some cases, which hinders SBL from convergence; 2) Poor scalability to
problems with high dimensional feature space or large data size; 3) SBL easily
suffers from memory overflow for large-scale data. This paper addresses these
issues with a newly proposed diagonal Quasi-Newton (DQN) method for SBL called
DQN-SBL where the inversion of big covariance matrix is ignored so that the
complexity and memory storage are reduced to O(M). The DQN-SBL is thoroughly
evaluated on non-linear classifiers and linear feature selection using various
benchmark datasets of different sizes. Experimental results verify that DQN-SBL
receives competitive generalization with a very sparse model and scales well to
large-scale problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiahua Luo&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Vong_C/0/1/0/all/0/1"&gt;Chi-Man Vong&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1"&gt;Jie Du&lt;/a&gt; (2) ((1) Department of Computer and Information Science, University of Macau, Macao SAR, China, (2) School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, China)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Objectives of Extractive Question Answering. (arXiv:2008.12804v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12804</id>
        <link href="http://arxiv.org/abs/2008.12804"/>
        <updated>2021-07-20T02:04:46.806Z</updated>
        <summary type="html"><![CDATA[This work demonstrates that, contrary to a common belief, using the objective
with independence assumption for modelling the span probability $P(a_s,a_e) =
P(a_s)P(a_e)$ of span starting at position $a_s$ and ending at position $a_e$
has adverse effects. Therefore we propose multiple approaches to modelling
joint probability $P(a_s,a_e)$ directly. Among those, we propose a compound
objective, composed from the joint probability while still keeping the
objective with independence assumption as an auxiliary objective. We find that
the compound objective is consistently superior or equal to other assumptions
in exact match. Additionally, we identified common errors caused by the
assumption of independence and manually checked the counterpart predictions,
demonstrating the impact of the compound objective on the real examples. Our
findings are supported via experiments with three extractive QA models (BIDAF,
BERT, ALBERT) over six datasets and our code, individual results and manual
analysis are available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1"&gt;Martin Fajcik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1"&gt;Josef Jon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1"&gt;Pavel Smrz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making transport more robust and interpretable by moving data through a small number of anchor points. (arXiv:2012.11589v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11589</id>
        <link href="http://arxiv.org/abs/2012.11589"/>
        <updated>2021-07-20T02:04:46.806Z</updated>
        <summary type="html"><![CDATA[Optimal transport (OT) is a widely used technique for distribution alignment,
with applications throughout the machine learning, graphics, and vision
communities. Without any additional structural assumptions on trans-port,
however, OT can be fragile to outliers or noise, especially in high dimensions.
Here, we introduce a new form of structured OT that simultaneously learns
low-dimensional structure in data while leveraging this structure to solve the
alignment task. Compared with OT, the resulting transport plan has better
structural interpretability, highlighting the connections between individual
data points and local geometry, and is more robust to noise and sampling. We
apply the method to synthetic as well as real datasets, where we show that our
method can facilitate alignment in noisy settings and can be used to both
correct and interpret domain shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chi-Heng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azabou_M/0/1/0/all/0/1"&gt;Mehdi Azabou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1"&gt;Eva L. Dyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Naive Bayes Classifier using Smooth Sensitivity. (arXiv:2003.13955v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.13955</id>
        <link href="http://arxiv.org/abs/2003.13955"/>
        <updated>2021-07-20T02:04:46.805Z</updated>
        <summary type="html"><![CDATA[With the increasing collection of users' data, protecting individual privacy
has gained more interest. Differential Privacy is a strong concept of
protecting individuals. Naive Bayes is one of the popular machine learning
algorithm, used as a baseline for many tasks. In this work, we have provided a
differentially private Naive Bayes classifier that adds noise proportional to
the Smooth Sensitivity of its parameters. We have compared our result to
Vaidya, Shafiq, Basu, and Hong in which they have scaled the noise to the
global sensitivity of the parameters. Our experiment results on the real-world
datasets show that the accuracy of our method has improved significantly while
still preserving $\varepsilon$-differential privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zafarani_F/0/1/0/all/0/1"&gt;Farzad Zafarani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifton_C/0/1/0/all/0/1"&gt;Chris Clifton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Optimization in Adversarial MDPs: Improved Exploration via Dilated Bonuses. (arXiv:2107.08346v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08346</id>
        <link href="http://arxiv.org/abs/2107.08346"/>
        <updated>2021-07-20T02:04:46.804Z</updated>
        <summary type="html"><![CDATA[Policy optimization is a widely-used method in reinforcement learning. Due to
its local-search nature, however, theoretical guarantees on global optimality
often rely on extra assumptions on the Markov Decision Processes (MDPs) that
bypass the challenge of global exploration. To eliminate the need of such
assumptions, in this work, we develop a general solution that adds dilated
bonuses to the policy update to facilitate global exploration. To showcase the
power and generality of this technique, we apply it to several episodic MDP
settings with adversarial losses and bandit feedback, improving and
generalizing the state-of-the-art. Specifically, in the tabular case, we obtain
$\widetilde{\mathcal{O}}(\sqrt{T})$ regret where $T$ is the number of episodes,
improving the $\widetilde{\mathcal{O}}({T}^{2/3})$ regret bound by Shani et al.
(2020). When the number of states is infinite, under the assumption that the
state-action values are linear in some low-dimensional features, we obtain
$\widetilde{\mathcal{O}}({T}^{2/3})$ regret with the help of a simulator,
matching the result of Neu and Olkhovskaya (2020) while importantly removing
the need of an exploratory policy that their algorithm requires. When a
simulator is unavailable, we further consider a linear MDP setting and obtain
$\widetilde{\mathcal{O}}({T}^{14/15})$ regret, which is the first result for
linear MDPs with adversarial losses and bandit feedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Chen-Yu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chung-Wei Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-free prediction of emergence of extreme events in a parametrically driven nonlinear dynamical system by Deep Learning. (arXiv:2107.08819v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08819</id>
        <link href="http://arxiv.org/abs/2107.08819"/>
        <updated>2021-07-20T02:04:46.804Z</updated>
        <summary type="html"><![CDATA[We predict the emergence of extreme events in a parametrically driven
nonlinear dynamical system using three Deep Learning models, namely Multi-Layer
Perceptron, Convolutional Neural Network and Long Short-Term Memory. The Deep
Learning models are trained using the training set and are allowed to predict
the test set data. After prediction, the time series of the actual and the
predicted values are plotted one over the other in order to visualize the
performance of the models. Upon evaluating the Root Mean Square Error value
between predicted and the actual values of all three models, we find that the
Long Short-Term Memory model can serve as the best model to forecast the
chaotic time series and to predict the emergence of extreme events for the
considered system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meiyazhagan_J/0/1/0/all/0/1"&gt;J.Meiyazhagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sudharsan_S/0/1/0/all/0/1"&gt;S. Sudharsan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Senthilvelan_M/0/1/0/all/0/1"&gt;M. Senthilvelan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Search for Feedback in Reinforcement Learning. (arXiv:2002.09478v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.09478</id>
        <link href="http://arxiv.org/abs/2002.09478"/>
        <updated>2021-07-20T02:04:46.803Z</updated>
        <summary type="html"><![CDATA[The problem of Reinforcement Learning (RL) in an unknown nonlinear dynamical
system is equivalent to the search for an optimal feedback law utilizing the
simulations/ rollouts of the unknown dynamical system. Most RL techniques
search over a complex global nonlinear feedback parametrization making them
suffer from high training times as well as variance. Instead, we advocate
searching over a local feedback representation consisting of an open-loop
sequence, and an associated optimal linear feedback law completely determined
by the open-loop. We show that this alternate approach results in highly
efficient training, the answers obtained are repeatable and hence reliable, and
the resulting closed performance is superior to global state-of-the-art RL
techniques. Finally, if we replan, whenever required, which is feasible due to
the fast and reliable local solution, allows us to recover global optimality of
the resulting feedback law.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parunandi_K/0/1/0/all/0/1"&gt;Karthikeya S. Parunandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Aayushman Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1"&gt;Raman Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravorty_S/0/1/0/all/0/1"&gt;Suman Chakravorty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Event Segmentation and Localization for Wildlife Extended Videos. (arXiv:2005.02463v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.02463</id>
        <link href="http://arxiv.org/abs/2005.02463"/>
        <updated>2021-07-20T02:04:46.803Z</updated>
        <summary type="html"><![CDATA[Using offline training schemes, researchers have tackled the event
segmentation problem by providing full or weak-supervision through manually
annotated labels or self-supervised epoch-based training. Most works consider
videos that are at most 10's of minutes long. We present a self-supervised
perceptual prediction framework capable of temporal event segmentation by
building stable representations of objects over time and demonstrate it on long
videos, spanning several days. The approach is deceptively simple but quite
effective. We rely on predictions of high-level features computed by a standard
deep learning backbone. For prediction, we use an LSTM, augmented with an
attention mechanism, trained in a self-supervised manner using the prediction
error. The self-learned attention maps effectively localize and track the
event-related objects in each frame. The proposed approach does not require
labels. It requires only a single pass through the video, with no separate
training set. Given the lack of datasets of very long videos, we demonstrate
our method on video from 10 days (254 hours) of continuous wildlife monitoring
data that we had collected with required permissions. We find that the approach
is robust to various environmental conditions such as day/night conditions,
rain, sharp shadows, and windy conditions. For the task of temporally locating
events, we had an 80% recall rate at 20% false-positive rate for frame-level
segmentation. At the activity level, we had an 80% activity recall rate for one
false activity detection every 50 minutes. We will make the dataset, which is
the first of its kind, and the code available to the research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mounir_R/0/1/0/all/0/1"&gt;Ramy Mounir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gula_R/0/1/0/all/0/1"&gt;Roman Gula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theuerkauf_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rn Theuerkauf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1"&gt;Sudeep Sarkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Projection Robust Wasserstein Distance and Riemannian Optimization. (arXiv:2006.07458v8 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07458</id>
        <link href="http://arxiv.org/abs/2006.07458"/>
        <updated>2021-07-20T02:04:46.802Z</updated>
        <summary type="html"><![CDATA[Projection robust Wasserstein (PRW) distance, or Wasserstein projection
pursuit (WPP), is a robust variant of the Wasserstein distance. Recent work
suggests that this quantity is more robust than the standard Wasserstein
distance, in particular when comparing probability measures in high-dimensions.
However, it is ruled out for practical application because the optimization
model is essentially non-convex and non-smooth which makes the computation
intractable. Our contribution in this paper is to revisit the original
motivation behind WPP/PRW, but take the hard route of showing that, despite its
non-convexity and lack of nonsmoothness, and even despite some hardness results
proved by~\citet{Niles-2019-Estimation} in a minimax sense, the original
formulation for PRW/WPP \textit{can} be efficiently computed in practice using
Riemannian optimization, yielding in relevant cases better behavior than its
convex relaxation. More specifically, we provide three simple algorithms with
solid theoretical guarantee on their complexity bound (one in the appendix),
and demonstrate their effectiveness and efficiency by conducing extensive
experiments on synthetic and real data. This paper provides a first step into a
computational theory of the PRW distance and provides the links between optimal
transport and Riemannian optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Chenyou Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1"&gt;Nhat Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1"&gt;Marco Cuturi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty quantification for Markov Random Fields. (arXiv:2009.00038v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00038</id>
        <link href="http://arxiv.org/abs/2009.00038"/>
        <updated>2021-07-20T02:04:46.801Z</updated>
        <summary type="html"><![CDATA[We present an information-based uncertainty quantification method for general
Markov Random Fields. Markov Random Fields (MRF) are structured, probabilistic
graphical models over undirected graphs, and provide a fundamental unifying
modeling tool for statistical mechanics, probabilistic machine learning, and
artificial intelligence. Typically MRFs are complex and high-dimensional with
nodes and edges (connections) built in a modular fashion from simpler,
low-dimensional probabilistic models and their local connections; in turn, this
modularity allows to incorporate available data to MRFs and efficiently
simulate them by leveraging their graph-theoretic structure. Learning graphical
models from data and/or constructing them from physical modeling and
constraints necessarily involves uncertainties inherited from data, modeling
choices, or numerical approximations. These uncertainties in the MRF can be
manifested either in the graph structure or the probability distribution
functions, and necessarily will propagate in predictions for quantities of
interest. Here we quantify such uncertainties using tight, information based
bounds on the predictions of quantities of interest; these bounds take
advantage of the graphical structure of MRFs and are capable of handling the
inherent high-dimensionality of such graphical models. We demonstrate our
methods in MRFs for medical diagnostics and statistical mechanics models. In
the latter, we develop uncertainty quantification bounds for finite size
effects and phase diagrams, which constitute two of the typical predictions
goals of statistical mechanics modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Birmpa_P/0/1/0/all/0/1"&gt;Panagiota Birmpa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Katsoulakis_M/0/1/0/all/0/1"&gt;Markos A. Katsoulakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Brain Signals to Rescue Aphasia, Apraxia and Dysarthria Speech Recognition. (arXiv:2103.00383v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00383</id>
        <link href="http://arxiv.org/abs/2103.00383"/>
        <updated>2021-07-20T02:04:46.801Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a deep learning-based algorithm to improve the
performance of automatic speech recognition (ASR) systems for aphasia, apraxia,
and dysarthria speech by utilizing electroencephalography (EEG) features
recorded synchronously with aphasia, apraxia, and dysarthria speech. We
demonstrate a significant decoding performance improvement by more than 50\%
during test time for isolated speech recognition task and we also provide
preliminary results indicating performance improvement for the more challenging
continuous speech recognition task by utilizing EEG features. The results
presented in this paper show the first step towards demonstrating the
possibility of utilizing non-invasive neural signals to design a real-time
robust speech prosthetic for stroke survivors recovering from aphasia, apraxia,
and dysarthria. Our aphasia, apraxia, and dysarthria speech-EEG data set will
be released to the public to help further advance this interesting and crucial
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_G/0/1/0/all/0/1"&gt;Gautam Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carnahan_M/0/1/0/all/0/1"&gt;Mason Carnahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamapant_S/0/1/0/all/0/1"&gt;Shilpa Shamapant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Surendranath_Y/0/1/0/all/0/1"&gt;Yashitha Surendranath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saumya Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Arundhati Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1"&gt;Co Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Millan_J/0/1/0/all/0/1"&gt;Jose del R Millan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1"&gt;Ahmed H Tewfik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08251</id>
        <link href="http://arxiv.org/abs/2107.08251"/>
        <updated>2021-07-20T02:04:46.799Z</updated>
        <summary type="html"><![CDATA[We introduce ParaBLEU, a paraphrase representation learning model and
evaluation metric for text generation. Unlike previous approaches, ParaBLEU
learns to understand paraphrasis using generative conditioning as a pretraining
objective. ParaBLEU correlates more strongly with human judgements than
existing metrics, obtaining new state-of-the-art results on the 2017 WMT
Metrics Shared Task. We show that our model is robust to data scarcity,
exceeding previous state-of-the-art performance using only $50\%$ of the
available training data and surpassing BLEU, ROUGE and METEOR with only $40$
labelled examples. Finally, we demonstrate that ParaBLEU can be used to
conditionally generate novel paraphrases from a single demonstration, which we
use to confirm our hypothesis that it learns abstract, generalized paraphrase
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jack Weston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1"&gt;Raphael Lenain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1"&gt;Udeepa Meepegama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1"&gt;Emil Fristed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Verification of Neural Networks Against Group Fairness. (arXiv:2107.08362v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08362</id>
        <link href="http://arxiv.org/abs/2107.08362"/>
        <updated>2021-07-20T02:04:46.799Z</updated>
        <summary type="html"><![CDATA[Fairness is crucial for neural networks which are used in applications with
important societal implication. Recently, there have been multiple attempts on
improving fairness of neural networks, with a focus on fairness testing (e.g.,
generating individual discriminatory instances) and fairness training (e.g.,
enhancing fairness through augmented training). In this work, we propose an
approach to formally verify neural networks against fairness, with a focus on
independence-based fairness such as group fairness. Our method is built upon an
approach for learning Markov Chains from a user-provided neural network (i.e.,
a feed-forward neural network or a recurrent neural network) which is
guaranteed to facilitate sound analysis. The learned Markov Chain not only
allows us to verify (with Probably Approximate Correctness guarantee) whether
the neural network is fair or not, but also facilities sensitivity analysis
which helps to understand why fairness is violated. We demonstrate that with
our analysis results, the neural weights can be optimized to improve fairness.
Our approach has been evaluated with multiple models trained on benchmark
datasets and the experiment results show that our approach is effective and
efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Bing Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1"&gt;Ting Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?. (arXiv:2102.11068v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11068</id>
        <link href="http://arxiv.org/abs/2102.11068"/>
        <updated>2021-07-20T02:04:46.798Z</updated>
        <summary type="html"><![CDATA[In deep model compression, the recent finding "Lottery Ticket Hypothesis"
(LTH) (Frankle & Carbin, 2018) pointed out that there could exist a winning
ticket (i.e., a properly pruned sub-network together with original weight
initialization) that can achieve competitive performance than the original
dense network. However, it is not easy to observe such winning property in many
scenarios, where for example, a relatively large learning rate is used even if
it benefits training the original dense model. In this work, we investigate the
underlying condition and rationale behind the winning property, and find that
the underlying reason is largely attributed to the correlation between
initialized weights and final-trained weights when the learning rate is not
sufficiently large. Thus, the existence of winning property is correlated with
an insufficient DNN pretraining, and is unlikely to occur for a well-trained
DNN. To overcome this limitation, we propose the "pruning & fine-tuning" method
that consistently outperforms lottery ticket sparse training under the same
pruning algorithm and the same total training epochs. Extensive experiments
over multiple deep models (VGG, ResNet, MobileNet-v2) on different datasets
have been conducted to justify our proposals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Ning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1"&gt;Zhengping Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xuan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jian Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic and explainable grading of meningiomas from histopathology images. (arXiv:2107.08850v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08850</id>
        <link href="http://arxiv.org/abs/2107.08850"/>
        <updated>2021-07-20T02:04:46.797Z</updated>
        <summary type="html"><![CDATA[Meningioma is one of the most prevalent brain tumors in adults. To determine
its malignancy, it is graded by a pathologist into three grades according to
WHO standards. This grade plays a decisive role in treatment, and yet may be
subject to inter-rater discordance. In this work, we present and compare three
approaches towards fully automatic meningioma grading from histology whole
slide images. All approaches are following a two-stage paradigm, where we first
identify a region of interest based on the detection of mitotic figures in the
slide using a state-of-the-art object detection deep learning network. This
region of highest mitotic rate is considered characteristic for biological
tumor behavior. In the second stage, we calculate a score corresponding to
tumor malignancy based on information contained in this region using three
different settings. In a first approach, image patches are sampled from this
region and regression is based on morphological features encoded by a
ResNet-based network. We compare this to learning a logistic regression from
the determined mitotic count, an approach which is easily traceable and
explainable. Lastly, we combine both approaches in a single network. We trained
the pipeline on 951 slides from 341 patients and evaluated them on a separate
set of 141 slides from 43 patients. All approaches yield a high correlation to
the WHO grade. The logistic regression and the combined approach had the best
results in our experiments, yielding correct predictions in 32 and 33 of all
cases, respectively, with the image-based approach only predicting 25 cases
correctly. Spearman's correlation was 0.716, 0.792 and 0.790 respectively. It
may seem counterintuitive at first that morphological features provided by
image patches do not improve model performance. Yet, this mirrors the criteria
of the grading scheme, where mitotic count is the only unequivocal parameter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ganz_J/0/1/0/all/0/1"&gt;Jonathan Ganz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kirsch_T/0/1/0/all/0/1"&gt;Tobias Kirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hoffmann_L/0/1/0/all/0/1"&gt;Lucas Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bertram_C/0/1/0/all/0/1"&gt;Christof A. Bertram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hoffmann_C/0/1/0/all/0/1"&gt;Christoph Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1"&gt;Katharina Breininger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blumcke_I/0/1/0/all/0/1"&gt;Ingmar Bl&amp;#xfc;mcke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jabari_S/0/1/0/all/0/1"&gt;Samir Jabari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1"&gt;Marc Aubreville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning De-identified Representations of Prosody from Raw Audio. (arXiv:2107.08248v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08248</id>
        <link href="http://arxiv.org/abs/2107.08248"/>
        <updated>2021-07-20T02:04:46.796Z</updated>
        <summary type="html"><![CDATA[We propose a method for learning de-identified prosody representations from
raw audio using a contrastive self-supervised signal. Whereas prior work has
relied on conditioning models on bottlenecks, we introduce a set of inductive
biases that exploit the natural structure of prosody to minimize timbral
information and decouple prosody from speaker representations. Despite
aggressive downsampling of the input and having no access to linguistic
information, our model performs comparably to state-of-the-art speech
representations on DAMMP, a new benchmark we introduce for spoken language
understanding. We use minimum description length probing to show that our
representations have selectively learned the subcomponents of non-timbral
prosody, and that the product quantizer naturally disentangles them without
using bottlenecks. We derive an information-theoretic definition of speech
de-identifiability and use it to demonstrate that our prosody representations
are less identifiable than other speech representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jack Weston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1"&gt;Raphael Lenain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1"&gt;Udeepa Meepegama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1"&gt;Emil Fristed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedification Operators for Policy Optimization: Investigating Forward and Reverse KL Divergences. (arXiv:2107.08285v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08285</id>
        <link href="http://arxiv.org/abs/2107.08285"/>
        <updated>2021-07-20T02:04:46.795Z</updated>
        <summary type="html"><![CDATA[Approximate Policy Iteration (API) algorithms alternate between (approximate)
policy evaluation and (approximate) greedification. Many different approaches
have been explored for approximate policy evaluation, but less is understood
about approximate greedification and what choices guarantee policy improvement.
In this work, we investigate approximate greedification when reducing the KL
divergence between the parameterized policy and the Boltzmann distribution over
action values. In particular, we investigate the difference between the forward
and reverse KL divergences, with varying degrees of entropy regularization. We
show that the reverse KL has stronger policy improvement guarantees, but that
reducing the forward KL can result in a worse policy. We also demonstrate,
however, that a large enough reduction of the forward KL can induce improvement
under additional assumptions. Empirically, we show on simple continuous-action
environments that the forward KL can induce more exploration, but at the cost
of a more suboptimal policy. No significant differences were observed in the
discrete-action setting or on a suite of benchmark problems. Throughout, we
highlight that many policy gradient methods can be seen as an instance of API,
with either the forward or reverse KL for the policy update, and discuss next
steps for understanding and improving our policy optimization algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Alan Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1"&gt;Hugo Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1"&gt;Sungsu Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kozuno_T/0/1/0/all/0/1"&gt;Tadashi Kozuno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1"&gt;A. Rupam Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1"&gt;Martha White&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Automated Machine Learning Pipeline for Echocardiogram Segmentation. (arXiv:2107.08440v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08440</id>
        <link href="http://arxiv.org/abs/2107.08440"/>
        <updated>2021-07-20T02:04:46.794Z</updated>
        <summary type="html"><![CDATA[Nowadays, cardiac diagnosis largely depends on left ventricular function
assessment. With the help of the segmentation deep learning model, the
assessment of the left ventricle becomes more accessible and accurate. However,
deep learning technique still faces two main obstacles: the difficulty in
acquiring sufficient training data and time-consuming in developing quality
models. In the ordinary data acquisition process, the dataset was selected
randomly from a large pool of unlabeled images for labeling, leading to massive
labor time to annotate those images. Besides that, hand-designed model
development is laborious and also costly. This paper introduces a pipeline that
relies on Active Learning to ease the labeling work and utilizes Neural
Architecture Search's idea to design the adequate deep learning model
automatically. We called this Fully automated machine learning pipeline for
echocardiogram segmentation. The experiment results show that our method
obtained the same IOU accuracy with only two-fifths of the original training
dataset, and the searched model got the same accuracy as the hand-designed
model given the same training dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thuy_H/0/1/0/all/0/1"&gt;Hang Duong Thi Thuy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minh_T/0/1/0/all/0/1"&gt;Tuan Nguyen Minh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Van_P/0/1/0/all/0/1"&gt;Phi Nguyen Van&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quoc_L/0/1/0/all/0/1"&gt;Long Tran Quoc&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting the Convergence of Reinforcement Learning-based Auto-pruning Using Historical Data. (arXiv:2107.08815v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08815</id>
        <link href="http://arxiv.org/abs/2107.08815"/>
        <updated>2021-07-20T02:04:46.794Z</updated>
        <summary type="html"><![CDATA[Recently, neural network compression schemes like channel pruning have been
widely used to reduce the model size and computational complexity of deep
neural network (DNN) for applications in power-constrained scenarios such as
embedded systems. Reinforcement learning (RL)-based auto-pruning has been
further proposed to automate the DNN pruning process to avoid expensive
hand-crafted work. However, the RL-based pruner involves a time-consuming
training process and the high expense of each sample further exacerbates this
problem. These impediments have greatly restricted the real-world application
of RL-based auto-pruning. Thus, in this paper, we propose an efficient
auto-pruning framework which solves this problem by taking advantage of the
historical data from the previous auto-pruning process. In our framework, we
first boost the convergence of the RL-pruner by transfer learning. Then, an
augmented transfer learning scheme is proposed to further speed up the training
process by improving the transferability. Finally, an assistant learning
process is proposed to improve the sample efficiency of the RL agent. The
experiments have shown that our framework can accelerate the auto-pruning
process by 1.5-2.5 times for ResNet20, and 1.81-2.375 times for other neural
networks like ResNet56, ResNet18, and MobileNet v1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1"&gt;Jiandong Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Feiwen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Augmented Online Facility Location. (arXiv:2107.08277v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.08277</id>
        <link href="http://arxiv.org/abs/2107.08277"/>
        <updated>2021-07-20T02:04:46.793Z</updated>
        <summary type="html"><![CDATA[Following the research agenda initiated by Munoz & Vassilvitskii [1] and
Lykouris & Vassilvitskii [2] on learning-augmented online algorithms for
classical online optimization problems, in this work, we consider the Online
Facility Location problem under this framework. In Online Facility Location
(OFL), demands arrive one-by-one in a metric space and must be (irrevocably)
assigned to an open facility upon arrival, without any knowledge about future
demands.

We present an online algorithm for OFL that exploits potentially imperfect
predictions on the locations of the optimal facilities. We prove that the
competitive ratio decreases smoothly from sublogarithmic in the number of
demands to constant, as the error, i.e., the total distance of the predicted
locations to the optimal facility locations, decreases towards zero. We
complement our analysis with a matching lower bound establishing that the
dependence of the algorithm's competitive ratio on the error is optimal, up to
constant factors. Finally, we evaluate our algorithm on real world data and
compare our learning augmented approach with the current best online algorithm
for the problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fotakis_D/0/1/0/all/0/1"&gt;Dimitris Fotakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gergatsouli_E/0/1/0/all/0/1"&gt;Evangelia Gergatsouli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gouleakis_T/0/1/0/all/0/1"&gt;Themis Gouleakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patris_N/0/1/0/all/0/1"&gt;Nikolas Patris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Data-driven Software Vulnerability Assessment and Prioritization. (arXiv:2107.08364v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2107.08364</id>
        <link href="http://arxiv.org/abs/2107.08364"/>
        <updated>2021-07-20T02:04:46.792Z</updated>
        <summary type="html"><![CDATA[Software Vulnerabilities (SVs) are increasing in complexity and scale, posing
great security risks to many software systems. Given the limited resources in
practice, SV assessment and prioritization help practitioners devise optimal SV
mitigation plans based on various SV characteristics. The surge in SV data
sources and data-driven techniques such as Machine Learning and Deep Learning
have taken SV assessment and prioritization to the next level. Our survey
provides a taxonomy of the past research efforts and highlights the best
practices for data-driven SV assessment and prioritization. We also discuss the
current limitations and propose potential solutions to address such issues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Triet H. M. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huaming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1"&gt;M. Ali Babar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards the Unification and Robustness of Perturbation and Gradient Based Explanations. (arXiv:2102.10618v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10618</id>
        <link href="http://arxiv.org/abs/2102.10618"/>
        <updated>2021-07-20T02:04:46.792Z</updated>
        <summary type="html"><![CDATA[As machine learning black boxes are increasingly being deployed in critical
domains such as healthcare and criminal justice, there has been a growing
emphasis on developing techniques for explaining these black boxes in a post
hoc manner. In this work, we analyze two popular post hoc interpretation
techniques: SmoothGrad which is a gradient based method, and a variant of LIME
which is a perturbation based method. More specifically, we derive explicit
closed form expressions for the explanations output by these two methods and
show that they both converge to the same explanation in expectation, i.e., when
the number of perturbed samples used by these methods is large. We then
leverage this connection to establish other desirable properties, such as
robustness, for these techniques. We also derive finite sample complexity
bounds for the number of perturbations required for these methods to converge
to their expected explanation. Finally, we empirically validate our theory
using extensive experimentation on both synthetic and real world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sushant Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jabbari_S/0/1/0/all/0/1"&gt;Shahin Jabbari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1"&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Sohini Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiwei Steven Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Deep Reinforcement Learning in IRS-Aided Wireless Communications Systems. (arXiv:2107.08293v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08293</id>
        <link href="http://arxiv.org/abs/2107.08293"/>
        <updated>2021-07-20T02:04:46.791Z</updated>
        <summary type="html"><![CDATA[We consider an Intelligent Reflecting Surface (IRS)-aided multiple-input
single-output (MISO) system for downlink transmission. We compare the
performance of Deep Reinforcement Learning (DRL) and conventional optimization
methods in finding optimal phase shifts of the IRS elements to maximize the
user signal-to-noise (SNR) ratio. Furthermore, we evaluate the robustness of
these methods to channel impairments and changes in the system. We demonstrate
numerically that DRL solutions show more robustness to noisy channels and user
mobility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1"&gt;Amal Feriani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mezghani_A/0/1/0/all/0/1"&gt;Amine Mezghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1"&gt;Ekram Hossain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing Online Engagement with Disinformation and Conspiracies in the 2020 U.S. Presidential Election. (arXiv:2107.08319v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2107.08319</id>
        <link href="http://arxiv.org/abs/2107.08319"/>
        <updated>2021-07-20T02:04:46.791Z</updated>
        <summary type="html"><![CDATA[Identifying and characterizing disinformation in political discourse on
social media is critical to ensure the integrity of elections and democratic
processes around the world. Persistent manipulation of social media has
resulted in increased concerns regarding the 2020 U.S. Presidential Election,
due to its potential to influence individual opinions and social dynamics. In
this work, we focus on the identification of distorted facts, in the form of
unreliable and conspiratorial narratives in election-related tweets, to
characterize discourse manipulation prior to the election. We apply a detection
model to separate factual from unreliable (or conspiratorial) claims analyzing
a dataset of 242 million election-related tweets. The identified claims are
used to investigate targeted topics of disinformation, and conspiracy groups,
most notably the far-right QAnon conspiracy group. Further, we characterize
account engagements with unreliable and conspiracy tweets, and with the QAnon
conspiracy group, by political leaning and tweet types. Finally, using a
regression discontinuity design, we investigate whether Twitter's actions to
curb QAnon activity on the platform were effective, and how QAnon accounts
adapt to Twitter's restrictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1"&gt;Karishma Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1"&gt;Emilio Ferrara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reservoir Memory Machines as Neural Computers. (arXiv:2009.06342v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06342</id>
        <link href="http://arxiv.org/abs/2009.06342"/>
        <updated>2021-07-20T02:04:46.774Z</updated>
        <summary type="html"><![CDATA[Differentiable neural computers extend artificial neural networks with an
explicit memory without interference, thus enabling the model to perform
classic computation tasks such as graph traversal. However, such models are
difficult to train, requiring long training times and large datasets. In this
work, we achieve some of the computational capabilities of differentiable
neural computers with a model that can be trained very efficiently, namely an
echo state network with an explicit memory without interference. This extension
enables echo state networks to recognize all regular languages, including those
that contractive echo state networks provably can not recognize. Further, we
demonstrate experimentally that our model performs comparably to its
fully-trained deep version on several typical benchmark tasks for
differentiable neural computers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paassen_B/0/1/0/all/0/1"&gt;Benjamin Paa&amp;#xdf;en&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1"&gt;Alexander Schulz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stewart_T/0/1/0/all/0/1"&gt;Terrence C. Stewart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1"&gt;Barbara Hammer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Review of Learning-based Longitudinal Motion Planning for Autonomous Vehicles: Research Gaps between Self-driving and Traffic Congestion. (arXiv:1910.06070v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.06070</id>
        <link href="http://arxiv.org/abs/1910.06070"/>
        <updated>2021-07-20T02:04:46.773Z</updated>
        <summary type="html"><![CDATA[Self-driving technology companies and the research community are accelerating
their pace to use machine learning longitudinal motion planning (mMP) for
autonomous vehicles (AVs). This paper reviews the current state of the art in
mMP, with an exclusive focus on its impact on traffic congestion. We identify
the availability of congestion scenarios in current datasets, and summarize the
required features for training mMP. For learning methods, we survey the major
methods in both imitation learning and non-imitation learning. We also
highlight the emerging technologies adopted by some leading AV companies, e.g.
Tesla, Waymo, and Comma.ai. We find that: i) the AV industry has been mostly
focusing on the long tail problem related to safety and overlooked the impact
on traffic congestion, ii) the current public self-driving datasets have not
included enough congestion scenarios, and mostly lack the necessary input
features/output labels to train mMP, and iii) albeit reinforcement learning
(RL) approach can integrate congestion mitigation into the learning goal, the
major mMP method adopted by industry is still behavior cloning (BC), whose
capability to learn a congestion-mitigating mMP remains to be seen. Based on
the review, the study identifies the research gaps in current mMP development.
Some suggestions towards congestion mitigation for future mMP studies are
proposed: i) enrich data collection to facilitate the congestion learning, ii)
incorporate non-imitation learning methods to combine traffic efficiency into a
safety-oriented technical route, and iii) integrate domain knowledge from the
traditional car following (CF) theory to improve the string stability of mMP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Laval_J/0/1/0/all/0/1"&gt;Jorge Laval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Anye Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenchao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qing_Z/0/1/0/all/0/1"&gt;Zhu Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peeta_S/0/1/0/all/0/1"&gt;Srinivas Peeta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensitivity -- Local Index to Control Chaoticity or Gradient Globally. (arXiv:2012.13134v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13134</id>
        <link href="http://arxiv.org/abs/2012.13134"/>
        <updated>2021-07-20T02:04:46.773Z</updated>
        <summary type="html"><![CDATA[Here, we introduce a fully local index named "sensitivity" for each neuron to
control chaoticity or gradient globally in a neural network (NN). We also
propose a learning method to adjust it named "sensitivity adjustment learning
(SAL)". The index is the gradient magnitude of its output with respect to its
inputs. By adjusting its time average to 1.0 in each neuron, information
transmission in the neuron changes to be moderate without shrinking or
expanding for both forward and backward computations. That results in moderate
information transmission through a layer of neurons when the weights and inputs
are random. Therefore, SAL can control the chaoticity of the network dynamics
in a recurrent NN (RNN). It can also solve the vanishing gradient problem in
error backpropagation (BP) learning in a deep feedforward NN or an RNN. We
demonstrate that when applying SAL to an RNN with small and random initial
weights, log-sensitivity, which is the logarithm of RMS (root mean square)
sensitivity over all the neurons, is equivalent to the maximum Lyapunov
exponent until it reaches 0.0. We also show that SAL works with BP or BPTT (BP
through time) to avoid the vanishing gradient problem in a 300-layer NN or an
RNN that learns a problem with a lag of 300 steps between the first input and
the output. Compared with manually fine-tuning the spectral radius of the
weight matrix before learning, SAL's continuous nonlinear learning nature
prevents loss of sensitivities during learning, resulting in a significant
improvement in learning performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shibata_K/0/1/0/all/0/1"&gt;Katsunari Shibata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ejima_T/0/1/0/all/0/1"&gt;Takuya Ejima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tokumaru_Y/0/1/0/all/0/1"&gt;Yuki Tokumaru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsuki_T/0/1/0/all/0/1"&gt;Toshitaka Matsuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Experimental Study of Data Heterogeneity in Federated Learning Methods for Medical Imaging. (arXiv:2107.08371v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08371</id>
        <link href="http://arxiv.org/abs/2107.08371"/>
        <updated>2021-07-20T02:04:46.772Z</updated>
        <summary type="html"><![CDATA[Federated learning enables multiple institutions to collaboratively train
machine learning models on their local data in a privacy-preserving way.
However, its distributed nature often leads to significant heterogeneity in
data distributions across institutions. In this paper, we investigate the
deleterious impact of a taxonomy of data heterogeneity regimes on federated
learning methods, including quantity skew, label distribution skew, and imaging
acquisition skew. We show that the performance degrades with the increasing
degrees of data heterogeneity. We present several mitigation strategies to
overcome performance drops from data heterogeneity, including weighted average
for data quantity skew, weighted loss and batch normalization averaging for
label distribution skew. The proposed optimizations to federated learning
methods improve their capability of handling heterogeneity across institutions,
which provides valuable guidance for the deployment of federated learning in
real clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Liangqiong Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balachandar_N/0/1/0/all/0/1"&gt;Niranjan Balachandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ab Initio Particle-based Object Manipulation. (arXiv:2107.08865v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.08865</id>
        <link href="http://arxiv.org/abs/2107.08865"/>
        <updated>2021-07-20T02:04:46.772Z</updated>
        <summary type="html"><![CDATA[This paper presents Particle-based Object Manipulation (Prompt), a new
approach to robot manipulation of novel objects ab initio, without prior object
models or pre-training on a large object data set. The key element of Prompt is
a particle-based object representation, in which each particle represents a
point in the object, the local geometric, physical, and other features of the
point, and also its relation with other particles. Like the model-based
analytic approaches to manipulation, the particle representation enables the
robot to reason about the object's geometry and dynamics in order to choose
suitable manipulation actions. Like the data-driven approaches, the particle
representation is learned online in real-time from visual sensor input,
specifically, multi-view RGB images. The particle representation thus connects
visual perception with robot control. Prompt combines the benefits of both
model-based reasoning and data-driven learning. We show empirically that Prompt
successfully handles a variety of everyday objects, some of which are
transparent. It handles various manipulation tasks, including grasping,
pushing, etc,. Our experiments also show that Prompt outperforms a
state-of-the-art data-driven grasping method on the daily objects, even though
it does not use any offline training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yunfan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1"&gt;David Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking. (arXiv:2105.11237v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11237</id>
        <link href="http://arxiv.org/abs/2105.11237"/>
        <updated>2021-07-20T02:04:46.771Z</updated>
        <summary type="html"><![CDATA[Recently, most siamese network based trackers locate targets via object
classification and bounding-box regression. Generally, they select the
bounding-box with maximum classification confidence as the final prediction.
This strategy may miss the right result due to the accuracy misalignment
between classification and regression. In this paper, we propose a novel
siamese tracking algorithm called SiamRCR, addressing this problem with a
simple, light and effective solution. It builds reciprocal links between
classification and regression branches, which can dynamically re-weight their
losses for each positive sample. In addition, we add a localization branch to
predict the localization accuracy, so that it can work as the replacement of
the regression assistance link during inference. This branch makes the training
and inference more consistent. Extensive experimental results demonstrate the
effectiveness of SiamRCR and its superiority over the state-of-the-art
competitors on GOT-10k, LaSOT, TrackingNet, OTB-2015, VOT-2018 and VOT-2019.
Moreover, our SiamRCR runs at 65 FPS, far above the real-time requirement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jinlong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhengkai Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yueyang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiyao Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WildGait: Learning Gait Representations from Raw Surveillance Streams. (arXiv:2105.05528v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05528</id>
        <link href="http://arxiv.org/abs/2105.05528"/>
        <updated>2021-07-20T02:04:46.705Z</updated>
        <summary type="html"><![CDATA[The use of gait for person identification has important advantages such as
being non-invasive, unobtrusive, not requiring cooperation and being less
likely to be obscured compared to other biometrics. Existing methods for gait
recognition require cooperative gait scenarios, in which a single person is
walking multiple times in a straight line in front of a camera. We aim to
address the hard challenges of real-world scenarios in which camera feeds
capture multiple people, who in most cases pass in front of the camera only
once. We address privacy concerns by using only the motion information of
walking individuals, with no identifiable appearance-based information. As
such, we propose a novel weakly supervised learning framework, WildGait, which
consists of training a Spatio-Temporal Graph Convolutional Network on a large
number of automatically annotated skeleton sequences obtained from raw,
real-world, surveillance streams to learn useful gait signatures. Our results
show that, with fine-tuning, we surpass in terms of recognition accuracy the
current state-of-the-art pose-based gait recognition solutions. Our proposed
method is reliable in training gait recognition methods in unconstrained
environments, especially in settings with scarce amounts of annotated data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1"&gt;Adrian Cosma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1"&gt;Emilian Radoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Limit Data Collection via Scaling Laws: Data Minimization Compliance in Practice. (arXiv:2107.08096v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08096</id>
        <link href="http://arxiv.org/abs/2107.08096"/>
        <updated>2021-07-20T02:04:46.699Z</updated>
        <summary type="html"><![CDATA[Data minimization is a legal obligation defined in the European Union's
General Data Protection Regulation (GDPR) as the responsibility to process an
adequate, relevant, and limited amount of personal data in relation to a
processing purpose. However, unlike fairness or transparency, the principle has
not seen wide adoption for machine learning systems due to a lack of
computational interpretation. In this paper, we build on literature in machine
learning and law to propose the first learning framework for limiting data
collection based on an interpretation that ties the data collection purpose to
system performance. We formalize a data minimization criterion based on
performance curve derivatives and provide an effective and interpretable
piecewise power law technique that models distinct stages of an algorithm's
performance throughout data collection. Results from our empirical
investigation offer deeper insights into the relevant considerations when
designing a data minimization framework, including the choice of feature
acquisition algorithm, initialization conditions, as well as impacts on
individuals that hint at tensions between data minimization and fairness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shanmugam_D/0/1/0/all/0/1"&gt;Divya Shanmugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shabanian_S/0/1/0/all/0/1"&gt;Samira Shabanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1"&gt;Fernando Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finck_M/0/1/0/all/0/1"&gt;Mich&amp;#xe8;le Finck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1"&gt;Asia Biega&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Out-of-distribution Detection with Energy-based Models. (arXiv:2107.08785v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08785</id>
        <link href="http://arxiv.org/abs/2107.08785"/>
        <updated>2021-07-20T02:04:46.698Z</updated>
        <summary type="html"><![CDATA[Several density estimation methods have shown to fail to detect
out-of-distribution (OOD) samples by assigning higher likelihoods to anomalous
data. Energy-based models (EBMs) are flexible, unnormalized density models
which seem to be able to improve upon this failure mode. In this work, we
provide an extensive study investigating OOD detection with EBMs trained with
different approaches on tabular and image data and find that EBMs do not
provide consistent advantages. We hypothesize that EBMs do not learn semantic
features despite their discriminative structure similar to Normalizing Flows.
To verify this hypotheses, we show that supervision and architectural
restrictions improve the OOD detection of EBMs independent of the training
approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elflein_S/0/1/0/all/0/1"&gt;Sven Elflein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charpentier_B/0/1/0/all/0/1"&gt;Bertrand Charpentier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zugner_D/0/1/0/all/0/1"&gt;Daniel Z&amp;#xfc;gner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-Optimal Algorithms for Linear Algebra in the Current Matrix Multiplication Time. (arXiv:2107.08090v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.08090</id>
        <link href="http://arxiv.org/abs/2107.08090"/>
        <updated>2021-07-20T02:04:46.697Z</updated>
        <summary type="html"><![CDATA[Currently, in the numerical linear algebra community, it is thought that to
obtain nearly-optimal bounds for various problems such as rank computation,
finding a maximal linearly independent subset of columns, regression, low rank
approximation, maximum matching on general graphs and linear matroid union, one
would need to resolve the main open question of Nelson and Nguyen (FOCS, 2013)
regarding the logarithmic factors in the sketching dimension for existing
constant factor approximation oblivious subspace embeddings. We show how to
bypass this question using a refined sketching technique, and obtain optimal or
nearly optimal bounds for these problems. A key technique we use is an explicit
mapping of Indyk based on uncertainty principles and extractors, which after
first applying known oblivious subspace embeddings, allows us to quickly spread
out the mass of the vector so that sampling is now effective, and we avoid a
logarithmic factor that is standard in the sketching dimension resulting from
matrix Chernoff bounds. For the fundamental problems of rank computation and
finding a linearly independent subset of columns, our algorithms improve
Cheung, Kwok, and Lau (JACM, 2013) and are optimal to within a constant factor
and a $\log\log(n)$-factor, respectively. Further, for constant factor
regression and low rank approximation we give the first optimal algorithms, for
the current matrix multiplication exponent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chepurko_N/0/1/0/all/0/1"&gt;Nadiia Chepurko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clarkson_K/0/1/0/all/0/1"&gt;Kenneth L. Clarkson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kacham_P/0/1/0/all/0/1"&gt;Praneeth Kacham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Iterative multi-path tracking for video and volume segmentation with sparse point supervision. (arXiv:1809.00970v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/1809.00970</id>
        <link href="http://arxiv.org/abs/1809.00970"/>
        <updated>2021-07-20T02:04:46.694Z</updated>
        <summary type="html"><![CDATA[Recent machine learning strategies for segmentation tasks have shown great
ability when trained on large pixel-wise annotated image datasets. It remains a
major challenge however to aggregate such datasets, as the time and monetary
cost associated with collecting extensive annotations is extremely high. This
is particularly the case for generating precise pixel-wise annotations in video
and volumetric image data. To this end, this work presents a novel framework to
produce pixel-wise segmentations using minimal supervision. Our method relies
on 2D point supervision, whereby a single 2D location within an object of
interest is provided on each image of the data. Our method then estimates the
object appearance in a semi-supervised fashion by learning
object-image-specific features and by using these in a semi-supervised learning
framework. Our object model is then used in a graph-based optimization problem
that takes into account all provided locations and the image data in order to
infer the complete pixel-wise segmentation. In practice, we solve this
optimally as a tracking problem using a K-shortest path approach. Both the
object model and segmentation are then refined iteratively to further improve
the final segmentation. We show that by collecting 2D locations using a gaze
tracker, our approach can provide state-of-the-art segmentations on a range of
objects and image modalities (video and 3D volumes), and that these can then be
used to train supervised machine learning classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lejeune_L/0/1/0/all/0/1"&gt;Laurent Lejeune&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grossrieder_J/0/1/0/all/0/1"&gt;Jan Grossrieder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1"&gt;Raphael Sznitman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14399</id>
        <link href="http://arxiv.org/abs/2105.14399"/>
        <updated>2021-07-20T02:04:46.693Z</updated>
        <summary type="html"><![CDATA[Current out-of-distribution detection approaches usually present special
requirements (e.g., collecting outlier data and hyperparameter validation) and
produce side effects (classification accuracy drop and slow/inefficient
inferences). Recently, entropic out-of-distribution detection has been proposed
as a seamless approach (i.e., a solution that avoids all the previously
mentioned drawbacks). The entropic out-of-distribution detection solution
comprises the IsoMax loss for training and the entropic score for
out-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in
replacement because swapping the SoftMax loss with the IsoMax loss requires no
changes in the model's architecture or training procedures/hyperparameters. In
this paper, we propose to perform what we call an isometrization of the
distances used in the IsoMax loss. Additionally, we propose to replace the
entropic score with the minimum distance score. Our experiments showed that
these simple modifications increase out-of-distribution detection performance
while keeping the solution seamless. Code available at
$\href{https://github.com/dlmacedo/entropic-out-of-distribution-detection}{\text{entropic
out-of-distribution detection}}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1"&gt;David Mac&amp;#xea;do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptive Robotic Gesture Recognition with Unsupervised Kinematic-Visual Data Alignment. (arXiv:2103.04075v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04075</id>
        <link href="http://arxiv.org/abs/2103.04075"/>
        <updated>2021-07-20T02:04:46.690Z</updated>
        <summary type="html"><![CDATA[Automated surgical gesture recognition is of great importance in
robot-assisted minimally invasive surgery. However, existing methods assume
that training and testing data are from the same domain, which suffers from
severe performance degradation when a domain gap exists, such as the simulator
and real robot. In this paper, we propose a novel unsupervised domain
adaptation framework which can simultaneously transfer multi-modality
knowledge, i.e., both kinematic and visual data, from simulator to real robot.
It remedies the domain gap with enhanced transferable features by using
temporal cues in videos, and inherent correlations in multi-modal towards
recognizing gesture. Specifically, we first propose an MDO-K to align
kinematics, which exploits temporal continuity to transfer motion directions
with smaller gap rather than position values, relieving the adaptation burden.
Moreover, we propose a KV-Relation-ATT to transfer the co-occurrence signals of
kinematics and vision. Such features attended by correlation similarity are
more informative for enhancing domain-invariance of the model. Two feature
alignment strategies benefit the model mutually during the end-to-end learning
process. We extensively evaluate our method for gesture recognition using DESK
dataset with peg transfer procedure. Results show that our approach recovers
the performance with great improvement gains, up to 12.91% in ACC and 20.16% in
F1score without using any annotations in real robot.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xueying Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yueming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1"&gt;Qi Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jing Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng-Ann Heng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Reinforcement Learning with Optimal Level Synchronization based on a Deep Generative Model. (arXiv:2107.08183v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08183</id>
        <link href="http://arxiv.org/abs/2107.08183"/>
        <updated>2021-07-20T02:04:46.690Z</updated>
        <summary type="html"><![CDATA[The high-dimensional or sparse reward task of a reinforcement learning (RL)
environment requires a superior potential controller such as hierarchical
reinforcement learning (HRL) rather than an atomic RL because it absorbs the
complexity of commands to achieve the purpose of the task in its hierarchical
structure. One of the HRL issues is how to train each level policy with the
optimal data collection from its experience. That is to say, how to synchronize
adjacent level policies optimally. Our research finds that a HRL model through
the off-policy correction technique of HRL, which trains a higher-level policy
with the goal of reflecting a lower-level policy which is newly trained using
the off-policy method, takes the critical role of synchronizing both level
policies at all times while they are being trained. We propose a novel HRL
model supporting the optimal level synchronization using the off-policy
correction technique with a deep generative model. This uses the advantage of
the inverse operation of a flow-based deep generative model (FDGM) to achieve
the goal corresponding to the current state of the lower-level policy. The
proposed model also considers the freedom of the goal dimension between HRL
policies which makes it the generalized inverse model of the model-free RL in
HRL with the optimal synchronization method. The comparative experiment results
show the performance of our proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;JaeYoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xuan_J/0/1/0/all/0/1"&gt;Junyu Xuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Christy Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_F/0/1/0/all/0/1"&gt;Farookh Hussain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Risk-Sensitive Reinforcement Learning Agents for Trading Markets. (arXiv:2107.08083v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08083</id>
        <link href="http://arxiv.org/abs/2107.08083"/>
        <updated>2021-07-20T02:04:46.689Z</updated>
        <summary type="html"><![CDATA[Trading markets represent a real-world financial application to deploy
reinforcement learning agents, however, they carry hard fundamental challenges
such as high variance and costly exploration. Moreover, markets are inherently
a multiagent domain composed of many actors taking actions and changing the
environment. To tackle these type of scenarios agents need to exhibit certain
characteristics such as risk-awareness, robustness to perturbations and low
learning variance. We take those as building blocks and propose a family of
four algorithms. First, we contribute with two algorithms that use risk-averse
objective functions and variance reduction techniques. Then, we augment the
framework to multi-agent learning and assume an adversary which can take over
and perturb the learning process. Our third and fourth algorithms perform well
under this setting and balance theoretical guarantees with practical use.
Additionally, we consider the multi-agent nature of the environment and our
work is the first one extending empirical game theory analysis for multi-agent
learning by considering risk-sensitive payoffs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lui_K/0/1/0/all/0/1"&gt;Kry Yik Chau Lui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Leal_P/0/1/0/all/0/1"&gt;Pablo Hernandez-Leal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Optimize under Non-Stationarity. (arXiv:1810.03024v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.03024</id>
        <link href="http://arxiv.org/abs/1810.03024"/>
        <updated>2021-07-20T02:04:46.689Z</updated>
        <summary type="html"><![CDATA[We introduce algorithms that achieve state-of-the-art \emph{dynamic regret}
bounds for non-stationary linear stochastic bandit setting. It captures natural
applications such as dynamic pricing and ads allocation in a changing
environment. We show how the difficulty posed by the non-stationarity can be
overcome by a novel marriage between stochastic and adversarial bandits
learning algorithms. Defining $d,B_T,$ and $T$ as the problem dimension, the
\emph{variation budget}, and the total time horizon, respectively, our main
contributions are the tuned Sliding Window UCB (\texttt{SW-UCB}) algorithm with
optimal $\widetilde{O}(d^{2/3}(B_T+1)^{1/3}T^{2/3})$ dynamic regret, and the
tuning free bandit-over-bandit (\texttt{BOB}) framework built on top of the
\texttt{SW-UCB} algorithm with best
$\widetilde{O}(d^{2/3}(B_T+1)^{1/4}T^{3/4})$ dynamic regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_W/0/1/0/all/0/1"&gt;Wang Chi Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1"&gt;David Simchi-Levi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Ruihao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematical Solution for Face De-identification. (arXiv:2107.08581v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08581</id>
        <link href="http://arxiv.org/abs/2107.08581"/>
        <updated>2021-07-20T02:04:46.688Z</updated>
        <summary type="html"><![CDATA[With the identity information in face data more closely related to personal
credit and property security, people pay increasing attention to the protection
of face data privacy. In different tasks, people have various requirements for
face de-identification (De-ID), so we propose a systematical solution
compatible for these De-ID operations. Firstly, an attribute disentanglement
and generative network is constructed to encode two parts of the face, which
are the identity (facial features like mouth, nose and eyes) and expression
(including expression, pose and illumination). Through face swapping, we can
remove the original ID completely. Secondly, we add an adversarial vector
mapping network to perturb the latent code of the face image, different from
previous traditional adversarial methods. Through this, we can construct
unrestricted adversarial image to decrease ID similarity recognized by model.
Our method can flexibly de-identify the face data in various ways and the
processed images have high image quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Songlin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yuehua Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jing Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Otimizacao de Redes Neurais atraves de Algoritmos Geneticos Celulares. (arXiv:2107.08326v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.08326</id>
        <link href="http://arxiv.org/abs/2107.08326"/>
        <updated>2021-07-20T02:04:46.687Z</updated>
        <summary type="html"><![CDATA[This works proposes a methodology to searching for automatically Artificial
Neural Networks (ANN) by using Cellular Genetic Algorithm (CGA). The goal of
this methodology is to find compact networks whit good performance for
classification problems. The main reason for developing this work is centered
at the difficulties of configuring compact ANNs with good performance rating.
The use of CGAs aims at seeking the components of the RNA in the same way that
a common Genetic Algorithm (GA), but it has the differential of incorporating a
Cellular Automaton (CA) to give location for the GA individuals. The location
imposed by the CA aims to control the spread of solutions in the populations to
maintain the genetic diversity for longer time. This genetic diversity is
important for obtain good results with the GAs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1"&gt;Anderson da Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning with Nonsmooth Objectives. (arXiv:2107.08800v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08800</id>
        <link href="http://arxiv.org/abs/2107.08800"/>
        <updated>2021-07-20T02:04:46.686Z</updated>
        <summary type="html"><![CDATA[We explore the potential for using a nonsmooth loss function based on the
max-norm in the training of an artificial neural network. We hypothesise that
this may lead to superior classification results in some special cases where
the training data is either very small or unbalanced.

Our numerical experiments performed on a simple artificial neural network
with no hidden layers (a setting immediately amenable to standard nonsmooth
optimisation techniques) appear to confirm our hypothesis that uniform
approximation based approaches may be more suitable for the datasets with
reliable training data that either is limited size or biased in terms of
relative cluster sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peiris_V/0/1/0/all/0/1"&gt;Vinesha Peiris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhorukova_N/0/1/0/all/0/1"&gt;Nadezda Sukhorukova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roshchina_V/0/1/0/all/0/1"&gt;Vera Roshchina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boost-R: Gradient Boosted Trees for Recurrence Data. (arXiv:2107.08784v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08784</id>
        <link href="http://arxiv.org/abs/2107.08784"/>
        <updated>2021-07-20T02:04:46.685Z</updated>
        <summary type="html"><![CDATA[Recurrence data arise from multi-disciplinary domains spanning reliability,
cyber security, healthcare, online retailing, etc. This paper investigates an
additive-tree-based approach, known as Boost-R (Boosting for Recurrence Data),
for recurrent event data with both static and dynamic features. Boost-R
constructs an ensemble of gradient boosted additive trees to estimate the
cumulative intensity function of the recurrent event process, where a new tree
is added to the ensemble by minimizing the regularized L2 distance between the
observed and predicted cumulative intensity. Unlike conventional regression
trees, a time-dependent function is constructed by Boost-R on each tree leaf.
The sum of these functions, from multiple trees, yields the ensemble estimator
of the cumulative intensity. The divide-and-conquer nature of tree-based
methods is appealing when hidden sub-populations exist within a heterogeneous
population. The non-parametric nature of regression trees helps to avoid
parametric assumptions on the complex interactions between event processes and
features. Critical insights and advantages of Boost-R are investigated through
comprehensive numerical examples. Datasets and computer code of Boost-R are
made available on GitHub. To our best knowledge, Boost-R is the first gradient
boosted additive-tree-based approach for modeling large-scale recurrent event
data with both static and dynamic feature information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1"&gt;Rong Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cardiac Segmentation on Late Gadolinium Enhancement MRI: A Benchmark Study from Multi-Sequence Cardiac MR Segmentation Challenge. (arXiv:2006.12434v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12434</id>
        <link href="http://arxiv.org/abs/2006.12434"/>
        <updated>2021-07-20T02:04:46.616Z</updated>
        <summary type="html"><![CDATA[Accurate computing, analysis and modeling of the ventricles and myocardium
from medical images are important, especially in the diagnosis and treatment
management for patients suffering from myocardial infarction (MI). Late
gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) provides an
important protocol to visualize MI. However, automated segmentation of LGE CMR
is still challenging, due to the indistinguishable boundaries, heterogeneous
intensity distribution and complex enhancement patterns of pathological
myocardium from LGE CMR. Furthermore, compared with the other sequences LGE CMR
images with gold standard labels are particularly limited, which represents
another obstacle for developing novel algorithms for automatic segmentation of
LGE CMR. This paper presents the selective results from the Multi-Sequence
Cardiac MR (MS-CMR) Segmentation challenge, in conjunction with MICCAI 2019.
The challenge offered a data set of paired MS-CMR images, including auxiliary
CMR sequences as well as LGE CMR, from 45 patients who underwent
cardiomyopathy. It was aimed to develop new algorithms, as well as benchmark
existing ones for LGE CMR segmentation and compare them objectively. In
addition, the paired MS-CMR images could enable algorithms to combine the
complementary information from the other sequences for the segmentation of LGE
CMR. Nine representative works were selected for evaluation and comparisons,
among which three methods are unsupervised methods and the other six are
supervised. The results showed that the average performance of the nine methods
was comparable to the inter-observer variations. The success of these methods
was mainly attributed to the inclusion of the auxiliary sequences from the
MS-CMR images, which provide important label information for the training of
deep neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiahai Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiahang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xinzhe Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ouyang_C/0/1/0/all/0/1"&gt;Cheng Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Campello_V/0/1/0/all/0/1"&gt;Victor M. Campello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vesal_S/0/1/0/all/0/1"&gt;Sulaiman Vesal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+RaviKumar_N/0/1/0/all/0/1"&gt;Nishant RaviKumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yashu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1"&gt;Gongning Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingkun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ly_B/0/1/0/all/0/1"&gt;Buntheng Ly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sermesant_M/0/1/0/all/0/1"&gt;Maxime Sermesant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1"&gt;Holger Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wentao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiexiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xinghao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Cell Detection in Time-lapse Images Using Temporal Consistency. (arXiv:2107.08639v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08639</id>
        <link href="http://arxiv.org/abs/2107.08639"/>
        <updated>2021-07-20T02:04:46.615Z</updated>
        <summary type="html"><![CDATA[Cell detection is the task of detecting the approximate positions of cell
centroids from microscopy images. Recently, convolutional neural network-based
approaches have achieved promising performance. However, these methods require
a certain amount of annotation for each imaging condition. This annotation is a
time-consuming and labor-intensive task. To overcome this problem, we propose a
semi-supervised cell-detection method that effectively uses a time-lapse
sequence with one labeled image and the other images unlabeled. First, we train
a cell-detection network with a one-labeled image and estimate the unlabeled
images with the trained network. We then select high-confidence positions from
the estimations by tracking the detected cells from the labeled frame to those
far from it. Next, we generate pseudo-labels from the tracking results and
train the network by using pseudo-labels. We evaluated our method for seven
conditions of public datasets, and we achieved the best results relative to
other semi-supervised methods. Our code is available at
https://github.com/naivete5656/SCDTC]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nishimura_K/0/1/0/all/0/1"&gt;Kazuya Nishimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hyeonwoo Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bise_R/0/1/0/all/0/1"&gt;Ryoma Bise&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Crowd Localization with Multi-focus Gaussian Neighbor Attention and a Large-Scale Benchmark. (arXiv:2107.08645v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08645</id>
        <link href="http://arxiv.org/abs/2107.08645"/>
        <updated>2021-07-20T02:04:46.614Z</updated>
        <summary type="html"><![CDATA[Video crowd localization is a crucial yet challenging task, which aims to
estimate exact locations of human heads in the given crowded videos. To model
spatial-temporal dependencies of human mobility, we propose a multi-focus
Gaussian neighbor attention (GNA), which can effectively exploit long-range
correspondences while maintaining the spatial topological structure of the
input videos. In particular, our GNA can also capture the scale variation of
human heads well using the equipped multi-focus mechanism. Based on the
multi-focus GNA, we develop a unified neural network called GNANet to
accurately locate head centers in video clips by fully aggregating
spatial-temporal information via a scene modeling module and a context
cross-attention module. Moreover, to facilitate future researches in this
field, we introduce a large-scale crowded video benchmark named SenseCrowd,
which consists of 60K+ frames captured in various surveillance scenarios and
2M+ head annotations. Finally, we conduct extensive experiments on three
datasets including our SenseCrowd, and the experiment results show that the
proposed method is capable to achieve state-of-the-art performance for both
video crowd localization and counting. The code and the dataset will be
released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haopeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kunlin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shinan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Junyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1"&gt;Bin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1"&gt;Jun Hou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TAEN: Temporal Aware Embedding Network for Few-Shot Action Recognition. (arXiv:2004.10141v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.10141</id>
        <link href="http://arxiv.org/abs/2004.10141"/>
        <updated>2021-07-20T02:04:46.614Z</updated>
        <summary type="html"><![CDATA[Classification of new class entities requires collecting and annotating
hundreds or thousands of samples that is often prohibitively costly. Few-shot
learning suggests learning to classify new classes using just a few examples.
Only a small number of studies address the challenge of few-shot learning on
spatio-temporal patterns such as videos. In this paper, we present the Temporal
Aware Embedding Network (TAEN) for few-shot action recognition, that learns to
represent actions, in a metric space as a trajectory, conveying both short term
semantics and longer term connectivity between action parts. We demonstrate the
effectiveness of TAEN on two few shot tasks, video classification and temporal
action detection and evaluate our method on the Kinetics-400 and on ActivityNet
1.2 few-shot benchmarks. With training of just a few fully connected layers we
reach comparable results to prior art on both few shot video classification and
temporal detection tasks, while reaching state-of-the-art in certain scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1"&gt;Rami Ben-Ari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shpigel_M/0/1/0/all/0/1"&gt;Mor Shpigel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azulai_O/0/1/0/all/0/1"&gt;Ophir Azulai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzelay_U/0/1/0/all/0/1"&gt;Udi Barzelay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rotman_D/0/1/0/all/0/1"&gt;Daniel Rotman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relatively Lazy: Indoor-Outdoor Navigation Using Vision and GNSS. (arXiv:2101.05107v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05107</id>
        <link href="http://arxiv.org/abs/2101.05107"/>
        <updated>2021-07-20T02:04:46.613Z</updated>
        <summary type="html"><![CDATA[Visual Teach and Repeat has shown relative navigation is a robust and
efficient solution for autonomous vision-based path following in difficult
environments. Adding additional absolute sensors such as Global Navigation
Satellite Systems (GNSS) has the potential to expand the domain of Visual Teach
and Repeat to environments where the ability to visually localize is not
guaranteed. Our method of lazy mapping and delaying estimation until a
path-tracking error is needed avoids the need to estimate absolute states. As a
result, map optimization is not required and paths can be driven immediately
after being taught. We validate our approach on a real robot through an
experiment in a joint indoor-outdoor environment comprising 3.5km of autonomous
route repeating across a variety of lighting conditions. We achieve smooth
error signals throughout the runs despite large sections of dropout for each
sensor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Congram_B/0/1/0/all/0/1"&gt;Benjamin Congram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barfoot_T/0/1/0/all/0/1"&gt;Timothy D. Barfoot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A2-FPN for Semantic Segmentation of Fine-Resolution Remotely Sensed Images. (arXiv:2102.07997v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07997</id>
        <link href="http://arxiv.org/abs/2102.07997"/>
        <updated>2021-07-20T02:04:46.613Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation using fine-resolution remotely sensed images plays a
critical role in many practical applications, such as urban planning,
environmental protection, natural and anthropogenic landscape monitoring, etc.
However, the automation of semantic segmentation, i.e., automatic
categorization/labeling and segmentation is still a challenging task,
particularly for fine-resolution images with huge spatial and spectral
complexity. Addressing such a problem represents an exciting research field,
which paves the way for scene-level landscape pattern analysis and decision
making. In this paper, we propose an approach for automatic land segmentation
based on the Feature Pyramid Network (FPN). As a classic architecture, FPN can
build a feature pyramid with high-level semantics throughout. However,
intrinsic defects in feature extraction and fusion hinder FPN from further
aggregating more discriminative features. Hence, we propose an Attention
Aggregation Module (AAM) to enhance multi-scale feature learning through
attention-guided feature aggregation. Based on FPN and AAM, a novel framework
named Attention Aggregation Feature Pyramid Network (A2-FPN) is developed for
semantic segmentation of fine-resolution remotely sensed images. Extensive
experiments conducted on three datasets demonstrate the effectiveness of our A2
-FPN in segmentation accuracy. Code is available at
https://github.com/lironui/A2-FPN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shunyi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1"&gt;Chenxi Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Libo Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient and Visualizable Convolutional Neural Networks for COVID-19 Classification Using Chest CT. (arXiv:2012.11860v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11860</id>
        <link href="http://arxiv.org/abs/2012.11860"/>
        <updated>2021-07-20T02:04:46.612Z</updated>
        <summary type="html"><![CDATA[With COVID-19 cases rising rapidly, deep learning has emerged as a promising
diagnosis technique. However, identifying the most accurate models to
characterize COVID-19 patients is challenging because comparing results
obtained with different types of data and acquisition processes is non-trivial.
In this paper we designed, evaluated, and compared the performance of 20
convolutional neutral networks in classifying patients as COVID-19 positive,
healthy, or suffering from other pulmonary lung infections based on Chest CT
scans, serving as the first to consider the EfficientNet family for COVID-19
diagnosis and employ intermediate activation maps for visualizing model
performance. All models are trained and evaluated in Python using 4173 Chest CT
images from the dataset entitled "A COVID multiclass dataset of CT scans," with
2168, 758, and 1247 images of patients that are COVID-19 positive, healthy, or
suffering from other pulmonary infections, respectively. EfficientNet-B5 was
identified as the best model with an F1 score of 0.9769+/-0.0046, accuracy of
0.9759+/-0.0048, sensitivity of 0.9788+/-0.0055, specificity of
0.9730+/-0.0057, and precision of 0.9751 +/- 0.0051. On an alternate 2-class
dataset, EfficientNetB5 obtained an accuracy of 0.9845+/-0.0109, F1 score of
0.9599+/-0.0251, sensitivity of 0.9682+/-0.0099, specificity of
0.9883+/-0.0150, and precision of 0.9526 +/- 0.0523. Intermediate activation
maps and Gradient-weighted Class Activation Mappings offered
human-interpretable evidence of the model's perception of ground-class
opacities and consolidations, hinting towards a promising use-case of
artificial intelligence-assisted radiology tools. With a prediction speed of
under 0.1 seconds on GPUs and 0.5 seconds on CPUs, our proposed model offers a
rapid, scalable, and accurate diagnostic for COVID-19.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Garg_A/0/1/0/all/0/1"&gt;Aksh Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salehi_S/0/1/0/all/0/1"&gt;Sana Salehi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rocca_M/0/1/0/all/0/1"&gt;Marianna La Rocca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garner_R/0/1/0/all/0/1"&gt;Rachael Garner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duncan_D/0/1/0/all/0/1"&gt;Dominique Duncan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-binary deep transfer learning for imageclassification. (arXiv:2107.08585v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08585</id>
        <link href="http://arxiv.org/abs/2107.08585"/>
        <updated>2021-07-20T02:04:46.611Z</updated>
        <summary type="html"><![CDATA[The current standard for a variety of computer vision tasks using smaller
numbers of labelled training examples is to fine-tune from weights pre-trained
on a large image classification dataset such as ImageNet. The application of
transfer learning and transfer learning methods tends to be rigidly binary. A
model is either pre-trained or not pre-trained. Pre-training a model either
increases performance or decreases it, the latter being defined as negative
transfer. Application of L2-SP regularisation that decays the weights towards
their pre-trained values is either applied or all weights are decayed towards
0. This paper re-examines these assumptions. Our recommendations are based on
extensive empirical evaluation that demonstrate the application of a non-binary
approach to achieve optimal results. (1) Achieving best performance on each
individual dataset requires careful adjustment of various transfer learning
hyperparameters not usually considered, including number of layers to transfer,
different learning rates for different layers and different combinations of
L2SP and L2 regularization. (2) Best practice can be achieved using a number of
measures of how well the pre-trained weights fit the target dataset to guide
optimal hyperparameters. We present methods for non-binary transfer learning
including combining L2SP and L2 regularization and performing non-traditional
fine-tuning hyperparameter searches. Finally we suggest heuristics for
determining the optimal transfer learning hyperparameters. The benefits of
using a non-binary approach are supported by final results that come close to
or exceed state of the art performance on a variety of tasks that have
traditionally been more difficult for transfer learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plested_J/0/1/0/all/0/1"&gt;Jo Plested&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xuyang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1"&gt;Tom Gedeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facial Expressions Recognition with Convolutional Neural Networks. (arXiv:2107.08640v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08640</id>
        <link href="http://arxiv.org/abs/2107.08640"/>
        <updated>2021-07-20T02:04:46.611Z</updated>
        <summary type="html"><![CDATA[Over the centuries, humans have developed and acquired a number of ways to
communicate. But hardly any of them can be as natural and instinctive as facial
expressions. On the other hand, neural networks have taken the world by storm.
And no surprises, that the area of Computer Vision and the problem of facial
expressions recognitions hasn't remained untouched. Although a wide range of
techniques have been applied, achieving extremely high accuracies and preparing
highly robust FER systems still remains a challenge due to heterogeneous
details in human faces. In this paper, we will be deep diving into implementing
a system for recognition of facial expressions (FER) by leveraging neural
networks, and more specifically, Convolutional Neural Networks (CNNs). We adopt
the fundamental concepts of deep learning and computer vision with various
architectures, fine-tune it's hyperparameters and experiment with various
optimization methods and demonstrate a state-of-the-art single-network-accuracy
of 70.10% on the FER2013 dataset without using any additional training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lonkar_S/0/1/0/all/0/1"&gt;Subodh Lonkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Disentangling Latent Space for Unsupervised Semantic Face Editing. (arXiv:2011.02638v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02638</id>
        <link href="http://arxiv.org/abs/2011.02638"/>
        <updated>2021-07-20T02:04:46.610Z</updated>
        <summary type="html"><![CDATA[Facial attributes in StyleGAN generated images are entangled in the latent
space which makes it very difficult to independently control a specific
attribute without affecting the others. Supervised attribute editing requires
annotated training data which is difficult to obtain and limits the editable
attributes to those with labels. Therefore, unsupervised attribute editing in
an disentangled latent space is key to performing neat and versatile semantic
face editing. In this paper, we present a new technique termed
Structure-Texture Independent Architecture with Weight Decomposition and
Orthogonal Regularization (STIA-WO) to disentangle the latent space for
unsupervised semantic face editing. By applying STIA-WO to GAN, we have
developed a StyleGAN termed STGAN-WO which performs weight decomposition
through utilizing the style vector to construct a fully controllable weight
matrix to regulate image synthesis, and employs orthogonal regularization to
ensure each entry of the style vector only controls one independent feature
matrix. To further disentangle the facial attributes, STGAN-WO introduces a
structure-texture independent architecture which utilizes two independently and
identically distributed (i.i.d.) latent vectors to control the synthesis of the
texture and structure components in a disentangled way. Unsupervised semantic
editing is achieved by moving the latent code in the coarse layers along its
orthogonal directions to change texture related attributes or changing the
latent code in the fine layers to manipulate structure related ones. We present
experimental results which show that our new STGAN-WO can achieve better
attribute editing than state of the art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kanglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1"&gt;Gaofeng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"&gt;Fei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bozhi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jiang Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1"&gt;Guoping Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face.evoLVe: A High-Performance Face Recognition Library. (arXiv:2107.08621v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08621</id>
        <link href="http://arxiv.org/abs/2107.08621"/>
        <updated>2021-07-20T02:04:46.609Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop face.evoLVe -- a comprehensive library that
collects and implements a wide range of popular deep learning-based methods for
face recognition. First of all, face.evoLVe is composed of key components that
cover the full process of face analytics, including face alignment, data
processing, various backbones, losses, and alternatives with bags of tricks for
improving performance. Later, face.evoLVe supports multi-GPU training on top of
different deep learning platforms, such as PyTorch and PaddlePaddle, which
facilitates researchers to work on both large-scale datasets with millions of
images and low-shot counterparts with limited well-annotated data. More
importantly, along with face.evoLVe, images before & after alignment in the
common benchmark datasets are released with source codes and trained models
provided. All these efforts lower the technical burdens in reproducing the
existing methods for comparison, while users of our library could focus on
developing advanced approaches more efficiently. Last but not least,
face.evoLVe is well designed and vibrantly evolving, so that new face
recognition approaches can be easily plugged into our framework. Note that we
have used face.evoLVe to participate in a number of face recognition
competitions and secured the first place. The version that supports PyTorch is
publicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the
PaddlePaddle version is available at
https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.
Face.evoLVe has been widely used for face analytics, receiving 2.4K stars and
622 forks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qingzhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengfei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jian Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PSIGAN: Joint probabilistic segmentation and image distribution matching for unpaired cross-modality adaptation based MRI segmentation. (arXiv:2007.09465v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.09465</id>
        <link href="http://arxiv.org/abs/2007.09465"/>
        <updated>2021-07-20T02:04:46.608Z</updated>
        <summary type="html"><![CDATA[We developed a new joint probabilistic segmentation and image distribution
matching generative adversarial network (PSIGAN) for unsupervised domain
adaptation (UDA) and multi-organ segmentation from magnetic resonance (MRI)
images. Our UDA approach models the co-dependency between images and their
segmentation as a joint probability distribution using a new structure
discriminator. The structure discriminator computes structure of interest
focused adversarial loss by combining the generated pseudo MRI with
probabilistic segmentations produced by a simultaneously trained segmentation
sub-network. The segmentation sub-network is trained using the pseudo MRI
produced by the generator sub-network. This leads to a cyclical optimization of
both the generator and segmentation sub-networks that are jointly trained as
part of an end-to-end network. Extensive experiments and comparisons against
multiple state-of-the-art methods were done on four different MRI sequences
totalling 257 scans for generating multi-organ and tumor segmentation. The
experiments included, (a) 20 T1-weighted (T1w) in-phase mdixon and (b) 20
T2-weighted (T2w) abdominal MRI for segmenting liver, spleen, left and right
kidneys, (c) 162 T2-weighted fat suppressed head and neck MRI (T2wFS) for
parotid gland segmentation, and (d) 75 T2w MRI for lung tumor segmentation. Our
method achieved an overall average DSC of 0.87 on T1w and 0.90 on T2w for the
abdominal organs, 0.82 on T2wFS for the parotid glands, and 0.77 on T2w MRI for
lung tumors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jue Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yu Chi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tyagi_N/0/1/0/all/0/1"&gt;Neelam Tyagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rimner_A/0/1/0/all/0/1"&gt;Andreas Rimner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_N/0/1/0/all/0/1"&gt;Nancy Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deasy_J/0/1/0/all/0/1"&gt;Joseph O. Deasy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Berry_S/0/1/0/all/0/1"&gt;Sean Berry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Veeraraghavan_H/0/1/0/all/0/1"&gt;Harini Veeraraghavan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Review of Video Predictive Understanding: Early Action Recognition and Future Action Prediction. (arXiv:2107.05140v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05140</id>
        <link href="http://arxiv.org/abs/2107.05140"/>
        <updated>2021-07-20T02:04:46.587Z</updated>
        <summary type="html"><![CDATA[Video predictive understanding encompasses a wide range of efforts that are
concerned with the anticipation of the unobserved future from the current as
well as historical video observations. Action prediction is a major sub-area of
video predictive understanding and is the focus of this review. This sub-area
has two major subdivisions: early action recognition and future action
prediction. Early action recognition is concerned with recognizing an ongoing
action as soon as possible. Future action prediction is concerned with the
anticipation of actions that follow those previously observed. In either case,
the \textbf{\textit{causal}} relationship between the past, current, and
potential future information is the main focus. Various mathematical tools such
as Markov Chains, Gaussian Processes, Auto-Regressive modeling, and Bayesian
recursive filtering are widely adopted jointly with computer vision techniques
for these two tasks. However, these approaches face challenges such as the
curse of dimensionality, poor generalization, and constraints from
domain-specific knowledge. Recently, structures that rely on deep convolutional
neural networks and recurrent neural networks have been extensively proposed
for improving the performance of existing vision tasks, in general, and action
prediction tasks, in particular. However, they have their own shortcomings, \eg
reliance on massive training data and lack of strong theoretical underpinnings.
In this survey, we start by introducing the major sub-areas of the broad area
of video predictive understanding, which recently have received intensive
attention and proven to have practical value. Next, a thorough review of
various early action recognition and future action prediction algorithms are
provided with suitably organized divisions. Finally, we conclude our discussion
with future research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;He Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1"&gt;Richard P. Wildes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Desiderata for Explainable AI in statistical production systems of the European Central Bank. (arXiv:2107.08045v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.08045</id>
        <link href="http://arxiv.org/abs/2107.08045"/>
        <updated>2021-07-20T02:04:46.586Z</updated>
        <summary type="html"><![CDATA[Explainable AI constitutes a fundamental step towards establishing fairness
and addressing bias in algorithmic decision-making. Despite the large body of
work on the topic, the benefit of solutions is mostly evaluated from a
conceptual or theoretical point of view and the usefulness for real-world use
cases remains uncertain. In this work, we aim to state clear user-centric
desiderata for explainable AI reflecting common explainability needs
experienced in statistical production systems of the European Central Bank. We
link the desiderata to archetypical user roles and give examples of techniques
and methods which can be used to address the user's needs. To this end, we
provide two concrete use cases from the domain of statistical data production
in central banks: the detection of outliers in the Centralised Securities
Database and the data-driven identification of data quality checks for the
Supervisory Banking data system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Navarro_C/0/1/0/all/0/1"&gt;Carlos Mougan Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanellos_G/0/1/0/all/0/1"&gt;Georgios Kanellos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gottron_T/0/1/0/all/0/1"&gt;Thomas Gottron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Hierarchical Graph Neural Networks for Image Clustering. (arXiv:2107.01319v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01319</id>
        <link href="http://arxiv.org/abs/2107.01319"/>
        <updated>2021-07-20T02:04:46.585Z</updated>
        <summary type="html"><![CDATA[We propose a hierarchical graph neural network (GNN) model that learns how to
cluster a set of images into an unknown number of identities using a training
set of images annotated with labels belonging to a disjoint set of identities.
Our hierarchical GNN uses a novel approach to merge connected components
predicted at each level of the hierarchy to form a new graph at the next level.
Unlike fully unsupervised hierarchical clustering, the choice of grouping and
complexity criteria stems naturally from supervision in the training set. The
resulting method, Hi-LANDER, achieves an average of 54% improvement in F-score
and 8% increase in Normalized Mutual Information (NMI) relative to current
GNN-based clustering algorithms. Additionally, state-of-the-art GNN-based
methods rely on separate models to predict linkage probabilities and node
densities as intermediate steps of the clustering process. In contrast, our
unified framework achieves a seven-fold decrease in computational cost. We
release our training and inference code at
https://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1"&gt;Yifan Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1"&gt;Tianjun Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yuanjun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1"&gt;Wei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1"&gt;David Wipf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Multi-Agent Reinforcement Learning for Task Offloading Under Uncertainty. (arXiv:2107.08114v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08114</id>
        <link href="http://arxiv.org/abs/2107.08114"/>
        <updated>2021-07-20T02:04:46.584Z</updated>
        <summary type="html"><![CDATA[Multi-Agent Reinforcement Learning (MARL) is a challenging subarea of
Reinforcement Learning due to the non-stationarity of the environments and the
large dimensionality of the combined action space. Deep MARL algorithms have
been applied to solve different task offloading problems. However, in
real-world applications, information required by the agents (i.e. rewards and
states) are subject to noise and alterations. The stability and the robustness
of deep MARL to practical challenges is still an open research problem. In this
work, we apply state-of-the art MARL algorithms to solve task offloading with
reward uncertainty. We show that perturbations in the reward signal can induce
decrease in the performance compared to learning with perfect rewards. We
expect this paper to stimulate more research in studying and addressing the
practical challenges of deploying deep MARL solutions in wireless
communications systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yuanchao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1"&gt;Amal Feriani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1"&gt;Ekram Hossain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06419</id>
        <link href="http://arxiv.org/abs/2103.06419"/>
        <updated>2021-07-20T02:04:46.582Z</updated>
        <summary type="html"><![CDATA[Background and objective: In this paper, a modified U-Net based framework is
presented, which leverages techniques from Squeeze-and-Excitation (SE) block,
Atrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and
robust liver CT segmentation, and the effectiveness of the proposed method was
tested on two public datasets LiTS17 and SLiver07.

Methods: A new network architecture called SAR-U-Net was designed. Firstly,
the SE block is introduced to adaptively extract image features after each
convolution in the U-Net encoder, while suppressing irrelevant regions, and
highlighting features of specific segmentation task; Secondly, ASPP was
employed to replace the transition layer and the output layer, and acquire
multi-scale image information via different receptive fields. Thirdly, to
alleviate the degradation problem, the traditional convolution block was
replaced with the residual block and thus prompt the network to gain accuracy
from considerably increased depth.

Results: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and
MSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other
closely related 2D-based models, the proposed method achieved the highest
accuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,
ASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared
with other closely related models, the proposed method achieved the highest
segmentation accuracy except for the RVD.

Conclusion: The proposed model enables a great improvement on the accuracy
compared to 2D-based models, and its robustness in circumvent challenging
problems, such as small liver regions, discontinuous liver regions, and fuzzy
liver boundaries, is also well demonstrated and validated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jinke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1"&gt;Peiqing Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comparative study of stochastic and deep generative models for multisite precipitation synthesis. (arXiv:2107.08074v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08074</id>
        <link href="http://arxiv.org/abs/2107.08074"/>
        <updated>2021-07-20T02:04:46.581Z</updated>
        <summary type="html"><![CDATA[Future climate change scenarios are usually hypothesized using simulations
from weather generators. However, there only a few works comparing and
evaluating promising deep learning models for weather generation against
classical approaches. This study shows preliminary results making such
evaluations for the multisite precipitation synthesis task. We compared two
open-source weather generators: IBMWeathergen (an extension of the Weathergen
library) and RGeneratePrec, and two deep generative models: GAN and VAE, on a
variety of metrics. Our preliminary results can serve as a guide for improving
the design of deep learning architectures and algorithms for the multisite
precipitation synthesis task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guevara_J/0/1/0/all/0/1"&gt;Jorge Guevara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borges_D/0/1/0/all/0/1"&gt;Dario Borges&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watson_C/0/1/0/all/0/1"&gt;Campbell Watson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zadrozny_B/0/1/0/all/0/1"&gt;Bianca Zadrozny&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Pyramid Network for Multi-task Affective Analysis. (arXiv:2107.03670v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03670</id>
        <link href="http://arxiv.org/abs/2107.03670"/>
        <updated>2021-07-20T02:04:46.580Z</updated>
        <summary type="html"><![CDATA[Affective Analysis is not a single task, and the valence-arousal value,
expression class, and action unit can be predicted at the same time. Previous
researches did not pay enough attention to the entanglement and hierarchical
relation of these three facial attributes. We propose a novel model named
feature pyramid networks for multi-task affect analysis. The hierarchical
features are extracted to predict three labels and we apply a teacher-student
training strategy to learn from pretrained single-task models. Extensive
experiment results demonstrate the proposed model outperforms other models.
This is a submission to The 2nd Workshop and Competition on Affective Behavior
Analysis in the wild (ABAW). The code and model are available for research
purposes at https://github.com/ryanhe312/ABAW2-FPNMAA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1"&gt;Ruian He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1"&gt;Zhen Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1"&gt;Weimin Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bo Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Data Balancing for Unlabeled Satellite Imagery. (arXiv:2107.03227v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.03227</id>
        <link href="http://arxiv.org/abs/2107.03227"/>
        <updated>2021-07-20T02:04:46.580Z</updated>
        <summary type="html"><![CDATA[Data imbalance is a ubiquitous problem in machine learning. In large scale
collected and annotated datasets, data imbalance is either mitigated manually
by undersampling frequent classes and oversampling rare classes, or planned for
with imputation and augmentation techniques. In both cases balancing data
requires labels. In other words, only annotated data can be balanced.
Collecting fully annotated datasets is challenging, especially for large scale
satellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.
Although the NASA Earth Imagery dataset is unlabeled, there are implicit
properties of the data source that we can rely on to hypothesize about its
imbalance, such as distribution of land and water in the case of the Earth's
imagery. We present a new iterative method to balance unlabeled data. Our
method utilizes image embeddings as a proxy for image labels that can be used
to balance data, and ultimately when trained increases overall accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1"&gt;Deep Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1"&gt;Erin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1"&gt;Anirudh Koul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1"&gt;Meher Anand Kasam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Industry and Academic Research in Computer Vision. (arXiv:2107.04902v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04902</id>
        <link href="http://arxiv.org/abs/2107.04902"/>
        <updated>2021-07-20T02:04:46.579Z</updated>
        <summary type="html"><![CDATA[This work aims to study the dynamic between research in the industry and
academia in computer vision. The results are demonstrated on a set of top-5
vision conferences that are representative of the field. Since data for such
analysis was not readily available, significant effort was spent on gathering
and processing meta-data from the original publications. First, this study
quantifies the share of industry-sponsored research. Specifically, it shows
that the proportion of papers published by industry-affiliated researchers is
increasing and that more academics join companies or collaborate with them.
Next, the possible impact of industry presence is further explored, namely in
the distribution of research topics and citation patterns. The results indicate
that the distribution of the research topics is similar in industry and
academic papers. However, there is a strong preference towards citing industry
papers. Finally, possible reasons for citation bias, such as code availability
and influence, are investigated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kotseruba_I/0/1/0/all/0/1"&gt;Iuliia Kotseruba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papagelis_M/0/1/0/all/0/1"&gt;Manos Papagelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1"&gt;John K. Tsotsos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization in Vision: A Survey. (arXiv:2103.02503v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02503</id>
        <link href="http://arxiv.org/abs/2103.02503"/>
        <updated>2021-07-20T02:04:46.578Z</updated>
        <summary type="html"><![CDATA[Generalization to out-of-distribution (OOD) data is a capability natural to
humans yet challenging for machines to reproduce. This is because most learning
algorithms strongly rely on the i.i.d.~assumption on source/target data, which
is often violated in practice due to domain shift. Domain generalization (DG)
aims to achieve OOD generalization by using only source data for model
learning. Since first introduced in 2011, research in DG has made great
progresses. In particular, intensive research in this topic has led to a broad
spectrum of methodologies, e.g., those based on domain alignment,
meta-learning, data augmentation, or ensemble learning, just to name a few; and
has covered various vision applications such as object recognition,
segmentation, action recognition, and person re-identification. In this paper,
for the first time a comprehensive literature review is provided to summarize
the developments in DG for computer vision over the past decade. Specifically,
we first cover the background by formally defining DG and relating it to other
research fields like domain adaptation and transfer learning. Second, we
conduct a thorough review into existing methods and present a categorization
based on their methodologies and motivations. Finally, we conclude this survey
with insights and discussions on future research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kaiyang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tao Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Training of Social Media Image Classification Models for Rapid Disaster Response. (arXiv:2104.04184v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04184</id>
        <link href="http://arxiv.org/abs/2104.04184"/>
        <updated>2021-07-20T02:04:46.578Z</updated>
        <summary type="html"><![CDATA[Images shared on social media help crisis managers gain situational awareness
and assess incurred damages, among other response tasks. As the volume and
velocity of such content are typically high, real-time image classification has
become an urgent need for a faster disaster response. Recent advances in
computer vision and deep neural networks have enabled the development of models
for real-time image classification for a number of tasks, including detecting
crisis incidents, filtering irrelevant images, classifying images into specific
humanitarian categories, and assessing the severity of the damage. To develop
robust real-time models, it is necessary to understand the capability of the
publicly available pre-trained models for these tasks, which remains to be
under-explored in the crisis informatics literature. In this study, we address
such limitations by investigating ten different network architectures for four
different tasks using the largest publicly available datasets for these tasks.
We also explore various data augmentation strategies, semi-supervised
techniques, and a multitask learning setup. In our extensive experiments, we
achieve promising results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1"&gt;Firoj Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1"&gt;Tanvirul Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1"&gt;Muhammad Imran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1"&gt;Ferda Ofli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shapes as Product Differentiation: Neural Network Embedding in the Analysis of Markets for Fonts. (arXiv:2107.02739v1 [econ.EM] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.02739</id>
        <link href="http://arxiv.org/abs/2107.02739"/>
        <updated>2021-07-20T02:04:46.577Z</updated>
        <summary type="html"><![CDATA[Many differentiated products have key attributes that are unstructured and
thus high-dimensional (e.g., design, text). Instead of treating unstructured
attributes as unobservables in economic models, quantifying them can be
important to answer interesting economic questions. To propose an analytical
framework for this type of products, this paper considers one of the simplest
design products -- fonts -- and investigates merger and product differentiation
using an original dataset from the world's largest online marketplace for
fonts. We quantify font shapes by constructing embeddings from a deep
convolutional neural network. Each embedding maps a font's shape onto a
low-dimensional vector. In the resulting product space, designers are assumed
to engage in Hotelling-type spatial competition. From the image embeddings, we
construct two alternative measures that capture the degree of design
differentiation. We then study the causal effects of a merger on the merging
firm's creative decisions using the constructed measures in a synthetic control
method. We find that the merger causes the merging firm to increase the visual
variety of font design. Notably, such effects are not captured when using
traditional measures for product offerings (e.g., specifications and the number
of products) constructed from structured data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Han_S/0/1/0/all/0/1"&gt;Sukjin Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Schulman_E/0/1/0/all/0/1"&gt;Eric H. Schulman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Ramakrishnan_S/0/1/0/all/0/1"&gt;Santhosh Ramakrishnan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Similarity-Aware Fusion Network for 3D Semantic Segmentation. (arXiv:2107.01579v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01579</id>
        <link href="http://arxiv.org/abs/2107.01579"/>
        <updated>2021-07-20T02:04:46.576Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a similarity-aware fusion network (SAFNet) to
adaptively fuse 2D images and 3D point clouds for 3D semantic segmentation.
Existing fusion-based methods achieve remarkable performances by integrating
information from multiple modalities. However, they heavily rely on the
correspondence between 2D pixels and 3D points by projection and can only
perform the information fusion in a fixed manner, and thus their performances
cannot be easily migrated to a more realistic scenario where the collected data
often lack strict pair-wise features for prediction. To address this, we employ
a late fusion strategy where we first learn the geometric and contextual
similarities between the input and back-projected (from 2D pixels) point clouds
and utilize them to guide the fusion of two modalities to further exploit
complementary information. Specifically, we employ a geometric similarity
module (GSM) to directly compare the spatial coordinate distributions of
pair-wise 3D neighborhoods, and a contextual similarity module (CSM) to
aggregate and compare spatial contextual information of corresponding central
points. The two proposed modules can effectively measure how much image
features can help predictions, enabling the network to adaptively adjust the
contributions of two modalities to the final prediction of each point.
Experimental results on the ScanNetV2 benchmark demonstrate that SAFNet
significantly outperforms existing state-of-the-art fusion-based approaches
across various data integrity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Linqing Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Experimental Investigation and Evaluation of Model-based Hyperparameter Optimization. (arXiv:2107.08761v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08761</id>
        <link href="http://arxiv.org/abs/2107.08761"/>
        <updated>2021-07-20T02:04:46.575Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms such as random forests or xgboost are gaining
more importance and are increasingly incorporated into production processes in
order to enable comprehensive digitization and, if possible, automation of
processes. Hyperparameters of these algorithms used have to be set
appropriately, which can be referred to as hyperparameter tuning or
optimization. Based on the concept of tunability, this article presents an
overview of theoretical and practical results for popular machine learning
algorithms. This overview is accompanied by an experimental analysis of 30
hyperparameters from six relevant machine learning algorithms. In particular,
it provides (i) a survey of important hyperparameters, (ii) two parameter
tuning studies, and (iii) one extensive global parameter tuning study, as well
as (iv) a new way, based on consensus ranking, to analyze results from multiple
algorithms. The R package mlr is used as a uniform interface to the machine
learning models. The R package SPOT is used to perform the actual tuning
(optimization). All additional code is provided together with this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bartz_E/0/1/0/all/0/1"&gt;Eva Bartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaefferer_M/0/1/0/all/0/1"&gt;Martin Zaefferer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mersmann_O/0/1/0/all/0/1"&gt;Olaf Mersmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1"&gt;Thomas Bartz-Beielstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08751</id>
        <link href="http://arxiv.org/abs/2107.08751"/>
        <updated>2021-07-20T02:04:46.574Z</updated>
        <summary type="html"><![CDATA[Deep learning for medical imaging suffers from temporal and privacy-related
restrictions on data availability. To still obtain viable models, continual
learning aims to train in sequential order, as and when data is available. The
main challenge that continual learning methods face is to prevent catastrophic
forgetting, i.e., a decrease in performance on the data encountered earlier.
This issue makes continuous training of segmentation models for medical
applications extremely difficult. Yet, often, data from at least two different
domains is available which we can exploit to train the model in a way that it
disregards domain-specific information. We propose an architecture that
leverages the simultaneous availability of two or more datasets to learn a
disentanglement between the content and domain in an adversarial fashion. The
domain-invariant content representation then lays the base for continual
semantic segmentation. Our approach takes inspiration from domain adaptation
and combines it with continual learning for hippocampal segmentation in brain
MRI. We showcase that our method reduces catastrophic forgetting and
outperforms state-of-the-art continual learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1"&gt;Marius Memmel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1"&gt;Camila Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1"&gt;Anirban Mukhopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Drawing and Sketching. (arXiv:2103.16194v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16194</id>
        <link href="http://arxiv.org/abs/2103.16194"/>
        <updated>2021-07-20T02:04:46.572Z</updated>
        <summary type="html"><![CDATA[We present a bottom-up differentiable relaxation of the process of drawing
points, lines and curves into a pixel raster. Our approach arises from the
observation that rasterising a pixel in an image given parameters of a
primitive can be reformulated in terms of the primitive's distance transform,
and then relaxed to allow the primitive's parameters to be learned. This
relaxation allows end-to-end differentiable programs and deep networks to be
learned and optimised and provides several building blocks that allow control
over how a compositional drawing process is modelled. We emphasise the
bottom-up nature of our proposed approach, which allows for drawing operations
to be composed in ways that can mimic the physical reality of drawing rather
than being tied to, for example, approaches in modern computer graphics. With
the proposed approach we demonstrate how sketches can be generated by directly
optimising against photographs and how auto-encoders can be built to transform
rasterised handwritten digits into vectors without supervision. Extensive
experimental results highlight the power of this approach under different
modelling assumptions for drawing tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mihai_D/0/1/0/all/0/1"&gt;Daniela Mihai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12056</id>
        <link href="http://arxiv.org/abs/2102.12056"/>
        <updated>2021-07-20T02:04:46.561Z</updated>
        <summary type="html"><![CDATA[Liver segmentation from abdominal CT images is an essential step for liver
cancer computer-aided diagnosis and surgical planning. However, both the
accuracy and robustness of existing liver segmentation methods cannot meet the
requirements of clinical applications. In particular, for the common clinical
cases where the liver tissue contains major pathology, current segmentation
methods show poor performance. In this paper, we propose a novel low-rank
tensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that
achieves accurate and robust pathological liver segmentation of CT images.
Firstly, we propose a multi-slice LRTD scheme to recover the underlying
low-rank structure embedded in 3D medical images. It performs the LRTD on small
image segments consisting of multiple consecutive image slices. Then, we
present an LRTD-based atlas construction method to generate tumor-free liver
atlases that mitigates the performance degradation of liver segmentation due to
the presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to
derive patient-specific liver atlases for each test image, and to achieve
accurate pairwise image registration and label propagation. Extensive
experiments on three public databases of pathological liver cases validate the
effectiveness of the proposed method. Both qualitative and quantitative results
demonstrate that, in the presence of major pathology, the proposed method is
more accurate and robust than state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1"&gt;Min Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiancheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Heng-Da Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Motion Prediction Using Manifold-Aware Wasserstein GAN. (arXiv:2105.08715v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08715</id>
        <link href="http://arxiv.org/abs/2105.08715"/>
        <updated>2021-07-20T02:04:46.557Z</updated>
        <summary type="html"><![CDATA[Human motion prediction aims to forecast future human poses given a prior
pose sequence. The discontinuity of the predicted motion and the performance
deterioration in long-term horizons are still the main challenges encountered
in current literature. In this work, we tackle these issues by using a compact
manifold-valued representation of human motion. Specifically, we model the
temporal evolution of the 3D human poses as trajectory, what allows us to map
human motions to single points on a sphere manifold. To learn these
non-Euclidean representations, we build a manifold-aware Wasserstein generative
adversarial model that captures the temporal and spatial dependencies of human
motion through different losses. Extensive experiments show that our approach
outperforms the state-of-the-art on CMU MoCap and Human 3.6M datasets. Our
qualitative results show the smoothness of the predicted motions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chopin_B/0/1/0/all/0/1"&gt;Baptiste Chopin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otberdout_N/0/1/0/all/0/1"&gt;Naima Otberdout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daoudi_M/0/1/0/all/0/1"&gt;Mohamed Daoudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartolo_A/0/1/0/all/0/1"&gt;Angela Bartolo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self Training with Ensemble of Teacher Models. (arXiv:2107.08211v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08211</id>
        <link href="http://arxiv.org/abs/2107.08211"/>
        <updated>2021-07-20T02:04:46.557Z</updated>
        <summary type="html"><![CDATA[In order to train robust deep learning models, large amounts of labelled data
is required. However, in the absence of such large repositories of labelled
data, unlabeled data can be exploited for the same. Semi-Supervised learning
aims to utilize such unlabeled data for training classification models. Recent
progress of self-training based approaches have shown promise in this area,
which leads to this study where we utilize an ensemble approach for the same. A
by-product of any semi-supervised approach may be loss of calibration of the
trained model especially in scenarios where unlabeled data may contain
out-of-distribution samples, which leads to this investigation on how to adapt
to such effects. Our proposed algorithm carefully avoids common pitfalls in
utilizing unlabeled data and leads to a more accurate and calibrated supervised
model compared to vanilla self-training based student-teacher algorithms. We
perform several experiments on the popular STL-10 database followed by an
extensive analysis of our approach and study its effects on model accuracy and
calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Soumyadeep Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjay Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_J/0/1/0/all/0/1"&gt;Janu Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Awanish Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cell Detection in Domain Shift Problem Using Pseudo-Cell-Position Heatmap. (arXiv:2107.08653v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08653</id>
        <link href="http://arxiv.org/abs/2107.08653"/>
        <updated>2021-07-20T02:04:46.488Z</updated>
        <summary type="html"><![CDATA[The domain shift problem is an important issue in automatic cell detection. A
detection network trained with training data under a specific condition (source
domain) may not work well in data under other conditions (target domain). We
propose an unsupervised domain adaptation method for cell detection using the
pseudo-cell-position heatmap, where a cell centroid becomes a peak with a
Gaussian distribution in the map. In the prediction result for the target
domain, even if a peak location is correct, the signal distribution around the
peak often has anon-Gaussian shape. The pseudo-cell-position heatmap is
re-generated using the peak positions in the predicted heatmap to have a clear
Gaussian shape. Our method selects confident pseudo-cell-position heatmaps
using a Bayesian network and adds them to the training data in the next
iteration. The method can incrementally extend the domain from the source
domain to the target domain in a semi-supervised manner. In the experiments
using 8 combinations of domains, the proposed method outperformed the existing
domain adaptation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hyeonwoo Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishimura_K/0/1/0/all/0/1"&gt;Kazuya Nishimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_K/0/1/0/all/0/1"&gt;Kazuhide Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bise_R/0/1/0/all/0/1"&gt;Ryoma Bise&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Weighted Difference of Anisotropic and Isotropic Total Variation for Relaxed Mumford-Shah Color and Multiphase Image Segmentation. (arXiv:2005.04401v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.04401</id>
        <link href="http://arxiv.org/abs/2005.04401"/>
        <updated>2021-07-20T02:04:46.483Z</updated>
        <summary type="html"><![CDATA[In a class of piecewise-constant image segmentation models, we propose to
incorporate a weighted difference of anisotropic and isotropic total variation
(AITV) to regularize the partition boundaries in an image. In particular, we
replace the total variation regularization in the Chan-Vese segmentation model
and a fuzzy region competition model by the proposed AITV. To deal with the
nonconvex nature of AITV, we apply the difference-of-convex algorithm (DCA), in
which the subproblems can be minimized by the primal-dual hybrid gradient
method with linesearch. The convergence of the DCA scheme is analyzed. In
addition, a generalization to color image segmentation is discussed. In the
numerical experiments, we compare the proposed models with the classic convex
approaches and the two-stage segmentation methods (smoothing and then
thresholding) on various images, showing that our models are effective in image
segmentation and robust with respect to impulsive noises.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bui_K/0/1/0/all/0/1"&gt;Kevin Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1"&gt;Fredrick Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1"&gt;Yifei Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Jack Xin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video captioning with stacked attention and semantic hard pull. (arXiv:2009.07335v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07335</id>
        <link href="http://arxiv.org/abs/2009.07335"/>
        <updated>2021-07-20T02:04:46.482Z</updated>
        <summary type="html"><![CDATA[Video captioning, i.e. the task of generating captions from video sequences
creates a bridge between the Natural Language Processing and Computer Vision
domains of computer science. The task of generating a semantically accurate
description of a video is quite complex. Considering the complexity, of the
problem, the results obtained in recent research works are praiseworthy.
However, there is plenty of scope for further investigation. This paper
addresses this scope and proposes a novel solution. Most video captioning
models comprise two sequential/recurrent layers - one as a video-to-context
encoder and the other as a context-to-caption decoder. This paper proposes a
novel architecture, namely Semantically Sensible Video Captioning (SSVC) which
modifies the context generation mechanism by using two novel approaches -
"stacked attention" and "spatial hard pull". As there are no exclusive metrics
for evaluating video captioning models, we emphasize both quantitative and
qualitative analysis of our model. Hence, we have used the BLEU scoring metric
for quantitative analysis and have proposed a human evaluation metric for
qualitative analysis, namely the Semantic Sensibility (SS) scoring metric. SS
Score overcomes the shortcomings of common automated scoring metrics. This
paper reports that the use of the aforementioned novelties improves the
performance of state-of-the-art architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Md. Mushfiqur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abedin_T/0/1/0/all/0/1"&gt;Thasin Abedin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prottoy_K/0/1/0/all/0/1"&gt;Khondokar S. S. Prottoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moshruba_A/0/1/0/all/0/1"&gt;Ayana Moshruba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddiqui_F/0/1/0/all/0/1"&gt;Fazlul Hasan Siddiqui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UNIK: A Unified Framework for Real-world Skeleton-based Action Recognition. (arXiv:2107.08580v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08580</id>
        <link href="http://arxiv.org/abs/2107.08580"/>
        <updated>2021-07-20T02:04:46.481Z</updated>
        <summary type="html"><![CDATA[Action recognition based on skeleton data has recently witnessed increasing
attention and progress. State-of-the-art approaches adopting Graph
Convolutional networks (GCNs) can effectively extract features on human
skeletons relying on the pre-defined human topology. Despite associated
progress, GCN-based methods have difficulties to generalize across domains,
especially with different human topological structures. In this context, we
introduce UNIK, a novel skeleton-based action recognition method that is not
only effective to learn spatio-temporal features on human skeleton sequences
but also able to generalize across datasets. This is achieved by learning an
optimal dependency matrix from the uniform distribution based on a multi-head
attention mechanism. Subsequently, to study the cross-domain generalizability
of skeleton-based action recognition in real-world videos, we re-evaluate
state-of-the-art approaches as well as the proposed UNIK in light of a novel
Posetics dataset. This dataset is created from Kinetics-400 videos by
estimating, refining and filtering poses. We provide an analysis on how much
performance improves on smaller benchmark datasets after pre-training on
Posetics for the action classification task. Experimental results show that the
proposed UNIK, with pre-training on Posetics, generalizes well and outperforms
state-of-the-art when transferred onto four target action classification
datasets: Toyota Smarthome, Penn Action, NTU-RGB+D 60 and NTU-RGB+D 120.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Di Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaohui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dantcheva_A/0/1/0/all/0/1"&gt;Antitza Dantcheva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garattoni_L/0/1/0/all/0/1"&gt;Lorenzo Garattoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1"&gt;Gianpiero Francesca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1"&gt;Francois Bremond&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CORAL: Colored structural representation for bi-modal place recognition. (arXiv:2011.10934v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10934</id>
        <link href="http://arxiv.org/abs/2011.10934"/>
        <updated>2021-07-20T02:04:46.481Z</updated>
        <summary type="html"><![CDATA[Place recognition is indispensable for a drift-free localization system. Due
to the variations of the environment, place recognition using single-modality
has limitations. In this paper, we propose a bi-modal place recognition method,
which can extract a compound global descriptor from the two modalities, vision
and LiDAR. Specifically, we first build the elevation image generated from 3D
points as a structural representation. Then, we derive the correspondences
between 3D points and image pixels that are further used in merging the
pixel-wise visual features into the elevation map grids. In this way, we fuse
the structural features and visual features in the consistent bird-eye view
frame, yielding a semantic representation, namely CORAL. And the whole network
is called CORAL-VLAD. Comparisons on the Oxford RobotCar show that CORAL-VLAD
has superior performance against other state-of-the-art methods. We also
demonstrate that our network can be generalized to other scenes and sensor
configurations on cross-city datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yiyuan Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xuecheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weijie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yunxiang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1"&gt;Rong Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LeViT-UNet: Make Faster Encoders with Transformer for Medical Image Segmentation. (arXiv:2107.08623v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08623</id>
        <link href="http://arxiv.org/abs/2107.08623"/>
        <updated>2021-07-20T02:04:46.480Z</updated>
        <summary type="html"><![CDATA[Medical image segmentation plays an essential role in developing
computer-assisted diagnosis and therapy systems, yet still faces many
challenges. In the past few years, the popular encoder-decoder architectures
based on CNNs (e.g., U-Net) have been successfully applied in the task of
medical image segmentation. However, due to the locality of convolution
operations, they demonstrate limitations in learning global context and
long-range spatial relations. Recently, several researchers try to introduce
transformers to both the encoder and decoder components with promising results,
but the efficiency requires further improvement due to the high computational
complexity of transformers. In this paper, we propose LeViT-UNet, which
integrates a LeViT Transformer module into the U-Net architecture, for fast and
accurate medical image segmentation. Specifically, we use LeViT as the encoder
of the LeViT-UNet, which better trades off the accuracy and efficiency of the
Transformer block. Moreover, multi-scale feature maps from transformer blocks
and convolutional blocks of LeViT are passed into the decoder via
skip-connection, which can effectively reuse the spatial information of the
feature maps. Our experiments indicate that the proposed LeViT-UNet achieves
better performance comparing to various competing methods on several
challenging medical image segmentation benchmarks including Synapse and ACDC.
Code and models will be publicly available at
https://github.com/apple1986/LeViT_UNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guoping Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xingrong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xinwei He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compound Figure Separation of Biomedical Images with Side Loss. (arXiv:2107.08650v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08650</id>
        <link href="http://arxiv.org/abs/2107.08650"/>
        <updated>2021-07-20T02:04:46.480Z</updated>
        <summary type="html"><![CDATA[Unsupervised learning algorithms (e.g., self-supervised learning,
auto-encoder, contrastive learning) allow deep learning models to learn
effective image representations from large-scale unlabeled data. In medical
image analysis, even unannotated data can be difficult to obtain for individual
labs. Fortunately, national-level efforts have been made to provide efficient
access to obtain biomedical image data from previous scientific publications.
For instance, NIH has launched the Open-i search engine that provides a
large-scale image database with free access. However, the images in scientific
publications consist of a considerable amount of compound figures with
subplots. To extract and curate individual subplots, many different compound
figure separation approaches have been developed, especially with the recent
advances in deep learning. However, previous approaches typically required
resource extensive bounding box annotation to train detection models. In this
paper, we propose a simple compound figure separation (SimCFS) framework that
uses weak classification annotations from individual images. Our technical
contribution is three-fold: (1) we introduce a new side loss that is designed
for compound figure separation; (2) we introduce an intra-class image
augmentation method to simulate hard cases; (3) the proposed framework enables
an efficient deployment to new classes of images, without requiring resource
extensive bounding box annotations. From the results, the SimCFS achieved a new
state-of-the-art performance on the ImageCLEF 2016 Compound Figure Separation
Database. The source code of SimCFS is made publicly available at
https://github.com/hrlblab/ImageSeperation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Tianyuan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1"&gt;Chang Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1"&gt;Ruining Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuanhan Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiachen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1"&gt;Aadarsh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1"&gt;Shunxing Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mengyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fogo_A/0/1/0/all/0/1"&gt;Agnes B. Fogo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1"&gt;Bennett A.Landman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Catie Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haichun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuankai Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Action Forecasting with Feature-wise Self-Attention. (arXiv:2107.08579v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08579</id>
        <link href="http://arxiv.org/abs/2107.08579"/>
        <updated>2021-07-20T02:04:46.479Z</updated>
        <summary type="html"><![CDATA[We present a new architecture for human action forecasting from videos. A
temporal recurrent encoder captures temporal information of input videos while
a self-attention model is used to attend on relevant feature dimensions of the
input space. To handle temporal variations in observed video data, a feature
masking techniques is employed. We classify observed actions accurately using
an auxiliary classifier which helps to understand what has happened so far.
Then the decoder generates actions for the future based on the output of the
recurrent encoder and the self-attention model. Experimentally, we validate
each component of our architecture where we see that the impact of
self-attention to identify relevant feature dimensions, temporal masking, and
observed auxiliary classifier. We evaluate our method on two standard action
forecasting benchmarks and obtain state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1"&gt;Yan Bin Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1"&gt;Basura Fernando&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ANFIC: Image Compression Using Augmented Normalizing Flows. (arXiv:2107.08470v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08470</id>
        <link href="http://arxiv.org/abs/2107.08470"/>
        <updated>2021-07-20T02:04:46.478Z</updated>
        <summary type="html"><![CDATA[This paper introduces an end-to-end learned image compression system, termed
ANFIC, based on Augmented Normalizing Flows (ANF). ANF is a new type of flow
model, which stacks multiple variational autoencoders (VAE) for greater model
expressiveness. The VAE-based image compression has gone mainstream, showing
promising compression performance. Our work presents the first attempt to
leverage VAE-based compression in a flow-based framework. ANFIC advances
further compression efficiency by stacking and extending hierarchically
multiple VAE's. The invertibility of ANF, together with our training
strategies, enables ANFIC to support a wide range of quality levels without
changing the encoding and decoding networks. Extensive experimental results
show that in terms of PSNR-RGB, ANFIC performs comparably to or better than the
state-of-the-art learned image compression. Moreover, it performs close to VVC
intra coding, from low-rate compression up to nearly-lossless compression. In
particular, ANFIC achieves the state-of-the-art performance, when extended with
conditional convolution for variable rate compression with a single model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ho_Y/0/1/0/all/0/1"&gt;Yung-Han Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1"&gt;Chih-Chun Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wen-Hsiao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hang_H/0/1/0/all/0/1"&gt;Hsueh-Ming Hang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Domanski_M/0/1/0/all/0/1"&gt;Marek Domanski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeHumor: Visual Analytics for Decomposing Humor. (arXiv:2107.08356v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08356</id>
        <link href="http://arxiv.org/abs/2107.08356"/>
        <updated>2021-07-20T02:04:46.339Z</updated>
        <summary type="html"><![CDATA[Despite being a critical communication skill, grasping humor is challenging
-- a successful use of humor requires a mixture of both engaging content
build-up and an appropriate vocal delivery (e.g., pause). Prior studies on
computational humor emphasize the textual and audio features immediately next
to the punchline, yet overlooking longer-term context setup. Moreover, the
theories are usually too abstract for understanding each concrete humor
snippet. To fill in the gap, we develop DeHumor, a visual analytical system for
analyzing humorous behaviors in public speaking. To intuitively reveal the
building blocks of each concrete example, DeHumor decomposes each humorous
video into multimodal features and provides inline annotations of them on the
video script. In particular, to better capture the build-ups, we introduce
content repetition as a complement to features introduced in theories of
computational humor and visualize them in a context linking graph. To help
users locate the punchlines that have the desired features to learn, we
summarize the content (with keywords) and humor feature statistics on an
augmented time matrix. With case studies on stand-up comedy shows and TED
talks, we show that DeHumor is able to highlight various building blocks of
humor examples. In addition, expert interviews with communication coaches and
humor researchers demonstrate the effectiveness of DeHumor for multimodal humor
analysis of speech content and vocal delivery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1"&gt;Yao Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tongshuang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Haipeng Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoder blind combinatorial compressed sensing. (arXiv:2004.05094v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.05094</id>
        <link href="http://arxiv.org/abs/2004.05094"/>
        <updated>2021-07-20T02:04:46.339Z</updated>
        <summary type="html"><![CDATA[In its most elementary form, compressed sensing studies the design of
decoding algorithms to recover a sufficiently sparse vector or code from a
lower dimensional linear measurement vector. Typically it is assumed that the
decoder has access to the encoder matrix, which in the combinatorial case is
sparse and binary. In this paper we consider the problem of designing a decoder
to recover a set of sparse codes from their linear measurements alone, that is
without access to encoder matrix. To this end we study the matrix factorisation
task of recovering both the encoder and sparse coding matrices from the
associated linear measurement matrix. The contribution of this paper is a
computationally efficient decoding algorithm, Decoder-Expander Based
Factorisation, with strong performance guarantees. In particular, under mild
assumptions on the sparse coding matrix and by deploying a novel random encoder
matrix, we prove that Decoder-Expander Based Factorisation recovers both the
encoder and sparse coding matrix at the optimal measurement rate with high
probability and from a near optimal number of measurement vectors. In addition,
our experiments demonstrate the efficacy and computational efficiency of our
algorithm in practice. Beyond compressed sensing our results may be of interest
for researchers working in areas such as linear sketching, coding theory and
matrix compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murray_M/0/1/0/all/0/1"&gt;Michael Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanner_J/0/1/0/all/0/1"&gt;Jared Tanner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top-label calibration. (arXiv:2107.08353v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08353</id>
        <link href="http://arxiv.org/abs/2107.08353"/>
        <updated>2021-07-20T02:04:46.338Z</updated>
        <summary type="html"><![CDATA[We study the problem of post-hoc calibration for multiclass classification,
with an emphasis on histogram binning. Multiple works have focused on
calibration with respect to the confidence of just the predicted class (or
'top-label'). We find that the popular notion of confidence calibration [Guo et
al., 2017] is not sufficiently strong -- there exist predictors that are not
calibrated in any meaningful way but are perfectly confidence calibrated. We
propose a closely related (but subtly different) notion, top-label calibration,
that accurately captures the intuition and simplicity of confidence
calibration, but addresses its drawbacks. We formalize a histogram binning (HB)
algorithm that reduces top-label multiclass calibration to the binary case,
prove that it has clean theoretical guarantees without distributional
assumptions, and perform a methodical study of its practical performance. Some
prediction tasks require stricter notions of multiclass calibration such as
class-wise or canonical calibration. We formalize appropriate HB algorithms
corresponding to each of these goals. In experiments with deep neural nets, we
find that our principled versions of HB are often better than temperature
scaling, for both top-label and class-wise calibration. Code for this work will
be made publicly available at https://github.com/aigen/df-posthoc-calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1"&gt;Chirag Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya K. Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving high-dimensional parabolic PDEs using the tensor train format. (arXiv:2102.11830v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11830</id>
        <link href="http://arxiv.org/abs/2102.11830"/>
        <updated>2021-07-20T02:04:46.338Z</updated>
        <summary type="html"><![CDATA[High-dimensional partial differential equations (PDEs) are ubiquitous in
economics, science and engineering. However, their numerical treatment poses
formidable challenges since traditional grid-based methods tend to be
frustrated by the curse of dimensionality. In this paper, we argue that tensor
trains provide an appealing approximation framework for parabolic PDEs: the
combination of reformulations in terms of backward stochastic differential
equations and regression-type methods in the tensor format holds the promise of
leveraging latent low-rank structures enabling both compression and efficient
computation. Following this paradigm, we develop novel iterative schemes,
involving either explicit and fast or implicit and accurate updates. We
demonstrate in a number of examples that our methods achieve a favorable
trade-off between accuracy and computational efficiency in comparison with
state-of-the-art neural network based approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Richter_L/0/1/0/all/0/1"&gt;Lorenz Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sallandt_L/0/1/0/all/0/1"&gt;Leon Sallandt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nusken_N/0/1/0/all/0/1"&gt;Nikolas N&amp;#xfc;sken&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12056</id>
        <link href="http://arxiv.org/abs/2102.12056"/>
        <updated>2021-07-20T02:04:46.337Z</updated>
        <summary type="html"><![CDATA[Liver segmentation from abdominal CT images is an essential step for liver
cancer computer-aided diagnosis and surgical planning. However, both the
accuracy and robustness of existing liver segmentation methods cannot meet the
requirements of clinical applications. In particular, for the common clinical
cases where the liver tissue contains major pathology, current segmentation
methods show poor performance. In this paper, we propose a novel low-rank
tensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that
achieves accurate and robust pathological liver segmentation of CT images.
Firstly, we propose a multi-slice LRTD scheme to recover the underlying
low-rank structure embedded in 3D medical images. It performs the LRTD on small
image segments consisting of multiple consecutive image slices. Then, we
present an LRTD-based atlas construction method to generate tumor-free liver
atlases that mitigates the performance degradation of liver segmentation due to
the presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to
derive patient-specific liver atlases for each test image, and to achieve
accurate pairwise image registration and label propagation. Extensive
experiments on three public databases of pathological liver cases validate the
effectiveness of the proposed method. Both qualitative and quantitative results
demonstrate that, in the presence of major pathology, the proposed method is
more accurate and robust than state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1"&gt;Min Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiancheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Heng-Da Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing a Family of Synthetic Datasets for Research on Bias in Machine Learning. (arXiv:2107.08928v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08928</id>
        <link href="http://arxiv.org/abs/2107.08928"/>
        <updated>2021-07-20T02:04:46.336Z</updated>
        <summary type="html"><![CDATA[A significant impediment to progress in research on bias in machine learning
(ML) is the availability of relevant datasets. This situation is unlikely to
change much given the sensitivity of such data. For this reason, there is a
role for synthetic data in this research. In this short paper, we present one
such family of synthetic data sets. We provide an overview of the data,
describe how the level of bias can be varied, and present a simple example of
an experiment on the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blanzeisky_W/0/1/0/all/0/1"&gt;William Blanzeisky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cunningham_P/0/1/0/all/0/1"&gt;P&amp;#xe1;draig Cunningham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kennedy_K/0/1/0/all/0/1"&gt;Kenneth Kennedy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STRODE: Stochastic Boundary Ordinary Differential Equation. (arXiv:2107.08273v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08273</id>
        <link href="http://arxiv.org/abs/2107.08273"/>
        <updated>2021-07-20T02:04:46.247Z</updated>
        <summary type="html"><![CDATA[Perception of time from sequentially acquired sensory inputs is rooted in
everyday behaviors of individual organisms. Yet, most algorithms for
time-series modeling fail to learn dynamics of random event timings directly
from visual or audio inputs, requiring timing annotations during training that
are usually unavailable for real-world applications. For instance, neuroscience
perspectives on postdiction imply that there exist variable temporal ranges
within which the incoming sensory inputs can affect the earlier perception, but
such temporal ranges are mostly unannotated for real applications such as
automatic speech recognition (ASR). In this paper, we present a probabilistic
ordinary differential equation (ODE), called STochastic boundaRy ODE (STRODE),
that learns both the timings and the dynamics of time series data without
requiring any timing annotations during training. STRODE allows the usage of
differential equations to sample from the posterior point processes,
efficiently and analytically. We further provide theoretical guarantees on the
learning of STRODE. Our empirical results show that our approach successfully
infers event timings of time series data. Our method achieves competitive or
superior performances compared to existing state-of-the-art methods for both
synthetic and real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hengguan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongfu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chang Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Ye Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08264</id>
        <link href="http://arxiv.org/abs/2107.08264"/>
        <updated>2021-07-20T02:04:46.210Z</updated>
        <summary type="html"><![CDATA[Multimodal sentiment analysis aims to recognize people's attitudes from
multiple communication channels such as verbal content (i.e., text), voice, and
facial expressions. It has become a vibrant and important research topic in
natural language processing. Much research focuses on modeling the complex
intra- and inter-modal interactions between different communication channels.
However, current multimodal models with strong performance are often
deep-learning-based techniques and work like black boxes. It is not clear how
models utilize multimodal information for sentiment predictions. Despite recent
advances in techniques for enhancing the explainability of machine learning
models, they often target unimodal scenarios (e.g., images, sentences), and
little research has been done on explaining multimodal models. In this paper,
we present an interactive visual analytics system, M2Lens, to visualize and
explain multimodal models for sentiment analysis. M2Lens provides explanations
on intra- and inter-modal interactions at the global, subset, and local levels.
Specifically, it summarizes the influence of three typical interaction types
(i.e., dominance, complement, and conflict) on the model predictions. Moreover,
M2Lens identifies frequent and influential multimodal features and supports the
multi-faceted exploration of model behaviors from language, acoustic, and
visual modalities. Through two case studies and expert interviews, we
demonstrate our system can help users gain deep insights into the multimodal
models for sentiment analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianben He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhihua Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Balance: Mitigating Machine Learning Bias Against Multiple Protected Attributes With Data Balancing. (arXiv:2107.08310v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08310</id>
        <link href="http://arxiv.org/abs/2107.08310"/>
        <updated>2021-07-20T02:04:46.210Z</updated>
        <summary type="html"><![CDATA[This paper aims to improve machine learning fairness on multiple protected
at-tributes. Machine learning fairness has attracted increasing attention since
machine learning models are increasingly used for high-stakes and high-risk
decisions. Most existing solutions for machine learning fairness only target
one protected attribute(e.g. sex) at a time. These solutions cannot generate a
machine learning model which is fair against every protected attribute (e.g.
both sex and race) at the same time. To solve this problem, we propose
FairBalance in this paper to balance the distribution of training data across
every protected attribute before training the machine learning models. Our
results show that, under the assumption of unbiased ground truth labels,
FairBalance can significantly reduce bias metrics (AOD, EOD, and SPD) on every
known protected attribute without much, if not any damage to the prediction
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhe Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedH2L: Federated Learning with Model and Statistical Heterogeneity. (arXiv:2101.11296v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11296</id>
        <link href="http://arxiv.org/abs/2101.11296"/>
        <updated>2021-07-20T02:04:46.209Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) enables distributed participants to collectively
learn a strong global model without sacrificing their individual data privacy.
Mainstream FL approaches require each participant to share a common network
architecture and further assume that data are are sampled IID across
participants. However, in real-world deployments participants may require
heterogeneous network architectures; and the data distribution is almost
certainly non-uniform across participants. To address these issues we introduce
FedH2L, which is agnostic to both the model architecture and robust to
different data distributions across participants. In contrast to approaches
sharing parameters or gradients, FedH2L relies on mutual distillation,
exchanging only posteriors on a shared seed set between participants in a
decentralized manner. This makes it extremely bandwidth efficient, model
agnostic, and crucially produces models capable of performing well on the whole
data distribution when learning from heterogeneous silos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huaimin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1"&gt;Haibo Mi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1"&gt;Timothy M. Hospedales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation. (arXiv:2006.06979v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06979</id>
        <link href="http://arxiv.org/abs/2006.06979"/>
        <updated>2021-07-20T02:04:46.206Z</updated>
        <summary type="html"><![CDATA[Density ratio estimation (DRE) is at the core of various machine learning
tasks such as anomaly detection and domain adaptation. In existing studies on
DRE, methods based on Bregman divergence (BD) minimization have been
extensively studied. However, BD minimization when applied with highly flexible
models, such as deep neural networks, tends to suffer from what we call
train-loss hacking, which is a source of overfitting caused by a typical
characteristic of empirical BD estimators. In this paper, to mitigate
train-loss hacking, we propose a non-negative correction for empirical BD
estimators. Theoretically, we confirm the soundness of the proposed method
through a generalization error bound. Through our experiments, the proposed
methods show a favorable performance in inlier-based outlier detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1"&gt;Masahiro Kato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teshima_T/0/1/0/all/0/1"&gt;Takeshi Teshima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02443</id>
        <link href="http://arxiv.org/abs/2007.02443"/>
        <updated>2021-07-20T02:04:46.205Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting (CF) happens whenever a neural network overwrites
past knowledge while being trained on new tasks. Common techniques to handle CF
include regularization of the weights (using, e.g., their importance on past
tasks), and rehearsal strategies, where the network is constantly re-trained on
past data. Generative models have also been applied for the latter, in order to
have endless sources of data. In this paper, we propose a novel method that
combines the strengths of regularization and generative-based rehearsal
approaches. Our generative model consists of a normalizing flow (NF), a
probabilistic and invertible neural network, trained on the internal embeddings
of the network. By keeping a single NF conditioned on the task, we show that
our memory overhead remains constant. In addition, exploiting the invertibility
of the NF, we propose a simple approach to regularize the network's embeddings
with respect to past tasks. We show that our method performs favorably with
respect to state-of-the-art approaches in the literature, with bounded
computational power and memory overheads.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1"&gt;Jary Pomponi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1"&gt;Simone Scardapane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1"&gt;Aurelio Uncini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scene Graph to Image Generation with Contextualized Object Layout Refinement. (arXiv:2009.10939v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.10939</id>
        <link href="http://arxiv.org/abs/2009.10939"/>
        <updated>2021-07-20T02:04:46.127Z</updated>
        <summary type="html"><![CDATA[Generating images from scene graphs is a challenging task that attracted
substantial interest recently. Prior works have approached this task by
generating an intermediate layout description of the target image. However, the
representation of each object in the layout was generated independently, which
resulted in high overlap, low coverage, and an overall blurry layout. We
propose a novel method that alleviates these issues by generating the entire
layout description gradually to improve inter-object dependency. We empirically
show on the COCO-STUFF dataset that our approach improves the quality of both
the intermediate layout and the final image. Our approach improves the layout
coverage by almost 20 points and drops object overlap to negligible amounts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivgi_M/0/1/0/all/0/1"&gt;Maor Ivgi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benny_Y/0/1/0/all/0/1"&gt;Yaniv Benny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_David_A/0/1/0/all/0/1"&gt;Avichai Ben-David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1"&gt;Jonathan Berant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimising quantifier variance under prior probability shift. (arXiv:2107.08209v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.08209</id>
        <link href="http://arxiv.org/abs/2107.08209"/>
        <updated>2021-07-20T02:04:46.127Z</updated>
        <summary type="html"><![CDATA[For the binary prevalence quantification problem under prior probability
shift, we determine the asymptotic variance of the maximum likelihood
estimator. We find that it is a function of the Brier score for the regression
of the class label against the features under the test data set distribution.
This observation suggests that optimising the accuracy of a base classifier on
the training data set helps to reduce the variance of the related quantifier on
the test data set. Therefore, we also point out training criteria for the base
classifier that imply optimisation of both of the Brier scores on the training
and the test data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tasche_D/0/1/0/all/0/1"&gt;Dirk Tasche&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boost AI Power: Data Augmentation Strategies with unlabelled Data and Conformal Prediction, a Case in Alternative Herbal Medicine Discrimination with Electronic Nose. (arXiv:2102.03088v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03088</id>
        <link href="http://arxiv.org/abs/2102.03088"/>
        <updated>2021-07-20T02:04:46.115Z</updated>
        <summary type="html"><![CDATA[Electronic nose has been proven to be effective in alternative herbal
medicine classification, but due to the nature of supervised learning, previous
research heavily relies on the labelled training data, which are time-costly
and labor-intensive to collect. To alleviate the critical dependency on the
training data in real-world applications, this study aims to improve
classification accuracy via data augmentation strategies. The effectiveness of
five data augmentation strategies under different training data inadequacy are
investigated in two scenarios: the noise-free scenario where different
availabilities of unlabelled data were considered, and the noisy scenario where
different levels of Gaussian noises and translational shifts were added to
represent sensor drifts. The five augmentation strategies, namely noise-adding
data augmentation, semi-supervised learning, classifier-based online learning,
Inductive Conformal Prediction (ICP) online learning and our novel ensemble ICP
online learning proposed in this study, are experimented and compared against
supervised learning baseline, with Linear Discriminant Analysis (LDA) and
Support Vector Machine (SVM) as the classifiers. Our novel strategy, ensemble
ICP online learning, outperforms the others by showing non-decreasing
classification accuracy on all tasks and a significant improvement on most
simulated tasks (25out of 36 tasks,p<=0.05). Furthermore, this study provides a
systematic analysis of different augmentation strategies. It shows at least one
strategy significantly improved the classification accuracy with LDA (p<=0.05)
and non-decreasing classification accuracy with SVM in each task. In
particular, our proposed strategy demonstrated both effectiveness and
robustness in boosting the classification model generalizability, which can be
employed in other machine learning applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Li Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xianghao Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Rumeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1"&gt;Xiaoqing Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1"&gt;Mert Pilanci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;You Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhiyuan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Projection Robust Optimal Transport: Sample Complexity and Model Misspecification. (arXiv:2006.12301v5 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12301</id>
        <link href="http://arxiv.org/abs/2006.12301"/>
        <updated>2021-07-20T02:04:46.114Z</updated>
        <summary type="html"><![CDATA[Optimal transport (OT) distances are increasingly used as loss functions for
statistical inference, notably in the learning of generative models or
supervised learning. Yet, the behavior of minimum Wasserstein estimators is
poorly understood, notably in high-dimensional regimes or under model
misspecification. In this work we adopt the viewpoint of projection robust (PR)
OT, which seeks to maximize the OT cost between two measures by choosing a
$k$-dimensional subspace onto which they can be projected. Our first
contribution is to establish several fundamental statistical properties of PR
Wasserstein distances, complementing and improving previous literature that has
been restricted to one-dimensional and well-specified cases. Next, we propose
the integral PR Wasserstein (IPRW) distance as an alternative to the PRW
distance, by averaging rather than optimizing on subspaces. Our complexity
bounds can help explain why both PRW and IPRW distances outperform Wasserstein
distances empirically in high-dimensional inference tasks. Finally, we consider
parametric inference using the PRW distance. We provide an asymptotic guarantee
of two types of minimum PRW estimators and formulate a central limit theorem
for max-sliced Wasserstein estimator under model misspecification. To enable
our analysis on PRW with projection dimension larger than one, we devise a
novel combination of variational analysis and statistical theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zeyu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chen_E/0/1/0/all/0/1"&gt;Elynn Y. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Cuturi_M/0/1/0/all/0/1"&gt;Marco Cuturi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.08142</id>
        <link href="http://arxiv.org/abs/2107.08142"/>
        <updated>2021-07-20T02:04:46.113Z</updated>
        <summary type="html"><![CDATA[Despite the numerous successes of machine learning over the past decade
(image recognition, decision-making, NLP, image synthesis), self-driving
technology has not yet followed the same trend. In this paper, we study the
history, composition, and development bottlenecks of the modern self-driving
stack. We argue that the slow progress is caused by approaches that require too
much hand-engineering, an over-reliance on road testing, and high fleet
deployment costs. We observe that the classical stack has several bottlenecks
that preclude the necessary scale needed to capture the long tail of rare
events. To resolve these problems, we outline the principles of Autonomy 2.0,
an ML-first approach to self-driving, as a viable alternative to the currently
adopted state-of-the-art. This approach is based on (i) a fully differentiable
AV stack trainable from human demonstrations, (ii) closed-loop data-driven
reactive simulation, and (iii) large-scale, low-cost data collections as
critical solutions towards scalability issues. We outline the general
architecture, survey promising works in this direction and propose key
challenges to be addressed by the community in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Ashesh Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1"&gt;Luca Del Pero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1"&gt;Hugo Grimmett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1"&gt;Peter Ondruska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Uncertainty and Correctability for Directed Graphical Models. (arXiv:2107.08179v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.08179</id>
        <link href="http://arxiv.org/abs/2107.08179"/>
        <updated>2021-07-20T02:04:46.106Z</updated>
        <summary type="html"><![CDATA[Probabilistic graphical models are a fundamental tool in probabilistic
modeling, machine learning and artificial intelligence. They allow us to
integrate in a natural way expert knowledge, physical modeling, heterogeneous
and correlated data and quantities of interest. For exactly this reason,
multiple sources of model uncertainty are inherent within the modular structure
of the graphical model. In this paper we develop information-theoretic, robust
uncertainty quantification methods and non-parametric stress tests for directed
graphical models to assess the effect and the propagation through the graph of
multi-sourced model uncertainties to quantities of interest. These methods
allow us to rank the different sources of uncertainty and correct the graphical
model by targeting its most impactful components with respect to the quantities
of interest. Thus, from a machine learning perspective, we provide a
mathematically rigorous approach to correctability that guarantees a systematic
selection for improvement of components of a graphical model while controlling
potential new errors created in the process in other parts of the model. We
demonstrate our methods in two physico-chemical examples, namely quantum
scale-informed chemical kinetics and materials screening to improve the
efficiency of fuel cells.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Birmpa_P/0/1/0/all/0/1"&gt;Panagiota Birmpa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jinchao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Katsoulakis_M/0/1/0/all/0/1"&gt;Markos A. Katsoulakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rey_Bellet_L/0/1/0/all/0/1"&gt;Luc Rey-Bellet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANs for Medical Image Synthesis: An Empirical Study. (arXiv:2105.05318v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05318</id>
        <link href="http://arxiv.org/abs/2105.05318"/>
        <updated>2021-07-20T02:04:46.096Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have become increasingly powerful,
generating mind-blowing photorealistic images that mimic the content of
datasets they were trained to replicate. One recurrent theme in medical imaging
is whether GANs can also be effective at generating workable medical data as
they are for generating realistic RGB images. In this paper, we perform a
multi-GAN and multi-application study to gauge the benefits of GANs in medical
imaging. We tested various GAN architectures from basic DCGAN to more
sophisticated style-based GANs on three medical imaging modalities and organs
namely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on
well-known and widely utilized datasets from which their FID score were
computed to measure the visual acuity of their generated images. We further
tested their usefulness by measuring the segmentation accuracy of a U-Net
trained on these generated images.

Results reveal that GANs are far from being equal as some are ill-suited for
medical imaging applications while others are much better off. The
top-performing GANs are capable of generating realistic-looking medical images
by FID standards that can fool trained experts in a visual Turing test and
comply to some metrics. However, segmentation results suggests that no GAN is
capable of reproducing the full richness of a medical datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Skandarani_Y/0/1/0/all/0/1"&gt;Youssef Skandarani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1"&gt;Pierre-Marc Jodoin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lalande_A/0/1/0/all/0/1"&gt;Alain Lalande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13797</id>
        <link href="http://arxiv.org/abs/2106.13797"/>
        <updated>2021-07-20T02:04:46.096Z</updated>
        <summary type="html"><![CDATA[Transformer recently has shown encouraging progresses in computer vision. In
this work, we present new baselines by improving the original Pyramid Vision
Transformer (abbreviated as PVTv1) by adding three designs, including (1)
overlapping patch embedding, (2) convolutional feed-forward networks, and (3)
linear complexity attention layers.

With these modifications, our PVTv2 significantly improves PVTv1 on three
tasks e.g., classification, detection, and segmentation. Moreover, PVTv2
achieves comparable or better performances than recent works such as Swin
Transformer. We hope this work will facilitate state-of-the-art Transformer
researches in computer vision. Code is available at
https://github.com/whai362/PVT .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1"&gt;Kaitao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1"&gt;Ding Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tong Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LeanML: A Design Pattern To Slash Avoidable Wastes in Machine Learning Projects. (arXiv:2107.08066v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08066</id>
        <link href="http://arxiv.org/abs/2107.08066"/>
        <updated>2021-07-20T02:04:46.095Z</updated>
        <summary type="html"><![CDATA[We introduce the first application of the lean methodology to machine
learning projects. Similar to lean startups and lean manufacturing, we argue
that lean machine learning (LeanML) can drastically slash avoidable wastes in
commercial machine learning projects, reduce the business risk in investing in
machine learning capabilities and, in so doing, further democratize access to
machine learning. The lean design pattern we propose in this paper is based on
two realizations. First, it is possible to estimate the best performance one
may achieve when predicting an outcome $y \in \mathcal{Y}$ using a given set of
explanatory variables $x \in \mathcal{X}$, for a wide range of performance
metrics, and without training any predictive model. Second, doing so is
considerably easier, faster, and cheaper than learning the best predictive
model. We derive formulae expressing the best $R^2$, MSE, classification
accuracy, and log-likelihood per observation achievable when using $x$ to
predict $y$ as a function of the mutual information $I\left(y; x\right)$, and
possibly a measure of the variability of $y$ (e.g. its Shannon entropy in the
case of classification accuracy, and its variance in the case regression MSE).
We illustrate the efficacy of the LeanML design pattern on a wide range of
regression and classification problems, synthetic and real-life.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samo_Y/0/1/0/all/0/1"&gt;Yves-Laurent Kom Samo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?. (arXiv:2102.11068v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11068</id>
        <link href="http://arxiv.org/abs/2102.11068"/>
        <updated>2021-07-20T02:04:46.094Z</updated>
        <summary type="html"><![CDATA[In deep model compression, the recent finding "Lottery Ticket Hypothesis"
(LTH) (Frankle & Carbin, 2018) pointed out that there could exist a winning
ticket (i.e., a properly pruned sub-network together with original weight
initialization) that can achieve competitive performance than the original
dense network. However, it is not easy to observe such winning property in many
scenarios, where for example, a relatively large learning rate is used even if
it benefits training the original dense model. In this work, we investigate the
underlying condition and rationale behind the winning property, and find that
the underlying reason is largely attributed to the correlation between
initialized weights and final-trained weights when the learning rate is not
sufficiently large. Thus, the existence of winning property is correlated with
an insufficient DNN pretraining, and is unlikely to occur for a well-trained
DNN. To overcome this limitation, we propose the "pruning & fine-tuning" method
that consistently outperforms lottery ticket sparse training under the same
pruning algorithm and the same total training epochs. Extensive experiments
over multiple deep models (VGG, ResNet, MobileNet-v2) on different datasets
have been conducted to justify our proposals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Ning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1"&gt;Zhengping Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xuan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jian Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Refined Policy Improvement Bounds for MDPs. (arXiv:2107.08068v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08068</id>
        <link href="http://arxiv.org/abs/2107.08068"/>
        <updated>2021-07-20T02:04:46.094Z</updated>
        <summary type="html"><![CDATA[The policy improvement bound on the difference of the discounted returns
plays a crucial role in the theoretical justification of the trust-region
policy optimization (TRPO) algorithm. The existing bound leads to a degenerate
bound when the discount factor approaches one, making the applicability of TRPO
and related algorithms questionable when the discount factor is close to one.
We refine the results in \cite{Schulman2015, Achiam2017} and propose a novel
bound that is "continuous" in the discount factor. In particular, our bound is
applicable for MDPs with the long-run average rewards as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;J. G. Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gluzman_M/0/1/0/all/0/1"&gt;Mark Gluzman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Review of Learning-based Longitudinal Motion Planning for Autonomous Vehicles: Research Gaps between Self-driving and Traffic Congestion. (arXiv:1910.06070v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.06070</id>
        <link href="http://arxiv.org/abs/1910.06070"/>
        <updated>2021-07-20T02:04:46.003Z</updated>
        <summary type="html"><![CDATA[Self-driving technology companies and the research community are accelerating
their pace to use machine learning longitudinal motion planning (mMP) for
autonomous vehicles (AVs). This paper reviews the current state of the art in
mMP, with an exclusive focus on its impact on traffic congestion. We identify
the availability of congestion scenarios in current datasets, and summarize the
required features for training mMP. For learning methods, we survey the major
methods in both imitation learning and non-imitation learning. We also
highlight the emerging technologies adopted by some leading AV companies, e.g.
Tesla, Waymo, and Comma.ai. We find that: i) the AV industry has been mostly
focusing on the long tail problem related to safety and overlooked the impact
on traffic congestion, ii) the current public self-driving datasets have not
included enough congestion scenarios, and mostly lack the necessary input
features/output labels to train mMP, and iii) albeit reinforcement learning
(RL) approach can integrate congestion mitigation into the learning goal, the
major mMP method adopted by industry is still behavior cloning (BC), whose
capability to learn a congestion-mitigating mMP remains to be seen. Based on
the review, the study identifies the research gaps in current mMP development.
Some suggestions towards congestion mitigation for future mMP studies are
proposed: i) enrich data collection to facilitate the congestion learning, ii)
incorporate non-imitation learning methods to combine traffic efficiency into a
safety-oriented technical route, and iii) integrate domain knowledge from the
traditional car following (CF) theory to improve the string stability of mMP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Laval_J/0/1/0/all/0/1"&gt;Jorge Laval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Anye Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenchao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qing_Z/0/1/0/all/0/1"&gt;Zhu Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peeta_S/0/1/0/all/0/1"&gt;Srinivas Peeta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impact of facial landmark localization on facial expression recognition. (arXiv:1905.10784v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.10784</id>
        <link href="http://arxiv.org/abs/1905.10784"/>
        <updated>2021-07-20T02:04:46.002Z</updated>
        <summary type="html"><![CDATA[Although facial landmark localization (FLL) approaches are becoming
increasingly accurate for characterizing facial regions, one question remains
unanswered: what is the impact of these approaches on subsequent related tasks?
In this paper, the focus is put on facial expression recognition (FER), where
facial landmarks are used for face registration, which is a common usage. Since
the most used datasets for facial landmark localization do not allow for a
proper measurement of performance according to the different difficulties
(e.g., pose, expression, illumination, occlusion, motion blur), we also
quantify the performance of recent approaches in the presence of head pose
variations and facial expressions. Finally, a study of the impact of these
approaches on FER is conducted. We show that the landmark accuracy achieved so
far optimizing the conventional Euclidean distance does not necessarily
guarantee a gain in performance for FER. To deal with this issue, we propose a
new evaluation metric for FLL adapted to FER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Belmonte_R/0/1/0/all/0/1"&gt;Romain Belmonte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allaert_B/0/1/0/all/0/1"&gt;Benjamin Allaert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tirilly_P/0/1/0/all/0/1"&gt;Pierre Tirilly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilasco_I/0/1/0/all/0/1"&gt;Ioan Marius Bilasco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djeraba_C/0/1/0/all/0/1"&gt;Chaabane Djeraba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Event Segmentation and Localization for Wildlife Extended Videos. (arXiv:2005.02463v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.02463</id>
        <link href="http://arxiv.org/abs/2005.02463"/>
        <updated>2021-07-20T02:04:46.001Z</updated>
        <summary type="html"><![CDATA[Using offline training schemes, researchers have tackled the event
segmentation problem by providing full or weak-supervision through manually
annotated labels or self-supervised epoch-based training. Most works consider
videos that are at most 10's of minutes long. We present a self-supervised
perceptual prediction framework capable of temporal event segmentation by
building stable representations of objects over time and demonstrate it on long
videos, spanning several days. The approach is deceptively simple but quite
effective. We rely on predictions of high-level features computed by a standard
deep learning backbone. For prediction, we use an LSTM, augmented with an
attention mechanism, trained in a self-supervised manner using the prediction
error. The self-learned attention maps effectively localize and track the
event-related objects in each frame. The proposed approach does not require
labels. It requires only a single pass through the video, with no separate
training set. Given the lack of datasets of very long videos, we demonstrate
our method on video from 10 days (254 hours) of continuous wildlife monitoring
data that we had collected with required permissions. We find that the approach
is robust to various environmental conditions such as day/night conditions,
rain, sharp shadows, and windy conditions. For the task of temporally locating
events, we had an 80% recall rate at 20% false-positive rate for frame-level
segmentation. At the activity level, we had an 80% activity recall rate for one
false activity detection every 50 minutes. We will make the dataset, which is
the first of its kind, and the code available to the research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mounir_R/0/1/0/all/0/1"&gt;Ramy Mounir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gula_R/0/1/0/all/0/1"&gt;Roman Gula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theuerkauf_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rn Theuerkauf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1"&gt;Sudeep Sarkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A High-Performance Adaptive Quantization Approach for Edge CNN Applications. (arXiv:2107.08382v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08382</id>
        <link href="http://arxiv.org/abs/2107.08382"/>
        <updated>2021-07-20T02:04:45.991Z</updated>
        <summary type="html"><![CDATA[Recent convolutional neural network (CNN) development continues to advance
the state-of-the-art model accuracy for various applications. However, the
enhanced accuracy comes at the cost of substantial memory bandwidth and storage
requirements and demanding computational resources. Although in the past the
quantization methods have effectively reduced the deployment cost for edge
devices, it suffers from significant information loss when processing the
biased activations of contemporary CNNs. In this paper, we hence introduce an
adaptive high-performance quantization method to resolve the issue of biased
activation by dynamically adjusting the scaling and shifting factors based on
the task loss. Our proposed method has been extensively evaluated on image
classification models (ResNet-18/34/50, MobileNet-V2, EfficientNet-B0) with
ImageNet dataset, object detection model (YOLO-V4) with COCO dataset, and
language models with PTB dataset. The results show that our 4-bit integer
(INT4) quantization models achieve better accuracy than the state-of-the-art
4-bit models, and in some cases, even surpass the golden full-precision models.
The final designs have been successfully deployed onto extremely
resource-constrained edge devices for many practical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chin_H/0/1/0/all/0/1"&gt;Hsu-Hsun Chin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsay_R/0/1/0/all/0/1"&gt;Ren-Song Tsay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hsin-I Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Salient Object Detection with Purificatory Mechanism and Structural Similarity Loss. (arXiv:1912.08393v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.08393</id>
        <link href="http://arxiv.org/abs/1912.08393"/>
        <updated>2021-07-20T02:04:45.988Z</updated>
        <summary type="html"><![CDATA[By the aid of attention mechanisms to weight the image features adaptively,
recent advanced deep learning-based models encourage the predicted results to
approximate the ground-truth masks with as large predictable areas as possible,
thus achieving the state-of-the-art performance. However, these methods do not
pay enough attention to small areas prone to misprediction. In this way, it is
still tough to accurately locate salient objects due to the existence of
regions with indistinguishable foreground and background and regions with
complex or fine structures. To address these problems, we propose a novel
convolutional neural network with purificatory mechanism and structural
similarity loss. Specifically, in order to better locate preliminary salient
objects, we first introduce the promotion attention, which is based on spatial
and channel attention mechanisms to promote attention to salient regions.
Subsequently, for the purpose of restoring the indistinguishable regions that
can be regarded as error-prone regions of one model, we propose the
rectification attention, which is learned from the areas of wrong prediction
and guide the network to focus on error-prone regions thus rectifying errors.
Through these two attentions, we use the Purificatory Mechanism to impose
strict weights with different regions of the whole salient objects and purify
results from hard-to-distinguish regions, thus accurately predicting the
locations and details of salient objects. In addition to paying different
attention to these hard-to-distinguish regions, we also consider the structural
constraints on complex regions and propose the Structural Similarity Loss. In
experiments, the proposed approach outperforms 19 state-of-the-art methods on
six datasets with a notable margin at over 27FPS on a single NVIDIA 1080Ti GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jinming Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1"&gt;Changqun Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Mingcan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double Similarity Distillation for Semantic Image Segmentation. (arXiv:2107.08591v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08591</id>
        <link href="http://arxiv.org/abs/2107.08591"/>
        <updated>2021-07-20T02:04:45.987Z</updated>
        <summary type="html"><![CDATA[The balance between high accuracy and high speed has always been a
challenging task in semantic image segmentation. Compact segmentation networks
are more widely used in the case of limited resources, while their performances
are constrained. In this paper, motivated by the residual learning and global
aggregation, we propose a simple yet general and effective knowledge
distillation framework called double similarity distillation (DSD) to improve
the classification accuracy of all existing compact networks by capturing the
similarity knowledge in pixel and category dimensions, respectively.
Specifically, we propose a pixel-wise similarity distillation (PSD) module that
utilizes residual attention maps to capture more detailed spatial dependencies
across multiple layers. Compared with exiting methods, the PSD module greatly
reduces the amount of calculation and is easy to expand. Furthermore,
considering the differences in characteristics between semantic segmentation
task and other computer vision tasks, we propose a category-wise similarity
distillation (CSD) module, which can help the compact segmentation network
strengthen the global category correlation by constructing the correlation
matrix. Combining these two modules, DSD framework has no extra parameters and
only a minimal increase in FLOPs. Extensive experiments on four challenging
datasets, including Cityscapes, CamVid, ADE20K, and Pascal VOC 2012, show that
DSD outperforms current state-of-the-art methods, proving its effectiveness and
generality. The code and models will be publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yingchao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diao_W/0/1/0/all/0/1"&gt;Wenhui Diao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jihao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xin Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02443</id>
        <link href="http://arxiv.org/abs/2007.02443"/>
        <updated>2021-07-20T02:04:45.985Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting (CF) happens whenever a neural network overwrites
past knowledge while being trained on new tasks. Common techniques to handle CF
include regularization of the weights (using, e.g., their importance on past
tasks), and rehearsal strategies, where the network is constantly re-trained on
past data. Generative models have also been applied for the latter, in order to
have endless sources of data. In this paper, we propose a novel method that
combines the strengths of regularization and generative-based rehearsal
approaches. Our generative model consists of a normalizing flow (NF), a
probabilistic and invertible neural network, trained on the internal embeddings
of the network. By keeping a single NF conditioned on the task, we show that
our memory overhead remains constant. In addition, exploiting the invertibility
of the NF, we propose a simple approach to regularize the network's embeddings
with respect to past tasks. We show that our method performs favorably with
respect to state-of-the-art approaches in the literature, with bounded
computational power and memory overheads.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1"&gt;Jary Pomponi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1"&gt;Simone Scardapane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1"&gt;Aurelio Uncini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Promoted Prototype Refinement for Few-Shot Class-Incremental Learning. (arXiv:2107.08918v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08918</id>
        <link href="http://arxiv.org/abs/2107.08918"/>
        <updated>2021-07-20T02:04:45.984Z</updated>
        <summary type="html"><![CDATA[Few-shot class-incremental learning is to recognize the new classes given few
samples and not forget the old classes. It is a challenging task since
representation optimization and prototype reorganization can only be achieved
under little supervision. To address this problem, we propose a novel
incremental prototype learning scheme. Our scheme consists of a random episode
selection strategy that adapts the feature representation to various generated
incremental episodes to enhance the corresponding extensibility, and a
self-promoted prototype refinement mechanism which strengthens the expression
ability of the new classes by explicitly considering the dependencies among
different classes. Particularly, a dynamic relation projection module is
proposed to calculate the relation matrix in a shared embedding space and
leverage it as the factor for bootstrapping the update of prototypes. Extensive
experiments on three benchmark datasets demonstrate the above-par incremental
performance, outperforming state-of-the-art methods by a margin of 13%, 17% and
11%, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1"&gt;Kai Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1"&gt;Wei Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jie Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08369</id>
        <link href="http://arxiv.org/abs/2107.08369"/>
        <updated>2021-07-20T02:04:45.661Z</updated>
        <summary type="html"><![CDATA[Floods wreak havoc throughout the world, causing billions of dollars in
damages, and uprooting communities, ecosystems and economies. Accurate and
robust flood detection including delineating open water flood areas and
identifying flood levels can aid in disaster response and mitigation. However,
estimating flood levels remotely is of essence as physical access to flooded
areas is limited and the ability to deploy instruments in potential flood zones
can be dangerous. Aligning flood extent mapping with local topography can
provide a plan-of-action that the disaster response team can consider. Thus,
remote flood level estimation via satellites like Sentinel-1 can prove to be
remedial. The Emerging Techniques in Computational Intelligence (ETCI)
competition on Flood Detection tasked participants with predicting flooded
pixels after training with synthetic aperture radar (SAR) images in a
supervised setting. We use a cyclical approach involving two stages (1)
training an ensemble model of multiple UNet architectures with available high
and low confidence labeled data and, (2) generating pseudo labels or low
confidence labels on the unlabeled test dataset, and then, combining the
generated labels with the previously available high confidence labeled dataset.
This assimilated dataset is used for the next round of training ensemble
models. This cyclical process is repeated until the performance improvement
plateaus. Additionally, we post process our results with Conditional Random
Fields. Our approach sets a high score on the public leaderboard for the ETCI
competition with 0.7654 IoU. Our method, which we release with all the code
including trained models, can also be used as an open science benchmark for the
Sentinel-1 released dataset on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Sayak Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariant Learning of Stochastic Fields: Gaussian Processes and Steerable Conditional Neural Processes. (arXiv:2011.12916v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12916</id>
        <link href="http://arxiv.org/abs/2011.12916"/>
        <updated>2021-07-20T02:04:45.372Z</updated>
        <summary type="html"><![CDATA[Motivated by objects such as electric fields or fluid streams, we study the
problem of learning stochastic fields, i.e. stochastic processes whose samples
are fields like those occurring in physics and engineering. Considering general
transformations such as rotations and reflections, we show that spatial
invariance of stochastic fields requires an inference model to be equivariant.
Leveraging recent advances from the equivariance literature, we study
equivariance in two classes of models. Firstly, we fully characterise
equivariant Gaussian processes. Secondly, we introduce Steerable Conditional
Neural Processes (SteerCNPs), a new, fully equivariant member of the Neural
Process family. In experiments with Gaussian process vector fields, images, and
real-world weather data, we observe that SteerCNPs significantly improve the
performance of previous models and equivariance leads to improvements in
transfer learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Holderrieth_P/0/1/0/all/0/1"&gt;Peter Holderrieth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutchinson_M/0/1/0/all/0/1"&gt;Michael Hutchinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doubly Robust Off-Policy Actor-Critic: Convergence and Optimality. (arXiv:2102.11866v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11866</id>
        <link href="http://arxiv.org/abs/2102.11866"/>
        <updated>2021-07-20T02:04:45.244Z</updated>
        <summary type="html"><![CDATA[Designing off-policy reinforcement learning algorithms is typically a very
challenging task, because a desirable iteration update often involves an
expectation over an on-policy distribution. Prior off-policy actor-critic (AC)
algorithms have introduced a new critic that uses the density ratio for
adjusting the distribution mismatch in order to stabilize the convergence, but
at the cost of potentially introducing high biases due to the estimation errors
of both the density ratio and value function. In this paper, we develop a
doubly robust off-policy AC (DR-Off-PAC) for discounted MDP, which can take
advantage of learned nuisance functions to reduce estimation errors. Moreover,
DR-Off-PAC adopts a single timescale structure, in which both actor and critics
are updated simultaneously with constant stepsize, and is thus more sample
efficient than prior algorithms that adopt either two timescale or nested-loop
structure. We study the finite-time convergence rate and characterize the
sample complexity for DR-Off-PAC to attain an $\epsilon$-accurate optimal
policy. We also show that the overall convergence of DR-Off-PAC is doubly
robust to the approximation errors that depend only on the expressive power of
approximation functions. To the best of our knowledge, our study establishes
the first overall sample complexity analysis for a single time-scale off-policy
AC algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tengyu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yingbin Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Domain Adaptation in CT Segmentation by Filtered Back Projection Augmentation. (arXiv:2107.08543v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08543</id>
        <link href="http://arxiv.org/abs/2107.08543"/>
        <updated>2021-07-20T02:04:45.229Z</updated>
        <summary type="html"><![CDATA[Domain shift is one of the most salient challenges in medical computer
vision. Due to immense variability in scanners' parameters and imaging
protocols, even images obtained from the same person and the same scanner could
differ significantly. We address variability in computed tomography (CT) images
caused by different convolution kernels used in the reconstruction process, the
critical domain shift factor in CT. The choice of a convolution kernel affects
pixels' granularity, image smoothness, and noise level. We analyze a dataset of
paired CT images, where smooth and sharp images were reconstructed from the
same sinograms with different kernels, thus providing identical anatomy but
different style. Though identical predictions are desired, we show that the
consistency, measured as the average Dice between predictions on pairs, is just
0.54. We propose Filtered Back-Projection Augmentation (FBPAug), a simple and
surprisingly efficient approach to augment CT images in sinogram space
emulating reconstruction with different kernels. We apply the proposed method
in a zero-shot domain adaptation setup and show that the consistency boosts
from 0.54 to 0.92 outperforming other augmentation approaches. Neither specific
preparation of source domain data nor target domain data is required, so our
publicly released FBPAug can be used as a plug-and-play module for zero-shot
domain adaptation in any CT-based task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Saparov_T/0/1/0/all/0/1"&gt;Talgat Saparov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kurmukov_A/0/1/0/all/0/1"&gt;Anvar Kurmukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shirokih_B/0/1/0/all/0/1"&gt;Boris Shirokih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Belyaev_M/0/1/0/all/0/1"&gt;Mikhail Belyaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Miniature Biological Eagle-Eye Vision System for Small Target Detection. (arXiv:2107.08406v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08406</id>
        <link href="http://arxiv.org/abs/2107.08406"/>
        <updated>2021-07-20T02:04:45.201Z</updated>
        <summary type="html"><![CDATA[Small target detection is known to be a challenging problem. Inspired by the
structural characteristics and physiological mechanism of eagle-eye, a
miniature vision system is designed for small target detection in this paper.
First, a hardware platform is established, which consists of a pan-tilt, a
short-focus camera and a long-focus camera. Then, based on the visual attention
mechanism of eagle-eye, the cameras with different focal lengths are controlled
cooperatively to achieve small target detection. Experimental results show that
the designed biological eagle-eye vision system can accurately detect small
targets, which has a strong adaptive ability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shutai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1"&gt;Qiang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yinhao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chunhua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wei He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A stepped sampling method for video detection using LSTM. (arXiv:2107.08471v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08471</id>
        <link href="http://arxiv.org/abs/2107.08471"/>
        <updated>2021-07-20T02:04:45.170Z</updated>
        <summary type="html"><![CDATA[Artificial neural networks that simulate human achieves great successes. From
the perspective of simulating human memory method, we propose a stepped sampler
based on the "repeated input". We repeatedly inputted data to the LSTM model
stepwise in a batch. The stepped sampler is used to strengthen the ability of
fusing the temporal information in LSTM. We tested the stepped sampler on the
LSTM built-in in PyTorch. Compared with the traditional sampler of PyTorch,
such as sequential sampler, batch sampler, the training loss of the proposed
stepped sampler converges faster in the training of the model, and the training
loss after convergence is more stable. Meanwhile, it can maintain a higher test
accuracy. We quantified the algorithm of the stepped sampler.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dengshan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rujing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chengjun Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AS-MLP: An Axial Shifted MLP Architecture for Vision. (arXiv:2107.08391v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08391</id>
        <link href="http://arxiv.org/abs/2107.08391"/>
        <updated>2021-07-20T02:04:45.154Z</updated>
        <summary type="html"><![CDATA[An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper.
Different from MLP-Mixer, where the global spatial feature is encoded for the
information flow through matrix transposition and one token-mixing MLP, we pay
more attention to the local features communication. By axially shifting
channels of the feature map, AS-MLP is able to obtain the information flow from
different axial directions, which captures the local dependencies. Such an
operation enables us to utilize a pure MLP architecture to achieve the same
local receptive field as CNN-like architecture. We can also design the
receptive field size and dilation of blocks of AS-MLP, etc, just like designing
those of convolution kernels. With the proposed AS-MLP architecture, our model
obtains 83.3% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the
ImageNet-1K dataset. Such a simple yet effective architecture outperforms all
MLP-based architectures and achieves competitive performance compared to the
transformer-based architectures (e.g., Swin Transformer) even with slightly
lower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be
applied to the downstream tasks (e.g., object detection and semantic
segmentation). The experimental results are also impressive. Our proposed
AS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the
ADE20K dataset, which is competitive compared to the transformer-based
architectures. Code is available at https://github.com/svip-lab/AS-MLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1"&gt;Dongze Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zehao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xing Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Shenghua Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Multidimensional Kaggle Literature Organization. (arXiv:2107.08190v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08190</id>
        <link href="http://arxiv.org/abs/2107.08190"/>
        <updated>2021-07-20T02:04:44.973Z</updated>
        <summary type="html"><![CDATA[The unprecedented outbreak of Severe Acute Respiratory Syndrome Coronavirus-2
(SARS-CoV-2), or COVID-19, continues to be a significant worldwide problem. As
a result, a surge of new COVID-19 related research has followed suit. The
growing number of publications requires document organization methods to
identify relevant information. In this paper, we expand upon our previous work
with clustering the CORD-19 dataset by applying multi-dimensional analysis
methods. Tensor factorization is a powerful unsupervised learning method
capable of discovering hidden patterns in a document corpus. We show that a
higher-order representation of the corpus allows for the simultaneous grouping
of similar articles, relevant journals, authors with similar research
interests, and topic keywords. These groupings are identified within and among
the latent components extracted via tensor decomposition. We further
demonstrate the application of this method with a publicly available
interactive visualization of the dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eren_M/0/1/0/all/0/1"&gt;Maksim E. Eren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solovyev_N/0/1/0/all/0/1"&gt;Nick Solovyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamer_C/0/1/0/all/0/1"&gt;Chris Hamer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1"&gt;Renee McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexandrov_B/0/1/0/all/0/1"&gt;Boian S. Alexandrov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nicholas_C/0/1/0/all/0/1"&gt;Charles Nicholas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Evolutionary Algorithm for Hierarchical Neural Architecture Search. (arXiv:2107.08484v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.08484</id>
        <link href="http://arxiv.org/abs/2107.08484"/>
        <updated>2021-07-20T02:04:44.663Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a novel evolutionary algorithm for neural
architecture search, applicable to global search spaces. The algorithm's
architectural representation organizes the topology in multiple hierarchical
modules, while the design process exploits this representation, in order to
explore the search space. We also employ a curation system, which promotes the
utilization of well performing sub-structures to subsequent generations. We
apply our method to Fashion-MNIST and NAS-Bench101, achieving accuracies of
$93.2\%$ and $94.8\%$ respectively in a relatively small number of generations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chrostoforidis_A/0/1/0/all/0/1"&gt;Aristeidis Chrostoforidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kyriakides_G/0/1/0/all/0/1"&gt;George Kyriakides&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Margaritis_K/0/1/0/all/0/1"&gt;Konstantinos Margaritis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-asymptotic estimates for TUSLA algorithm for non-convex learning with applications to neural networks with ReLU activation function. (arXiv:2107.08649v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.08649</id>
        <link href="http://arxiv.org/abs/2107.08649"/>
        <updated>2021-07-20T02:04:44.607Z</updated>
        <summary type="html"><![CDATA[We consider non-convex stochastic optimization problems where the objective
functions have super-linearly growing and discontinuous stochastic gradients.
In such a setting, we provide a non-asymptotic analysis for the tamed
unadjusted stochastic Langevin algorithm (TUSLA) introduced in Lovas et al.
(2021). In particular, we establish non-asymptotic error bounds for the TUSLA
algorithm in Wasserstein-1 and Wasserstein-2 distances. The latter result
enables us to further derive non-asymptotic estimates for the expected excess
risk. To illustrate the applicability of the main results, we consider an
example from transfer learning with ReLU neural networks, which represents a
key paradigm in machine learning. Numerical experiments are presented for the
aforementioned example which supports our theoretical findings. Hence, in this
setting, we demonstrate both theoretically and numerically that the TUSLA
algorithm can solve the optimization problem involving neural networks with
ReLU activation function. Besides, we provide simulation results for synthetic
examples where popular algorithms, e.g. ADAM, AMSGrad, RMSProp, and (vanilla)
SGD, may fail to find the minimizer of the objective functions due to the
super-linear growth and the discontinuity of the corresponding stochastic
gradient, while the TUSLA algorithm converges rapidly to the optimal solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Lim_D/0/1/0/all/0/1"&gt;Dong-Young Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Neufeld_A/0/1/0/all/0/1"&gt;Ariel Neufeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sabanis_S/0/1/0/all/0/1"&gt;Sotirios Sabanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ying Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sleep Staging Based on Serialized Dual Attention Network. (arXiv:2107.08442v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08442</id>
        <link href="http://arxiv.org/abs/2107.08442"/>
        <updated>2021-07-20T02:04:44.589Z</updated>
        <summary type="html"><![CDATA[Sleep staging assumes an important role in the diagnosis of sleep disorders.
In general, experts classify sleep stages manually based on polysomnography
(PSG), which is quite time-consuming. Meanwhile, the acquisition of multiple
signals is complex, which can affect the subject's sleep. Therefore, the use of
single-channel electroencephalogram (EEG) for automatic sleep staging has
become mainstream. In the literature, a large number of sleep staging methods
based on single-channel EEG have been proposed with good results and realize
the preliminary automation of sleep staging. However, the performance for most
of these methods in the N1 stage is generally not high. In this paper, we
propose a deep learning model SDAN based on raw EEG. The method utilises a
one-dimensional convolutional neural network (CNN) to automatically extract
features from raw EEG. It serially combines the channel attention and spatial
attention mechanisms to filter and highlight key information and then uses soft
threshold to eliminate redundant information. Additionally, we introduce a
residual network to avoid degradation problems caused by network deepening.
Experiments were conducted using two datasets with 5-fold cross-validation and
hold-out validation method. The final average accuracy, overall accuracy, macro
F1 score and Cohen's Kappa coefficient of the model reach 96.74%, 91.86%,
82.64% and 0.8742 on the Sleep-EDF dataset, and 95.98%, 89.96%, 79.08% and
0.8216 on the Sleep-EDFx dataset. Significantly, our model performed superiorly
in the N1 stage, with F1 scores of 54.08% and 52.49% on the two datasets
respectively. The results show the superiority of our network over the best
existing methods, reaching a new state-of-the-art. In particular, the present
method achieves excellent results in the N1 sleep stage compared to other
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huafeng Wang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chonggang Lu&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhimin Hu&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiaodong Yuan&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingshu Zhang&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanquan Liu&lt;/a&gt; (3) ((1) School of Information, North China University of Technology,(2) Department of Neurology, Kailuan General Hospital, Tangshan,(3) School of Intelligent Systems Engineering, Sun Yat-sen University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Graph Autoencoder Models for Attributed Graph Clustering. (arXiv:2107.08562v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08562</id>
        <link href="http://arxiv.org/abs/2107.08562"/>
        <updated>2021-07-20T02:04:44.571Z</updated>
        <summary type="html"><![CDATA[Most recent graph clustering methods have resorted to Graph Auto-Encoders
(GAEs) to perform joint clustering and embedding learning. However, two
critical issues have been overlooked. First, the accumulative error, inflicted
by learning with noisy clustering assignments, degrades the effectiveness and
robustness of the clustering model. This problem is called Feature Randomness.
Second, reconstructing the adjacency matrix sets the model to learn irrelevant
similarities for the clustering task. This problem is called Feature Drift.
Interestingly, the theoretical relation between the aforementioned problems has
not yet been investigated. We study these issues from two aspects: (1) the
existence of a trade-off between Feature Randomness and Feature Drift when
clustering and reconstruction are performed at the same level, and (2) the
problem of Feature Drift is more pronounced for GAE models, compared with
vanilla auto-encoder models, due to the graph convolutional operation and the
graph decoding design. Motivated by these findings, we reformulate the
GAE-based clustering methodology. Our solution is two-fold. First, we propose a
sampling operator $\Xi$ that triggers a protection mechanism against the noisy
clustering assignments. Second, we propose an operator $\Upsilon$ that triggers
a correction mechanism against Feature Drift by gradually transforming the
reconstructed graph into a clustering-oriented one. As principal advantages,
our solution grants a considerable improvement in clustering effectiveness and
robustness and can be easily tailored to existing GAE models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mrabah_N/0/1/0/all/0/1"&gt;Nairouz Mrabah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouguessa_M/0/1/0/all/0/1"&gt;Mohamed Bouguessa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Touati_M/0/1/0/all/0/1"&gt;Mohamed Fawzi Touati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ksantini_R/0/1/0/all/0/1"&gt;Riadh Ksantini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Problem of Nonlinear Schr\"odinger Equation as Learning of Convolutional Neural Network. (arXiv:2107.08593v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2107.08593</id>
        <link href="http://arxiv.org/abs/2107.08593"/>
        <updated>2021-07-20T02:04:44.554Z</updated>
        <summary type="html"><![CDATA[In this work, we use an explainable convolutional neural network (NLS-Net) to
solve an inverse problem of the nonlinear Schr\"odinger equation, which is
widely used in fiber-optic communications. The landscape and minimizers of the
non-convex loss function of the learning problem are studied empirically. It
provides a guidance for choosing hyper-parameters of the method. The estimation
error of the optimal solution is discussed in terms of expressive power of the
NLS-Net and data. Besides, we compare the performance of several training
algorithms that are popular in deep learning. It is shown that one can obtain a
relatively accurate estimate of the considered parameters using the proposed
method. The study provides a natural framework of solving inverse problems of
nonlinear partial differential equations with deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Queueing-Theoretic Framework for Vehicle Dispatching in Dynamic Car-Hailing [technical report]. (arXiv:2107.08662v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08662</id>
        <link href="http://arxiv.org/abs/2107.08662"/>
        <updated>2021-07-20T02:04:44.536Z</updated>
        <summary type="html"><![CDATA[With the rapid development of smart mobile devices, the car-hailing platforms
(e.g., Uber or Lyft) have attracted much attention from both the academia and
the industry. In this paper, we consider an important dynamic car-hailing
problem, namely \textit{maximum revenue vehicle dispatching} (MRVD), in which
rider requests dynamically arrive and drivers need to serve as many riders as
possible such that the entire revenue of the platform is maximized. We prove
that the MRVD problem is NP-hard and intractable. In addition, the dynamic
car-hailing platforms have no information of the future riders, which makes the
problem even harder. To handle the MRVD problem, we propose a queueing-based
vehicle dispatching framework, which first uses existing machine learning
algorithms to predict the future vehicle demand of each region, then estimates
the idle time periods of drivers through a queueing model for each region. With
the information of the predicted vehicle demands and estimated idle time
periods of drivers, we propose two batch-based vehicle dispatching algorithms
to efficiently assign suitable drivers to riders such that the expected overall
revenue of the platform is maximized during each batch processing. Through
extensive experiments, we demonstrate the efficiency and effectiveness of our
proposed approaches over both real and synthetic datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Peng Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Jiabao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xuemin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Libin Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wave-based extreme deep learning based on non-linear time-Floquet entanglement. (arXiv:2107.08564v1 [cs.ET])]]></title>
        <id>http://arxiv.org/abs/2107.08564</id>
        <link href="http://arxiv.org/abs/2107.08564"/>
        <updated>2021-07-20T02:04:44.473Z</updated>
        <summary type="html"><![CDATA[Wave-based analog signal processing holds the promise of extremely fast,
on-the-fly, power-efficient data processing, occurring as a wave propagates
through an artificially engineered medium. Yet, due to the fundamentally weak
non-linearities of traditional wave materials, such analog processors have been
so far largely confined to simple linear projections such as image edge
detection or matrix multiplications. Complex neuromorphic computing tasks,
which inherently require strong non-linearities, have so far remained
out-of-reach of wave-based solutions, with a few attempts that implemented
non-linearities on the digital front, or used weak and inflexible non-linear
sensors, restraining the learning performance. Here, we tackle this issue by
demonstrating the relevance of Time-Floquet physics to induce a strong
non-linear entanglement between signal inputs at different frequencies,
enabling a power-efficient and versatile wave platform for analog extreme deep
learning involving a single, uniformly modulated dielectric layer and a
scattering medium. We prove the efficiency of the method for extreme learning
machines and reservoir computing to solve a range of challenging learning
tasks, from forecasting chaotic time series to the simultaneous classification
of distinct datasets. Our results open the way for wave-based machine learning
with high energy efficiency, speed, and scalability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Momeni_A/0/1/0/all/0/1"&gt;Ali Momeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleury_R/0/1/0/all/0/1"&gt;Romain Fleury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphGen-Redux: a Fast and Lightweight Recurrent Model for labeled Graph Generation. (arXiv:2107.08396v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08396</id>
        <link href="http://arxiv.org/abs/2107.08396"/>
        <updated>2021-07-20T02:04:44.455Z</updated>
        <summary type="html"><![CDATA[The problem of labeled graph generation is gaining attention in the Deep
Learning community. The task is challenging due to the sparse and discrete
nature of graph spaces. Several approaches have been proposed in the
literature, most of which require to transform the graphs into sequences that
encode their structure and labels and to learn the distribution of such
sequences through an auto-regressive generative model. Among this family of
approaches, we focus on the GraphGen model. The preprocessing phase of GraphGen
transforms graphs into unique edge sequences called Depth-First Search (DFS)
codes, such that two isomorphic graphs are assigned the same DFS code. Each
element of a DFS code is associated with a graph edge: specifically, it is a
quintuple comprising one node identifier for each of the two endpoints, their
node labels, and the edge label. GraphGen learns to generate such sequences
auto-regressively and models the probability of each component of the quintuple
independently. While effective, the independence assumption made by the model
is too loose to capture the complex label dependencies of real-world graphs
precisely. By introducing a novel graph preprocessing approach, we are able to
process the labeling information of both nodes and edges jointly. The
corresponding model, which we term GraphGen-Redux, improves upon the generative
performances of GraphGen in a wide range of datasets of chemical and social
graphs. In addition, it uses approximately 78% fewer parameters than the
vanilla variant and requires 50% fewer epochs of training on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Podda_M/0/1/0/all/0/1"&gt;Marco Podda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Theory of PAC Learnability of Partial Concept Classes. (arXiv:2107.08444v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08444</id>
        <link href="http://arxiv.org/abs/2107.08444"/>
        <updated>2021-07-20T02:04:44.435Z</updated>
        <summary type="html"><![CDATA[We extend the theory of PAC learning in a way which allows to model a rich
variety of learning tasks where the data satisfy special properties that ease
the learning process. For example, tasks where the distance of the data from
the decision boundary is bounded away from zero. The basic and simple idea is
to consider partial concepts: these are functions that can be undefined on
certain parts of the space. When learning a partial concept, we assume that the
source distribution is supported only on points where the partial concept is
defined.

This way, one can naturally express assumptions on the data such as lying on
a lower dimensional surface or margin conditions. In contrast, it is not at all
clear that such assumptions can be expressed by the traditional PAC theory. In
fact we exhibit easy-to-learn partial concept classes which provably cannot be
captured by the traditional PAC theory. This also resolves a question posed by
Attias, Kontorovich, and Mansour 2019.

We characterize PAC learnability of partial concept classes and reveal an
algorithmic landscape which is fundamentally different than the classical one.
For example, in the classical PAC model, learning boils down to Empirical Risk
Minimization (ERM). In stark contrast, we show that the ERM principle fails in
explaining learnability of partial concept classes. In fact, we demonstrate
classes that are incredibly easy to learn, but such that any algorithm that
learns them must use an hypothesis space with unbounded VC dimension. We also
find that the sample compression conjecture fails in this setting.

Thus, this theory features problems that cannot be represented nor solved in
the traditional way. We view this as evidence that it might provide insights on
the nature of learnability in realistic scenarios which the classical theory
fails to explain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alon_N/0/1/0/all/0/1"&gt;Noga Alon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1"&gt;Steve Hanneke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holzman_R/0/1/0/all/0/1"&gt;Ron Holzman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1"&gt;Shay Moran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-term series forecasting with Query Selector -- efficient model of sparse attention. (arXiv:2107.08687v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08687</id>
        <link href="http://arxiv.org/abs/2107.08687"/>
        <updated>2021-07-20T02:04:44.416Z</updated>
        <summary type="html"><![CDATA[Various modifications of TRANSFORMER were recently used to solve time-series
forecasting problem. We propose Query Selector - an efficient, deterministic
algorithm for sparse attention matrix. Experiments show it achieves
state-of-the art results on ETT data set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1"&gt;Jacek Klimek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1"&gt;Jakub Klimek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kraskiewicz_W/0/1/0/all/0/1"&gt;Witold Kraskiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topolewski_M/0/1/0/all/0/1"&gt;Mateusz Topolewski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Dimensional Simulation Optimization via Brownian Fields and Sparse Grids. (arXiv:2107.08595v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.08595</id>
        <link href="http://arxiv.org/abs/2107.08595"/>
        <updated>2021-07-20T02:04:44.400Z</updated>
        <summary type="html"><![CDATA[High-dimensional simulation optimization is notoriously challenging. We
propose a new sampling algorithm that converges to a global optimal solution
and suffers minimally from the curse of dimensionality. The algorithm consists
of two stages. First, we take samples following a sparse grid experimental
design and approximate the response surface via kernel ridge regression with a
Brownian field kernel. Second, we follow the expected improvement strategy --
with critical modifications that boost the algorithm's sample efficiency -- to
iteratively sample from the next level of the sparse grid. Under mild
conditions on the smoothness of the response surface and the simulation noise,
we establish upper bounds on the convergence rate for both noise-free and noisy
simulation samples. These upper rates deteriorate only slightly in the
dimension of the feasible set, and they can be improved if the objective
function is known be of a higher-order smoothness. Extensive numerical
experiments demonstrate that the proposed algorithm dramatically outperforms
typical alternatives in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ding_L/0/1/0/all/0/1"&gt;Liang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tuo_R/0/1/0/all/0/1"&gt;Rui Tuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaowei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured World Belief for Reinforcement Learning in POMDP. (arXiv:2107.08577v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08577</id>
        <link href="http://arxiv.org/abs/2107.08577"/>
        <updated>2021-07-20T02:04:44.344Z</updated>
        <summary type="html"><![CDATA[Object-centric world models provide structured representation of the scene
and can be an important backbone in reinforcement learning and planning.
However, existing approaches suffer in partially-observable environments due to
the lack of belief states. In this paper, we propose Structured World Belief, a
model for learning and inference of object-centric belief states. Inferred by
Sequential Monte Carlo (SMC), our belief states provide multiple object-centric
scene hypotheses. To synergize the benefits of SMC particles with object
representations, we also propose a new object-centric dynamics model that
considers the inductive bias of object permanence. This enables tracking of
object states even when they are invisible for a long time. To further
facilitate object tracking in this regime, we allow our model to attend
flexibly to any spatial location in the image which was restricted in previous
models. In experiments, we show that object-centric belief provides a more
accurate and robust performance for filtering and generation. Furthermore, we
show the efficacy of structured world belief in improving the performance of
reinforcement learning, planning and supervised reasoning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1"&gt;Gautam Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peri_S/0/1/0/all/0/1"&gt;Skand Peri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junghyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunseok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sungjin Ahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facial Expressions Recognition with Convolutional Neural Networks. (arXiv:2107.08640v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08640</id>
        <link href="http://arxiv.org/abs/2107.08640"/>
        <updated>2021-07-20T02:04:44.326Z</updated>
        <summary type="html"><![CDATA[Over the centuries, humans have developed and acquired a number of ways to
communicate. But hardly any of them can be as natural and instinctive as facial
expressions. On the other hand, neural networks have taken the world by storm.
And no surprises, that the area of Computer Vision and the problem of facial
expressions recognitions hasn't remained untouched. Although a wide range of
techniques have been applied, achieving extremely high accuracies and preparing
highly robust FER systems still remains a challenge due to heterogeneous
details in human faces. In this paper, we will be deep diving into implementing
a system for recognition of facial expressions (FER) by leveraging neural
networks, and more specifically, Convolutional Neural Networks (CNNs). We adopt
the fundamental concepts of deep learning and computer vision with various
architectures, fine-tune it's hyperparameters and experiment with various
optimization methods and demonstrate a state-of-the-art single-network-accuracy
of 70.10% on the FER2013 dataset without using any additional training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lonkar_S/0/1/0/all/0/1"&gt;Subodh Lonkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Learning Rates for Stochastic Optimization: Two Theoretical Viewpoints. (arXiv:2107.08686v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08686</id>
        <link href="http://arxiv.org/abs/2107.08686"/>
        <updated>2021-07-20T02:04:44.309Z</updated>
        <summary type="html"><![CDATA[Generalization performance of stochastic optimization stands a central place
in machine learning. In this paper, we investigate the excess risk performance
and towards improved learning rates for two popular approaches of stochastic
optimization: empirical risk minimization (ERM) and stochastic gradient descent
(SGD). Although there exists plentiful generalization analysis of ERM and SGD
for supervised learning, current theoretical understandings of ERM and SGD are
either have stronger assumptions in convex learning, e.g., strong convexity
condition, or show slow rates and less studied in nonconvex learning. Motivated
by these problems, we aim to provide improved rates under milder assumptions in
convex learning and derive faster rates in nonconvex learning. It is notable
that our analysis span two popular theoretical viewpoints: stability and
uniform convergence. To be specific, in stability regime, we present high
probability rates of order $\mathcal{O} (1/n)$ w.r.t. the sample size $n$ for
ERM and SGD with milder assumptions in convex learning and similar high
probability rates of order $\mathcal{O} (1/n)$ in nonconvex learning, rather
than in expectation. Furthermore, this type of learning rate is improved to
faster order $\mathcal{O} (1/n^2)$ in uniform convergence regime. To the best
of our knowledge, for ERM and SGD, the learning rates presented in this paper
are all state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaojie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08661</id>
        <link href="http://arxiv.org/abs/2107.08661"/>
        <updated>2021-07-20T02:04:44.288Z</updated>
        <summary type="html"><![CDATA[We present Translatotron 2, a neural direct speech-to-speech translation
model that can be trained end-to-end. Translatotron 2 consists of a speech
encoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention
module that connects all the previous three components. Experimental results
suggest that Translatotron 2 outperforms the original Translatotron by a large
margin in terms of translation quality and predicted speech naturalness, and
drastically improves the robustness of the predicted speech by mitigating
over-generation, such as babbling or long pause. We also propose a new method
for retaining the source speaker's voice in the translated speech. The trained
model is restricted to retain the source speaker's voice, and unlike the
original Translatotron, it is not able to generate speech in a different
speaker's voice, making the model more robust for production deployment, by
mitigating potential misuse for creating spoofing audio artifacts. When the new
method is used together with a simple concatenation-based data augmentation,
the trained Translatotron 2 model is able to retain each speaker's voice for
input with speaker turns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Ye Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1"&gt;Michelle Tadmor Ramanovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1"&gt;Tal Remez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1"&gt;Roi Pomerantz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A note on the article "On Exploiting Spectral Properties for Solving MDP with Large State Space". (arXiv:2107.08488v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08488</id>
        <link href="http://arxiv.org/abs/2107.08488"/>
        <updated>2021-07-20T02:04:44.270Z</updated>
        <summary type="html"><![CDATA[We improve a theoretical result of the article "On Exploiting Spectral
Properties for Solving MDP with Large State Space" showing that their
algorithm, which was proved to converge under some unrealistic assumptions, is
actually guaranteed to converge always.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maran_D/0/1/0/all/0/1"&gt;D. Maran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Topological Perspective on Causal Inference. (arXiv:2107.08558v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.08558</id>
        <link href="http://arxiv.org/abs/2107.08558"/>
        <updated>2021-07-20T02:04:44.206Z</updated>
        <summary type="html"><![CDATA[This paper presents a topological learning-theoretic perspective on causal
inference by introducing a series of topologies defined on general spaces of
structural causal models (SCMs). As an illustration of the framework we prove a
topological causal hierarchy theorem, showing that substantive assumption-free
causal inference is possible only in a meager set of SCMs. Thanks to a known
correspondence between open sets in the weak topology and statistically
verifiable hypotheses, our results show that inductive assumptions sufficient
to license valid causal inferences are statistically unverifiable in principle.
Similar to no-free-lunch theorems for statistical inference, the present
results clarify the inevitability of substantial assumptions for causal
inference. An additional benefit of our topological approach is that it easily
accommodates SCMs with infinitely many variables. We finally suggest that the
framework may be helpful for the positive project of exploring and assessing
alternative causal-inductive assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibeling_D/0/1/0/all/0/1"&gt;Duligur Ibeling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1"&gt;Thomas Icard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Efficient Multi-Task Reinforcement Learning with Model Transfer. (arXiv:2107.08622v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08622</id>
        <link href="http://arxiv.org/abs/2107.08622"/>
        <updated>2021-07-20T02:04:44.188Z</updated>
        <summary type="html"><![CDATA[We study multi-task reinforcement learning (RL) in tabular episodic Markov
decision processes (MDPs). We formulate a heterogeneous multi-player RL
problem, in which a group of players concurrently face similar but not
necessarily identical MDPs, with a goal of improving their collective
performance through inter-player information sharing. We design and analyze an
algorithm based on the idea of model transfer, and provide gap-dependent and
gap-independent upper and lower bounds that characterize the intrinsic
complexity of the problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chicheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues. (arXiv:2107.08574v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08574</id>
        <link href="http://arxiv.org/abs/2107.08574"/>
        <updated>2021-07-20T02:04:44.170Z</updated>
        <summary type="html"><![CDATA[Data quality is a common problem in machine learning, especially in
high-stakes settings such as healthcare. Missing data affects accuracy,
calibration, and feature attribution in complex patterns. Developers often
train models on carefully curated datasets to minimize missing data bias;
however, this reduces the usability of such models in production environments,
such as real-time healthcare records. Making machine learning models robust to
missing data is therefore crucial for practical application. While some
classifiers naturally handle missing data, others, such as deep neural
networks, are not designed for unknown values. We propose a novel neural
network modification to mitigate the impacts of missing data. The approach is
inspired by neuromodulation that is performed by biological neural networks.
Our proposal replaces the fixed weights of a fully-connected layer with a
function of an additional input (reliability score) at each input, mimicking
the ability of cortex to up- and down-weight inputs based on the presence of
other data. The modulation function is jointly learned with the main task using
a multi-layer perceptron. We tested our modulating fully connected layer on
multiple classification, regression, and imputation problems, and it either
improved performance or generated comparable performance to conventional neural
network architectures concatenating reliability to the inputs. Models with
modulating layers were more robust against degradation of data quality by
introducing additional missingness at evaluation time. These results suggest
that explicitly accounting for reduced information quality with a modulating
fully connected layer can enable the deployment of artificial intelligence
systems in real-time settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdelhack_M/0/1/0/all/0/1"&gt;Mohamed Abdelhack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Sandhya Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fritz_B/0/1/0/all/0/1"&gt;Bradley Fritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avidan_M/0/1/0/all/0/1"&gt;Michael Avidan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yixin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_C/0/1/0/all/0/1"&gt;Christopher King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deceptive Logic Locking for Hardware Integrity Protection against Machine Learning Attacks. (arXiv:2107.08695v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.08695</id>
        <link href="http://arxiv.org/abs/2107.08695"/>
        <updated>2021-07-20T02:04:44.143Z</updated>
        <summary type="html"><![CDATA[Logic locking has emerged as a prominent key-driven technique to protect the
integrity of integrated circuits. However, novel machine-learning-based attacks
have recently been introduced to challenge the security foundations of locking
schemes. These attacks are able to recover a significant percentage of the key
without having access to an activated circuit. This paper address this issue
through two focal points. First, we present a theoretical model to test locking
schemes for key-related structural leakage that can be exploited by machine
learning. Second, based on the theoretical model, we introduce D-MUX: a
deceptive multiplexer-based logic-locking scheme that is resilient against
structure-exploiting machine learning attacks. Through the design of D-MUX, we
uncover a major fallacy in existing multiplexer-based locking schemes in the
form of a structural-analysis attack. Finally, an extensive cost evaluation of
D-MUX is presented. To the best of our knowledge, D-MUX is the first
machine-learning-resilient locking scheme capable of protecting against all
known learning-based attacks. Hereby, the presented work offers a starting
point for the design and evaluation of future-generation logic locking in the
era of machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sisejkovic_D/0/1/0/all/0/1"&gt;Dominik Sisejkovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merchant_F/0/1/0/all/0/1"&gt;Farhad Merchant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reimann_L/0/1/0/all/0/1"&gt;Lennart M. Reimann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leupers_R/0/1/0/all/0/1"&gt;Rainer Leupers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Distributed Method for Training Generative Adversarial Networks. (arXiv:2107.08681v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08681</id>
        <link href="http://arxiv.org/abs/2107.08681"/>
        <updated>2021-07-20T02:04:44.124Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are emerging machine learning models
for generating synthesized data similar to real data by jointly training a
generator and a discriminator. In many applications, data and computational
resources are distributed over many devices, so centralized computation with
all data in one location is infeasible due to privacy and/or communication
constraints. This paper proposes a new framework for training GANs in a
distributed fashion: Each device computes a local discriminator using local
data; a single server aggregates their results and computes a global GAN.
Specifically, in each iteration, the server sends the global GAN to the
devices, which then update their local discriminators; the devices send their
results to the server, which then computes their average as the global
discriminator and updates the global generator accordingly. Two different
update schedules are designed with different levels of parallelism between the
devices and the server. Numerical results obtained using three popular datasets
demonstrate that the proposed framework can outperform a state-of-the-art
framework in terms of convergence speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jinke Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chonghe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1"&gt;Guanding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1"&gt;Dongning Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Resource Allocation for Serverless Queries. (arXiv:2107.08594v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2107.08594</id>
        <link href="http://arxiv.org/abs/2107.08594"/>
        <updated>2021-07-20T02:04:44.093Z</updated>
        <summary type="html"><![CDATA[Optimizing resource allocation for analytical workloads is vital for reducing
costs of cloud-data services. At the same time, it is incredibly hard for users
to allocate resources per query in serverless processing systems, and they
frequently misallocate by orders of magnitude. Unfortunately, prior work
focused on predicting peak allocation while ignoring aggressive trade-offs
between resource allocation and run-time. Additionally, these methods fail to
predict allocation for queries that have not been observed in the past. In this
paper, we tackle both these problems. We introduce a system for optimal
resource allocation that can predict performance with aggressive trade-offs,
for both new and past observed queries. We introduce the notion of a
performance characteristic curve (PCC) as a parameterized representation that
can compactly capture the relationship between resources and performance. To
tackle training data sparsity, we introduce a novel data augmentation technique
to efficiently synthesize the entire PCC using a single run of the query.
Lastly, we demonstrate the advantages of a constrained loss function coupled
with GNNs, over traditional ML methods, for capturing the domain specific
behavior through an extensive experimental evaluation over SCOPE big data
workloads at Microsoft.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pimpley_A/0/1/0/all/0/1"&gt;Anish Pimpley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Anubha Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohra_V/0/1/0/all/0/1"&gt;Vishal Rohra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1"&gt;Soundararajan Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jindal_A/0/1/0/all/0/1"&gt;Alekh Jindal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1"&gt;Hiren Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1"&gt;Shi Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_R/0/1/0/all/0/1"&gt;Rathijit Sen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chef: a cheap and fast pipeline for iteratively cleaning label uncertainties. (arXiv:2107.08588v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2107.08588</id>
        <link href="http://arxiv.org/abs/2107.08588"/>
        <updated>2021-07-20T02:04:44.023Z</updated>
        <summary type="html"><![CDATA[High-quality labels are expensive to obtain for many machine learning tasks,
such as medical image classification tasks. Therefore, probabilistic (weak)
labels produced by weak supervision tools are used to seed a process in which
influential samples with weak labels are identified and cleaned by several
human annotators to improve the model performance. To lower the overall cost
and computational overhead of this process, we propose a solution called
Chef(CHEap and Fast label cleaning), which consists of the following three
components. First, to reduce the cost of human annotators, we use Infl, which
prioritizes the most influential training samples for cleaning and provides
cleaned labels to save the cost of one human annotator. Second, to accelerate
the sample selector phase and the model constructor phase, we use Increm-Infl
to incrementally produce influential samples, and DeltaGrad-L to incrementally
update the model. Third, we redesign the typical label cleaning pipeline so
that human annotators iteratively clean smaller batch of samples rather than
one big batch of samples. This yields better over all model performance and
enables possible early termination when the expected model performance has been
achieved. Extensive experiments show that our approach gives good model
prediction performance while achieving significant speed-ups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yinjun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weimer_J/0/1/0/all/0/1"&gt;James Weimer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davidson_S/0/1/0/all/0/1"&gt;Susan B. Davidson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Input Agnostic Deep Learning for Alzheimer's Disease Classification Using Multimodal MRI Images. (arXiv:2107.08673v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08673</id>
        <link href="http://arxiv.org/abs/2107.08673"/>
        <updated>2021-07-20T02:04:44.004Z</updated>
        <summary type="html"><![CDATA[Alzheimer's disease (AD) is a progressive brain disorder that causes memory
and functional impairments. The advances in machine learning and publicly
available medical datasets initiated multiple studies in AD diagnosis. In this
work, we utilize a multi-modal deep learning approach in classifying normal
cognition, mild cognitive impairment and AD classes on the basis of structural
MRI and diffusion tensor imaging (DTI) scans from the OASIS-3 dataset. In
addition to a conventional multi-modal network, we also present an input
agnostic architecture that allows diagnosis with either sMRI or DTI scan, which
distinguishes our method from previous multi-modal machine learning-based
methods. The results show that the input agnostic model achieves 0.96 accuracy
when both structural MRI and DTI scans are provided as inputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Massalimova_A/0/1/0/all/0/1"&gt;Aidana Massalimova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1"&gt;Huseyin Atakan Varol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A method for estimating the entropy of time series using artificial neural network. (arXiv:2107.08399v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08399</id>
        <link href="http://arxiv.org/abs/2107.08399"/>
        <updated>2021-07-20T02:04:43.986Z</updated>
        <summary type="html"><![CDATA[Measuring the predictability and complexity of time series is an essential
tool in designing and controlling the nonlinear system. There exist different
entropy measures in the literature to analyze the predictability and complexity
of time series. However, these measures have some drawbacks especially in short
time series. To overcome the difficulties, this paper proposes a new method for
estimating the entropy of a time series using the LogNNet 784:25:10 neural
network model. The LogNNet reservoir matrix consists of 19625 elements which is
filled with the time series elements. After that, the network is trained on
MNIST-10 dataset and the classification accuracy is calculated. The accuracy is
considered as the entropy measure and denoted by NNetEn. A more complex
transformation of the input information by the time series in the reservoir
leads to higher NNetEn values. Many practical time series data have less than
19625 elements. Some duplicating or stretching methods are investigated to
overcome this difficulty and the most successful method is identified for
practical applications. The epochs number in the training process of LogNNet is
considered as the input parameter. A new time series characteristic called time
series learning inertia is introduced to investigate the effect of epochs
number in the efficiency of neural network. To show the robustness and
efficiency of the proposed method, it is applied on some chaotic, periodic,
random, binary and constant time series. The NNetEn is compared with some
existing entropy measures. The results show that the proposed method is more
robust and accurate than existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Velichko_A/0/1/0/all/0/1"&gt;Andrei Velichko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1"&gt;Hanif Heidari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Adaptive Gradient Method with Gradient Decomposition. (arXiv:2107.08377v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08377</id>
        <link href="http://arxiv.org/abs/2107.08377"/>
        <updated>2021-07-20T02:04:43.967Z</updated>
        <summary type="html"><![CDATA[Adaptive gradient methods, especially Adam-type methods (such as Adam,
AMSGrad, and AdaBound), have been proposed to speed up the training process
with an element-wise scaling term on learning rates. However, they often
generalize poorly compared with stochastic gradient descent (SGD) and its
accelerated schemes such as SGD with momentum (SGDM). In this paper, we propose
a new adaptive method called DecGD, which simultaneously achieves good
generalization like SGDM and obtain rapid convergence like Adam-type methods.
In particular, DecGD decomposes the current gradient into the product of two
terms including a surrogate gradient and a loss based vector. Our method
adjusts the learning rates adaptively according to the current loss based
vector instead of the squared gradients used in Adam-type methods. The
intuition for adaptive learning rates of DecGD is that a good optimizer, in
general cases, needs to decrease the learning rates as the loss decreases,
which is similar to the learning rates decay scheduling technique. Therefore,
DecGD gets a rapid convergence in the early phases of training and controls the
effective learning rates according to the loss based vectors which help lead to
a better generalization. Convergence analysis is discussed in both convex and
non-convex situations. Finally, empirical results on widely-used tasks and
models demonstrate that DecGD shows better generalization performance than SGDM
and rapid convergence like Adam-type methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhou Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tong Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Deep Learning: Sampling Neural Nets with a Quantum Annealer. (arXiv:2107.08710v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.08710</id>
        <link href="http://arxiv.org/abs/2107.08710"/>
        <updated>2021-07-20T02:04:43.913Z</updated>
        <summary type="html"><![CDATA[We demonstrate the feasibility of framing a classically learned deep neural
network as an energy based model that can be processed on a one-step quantum
annealer in order to exploit fast sampling times. We propose approaches to
overcome two hurdles for high resolution image classification on a quantum
processing unit (QPU): the required number and binary nature of the model
states. With this novel method we successfully transfer a convolutional neural
network to the QPU and show the potential for classification speedup of at
least one order of magnitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Higham_C/0/1/0/all/0/1"&gt;Catherine F. Higham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Bedford_A/0/1/0/all/0/1"&gt;Adrian Bedford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariant Manifold Flows. (arXiv:2107.08596v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.08596</id>
        <link href="http://arxiv.org/abs/2107.08596"/>
        <updated>2021-07-20T02:04:43.826Z</updated>
        <summary type="html"><![CDATA[Tractably modelling distributions over manifolds has long been an important
goal in the natural sciences. Recent work has focused on developing general
machine learning models to learn such distributions. However, for many
applications these distributions must respect manifold symmetries -- a trait
which most previous models disregard. In this paper, we lay the theoretical
foundations for learning symmetry-invariant distributions on arbitrary
manifolds via equivariant manifold flows. We demonstrate the utility of our
approach by using it to learn gauge invariant densities over $SU(n)$ in the
context of quantum field theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Katsman_I/0/1/0/all/0/1"&gt;Isay Katsman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lou_A/0/1/0/all/0/1"&gt;Aaron Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lim_D/0/1/0/all/0/1"&gt;Derek Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jiang_Q/0/1/0/all/0/1"&gt;Qingxuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lim_S/0/1/0/all/0/1"&gt;Ser-Nam Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Skill-Discovery and Skill-Learning in Minecraft. (arXiv:2107.08398v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.08398</id>
        <link href="http://arxiv.org/abs/2107.08398"/>
        <updated>2021-07-20T02:04:43.808Z</updated>
        <summary type="html"><![CDATA[Pre-training Reinforcement Learning agents in a task-agnostic manner has
shown promising results. However, previous works still struggle in learning and
discovering meaningful skills in high-dimensional state-spaces, such as
pixel-spaces. We approach the problem by leveraging unsupervised skill
discovery and self-supervised learning of state representations. In our work,
we learn a compact latent representation by making use of variational and
contrastive techniques. We demonstrate that both enable RL agents to learn a
set of basic navigation skills by maximizing an information theoretic
objective. We assess our method in Minecraft 3D pixel maps with different
complexities. Our results show that representations and conditioned policies
learned from pixels are enough for toy examples, but do not scale to realistic
and complex maps. To overcome these limitations, we explore alternative input
observations such as the relative position of the agent along with the raw
pixels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nieto_J/0/1/0/all/0/1"&gt;Juan Jos&amp;#xe9; Nieto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Creus_R/0/1/0/all/0/1"&gt;Roger Creus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1"&gt;Xavier Giro-i-Nieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Support vector machines for learning reactive islands. (arXiv:2107.08429v1 [math.DS])]]></title>
        <id>http://arxiv.org/abs/2107.08429</id>
        <link href="http://arxiv.org/abs/2107.08429"/>
        <updated>2021-07-20T02:04:43.788Z</updated>
        <summary type="html"><![CDATA[We develop a machine learning framework that can be applied to data sets
derived from the trajectories of Hamilton's equations. The goal is to learn the
phase space structures that play the governing role for phase space transport
relevant to particular applications. Our focus is on learning reactive islands
in two degrees-of-freedom Hamiltonian systems. Reactive islands are constructed
from the stable and unstable manifolds of unstable periodic orbits and play the
role of quantifying transition dynamics. We show that support vector machines
(SVM) is an appropriate machine learning framework for this purpose as it
provides an approach for finding the boundaries between qualitatively distinct
dynamical behaviors, which is in the spirit of the phase space transport
framework. We show how our method allows us to find reactive islands directly
in the sense that we do not have to first compute unstable periodic orbits and
their stable and unstable manifolds. We apply our approach to the
H\'enon-Heiles Hamiltonian system, which is a benchmark system in the dynamical
systems community. We discuss different sampling and learning approaches and
their advantages and disadvantages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Naik_S/0/1/0/all/0/1"&gt;Shibabrat Naik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Krajnak_V/0/1/0/all/0/1"&gt;Vladim&amp;#xed;r Kraj&amp;#x148;&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wiggins_S/0/1/0/all/0/1"&gt;Stephen Wiggins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early-Phase Performance-Driven Design using Generative Models. (arXiv:2107.08572v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08572</id>
        <link href="http://arxiv.org/abs/2107.08572"/>
        <updated>2021-07-20T02:04:43.769Z</updated>
        <summary type="html"><![CDATA[Current performance-driven building design methods are not widely adopted
outside the research field for several reasons that make them difficult to
integrate into a typical design process. In the early design phase, in
particular, the time-intensity and the cognitive load associated with
optimization and form parametrization are incompatible with design exploration,
which requires quick iteration. This research introduces a novel method for
performance-driven geometry generation that can afford interaction directly in
the 3d modeling environment, eliminating the need for explicit parametrization,
and is multiple orders faster than the equivalent form optimization. The method
uses Machine Learning techniques to train a generative model offline. The
generative model learns a distribution of optimal performing geometries and
their simulation contexts based on a dataset that addresses the performance(s)
of interest. By navigating the generative model's latent space, geometries with
the desired characteristics can be quickly generated. A case study is
presented, demonstrating the generation of a synthetic dataset and the use of a
Variational Autoencoder (VAE) as a generative model for geometries with optimal
solar gain. The results show that the VAE-generated geometries perform on
average at least as well as the optimized ones, suggesting that the introduced
method shows a feasible path towards more intuitive and interactive early-phase
performance-driven design assistance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ampanavos_S/0/1/0/all/0/1"&gt;Spyridon Ampanavos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malkawi_A/0/1/0/all/0/1"&gt;Ali Malkawi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ANFIC: Image Compression Using Augmented Normalizing Flows. (arXiv:2107.08470v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08470</id>
        <link href="http://arxiv.org/abs/2107.08470"/>
        <updated>2021-07-20T02:04:43.752Z</updated>
        <summary type="html"><![CDATA[This paper introduces an end-to-end learned image compression system, termed
ANFIC, based on Augmented Normalizing Flows (ANF). ANF is a new type of flow
model, which stacks multiple variational autoencoders (VAE) for greater model
expressiveness. The VAE-based image compression has gone mainstream, showing
promising compression performance. Our work presents the first attempt to
leverage VAE-based compression in a flow-based framework. ANFIC advances
further compression efficiency by stacking and extending hierarchically
multiple VAE's. The invertibility of ANF, together with our training
strategies, enables ANFIC to support a wide range of quality levels without
changing the encoding and decoding networks. Extensive experimental results
show that in terms of PSNR-RGB, ANFIC performs comparably to or better than the
state-of-the-art learned image compression. Moreover, it performs close to VVC
intra coding, from low-rate compression up to nearly-lossless compression. In
particular, ANFIC achieves the state-of-the-art performance, when extended with
conditional convolution for variable rate compression with a single model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ho_Y/0/1/0/all/0/1"&gt;Yung-Han Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1"&gt;Chih-Chun Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wen-Hsiao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hang_H/0/1/0/all/0/1"&gt;Hsueh-Ming Hang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Domanski_M/0/1/0/all/0/1"&gt;Marek Domanski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A High-Performance Adaptive Quantization Approach for Edge CNN Applications. (arXiv:2107.08382v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08382</id>
        <link href="http://arxiv.org/abs/2107.08382"/>
        <updated>2021-07-20T02:04:43.694Z</updated>
        <summary type="html"><![CDATA[Recent convolutional neural network (CNN) development continues to advance
the state-of-the-art model accuracy for various applications. However, the
enhanced accuracy comes at the cost of substantial memory bandwidth and storage
requirements and demanding computational resources. Although in the past the
quantization methods have effectively reduced the deployment cost for edge
devices, it suffers from significant information loss when processing the
biased activations of contemporary CNNs. In this paper, we hence introduce an
adaptive high-performance quantization method to resolve the issue of biased
activation by dynamically adjusting the scaling and shifting factors based on
the task loss. Our proposed method has been extensively evaluated on image
classification models (ResNet-18/34/50, MobileNet-V2, EfficientNet-B0) with
ImageNet dataset, object detection model (YOLO-V4) with COCO dataset, and
language models with PTB dataset. The results show that our 4-bit integer
(INT4) quantization models achieve better accuracy than the state-of-the-art
4-bit models, and in some cases, even surpass the golden full-precision models.
The final designs have been successfully deployed onto extremely
resource-constrained edge devices for many practical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chin_H/0/1/0/all/0/1"&gt;Hsu-Hsun Chin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsay_R/0/1/0/all/0/1"&gt;Ren-Song Tsay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hsin-I Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Process Predictions using a Milestone-Aware Counterfactual Approach. (arXiv:2107.08697v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08697</id>
        <link href="http://arxiv.org/abs/2107.08697"/>
        <updated>2021-07-20T02:04:43.676Z</updated>
        <summary type="html"><![CDATA[Predictive process analytics often apply machine learning to predict the
future states of a running business process. However, the internal mechanisms
of many existing predictive algorithms are opaque and a human decision-maker is
unable to understand \emph{why} a certain activity was predicted. Recently,
counterfactuals have been proposed in the literature to derive
human-understandable explanations from predictive models. Current
counterfactual approaches consist of finding the minimum feature change that
can make a certain prediction flip its outcome. Although many algorithms have
been proposed, their application to the sequence and multi-dimensional data
like event logs has not been explored in the literature.

In this paper, we explore the use of a recent, popular model-agnostic
counterfactual algorithm, DiCE, in the context of predictive process analytics.
The analysis reveals that the algorithm is limited when being applied to derive
explanations of process predictions, due to (1) process domain knowledge not
being taken into account, (2) long traces that often tend to be less
understandable, and (3) difficulties in optimising the counterfactual search
with categorical variables. We design an extension of DiCE that can generate
counterfactuals for process predictions, and propose an approach that supports
deriving milestone-aware counterfactuals at different stages of a trace to
promote interpretability. We apply our approach to BPIC2012 event log and the
analysis results demonstrate the effectiveness of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Chihcheng Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreira_C/0/1/0/all/0/1"&gt;Catarina Moreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1"&gt;Chun Ouyang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Train on Small, Play the Large: Scaling Up Board Games with AlphaZero and GNN. (arXiv:2107.08387v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08387</id>
        <link href="http://arxiv.org/abs/2107.08387"/>
        <updated>2021-07-20T02:04:43.657Z</updated>
        <summary type="html"><![CDATA[Playing board games is considered a major challenge for both humans and AI
researchers. Because some complicated board games are quite hard to learn,
humans usually begin with playing on smaller boards and incrementally advance
to master larger board strategies. Most neural network frameworks that are
currently tasked with playing board games neither perform such incremental
learning nor possess capabilities to automatically scale up. In this work, we
look at the board as a graph and combine a graph neural network architecture
inside the AlphaZero framework, along with some other innovative improvements.
Our ScalableAlphaZero is capable of learning to play incrementally on small
boards, and advancing to play on large ones. Our model can be trained quickly
to play different challenging board games on multiple board sizes, without
using any domain knowledge. We demonstrate the effectiveness of
ScalableAlphaZero and show, for example, that by training it for only three
days on small Othello boards, it can defeat the AlphaZero model on a large
board, which was trained to play the large board for $30$ days.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Assayag_S/0/1/0/all/0/1"&gt;Shai Ben-Assayag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Yaniv_R/0/1/0/all/0/1"&gt;Ran El-Yaniv&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-To-Ensemble by Contextual Rank Aggregation in E-Commerce. (arXiv:2107.08598v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08598</id>
        <link href="http://arxiv.org/abs/2107.08598"/>
        <updated>2021-07-20T02:04:43.636Z</updated>
        <summary type="html"><![CDATA[Ensemble models in E-commerce combine predictions from multiple sub-models
for ranking and revenue improvement. Industrial ensemble models are typically
deep neural networks, following the supervised learning paradigm to infer
conversion rate given inputs from sub-models. However, this process has the
following two problems. Firstly, the point-wise scoring approach disregards the
relationships between items and leads to homogeneous displayed results, while
diversified display benefits user experience and revenue. Secondly, the
learning paradigm focuses on the ranking metrics and does not directly optimize
the revenue. In our work, we propose a new Learning-To-Ensemble (LTE) framework
RAEGO, which replaces the ensemble model with a contextual Rank Aggregator (RA)
and explores the best weights of sub-models by the Evaluator-Generator
Optimization (EGO). To achieve the best online performance, we propose a new
rank aggregation algorithm TournamentGreedy as a refinement of classic rank
aggregators, which also produces the best average weighted Kendall Tau Distance
(KTD) amongst all the considered algorithms with quadratic time complexity.
Under the assumption that the best output list should be Pareto Optimal on the
KTD metric for sub-models, we show that our RA algorithm has higher efficiency
and coverage in exploring the optimal weights. Combined with the idea of
Bayesian Optimization and gradient descent, we solve the online contextual
Black-Box Optimization task that finds the optimal weights for sub-models given
a chosen RA model. RA-EGO has been deployed in our online system and has
improved the revenue significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuesi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huzhang_G/0/1/0/all/0/1"&gt;Guangda Huzhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1"&gt;Qianying Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1"&gt;Qing Da&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1"&gt;Dan Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized federated learning of deep neural networks on non-iid data. (arXiv:2107.08517v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08517</id>
        <link href="http://arxiv.org/abs/2107.08517"/>
        <updated>2021-07-20T02:04:43.618Z</updated>
        <summary type="html"><![CDATA[We tackle the non-convex problem of learning a personalized deep learning
model in a decentralized setting. More specifically, we study decentralized
federated learning, a peer-to-peer setting where data is distributed among many
clients and where there is no central server to orchestrate the training. In
real world scenarios, the data distributions are often heterogeneous between
clients. Therefore, in this work we study the problem of how to efficiently
learn a model in a peer-to-peer system with non-iid client data. We propose a
method named Performance-Based Neighbor Selection (PENS) where clients with
similar data distributions detect each other and cooperate by evaluating their
training losses on each other's data to learn a model suitable for the local
data distribution. Our experiments on benchmark datasets show that our proposed
method is able to achieve higher accuracies as compared to strong baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Onoszko_N/0/1/0/all/0/1"&gt;Noa Onoszko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlsson_G/0/1/0/all/0/1"&gt;Gustav Karlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mogren_O/0/1/0/all/0/1"&gt;Olof Mogren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zec_E/0/1/0/all/0/1"&gt;Edvin Listo Zec&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A stepped sampling method for video detection using LSTM. (arXiv:2107.08471v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08471</id>
        <link href="http://arxiv.org/abs/2107.08471"/>
        <updated>2021-07-20T02:04:43.555Z</updated>
        <summary type="html"><![CDATA[Artificial neural networks that simulate human achieves great successes. From
the perspective of simulating human memory method, we propose a stepped sampler
based on the "repeated input". We repeatedly inputted data to the LSTM model
stepwise in a batch. The stepped sampler is used to strengthen the ability of
fusing the temporal information in LSTM. We tested the stepped sampler on the
LSTM built-in in PyTorch. Compared with the traditional sampler of PyTorch,
such as sequential sampler, batch sampler, the training loss of the proposed
stepped sampler converges faster in the training of the model, and the training
loss after convergence is more stable. Meanwhile, it can maintain a higher test
accuracy. We quantified the algorithm of the stepped sampler.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dengshan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rujing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chengjun Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Representation of Successor Features for Transfer across Dissimilar Environments. (arXiv:2107.08426v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08426</id>
        <link href="http://arxiv.org/abs/2107.08426"/>
        <updated>2021-07-20T02:04:43.537Z</updated>
        <summary type="html"><![CDATA[Transfer in reinforcement learning is usually achieved through generalisation
across tasks. Whilst many studies have investigated transferring knowledge when
the reward function changes, they have assumed that the dynamics of the
environments remain consistent. Many real-world RL problems require transfer
among environments with different dynamics. To address this problem, we propose
an approach based on successor features in which we model successor feature
functions with Gaussian Processes permitting the source successor features to
be treated as noisy measurements of the target successor feature function. Our
theoretical analysis proves the convergence of this approach as well as the
bounded error on modelling successor feature functions with Gaussian Processes
in environments with both different dynamics and rewards. We demonstrate our
method on benchmark datasets and show that it outperforms current baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdolshah_M/0/1/0/all/0/1"&gt;Majid Abdolshah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hung Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1"&gt;Thommen Karimpanal George&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sunil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rana_S/0/1/0/all/0/1"&gt;Santu Rana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1"&gt;Svetha Venkatesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GuideBoot: Guided Bootstrap for Deep Contextual Bandits. (arXiv:2107.08383v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08383</id>
        <link href="http://arxiv.org/abs/2107.08383"/>
        <updated>2021-07-20T02:04:43.515Z</updated>
        <summary type="html"><![CDATA[The exploration/exploitation (E&E) dilemma lies at the core of interactive
systems such as online advertising, for which contextual bandit algorithms have
been proposed. Bayesian approaches provide guided exploration with principled
uncertainty estimation, but the applicability is often limited due to
over-simplified assumptions. Non-Bayesian bootstrap methods, on the other hand,
can apply to complex problems by using deep reward models, but lacks clear
guidance to the exploration behavior. It still remains largely unsolved to
develop a practical method for complex deep contextual bandits.

In this paper, we introduce Guided Bootstrap (GuideBoot for short), combining
the best of both worlds. GuideBoot provides explicit guidance to the
exploration behavior by training multiple models over both real samples and
noisy samples with fake labels, where the noise is added according to the
predictive uncertainty. The proposed method is efficient as it can make
decisions on-the-fly by utilizing only one randomly chosen model, but is also
effective as we show that it can be viewed as a non-Bayesian approximation of
Thompson sampling. Moreover, we extend it to an online version that can learn
solely from streaming data, which is favored in real applications. Extensive
experiments on both synthetic task and large-scale advertising environments
show that GuideBoot achieves significant improvements against previous
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Feiyang Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haoming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1"&gt;Xiang Ao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yanrong Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1"&gt;Ao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1"&gt;Qing He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Experimental Study of Data Heterogeneity in Federated Learning Methods for Medical Imaging. (arXiv:2107.08371v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08371</id>
        <link href="http://arxiv.org/abs/2107.08371"/>
        <updated>2021-07-20T02:04:43.497Z</updated>
        <summary type="html"><![CDATA[Federated learning enables multiple institutions to collaboratively train
machine learning models on their local data in a privacy-preserving way.
However, its distributed nature often leads to significant heterogeneity in
data distributions across institutions. In this paper, we investigate the
deleterious impact of a taxonomy of data heterogeneity regimes on federated
learning methods, including quantity skew, label distribution skew, and imaging
acquisition skew. We show that the performance degrades with the increasing
degrees of data heterogeneity. We present several mitigation strategies to
overcome performance drops from data heterogeneity, including weighted average
for data quantity skew, weighted loss and batch normalization averaging for
label distribution skew. The proposed optimizations to federated learning
methods improve their capability of handling heterogeneity across institutions,
which provides valuable guidance for the deployment of federated learning in
real clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Liangqiong Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balachandar_N/0/1/0/all/0/1"&gt;Niranjan Balachandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GoTube: Scalable Stochastic Verification of Continuous-Depth Models. (arXiv:2107.08467v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08467</id>
        <link href="http://arxiv.org/abs/2107.08467"/>
        <updated>2021-07-20T02:04:43.457Z</updated>
        <summary type="html"><![CDATA[We introduce a new stochastic verification algorithm that formally quantifies
the behavioral robustness of any time-continuous process formulated as a
continuous-depth model. The algorithm solves a set of global optimization (Go)
problems over a given time horizon to construct a tight enclosure (Tube) of the
set of all process executions starting from a ball of initial states. We call
our algorithm GoTube. Through its construction, GoTube ensures that the
bounding tube is conservative up to a desired probability. GoTube is
implemented in JAX and optimized to scale to complex continuous-depth models.
Compared to advanced reachability analysis tools for time-continuous neural
networks, GoTube provably does not accumulate over-approximation errors between
time steps and avoids the infamous wrapping effect inherent in symbolic
techniques. We show that GoTube substantially outperforms state-of-the-art
verification tools in terms of the size of the initial ball, speed,
time-horizon, task completion, and scalability, on a large set of experiments.
GoTube is stable and sets the state-of-the-art for its ability to scale up to
time horizons well beyond what has been possible before.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gruenbacher_S/0/1/0/all/0/1"&gt;Sophie Gruenbacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1"&gt;Mathias Lechner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1"&gt;Ramin Hasani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1"&gt;Thomas A. Henzinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smolka_S/0/1/0/all/0/1"&gt;Scott Smolka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1"&gt;Radu Grosu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structural Design Recommendations in the Early Design Phase using Machine Learning. (arXiv:2107.08567v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08567</id>
        <link href="http://arxiv.org/abs/2107.08567"/>
        <updated>2021-07-20T02:04:43.390Z</updated>
        <summary type="html"><![CDATA[Structural engineering knowledge can be of significant importance to the
architectural design team during the early design phase. However, architects
and engineers do not typically work together during the conceptual phase; in
fact, structural engineers are often called late into the process. As a result,
updates in the design are more difficult and time-consuming to complete. At the
same time, there is a lost opportunity for better design exploration guided by
structural feedback. In general, the earlier in the design process the
iteration happens, the greater the benefits in cost efficiency and informed
de-sign exploration, which can lead to higher-quality creative results. In
order to facilitate an informed exploration in the early design stage, we
suggest the automation of fundamental structural engineering tasks and
introduce ApproxiFramer, a Machine Learning-based system for the automatic
generation of structural layouts from building plan sketches in real-time. The
system aims to assist architects by presenting them with feasible structural
solutions during the conceptual phase so that they proceed with their design
with adequate knowledge of its structural implications. In this paper, we
describe the system and evaluate the performance of a proof-of-concept
implementation in the domain of orthogonal, metal, rigid structures. We trained
a Convolutional Neural Net to iteratively generate structural design solutions
for sketch-level building plans using a synthetic dataset and achieved an
average error of 2.2% in the predicted positions of the columns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ampanavos_S/0/1/0/all/0/1"&gt;Spyridon Ampanavos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nourbakhsh_M/0/1/0/all/0/1"&gt;Mehdi Nourbakhsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Chin-Yi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Bayesian Neural Networks on Accuracy, Privacy and Reliability. (arXiv:2107.08461v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08461</id>
        <link href="http://arxiv.org/abs/2107.08461"/>
        <updated>2021-07-20T02:04:43.317Z</updated>
        <summary type="html"><![CDATA[Bayesian neural network (BNN) allows for uncertainty quantification in
prediction, offering an advantage over regular neural networks that has not
been explored in the differential privacy (DP) framework. We fill this
important gap by leveraging recent development in Bayesian deep learning and
privacy accounting to offer a more precise analysis of the trade-off between
privacy and accuracy in BNN. We propose three DP-BNNs that characterize the
weight uncertainty for the same network architecture in distinct ways, namely
DP-SGLD (via the noisy gradient method), DP-BBP (via changing the parameters of
interest) and DP-MC Dropout (via the model architecture). Interestingly, we
show a new equivalence between DP-SGD and DP-SGLD, implying that some
non-Bayesian DP training naturally allows for uncertainty quantification.
However, the hyperparameters such as learning rate and batch size, can have
different or even opposite effects in DP-SGD and DP-SGLD.

Extensive experiments are conducted to compare DP-BNNs, in terms of privacy
guarantee, prediction accuracy, uncertainty quantification, calibration,
computation speed, and generalizability to network architecture. As a result,
we observe a new tradeoff between the privacy and the reliability. When
compared to non-DP and non-Bayesian approaches, DP-SGLD is remarkably accurate
under strong privacy guarantee, demonstrating the great potential of DP-BNN in
real-world tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiyiwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1"&gt;Zhiqi Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1"&gt;Qi Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Upper Arm Movements from EEG signals using Machine Learning with ICA Analysis. (arXiv:2107.08514v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.08514</id>
        <link href="http://arxiv.org/abs/2107.08514"/>
        <updated>2021-07-20T02:04:43.244Z</updated>
        <summary type="html"><![CDATA[The Brain-Computer Interface system is a profoundly developing area of
experimentation for Motor activities which plays vital role in decoding
cognitive activities. Classification of Cognitive-Motor Imagery activities from
EEG signals is a critical task. Hence proposed a unique algorithm for
classifying left/right-hand movements by utilizing Multi-layer Perceptron
Neural Network. Handcrafted statistical Time domain and Power spectral density
frequency domain features were extracted and obtained a combined accuracy of
96.02%. Results were compared with the deep learning framework. In addition to
accuracy, Precision, F1-Score, and recall was considered as the performance
metrics. The intervention of unwanted signals contaminates the EEG signals
which influence the performance of the algorithm. Therefore, a novel approach
was approached to remove the artifacts using Independent Components Analysis
which boosted the performance. Following the selection of appropriate feature
vectors that provided acceptable accuracy. The same method was used on all nine
subjects. As a result, intra-subject accuracy was obtained for 9 subjects
94.72%. The results show that the proposed approach would be useful to classify
the upper limb movements accurately.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kokate_P/0/1/0/all/0/1"&gt;Pranali Kokate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pancholi_S/0/1/0/all/0/1"&gt;Sidharth Pancholi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Joshi_A/0/1/0/all/0/1"&gt;Amit M. Joshi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RobustFed: A Truth Inference Approach for Robust Federated Learning. (arXiv:2107.08402v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08402</id>
        <link href="http://arxiv.org/abs/2107.08402"/>
        <updated>2021-07-20T02:04:43.220Z</updated>
        <summary type="html"><![CDATA[Federated learning is a prominent framework that enables clients (e.g.,
mobile devices or organizations) to train a collaboratively global model under
a central server's orchestration while keeping local training datasets'
privacy. However, the aggregation step in federated learning is vulnerable to
adversarial attacks as the central server cannot manage clients' behavior.
Therefore, the global model's performance and convergence of the training
process will be affected under such attacks.To mitigate this vulnerability
issue, we propose a novel robust aggregation algorithm inspired by the truth
inference methods in crowdsourcing via incorporating the worker's reliability
into aggregation. We evaluate our solution on three real-world datasets with a
variety of machine learning models. Experimental results show that our solution
ensures robust federated learning and is resilient to various types of attacks,
including noisy data attacks, Byzantine attacks, and label flipping attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tahmasebian_F/0/1/0/all/0/1"&gt;Farnaz Tahmasebian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1"&gt;Jian Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Path Integrals for the Attribution of Model Uncertainties. (arXiv:2107.08756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08756</id>
        <link href="http://arxiv.org/abs/2107.08756"/>
        <updated>2021-07-20T02:04:42.595Z</updated>
        <summary type="html"><![CDATA[Enabling interpretations of model uncertainties is of key importance in
Bayesian machine learning applications. Often, this requires to meaningfully
attribute predictive uncertainties to source features in an image, text or
categorical array. However, popular attribution methods are particularly
designed for classification and regression scores. In order to explain
uncertainties, state of the art alternatives commonly procure counterfactual
feature vectors, and proceed by making direct comparisons. In this paper, we
leverage path integrals to attribute uncertainties in Bayesian differentiable
models. We present a novel algorithm that relies on in-distribution curves
connecting a feature vector to some counterfactual counterpart, and we retain
desirable properties of interpretability methods. We validate our approach on
benchmark image data sets with varying resolution, and show that it
significantly simplifies interpretability over the existing alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1"&gt;Iker Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skalski_P/0/1/0/all/0/1"&gt;Piotr Skalski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barns_Graham_A/0/1/0/all/0/1"&gt;Alec Barns-Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1"&gt;Jason Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_D/0/1/0/all/0/1"&gt;David Sutton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Embedding Learning from Uncertainty Momentum Modeling. (arXiv:2107.08892v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08892</id>
        <link href="http://arxiv.org/abs/2107.08892"/>
        <updated>2021-07-20T02:04:42.575Z</updated>
        <summary type="html"><![CDATA[Existing popular unsupervised embedding learning methods focus on enhancing
the instance-level local discrimination of the given unlabeled images by
exploring various negative data. However, the existed sample outliers which
exhibit large intra-class divergences or small inter-class variations severely
limit their learning performance. We justify that the performance limitation is
caused by the gradient vanishing on these sample outliers. Moreover, the
shortage of positive data and disregard for global discrimination consideration
also pose critical issues for unsupervised learning but are always ignored by
existing methods. To handle these issues, we propose a novel solution to
explicitly model and directly explore the uncertainty of the given unlabeled
learning samples. Instead of learning a deterministic feature point for each
sample in the embedding space, we propose to represent a sample by a stochastic
Gaussian with the mean vector depicting its space localization and covariance
vector representing the sample uncertainty. We leverage such uncertainty
modeling as momentum to the learning which is helpful to tackle the outliers.
Furthermore, abundant positive candidates can be readily drawn from the learned
instance-specific distributions which are further adopted to mitigate the
aforementioned issues. Thorough rationale analyses and extensive experiments
are presented to verify our superiority.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiahuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yansong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1"&gt;Bing Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Ying Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08751</id>
        <link href="http://arxiv.org/abs/2107.08751"/>
        <updated>2021-07-20T02:04:42.557Z</updated>
        <summary type="html"><![CDATA[Deep learning for medical imaging suffers from temporal and privacy-related
restrictions on data availability. To still obtain viable models, continual
learning aims to train in sequential order, as and when data is available. The
main challenge that continual learning methods face is to prevent catastrophic
forgetting, i.e., a decrease in performance on the data encountered earlier.
This issue makes continuous training of segmentation models for medical
applications extremely difficult. Yet, often, data from at least two different
domains is available which we can exploit to train the model in a way that it
disregards domain-specific information. We propose an architecture that
leverages the simultaneous availability of two or more datasets to learn a
disentanglement between the content and domain in an adversarial fashion. The
domain-invariant content representation then lays the base for continual
semantic segmentation. Our approach takes inspiration from domain adaptation
and combines it with continual learning for hippocampal segmentation in brain
MRI. We showcase that our method reduces catastrophic forgetting and
outperforms state-of-the-art continual learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1"&gt;Marius Memmel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1"&gt;Camila Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1"&gt;Anirban Mukhopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic and explainable grading of meningiomas from histopathology images. (arXiv:2107.08850v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08850</id>
        <link href="http://arxiv.org/abs/2107.08850"/>
        <updated>2021-07-20T02:04:42.480Z</updated>
        <summary type="html"><![CDATA[Meningioma is one of the most prevalent brain tumors in adults. To determine
its malignancy, it is graded by a pathologist into three grades according to
WHO standards. This grade plays a decisive role in treatment, and yet may be
subject to inter-rater discordance. In this work, we present and compare three
approaches towards fully automatic meningioma grading from histology whole
slide images. All approaches are following a two-stage paradigm, where we first
identify a region of interest based on the detection of mitotic figures in the
slide using a state-of-the-art object detection deep learning network. This
region of highest mitotic rate is considered characteristic for biological
tumor behavior. In the second stage, we calculate a score corresponding to
tumor malignancy based on information contained in this region using three
different settings. In a first approach, image patches are sampled from this
region and regression is based on morphological features encoded by a
ResNet-based network. We compare this to learning a logistic regression from
the determined mitotic count, an approach which is easily traceable and
explainable. Lastly, we combine both approaches in a single network. We trained
the pipeline on 951 slides from 341 patients and evaluated them on a separate
set of 141 slides from 43 patients. All approaches yield a high correlation to
the WHO grade. The logistic regression and the combined approach had the best
results in our experiments, yielding correct predictions in 32 and 33 of all
cases, respectively, with the image-based approach only predicting 25 cases
correctly. Spearman's correlation was 0.716, 0.792 and 0.790 respectively. It
may seem counterintuitive at first that morphological features provided by
image patches do not improve model performance. Yet, this mirrors the criteria
of the grading scheme, where mitotic count is the only unequivocal parameter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ganz_J/0/1/0/all/0/1"&gt;Jonathan Ganz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kirsch_T/0/1/0/all/0/1"&gt;Tobias Kirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hoffmann_L/0/1/0/all/0/1"&gt;Lucas Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bertram_C/0/1/0/all/0/1"&gt;Christof A. Bertram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hoffmann_C/0/1/0/all/0/1"&gt;Christoph Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1"&gt;Katharina Breininger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blumcke_I/0/1/0/all/0/1"&gt;Ingmar Bl&amp;#xfc;mcke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jabari_S/0/1/0/all/0/1"&gt;Samir Jabari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1"&gt;Marc Aubreville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Interpretability of Deep Neural Networks in Medical Diagnosis by Investigating the Individual Units. (arXiv:2107.08767v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08767</id>
        <link href="http://arxiv.org/abs/2107.08767"/>
        <updated>2021-07-20T02:04:42.431Z</updated>
        <summary type="html"><![CDATA[As interpretability has been pointed out as the obstacle to the adoption of
Deep Neural Networks (DNNs), there is an increasing interest in solving a
transparency issue to guarantee the impressive performance. In this paper, we
demonstrate the efficiency of recent attribution techniques to explain the
diagnostic decision by visualizing the significant factors in the input image.
By utilizing the characteristics of objectness that DNNs have learned, fully
decomposing the network prediction visualizes clear localization of target
lesion. To verify our work, we conduct our experiments on Chest X-ray diagnosis
with publicly accessible datasets. As an intuitive assessment metric for
explanations, we report the performance of intersection of Union between visual
explanation and bounding box of lesions. Experiment results show that recently
proposed attribution methods visualize the more accurate localization for the
diagnostic decision compared to the traditionally used CAM. Furthermore, we
analyze the inconsistency of intentions between humans and DNNs, which is
easily obscured by high performance. By visualizing the relevant factors, it is
possible to confirm that the criterion for decision is in line with the
learning strategy. Our analysis of unmasking machine intelligence represents
the necessity of explainability in the medical diagnostic decision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nam_W/0/1/0/all/0/1"&gt;Woo-Jeoung Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Precise Aerial Image Matching based on Deep Homography Estimation. (arXiv:2107.08768v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08768</id>
        <link href="http://arxiv.org/abs/2107.08768"/>
        <updated>2021-07-20T02:04:42.402Z</updated>
        <summary type="html"><![CDATA[Aerial image registration or matching is a geometric process of aligning two
aerial images captured in different environments. Estimating the precise
transformation parameters is hindered by various environments such as time,
weather, and viewpoints. The characteristics of the aerial images are mainly
composed of a straight line owing to building and road. Therefore, the straight
lines are distorted when estimating homography parameters directly between two
images. In this paper, we propose a deep homography alignment network to
precisely match two aerial images by progressively estimating the various
transformation parameters. The proposed network is possible to train the
matching network with a higher degree of freedom by progressively analyzing the
transformation parameters. The precision matching performances have been
increased by applying homography transformation. In addition, we introduce a
method that can effectively learn the difficult-to-learn homography estimation
network. Since there is no published learning data for aerial image
registration, in this paper, a pair of images to which random homography
transformation is applied within a certain range is used for learning. Hence,
we could confirm that the deep homography alignment network shows high
precision matching performance compared with conventional works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1"&gt;Myeong-Seok Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yong-Ju Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling and Vectorization: A 3D Visual Perception Approach for Autonomous Driving Based on Surround-View Fisheye Cameras. (arXiv:2107.08862v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08862</id>
        <link href="http://arxiv.org/abs/2107.08862"/>
        <updated>2021-07-20T02:04:42.276Z</updated>
        <summary type="html"><![CDATA[The 3D visual perception for vehicles with the surround-view fisheye camera
system is a critical and challenging task for low-cost urban autonomous
driving. While existing monocular 3D object detection methods perform not well
enough on the fisheye images for mass production, partly due to the lack of 3D
datasets of such images. In this paper, we manage to overcome and avoid the
difficulty of acquiring the large scale of accurate 3D labeled truth data, by
breaking down the 3D object detection task into some sub-tasks, such as
vehicle's contact point detection, type classification, re-identification and
unit assembling, etc. Particularly, we propose the concept of Multidimensional
Vector to include the utilizable information generated in different dimensions
and stages, instead of the descriptive approach for the bird's eye view (BEV)
or a cube of eight points. The experiments of real fisheye images demonstrate
that our solution achieves state-of-the-art accuracy while being real-time in
practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zizhang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenkai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jizheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Man Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1"&gt;Yuanzhu Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gou_X/0/1/0/all/0/1"&gt;Xinchao Gou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1"&gt;Muqing Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jing Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Input Agnostic Deep Learning for Alzheimer's Disease Classification Using Multimodal MRI Images. (arXiv:2107.08673v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08673</id>
        <link href="http://arxiv.org/abs/2107.08673"/>
        <updated>2021-07-20T02:04:42.239Z</updated>
        <summary type="html"><![CDATA[Alzheimer's disease (AD) is a progressive brain disorder that causes memory
and functional impairments. The advances in machine learning and publicly
available medical datasets initiated multiple studies in AD diagnosis. In this
work, we utilize a multi-modal deep learning approach in classifying normal
cognition, mild cognitive impairment and AD classes on the basis of structural
MRI and diffusion tensor imaging (DTI) scans from the OASIS-3 dataset. In
addition to a conventional multi-modal network, we also present an input
agnostic architecture that allows diagnosis with either sMRI or DTI scan, which
distinguishes our method from previous multi-modal machine learning-based
methods. The results show that the input agnostic model achieves 0.96 accuracy
when both structural MRI and DTI scans are provided as inputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Massalimova_A/0/1/0/all/0/1"&gt;Aidana Massalimova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1"&gt;Huseyin Atakan Varol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Set Similarity for Dense Self-supervised Representation Learning. (arXiv:2107.08712v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08712</id>
        <link href="http://arxiv.org/abs/2107.08712"/>
        <updated>2021-07-20T02:04:42.135Z</updated>
        <summary type="html"><![CDATA[By considering the spatial correspondence, dense self-supervised
representation learning has achieved superior performance on various dense
prediction tasks. However, the pixel-level correspondence tends to be noisy
because of many similar misleading pixels, e.g., backgrounds. To address this
issue, in this paper, we propose to explore \textbf{set} \textbf{sim}ilarity
(SetSim) for dense self-supervised representation learning. We generalize
pixel-wise similarity learning to set-wise one to improve the robustness
because sets contain more semantic and structure information. Specifically, by
resorting to attentional features of views, we establish corresponding sets,
thus filtering out noisy backgrounds that may cause incorrect correspondences.
Meanwhile, these attentional features can keep the coherence of the same image
across different views to alleviate semantic inconsistency. We further search
the cross-view nearest neighbours of sets and employ the structured
neighbourhood information to enhance the robustness. Empirical evaluations
demonstrate that SetSim is superior to state-of-the-art methods on object
detection, keypoint detection, instance segmentation, and semantic
segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guoxin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1"&gt;Pengfei Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wen Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Nannan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Mingming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RECIST-Net: Lesion detection via grouping keypoints on RECIST-based annotation. (arXiv:2107.08715v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08715</id>
        <link href="http://arxiv.org/abs/2107.08715"/>
        <updated>2021-07-20T02:04:42.115Z</updated>
        <summary type="html"><![CDATA[Universal lesion detection in computed tomography (CT) images is an important
yet challenging task due to the large variations in lesion type, size, shape,
and appearance. Considering that data in clinical routine (such as the
DeepLesion dataset) are usually annotated with a long and a short diameter
according to the standard of Response Evaluation Criteria in Solid Tumors
(RECIST) diameters, we propose RECIST-Net, a new approach to lesion detection
in which the four extreme points and center point of the RECIST diameters are
detected. By detecting a lesion as keypoints, we provide a more conceptually
straightforward formulation for detection, and overcome several drawbacks
(e.g., requiring extensive effort in designing data-appropriate anchors and
losing shape information) of existing bounding-box-based methods while
exploring a single-task, one-stage approach compared to other RECIST-based
approaches. Experiments show that RECIST-Net achieves a sensitivity of 92.49%
at four false positives per image, outperforming other recent methods including
those using multi-task learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Cong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Shilei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1"&gt;Dong Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hongyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xianli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1"&gt;Buyue Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liansheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesizing Human Faces using Latent Space Factorization and Local Weights (Extended Version). (arXiv:2107.08737v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2107.08737</id>
        <link href="http://arxiv.org/abs/2107.08737"/>
        <updated>2021-07-20T02:04:42.096Z</updated>
        <summary type="html"><![CDATA[We propose a 3D face generative model with local weights to increase the
model's variations and expressiveness. The proposed model allows partial
manipulation of the face while still learning the whole face mesh. For this
purpose, we address an effective way to extract local facial features from the
entire data and explore a way to manipulate them during a holistic generation.
First, we factorize the latent space of the whole face to the subspace
indicating different parts of the face. In addition, local weights generated by
non-negative matrix factorization are applied to the factorized latent space so
that the decomposed part space is semantically meaningful. We experiment with
our model and observe that effective facial part manipulation is possible and
that the model's expressiveness is improved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minyoung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young J. Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning point embedding for 3D data processing. (arXiv:2107.08565v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08565</id>
        <link href="http://arxiv.org/abs/2107.08565"/>
        <updated>2021-07-20T02:04:41.858Z</updated>
        <summary type="html"><![CDATA[Among 2D convolutional networks on point clouds, point-based approaches
consume point clouds of fixed size directly. By analysis of PointNet, a pioneer
in introducing deep learning into point sets, we reveal that current
point-based methods are essentially spatial relationship processing networks.
In this paper, we take a different approach. Our architecture, named PE-Net,
learns the representation of point clouds in high-dimensional space, and
encodes the unordered input points to feature vectors, which standard 2D CNNs
can be applied to. The recommended network can adapt to changes in the number
of input points which is the limit of current methods. Experiments show that in
the tasks of classification and part segmentation, PE-Net achieves the
state-of-the-art performance in multiple challenging datasets, such as ModelNet
and ShapeNetPart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenpeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[YOLOX: Exceeding YOLO Series in 2021. (arXiv:2107.08430v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08430</id>
        <link href="http://arxiv.org/abs/2107.08430"/>
        <updated>2021-07-20T02:04:41.791Z</updated>
        <summary type="html"><![CDATA[In this report, we present some experienced improvements to YOLO series,
forming a new high-performance detector -- YOLOX. We switch the YOLO detector
to an anchor-free manner and conduct other advanced detection techniques, i.e.,
a decoupled head and the leading label assignment strategy SimOTA to achieve
state-of-the-art results across a large scale range of models: For YOLO-Nano
with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing
NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in
industry, we boost it to 47.3% AP on COCO, outperforming the current best
practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as
YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on
Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on
Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021)
using a single YOLOX-L model. We hope this report can provide useful experience
for developers and researchers in practical scenes, and we also provide deploy
versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at
https://github.com/Megvii-BaseDetection/YOLOX.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1"&gt;Zheng Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Songtao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Feng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zeming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Convolution for 3D Point Cloud Instance Segmentation. (arXiv:2107.08392v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08392</id>
        <link href="http://arxiv.org/abs/2107.08392"/>
        <updated>2021-07-20T02:04:41.770Z</updated>
        <summary type="html"><![CDATA[We propose an approach to instance segmentation from 3D point clouds based on
dynamic convolution. This enables it to adapt, at inference, to varying feature
and object scales. Doing so avoids some pitfalls of bottom up approaches,
including a dependence on hyper-parameter tuning and heuristic post-processing
pipelines to compensate for the inevitable variability in object sizes, even
within a single scene. The representation capability of the network is greatly
improved by gathering homogeneous points that have identical semantic
categories and close votes for the geometric centroids. Instances are then
decoded via several simple convolution layers, where the parameters are
generated conditioned on the input. The proposed approach is proposal-free, and
instead exploits a convolution process that adapts to the spatial and semantic
characteristics of each instance. A light-weight transformer, built on the
bottleneck layer, allows the model to capture long-range dependencies, with
limited computational overhead. The result is a simple, efficient, and robust
approach that yields strong performance on various datasets: ScanNetV2, S3DIS,
and PartNet. The consistent improvements on both voxel- and point-based
architectures imply the effectiveness of the proposed method. Code is available
at: https://git.io/DyCo3D]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1"&gt;Anton van den Hengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content-Augmented Feature Pyramid Network with Light Linear Spatial Transformers for Object Detection. (arXiv:2105.09464v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09464</id>
        <link href="http://arxiv.org/abs/2105.09464"/>
        <updated>2021-07-20T02:04:41.740Z</updated>
        <summary type="html"><![CDATA[As one of the prevalent components, Feature Pyramid Network (FPN) is widely
used in the current object detection models to improve the performance of
multi-scale detection. However, its interaction is still in a local and lossy
manner, thus limiting the representation power. In this paper, to simulate a
global view of human vision in object detection and address the inherent
defects of interaction mode in FPN, we construct a novel architecture termed
Content-Augmented Feature Pyramid Network (CA-FPN). Unlike the vanilla FPN,
which fuses features within a local receptive field, CA-FPN can adaptively
aggregate similar features from a global view. It is equipped with a global
content extraction module and light linear spatial transformers. The former
allows to extract multi-scale context information and the latter can deeply
combine the global content extraction module with the vanilla FPN using the
linearized attention function, which is designed to reduce model complexity.
Furthermore, CA-FPN can be readily plugged into existing FPN-based models.
Extensive experiments on the challenging COCO and PASCAL VOC object detection
datasets demonstrated that our CA-FPN significantly outperforms competitive
FPN-based detectors without bells and whistles. When plugging CA-FPN into
Cascade R-CNN framework built upon a standard ResNet-50 backbone, our method
can achieve 44.8 AP on COCO mini-val. Its performance surpasses the previous
state-of-the-art by 1.5 AP, demonstrating the potentiality of application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yongxiang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1"&gt;Xiaolin Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yuncong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01198</id>
        <link href="http://arxiv.org/abs/2107.01198"/>
        <updated>2021-07-20T02:04:41.716Z</updated>
        <summary type="html"><![CDATA[In this work, we present to the NLP community, and to the wider research
community as a whole, an application for the diachronic analysis of research
corpora. We open source an easy-to-use tool coined: DRIFT, which allows
researchers to track research trends and development over the years. The
analysis methods are collated from well-cited research works, with a few of our
own methods added for good measure. Succinctly put, some of the analysis
methods are: keyword extraction, word clouds, predicting
declining/stagnant/growing trends using Productivity, tracking bi-grams using
Acceleration plots, finding the Semantic Drift of words, tracking trends using
similarity, etc. To demonstrate the utility and efficacy of our tool, we
perform a case study on the cs.CL corpus of the arXiv repository and draw
inferences from the analysis methods. The toolkit and the associated code are
available here: https://github.com/rajaswa/DRIFT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abheesht Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1"&gt;Gunjan Chhablani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1"&gt;Harshit Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1"&gt;Rajaswa Patil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Implicit Image Function for Guided Depth Super-Resolution. (arXiv:2107.08717v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08717</id>
        <link href="http://arxiv.org/abs/2107.08717"/>
        <updated>2021-07-20T02:04:41.695Z</updated>
        <summary type="html"><![CDATA[Guided depth super-resolution is a practical task where a low-resolution and
noisy input depth map is restored to a high-resolution version, with the help
of a high-resolution RGB guide image. Existing methods usually view this task
as a generalized guided filtering problem that relies on designing explicit
filters and objective functions, or a dense regression problem that directly
predicts the target image via deep neural networks. These methods suffer from
either model capability or interpretability. Inspired by the recent progress in
implicit neural representation, we propose to formulate the guided
super-resolution as a neural implicit image interpolation problem, where we
take the form of a general image interpolation but use a novel Joint Implicit
Image Function (JIIF) representation to learn both the interpolation weights
and values. JIIF represents the target image domain with spatially distributed
local latent codes extracted from the input image and the guide image, and uses
a graph attention mechanism to learn the interpolation weights at the same time
in one unified deep implicit function. We demonstrate the effectiveness of our
JIIF representation on guided depth super-resolution task, significantly
outperforming state-of-the-art methods on three public benchmarks. Code can be
found at \url{https://git.io/JC2sU}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiaxiang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaokang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1"&gt;Gang Zeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization via Inference-time Label-Preserving Target Projections. (arXiv:2103.01134v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01134</id>
        <link href="http://arxiv.org/abs/2103.01134"/>
        <updated>2021-07-20T02:04:41.676Z</updated>
        <summary type="html"><![CDATA[Generalization of machine learning models trained on a set of source domains
on unseen target domains with different statistics, is a challenging problem.
While many approaches have been proposed to solve this problem, they only
utilize source data during training but do not take advantage of the fact that
a single target example is available at the time of inference. Motivated by
this, we propose a method that effectively uses the target sample during
inference beyond mere classification. Our method has three components - (i) A
label-preserving feature or metric transformation on source data such that the
source samples are clustered in accordance with their class irrespective of
their domain (ii) A generative model trained on the these features (iii) A
label-preserving projection of the target point on the source-feature manifold
during inference via solving an optimization problem on the input space of the
generative model using the learned metric. Finally, the projected target is
used in the classifier. Since the projected target feature comes from the
source manifold and has the same label as the real target by design, the
classifier is expected to perform better on it than the true target. We
demonstrate that our method outperforms the state-of-the-art Domain
Generalization methods on multiple datasets and tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1"&gt;Prashant Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1"&gt;Mrigank Raman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varambally_S/0/1/0/all/0/1"&gt;Sumanth Varambally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1"&gt;Prathosh AP&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Whole Prostate Segmentation in MRI with Personalized Neural Architectures. (arXiv:2107.08111v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08111</id>
        <link href="http://arxiv.org/abs/2107.08111"/>
        <updated>2021-07-20T02:04:41.619Z</updated>
        <summary type="html"><![CDATA[Building robust deep learning-based models requires diverse training data,
ideally from several sources. However, these datasets cannot be combined easily
because of patient privacy concerns or regulatory hurdles, especially if
medical data is involved. Federated learning (FL) is a way to train machine
learning models without the need for centralized datasets. Each FL client
trains on their local data while only sharing model parameters with a global
server that aggregates the parameters from all clients. At the same time, each
client's data can exhibit differences and inconsistencies due to the local
variation in the patient population, imaging equipment, and acquisition
protocols. Hence, the federated learned models should be able to adapt to the
local particularities of a client's data. In this work, we combine FL with an
AutoML technique based on local neural architecture search by training a
"supernet". Furthermore, we propose an adaptation scheme to allow for
personalized model architectures at each FL client's site. The proposed method
is evaluated on four different datasets from 3D prostate MRI and shown to
improve the local models' performance after adaptation through selecting an
optimal path through the AutoML supernet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1"&gt;Holger R. Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1"&gt;Dong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Myronenko_A/0/1/0/all/0/1"&gt;Andriy Myronenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wentao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Ziyue Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaosong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1"&gt;Daguang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.08142</id>
        <link href="http://arxiv.org/abs/2107.08142"/>
        <updated>2021-07-20T02:04:41.599Z</updated>
        <summary type="html"><![CDATA[Despite the numerous successes of machine learning over the past decade
(image recognition, decision-making, NLP, image synthesis), self-driving
technology has not yet followed the same trend. In this paper, we study the
history, composition, and development bottlenecks of the modern self-driving
stack. We argue that the slow progress is caused by approaches that require too
much hand-engineering, an over-reliance on road testing, and high fleet
deployment costs. We observe that the classical stack has several bottlenecks
that preclude the necessary scale needed to capture the long tail of rare
events. To resolve these problems, we outline the principles of Autonomy 2.0,
an ML-first approach to self-driving, as a viable alternative to the currently
adopted state-of-the-art. This approach is based on (i) a fully differentiable
AV stack trainable from human demonstrations, (ii) closed-loop data-driven
reactive simulation, and (iii) large-scale, low-cost data collections as
critical solutions towards scalability issues. We outline the general
architecture, survey promising works in this direction and propose key
challenges to be addressed by the community in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Ashesh Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1"&gt;Luca Del Pero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1"&gt;Hugo Grimmett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1"&gt;Peter Ondruska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation. (arXiv:2010.15728v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15728</id>
        <link href="http://arxiv.org/abs/2010.15728"/>
        <updated>2021-07-20T02:04:41.578Z</updated>
        <summary type="html"><![CDATA[Diagnostic or procedural coding of clinical notes aims to derive a coded
summary of disease-related information about patients. Such coding is usually
done manually in hospitals but could potentially be automated to improve the
efficiency and accuracy of medical coding. Recent studies on deep learning for
automated medical coding achieved promising performances. However, the
explainability of these models is usually poor, preventing them to be used
confidently in supporting clinical practice. Another limitation is that these
models mostly assume independence among labels, ignoring the complex
correlation among medical codes which can potentially be exploited to improve
the performance. We propose a Hierarchical Label-wise Attention Network (HLAN),
which aimed to interpret the model by quantifying importance (as attention
weights) of words and sentences related to each of the labels. Secondly, we
propose to enhance the major deep learning models with a label embedding (LE)
initialisation approach, which learns a dense, continuous vector representation
and then injects the representation into the final layers and the label-wise
attention layers in the models. We evaluated the methods using three settings
on the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS
COVID-19 shielding codes. Experiments were conducted to compare HLAN and LE
initialisation to the state-of-the-art neural network based methods. HLAN
achieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and
comparable results on the NHS COVID-19 shielding code prediction to other
models. By highlighting the most salient words and sentences for each label,
HLAN showed more meaningful and comprehensive model interpretation compared to
its downgraded baselines and the CNN-based models. LE initialisation
consistently boosted most deep learning models for automated medical coding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor Su&amp;#xe1;rez-Paniagua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteley_W/0/1/0/all/0/1"&gt;William Whiteley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Honghan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-based Multi-scale Gated Recurrent Encoder with Novel Correlation Loss for COVID-19 Progression Prediction. (arXiv:2107.08330v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08330</id>
        <link href="http://arxiv.org/abs/2107.08330"/>
        <updated>2021-07-20T02:04:41.508Z</updated>
        <summary type="html"><![CDATA[COVID-19 image analysis has mostly focused on diagnostic tasks using single
timepoint scans acquired upon disease presentation or admission. We present a
deep learning-based approach to predict lung infiltrate progression from serial
chest radiographs (CXRs) of COVID-19 patients. Our method first utilizes
convolutional neural networks (CNNs) for feature extraction from patches within
the concerned lung zone, and also from neighboring and remote boundary regions.
The framework further incorporates a multi-scale Gated Recurrent Unit (GRU)
with a correlation module for effective predictions. The GRU accepts CNN
feature vectors from three different areas as input and generates a fused
representation. The correlation module attempts to minimize the correlation
loss between hidden representations of concerned and neighboring area feature
vectors, while maximizing the loss between the same from concerned and remote
regions. Further, we employ an attention module over the output hidden states
of each encoder timepoint to generate a context vector. This vector is used as
an input to a decoder module to predict patch severity grades at a future
timepoint. Finally, we ensemble the patch classification scores to calculate
patient-wise grades. Specifically, our framework predicts zone-wise disease
severity for a patient on a given day by learning representations from the
previous temporal CXRs. Our novel multi-institutional dataset comprises
sequential CXR scans from N=93 patients. Our approach outperforms transfer
learning and radiomic feature-based baseline approaches on this dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Konwer_A/0/1/0/all/0/1"&gt;Aishik Konwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1"&gt;Joseph Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Singh_G/0/1/0/all/0/1"&gt;Gagandeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gattu_R/0/1/0/all/0/1"&gt;Rishabh Gattu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1"&gt;Syed Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Green_J/0/1/0/all/0/1"&gt;Jeremy Green&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Phatak_T/0/1/0/all/0/1"&gt;Tej Phatak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prasanna_P/0/1/0/all/0/1"&gt;Prateek Prasanna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Polarimetric SAR and Single-Polarization SAR Image Fusion Network. (arXiv:2107.08355v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08355</id>
        <link href="http://arxiv.org/abs/2107.08355"/>
        <updated>2021-07-20T02:04:41.490Z</updated>
        <summary type="html"><![CDATA[The data fusion technology aims to aggregate the characteristics of different
data and obtain products with multiple data advantages. To solves the problem
of reduced resolution of PolSAR images due to system limitations, we propose a
fully polarimetric synthetic aperture radar (PolSAR) images and
single-polarization synthetic aperture radar SAR (SinSAR) images fusion network
to generate high-resolution PolSAR (HR-PolSAR) images. To take advantage of the
polarimetric information of the low-resolution PolSAR (LR-PolSAR) image and the
spatial information of the high-resolution single-polarization SAR (HR-SinSAR)
image, we propose a fusion framework for joint LR-PolSAR image and HR-SinSAR
image and design a cross-attention mechanism to extract features from the joint
input data. Besides, based on the physical imaging mechanism, we designed the
PolSAR polarimetric loss function for constrained network training. The
experimental results confirm the superiority of fusion network over traditional
algorithms. The average PSNR is increased by more than 3.6db, and the average
MAE is reduced to less than 0.07. Experiments on polarimetric decomposition and
polarimetric signature show that it maintains polarimetric information well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liupeng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1"&gt;Huanfeng Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Lingli Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_Q/0/1/0/all/0/1"&gt;Qiangqiang Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinghua Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lesion-based Contrastive Learning for Diabetic Retinopathy Grading from Fundus Images. (arXiv:2107.08274v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08274</id>
        <link href="http://arxiv.org/abs/2107.08274"/>
        <updated>2021-07-20T02:04:41.472Z</updated>
        <summary type="html"><![CDATA[Manually annotating medical images is extremely expensive, especially for
large-scale datasets. Self-supervised contrastive learning has been explored to
learn feature representations from unlabeled images. However, unlike natural
images, the application of contrastive learning to medical images is relatively
limited. In this work, we propose a self-supervised framework, namely
lesion-based contrastive learning for automated diabetic retinopathy (DR)
grading. Instead of taking entire images as the input in the common contrastive
learning scheme, lesion patches are employed to encourage the feature extractor
to learn representations that are highly discriminative for DR grading. We also
investigate different data augmentation operations in defining our contrastive
prediction task. Extensive experiments are conducted on the publicly-accessible
dataset EyePACS, demonstrating that our proposed framework performs
outstandingly on DR grading in terms of both linear evaluation and transfer
capacity evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yijin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Li Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Pujin Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1"&gt;Junyan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaoying Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Agent-Environment Network for Temporal Action Proposal Generation. (arXiv:2107.08323v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08323</id>
        <link href="http://arxiv.org/abs/2107.08323"/>
        <updated>2021-07-20T02:04:41.454Z</updated>
        <summary type="html"><![CDATA[Temporal action proposal generation is an essential and challenging task that
aims at localizing temporal intervals containing human actions in untrimmed
videos. Most of existing approaches are unable to follow the human cognitive
process of understanding the video context due to lack of attention mechanism
to express the concept of an action or an agent who performs the action or the
interaction between the agent and the environment. Based on the action
definition that a human, known as an agent, interacts with the environment and
performs an action that affects the environment, we propose a contextual
Agent-Environment Network. Our proposed contextual AEN involves (i) agent
pathway, operating at a local level to tell about which humans/agents are
acting and (ii) environment pathway operating at a global level to tell about
how the agents interact with the environment. Comprehensive evaluations on
20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different
backbone networks, i.e C3D and SlowFast, show that our method robustly exhibits
outperformance against state-of-the-art methods regardless of the employed
backbone network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vo_Ho_V/0/1/0/all/0/1"&gt;Viet-Khoa Vo-Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1"&gt;Ngan Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1"&gt;Kashu Yamazaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1"&gt;Akihiro Sugimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1"&gt;Minh-Triet Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PICASO: Permutation-Invariant Cascaded Attentional Set Operator. (arXiv:2107.08305v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08305</id>
        <link href="http://arxiv.org/abs/2107.08305"/>
        <updated>2021-07-20T02:04:41.436Z</updated>
        <summary type="html"><![CDATA[Set-input deep networks have recently drawn much interest in computer vision
and machine learning. This is in part due to the increasing number of important
tasks such as meta-learning, clustering, and anomaly detection that are defined
on set inputs. These networks must take an arbitrary number of input samples
and produce the output invariant to the input set permutation. Several
algorithms have been recently developed to address this urgent need. Our paper
analyzes these algorithms using both synthetic and real-world datasets, and
shows that they are not effective in dealing with common data variations such
as image translation or viewpoint change. To address this limitation, we
propose a permutation-invariant cascaded attentional set operator (PICASO). The
gist of PICASO is a cascade of multihead attention blocks with dynamic
templates. The proposed operator is a stand-alone module that can be adapted
and extended to serve different machine learning tasks. We demonstrate the
utilities of PICASO in four diverse scenarios: (i) clustering, (ii) image
classification under novel viewpoints, (iii) image anomaly detection, and (iv)
state prediction. PICASO increases the SmallNORB image classification accuracy
with novel viewpoints by about 10% points. For set anomaly detection on CelebA
dataset, our model improves the areas under ROC and PR curves dataset by about
22% and 10%, respectively. For the state prediction on CLEVR dataset, it
improves the AP by about 40%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zare_S/0/1/0/all/0/1"&gt;Samira Zare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hien Van Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Woodscape Fisheye Semantic Segmentation for Autonomous Driving -- CVPR 2021 OmniCV Workshop Challenge. (arXiv:2107.08246v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08246</id>
        <link href="http://arxiv.org/abs/2107.08246"/>
        <updated>2021-07-20T02:04:41.417Z</updated>
        <summary type="html"><![CDATA[We present the WoodScape fisheye semantic segmentation challenge for
autonomous driving which was held as part of the CVPR 2021 Workshop on
Omnidirectional Computer Vision (OmniCV). This challenge is one of the first
opportunities for the research community to evaluate the semantic segmentation
techniques targeted for fisheye camera perception. Due to strong radial
distortion standard models don't generalize well to fisheye images and hence
the deformations in the visual appearance of objects and entities needs to be
encoded implicitly or as explicit knowledge. This challenge served as a medium
to investigate the challenges and new methodologies to handle the complexities
with perception on fisheye images. The challenge was hosted on CodaLab and used
the recently released WoodScape dataset comprising of 10k samples. In this
paper, we provide a summary of the competition which attracted the
participation of 71 global teams and a total of 395 submissions. The top teams
recorded significantly improved mean IoU and accuracy scores over the baseline
PSPNet with ResNet-50 backbone. We summarize the methods of winning algorithms
and analyze the failure cases. We conclude by providing future directions for
the research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1"&gt;Saravanabalagi Ramachandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1"&gt;Ganesh Sistu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1"&gt;John McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Representation Learning Does Not Generalize Strongly Within the Same Domain. (arXiv:2107.08221v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08221</id>
        <link href="http://arxiv.org/abs/2107.08221"/>
        <updated>2021-07-20T02:04:41.399Z</updated>
        <summary type="html"><![CDATA[An important component for generalization in machine learning is to uncover
underlying latent factors of variation as well as the mechanism through which
each factor acts in the world. In this paper, we test whether 17 unsupervised,
weakly supervised, and fully supervised representation learning approaches
correctly infer the generative factors of variation in simple datasets
(dSprites, Shapes3D, MPI3D). In contrast to prior robustness work that
introduces novel factors of variation during test time, such as blur or other
(un)structured noise, we here recompose, interpolate, or extrapolate only
existing factors of variation from the training data set (e.g., small and
medium-sized objects during training and large objects during testing). Models
that learn the correct mechanism should be able to generalize to this
benchmark. In total, we train and test 2000+ models and observe that all of
them struggle to learn the underlying mechanism regardless of supervision
signal and architectural bias. Moreover, the generalization capabilities of all
tested models drop significantly as we move from artificial datasets towards
more realistic real-world datasets. Despite their inability to identify the
correct mechanism, the models are quite modular as their ability to infer other
in-distribution factors remains fairly stable, providing only a single factor
is out-of-distribution. These results point to an important yet understudied
problem of learning mechanistic models of observations that can facilitate
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1"&gt;Lukas Schott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1"&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1"&gt;Frederik Tr&amp;#xe4;uble&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1"&gt;Peter Gehler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1"&gt;Chris Russell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1"&gt;Francesco Locatello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking Twice for Partial Clues: Weakly-supervised Part-Mentored Attention Network for Vehicle Re-Identification. (arXiv:2107.08228v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08228</id>
        <link href="http://arxiv.org/abs/2107.08228"/>
        <updated>2021-07-20T02:04:41.369Z</updated>
        <summary type="html"><![CDATA[Vehicle re-identification (Re-ID) is to retrieve images of the same vehicle
across different cameras. Two key challenges lie in the subtle inter-instance
discrepancy caused by near-duplicate identities and the large intra-instance
variance caused by different views. Since the holistic appearance suffers from
viewpoint variation and distortion, part-level feature learning has been
introduced to enhance vehicle description. However, existing approaches to
localize and amplify significant parts often fail to handle spatial
misalignment as well as occlusion and require expensive annotations. In this
paper, we propose a weakly supervised Part-Mentored Attention Network (PMANet)
composed of a Part Attention Network (PANet) for vehicle part localization with
self-attention and a Part-Mentored Network (PMNet) for mentoring the global and
local feature aggregation. Firstly, PANet is introduced to predict a foreground
mask and pinpoint $K$ prominent vehicle parts only with weak identity
supervision. Secondly, we propose a PMNet to learn global and part-level
features with multi-scale attention and aggregate them in $K$ main-partial
tasks via part transfer. Like humans who first differentiate objects with
general information and then observe salient parts for more detailed clues,
PANet and PMNet construct a two-stage attention structure to perform a
coarse-to-fine search among identities. Finally, we address this Re-ID issue as
a multi-task problem, including global feature learning, identity
classification, and part transfer. We adopt Homoscedastic Uncertainty to learn
the optimal weighing of different losses. Comprehensive experiments are
conducted on two benchmark datasets. Our approach outperforms recent
state-of-the-art methods by averagely 2.63% in CMC@1 on VehicleID and 2.2% in
mAP on VeRi776. Results on occluded test sets also demonstrate the
generalization ability of PMANet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1"&gt;Lisha Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1"&gt;Lap-Pui Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RAMS-Trans: Recurrent Attention Multi-scale Transformer forFine-grained Image Recognition. (arXiv:2107.08192v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08192</id>
        <link href="http://arxiv.org/abs/2107.08192"/>
        <updated>2021-07-20T02:04:41.304Z</updated>
        <summary type="html"><![CDATA[In fine-grained image recognition (FGIR), the localization and amplification
of region attention is an important factor, which has been explored a lot by
convolutional neural networks (CNNs) based approaches. The recently developed
vision transformer (ViT) has achieved promising results on computer vision
tasks. Compared with CNNs, Image sequentialization is a brand new manner.
However, ViT is limited in its receptive field size and thus lacks local
attention like CNNs due to the fixed size of its patches, and is unable to
generate multi-scale features to learn discriminative region attention. To
facilitate the learning of discriminative region attention without box/part
annotations, we use the strength of the attention weights to measure the
importance of the patch tokens corresponding to the raw images. We propose the
recurrent attention multi-scale transformer (RAMS-Trans), which uses the
transformer's self-attention to recursively learn discriminative region
attention in a multi-scale manner. Specifically, at the core of our approach
lies the dynamic patch proposal module (DPPM) guided region amplification to
complete the integration of multi-scale image patches. The DPPM starts with the
full-size image patches and iteratively scales up the region attention to
generate new patches from global to local by the intensity of the attention
weights generated at each scale as an indicator. Our approach requires only the
attention weights that come with ViT itself and can be easily trained
end-to-end. Extensive experiments demonstrate that RAMS-Trans performs better
than concurrent works, in addition to efficient CNN models, achieving
state-of-the-art results on three benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yunqing Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xuan Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1"&gt;Haiwen Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Hui Xue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SCV-Stereo: Learning Stereo Matching from a Sparse Cost Volume. (arXiv:2107.08187v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08187</id>
        <link href="http://arxiv.org/abs/2107.08187"/>
        <updated>2021-07-20T02:04:41.252Z</updated>
        <summary type="html"><![CDATA[Convolutional neural network (CNN)-based stereo matching approaches generally
require a dense cost volume (DCV) for disparity estimation. However, generating
such cost volumes is computationally-intensive and memory-consuming, hindering
CNN training and inference efficiency. To address this problem, we propose
SCV-Stereo, a novel CNN architecture, capable of learning dense stereo matching
from sparse cost volume (SCV) representations. Our inspiration is derived from
the fact that DCV representations are somewhat redundant and can be replaced
with SCV representations. Benefiting from these SCV representations, our
SCV-Stereo can update disparity estimations in an iterative fashion for
accurate and efficient stereo matching. Extensive experiments carried out on
the KITTI Stereo benchmarks demonstrate that our SCV-Stereo can significantly
minimize the trade-off between accuracy and efficiency for stereo matching. Our
project page is https://sites.google.com/view/scv-stereo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hengli Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1"&gt;Rui Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Co-Teaching: An Ark to Unsupervised Stereo Matching. (arXiv:2107.08186v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08186</id>
        <link href="http://arxiv.org/abs/2107.08186"/>
        <updated>2021-07-20T02:04:41.214Z</updated>
        <summary type="html"><![CDATA[Stereo matching is a key component of autonomous driving perception. Recent
unsupervised stereo matching approaches have received adequate attention due to
their advantage of not requiring disparity ground truth. These approaches,
however, perform poorly near occlusions. To overcome this drawback, in this
paper, we propose CoT-Stereo, a novel unsupervised stereo matching approach.
Specifically, we adopt a co-teaching framework where two networks interactively
teach each other about the occlusions in an unsupervised fashion, which greatly
improves the robustness of unsupervised stereo matching. Extensive experiments
on the KITTI Stereo benchmarks demonstrate the superior performance of
CoT-Stereo over all other state-of-the-art unsupervised stereo matching
approaches in terms of both accuracy and speed. Our project webpage is
https://sites.google.com/view/cot-stereo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hengli Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1"&gt;Rui Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thinking Like Transformers. (arXiv:2106.06981v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06981</id>
        <link href="http://arxiv.org/abs/2106.06981"/>
        <updated>2021-07-20T02:04:41.119Z</updated>
        <summary type="html"><![CDATA[What is the computational model behind a Transformer? Where recurrent neural
networks have direct parallels in finite state machines, allowing clear
discussion and thought around architecture variants or trained models,
Transformers have no such familiar parallel. In this paper we aim to change
that, proposing a computational model for the transformer-encoder in the form
of a programming language. We map the basic components of a transformer-encoder
-- attention and feed-forward computation -- into simple primitives, around
which we form a programming language: the Restricted Access Sequence Processing
Language (RASP). We show how RASP can be used to program solutions to tasks
that could conceivably be learned by a Transformer, and how a Transformer can
be trained to mimic a RASP solution. In particular, we provide RASP programs
for histograms, sorting, and Dyck-languages. We further use our model to relate
their difficulty in terms of the number of required layers and attention heads:
analyzing a RASP program implies a maximum number of heads and layers necessary
to encode a task in a transformer. Finally, we see how insights gained from our
abstraction might be used to explain phenomena seen in recent works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1"&gt;Gail Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1"&gt;Eran Yahav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Streaming End-to-End Framework For Spoken Language Understanding. (arXiv:2105.10042v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10042</id>
        <link href="http://arxiv.org/abs/2105.10042"/>
        <updated>2021-07-20T02:04:41.090Z</updated>
        <summary type="html"><![CDATA[End-to-end spoken language understanding (SLU) has recently attracted
increasing interest. Compared to the conventional tandem-based approach that
combines speech recognition and language understanding as separate modules, the
new approach extracts users' intentions directly from the speech signals,
resulting in joint optimization and low latency. Such an approach, however, is
typically designed to process one intention at a time, which leads users to
take multiple rounds to fulfill their requirements while interacting with a
dialogue system. In this paper, we propose a streaming end-to-end framework
that can process multiple intentions in an online and incremental way. The
backbone of our framework is a unidirectional RNN trained with the
connectionist temporal classification (CTC) criterion. By this design, an
intention can be identified when sufficient evidence has been accumulated, and
multiple intentions can be identified sequentially. We evaluate our solution on
the Fluent Speech Commands (FSC) dataset and the intent detection accuracy is
about 97 % on all multi-intent settings. This result is comparable to the
performance of the state-of-the-art non-streaming models, but is achieved in an
online and incremental way. We also employ our model to a keyword spotting task
using the Google Speech Commands dataset and the results are also highly
promising.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1"&gt;Nihal Potdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1"&gt;Anderson R. Avila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1"&gt;Chao Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yiran Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysing Cyberbullying using Natural Language Processing by Understanding Jargon in Social Media. (arXiv:2107.08902v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08902</id>
        <link href="http://arxiv.org/abs/2107.08902"/>
        <updated>2021-07-20T02:04:41.071Z</updated>
        <summary type="html"><![CDATA[Cyberbullying is of extreme prevalence today. Online-hate comments, toxicity,
cyberbullying amongst children and other vulnerable groups are only growing
over online classes, and increased access to social platforms, especially post
COVID-19. It is paramount to detect and ensure minors' safety across social
platforms so that any violence or hate-crime is automatically detected and
strict action is taken against it. In our work, we explore binary
classification by using a combination of datasets from various social media
platforms that cover a wide range of cyberbullying such as sexism, racism,
abusive, and hate-speech. We experiment through multiple models such as
Bi-LSTM, GloVe, state-of-the-art models like BERT, and apply a unique
preprocessing technique by introducing a slang-abusive corpus, achieving a
higher precision in comparison to models without slang preprocessing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_B/0/1/0/all/0/1"&gt;Bhumika Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1"&gt;Anuj Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anjum/0/1/0/all/0/1"&gt;Anjum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katarya_R/0/1/0/all/0/1"&gt;Rahul Katarya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identity-Based Patterns in Deep Convolutional Networks: Generative Adversarial Phonology and Reduplication. (arXiv:2009.06110v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06110</id>
        <link href="http://arxiv.org/abs/2009.06110"/>
        <updated>2021-07-20T02:04:41.052Z</updated>
        <summary type="html"><![CDATA[This paper models unsupervised learning of an identity-based pattern (or
copying) in speech called reduplication from raw continuous data with deep
convolutional neural networks. We use the ciwGAN architecture Begu\v{s} (2021a;
arXiv:2006.02951) in which learning of meaningful representations in speech
emerges from a requirement that the CNNs generate informative data. We propose
a technique to wug-test CNNs trained on speech and, based on four generative
tests, argue that the network learns to represent an identity-based pattern in
its latent space. By manipulating only two categorical variables in the latent
space, we can actively turn an unreduplicated form into a reduplicated form
with no other substantial changes to the output in the majority of cases. We
also argue that the network extends the identity-based pattern to unobserved
data. Exploration of how meaningful representations of identity-based patterns
emerge in CNNs and how the latent space variables outside of the training range
correlate with identity-based patterns in the output has general implications
for neural network interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1"&gt;Ga&amp;#x161;per Begu&amp;#x161;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Objectives of Extractive Question Answering. (arXiv:2008.12804v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12804</id>
        <link href="http://arxiv.org/abs/2008.12804"/>
        <updated>2021-07-20T02:04:41.021Z</updated>
        <summary type="html"><![CDATA[This work demonstrates that, contrary to a common belief, using the objective
with independence assumption for modelling the span probability $P(a_s,a_e) =
P(a_s)P(a_e)$ of span starting at position $a_s$ and ending at position $a_e$
has adverse effects. Therefore we propose multiple approaches to modelling
joint probability $P(a_s,a_e)$ directly. Among those, we propose a compound
objective, composed from the joint probability while still keeping the
objective with independence assumption as an auxiliary objective. We find that
the compound objective is consistently superior or equal to other assumptions
in exact match. Additionally, we identified common errors caused by the
assumption of independence and manually checked the counterpart predictions,
demonstrating the impact of the compound objective on the real examples. Our
findings are supported via experiments with three extractive QA models (BIDAF,
BERT, ALBERT) over six datasets and our code, individual results and manual
analysis are available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1"&gt;Martin Fajcik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1"&gt;Josef Jon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1"&gt;Pavel Smrz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weight Distillation: Transferring the Knowledge in Neural Network Parameters. (arXiv:2009.09152v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09152</id>
        <link href="http://arxiv.org/abs/2009.09152"/>
        <updated>2021-07-20T02:04:40.957Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation has been proven to be effective in model acceleration
and compression. It allows a small network to learn to generalize in the same
way as a large network. Recent successes in pre-training suggest the
effectiveness of transferring model parameters. Inspired by this, we
investigate methods of model acceleration and compression in another line of
research. We propose Weight Distillation to transfer the knowledge in the large
network parameters through a parameter generator. Our experiments on WMT16
En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks show that weight
distillation can train a small network that is 1.88~2.94x faster than the large
network but with competitive performance. With the same sized small network,
weight distillation can outperform knowledge distillation by 0.51~1.82 BLEU
points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Ye Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1"&gt;Quan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1"&gt;Tong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jingbo Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Transformer Decoder with Compressed Sub-layers. (arXiv:2101.00542v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00542</id>
        <link href="http://arxiv.org/abs/2101.00542"/>
        <updated>2021-07-20T02:04:40.927Z</updated>
        <summary type="html"><![CDATA[The large attention-based encoder-decoder network (Transformer) has become
prevailing recently due to its effectiveness. But the high computation
complexity of its decoder raises the inefficiency issue. By examining the
mathematic formulation of the decoder, we show that under some mild conditions,
the architecture could be simplified by compressing its sub-layers, the basic
building block of Transformer, and achieves a higher parallelism. We thereby
propose Compressed Attention Network, whose decoder layer consists of only one
sub-layer instead of three. Extensive experiments on 14 WMT machine translation
tasks show that our model is 1.42x faster with performance on par with a strong
baseline. This strong baseline is already 2x faster than the widely used
standard baseline without loss in performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Ye Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1"&gt;Tong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jingbo Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prior Art Search and Reranking for Generated Patent Text. (arXiv:2009.09132v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09132</id>
        <link href="http://arxiv.org/abs/2009.09132"/>
        <updated>2021-07-20T02:04:40.905Z</updated>
        <summary type="html"><![CDATA[Generative models, such as GPT-2, have demonstrated impressive results
recently. A fundamental question we'd like to address is: where did the
generated text come from? This work is our initial effort toward answering the
question by using prior art search. The purpose of the prior art search is to
find the most similar prior text in the training data of GPT-2. We take a
reranking approach and apply it to the patent domain. Specifically, we
pre-train GPT-2 models from scratch by using the patent data from the USPTO.
The input for the prior art search is the patent text generated by the GPT-2
model. We also pre-trained BERT models from scratch for converting patent text
to embeddings. The steps of reranking are: (1) search the most similar text in
the training data of GPT-2 by taking a bag-of-word ranking approach (BM25), (2)
convert the search results in text format to BERT embeddings, and (3) provide
the final result by ranking the BERT embeddings based on their similarities
with the patent text generated by GPT-2. The experiments in this work show that
such reranking is better than ranking with embeddings alone. However, our mixed
results also indicate that calculating the semantic similarities among long
text spans is still challenging. To our knowledge, this work is the first to
implement a reranking system to identify retrospectively the most similar
inputs to a GPT model based on its output.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jieh-Sheng Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsiang_J/0/1/0/all/0/1"&gt;Jieh Hsiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Addressing Practical Challenges for RNN-Transducer. (arXiv:2105.00858v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00858</id>
        <link href="http://arxiv.org/abs/2105.00858"/>
        <updated>2021-07-20T02:04:40.886Z</updated>
        <summary type="html"><![CDATA[In this paper, several works are proposed to address practical challenges for
deploying RNN Transducer (RNN-T) based speech recognition system. These
challenges are adapting a well-trained RNN-T model to a new domain without
collecting the audio data, obtaining time stamps and confidence scores at word
level. The first challenge is solved with a splicing data method which
concatenates the speech segments extracted from the source domain data. To get
the time stamp, a phone prediction branch is added to the RNN-T model by
sharing the encoder for the purpose of force alignment. Finally, we obtain
word-level confidence scores by utilizing several types of features calculated
during decoding and from confusion network. Evaluated with Microsoft production
data, the splicing data adaptation method improves the baseline and adaptation
with the text to speech method by 58.03% and 15.25% relative word error rate
reduction, respectively. The proposed time stamping method can get less than
50ms word timing difference from the ground truth alignment on average while
maintaining the recognition accuracy of the RNN-T model. We also obtain high
confidence annotation performance with limited computation cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_R/0/1/0/all/0/1"&gt;Rui Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xue_J/0/1/0/all/0/1"&gt;Jian Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_W/0/1/0/all/0/1"&gt;Wenning Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1"&gt;Lei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yifan Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phase-based Minimalist Parsing and complexity in non-local dependencies. (arXiv:1906.00908v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.00908</id>
        <link href="http://arxiv.org/abs/1906.00908"/>
        <updated>2021-07-20T02:04:40.864Z</updated>
        <summary type="html"><![CDATA[A cognitively plausible parsing algorithm should perform like the human
parser in critical contexts. Here I propose an adaptation of Earley's parsing
algorithm, suitable for Phase-based Minimalist Grammars (PMG, Chesi 2012), that
is able to predict complexity effects in performance. Focusing on self-paced
reading experiments of object clefts sentences (Warren & Gibson 2005) I will
associate to parsing a complexity metric based on cued features to be retrieved
at the verb segment (Feature Retrieval & Encoding Cost, FREC). FREC is
crucially based on the usage of memory predicted by the discussed parsing
algorithm and it correctly fits with the reading time revealed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chesi_C/0/1/0/all/0/1"&gt;Cristiano Chesi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting of a Patient's Condition From Clinical Narratives Using Natural Language Representation. (arXiv:2104.03969v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03969</id>
        <link href="http://arxiv.org/abs/2104.03969"/>
        <updated>2021-07-20T02:04:40.806Z</updated>
        <summary type="html"><![CDATA[The rapid progress in clinical data management systems and artificial
intelligence approaches enable the era of personalized medicine. Intensive care
units (ICUs) are the ideal clinical research environment for such development
because they collect many clinical data and are highly computerized
environments. We designed a retrospective clinical study on a prospective ICU
database using clinical natural language to help in the early diagnosis of
heart failure in critically ill children. The methodology consisted of
empirical experiments of a learning algorithm to learn the hidden
interpretation and presentation of the French clinical note data. This study
included 1386 patients' clinical notes with 5444 single lines of notes. There
were 1941 positive cases (36 % of total) and 3503 negative cases classified by
two independent physicians using a standardized approach. The multilayer
perceptron neural network outperforms other discriminative and generative
classifiers. Consequently, the proposed framework yields an overall
classification performance with 89 % accuracy, 88 % recall, and 89 % precision.
Furthermore, a generative autoencoder learning algorithm was proposed to
leverage the sparsity reduction that achieved 91% accuracy, 91% recall, and 91%
precision. This study successfully applied learning representation and machine
learning algorithms to detect heart failure from clinical natural language in a
single French institution. Further work is needed to use the same methodology
in other institutions and other languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Thanh-Dung Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noumeir_R/0/1/0/all/0/1"&gt;Rita Noumeir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rambaud_J/0/1/0/all/0/1"&gt;Jerome Rambaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sans_G/0/1/0/all/0/1"&gt;Guillaume Sans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jouvet_P/0/1/0/all/0/1"&gt;Philippe Jouvet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Conversational Networks. (arXiv:2106.08484v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08484</id>
        <link href="http://arxiv.org/abs/2106.08484"/>
        <updated>2021-07-20T02:04:40.786Z</updated>
        <summary type="html"><![CDATA[Inspired by recent work in meta-learning and generative teaching networks, we
propose a framework called Generative Conversational Networks, in which
conversational agents learn to generate their own labelled training data (given
some seed data) and then train themselves from that data to perform a given
task. We use reinforcement learning to optimize the data generation process
where the reward signal is the agent's performance on the task. The task can be
any language-related task, from intent detection to full task-oriented
conversations. In this work, we show that our approach is able to generalise
from seed data and performs well in limited data and limited computation
settings, with significant gains for intent detection and slot tagging across
multiple datasets: ATIS, TOD, SNIPS, and Restaurants8k. We show an average
improvement of 35% in intent detection and 21% in slot tagging over a baseline
model trained from the seed data. We also conduct an analysis of the novelty of
the generated data and provide generated examples for intent detection, slot
tagging, and non-goal oriented conversations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1"&gt;Alexandros Papangelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1"&gt;Karthik Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1"&gt;Aishwarya Padmakumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seokhwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1"&gt;Gokhan Tur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1"&gt;Dilek Hakkani-Tur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Definition and a Test for Human-Level Artificial Intelligence. (arXiv:2011.09410v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09410</id>
        <link href="http://arxiv.org/abs/2011.09410"/>
        <updated>2021-07-20T02:04:40.762Z</updated>
        <summary type="html"><![CDATA[Despite recent advances in many application-specific domains, we do not know
how to build a human-level artificial intelligence (HLAI). We conjecture that
learning from others' experience with the language is the essential
characteristic that distinguishes human intelligence from the rest. Humans can
update the action-value function with the verbal description as if they
experience states, actions, and corresponding rewards sequences firsthand. In
this paper, we present a classification of intelligence according to how
individual agents learn and propose a definition and a test for HLAI. The main
idea is that language acquisition without explicit rewards can be a sufficient
test for HLAI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Deokgun Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous Speech Translation for Live Subtitling: from Delay to Display. (arXiv:2107.08807v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08807</id>
        <link href="http://arxiv.org/abs/2107.08807"/>
        <updated>2021-07-20T02:04:40.743Z</updated>
        <summary type="html"><![CDATA[With the increased audiovisualisation of communication, the need for live
subtitles in multilingual events is more relevant than ever. In an attempt to
automatise the process, we aim at exploring the feasibility of simultaneous
speech translation (SimulST) for live subtitling. However, the word-for-word
rate of generation of SimulST systems is not optimal for displaying the
subtitles in a comprehensible and readable way. In this work, we adapt SimulST
systems to predict subtitle breaks along with the translation. We then propose
a display mode that exploits the predicted break structure by presenting the
subtitles in scrolling lines. We compare our proposed mode with a display 1)
word-for-word and 2) in blocks, in terms of reading speed and delay.
Experiments on three language pairs (en$\rightarrow$it, de, fr) show that
scrolling lines is the only mode achieving an acceptable reading speed while
keeping delay close to a 4-second threshold. We argue that simultaneous
translation for readable live subtitles still faces challenges, the main one
being poor translation quality, and propose directions for steering future
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karakanta_A/0/1/0/all/0/1"&gt;Alina Karakanta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1"&gt;Sara Papi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1"&gt;Matteo Negri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1"&gt;Marco Turchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08264</id>
        <link href="http://arxiv.org/abs/2107.08264"/>
        <updated>2021-07-20T02:04:40.723Z</updated>
        <summary type="html"><![CDATA[Multimodal sentiment analysis aims to recognize people's attitudes from
multiple communication channels such as verbal content (i.e., text), voice, and
facial expressions. It has become a vibrant and important research topic in
natural language processing. Much research focuses on modeling the complex
intra- and inter-modal interactions between different communication channels.
However, current multimodal models with strong performance are often
deep-learning-based techniques and work like black boxes. It is not clear how
models utilize multimodal information for sentiment predictions. Despite recent
advances in techniques for enhancing the explainability of machine learning
models, they often target unimodal scenarios (e.g., images, sentences), and
little research has been done on explaining multimodal models. In this paper,
we present an interactive visual analytics system, M2Lens, to visualize and
explain multimodal models for sentiment analysis. M2Lens provides explanations
on intra- and inter-modal interactions at the global, subset, and local levels.
Specifically, it summarizes the influence of three typical interaction types
(i.e., dominance, complement, and conflict) on the model predictions. Moreover,
M2Lens identifies frequent and influential multimodal features and supports the
multi-faceted exploration of model behaviors from language, acoustic, and
visual modalities. Through two case studies and expert interviews, we
demonstrate our system can help users gain deep insights into the multimodal
models for sentiment analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianben He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhihua Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Mapping of Tissue Properties for Magnetic Resonance Fingerprinting. (arXiv:2107.08120v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.08120</id>
        <link href="http://arxiv.org/abs/2107.08120"/>
        <updated>2021-07-20T02:04:40.658Z</updated>
        <summary type="html"><![CDATA[Magnetic resonance Fingerprinting (MRF) is a relatively new multi-parametric
quantitative imaging method that involves a two-step process: (i)
reconstructing a series of time frames from highly-undersampled non-Cartesian
spiral k-space data and (ii) pattern matching using the time frames to infer
tissue properties (e.g., T1 and T2 relaxation times). In this paper, we
introduce a novel end-to-end deep learning framework to seamlessly map the
tissue properties directly from spiral k-space MRF data, thereby avoiding
time-consuming processing such as the nonuniform fast Fourier transform (NUFFT)
and the dictionary-based Fingerprint matching. Our method directly consumes the
non-Cartesian k- space data, performs adaptive density compensation, and
predicts multiple tissue property maps in one forward pass. Experiments on both
2D and 3D MRF data demonstrate that quantification accuracy comparable to
state-of-the-art methods can be accomplished within 0.5 second, which is 1100
to 7700 times faster than the original MRF framework. The proposed method is
thus promising for facilitating the adoption of MRF in clinical settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yilin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yap_P/0/1/0/all/0/1"&gt;Pew-Thian Yap&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Group-based Search Engine Marketing System for E-Commerce. (arXiv:2106.12700v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12700</id>
        <link href="http://arxiv.org/abs/2106.12700"/>
        <updated>2021-07-20T02:04:40.640Z</updated>
        <summary type="html"><![CDATA[With the increasing scale of search engine marketing, designing an efficient
bidding system is becoming paramount for the success of e-commerce companies.
The critical challenges faced by a modern industrial-level bidding system
include: 1. the catalog is enormous, and the relevant bidding features are of
high sparsity; 2. the large volume of bidding requests induces significant
computation burden to both the offline and online serving. Leveraging
extraneous user-item information proves essential to mitigate the sparsity
issue, for which we exploit the natural language signals from the users' query
and the contextual knowledge from the products. In particular, we extract the
vector representations of ads via the Transformer model and leverage their
geometric relation to building collaborative bidding predictions via
clustering. The two-step procedure also significantly reduces the computation
stress of bid evaluation and optimization. In this paper, we introduce the
end-to-end structure of the bidding system for search engine marketing for
Walmart e-commerce, which successfully handles tens of millions of bids each
day. We analyze the online and offline performances of our approach and discuss
how we find it as a production-efficient solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1"&gt;Cheng Jie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Da Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zigeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;Wei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Splitting EUD graphs into trees: A quick and clatty approach. (arXiv:2106.13155v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13155</id>
        <link href="http://arxiv.org/abs/2106.13155"/>
        <updated>2021-07-20T02:04:40.621Z</updated>
        <summary type="html"><![CDATA[We present the system submission from the FASTPARSE team for the EUD Shared
Task at IWPT 2021. We engaged in the task last year by focusing on efficiency.
This year we have focused on experimenting with new ideas on a limited time
budget. Our system is based on splitting the EUD graph into several trees,
based on linguistic criteria. We predict these trees using a sequence-labelling
parser and combine them into an EUD graph. The results were relatively poor,
although not a total disaster and could probably be improved with some
polishing of the system's rough edges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1"&gt;Mark Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1"&gt;Carlos G&amp;#xf3;mez-Rodr&amp;#xed;guez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Missing Cone Artifacts Removal in ODT using Unsupervised Deep Learning in Projection Domain. (arXiv:2103.09022v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09022</id>
        <link href="http://arxiv.org/abs/2103.09022"/>
        <updated>2021-07-20T02:04:40.602Z</updated>
        <summary type="html"><![CDATA[Optical diffraction tomography (ODT) produces three dimensional distribution
of refractive index (RI) by measuring scattering fields at various angles.
Although the distribution of RI index is highly informative, due to the missing
cone problem stemming from the limited-angle acquisition of holograms,
reconstructions have very poor resolution along axial direction compared to the
horizontal imaging plane. To solve this issue, here we present a novel
unsupervised deep learning framework, which learns the probability distribution
of missing projection views through optimal transport driven cycleGAN.
Experimental results show that missing cone artifact in ODT can be
significantly resolved by the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1"&gt;Hyungjin Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huh_J/0/1/0/all/0/1"&gt;Jaeyoung Huh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_G/0/1/0/all/0/1"&gt;Geon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1"&gt;Yong Keun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extreme Rotation Estimation using Dense Correlation Volumes. (arXiv:2104.13530v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13530</id>
        <link href="http://arxiv.org/abs/2104.13530"/>
        <updated>2021-07-20T02:04:40.583Z</updated>
        <summary type="html"><![CDATA[We present a technique for estimating the relative 3D rotation of an RGB
image pair in an extreme setting, where the images have little or no overlap.
We observe that, even when images do not overlap, there may be rich hidden cues
as to their geometric relationship, such as light source directions, vanishing
points, and symmetries present in the scene. We propose a network design that
can automatically learn such implicit cues by comparing all pairs of points
between the two input images. Our method therefore constructs dense feature
correlation volumes and processes these to predict relative 3D rotations. Our
predictions are formed over a fine-grained discretization of rotations,
bypassing difficulties associated with regressing 3D rotations. We demonstrate
our approach on a large variety of extreme RGB image pairs, including indoor
and outdoor images captured under different lighting conditions and geographic
locations. Our evaluation shows that our model can successfully estimate
relative rotations among non-overlapping images without compromising
performance over overlapping image pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1"&gt;Ruojin Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1"&gt;Bharath Hariharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1"&gt;Noah Snavely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1"&gt;Hadar Averbuch-Elor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation. (arXiv:2006.06979v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06979</id>
        <link href="http://arxiv.org/abs/2006.06979"/>
        <updated>2021-07-20T02:04:40.526Z</updated>
        <summary type="html"><![CDATA[Density ratio estimation (DRE) is at the core of various machine learning
tasks such as anomaly detection and domain adaptation. In existing studies on
DRE, methods based on Bregman divergence (BD) minimization have been
extensively studied. However, BD minimization when applied with highly flexible
models, such as deep neural networks, tends to suffer from what we call
train-loss hacking, which is a source of overfitting caused by a typical
characteristic of empirical BD estimators. In this paper, to mitigate
train-loss hacking, we propose a non-negative correction for empirical BD
estimators. Theoretically, we confirm the soundness of the proposed method
through a generalization error bound. Through our experiments, the proposed
methods show a favorable performance in inlier-based outlier detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1"&gt;Masahiro Kato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teshima_T/0/1/0/all/0/1"&gt;Takeshi Teshima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EXACT: A collaboration toolset for algorithm-aided annotation of images with annotation version control. (arXiv:2004.14595v3 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.14595</id>
        <link href="http://arxiv.org/abs/2004.14595"/>
        <updated>2021-07-20T02:04:40.502Z</updated>
        <summary type="html"><![CDATA[In many research areas, scientific progress is accelerated by
multidisciplinary access to image data and their interdisciplinary annotation.
However, keeping track of these annotations to ensure a high-quality
multi-purpose data set is a challenging and labour intensive task. We developed
the open-source online platform EXACT (EXpert Algorithm Collaboration Tool)
that enables the collaborative interdisciplinary analysis of images from
different domains online and offline. EXACT supports multi-gigapixel medical
whole slide images as well as image series with thousands of images. The
software utilises a flexible plugin system that can be adapted to diverse
applications such as counting mitotic figures with a screening mode, finding
false annotations on a novel validation view, or using the latest deep learning
image analysis technologies. This is combined with a version control system
which makes it possible to keep track of changes in the data sets and, for
example, to link the results of deep learning experiments to specific data set
versions. EXACT is freely available and has already been successfully applied
to a broad range of annotation tasks, including highly diverse applications
like deep learning supported cytology scoring, interdisciplinary multi-centre
whole slide image tumour annotation, and highly specialised whale sound
spectroscopy clustering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marzahl_C/0/1/0/all/0/1"&gt;Christian Marzahl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aubreville_M/0/1/0/all/0/1"&gt;Marc Aubreville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertram_C/0/1/0/all/0/1"&gt;Christof A. Bertram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_J/0/1/0/all/0/1"&gt;Jennifer Maier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergler_C/0/1/0/all/0/1"&gt;Christian Bergler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kroger_C/0/1/0/all/0/1"&gt;Christine Kr&amp;#xf6;ger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voigt_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rn Voigt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breininger_K/0/1/0/all/0/1"&gt;Katharina Breininger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klopfleisch_R/0/1/0/all/0/1"&gt;Robert Klopfleisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Positive/Unlabeled Approach for the Segmentation of Medical Sequences using Point-Wise Supervision. (arXiv:2107.08394v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08394</id>
        <link href="http://arxiv.org/abs/2107.08394"/>
        <updated>2021-07-20T02:04:40.480Z</updated>
        <summary type="html"><![CDATA[The ability to quickly annotate medical imaging data plays a critical role in
training deep learning frameworks for segmentation. Doing so for image volumes
or video sequences is even more pressing as annotating these is particularly
burdensome. To alleviate this problem, this work proposes a new method to
efficiently segment medical imaging volumes or videos using point-wise
annotations only. This allows annotations to be collected extremely quickly and
remains applicable to numerous segmentation tasks. Our approach trains a deep
learning model using an appropriate Positive/Unlabeled objective function using
sparse point-wise annotations. While most methods of this kind assume that the
proportion of positive samples in the data is known a-priori, we introduce a
novel self-supervised method to estimate this prior efficiently by combining a
Bayesian estimation framework and new stopping criteria. Our method iteratively
estimates appropriate class priors and yields high segmentation quality for a
variety of object types and imaging modalities. In addition, by leveraging a
spatio-temporal tracking framework, we regularize our predictions by leveraging
the complete data volume. We show experimentally that our approach outperforms
state-of-the-art methods tailored to the same problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lejeune_L/0/1/0/all/0/1"&gt;Laurent Lejeune&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1"&gt;Raphael Sznitman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Unsupervised Data Generation into Self-Supervised Neural Machine Translation for Low-Resource Languages. (arXiv:2107.08772v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08772</id>
        <link href="http://arxiv.org/abs/2107.08772"/>
        <updated>2021-07-20T02:04:40.442Z</updated>
        <summary type="html"><![CDATA[For most language combinations, parallel data is either scarce or simply
unavailable. To address this, unsupervised machine translation (UMT) exploits
large amounts of monolingual data by using synthetic data generation techniques
such as back-translation and noising, while self-supervised NMT (SSNMT)
identifies parallel sentences in smaller comparable data and trains on them. To
date, the inclusion of UMT data generation techniques in SSNMT has not been
investigated. We show that including UMT techniques into SSNMT significantly
outperforms SSNMT and UMT on all tested language pairs, with improvements of up
to +4.3 BLEU, +50.8 BLEU, +51.5 over SSNMT, statistical UMT and hybrid UMT,
respectively, on Afrikaans to English. We further show that the combination of
multilingual denoising autoencoding, SSNMT with backtranslation and bilingual
finetuning enables us to learn machine translation even for distant language
pairs for which only small amounts of monolingual data are available, e.g.
yielding BLEU scores of 11.6 (English to Swahili).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1"&gt;Dana Ruiter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1"&gt;Dietrich Klakow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1"&gt;Josef van Genabith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1"&gt;Cristina Espa&amp;#xf1;a-Bonet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Directions in Abusive Language Training Data: Garbage In, Garbage Out. (arXiv:2004.01670v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.01670</id>
        <link href="http://arxiv.org/abs/2004.01670"/>
        <updated>2021-07-20T02:04:40.423Z</updated>
        <summary type="html"><![CDATA[Data-driven analysis and detection of abusive online content covers many
different tasks, phenomena, contexts, and methodologies. This paper
systematically reviews abusive language dataset creation and content in
conjunction with an open website for cataloguing abusive language data. This
collection of knowledge leads to a synthesis providing evidence-based
recommendations for practitioners working with this complex and highly diverse
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1"&gt;Bertie Vidgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1"&gt;Leon Derczynski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cobordisms and commutative categorial grammars. (arXiv:2107.08728v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08728</id>
        <link href="http://arxiv.org/abs/2107.08728"/>
        <updated>2021-07-20T02:04:40.362Z</updated>
        <summary type="html"><![CDATA[We propose a concrete surface representation of abstract categorial grammars
in the category of word cobordisms or cowordisms for short, which are certain
bipartite graphs decorated with words in a given alphabet, generalizing linear
logic proof-nets. We also introduce and study linear logic grammars, directly
based on cobordisms and using classical multiplicative linear logic as a typing
system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slavnov_S/0/1/0/all/0/1"&gt;Sergey Slavnov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Storywrangler: A massive exploratorium for sociolinguistic, cultural, socioeconomic, and political timelines using Twitter. (arXiv:2007.12988v5 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.12988</id>
        <link href="http://arxiv.org/abs/2007.12988"/>
        <updated>2021-07-20T02:04:40.344Z</updated>
        <summary type="html"><![CDATA[In real-time, social media data strongly imprints world events, popular
culture, and day-to-day conversations by millions of ordinary people at a scale
that is scarcely conventionalized and recorded. Vitally, and absent from many
standard corpora such as books and news archives, sharing and commenting
mechanisms are native to social media platforms, enabling us to quantify social
amplification (i.e., popularity) of trending storylines and contemporary
cultural phenomena. Here, we describe Storywrangler, a natural language
processing instrument designed to carry out an ongoing, day-scale curation of
over 100 billion tweets containing roughly 1 trillion 1-grams from 2008 to
2021. For each day, we break tweets into unigrams, bigrams, and trigrams
spanning over 100 languages. We track n-gram usage frequencies, and generate
Zipf distributions, for words, hashtags, handles, numerals, symbols, and
emojis. We make the data set available through an interactive time series
viewer, and as downloadable time series and daily distributions. Although
Storywrangler leverages Twitter data, our method of extracting and tracking
dynamic changes of n-grams can be extended to any similar social media
platform. We showcase a few examples of the many possible avenues of study we
aim to enable including how social amplification can be visualized through
'contagiograms'. We also present some example case studies that bridge n-gram
time series with disparate data sources to explore sociotechnical dynamics of
famous individuals, box office success, and social unrest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1"&gt;Thayer Alshaabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1"&gt;Jane L. Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1"&gt;Michael V. Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minot_J/0/1/0/all/0/1"&gt;Joshua R. Minot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dewhurst_D/0/1/0/all/0/1"&gt;David R. Dewhurst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reagan_A/0/1/0/all/0/1"&gt;Andrew J. Reagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1"&gt;Christopher M. Danforth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1"&gt;Peter Sheridan Dodds&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MemSum: Extractive Summarization of Long Documents using Multi-step Episodic Markov Decision Processes. (arXiv:2107.08929v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08929</id>
        <link href="http://arxiv.org/abs/2107.08929"/>
        <updated>2021-07-20T02:04:40.322Z</updated>
        <summary type="html"><![CDATA[We introduce MemSum (Multi-step Episodic Markov decision process extractive
SUMmarizer), a reinforcement-learning-based extractive summarizer enriched at
any given time step with information on the current extraction history. Similar
to previous models in this vein, MemSum iteratively selects sentences into the
summary. Our innovation is in considering a broader information set when
summarizing that would intuitively also be used by humans in this task: 1) the
text content of the sentence, 2) the global text context of the rest of the
document, and 3) the extraction history consisting of the set of sentences that
have already been extracted. With a lightweight architecture, MemSum
nonetheless obtains state-of-the-art test-set performance (ROUGE score) on long
document datasets (PubMed, arXiv, and GovReport). Supporting analysis
demonstrates that the added awareness of extraction history gives MemSum
robustness against redundancy in the source document.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1"&gt;Nianlong Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ash_E/0/1/0/all/0/1"&gt;Elliott Ash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hahnloser_R/0/1/0/all/0/1"&gt;Richard H.R. Hahnloser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stock Movement Prediction with Financial News using Contextualized Embedding from BERT. (arXiv:2107.08721v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2107.08721</id>
        <link href="http://arxiv.org/abs/2107.08721"/>
        <updated>2021-07-20T02:04:40.303Z</updated>
        <summary type="html"><![CDATA[News events can greatly influence equity markets. In this paper, we are
interested in predicting the short-term movement of stock prices after
financial news events using only the headlines of the news. To achieve this
goal, we introduce a new text mining method called Fine-Tuned
Contextualized-Embedding Recurrent Neural Network (FT-CE-RNN). Compared with
previous approaches which use static vector representations of the news (static
embedding), our model uses contextualized vector representations of the
headlines (contextualized embeddings) generated from Bidirectional Encoder
Representations from Transformers (BERT). Our model obtains the
state-of-the-art result on this stock movement prediction task. It shows
significant improvement compared with other baseline models, in both accuracy
and trading simulations. Through various trading simulations based on millions
of headlines from Bloomberg News, we demonstrate the ability of this model in
real scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qinkai Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[E-PDDL: A Standardized Way of Defining Epistemic Planning Problems. (arXiv:2107.08739v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.08739</id>
        <link href="http://arxiv.org/abs/2107.08739"/>
        <updated>2021-07-20T02:04:40.282Z</updated>
        <summary type="html"><![CDATA[Epistemic Planning (EP) refers to an automated planning setting where the
agent reasons in the space of knowledge states and tries to find a plan to
reach a desirable state from the current state. Its general form, the
Multi-agent Epistemic Planning (MEP) problem involves multiple agents who need
to reason about both the state of the world and the information flow between
agents. In a MEP problem, multiple approaches have been developed recently with
varying restrictions, such as considering only the concept of knowledge while
not allowing the idea of belief, or not allowing for ``complex" modal operators
such as those needed to handle dynamic common knowledge. While the diversity of
approaches has led to a deeper understanding of the problem space, the lack of
a standardized way to specify MEP problems independently of solution approaches
has created difficulties in comparing performance of planners, identifying
promising techniques, exploring new strategies like ensemble methods, and
making it easy for new researchers to contribute to this research area. To
address the situation, we propose a unified way of specifying EP problems - the
Epistemic Planning Domain Definition Language, E-PDDL. We show that E-PPDL can
be supported by leading MEP planners and provide corresponding parser code that
translates EP problems specified in E-PDDL into (M)EP problems that can be
handled by several planners. This work is also useful in building more general
epistemic planning environments where we envision a meta-cognitive module that
takes a planning problem in E-PDDL, identifies and assesses some of its
features, and autonomously decides which planner is the best one to solve it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fabiano_F/0/1/0/all/0/1"&gt;Francesco Fabiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1"&gt;Biplav Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenchner_J/0/1/0/all/0/1"&gt;Jonathan Lenchner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horesh_L/0/1/0/all/0/1"&gt;Lior Horesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1"&gt;Francesca Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganapini_M/0/1/0/all/0/1"&gt;Marianna Bergamaschi Ganapini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proactive Retrieval-based Chatbots based on Relevant Knowledge and Goals. (arXiv:2107.08329v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08329</id>
        <link href="http://arxiv.org/abs/2107.08329"/>
        <updated>2021-07-20T02:04:40.260Z</updated>
        <summary type="html"><![CDATA[A proactive dialogue system has the ability to proactively lead the
conversation. Different from the general chatbots which only react to the user,
proactive dialogue systems can be used to achieve some goals, e.g., to
recommend some items to the user. Background knowledge is essential to enable
smooth and natural transitions in dialogue. In this paper, we propose a new
multi-task learning framework for retrieval-based knowledge-grounded proactive
dialogue. To determine the relevant knowledge to be used, we frame knowledge
prediction as a complementary task and use explicit signals to supervise its
learning. The final response is selected according to the predicted knowledge,
the goal to achieve, and the context. Experimental results show that explicit
modeling of knowledge prediction and goal selection can greatly improve the
final response selection. Our code is available at
https://github.com/DaoD/KPN/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yutao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1"&gt;Jian-Yun Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_P/0/1/0/all/0/1"&gt;Pan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1"&gt;Zhicheng Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Law of Large Documents: Understanding the Structure of Legal Contracts Using Visual Cues. (arXiv:2107.08128v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08128</id>
        <link href="http://arxiv.org/abs/2107.08128"/>
        <updated>2021-07-20T02:04:40.191Z</updated>
        <summary type="html"><![CDATA[Large, pre-trained transformer models like BERT have achieved
state-of-the-art results on document understanding tasks, but most
implementations can only consider 512 tokens at a time. For many real-world
applications, documents can be much longer, and the segmentation strategies
typically used on longer documents miss out on document structure and
contextual information, hurting their results on downstream tasks. In our work
on legal agreements, we find that visual cues such as layout, style, and
placement of text in a document are strong features that are crucial to
achieving an acceptable level of accuracy on long documents. We measure the
impact of incorporating such visual cues, obtained via computer vision methods,
on the accuracy of document understanding tasks including document
segmentation, entity extraction, and attribute classification. Our method of
segmenting documents based on structural metadata out-performs existing methods
on four long-document understanding tasks as measured on the Contract
Understanding Atticus Dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hegel_A/0/1/0/all/0/1"&gt;Allison Hegel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Marina Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peaslee_G/0/1/0/all/0/1"&gt;Genevieve Peaslee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roof_B/0/1/0/all/0/1"&gt;Brendan Roof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elwany_E/0/1/0/all/0/1"&gt;Emad Elwany&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison of Methods for OOV-word Recognition on a New Public Dataset. (arXiv:2107.08091v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08091</id>
        <link href="http://arxiv.org/abs/2107.08091"/>
        <updated>2021-07-20T02:04:40.151Z</updated>
        <summary type="html"><![CDATA[A common problem for automatic speech recognition systems is how to recognize
words that they did not see during training. Currently there is no established
method of evaluating different techniques for tackling this problem. We propose
using the CommonVoice dataset to create test sets for multiple languages which
have a high out-of-vocabulary (OOV) ratio relative to a training set and
release a new tool for calculating relevant performance metrics. We then
evaluate, within the context of a hybrid ASR system, how much better subword
models are at recognizing OOVs, and how much benefit one can get from
incorporating OOV-word information into an existing system by modifying WFSTs.
Additionally, we propose a new method for modifying a subword-based language
model so as to better recognize OOV-words. We showcase very large improvements
in OOV-word recognition and make both the data and code available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Braun_R/0/1/0/all/0/1"&gt;Rudolf A. Braun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madikeri_S/0/1/0/all/0/1"&gt;Srikanth Madikeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1"&gt;Petr Motlicek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Knowledge-Driven Approach to Classifying Object and Attribute Coreferences in Opinion Mining. (arXiv:2010.05357v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05357</id>
        <link href="http://arxiv.org/abs/2010.05357"/>
        <updated>2021-07-20T02:04:40.129Z</updated>
        <summary type="html"><![CDATA[Classifying and resolving coreferences of objects (e.g., product names) and
attributes (e.g., product aspects) in opinionated reviews is crucial for
improving the opinion mining performance. However, the task is challenging as
one often needs to consider domain-specific knowledge (e.g., iPad is a tablet
and has aspect resolution) to identify coreferences in opinionated reviews.
Also, compiling a handcrafted and curated domain-specific knowledge base for
each domain is very time consuming and arduous. This paper proposes an approach
to automatically mine and leverage domain-specific knowledge for classifying
objects and attribute coreferences. The approach extracts domain-specific
knowledge from unlabeled review data and trains a knowledgeaware neural
coreference classification model to leverage (useful) domain knowledge together
with general commonsense knowledge for the task. Experimental evaluation on
realworld datasets involving five domains (product types) shows the
effectiveness of the approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiahua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumder_S/0/1/0/all/0/1"&gt;Sahisnu Mazumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech. (arXiv:2107.08720v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08720</id>
        <link href="http://arxiv.org/abs/2107.08720"/>
        <updated>2021-07-20T02:04:40.103Z</updated>
        <summary type="html"><![CDATA[Undermining the impact of hateful content with informed and non-aggressive
responses, called counter narratives, has emerged as a possible solution for
having healthier online communities. Thus, some NLP studies have started
addressing the task of counter narrative generation. Although such studies have
made an effort to build hate speech / counter narrative (HS/CN) datasets for
neural generation, they fall short in reaching either high-quality and/or
high-quantity. In this paper, we propose a novel human-in-the-loop data
collection methodology in which a generative language model is refined
iteratively by using its own data from the previous loops to generate new
training samples that experts review and/or post-edit. Our experiments
comprised several loops including dynamic variations. Results show that the
methodology is scalable and facilitates diverse, novel, and cost-effective data
collection. To our knowledge, the resulting dataset is the only expert-based
multi-target HS/CN dataset available to the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fanton_M/0/1/0/all/0/1"&gt;Margherita Fanton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonaldi_H/0/1/0/all/0/1"&gt;Helena Bonaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tekiroglu_S/0/1/0/all/0/1"&gt;Serra Sinem Tekiroglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1"&gt;Marco Guerini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08661</id>
        <link href="http://arxiv.org/abs/2107.08661"/>
        <updated>2021-07-20T02:04:40.071Z</updated>
        <summary type="html"><![CDATA[We present Translatotron 2, a neural direct speech-to-speech translation
model that can be trained end-to-end. Translatotron 2 consists of a speech
encoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention
module that connects all the previous three components. Experimental results
suggest that Translatotron 2 outperforms the original Translatotron by a large
margin in terms of translation quality and predicted speech naturalness, and
drastically improves the robustness of the predicted speech by mitigating
over-generation, such as babbling or long pause. We also propose a new method
for retaining the source speaker's voice in the translated speech. The trained
model is restricted to retain the source speaker's voice, and unlike the
original Translatotron, it is not able to generate speech in a different
speaker's voice, making the model more robust for production deployment, by
mitigating potential misuse for creating spoofing audio artifacts. When the new
method is used together with a simple concatenation-based data augmentation,
the trained Translatotron 2 model is able to retain each speaker's voice for
input with speaker turns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Ye Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1"&gt;Michelle Tadmor Ramanovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1"&gt;Tal Remez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1"&gt;Roi Pomerantz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images. (arXiv:2107.08685v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08685</id>
        <link href="http://arxiv.org/abs/2107.08685"/>
        <updated>2021-07-20T02:04:40.007Z</updated>
        <summary type="html"><![CDATA[In multi-modal dialogue systems, it is important to allow the use of images
as part of a multi-turn conversation. Training such dialogue systems generally
requires a large-scale dataset consisting of multi-turn dialogues that involve
images, but such datasets rarely exist. In response, this paper proposes a 45k
multi-modal dialogue dataset created with minimal human intervention. Our
method to create such a dataset consists of (1) preparing and pre-processing
text dialogue datasets, (2) creating image-mixed dialogues by using a
text-to-image replacement technique, and (3) employing a
contextual-similarity-based filtering step to ensure the contextual coherence
of the dataset. To evaluate the validity of our dataset, we devise a simple
retrieval model for dialogue sentence prediction tasks. Automatic metrics and
human evaluation results on such tasks show that our dataset can be effectively
used as training data for multi-modal dialogue systems which require an
understanding of images and text in a context-aware manner. Our dataset and
generation code is available at
https://github.com/shh1574/multi-modal-dialogue-dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1"&gt;Nyoungwoo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1"&gt;Suwon Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1"&gt;Jaegul Choo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1"&gt;Ho-Jin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Myaeng_S/0/1/0/all/0/1"&gt;Sung-Hyun Myaeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Language Models as Prior Knowledge for Playing Text-based Games. (arXiv:2107.08408v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08408</id>
        <link href="http://arxiv.org/abs/2107.08408"/>
        <updated>2021-07-20T02:04:39.987Z</updated>
        <summary type="html"><![CDATA[Recently, text world games have been proposed to enable artificial agents to
understand and reason about real-world scenarios. These text-based games are
challenging for artificial agents, as it requires understanding and interaction
using natural language in a partially observable environment. In this paper, we
improve the semantic understanding of the agent by proposing a simple RL with
LM framework where we use transformer-based language models with Deep RL
models. We perform a detailed study of our framework to demonstrate how our
model outperforms all existing agents on the popular game, Zork1, to achieve a
score of 44.7, which is 1.6 higher than the state-of-the-art model. Our
proposed approach also performs comparably to the state-of-the-art models on
the other set of text games.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1"&gt;Ishika Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1"&gt;Gargi Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1"&gt;Ashutosh Modi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Argument Linking: A Survey and Forecast. (arXiv:2107.08523v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08523</id>
        <link href="http://arxiv.org/abs/2107.08523"/>
        <updated>2021-07-20T02:04:39.968Z</updated>
        <summary type="html"><![CDATA[Semantic role labeling (SRL) -- identifying the semantic relationships
between a predicate and other constituents in the same sentence -- is a
well-studied task in natural language understanding (NLU). However, many of
these relationships are evident only at the level of the document, as a role
for a predicate in one sentence may often be filled by an argument in a
different one. This more general task, known as implicit semantic role labeling
or argument linking, has received increased attention in recent years, as
researchers have recognized its centrality to information extraction and NLU.
This paper surveys the literature on argument linking and identifies several
notable shortcomings of existing approaches that indicate the paths along which
future research effort could most profitably be spent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gantt_W/0/1/0/all/0/1"&gt;William Gantt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking. (arXiv:2107.08173v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08173</id>
        <link href="http://arxiv.org/abs/2107.08173"/>
        <updated>2021-07-20T02:04:39.940Z</updated>
        <summary type="html"><![CDATA[This ability to learn consecutive tasks without forgetting how to perform
previously trained problems is essential for developing an online dialogue
system. This paper proposes an effective continual learning for the
task-oriented dialogue system with iterative network pruning, expanding and
masking (TPEM), which preserves performance on previously encountered tasks
while accelerating learning progress on subsequent tasks. Specifically, TPEM
(i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts
network expanding to create free weights for new tasks, and (iii) introduces
task-specific network masking to alleviate the negative impact of fixed weights
of old tasks on new tasks. We conduct extensive experiments on seven different
tasks from three benchmark datasets and show empirically that TPEM leads to
significantly improved results over the strong competitors. For
reproducibility, we submit the code and data at:
https://github.com/siat-nlp/TPEM]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geng_B/0/1/0/all/0/1"&gt;Binzong Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1"&gt;Fajie Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiancheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Ying Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruifeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Min Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap between Language Model and Reading Comprehension: Unsupervised MRC via Self-Supervision. (arXiv:2107.08582v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08582</id>
        <link href="http://arxiv.org/abs/2107.08582"/>
        <updated>2021-07-20T02:04:39.921Z</updated>
        <summary type="html"><![CDATA[Despite recent success in machine reading comprehension (MRC), learning
high-quality MRC models still requires large-scale labeled training data, even
using strong pre-trained language models (PLMs). The pre-training tasks for
PLMs are not question-answering or MRC-based tasks, making existing PLMs unable
to be directly used for unsupervised MRC. Specifically, MRC aims to spot an
accurate answer span from the given document, but PLMs focus on token filling
in sentences. In this paper, we propose a new framework for unsupervised MRC.
Firstly, we propose to learn to spot answer spans in documents via
self-supervised learning, by designing a self-supervision pretext task for MRC
- Spotting-MLM. Solving this task requires capturing deep interactions between
sentences in documents. Secondly, we apply a simple sentence rewriting strategy
in the inference stage to alleviate the expression mismatch between questions
and documents. Experiments show that our method achieves a new state-of-the-art
performance for unsupervised MRC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bian_N/0/1/0/all/0/1"&gt;Ning Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xianpei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hongyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1"&gt;Ben He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Le Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08251</id>
        <link href="http://arxiv.org/abs/2107.08251"/>
        <updated>2021-07-20T02:04:39.649Z</updated>
        <summary type="html"><![CDATA[We introduce ParaBLEU, a paraphrase representation learning model and
evaluation metric for text generation. Unlike previous approaches, ParaBLEU
learns to understand paraphrasis using generative conditioning as a pretraining
objective. ParaBLEU correlates more strongly with human judgements than
existing metrics, obtaining new state-of-the-art results on the 2017 WMT
Metrics Shared Task. We show that our model is robust to data scarcity,
exceeding previous state-of-the-art performance using only $50\%$ of the
available training data and surpassing BLEU, ROUGE and METEOR with only $40$
labelled examples. Finally, we demonstrate that ParaBLEU can be used to
conditionally generate novel paraphrases from a single demonstration, which we
use to confirm our hypothesis that it learns abstract, generalized paraphrase
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jack Weston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1"&gt;Raphael Lenain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1"&gt;Udeepa Meepegama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1"&gt;Emil Fristed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeHumor: Visual Analytics for Decomposing Humor. (arXiv:2107.08356v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08356</id>
        <link href="http://arxiv.org/abs/2107.08356"/>
        <updated>2021-07-20T02:04:39.589Z</updated>
        <summary type="html"><![CDATA[Despite being a critical communication skill, grasping humor is challenging
-- a successful use of humor requires a mixture of both engaging content
build-up and an appropriate vocal delivery (e.g., pause). Prior studies on
computational humor emphasize the textual and audio features immediately next
to the punchline, yet overlooking longer-term context setup. Moreover, the
theories are usually too abstract for understanding each concrete humor
snippet. To fill in the gap, we develop DeHumor, a visual analytical system for
analyzing humorous behaviors in public speaking. To intuitively reveal the
building blocks of each concrete example, DeHumor decomposes each humorous
video into multimodal features and provides inline annotations of them on the
video script. In particular, to better capture the build-ups, we introduce
content repetition as a complement to features introduced in theories of
computational humor and visualize them in a context linking graph. To help
users locate the punchlines that have the desired features to learn, we
summarize the content (with keywords) and humor feature statistics on an
augmented time matrix. With case studies on stand-up comedy shows and TED
talks, we show that DeHumor is able to highlight various building blocks of
humor examples. In addition, expert interviews with communication coaches and
humor researchers demonstrate the effectiveness of DeHumor for multimodal humor
analysis of speech content and vocal delivery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1"&gt;Yao Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tongshuang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Haipeng Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A pattern recognition approach for distinguishing between prose and poetry. (arXiv:2107.08512v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08512</id>
        <link href="http://arxiv.org/abs/2107.08512"/>
        <updated>2021-07-20T02:04:39.557Z</updated>
        <summary type="html"><![CDATA[Poetry and prose are written artistic expressions that help us to appreciate
the reality we live. Each of these styles has its own set of subjective
properties, such as rhyme and rhythm, which are easily caught by a human
reader's eye and ear. With the recent advances in artificial intelligence, the
gap between humans and machines may have decreased, and today we observe
algorithms mastering tasks that were once exclusively performed by humans. In
this paper, we propose an automated method to distinguish between poetry and
prose based solely on aural and rhythmic properties. In other to compare prose
and poetry rhythms, we represent the rhymes and phones as temporal sequences
and thus we propose a procedure for extracting rhythmic features from these
sequences. The classification of the considered texts using the set of features
extracted resulted in a best accuracy of 0.78, obtained with a neural network.
Interestingly, by using an approach based on complex networks to visualize the
similarities between the different texts considered, we found that the patterns
of poetry vary much more than prose. Consequently, a much richer and complex
set of rhythmic possibilities tends to be found in that modality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arruda_H/0/1/0/all/0/1"&gt;Henrique F. de Arruda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reia_S/0/1/0/all/0/1"&gt;Sandro M. Reia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_F/0/1/0/all/0/1"&gt;Filipi N. Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amancio_D/0/1/0/all/0/1"&gt;Diego R. Amancio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1"&gt;Luciano da F. Costa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[As Easy as 1, 2, 3: Behavioural Testing of NMT Systems for Numerical Translation. (arXiv:2107.08357v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08357</id>
        <link href="http://arxiv.org/abs/2107.08357"/>
        <updated>2021-07-20T02:04:39.369Z</updated>
        <summary type="html"><![CDATA[Mistranslated numbers have the potential to cause serious effects, such as
financial loss or medical misinformation. In this work we develop comprehensive
assessments of the robustness of neural machine translation systems to
numerical text via behavioural testing. We explore a variety of numerical
translation capabilities a system is expected to exhibit and design effective
test examples to expose system underperformance. We find that numerical
mistranslation is a general issue: major commercial systems and
state-of-the-art research models fail on many of our test examples, for high-
and low-resource languages. Our tests reveal novel errors that have not
previously been reported in NMT systems, to the best of our knowledge. Lastly,
we discuss strategies to mitigate numerical mistranslation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1"&gt;Francisco Guzman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1"&gt;Ahmed El-Kishky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1"&gt;Benjamin I. P. Rubinstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1"&gt;Trevor Cohn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08199</id>
        <link href="http://arxiv.org/abs/2107.08199"/>
        <updated>2021-07-20T02:04:39.289Z</updated>
        <summary type="html"><![CDATA[The Transformer architecture is widely used for machine translation tasks.
However, its resource-intensive nature makes it challenging to implement on
constrained embedded devices, particularly where available hardware resources
can vary at run-time. We propose a dynamic machine translation model that
scales the Transformer architecture based on the available resources at any
particular time. The proposed approach, 'Dynamic-HAT', uses a HAT
SuperTransformer as the backbone to search for SubTransformers with different
accuracy-latency trade-offs at design time. The optimal SubTransformers are
sampled from the SuperTransformer at run-time, depending on latency
constraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses
inherited SubTransformers sampled directly from the SuperTransformer with a
switching time of <1s. Using inherited SubTransformers results in a BLEU score
loss of <1.5% because the SubTransformer configuration is not retrained from
scratch after sampling. However, to recover this loss in performance, the
dimensions of the design space can be reduced to tailor it to a family of
target hardware. The new reduced design space results in a BLEU score increase
of approximately 1% for sub-optimal models from the original design space, with
a wide range for performance scaling between 0.356s - 1.526s for the GPU and
2.9s - 7.31s for the CPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1"&gt;Hishan Parry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1"&gt;Lei Xun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1"&gt;Amin Sabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1"&gt;Jia Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1"&gt;Geoff V. Merrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Potential of Lexical Paraphrases for Mitigating Noise-Induced Comprehension Errors. (arXiv:2107.08337v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08337</id>
        <link href="http://arxiv.org/abs/2107.08337"/>
        <updated>2021-07-20T02:04:39.266Z</updated>
        <summary type="html"><![CDATA[Listening in noisy environments can be difficult even for individuals with a
normal hearing thresholds. The speech signal can be masked by noise, which may
lead to word misperceptions on the side of the listener, and overall difficulty
to understand the message. To mitigate hearing difficulties on listeners, a
co-operative speaker utilizes voice modulation strategies like Lombard speech
to generate noise-robust utterances, and similar solutions have been developed
for speech synthesis systems. In this work, we propose an alternate solution of
choosing noise-robust lexical paraphrases to represent an intended meaning. Our
results show that lexical paraphrases differ in their intelligibility in noise.
We evaluate the intelligibility of synonyms in context and find that choosing a
lexical unit that is less risky to be misheard than its synonym introduced an
average gain in comprehension of 37% at SNR -5 dB and 21% at SNR 0 dB for
babble noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chingacham_A/0/1/0/all/0/1"&gt;Anupama Chingacham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1"&gt;Vera Demberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1"&gt;Dietrich Klakow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Copying Behaviors of Pre-Training for Neural Machine Translation. (arXiv:2107.08212v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08212</id>
        <link href="http://arxiv.org/abs/2107.08212"/>
        <updated>2021-07-20T02:04:39.238Z</updated>
        <summary type="html"><![CDATA[Previous studies have shown that initializing neural machine translation
(NMT) models with the pre-trained language models (LM) can speed up the model
training and boost the model performance. In this work, we identify a critical
side-effect of pre-training for NMT, which is due to the discrepancy between
the training objectives of LM-based pre-training and NMT. Since the LM
objective learns to reconstruct a few source tokens and copy most of them, the
pre-training initialization would affect the copying behaviors of NMT models.
We provide a quantitative analysis of copying behaviors by introducing a metric
called copying ratio, which empirically shows that pre-training based NMT
models have a larger copying ratio than the standard one. In response to this
problem, we propose a simple and effective method named copying penalty to
control the copying behaviors in decoding. Extensive experiments on both
in-domain and out-of-domain benchmarks show that the copying penalty method
consistently improves translation performance by controlling copying behaviors
for pre-training based NMT models. Source code is freely available at
https://github.com/SunbowLiu/CopyingPenalty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuebo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Longyue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1"&gt;Derek F. Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"&gt;Liang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1"&gt;Lidia S. Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuming Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhaopeng Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning De-identified Representations of Prosody from Raw Audio. (arXiv:2107.08248v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08248</id>
        <link href="http://arxiv.org/abs/2107.08248"/>
        <updated>2021-07-20T02:04:39.186Z</updated>
        <summary type="html"><![CDATA[We propose a method for learning de-identified prosody representations from
raw audio using a contrastive self-supervised signal. Whereas prior work has
relied on conditioning models on bottlenecks, we introduce a set of inductive
biases that exploit the natural structure of prosody to minimize timbral
information and decouple prosody from speaker representations. Despite
aggressive downsampling of the input and having no access to linguistic
information, our model performs comparably to state-of-the-art speech
representations on DAMMP, a new benchmark we introduce for spoken language
understanding. We use minimum description length probing to show that our
representations have selectively learned the subcomponents of non-timbral
prosody, and that the product quantizer naturally disentangles them without
using bottlenecks. We derive an information-theoretic definition of speech
de-identifiability and use it to demonstrate that our prosody representations
are less identifiable than other speech representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jack Weston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1"&gt;Raphael Lenain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1"&gt;Udeepa Meepegama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1"&gt;Emil Fristed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond a binary of (non)racist tweets: A four-dimensional categorical detection and analysis of racist and xenophobic opinions on Twitter in early Covid-19. (arXiv:2107.08347v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2107.08347</id>
        <link href="http://arxiv.org/abs/2107.08347"/>
        <updated>2021-07-20T02:04:39.148Z</updated>
        <summary type="html"><![CDATA[Transcending the binary categorization of racist and xenophobic texts, this
research takes cues from social science theories to develop a four dimensional
category for racism and xenophobia detection, namely stigmatization,
offensiveness, blame, and exclusion. With the aid of deep learning techniques,
this categorical detection enables insights into the nuances of emergent topics
reflected in racist and xenophobic expression on Twitter. Moreover, a stage
wise analysis is applied to capture the dynamic changes of the topics across
the stages of early development of Covid-19 from a domestic epidemic to an
international public health emergency, and later to a global pandemic. The main
contributions of this research include, first the methodological advancement.
By bridging the state-of-the-art computational methods with social science
perspective, this research provides a meaningful approach for future research
to gain insight into the underlying subtlety of racist and xenophobic
discussion on digital platforms. Second, by enabling a more accurate
comprehension and even prediction of public opinions and actions, this research
paves the way for the enactment of effective intervention policies to combat
racist crimes and social exclusion under Covid-19.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pei_X/0/1/0/all/0/1"&gt;Xin Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1"&gt;Deval Mehta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overview and Insights from the SciVer Shared Task on Scientific Claim Verification. (arXiv:2107.08188v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08188</id>
        <link href="http://arxiv.org/abs/2107.08188"/>
        <updated>2021-07-20T02:04:39.083Z</updated>
        <summary type="html"><![CDATA[We present an overview of the SciVer shared task, presented at the 2nd
Scholarly Document Processing (SDP) workshop at NAACL 2021. In this shared
task, systems were provided a scientific claim and a corpus of research
abstracts, and asked to identify which articles SUPPORT or REFUTE the claim as
well as provide evidentiary sentences justifying those labels. 11 teams made a
total of 14 submissions to the shared task leaderboard, leading to an
improvement of more than +23 F1 on the primary task evaluation metric. In
addition to surveying the participating systems, we provide several insights
into modeling approaches to support continued progress and future research on
the important and challenging task of scientific claim verification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1"&gt;David Wadden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1"&gt;Kyle Lo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Darmok and Jalad at Tanagra: A Dataset and Model for English-to-Tamarian Translation. (arXiv:2107.08146v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08146</id>
        <link href="http://arxiv.org/abs/2107.08146"/>
        <updated>2021-07-20T02:04:39.061Z</updated>
        <summary type="html"><![CDATA[Tamarian, a fictional language introduced in the Star Trek episode Darmok,
communicates meaning through utterances of metaphorical references, such as
"Darmok and Jalad at Tanagra" instead of "We should work together." This work
assembles a Tamarian-English dictionary of utterances from the original episode
and several follow-on novels, and uses this to construct a parallel corpus of
456 English-Tamarian utterances. A machine translation system based on a large
language model (T5) is trained using this parallel corpus, and is shown to
produce an accuracy of 76% when translating from English to Tamarian on known
utterances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1"&gt;Peter Jansen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Architectures of Meaning, A Systematic Corpus Analysis of NLP Systems. (arXiv:2107.08124v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08124</id>
        <link href="http://arxiv.org/abs/2107.08124"/>
        <updated>2021-07-20T02:04:39.034Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel statistical corpus analysis framework targeted
towards the interpretation of Natural Language Processing (NLP) architectural
patterns at scale. The proposed approach combines saturation-based lexicon
construction, statistical corpus analysis methods and graph collocations to
induce a synthesis representation of NLP architectural patterns from corpora.
The framework is validated in the full corpus of Semeval tasks and demonstrated
coherent architectural patterns which can be used to answer architectural
questions on a data-driven fashion, providing a systematic mechanism to
interpret a largely dynamic and exponentially growing field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wysocki_O/0/1/0/all/0/1"&gt;Oskar Wysocki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Florea_M/0/1/0/all/0/1"&gt;Malina Florea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1"&gt;Donal Landers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1"&gt;Andre Freitas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Form 10-Q Itemization. (arXiv:2104.11783v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11783</id>
        <link href="http://arxiv.org/abs/2104.11783"/>
        <updated>2021-07-20T02:04:39.008Z</updated>
        <summary type="html"><![CDATA[The quarterly financial statement, or Form 10-Q, is one of the most
frequently required filings for US public companies to disclose financial and
other important business information. Due to the massive volume of 10-Q filings
and the enormous variations in the reporting format, it has been a
long-standing challenge to retrieve item-specific information from 10-Q filings
that lack machine-readable hierarchy. This paper presents a solution for
itemizing 10-Q files by complementing a rule-based algorithm with a
Convolutional Neural Network (CNN) image classifier. This solution demonstrates
a pipeline that can be generalized to a rapid data retrieval solution among a
large volume of textual data using only typographic items. The extracted
textual data can be used as unlabeled content-specific data to train
transformer models (e.g., BERT) or fit into various field-focus natural
language processing (NLP) applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanci Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1"&gt;Tianming Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yujie Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donohue_L/0/1/0/all/0/1"&gt;Lawrence Donohue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1"&gt;Rui Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy Efficient Data Recovery from Corrupted LoRa Frames. (arXiv:2107.08868v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.08868</id>
        <link href="http://arxiv.org/abs/2107.08868"/>
        <updated>2021-07-20T02:04:38.982Z</updated>
        <summary type="html"><![CDATA[High frame-corruption is widely observed in Long Range Wide Area Networks
(LoRaWAN) due to the coexistence with other networks in ISM bands and an
Aloha-like MAC layer. LoRa's Forward Error Correction (FEC) mechanism is often
insufficient to retrieve corrupted data. In fact, real-life measurements show
that at least one-fourth of received transmissions are corrupted. When more
frames are dropped, LoRa nodes usually switch over to higher spreading factors
(SF), thus increasing transmission times and increasing the required energy.
This paper introduces ReDCoS, a novel coding technique at the application layer
that improves recovery of corrupted LoRa frames, thus reducing the overall
transmission time and energy invested by LoRa nodes by several-fold. ReDCoS
utilizes lightweight coding techniques to pre-encode the transmitted data.
Therefore, the inbuilt Cyclic Redundancy Check (CRC) that follows is computed
based on an already encoded data. At the receiver, we use both the CRC and the
coded data to recover data from a corrupted frame beyond the built-in Error
Correcting Code (ECC). We compare the performance of ReDCoS to (I) the standard
FEC of vanilla-LoRaWAN, and to (ii) RS coding applied as ECC to the data of
LoRaWAN. The results indicated a 54x and 13.5x improvement of decoding ratio,
respectively, when 20 data symbols were sent. Furthermore, we evaluated ReDCoS
on-field using LoRa SX1261 transceivers showing that it outperformed RS-coding
by factor of at least 2x (and up to 6x) in terms of the decoding ratio while
consuming 38.5% less energy per correctly received transmission.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yazdani_N/0/1/0/all/0/1"&gt;Niloofar Yazdani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kouvelas_N/0/1/0/all/0/1"&gt;Nikolaos Kouvelas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prasad_R/0/1/0/all/0/1"&gt;R Venkatesha Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucani_D/0/1/0/all/0/1"&gt;Daniel E. Lucani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Document Visual Question Answering Challenge 2020. (arXiv:2008.08899v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08899</id>
        <link href="http://arxiv.org/abs/2008.08899"/>
        <updated>2021-07-20T02:04:38.396Z</updated>
        <summary type="html"><![CDATA[This paper presents results of Document Visual Question Answering Challenge
organized as part of "Text and Documents in the Deep Learning Era" workshop, in
CVPR 2020. The challenge introduces a new problem - Visual Question Answering
on document images. The challenge comprised two tasks. The first task concerns
with asking questions on a single document image. On the other hand, the second
task is set as a retrieval task where the question is posed over a collection
of images. For the task 1 a new dataset is introduced comprising 50,000
questions-answer(s) pairs defined over 12,767 document images. For task 2
another dataset has been created comprising 20 questions over 14,362 document
images which share the same document template.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1"&gt;Minesh Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1"&gt;Ruben Tito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1"&gt;Dimosthenis Karatzas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1"&gt;R. Manmatha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1"&gt;C.V. Jawahar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mismatched Estimation of rank-one symmetric matrices under Gaussian noise. (arXiv:2107.08927v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.08927</id>
        <link href="http://arxiv.org/abs/2107.08927"/>
        <updated>2021-07-20T02:04:38.368Z</updated>
        <summary type="html"><![CDATA[We consider the estimation of an n-dimensional vector s from the noisy
element-wise measurements of $\mathbf{s}\mathbf{s}^T$, a generic problem that
arises in statistics and machine learning. We study a mismatched Bayesian
inference setting, where some of the parameters are not known to the
statistician. We derive the full exact analytic expression of the asymptotic
mean squared error (MSE) in the large system size limit for the particular case
of Gaussian priors and additive noise. From our formulas, we see that
estimation is still possible in the mismatched case; and also that the minimum
MSE (MMSE) can be achieved if the statistician chooses suitable parameters. Our
technique relies on the asymptotics of the spherical integrals and can be
applied as long as the statistician chooses a rotationally invariant prior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pourkamali_F/0/1/0/all/0/1"&gt;Farzad Pourkamali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macris_N/0/1/0/all/0/1"&gt;Nicolas Macris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GuideBoot: Guided Bootstrap for Deep Contextual Bandits. (arXiv:2107.08383v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08383</id>
        <link href="http://arxiv.org/abs/2107.08383"/>
        <updated>2021-07-20T02:04:38.289Z</updated>
        <summary type="html"><![CDATA[The exploration/exploitation (E&E) dilemma lies at the core of interactive
systems such as online advertising, for which contextual bandit algorithms have
been proposed. Bayesian approaches provide guided exploration with principled
uncertainty estimation, but the applicability is often limited due to
over-simplified assumptions. Non-Bayesian bootstrap methods, on the other hand,
can apply to complex problems by using deep reward models, but lacks clear
guidance to the exploration behavior. It still remains largely unsolved to
develop a practical method for complex deep contextual bandits.

In this paper, we introduce Guided Bootstrap (GuideBoot for short), combining
the best of both worlds. GuideBoot provides explicit guidance to the
exploration behavior by training multiple models over both real samples and
noisy samples with fake labels, where the noise is added according to the
predictive uncertainty. The proposed method is efficient as it can make
decisions on-the-fly by utilizing only one randomly chosen model, but is also
effective as we show that it can be viewed as a non-Bayesian approximation of
Thompson sampling. Moreover, we extend it to an online version that can learn
solely from streaming data, which is favored in real applications. Extensive
experiments on both synthetic task and large-scale advertising environments
show that GuideBoot achieves significant improvements against previous
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Feiyang Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haoming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1"&gt;Xiang Ao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yanrong Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1"&gt;Ao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1"&gt;Qing He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structural Watermarking to Deep Neural Networks via Network Channel Pruning. (arXiv:2107.08688v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.08688</id>
        <link href="http://arxiv.org/abs/2107.08688"/>
        <updated>2021-07-20T02:04:38.180Z</updated>
        <summary type="html"><![CDATA[In order to protect the intellectual property (IP) of deep neural networks
(DNNs), many existing DNN watermarking techniques either embed watermarks
directly into the DNN parameters or insert backdoor watermarks by fine-tuning
the DNN parameters, which, however, cannot resist against various attack
methods that remove watermarks by altering DNN parameters. In this paper, we
bypass such attacks by introducing a structural watermarking scheme that
utilizes channel pruning to embed the watermark into the host DNN architecture
instead of crafting the DNN parameters. To be specific, during watermark
embedding, we prune the internal channels of the host DNN with the channel
pruning rates controlled by the watermark. During watermark extraction, the
watermark is retrieved by identifying the channel pruning rates from the
architecture of the target DNN model. Due to the superiority of pruning
mechanism, the performance of the DNN model on its original task is reserved
during watermark embedding. Experimental results have shown that, the proposed
work enables the embedded watermark to be reliably recovered and provides a
high watermark capacity, without sacrificing the usability of the DNN model. It
is also demonstrated that the work is robust against common transforms and
attacks designed for conventional watermarking approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yinzhe Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hanzhou Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinpeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Limit Data Collection via Scaling Laws: Data Minimization Compliance in Practice. (arXiv:2107.08096v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08096</id>
        <link href="http://arxiv.org/abs/2107.08096"/>
        <updated>2021-07-20T02:04:38.147Z</updated>
        <summary type="html"><![CDATA[Data minimization is a legal obligation defined in the European Union's
General Data Protection Regulation (GDPR) as the responsibility to process an
adequate, relevant, and limited amount of personal data in relation to a
processing purpose. However, unlike fairness or transparency, the principle has
not seen wide adoption for machine learning systems due to a lack of
computational interpretation. In this paper, we build on literature in machine
learning and law to propose the first learning framework for limiting data
collection based on an interpretation that ties the data collection purpose to
system performance. We formalize a data minimization criterion based on
performance curve derivatives and provide an effective and interpretable
piecewise power law technique that models distinct stages of an algorithm's
performance throughout data collection. Results from our empirical
investigation offer deeper insights into the relevant considerations when
designing a data minimization framework, including the choice of feature
acquisition algorithm, initialization conditions, as well as impacts on
individuals that hint at tensions between data minimization and fairness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shanmugam_D/0/1/0/all/0/1"&gt;Divya Shanmugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shabanian_S/0/1/0/all/0/1"&gt;Samira Shabanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1"&gt;Fernando Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finck_M/0/1/0/all/0/1"&gt;Mich&amp;#xe8;le Finck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1"&gt;Asia Biega&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Search: Learning Query and Product Representations in Fashion E-commerce. (arXiv:2107.08291v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.08291</id>
        <link href="http://arxiv.org/abs/2107.08291"/>
        <updated>2021-07-20T02:04:38.103Z</updated>
        <summary type="html"><![CDATA[Typical e-commerce platforms contain millions of products in the catalog.
Users visit these platforms and enter search queries to retrieve their desired
products. Therefore, showing the relevant products at the top is essential for
the success of e-commerce platforms. We approach this problem by learning low
dimension representations for queries and product descriptions by leveraging
user click-stream data as our main source of signal for product relevance.
Starting from GRU-based architectures as our baseline model, we move towards a
more advanced transformer-based architecture. This helps the model to learn
contextual representations of queries and products to serve better search
results and understand the user intent in an efficient manner. We perform
experiments related to pre-training of the Transformer based RoBERTa model
using a fashion corpus and fine-tuning it over the triplet loss. Our
experiments on the product ranking task show that the RoBERTa model is able to
give an improvement of 7.8% in Mean Reciprocal Rank(MRR), 15.8% in Mean Average
Precision(MAP) and 8.8% in Normalized Discounted Cumulative Gain(NDCG), thus
outperforming our GRU based baselines. For the product retrieval task, RoBERTa
model is able to outperform other two models with an improvement of 164.7% in
Precision@50 and 145.3% in Recall@50. In order to highlight the importance of
pre-training RoBERTa for fashion domain, we qualitatively compare already
pre-trained RoBERTa on standard datasets with our custom pre-trained RoBERTa
over a fashion corpus for the query token prediction task. Finally, we also
show a qualitative comparison between GRU and RoBERTa results for product
retrieval task for some test queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1"&gt;Lakshya Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1"&gt;Sagnik Sarkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Discriminative Semantic Ranker for Question Retrieval. (arXiv:2107.08345v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.08345</id>
        <link href="http://arxiv.org/abs/2107.08345"/>
        <updated>2021-07-20T02:04:38.062Z</updated>
        <summary type="html"><![CDATA[Similar question retrieval is a core task in community-based question
answering (CQA) services. To balance the effectiveness and efficiency, the
question retrieval system is typically implemented as multi-stage rankers: The
first-stage ranker aims to recall potentially relevant questions from a large
repository, and the latter stages attempt to re-rank the retrieved results.
Most existing works on question retrieval mainly focused on the re-ranking
stages, leaving the first-stage ranker to some traditional term-based methods.
However, term-based methods often suffer from the vocabulary mismatch problem,
especially on short texts, which may block the re-rankers from relevant
questions at the very beginning. An alternative is to employ embedding-based
methods for the first-stage ranker, which compress texts into dense vectors to
enhance the semantic matching. However, these methods often lose the
discriminative power as term-based methods, thus introduce noise during
retrieval and hurt the recall performance. In this work, we aim to tackle the
dilemma of the first-stage ranker, and propose a discriminative semantic
ranker, namely DenseTrans, for high-recall retrieval. Specifically, DenseTrans
is a densely connected Transformer, which learns semantic embeddings for texts
based on Transformer layers. Meanwhile, DenseTrans promotes low-level features
through dense connections to keep the discriminative power of the learned
representations. DenseTrans is inspired by DenseNet in computer vision (CV),
but poses a new way to use the dense connectivity which is totally different
from its original design purpose. Experimental results over two question
retrieval benchmark datasets show that our model can obtain significant gain on
recall against strong term-based methods as well as state-of-the-art
embedding-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yinqiong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yixing Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiafeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yanyan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face.evoLVe: A High-Performance Face Recognition Library. (arXiv:2107.08621v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08621</id>
        <link href="http://arxiv.org/abs/2107.08621"/>
        <updated>2021-07-20T02:04:37.964Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop face.evoLVe -- a comprehensive library that
collects and implements a wide range of popular deep learning-based methods for
face recognition. First of all, face.evoLVe is composed of key components that
cover the full process of face analytics, including face alignment, data
processing, various backbones, losses, and alternatives with bags of tricks for
improving performance. Later, face.evoLVe supports multi-GPU training on top of
different deep learning platforms, such as PyTorch and PaddlePaddle, which
facilitates researchers to work on both large-scale datasets with millions of
images and low-shot counterparts with limited well-annotated data. More
importantly, along with face.evoLVe, images before & after alignment in the
common benchmark datasets are released with source codes and trained models
provided. All these efforts lower the technical burdens in reproducing the
existing methods for comparison, while users of our library could focus on
developing advanced approaches more efficiently. Last but not least,
face.evoLVe is well designed and vibrantly evolving, so that new face
recognition approaches can be easily plugged into our framework. Note that we
have used face.evoLVe to participate in a number of face recognition
competitions and secured the first place. The version that supports PyTorch is
publicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the
PaddlePaddle version is available at
https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.
Face.evoLVe has been widely used for face analytics, receiving 2.4K stars and
622 forks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qingzhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengfei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jian Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08264</id>
        <link href="http://arxiv.org/abs/2107.08264"/>
        <updated>2021-07-20T02:04:37.888Z</updated>
        <summary type="html"><![CDATA[Multimodal sentiment analysis aims to recognize people's attitudes from
multiple communication channels such as verbal content (i.e., text), voice, and
facial expressions. It has become a vibrant and important research topic in
natural language processing. Much research focuses on modeling the complex
intra- and inter-modal interactions between different communication channels.
However, current multimodal models with strong performance are often
deep-learning-based techniques and work like black boxes. It is not clear how
models utilize multimodal information for sentiment predictions. Despite recent
advances in techniques for enhancing the explainability of machine learning
models, they often target unimodal scenarios (e.g., images, sentences), and
little research has been done on explaining multimodal models. In this paper,
we present an interactive visual analytics system, M2Lens, to visualize and
explain multimodal models for sentiment analysis. M2Lens provides explanations
on intra- and inter-modal interactions at the global, subset, and local levels.
Specifically, it summarizes the influence of three typical interaction types
(i.e., dominance, complement, and conflict) on the model predictions. Moreover,
M2Lens identifies frequent and influential multimodal features and supports the
multi-faceted exploration of model behaviors from language, acoustic, and
visual modalities. Through two case studies and expert interviews, we
demonstrate our system can help users gain deep insights into the multimodal
models for sentiment analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianben He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhihua Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeHumor: Visual Analytics for Decomposing Humor. (arXiv:2107.08356v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.08356</id>
        <link href="http://arxiv.org/abs/2107.08356"/>
        <updated>2021-07-20T02:04:37.706Z</updated>
        <summary type="html"><![CDATA[Despite being a critical communication skill, grasping humor is challenging
-- a successful use of humor requires a mixture of both engaging content
build-up and an appropriate vocal delivery (e.g., pause). Prior studies on
computational humor emphasize the textual and audio features immediately next
to the punchline, yet overlooking longer-term context setup. Moreover, the
theories are usually too abstract for understanding each concrete humor
snippet. To fill in the gap, we develop DeHumor, a visual analytical system for
analyzing humorous behaviors in public speaking. To intuitively reveal the
building blocks of each concrete example, DeHumor decomposes each humorous
video into multimodal features and provides inline annotations of them on the
video script. In particular, to better capture the build-ups, we introduce
content repetition as a complement to features introduced in theories of
computational humor and visualize them in a context linking graph. To help
users locate the punchlines that have the desired features to learn, we
summarize the content (with keywords) and humor feature statistics on an
augmented time matrix. With case studies on stand-up comedy shows and TED
talks, we show that DeHumor is able to highlight various building blocks of
humor examples. In addition, expert interviews with communication coaches and
humor researchers demonstrate the effectiveness of DeHumor for multimodal humor
analysis of speech content and vocal delivery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1"&gt;Yao Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tongshuang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Haipeng Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v3 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10507</id>
        <link href="http://arxiv.org/abs/2106.10507"/>
        <updated>2021-07-19T01:59:51.065Z</updated>
        <summary type="html"><![CDATA[Graphically-rich applications such as games are ubiquitous with attractive
visual effects of Graphical User Interface (GUI) that offers a bridge between
software applications and end-users. However, various types of graphical
glitches may arise from such GUI complexity and have become one of the main
component of software compatibility issues. Our study on bug reports from game
development teams in NetEase Inc. indicates that graphical glitches frequently
occur during the GUI rendering and severely degrade the quality of
graphically-rich applications such as video games. Existing automated testing
techniques for such applications focus mainly on generating various GUI test
sequences and check whether the test sequences can cause crashes. These
techniques require constant human attention to captures non-crashing bugs such
as bugs causing graphical glitches. In this paper, we present the first step in
automating the test oracle for detecting non-crashing bugs in graphically-rich
applications. Specifically, we propose \texttt{GLIB} based on a code-based data
augmentation technique to detect game GUI glitches. We perform an evaluation of
\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the
result shows that \texttt{GLIB} can achieve 100\% precision and 99.5\% recall
in detecting non-crashing bugs such as game GUI glitches. Practical application
of \texttt{GLIB} on another 14 real-world games (without bug reports) further
demonstrates that \texttt{GLIB} can effectively uncover GUI glitches, with 48
of 53 bugs reported by \texttt{GLIB} having been confirmed and fixed so far.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Ke Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yufei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingfeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hoechst Is All You Need: Lymphocyte Classification with Deep Learning. (arXiv:2107.04388v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04388</id>
        <link href="http://arxiv.org/abs/2107.04388"/>
        <updated>2021-07-19T01:59:51.035Z</updated>
        <summary type="html"><![CDATA[Multiplex immunofluorescence and immunohistochemistry benefit patients by
allowing cancer pathologists to identify several proteins expressed on the
surface of cells, enabling cell classification, better understanding of the
tumour micro-environment, more accurate diagnoses, prognoses, and tailored
immunotherapy based on the immune status of individual patients. However, they
are expensive and time consuming processes which require complex staining and
imaging techniques by expert technicians. Hoechst staining is much cheaper and
easier to perform, but is not typically used in this case as it binds to DNA
rather than to the proteins targeted by immunofluorescent techniques, and it
was not previously thought possible to differentiate cells expressing these
proteins based only on DNA morphology. In this work we show otherwise, training
a deep convolutional neural network to identify cells expressing three proteins
(T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with
greater than 90% precision and recall, from Hoechst 33342 stained tissue only.
Our model learns previously unknown morphological features associated with
expression of these proteins which can be used to accurately differentiate
lymphocyte subtypes for use in key prognostic metrics such as assessment of
immune cell infiltration,and thereby predict and improve patient outcomes
without the need for costly multiplex immunofluorescence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1"&gt;Jessica Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Um_I/0/1/0/all/0/1"&gt;In Hwa Um&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1"&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1"&gt;David J Harrison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v3 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10507</id>
        <link href="http://arxiv.org/abs/2106.10507"/>
        <updated>2021-07-19T01:59:51.012Z</updated>
        <summary type="html"><![CDATA[Graphically-rich applications such as games are ubiquitous with attractive
visual effects of Graphical User Interface (GUI) that offers a bridge between
software applications and end-users. However, various types of graphical
glitches may arise from such GUI complexity and have become one of the main
component of software compatibility issues. Our study on bug reports from game
development teams in NetEase Inc. indicates that graphical glitches frequently
occur during the GUI rendering and severely degrade the quality of
graphically-rich applications such as video games. Existing automated testing
techniques for such applications focus mainly on generating various GUI test
sequences and check whether the test sequences can cause crashes. These
techniques require constant human attention to captures non-crashing bugs such
as bugs causing graphical glitches. In this paper, we present the first step in
automating the test oracle for detecting non-crashing bugs in graphically-rich
applications. Specifically, we propose \texttt{GLIB} based on a code-based data
augmentation technique to detect game GUI glitches. We perform an evaluation of
\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the
result shows that \texttt{GLIB} can achieve 100\% precision and 99.5\% recall
in detecting non-crashing bugs such as game GUI glitches. Practical application
of \texttt{GLIB} on another 14 real-world games (without bug reports) further
demonstrates that \texttt{GLIB} can effectively uncover GUI glitches, with 48
of 53 bugs reported by \texttt{GLIB} having been confirmed and fixed so far.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Ke Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yufei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingfeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hoechst Is All You Need: Lymphocyte Classification with Deep Learning. (arXiv:2107.04388v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04388</id>
        <link href="http://arxiv.org/abs/2107.04388"/>
        <updated>2021-07-19T01:59:50.979Z</updated>
        <summary type="html"><![CDATA[Multiplex immunofluorescence and immunohistochemistry benefit patients by
allowing cancer pathologists to identify several proteins expressed on the
surface of cells, enabling cell classification, better understanding of the
tumour micro-environment, more accurate diagnoses, prognoses, and tailored
immunotherapy based on the immune status of individual patients. However, they
are expensive and time consuming processes which require complex staining and
imaging techniques by expert technicians. Hoechst staining is much cheaper and
easier to perform, but is not typically used in this case as it binds to DNA
rather than to the proteins targeted by immunofluorescent techniques, and it
was not previously thought possible to differentiate cells expressing these
proteins based only on DNA morphology. In this work we show otherwise, training
a deep convolutional neural network to identify cells expressing three proteins
(T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with
greater than 90% precision and recall, from Hoechst 33342 stained tissue only.
Our model learns previously unknown morphological features associated with
expression of these proteins which can be used to accurately differentiate
lymphocyte subtypes for use in key prognostic metrics such as assessment of
immune cell infiltration,and thereby predict and improve patient outcomes
without the need for costly multiplex immunofluorescence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1"&gt;Jessica Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Um_I/0/1/0/all/0/1"&gt;In Hwa Um&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1"&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1"&gt;David J Harrison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms. (arXiv:2102.00815v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00815</id>
        <link href="http://arxiv.org/abs/2102.00815"/>
        <updated>2021-07-19T00:49:08.148Z</updated>
        <summary type="html"><![CDATA[Finding the minimal structural assumptions that empower sample-efficient
learning is one of the most important research directions in Reinforcement
Learning (RL). This paper advances our understanding of this fundamental
question by introducing a new complexity measure -- Bellman Eluder (BE)
dimension. We show that the family of RL problems of low BE dimension is
remarkably rich, which subsumes a vast majority of existing tractable RL
problems including but not limited to tabular MDPs, linear MDPs, reactive
POMDPs, low Bellman rank problems as well as low Eluder dimension problems.
This paper further designs a new optimization-based algorithm -- GOLF, and
reanalyzes a hypothesis elimination-based algorithm -- OLIVE (proposed in Jiang
et al., 2017). We prove that both algorithms learn the near-optimal policies of
low BE dimension problems in a number of samples that is polynomial in all
relevant parameters, but independent of the size of state-action space. Our
regret and sample complexity results match or improve the best existing results
for several well-known subclasses of low BE dimension problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qinghua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1"&gt;Sobhan Miryoosefi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Directed Graph Convolution for 3D Human Pose Estimation. (arXiv:2107.07797v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07797</id>
        <link href="http://arxiv.org/abs/2107.07797"/>
        <updated>2021-07-19T00:49:08.141Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks have significantly improved 3D human pose
estimation by representing the human skeleton as an undirected graph. However,
this representation fails to reflect the articulated characteristic of human
skeletons as the hierarchical orders among the joints are not explicitly
presented. In this paper, we propose to represent the human skeleton as a
directed graph with the joints as nodes and bones as edges that are directed
from parent joints to child joints. By so doing, the directions of edges can
explicitly reflect the hierarchical relationships among the nodes. Based on
this representation, we adopt the spatial-temporal directed graph convolution
(ST-DGConv) to extract features from 2D poses represented in a temporal
sequence of directed graphs. We further propose a spatial-temporal conditional
directed graph convolution (ST-CondDGConv) to leverage varying non-local
dependence for different poses by conditioning the graph topology on input
poses. Altogether, we form a U-shaped network with ST-DGConv and ST-CondDGConv
layers, named U-shaped Conditional Directed Graph Convolutional Network
(U-CondDGCN), for 3D human pose estimation from monocular videos. To evaluate
the effectiveness of our U-CondDGCN, we conducted extensive experiments on two
challenging large-scale benchmarks: Human3.6M and MPI-INF-3DHP. Both
quantitative and qualitative results show that our method achieves top
performance. Also, ablation studies show that directed graphs can better
exploit the hierarchy of articulated human skeletons than undirected graphs,
and the conditional connections can yield adaptive graph topologies for
different kinds of poses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wenbo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changgong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1"&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1"&gt;Tien-Tsin Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing. (arXiv:2106.03686v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03686</id>
        <link href="http://arxiv.org/abs/2106.03686"/>
        <updated>2021-07-19T00:49:08.127Z</updated>
        <summary type="html"><![CDATA[We address the detection of material defects, which are inside a layered
material structure using compressive sensing based multiple-input and
multiple-output (MIMO) wireless radar. Here, the strong clutter due to the
reflection of the layered structure's surface often makes the detection of the
defects challenging. Thus, sophisticated signal separation methods are required
for improved defect detection. In many scenarios, the number of defects that we
are interested in is limited and the signaling response of the layered
structure can be modeled as a low-rank structure. Therefore, we propose joint
rank and sparsity minimization for defect detection. In particular, we propose
a non-convex approach based on the iteratively reweighted nuclear and
$\ell_1-$norm (a double-reweighted approach) to obtain a higher accuracy
compared to the conventional nuclear norm and $\ell_1-$norm minimization. To
this end, an iterative algorithm is designed to estimate the low-rank and
sparse contributions. Further, we propose deep learning to learn the parameters
of the algorithm (i.e., algorithm unfolding) to improve the accuracy and the
speed of convergence of the algorithm. Our numerical results show that the
proposed approach outperforms the conventional approaches in terms of mean
square errors of the recovered low-rank and sparse components and the speed of
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Thanthrige_U/0/1/0/all/0/1"&gt;Udaya S.K.P. Miriya Thanthrige&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jung_P/0/1/0/all/0/1"&gt;Peter Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sezgin_A/0/1/0/all/0/1"&gt;Aydin Sezgin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks. (arXiv:2107.07933v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07933</id>
        <link href="http://arxiv.org/abs/2107.07933"/>
        <updated>2021-07-19T00:49:08.120Z</updated>
        <summary type="html"><![CDATA[Unprecedented access to multi-temporal satellite imagery has opened new
perspectives for a variety of Earth observation tasks. Among them,
pixel-precise panoptic segmentation of agricultural parcels has major economic
and environmental implications. While researchers have explored this problem
for single images, we argue that the complex temporal patterns of crop
phenology are better addressed with temporal sequences of images. In this
paper, we present the first end-to-end, single-stage method for panoptic
segmentation of Satellite Image Time Series (SITS). This module can be combined
with our novel image sequence encoding network which relies on temporal
self-attention to extract rich and adaptive multi-scale spatio-temporal
features. We also introduce PASTIS, the first open-access SITS dataset with
panoptic annotations. We demonstrate the superiority of our encoder for
semantic segmentation against multiple competing architectures, and set up the
first state-of-the-art of panoptic segmentation of SITS. Our implementation and
PASTIS are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garnot_V/0/1/0/all/0/1"&gt;Vivien Sainte Fare Garnot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1"&gt;Loic Landrieu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Optimal Representations with the Decodable Information Bottleneck. (arXiv:2009.12789v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12789</id>
        <link href="http://arxiv.org/abs/2009.12789"/>
        <updated>2021-07-19T00:49:08.103Z</updated>
        <summary type="html"><![CDATA[We address the question of characterizing and finding optimal representations
for supervised learning. Traditionally, this question has been tackled using
the Information Bottleneck, which compresses the inputs while retaining
information about the targets, in a decoder-agnostic fashion. In machine
learning, however, our goal is not compression but rather generalization, which
is intimately linked to the predictive family or decoder of interest (e.g.
linear classifier). We propose the Decodable Information Bottleneck (DIB) that
considers information retention and compression from the perspective of the
desired predictive family. As a result, DIB gives rise to representations that
are optimal in terms of expected test performance and can be estimated with
guarantees. Empirically, we show that the framework can be used to enforce a
small generalization gap on downstream classifiers and to predict the
generalization ability of neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1"&gt;Yann Dubois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1"&gt;David J. Schwab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedantam_R/0/1/0/all/0/1"&gt;Ramakrishna Vedantam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections. (arXiv:2107.07859v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07859</id>
        <link href="http://arxiv.org/abs/2107.07859"/>
        <updated>2021-07-19T00:49:08.097Z</updated>
        <summary type="html"><![CDATA[We propose Steadiness and Cohesiveness, two novel metrics to measure the
inter-cluster reliability of multidimensional projection (MDP), specifically
how well the inter-cluster structures are preserved between the original
high-dimensional space and the low-dimensional projection space. Measuring
inter-cluster reliability is crucial as it directly affects how well
inter-cluster tasks (e.g., identifying cluster relationships in the original
space from a projected view) can be conducted; however, despite the importance
of inter-cluster tasks, we found that previous metrics, such as Trustworthiness
and Continuity, fail to measure inter-cluster reliability. Our metrics consider
two aspects of the inter-cluster reliability: Steadiness measures the extent to
which clusters in the projected space form clusters in the original space, and
Cohesiveness measures the opposite. They extract random clusters with arbitrary
shapes and positions in one space and evaluate how much the clusters are
stretched or dispersed in the other space. Furthermore, our metrics can
quantify pointwise distortions, allowing for the visualization of inter-cluster
reliability in a projection, which we call a reliability map. Through
quantitative experiments, we verify that our metrics precisely capture the
distortions that harm inter-cluster reliability while previous metrics have
difficulty capturing the distortions. A case study also demonstrates that our
metrics and the reliability map 1) support users in selecting the proper
projection techniques or hyperparameters and 2) prevent misinterpretation while
performing inter-cluster tasks, thus allow an adequate identification of
inter-cluster structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1"&gt;Hyeon Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1"&gt;Hyung-Kwon Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1"&gt;Jaemin Jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngtaek Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1"&gt;Jinwook Seo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Painting Style-Aware Manga Colorization Based on Generative Adversarial Networks. (arXiv:2107.07943v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07943</id>
        <link href="http://arxiv.org/abs/2107.07943"/>
        <updated>2021-07-19T00:49:08.092Z</updated>
        <summary type="html"><![CDATA[Japanese comics (called manga) are traditionally created in monochrome
format. In recent years, in addition to monochrome comics, full color comics, a
more attractive medium, have appeared. Unfortunately, color comics require
manual colorization, which incurs high labor costs. Although automatic
colorization methods have been recently proposed, most of them are designed for
illustrations, not for comics. Unlike illustrations, since comics are composed
of many consecutive images, the painting style must be consistent. To realize
consistent colorization, we propose here a semi-automatic colorization method
based on generative adversarial networks (GAN); the method learns the painting
style of a specific comic from small amount of training data. The proposed
method takes a pair of a screen tone image and a flat colored image as input,
and outputs a colorized image. Experiments show that the proposed method
achieves better performance than the existing alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shimizu_Y/0/1/0/all/0/1"&gt;Yugo Shimizu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1"&gt;Ryosuke Furuta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_D/0/1/0/all/0/1"&gt;Delong Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taniguchi_Y/0/1/0/all/0/1"&gt;Yukinobu Taniguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hinami_R/0/1/0/all/0/1"&gt;Ryota Hinami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishiwatari_S/0/1/0/all/0/1"&gt;Shonosuke Ishiwatari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLeaR: An Adaptive Continual Learning Framework for Regression Tasks. (arXiv:2101.00926v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00926</id>
        <link href="http://arxiv.org/abs/2101.00926"/>
        <updated>2021-07-19T00:49:08.077Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting means that a trained neural network model gradually
forgets the previously learned tasks when being retrained on new tasks.
Overcoming the forgetting problem is a major problem in machine learning.
Numerous continual learning algorithms are very successful in incremental
learning of classification tasks, where new samples with their labels appear
frequently. However, there is currently no research that addresses the
catastrophic forgetting problem in regression tasks as far as we know. This
problem has emerged as one of the primary constraints in some applications,
such as renewable energy forecasts. This article clarifies problem-related
definitions and proposes a new methodological framework that can forecast
targets and update itself by means of continual learning. The framework
consists of forecasting neural networks and buffers, which store newly
collected data from a non-stationary data stream in an application. The changed
probability distribution of the data stream, which the framework has
identified, will be learned sequentially. The framework is called CLeaR
(Continual Learning for Regression Tasks), where components can be flexibly
customized for a specific application scenario. We design two sets of
experiments to evaluate the CLeaR framework concerning fitting error
(training), prediction error (test), and forgetting ratio. The first one is
based on an artificial time series to explore how hyperparameters affect the
CLeaR framework. The second one is designed with data collected from European
wind farms to evaluate the CLeaR framework's performance in a real-world
application. The experimental results demonstrate that the CLeaR framework can
continually acquire knowledge in the data stream and improve the prediction
accuracy. The article concludes with further research issues arising from
requirements to extend the framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yujiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1"&gt;Bernhard Sick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exact Backpropagation in Binary Weighted Networks with Group Weight Transformations. (arXiv:2107.01400v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01400</id>
        <link href="http://arxiv.org/abs/2107.01400"/>
        <updated>2021-07-19T00:49:08.072Z</updated>
        <summary type="html"><![CDATA[Quantization based model compression serves as high performing and fast
approach for inference that yields models which are highly compressed when
compared to their full-precision floating point counterparts. The most extreme
quantization is a 1-bit representation of parameters such that they have only
two possible values, typically -1(0) or +1, enabling efficient implementation
of the ubiquitous dot product using only additions. The main contribution of
this work is the introduction of a method to smooth the combinatorial problem
of determining a binary vector of weights to minimize the expected loss for a
given objective by means of empirical risk minimization with backpropagation.
This is achieved by approximating a multivariate binary state over the weights
utilizing a deterministic and differentiable transformation of real-valued,
continuous parameters. The proposed method adds little overhead in training,
can be readily applied without any substantial modifications to the original
architecture, does not introduce additional saturating nonlinearities or
auxiliary losses, and does not prohibit applying other methods for binarizing
the activations. Contrary to common assertions made in the literature, it is
demonstrated that binary weighted networks can train well with the same
standard optimization techniques and similar hyperparameter settings as their
full-precision counterparts, specifically momentum SGD with large learning
rates and $L_2$ regularization. To conclude experiments demonstrate the method
performs remarkably well across a number of inductive image classification
tasks with various architectures compared to their full-precision counterparts.
The source code is publicly available at
https://bitbucket.org/YanivShu/binary_weighted_networks_public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shulman_Y/0/1/0/all/0/1"&gt;Yaniv Shulman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Clinical Context for User-Centered Explainability: A Diabetes Use Case. (arXiv:2107.02359v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02359</id>
        <link href="http://arxiv.org/abs/2107.02359"/>
        <updated>2021-07-19T00:49:07.963Z</updated>
        <summary type="html"><![CDATA[Academic advances of AI models in high-precision domains, like healthcare,
need to be made explainable in order to enhance real-world adoption. Our past
studies and ongoing interactions indicate that medical experts can use AI
systems with greater trust if there are ways to connect the model inferences
about patients to explanations that are tied back to the context of use.
Specifically, risk prediction is a complex problem of diagnostic and
interventional importance to clinicians wherein they consult different sources
to make decisions. To enable the adoption of the ever improving AI risk
prediction models in practice, we have begun to explore techniques to
contextualize such models along three dimensions of interest: the patients'
clinical state, AI predictions about their risk of complications, and
algorithmic explanations supporting the predictions. We validate the importance
of these dimensions by implementing a proof-of-concept (POC) in type-2 diabetes
(T2DM) use case where we assess the risk of chronic kidney disease (CKD) - a
common T2DM comorbidity. Within the POC, we include risk prediction models for
CKD, post-hoc explainers of the predictions, and other natural-language modules
which operationalize domain knowledge and CPGs to provide context. With primary
care physicians (PCP) as our end-users, we present our initial results and
clinician feedback in this paper. Our POC approach covers multiple knowledge
sources and clinical scenarios, blends knowledge to explain data and
predictions to PCPs, and received an enthusiastic response from our medical
expert.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chari_S/0/1/0/all/0/1"&gt;Shruthi Chari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1"&gt;Prithwish Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghalwash_M/0/1/0/all/0/1"&gt;Mohamed Ghalwash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seneviratne_O/0/1/0/all/0/1"&gt;Oshani Seneviratne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eyigoz_E/0/1/0/all/0/1"&gt;Elif K. Eyigoz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gruen_D/0/1/0/all/0/1"&gt;Daniel M. Gruen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saiz_F/0/1/0/all/0/1"&gt;Fernando Suarez Saiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Ching-Hua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rojas_P/0/1/0/all/0/1"&gt;Pablo Meyer Rojas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGuinness_D/0/1/0/all/0/1"&gt;Deborah L. McGuinness&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold learning with arbitrary norms. (arXiv:2012.14172v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14172</id>
        <link href="http://arxiv.org/abs/2012.14172"/>
        <updated>2021-07-19T00:49:07.957Z</updated>
        <summary type="html"><![CDATA[Manifold learning methods play a prominent role in nonlinear dimensionality
reduction and other tasks involving high-dimensional data sets with low
intrinsic dimensionality. Many of these methods are graph-based: they associate
a vertex with each data point and a weighted edge with each pair. Existing
theory shows that the Laplacian matrix of the graph converges to the
Laplace-Beltrami operator of the data manifold, under the assumption that the
pairwise affinities are based on the Euclidean norm. In this paper, we
determine the limiting differential operator for graph Laplacians constructed
using $\textit{any}$ norm. Our proof involves an interplay between the second
fundamental form of the manifold and the convex geometry of the given norm's
unit ball. To demonstrate the potential benefits of non-Euclidean norms in
manifold learning, we consider the task of mapping the motion of large
molecules with continuous variability. In a numerical simulation we show that a
modified Laplacian eigenmaps algorithm, based on the Earthmover's distance,
outperforms the classic Euclidean Laplacian eigenmaps, both in terms of
computational cost and the sample size needed to recover the intrinsic
geometry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kileel_J/0/1/0/all/0/1"&gt;Joe Kileel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moscovich_A/0/1/0/all/0/1"&gt;Amit Moscovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelesko_N/0/1/0/all/0/1"&gt;Nathan Zelesko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singer_A/0/1/0/all/0/1"&gt;Amit Singer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation Consolidation for Training Expert Students. (arXiv:2107.08039v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08039</id>
        <link href="http://arxiv.org/abs/2107.08039"/>
        <updated>2021-07-19T00:49:07.951Z</updated>
        <summary type="html"><![CDATA[Traditionally, distillation has been used to train a student model to emulate
the input/output functionality of a teacher. A more useful goal than emulation,
yet under-explored, is for the student to learn feature representations that
transfer well to future tasks. However, we observe that standard distillation
of task-specific teachers actually *reduces* the transferability of student
representations to downstream tasks. We show that a multi-head, multi-task
distillation method using an unlabeled proxy dataset and a generalist teacher
is sufficient to consolidate representations from task-specific teacher(s) and
improve downstream performance, outperforming the teacher(s) and the strong
baseline of ImageNet pretrained features. Our method can also combine the
representational knowledge of multiple teachers trained on one or multiple
domains into a single model, whose representation is improved on all teachers'
domain(s).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhizhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1"&gt;Charless Fowlkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1"&gt;Marzia Polito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1"&gt;Rahul Bhotika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Role of Entropy in Guiding a Connection Prover. (arXiv:2105.14706v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14706</id>
        <link href="http://arxiv.org/abs/2105.14706"/>
        <updated>2021-07-19T00:49:07.934Z</updated>
        <summary type="html"><![CDATA[In this work we study how to learn good algorithms for selecting reasoning
steps in theorem proving. We explore this in the connection tableau calculus
implemented by leanCoP where the partial tableau provides a clean and compact
notion of a state to which a limited number of inferences can be applied. We
start by incorporating a state-of-the-art learning algorithm -- a graph neural
network (GNN) -- into the plCoP theorem prover. Then we use it to observe the
system's behaviour in a reinforcement learning setting, i.e., when learning
inference guidance from successful Monte-Carlo tree searches on many problems.
Despite its better pattern matching capability, the GNN initially performs
worse than a simpler previously used learning algorithm. We observe that the
simpler algorithm is less confident, i.e., its recommendations have higher
entropy. This leads us to explore how the entropy of the inference selection
implemented via the neural network influences the proof search. This is related
to research in human decision-making under uncertainty, and in particular the
probability matching theory. Our main result shows that a proper entropy
regularisation, i.e., training the GNN not to be overconfident, greatly
improves plCoP's performance on a large mathematical corpus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zombori_Z/0/1/0/all/0/1"&gt;Zsolt Zombori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1"&gt;Josef Urban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1"&gt;Miroslav Ol&amp;#x161;&amp;#xe1;k&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing All the Angles: Learning Multiview Manipulation Policies for Contact-Rich Tasks from Demonstrations. (arXiv:2104.13907v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13907</id>
        <link href="http://arxiv.org/abs/2104.13907"/>
        <updated>2021-07-19T00:49:07.928Z</updated>
        <summary type="html"><![CDATA[Learned visuomotor policies have shown considerable success as an alternative
to traditional, hand-crafted frameworks for robotic manipulation. Surprisingly,
an extension of these methods to the multiview domain is relatively unexplored.
A successful multiview policy could be deployed on a mobile manipulation
platform, allowing the robot to complete a task regardless of its view of the
scene. In this work, we demonstrate that a multiview policy can be found
through imitation learning by collecting data from a variety of viewpoints. We
illustrate the general applicability of the method by learning to complete
several challenging multi-stage and contact-rich tasks, from numerous
viewpoints, both in a simulated environment and on a real mobile manipulation
platform. Furthermore, we analyze our policies to determine the benefits of
learning from multiview data compared to learning with data collected from a
fixed perspective. We show that learning from multiview data results in little,
if any, penalty to performance for a fixed-view task compared to learning with
an equivalent amount of fixed-view data. Finally, we examine the visual
features learned by the multiview and fixed-view policies. Our results indicate
that multiview policies implicitly learn to identify spatially correlated
features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ablett_T/0/1/0/all/0/1"&gt;Trevor Ablett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1"&gt;Yifan Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1"&gt;Jonathan Kelly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating subgroup disparity using epistemic uncertainty in mammography. (arXiv:2107.02716v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02716</id>
        <link href="http://arxiv.org/abs/2107.02716"/>
        <updated>2021-07-19T00:49:07.921Z</updated>
        <summary type="html"><![CDATA[As machine learning (ML) continue to be integrated into healthcare systems
that affect clinical decision making, new strategies will need to be
incorporated in order to effectively detect and evaluate subgroup disparities
to ensure accountability and generalizability in clinical workflows. In this
paper, we explore how epistemic uncertainty can be used to evaluate disparity
in patient demographics (race) and data acquisition (scanner) subgroups for
breast density assessment on a dataset of 108,190 mammograms collected from 33
clinical sites. Our results show that even if aggregate performance is
comparable, the choice of uncertainty quantification metric can significantly
the subgroup level. We hope this analysis can promote further work on how
uncertainty can be leveraged to increase transparency of machine learning
applications for clinical deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Charles Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lemay_A/0/1/0/all/0/1"&gt;Andreanne Lemay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoebel_K/0/1/0/all/0/1"&gt;Katharina Hoebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1"&gt;Jayashree Kalpathy-Cramer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stable and consistent density-based clustering. (arXiv:2005.09048v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.09048</id>
        <link href="http://arxiv.org/abs/2005.09048"/>
        <updated>2021-07-19T00:49:07.914Z</updated>
        <summary type="html"><![CDATA[We present a multiscale, consistent approach to density-based clustering that
satisfies stability theorems -- in both the input data and in the parameters --
which hold without distributional assumptions. The stability in the input data
is with respect to the Gromov--Hausdorff--Prokhorov distance on metric
probability spaces and interleaving distances between (multi-parameter)
hierarchical clusterings we introduce. We prove stability results for standard
simplification procedures for hierarchical clusterings, which can be combined
with our approach to yield a stable flat clustering algorithm. We illustrate
the stability of the approach with computational examples. Our framework is
based on the concepts of persistence and interleaving distance from Topological
Data Analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Rolle_A/0/1/0/all/0/1"&gt;Alexander Rolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Scoccola_L/0/1/0/all/0/1"&gt;Luis Scoccola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Price graphs: Utilizing the structural information of financial time series for stock prediction. (arXiv:2106.02522v3 [q-fin.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02522</id>
        <link href="http://arxiv.org/abs/2106.02522"/>
        <updated>2021-07-19T00:49:07.896Z</updated>
        <summary type="html"><![CDATA[Great research efforts have been devoted to exploiting deep neural networks
in stock prediction. While long-range dependencies and chaotic property are
still two major issues that lower the performance of state-of-the-art deep
learning models in forecasting future price trends. In this study, we propose a
novel framework to address both issues. Specifically, in terms of transforming
time series into complex networks, we convert market price series into graphs.
Then, structural information, referring to associations among temporal points
and the node weights, is extracted from the mapped graphs to resolve the
problems regarding long-range dependencies and the chaotic property. We take
graph embeddings to represent the associations among temporal points as the
prediction model inputs. Node weights are used as a priori knowledge to enhance
the learning of temporal attention. The effectiveness of our proposed framework
is validated using real-world stock data, and our approach obtains the best
performance among several state-of-the-art benchmarks. Moreover, in the
conducted trading simulations, our framework further obtains the highest
cumulative profits. Our results supplement the existing applications of complex
network methods in the financial realm and provide insightful implications for
investment applications regarding decision support in financial markets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Wu_J/0/1/0/all/0/1"&gt;Junran Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xueyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Li_S/0/1/0/all/0/1"&gt;Shangzhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jichang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Transfer and Interference in Multi-Domain Learning. (arXiv:2107.05445v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05445</id>
        <link href="http://arxiv.org/abs/2107.05445"/>
        <updated>2021-07-19T00:49:07.890Z</updated>
        <summary type="html"><![CDATA[Humans are incredibly good at transferring knowledge from one domain to
another, enabling rapid learning of new tasks. Likewise, transfer learning has
enabled enormous success in many computer vision problems using pretraining.
However, the benefits of transfer in multi-domain learning, where a network
learns multiple tasks defined by different datasets, has not been adequately
studied. Learning multiple domains could be beneficial or these domains could
interfere with each other given limited network capacity. In this work, we
decipher the conditions where interference and knowledge transfer occur in
multi-domain learning. We propose new metrics disentangling interference and
transfer and set up experimental protocols. We further examine the roles of
network capacity, task grouping, and dynamic loss weighting in reducing
interference and facilitating transfer. We demonstrate our findings on the
CIFAR-100, MiniPlaces, and Tiny-ImageNet datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yipeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1"&gt;Tyler L. Hayes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1"&gt;Christopher Kanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-training Converts Weak Learners to Strong Learners in Mixture Models. (arXiv:2106.13805v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13805</id>
        <link href="http://arxiv.org/abs/2106.13805"/>
        <updated>2021-07-19T00:49:07.884Z</updated>
        <summary type="html"><![CDATA[We consider a binary classification problem when the data comes from a
mixture of two isotropic distributions satisfying concentration and
anti-concentration properties enjoyed by log-concave distributions among
others. We show that there exists a universal constant $C_{\mathrm{err}}>0$
such that if a pseudolabeler $\boldsymbol{\beta}_{\mathrm{pl}}$ can achieve
classification error at most $C_{\mathrm{err}}$, then for any $\varepsilon>0$,
an iterative self-training algorithm initialized at $\boldsymbol{\beta}_0 :=
\boldsymbol{\beta}_{\mathrm{pl}}$ using pseudolabels $\hat y =
\mathrm{sgn}(\langle \boldsymbol{\beta}_t, \mathbf{x}\rangle)$ and using at
most $\tilde O(d/\varepsilon^2)$ unlabeled examples suffices to learn the
Bayes-optimal classifier up to $\varepsilon$ error, where $d$ is the ambient
dimension. That is, self-training converts weak learners to strong learners
using only unlabeled examples. We additionally show that by running gradient
descent on the logistic loss one can obtain a pseudolabeler
$\boldsymbol{\beta}_{\mathrm{pl}}$ with classification error $C_{\mathrm{err}}$
using only $O(d)$ labeled examples (i.e., independent of $\varepsilon$).
Together our results imply that mixture models can be learned to within
$\varepsilon$ of the Bayes-optimal accuracy using at most $O(d)$ labeled
examples and $\tilde O(d/\varepsilon^2)$ unlabeled examples by way of a
semi-supervised self-training algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1"&gt;Spencer Frei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1"&gt;Difan Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zixiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural-Swarm2: Planning and Control of Heterogeneous Multirotor Swarms using Learned Interactions. (arXiv:2012.05457v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05457</id>
        <link href="http://arxiv.org/abs/2012.05457"/>
        <updated>2021-07-19T00:49:07.877Z</updated>
        <summary type="html"><![CDATA[We present Neural-Swarm2, a learning-based method for motion planning and
control that allows heterogeneous multirotors in a swarm to safely fly in close
proximity. Such operation for drones is challenging due to complex aerodynamic
interaction forces, such as downwash generated by nearby drones and ground
effect. Conventional planning and control methods neglect capturing these
interaction forces, resulting in sparse swarm configuration during flight. Our
approach combines a physics-based nominal dynamics model with learned Deep
Neural Networks (DNNs) with strong Lipschitz properties. We make use of two
techniques to accurately predict the aerodynamic interactions between
heterogeneous multirotors: i) spectral normalization for stability and
generalization guarantees of unseen data and ii) heterogeneous deep sets for
supporting any number of heterogeneous neighbors in a permutation-invariant
manner without reducing expressiveness. The learned residual dynamics benefit
both the proposed interaction-aware multi-robot motion planning and the
nonlinear tracking control design because the learned interaction forces reduce
the modelling errors. Experimental results demonstrate that Neural-Swarm2 is
able to generalize to larger swarms beyond training cases and significantly
outperforms a baseline nonlinear tracking controller with up to three times
reduction in worst-case tracking errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1"&gt;Guanya Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honig_W/0/1/0/all/0/1"&gt;Wolfgang H&amp;#xf6;nig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xichen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yisong Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1"&gt;Soon-Jo Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning. (arXiv:2105.05682v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05682</id>
        <link href="http://arxiv.org/abs/2105.05682"/>
        <updated>2021-07-19T00:49:07.866Z</updated>
        <summary type="html"><![CDATA[Graph representation learning plays a vital role in processing
graph-structured data. However, prior arts on graph representation learning
heavily rely on labeling information. To overcome this problem, inspired by the
recent success of graph contrastive learning and Siamese networks in visual
representation learning, we propose a novel self-supervised approach in this
paper to learn node representations by enhancing Siamese self-distillation with
multi-scale contrastive learning. Specifically, we first generate two augmented
views from the input graph based on local and global perspectives. Then, we
employ two objectives called cross-view and cross-network contrastiveness to
maximize the agreement between node representations across different views and
networks. To demonstrate the effectiveness of our approach, we perform
empirical experiments on five real-world datasets. Our method not only achieves
new state-of-the-art results but also surpasses some semi-supervised
counterparts by large margins. Code is made available at
https://github.com/GRAND-Lab/MERIT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1"&gt;Ming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yizhen Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuan-Fang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chen Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ranking labs-of-origin for genetically engineered DNA using Metric Learning. (arXiv:2107.07878v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07878</id>
        <link href="http://arxiv.org/abs/2107.07878"/>
        <updated>2021-07-19T00:49:07.860Z</updated>
        <summary type="html"><![CDATA[With the constant advancements of genetic engineering, a common concern is to
be able to identify the lab-of-origin of genetically engineered DNA sequences.
For that reason, AltLabs has hosted the genetic Engineering Attribution
Challenge to gather many teams to propose new tools to solve this problem. Here
we show our proposed method to rank the most likely labs-of-origin and generate
embeddings for DNA sequences and labs. These embeddings can also perform
various other tasks, like clustering both DNA sequences and labs and using them
as features for Machine Learning models applied to solve other problems. This
work demonstrates that our method outperforms the classic training method for
this task while generating other helpful information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muniz_I/0/1/0/all/0/1"&gt;I. Muniz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camargo_F/0/1/0/all/0/1"&gt;F. H. F. Camargo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marques_A/0/1/0/all/0/1"&gt;A. Marques&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10785</id>
        <link href="http://arxiv.org/abs/2104.10785"/>
        <updated>2021-07-19T00:49:07.855Z</updated>
        <summary type="html"><![CDATA[In this paper we tackle two important challenges related to the accurate
partial singular value decomposition (SVD) and numerical rank estimation of a
huge matrix to use in low-rank learning problems in a fast way. We use the
concepts of Krylov subspaces such as the Golub-Kahan bidiagonalization process
as well as Ritz vectors to achieve these goals. Our experiments identify
various advantages of the proposed methods compared to traditional and
randomized SVD (R-SVD) methods with respect to the accuracy of the singular
values and corresponding singular vectors computed in a similar execution time.
The proposed methods are appropriate for applications involving huge matrices
where accuracy in all spectrum of the desired singular values, and also all of
corresponding singular vectors is essential. We evaluate our method in the real
application of Riemannian similarity learning (RSL) between two various image
datasets of MNIST and USPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Godaz_R/0/1/0/all/0/1"&gt;Reza Godaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Monsefi_R/0/1/0/all/0/1"&gt;Reza Monsefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Toutounian_F/0/1/0/all/0/1"&gt;Faezeh Toutounian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hosseini_R/0/1/0/all/0/1"&gt;Reshad Hosseini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accuracy Prediction with Non-neural Model for Neural Architecture Search. (arXiv:2007.04785v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04785</id>
        <link href="http://arxiv.org/abs/2007.04785"/>
        <updated>2021-07-19T00:49:07.849Z</updated>
        <summary type="html"><![CDATA[Neural architecture search (NAS) with an accuracy predictor that predicts the
accuracy of candidate architectures has drawn increasing attention due to its
simplicity and effectiveness. Previous works usually employ neural
network-based predictors which require more delicate design and are easy to
overfit. Considering that most architectures are represented as sequences of
discrete symbols which are more like tabular data and preferred by non-neural
predictors, in this paper, we study an alternative approach which uses
non-neural model for accuracy prediction. Specifically, as decision tree based
models can better handle tabular data, we leverage gradient boosting decision
tree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor
can achieve comparable (if not better) prediction accuracy than neural network
based predictors. Moreover, considering that a compact search space can ease
the search process, we propose to prune the search space gradually according to
important features derived from GBDT. In this way, NAS can be performed by
first pruning the search space and then searching a neural architecture, which
is more efficient and effective. Experiments on NASBench-101 and ImageNet
demonstrate the effectiveness of using GBDT as predictor for NAS: (1) On
NASBench-101, it is 22x, 8x, and 6x more sample efficient than random search,
regularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global
optimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further
achieves 23.4% top-1 error rate on ImageNet when enhanced with search space
pruning. Code is provided in the supplementary materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Renqian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Enhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Missing Value Imputation on Multidimensional Time Series. (arXiv:2103.01600v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01600</id>
        <link href="http://arxiv.org/abs/2103.01600"/>
        <updated>2021-07-19T00:49:07.816Z</updated>
        <summary type="html"><![CDATA[We present DeepMVI, a deep learning method for missing value imputation in
multidimensional time-series datasets. Missing values are commonplace in
decision support platforms that aggregate data over long time stretches from
disparate sources, and reliable data analytics calls for careful handling of
missing data. One strategy is imputing the missing values, and a wide variety
of algorithms exist spanning simple interpolation, matrix factorization methods
like SVD, statistical models like Kalman filters, and recent deep learning
methods. We show that often these provide worse results on aggregate analytics
compared to just excluding the missing data. DeepMVI uses a neural network to
combine fine-grained and coarse-grained patterns along a time series, and
trends from related series across categorical dimensions. After failing with
off-the-shelf neural architectures, we design our own network that includes a
temporal transformer with a novel convolutional window feature, and kernel
regression with learned embeddings. The parameters and their training are
designed carefully to generalize across different placements of missing blocks
and data characteristics. Experiments across nine real datasets, four different
missing scenarios, comparing seven existing methods show that DeepMVI is
significantly more accurate, reducing error by more than 50% in more than half
the cases, compared to the best existing method. Although slower than simpler
matrix factorization methods, we justify the increased time overheads by
showing that DeepMVI is the only option that provided overall more accurate
analytics than dropping missing values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_P/0/1/0/all/0/1"&gt;Parikshit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1"&gt;Prathamesh Deshpande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1"&gt;Sunita Sarawagi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Ranking Approach to Fair Classification. (arXiv:2102.04565v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04565</id>
        <link href="http://arxiv.org/abs/2102.04565"/>
        <updated>2021-07-19T00:49:07.808Z</updated>
        <summary type="html"><![CDATA[Algorithmic decision systems are increasingly used in areas such as hiring,
school admission, or loan approval. Typically, these systems rely on labeled
data for training a classification model. However, in many scenarios,
ground-truth labels are unavailable, and instead we have only access to
imperfect labels as the result of (potentially biased) human-made decisions.
Despite being imperfect, historical decisions often contain some useful
information on the unobserved true labels. In this paper, we focus on scenarios
where only imperfect labels are available and propose a new fair ranking-based
decision system based on monotonic relationships between legitimate features
and the outcome. Our approach is both intuitive and easy to implement, and thus
particularly suitable for adoption in real-world settings. More in detail, we
introduce a distance-based decision criterion, which incorporates useful
information from historical decisions and accounts for unwanted correlation
between protected and legitimate features. Through extensive experiments on
synthetic and real-world data, we show that our method is fair in the sense
that a) it assigns the desirable outcome to the most qualified individuals, and
b) it removes the effect of stereotypes in decision-making, thereby
outperforming traditional classification algorithms. Additionally, we are able
to show theoretically that our method is consistent with a prominent concept of
individual fairness which states that "similar individuals should be treated
similarly."]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schoeffer_J/0/1/0/all/0/1"&gt;Jakob Schoeffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehl_N/0/1/0/all/0/1"&gt;Niklas Kuehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valera_I/0/1/0/all/0/1"&gt;Isabel Valera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Capturing Delayed Feedback in Conversion Rate Prediction via Elapsed-Time Sampling. (arXiv:2012.03245v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03245</id>
        <link href="http://arxiv.org/abs/2012.03245"/>
        <updated>2021-07-19T00:49:07.797Z</updated>
        <summary type="html"><![CDATA[Conversion rate (CVR) prediction is one of the most critical tasks for
digital display advertising. Commercial systems often require to update models
in an online learning manner to catch up with the evolving data distribution.
However, conversions usually do not happen immediately after a user click. This
may result in inaccurate labeling, which is called delayed feedback problem. In
previous studies, delayed feedback problem is handled either by waiting
positive label for a long period of time, or by consuming the negative sample
on its arrival and then insert a positive duplicate when a conversion happens
later. Indeed, there is a trade-off between waiting for more accurate labels
and utilizing fresh data, which is not considered in existing works. To strike
a balance in this trade-off, we propose Elapsed-Time Sampling Delayed Feedback
Model (ES-DFM), which models the relationship between the observed conversion
distribution and the true conversion distribution. Then we optimize the
expectation of true conversion distribution via importance sampling under the
elapsed-time sampling distribution. We further estimate the importance weight
for each instance, which is used as the weight of loss function in CVR
prediction. To demonstrate the effectiveness of ES-DFM, we conduct extensive
experiments on a public data and a private industrial dataset. Experimental
results confirm that our method consistently outperforms the previous
state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jia-Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shuguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_T/0/1/0/all/0/1"&gt;Tao Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xiaoyi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_B/0/1/0/all/0/1"&gt;Bin Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A data-based comparative review and AI-driven symbolic model for longitudinal dispersion coefficient in natural streams. (arXiv:2106.11026v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11026</id>
        <link href="http://arxiv.org/abs/2106.11026"/>
        <updated>2021-07-19T00:49:07.788Z</updated>
        <summary type="html"><![CDATA[A better understanding of dispersion in natural streams requires knowledge of
longitudinal dispersion coefficient(LDC). Various methods have been proposed
for predictions of LDC. Those studies can be grouped into three types:
analytical, statistical and ML-driven researches(Implicit and explicit).
However, a comprehensive evaluation of them is still lacking. In this paper, we
first present an in-depth analysis of those methods and find out their defects.
This is carried out on an extensive database composed of 660 samples of
hydraulic and channel properties worldwide. The reliability and
representativeness of utilized data are enhanced through the deployment of the
Subset Selection of Maximum Dissimilarity(SSMD) for testing set selection and
the Inter Quartile Range(IQR) for removal of the outlier. The evaluation
reveals the rank of those methods as: ML-driven method > the statistical method
> the analytical method. Whereas implicit ML-driven methods are black-boxes in
nature, explicit ML-driven methods have more potential in prediction of LDC.
Besides, overfitting is a universal problem in existing models. Those models
also suffer from a fixed parameter combination. To establish an interpretable
model for LDC prediction with higher performance, we then design a novel
symbolic regression method called evolutionary symbolic regression
network(ESRN). It is a combination of genetic algorithms and neural networks.
Strategies are introduced to avoid overfitting and explore more parameter
combinations. Results show that the ESRN model has superiorities over other
existing symbolic models in performance. The proposed model is suitable for
practical engineering problems due to its advantage in low requirement of
parameters (only w and U* are required). It can provide convincing solutions
for situations where the field test cannot be carried out or limited field
information can be obtained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yifeng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galindo_Torres_S/0/1/0/all/0/1"&gt;S.A. Galindo-Torres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Stan Z. Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised 3D Human Mesh Recovery from Noisy Point Clouds. (arXiv:2107.07539v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07539</id>
        <link href="http://arxiv.org/abs/2107.07539"/>
        <updated>2021-07-19T00:49:07.772Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel unsupervised approach to reconstruct human shape
and pose from noisy point cloud. Traditional approaches search for
correspondences and conduct model fitting iteratively where a good
initialization is critical. Relying on large amount of dataset with
ground-truth annotations, recent learning-based approaches predict
correspondences for every vertice on the point cloud; Chamfer distance is
usually used to minimize the distance between a deformed template model and the
input point cloud. However, Chamfer distance is quite sensitive to noise and
outliers, thus could be unreliable to assign correspondences. To address these
issues, we model the probability distribution of the input point cloud as
generated from a parametric human model under a Gaussian Mixture Model. Instead
of explicitly aligning correspondences, we treat the process of correspondence
search as an implicit probabilistic association by updating the posterior
probability of the template model given the input. A novel unsupervised loss is
further derived that penalizes the discrepancy between the deformed template
and the input point cloud conditioned on the posterior probability. Our
approach is very flexible, which works with both complete point cloud and
incomplete ones including even a single depth image as input. Our network is
trained from scratch with no need to warm-up the network with supervised data.
Compared to previous unsupervised methods, our method shows the capability to
deal with substantial noise and outliers. Extensive experiments conducted on
various public synthetic datasets as well as a very noisy real dataset (i.e.
CMU Panoptic) demonstrate the superior performance of our approach over the
state-of-the-art methods. Code can be found
\url{https://github.com/wangsen1312/unsupervised3dhuman.git}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xinxin Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Minglun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Li Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. (arXiv:2107.07651v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07651</id>
        <link href="http://arxiv.org/abs/2107.07651"/>
        <updated>2021-07-19T00:49:07.747Z</updated>
        <summary type="html"><![CDATA[Large-scale vision and language representation learning has shown promising
improvements on various vision-language tasks. Most existing methods employ a
transformer-based multimodal encoder to jointly model visual tokens
(region-based image features) and word tokens. Because the visual tokens and
word tokens are unaligned, it is challenging for the multimodal encoder to
learn image-text interactions. In this paper, we introduce a contrastive loss
to ALign the image and text representations BEfore Fusing (ALBEF) them through
cross-modal attention, which enables more grounded vision and language
representation learning. Unlike most existing methods, our method does not
require bounding box annotations nor high-resolution images. In order to
improve learning from noisy web data, we propose momentum distillation, a
self-training method which learns from pseudo-targets produced by a momentum
model. We provide a theoretical analysis of ALBEF from a mutual information
maximization perspective, showing that different training tasks can be
interpreted as different ways to generate views for an image-text pair. ALBEF
achieves state-of-the-art performance on multiple downstream vision-language
tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained
on orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves
absolute improvements of 2.37% and 3.84% compared to the state-of-the-art,
while enjoying faster inference speed. Code and pre-trained models are
available at https://github.com/salesforce/ALBEF/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junnan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Selvaraju_R/0/1/0/all/0/1"&gt;Ramprasaath R. Selvaraju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gotmare_A/0/1/0/all/0/1"&gt;Akhilesh Deepak Gotmare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1"&gt;Steven Hoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison of Deep Learning Classification Methods on Small-scale Image Data set: from Converlutional Neural Networks to Visual Transformers. (arXiv:2107.07699v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07699</id>
        <link href="http://arxiv.org/abs/2107.07699"/>
        <updated>2021-07-19T00:49:07.742Z</updated>
        <summary type="html"><![CDATA[In recent years, deep learning has made brilliant achievements in image
classification. However, image classification of small datasets is still not
obtained good research results. This article first briefly explains the
application and characteristics of convolutional neural networks and visual
transformers. Meanwhile, the influence of small data set on classification and
the solution are introduced. Then a series of experiments are carried out on
the small datasets by using various models, and the problems of some models in
the experiments are discussed. Through the comparison of experimental results,
the recommended deep learning model is given according to the model application
environment. Finally, we give directions for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1"&gt;Md Mamunur Rahaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hechen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1"&gt;Tao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous boundary shape estimation and velocity field de-noising in Magnetic Resonance Velocimetry using Physics-informed Neural Networks. (arXiv:2107.07863v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2107.07863</id>
        <link href="http://arxiv.org/abs/2107.07863"/>
        <updated>2021-07-19T00:49:07.736Z</updated>
        <summary type="html"><![CDATA[Magnetic resonance velocimetry (MRV) is a non-invasive experimental technique
widely used in medicine and engineering to measure the velocity field of a
fluid. These measurements are dense but have a low signal-to-noise ratio (SNR).
The measurements can be de-noised by imposing physical constraints on the flow,
which are encapsulated in governing equations for mass and momentum. Previous
studies have required the shape of the boundary (for example, a blood vessel)
to be known a priori. This, however, requires a set of additional measurements,
which can be expensive to obtain. In this paper, we present a physics-informed
neural network that instead uses the noisy MRV data alone to simultaneously
infer the most likely boundary shape and de-noised velocity field. We achieve
this by training an auxiliary neural network that takes the value 1.0 within
the inferred domain of the governing PDE and 0.0 outside. This network is used
to weight the PDE residual term in the loss function accordingly and implicitly
learns the geometry of the system. We test our algorithm by assimilating both
synthetic and real MRV measurements for flows that can be well modeled by the
Poisson and Stokes equations. We find that we are able to reconstruct very
noisy (SNR = 2.5) MRV signals and recover the ground truth with low
reconstruction errors of 3.7 - 7.5%. The simplicity and flexibility of our
physics-informed neural network approach can readily scale to assimilating MRV
data with complex 3D geometries, time-varying 4D data, or unknown parameters in
the physical model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Sengupta_U/0/1/0/all/0/1"&gt;Ushnish Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kontogiannis_A/0/1/0/all/0/1"&gt;Alexandros Kontogiannis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Juniper_M/0/1/0/all/0/1"&gt;Matthew P. Juniper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Invariant Risk Minimization: A Causal Approach. (arXiv:2102.12353v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12353</id>
        <link href="http://arxiv.org/abs/2102.12353"/>
        <updated>2021-07-19T00:49:07.724Z</updated>
        <summary type="html"><![CDATA[Due to spurious correlations, machine learning systems often fail to
generalize to environments whose distributions differ from the ones used at
training time. Prior work addressing this, either explicitly or implicitly,
attempted to find a data representation that has an invariant relationship with
the target. This is done by leveraging a diverse set of training environments
to reduce the effect of spurious features and build an invariant predictor.
However, these methods have generalization guarantees only when both data
representation and classifiers come from a linear model class. We propose
invariant Causal Representation Learning (iCaRL), an approach that enables
out-of-distribution (OOD) generalization in the nonlinear setting (i.e.,
nonlinear representations and nonlinear classifiers). It builds upon a
practical and general assumption: the prior over the data representation (i.e.,
a set of latent variables encoding the data) given the target and the
environment belongs to general exponential family distributions. Based on this,
we show that it is possible to identify the data representation up to simple
transformations. We also prove that all direct causes of the target can be
fully discovered, which further enables us to obtain generalization guarantees
in the nonlinear setting. Extensive experiments on both synthetic and
real-world datasets show that our approach outperforms a variety of baseline
methods. Finally, in the discussion, we further explore the aforementioned
assumption and propose a more general hypothesis, called the Agnostic
Hypothesis: there exist a set of hidden causal factors affecting both inputs
and outcomes. The Agnostic Hypothesis can provide a unifying view of machine
learning. More importantly, it can inspire a new direction to explore a general
theory for identifying hidden causal factors, which is key to enabling the OOD
generalization guarantees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chaochao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuhuai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1"&gt;Jo&amp;#x15b;e Miguel Hern&amp;#xe1;ndez-Lobato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Projection Robust Wasserstein Barycenters. (arXiv:2102.03390v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03390</id>
        <link href="http://arxiv.org/abs/2102.03390"/>
        <updated>2021-07-19T00:49:07.718Z</updated>
        <summary type="html"><![CDATA[Collecting and aggregating information from several probability measures or
histograms is a fundamental task in machine learning. One of the popular
solution methods for this task is to compute the barycenter of the probability
measures under the Wasserstein metric. However, approximating the Wasserstein
barycenter is numerically challenging because of the curse of dimensionality.
This paper proposes the projection robust Wasserstein barycenter (PRWB) that
has the potential to mitigate the curse of dimensionality. Since PRWB is
numerically very challenging to solve, we further propose a relaxed PRWB
(RPRWB) model, which is more tractable. The RPRWB projects the probability
measures onto a lower-dimensional subspace that maximizes the Wasserstein
barycenter objective. The resulting problem is a max-min problem over the
Stiefel manifold. By combining the iterative Bregman projection algorithm and
Riemannian optimization, we propose two new algorithms for computing the RPRWB.
The complexity of arithmetic operations of the proposed algorithms for
obtaining an $\epsilon$-stationary solution is analyzed. We incorporate the
RPRWB into a discrete distribution clustering algorithm, and the numerical
results on real text datasets confirm that our RPRWB model helps improve the
clustering performance significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minhui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shiqian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1"&gt;Lifeng Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Riemannian Block Coordinate Descent Method for Computing the Projection Robust Wasserstein Distance. (arXiv:2012.05199v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05199</id>
        <link href="http://arxiv.org/abs/2012.05199"/>
        <updated>2021-07-19T00:49:07.702Z</updated>
        <summary type="html"><![CDATA[The Wasserstein distance has become increasingly important in machine
learning and deep learning. Despite its popularity, the Wasserstein distance is
hard to approximate because of the curse of dimensionality. A recently proposed
approach to alleviate the curse of dimensionality is to project the sampled
data from the high dimensional probability distribution onto a
lower-dimensional subspace, and then compute the Wasserstein distance between
the projected data. However, this approach requires to solve a max-min problem
over the Stiefel manifold, which is very challenging in practice. The only
existing work that solves this problem directly is the RGAS (Riemannian
Gradient Ascent with Sinkhorn Iteration) algorithm, which requires to solve an
entropy-regularized optimal transport problem in each iteration, and thus can
be costly for large-scale problems. In this paper, we propose a Riemannian
block coordinate descent (RBCD) method to solve this problem, which is based on
a novel reformulation of the regularized max-min problem over the Stiefel
manifold. We show that the complexity of arithmetic operations for RBCD to
obtain an $\epsilon$-stationary point is $O(\epsilon^{-3})$. This significantly
improves the corresponding complexity of RGAS, which is $O(\epsilon^{-12})$.
Moreover, our RBCD has very low per-iteration complexity, and hence is suitable
for large-scale problems. Numerical results on both synthetic and real datasets
demonstrate that our method is more efficient than existing methods, especially
when the number of sampled data is very large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minhui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shiqian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1"&gt;Lifeng Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Crowdsourcing with Constraints. (arXiv:2012.11048v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11048</id>
        <link href="http://arxiv.org/abs/2012.11048"/>
        <updated>2021-07-19T00:49:07.695Z</updated>
        <summary type="html"><![CDATA[Crowdsourcing has emerged as a powerful paradigm for efficiently labeling
large datasets and performing various learning tasks, by leveraging crowds of
human annotators. When additional information is available about the data,
semi-supervised crowdsourcing approaches that enhance the aggregation of labels
from human annotators are well motivated. This work deals with semi-supervised
crowdsourced classification, under two regimes of semi-supervision: a) label
constraints, that provide ground-truth labels for a subset of data; and b)
potentially easier to obtain instance-level constraints, that indicate
relationships between pairs of data. Bayesian algorithms based on variational
inference are developed for each regime, and their quantifiably improved
performance, compared to unsupervised crowdsourcing, is analytically and
empirically validated on several crowdsourcing datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Traganitis_P/0/1/0/all/0/1"&gt;Panagiotis A. Traganitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannakis_G/0/1/0/all/0/1"&gt;Georgios B. Giannakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WILDS: A Benchmark of in-the-Wild Distribution Shifts. (arXiv:2012.07421v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07421</id>
        <link href="http://arxiv.org/abs/2012.07421"/>
        <updated>2021-07-19T00:49:07.662Z</updated>
        <summary type="html"><![CDATA[Distribution shifts -- where the training distribution differs from the test
distribution -- can substantially degrade the accuracy of machine learning (ML)
systems deployed in the wild. Despite their ubiquity in the real-world
deployments, these distribution shifts are under-represented in the datasets
widely used in the ML community today. To address this gap, we present WILDS, a
curated benchmark of 10 datasets reflecting a diverse range of distribution
shifts that naturally arise in real-world applications, such as shifts across
hospitals for tumor identification; across camera traps for wildlife
monitoring; and across time and location in satellite imaging and poverty
mapping. On each dataset, we show that standard training yields substantially
lower out-of-distribution than in-distribution performance. This gap remains
even with models trained by existing methods for tackling distribution shifts,
underscoring the need for new methods for training models that are more robust
to the types of distribution shifts that arise in practice. To facilitate
method development, we provide an open-source package that automates dataset
loading, contains default model architectures and hyperparameters, and
standardizes evaluations. Code and leaderboards are available at
https://wilds.stanford.edu.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1"&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sagawa_S/0/1/0/all/0/1"&gt;Shiori Sagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marklund_H/0/1/0/all/0/1"&gt;Henrik Marklund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Sang Michael Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Marvin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balsubramani_A/0/1/0/all/0/1"&gt;Akshay Balsubramani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weihua Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1"&gt;Michihiro Yasunaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phillips_R/0/1/0/all/0/1"&gt;Richard Lanas Phillips&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_I/0/1/0/all/0/1"&gt;Irena Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1"&gt;Tony Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+David_E/0/1/0/all/0/1"&gt;Etienne David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stavness_I/0/1/0/all/0/1"&gt;Ian Stavness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Earnshaw_B/0/1/0/all/0/1"&gt;Berton A. Earnshaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haque_I/0/1/0/all/0/1"&gt;Imran S. Haque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1"&gt;Sara Beery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1"&gt;Jure Leskovec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kundaje_A/0/1/0/all/0/1"&gt;Anshul Kundaje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pierson_E/0/1/0/all/0/1"&gt;Emma Pierson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SlimIPL: Language-Model-Free Iterative Pseudo-Labeling. (arXiv:2010.11524v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11524</id>
        <link href="http://arxiv.org/abs/2010.11524"/>
        <updated>2021-07-19T00:49:07.642Z</updated>
        <summary type="html"><![CDATA[Recent results in end-to-end automatic speech recognition have demonstrated
the efficacy of pseudo-labeling for semi-supervised models trained both with
Connectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq)
losses. Iterative Pseudo-Labeling (IPL), which continuously trains a single
model using pseudo-labels iteratively re-generated as the model learns, has
been shown to further improve performance in ASR. We improve upon the IPL
algorithm: as the model learns, we propose to iteratively re-generate
transcriptions with hard labels (the most probable tokens), that is, without a
language model. We call this approach Language-Model-Free IPL (slimIPL) and
give a resultant training setup for low-resource settings with CTC-based
models. slimIPL features a dynamic cache for pseudo-labels which reduces
sensitivity to changes in relabeling hyperparameters and results in improves
training stability. slimIPL is also highly-efficient and requires 3.5-4x fewer
computational resources to converge than other state-of-the-art
semi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL
is competitive with self-supervised approaches, and is state-of-the-art with
100 hours of labeled audio without the use of a language model both at test
time and during pseudo-label generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1"&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiantong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1"&gt;Jacob Kahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1"&gt;Gabriel Synnaeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1"&gt;Ronan Collobert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling User Behaviour in Research Paper Recommendation System. (arXiv:2107.07831v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07831</id>
        <link href="http://arxiv.org/abs/2107.07831"/>
        <updated>2021-07-19T00:49:07.625Z</updated>
        <summary type="html"><![CDATA[User intention which often changes dynamically is considered to be an
important factor for modeling users in the design of recommendation systems.
Recent studies are starting to focus on predicting user intention (what users
want) beyond user preference (what users like). In this work, a user intention
model is proposed based on deep sequential topic analysis. The model predicts a
user's intention in terms of the topic of interest. The Hybrid Topic Model
(HTM) comprising Latent Dirichlet Allocation (LDA) and Word2Vec is proposed to
derive the topic of interest of users and the history of preferences. HTM finds
the true topics of papers estimating word-topic distribution which includes
syntactic and semantic correlations among words. Next, to model user intention,
a Long Short Term Memory (LSTM) based sequential deep learning model is
proposed. This model takes into account temporal context, namely the time
difference between clicks of two consecutive papers seen by a user. Extensive
experiments with the real-world research paper dataset indicate that the
proposed approach significantly outperforms the state-of-the-art methods.
Further, the proposed approach introduces a new road map to model a user
activity suitable for the design of a research paper recommendation system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1"&gt;Arpita Chaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samanta_D/0/1/0/all/0/1"&gt;Debasis Samanta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarma_M/0/1/0/all/0/1"&gt;Monalisa Sarma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Local Structures to Size Generalization in Graph Neural Networks. (arXiv:2010.08853v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08853</id>
        <link href="http://arxiv.org/abs/2010.08853"/>
        <updated>2021-07-19T00:49:07.610Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) can process graphs of different sizes, but their
ability to generalize across sizes, specifically from small to large graphs, is
still not well understood. In this paper, we identify an important type of data
where generalization from small to large graphs is challenging: graph
distributions for which the local structure depends on the graph size. This
effect occurs in multiple important graph learning domains, including social
and biological networks. We first prove that when there is a difference between
the local structures, GNNs are not guaranteed to generalize across sizes: there
are "bad" global minima that do well on small graphs but fail on large graphs.
We then study the size-generalization problem empirically and demonstrate that
when there is a discrepancy in local structure, GNNs tend to converge to
non-generalizing solutions. Finally, we suggest two approaches for improving
size generalization, motivated by our findings. Notably, we propose a novel
Self-Supervised Learning (SSL) task aimed at learning meaningful
representations of local structures that appear in large graphs. Our SSL task
improves classification accuracy on several popular datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1"&gt;Gilad Yehudai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1"&gt;Ethan Fetaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meirom_E/0/1/0/all/0/1"&gt;Eli Meirom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1"&gt;Haggai Maron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Integrator Graph Networks for Learning Energy Conserving Dynamical Systems. (arXiv:2004.13688v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13688</id>
        <link href="http://arxiv.org/abs/2004.13688"/>
        <updated>2021-07-19T00:49:07.603Z</updated>
        <summary type="html"><![CDATA[Recent advances show that neural networks embedded with physics-informed
priors significantly outperform vanilla neural networks in learning and
predicting the long term dynamics of complex physical systems from noisy data.
Despite this success, there has only been a limited study on how to optimally
combine physics priors to improve predictive performance. To tackle this
problem we unpack and generalize recent innovations into individual inductive
bias segments. As such, we are able to systematically investigate all possible
combinations of inductive biases of which existing methods are a natural
subset. Using this framework we introduce Variational Integrator Graph Networks
- a novel method that unifies the strengths of existing approaches by combining
an energy constraint, high-order symplectic variational integrators, and graph
neural networks. We demonstrate, across an extensive ablation, that the
proposed unifying framework outperforms existing methods, for data-efficient
learning and in predictive accuracy, across both single and many-body problems
studied in recent literature. We empirically show that the improvements arise
because high order variational integrators combined with a potential energy
constraint induce coupled learning of generalized position and momentum updates
which can be formalized via the Partitioned Runge-Kutta method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1"&gt;Shaan Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattheakis_M/0/1/0/all/0/1"&gt;Marios Mattheakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropic alternatives to initialization. (arXiv:2107.07757v1 [cond-mat.dis-nn])]]></title>
        <id>http://arxiv.org/abs/2107.07757</id>
        <link href="http://arxiv.org/abs/2107.07757"/>
        <updated>2021-07-19T00:49:07.595Z</updated>
        <summary type="html"><![CDATA[Local entropic loss functions provide a versatile framework to define
architecture-aware regularization procedures. Besides the possibility of being
anisotropic in the synaptic space, the local entropic smoothening of the loss
function can vary during training, thus yielding a tunable model complexity. A
scoping protocol where the regularization is strong in the early-stage of the
training and then fades progressively away constitutes an alternative to
standard initialization procedures for deep convolutional neural networks,
nonetheless, it has wider applicability. We analyze anisotropic, local entropic
smoothenings in the language of statistical physics and information theory,
providing insight into both their interpretation and workings. We comment some
aspects related to the physics of renormalization and the spacetime structure
of convolutional networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Musso_D/0/1/0/all/0/1"&gt;Daniele Musso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CutDepth:Edge-aware Data Augmentation in Depth Estimation. (arXiv:2107.07684v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07684</id>
        <link href="http://arxiv.org/abs/2107.07684"/>
        <updated>2021-07-19T00:49:07.581Z</updated>
        <summary type="html"><![CDATA[It is difficult to collect data on a large scale in a monocular depth
estimation because the task requires the simultaneous acquisition of RGB images
and depths. Data augmentation is thus important to this task. However, there
has been little research on data augmentation for tasks such as monocular depth
estimation, where the transformation is performed pixel by pixel. In this
paper, we propose a data augmentation method, called CutDepth. In CutDepth,
part of the depth is pasted onto an input image during training. The method
extends variations data without destroying edge features. Experiments
objectively and subjectively show that the proposed method outperforms
conventional methods of data augmentation. The estimation accuracy is improved
with CutDepth even though there are few training data at long distances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_Y/0/1/0/all/0/1"&gt;Yasunori Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamashita_T/0/1/0/all/0/1"&gt;Takayoshi Yamashita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Feedforward Neural Network Training via Reachability Analysis. (arXiv:2107.07696v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07696</id>
        <link href="http://arxiv.org/abs/2107.07696"/>
        <updated>2021-07-19T00:49:07.574Z</updated>
        <summary type="html"><![CDATA[Neural networks have recently become popular for a wide variety of uses, but
have seen limited application in safety-critical domains such as robotics near
and around humans. This is because it remains an open challenge to train a
neural network to obey safety constraints. Most existing safety-related methods
only seek to verify that already-trained networks obey constraints, requiring
alternating training and verification. Instead, this work proposes a
constrained method to simultaneously train and verify a feedforward neural
network with rectified linear unit (ReLU) nonlinearities. Constraints are
enforced by computing the network's output-space reachable set and ensuring
that it does not intersect with unsafe sets; training is achieved by
formulating a novel collision-check loss function between the reachable set and
unsafe portions of the output space. The reachable and unsafe sets are
represented by constrained zonotopes, a convex polytope representation that
enables differentiable collision checking. The proposed method is demonstrated
successfully on a network with one nonlinearity layer and approximately 50
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chung_L/0/1/0/all/0/1"&gt;Long Kiu Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1"&gt;Adam Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knowles_D/0/1/0/all/0/1"&gt;Derek Knowles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kousik_S/0/1/0/all/0/1"&gt;Shreyas Kousik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1"&gt;Grace X. Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10785</id>
        <link href="http://arxiv.org/abs/2104.10785"/>
        <updated>2021-07-19T00:49:07.530Z</updated>
        <summary type="html"><![CDATA[In this paper we tackle two important challenges related to the accurate
partial singular value decomposition (SVD) and numerical rank estimation of a
huge matrix to use in low-rank learning problems in a fast way. We use the
concepts of Krylov subspaces such as the Golub-Kahan bidiagonalization process
as well as Ritz vectors to achieve these goals. Our experiments identify
various advantages of the proposed methods compared to traditional and
randomized SVD (R-SVD) methods with respect to the accuracy of the singular
values and corresponding singular vectors computed in a similar execution time.
The proposed methods are appropriate for applications involving huge matrices
where accuracy in all spectrum of the desired singular values, and also all of
corresponding singular vectors is essential. We evaluate our method in the real
application of Riemannian similarity learning (RSL) between two various image
datasets of MNIST and USPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Godaz_R/0/1/0/all/0/1"&gt;Reza Godaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Monsefi_R/0/1/0/all/0/1"&gt;Reza Monsefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Toutounian_F/0/1/0/all/0/1"&gt;Faezeh Toutounian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hosseini_R/0/1/0/all/0/1"&gt;Reshad Hosseini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSCFNet: A Lightweight Network With Multi-Scale Context Fusion for Real-Time Semantic Segmentation. (arXiv:2103.13044v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13044</id>
        <link href="http://arxiv.org/abs/2103.13044"/>
        <updated>2021-07-19T00:49:07.523Z</updated>
        <summary type="html"><![CDATA[In recent years, how to strike a good trade-off between accuracy and
inference speed has become the core issue for real-time semantic segmentation
applications, which plays a vital role in real-world scenarios such as
autonomous driving systems and drones. In this study, we devise a novel
lightweight network using a multi-scale context fusion (MSCFNet) scheme, which
explores an asymmetric encoder-decoder architecture to dispose this problem.
More specifically, the encoder adopts some developed efficient asymmetric
residual (EAR) modules, which are composed of factorization depth-wise
convolution and dilation convolution. Meanwhile, instead of complicated
computation, simple deconvolution is applied in the decoder to further reduce
the amount of parameters while still maintaining high segmentation accuracy.
Also, MSCFNet has branches with efficient attention modules from different
stages of the network to well capture multi-scale contextual information. Then
we combine them before the final classification to enhance the expression of
the features and improve the segmentation efficiency. Comprehensive experiments
on challenging datasets have demonstrated that the proposed MSCFNet, which
contains only 1.15M parameters, achieves 71.9\% Mean IoU on the Cityscapes
testing dataset and can run at over 50 FPS on a single Titan XP GPU
configuration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1"&gt;Guangwei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guoan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_D/0/1/0/all/0/1"&gt;Dong Yue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of preventable fetal distress during labor from scanned cardiotocogram tracings using deep learning. (arXiv:2106.00628v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00628</id>
        <link href="http://arxiv.org/abs/2106.00628"/>
        <updated>2021-07-19T00:49:07.517Z</updated>
        <summary type="html"><![CDATA[Despite broad application during labor and delivery, there remains
considerable debate about the value of electronic fetal monitoring (EFM). EFM
includes the surveillance of the fetal heart rate (FHR) patterns in conjunction
with the maternal uterine contractions providing a wealth of data about fetal
behavior and the threat of diminished oxygenation and perfusion. Adverse
outcomes universally associate a fetal injury with the failure to timely
respond to FHR pattern information. Historically, the EFM data, stored
digitally, are available only as rasterized pdf images for contemporary or
historical discussion and examination. In reality, however, they are rarely
reviewed systematically. Using a unique archive of EFM collected over 50 years
of practice in conjunction with adverse outcomes, we present a deep learning
framework for training and detection of incipient or past fetal injury. We
report 94% accuracy in identifying early, preventable fetal injury intrapartum.
This framework is suited for automating an early warning and decision support
system for maintaining fetal well-being during the stresses of labor.
Ultimately, such a system could enable a physician to timely respond during
labor and prevent adverse outcomes. When adverse outcomes cannot be avoided,
they can provide guidance to the early neuroprotective treatment of the
newborn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Frasch_M/0/1/0/all/0/1"&gt;Martin G. Frasch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Strong_S/0/1/0/all/0/1"&gt;Shadrian B. Strong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Nilosek_D/0/1/0/all/0/1"&gt;David Nilosek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Leaverton_J/0/1/0/all/0/1"&gt;Joshua Leaverton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Schifrin_B/0/1/0/all/0/1"&gt;Barry S. Schifrin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Adversarial Image Synthesis. (arXiv:2106.16056v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.16056</id>
        <link href="http://arxiv.org/abs/2106.16056"/>
        <updated>2021-07-19T00:49:07.510Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have been extremely successful in
various application domains. Adversarial image synthesis has drawn increasing
attention and made tremendous progress in recent years because of its wide
range of applications in many computer vision and image processing problems.
Among the many applications of GAN, image synthesis is the most well-studied
one, and research in this area has already demonstrated the great potential of
using GAN in image synthesis. In this paper, we provide a taxonomy of methods
used in image synthesis, review different models for text-to-image synthesis
and image-to-image translation, and discuss some evaluation metrics as well as
possible future research directions in image synthesis with GAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_W/0/1/0/all/0/1"&gt;William Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_G/0/1/0/all/0/1"&gt;Glen Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leer_R/0/1/0/all/0/1"&gt;Robert Leer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricardo_F/0/1/0/all/0/1"&gt;Frederick Ricardo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Symbolic Operators for Task and Motion Planning. (arXiv:2103.00589v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00589</id>
        <link href="http://arxiv.org/abs/2103.00589"/>
        <updated>2021-07-19T00:49:07.500Z</updated>
        <summary type="html"><![CDATA[Robotic planning problems in hybrid state and action spaces can be solved by
integrated task and motion planners (TAMP) that handle the complex interaction
between motion-level decisions and task-level plan feasibility. TAMP approaches
rely on domain-specific symbolic operators to guide the task-level search,
making planning efficient. In this work, we formalize and study the problem of
operator learning for TAMP. Central to this study is the view that operators
define a lossy abstraction of the transition model of a domain. We then propose
a bottom-up relational learning method for operator learning and show how the
learned operators can be used for planning in a TAMP system. Experimentally, we
provide results in three domains, including long-horizon robotic planning
tasks. We find our approach to substantially outperform several baselines,
including three graph neural network-based model-free approaches from the
recent literature. Video: https://youtu.be/iVfpX9BpBRo Code:
https://git.io/JCT0g]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silver_T/0/1/0/all/0/1"&gt;Tom Silver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chitnis_R/0/1/0/all/0/1"&gt;Rohan Chitnis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1"&gt;Leslie Pack Kaelbling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1"&gt;Tomas Lozano-Perez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Disentangled Representations Learned From Correlated Data. (arXiv:2006.07886v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07886</id>
        <link href="http://arxiv.org/abs/2006.07886"/>
        <updated>2021-07-19T00:49:07.484Z</updated>
        <summary type="html"><![CDATA[The focus of disentanglement approaches has been on identifying independent
factors of variation in data. However, the causal variables underlying
real-world observations are often not statistically independent. In this work,
we bridge the gap to real-world scenarios by analyzing the behavior of the most
prominent disentanglement approaches on correlated data in a large-scale
empirical study (including 4260 models). We show and quantify that
systematically induced correlations in the dataset are being learned and
reflected in the latent representations, which has implications for downstream
applications of disentanglement such as fairness. We also demonstrate how to
resolve these latent correlations, either using weak supervision during
training or by post-hoc correcting a pre-trained model with a small number of
labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1"&gt;Frederik Tr&amp;#xe4;uble&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1"&gt;Elliot Creager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1"&gt;Niki Kilbertus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1"&gt;Francesco Locatello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1"&gt;Andrea Dittadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1"&gt;Anirudh Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1"&gt;Stefan Bauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-task Learning with Cross Attention for Keyword Spotting. (arXiv:2107.07634v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.07634</id>
        <link href="http://arxiv.org/abs/2107.07634"/>
        <updated>2021-07-19T00:49:07.478Z</updated>
        <summary type="html"><![CDATA[Keyword spotting (KWS) is an important technique for speech applications,
which enables users to activate devices by speaking a keyword phrase. Although
a phoneme classifier can be used for KWS, exploiting a large amount of
transcribed data for automatic speech recognition (ASR), there is a mismatch
between the training criterion (phoneme recognition) and the target task (KWS).
Recently, multi-task learning has been applied to KWS to exploit both ASR and
KWS training data. In this approach, an output of an acoustic model is split
into two branches for the two tasks, one for phoneme transcription trained with
the ASR data and one for keyword classification trained with the KWS data. In
this paper, we introduce a cross attention decoder in the multi-task learning
framework. Unlike the conventional multi-task learning approach with the simple
split of the output layer, the cross attention decoder summarizes information
from a phonetic encoder by performing cross attention between the encoder
outputs and a trainable query sequence to predict a confidence score for the
KWS task. Experimental results on KWS tasks show that the proposed approach
outperformed the conventional multi-task learning with split branches and a
bi-directional long short-team memory decoder by 12% on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Higuchi_T/0/1/0/all/0/1"&gt;Takuya Higuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Anmol Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dhir_C/0/1/0/all/0/1"&gt;Chandra Dhir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Cloud Transformers applied to Collider Physics. (arXiv:2102.05073v2 [physics.data-an] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05073</id>
        <link href="http://arxiv.org/abs/2102.05073"/>
        <updated>2021-07-19T00:49:07.471Z</updated>
        <summary type="html"><![CDATA[Methods for processing point cloud information have seen a great success in
collider physics applications. One recent breakthrough in machine learning is
the usage of Transformer networks to learn semantic relationships between
sequences in language processing. In this work, we apply a modified Transformer
network called Point Cloud Transformer as a method to incorporate the
advantages of the Transformer architecture to an unordered set of particles
resulting from collision events. To compare the performance with other
strategies, we study jet-tagging applications for highly-boosted particles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Mikuni_V/0/1/0/all/0/1"&gt;Vinicius Mikuni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Canelli_F/0/1/0/all/0/1"&gt;Florencia Canelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information flows of diverse autoencoders. (arXiv:2102.07402v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07402</id>
        <link href="http://arxiv.org/abs/2102.07402"/>
        <updated>2021-07-19T00:49:07.466Z</updated>
        <summary type="html"><![CDATA[The outstanding performance of deep learning in various fields has been a
fundamental query, which can be potentially examined using information theory
that interprets the learning process as the transmission and compression of
information. Information plane analyses of the mutual information between the
input-hidden-output layers demonstrated two distinct learning phases of fitting
and compression. It is debatable if the compression phase is necessary to
generalize the input-output relations extracted from training data. In this
study, we investigated this through experiments with various species of
autoencoders and evaluated their information processing phase with an accurate
kernel-based estimator of mutual information. Given sufficient training data,
vanilla autoencoders demonstrated the compression phase, which was amplified
after imposing sparsity regularization for hidden activities. However, we found
that the compression phase is not universally observed in different species of
autoencoders, including variational autoencoders, that have special constraints
on network weights or manifold of hidden space. These types of autoencoders
exhibited perfect generalization ability for test data without requiring the
compression phase. Thus, we conclude that the compression phase is not
necessary for generalization in representation learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sungyeop Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1"&gt;Junghyo Jo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DANCE: DAta-Network Co-optimization for Efficient Segmentation Model Training and Inference. (arXiv:2107.07706v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07706</id>
        <link href="http://arxiv.org/abs/2107.07706"/>
        <updated>2021-07-19T00:49:07.450Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation for scene understanding is nowadays widely demanded,
raising significant challenges for the algorithm efficiency, especially its
applications on resource-limited platforms. Current segmentation models are
trained and evaluated on massive high-resolution scene images ("data level")
and suffer from the expensive computation arising from the required multi-scale
aggregation("network level"). In both folds, the computational and energy costs
in training and inference are notable due to the often desired large input
resolutions and heavy computational burden of segmentation models. To this end,
we propose DANCE, general automated DAta-Network Co-optimization for Efficient
segmentation model training and inference. Distinct from existing efficient
segmentation approaches that focus merely on light-weight network design, DANCE
distinguishes itself as an automated simultaneous data-network co-optimization
via both input data manipulation and network architecture slimming.
Specifically, DANCE integrates automated data slimming which adaptively
downsamples/drops input images and controls their corresponding contribution to
the training loss guided by the images' spatial complexity. Such a downsampling
operation, in addition to slimming down the cost associated with the input size
directly, also shrinks the dynamic range of input object and context scales,
therefore motivating us to also adaptively slim the network to match the
downsampled data. Extensive experiments and ablating studies (on four SOTA
segmentation models with three popular segmentation datasets under two training
settings) demonstrate that DANCE can achieve "all-win" towards efficient
segmentation(reduced training cost, less expensive inference, and better mean
Intersection-over-Union (mIoU)).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chaojian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wuyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yuchen Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yonggan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yingyan Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning for Automated Audio Captioning Using The Learning Without Forgetting Approach. (arXiv:2107.08028v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.08028</id>
        <link href="http://arxiv.org/abs/2107.08028"/>
        <updated>2021-07-19T00:49:07.440Z</updated>
        <summary type="html"><![CDATA[Automated audio captioning (AAC) is the task of automatically creating
textual descriptions (i.e. captions) for the contents of a general audio
signal. Most AAC methods are using existing datasets to optimize and/or
evaluate upon. Given the limited information held by the AAC datasets, it is
very likely that AAC methods learn only the information contained in the
utilized datasets. In this paper we present a first approach for continuously
adapting an AAC method to new information, using a continual learning method.
In our scenario, a pre-optimized AAC method is used for some unseen general
audio signals and can update its parameters in order to adapt to the new
information, given a new reference caption. We evaluate our method using a
freely available, pre-optimized AAC method and two freely available AAC
datasets. We compare our proposed method with three scenarios, two of training
on one of the datasets and evaluating on the other and a third of training on
one dataset and fine-tuning on the other. Obtained results show that our method
achieves a good balance between distilling new knowledge and not forgetting the
previous one.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Berg_J/0/1/0/all/0/1"&gt;Jan Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drossos_K/0/1/0/all/0/1"&gt;Konstantinos Drossos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Transfer and Interference in Multi-Domain Learning. (arXiv:2107.05445v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05445</id>
        <link href="http://arxiv.org/abs/2107.05445"/>
        <updated>2021-07-19T00:49:07.433Z</updated>
        <summary type="html"><![CDATA[Humans are incredibly good at transferring knowledge from one domain to
another, enabling rapid learning of new tasks. Likewise, transfer learning has
enabled enormous success in many computer vision problems using pretraining.
However, the benefits of transfer in multi-domain learning, where a network
learns multiple tasks defined by different datasets, has not been adequately
studied. Learning multiple domains could be beneficial or these domains could
interfere with each other given limited network capacity. In this work, we
decipher the conditions where interference and knowledge transfer occur in
multi-domain learning. We propose new metrics disentangling interference and
transfer and set up experimental protocols. We further examine the roles of
network capacity, task grouping, and dynamic loss weighting in reducing
interference and facilitating transfer. We demonstrate our findings on the
CIFAR-100, MiniPlaces, and Tiny-ImageNet datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yipeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1"&gt;Tyler L. Hayes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1"&gt;Christopher Kanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs. (arXiv:2010.04029v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04029</id>
        <link href="http://arxiv.org/abs/2010.04029"/>
        <updated>2021-07-19T00:49:07.391Z</updated>
        <summary type="html"><![CDATA[This paper studies learning logic rules for reasoning on knowledge graphs.
Logic rules provide interpretable explanations when used for prediction as well
as being able to generalize to other tasks, and hence are critical to learn.
Existing methods either suffer from the problem of searching in a large search
space (e.g., neural logic programming) or ineffective optimization due to
sparse rewards (e.g., techniques based on reinforcement learning). To address
these limitations, this paper proposes a probabilistic model called RNNLogic.
RNNLogic treats logic rules as a latent variable, and simultaneously trains a
rule generator as well as a reasoning predictor with logic rules. We develop an
EM-based algorithm for optimization. In each iteration, the reasoning predictor
is first updated to explore some generated logic rules for reasoning. Then in
the E-step, we select a set of high-quality rules from all generated rules with
both the rule generator and reasoning predictor via posterior inference; and in
the M-step, the rule generator is updated with the rules selected in the
E-step. Experiments on four datasets prove the effectiveness of RNNLogic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_M/0/1/0/all/0/1"&gt;Meng Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Junkun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xhonneux_L/0/1/0/all/0/1"&gt;Louis-Pascal Xhonneux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization Properties of hyper-RKHS and its Applications. (arXiv:1809.09910v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1809.09910</id>
        <link href="http://arxiv.org/abs/1809.09910"/>
        <updated>2021-07-19T00:49:07.386Z</updated>
        <summary type="html"><![CDATA[This paper generalizes regularized regression problems in a hyper-reproducing
kernel Hilbert space (hyper-RKHS), illustrates its utility for kernel learning
and out-of-sample extensions, and proves asymptotic convergence results for the
introduced regression models in an approximation theory view. Algorithmically,
we consider two regularized regression models with bivariate forms in this
space, including kernel ridge regression (KRR) and support vector regression
(SVR) endowed with hyper-RKHS, and further combine divide-and-conquer with
Nystr\"{o}m approximation for scalability in large sample cases. This framework
is general: the underlying kernel is learned from a broad class, and can be
positive definite or not, which adapts to various requirements in kernel
learning. Theoretically, we study the convergence behavior of regularized
regression algorithms in hyper-RKHS and derive the learning rates, which goes
beyond the classical analysis on RKHS due to the non-trivial independence of
pairwise samples and the characterisation of hyper-RKHS. Experimentally,
results on several benchmarks suggest that the employed framework is able to
learn a general kernel function form an arbitrary similarity matrix, and thus
achieves a satisfactory performance on classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fanghui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1"&gt;Lei Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1"&gt;Johan A.K. Suykens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is attention to bounding boxes all you need for pedestrian action prediction?. (arXiv:2107.08031v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08031</id>
        <link href="http://arxiv.org/abs/2107.08031"/>
        <updated>2021-07-19T00:49:07.373Z</updated>
        <summary type="html"><![CDATA[The human driver is no longer the only one concerned with the complexity of
the driving scenarios. Autonomous vehicles (AV) are similarly becoming involved
in the process. Nowadays, the development of AV in urban places underpins
essential safety concerns for vulnerable road users (VRUs) such as pedestrians.
Therefore, to make the roads safer, it is critical to classify and predict
their future behavior. In this paper, we present a framework based on multiple
variations of the Transformer models to reason attentively about the dynamic
evolution of the pedestrians' past trajectory and predict its future actions of
crossing or not crossing the street. We proved that using only bounding boxes
as input to our model can outperform the previous state-of-the-art models and
reach a prediction accuracy of 91 % and an F1-score of 0.83 on the PIE dataset
up to two seconds ahead in the future. In addition, we introduced a large-size
simulated dataset (CP2A) using CARLA for action prediction. Our model has
similarly reached high accuracy (91 %) and F1-score (0.91) on this dataset.
Interestingly, we showed that pre-training our Transformer model on the
simulated dataset and then fine-tuning it on the real dataset can be very
effective for the action prediction task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Achaji_L/0/1/0/all/0/1"&gt;Lina Achaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1"&gt;Julien Moreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fouqueray_T/0/1/0/all/0/1"&gt;Thibault Fouqueray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aioun_F/0/1/0/all/0/1"&gt;Francois Aioun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charpillet_F/0/1/0/all/0/1"&gt;Francois Charpillet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Kernel Attention Transformers. (arXiv:2107.07999v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07999</id>
        <link href="http://arxiv.org/abs/2107.07999"/>
        <updated>2021-07-19T00:49:07.330Z</updated>
        <summary type="html"><![CDATA[We introduce a new class of graph neural networks (GNNs), by combining
several concepts that were so far studied independently - graph kernels,
attention-based networks with structural priors and more recently, efficient
Transformers architectures applying small memory footprint implicit attention
methods via low rank decomposition techniques. The goal of the paper is
twofold. Proposed by us Graph Kernel Attention Transformers (or GKATs) are much
more expressive than SOTA GNNs as capable of modeling longer-range dependencies
within a single layer. Consequently, they can use more shallow architecture
design. Furthermore, GKAT attention layers scale linearly rather than
quadratically in the number of nodes of the input graphs, even when those
graphs are dense, requiring less compute than their regular graph attention
counterparts. They achieve it by applying new classes of graph kernels
admitting random feature map decomposition via random walks on graphs. As a
byproduct of the introduced techniques, we obtain a new class of learnable
graph sketches, called graphots, compactly encoding topological graph
properties as well as nodes' features. We conducted exhaustive empirical
comparison of our method with nine different GNN classes on tasks ranging from
motif detection through social network classification to bioinformatics
challenges, showing consistent gains coming from GKATs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Han Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoxian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOK: Seeing and Believing: Evaluating the Trustworthiness of Twitter Users. (arXiv:2107.08027v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2107.08027</id>
        <link href="http://arxiv.org/abs/2107.08027"/>
        <updated>2021-07-19T00:49:07.323Z</updated>
        <summary type="html"><![CDATA[Social networking and micro-blogging services, such as Twitter, play an
important role in sharing digital information. Despite the popularity and
usefulness of social media, there have been many instances where corrupted
users found ways to abuse it, as for instance, through raising or lowering
user's credibility. As a result, while social media facilitates an
unprecedented ease of access to information, it also introduces a new challenge
- that of ascertaining the credibility of shared information. Currently, there
is no automated way of determining which news or users are credible and which
are not. Hence, establishing a system that can measure the social media user's
credibility has become an issue of great importance. Assigning a credibility
score to a user has piqued the interest of not only the research community but
also most of the big players on both sides - such as Facebook, on the side of
industry, and political parties on the societal one. In this work, we created a
model which, we hope, will ultimately facilitate and support the increase of
trust in the social network communities. Our model collected data and analysed
the behaviour of~50,000 politicians on Twitter. Influence score, based on
several chosen features, was assigned to each evaluated user. Further, we
classified the political Twitter users as either trusted or untrusted using
random forest, multilayer perceptron, and support vector machine. An active
learning model was used to classify any unlabelled ambiguous records from our
dataset. Finally, to measure the performance of the proposed model, we used
precision, recall, F1 score, and accuracy as the main evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1"&gt;Tanveer Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michalas_A/0/1/0/all/0/1"&gt;Antonis Michalas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S2TA: Exploiting Structured Sparsity for Energy-Efficient Mobile CNN Acceleration. (arXiv:2107.07983v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2107.07983</id>
        <link href="http://arxiv.org/abs/2107.07983"/>
        <updated>2021-07-19T00:49:07.318Z</updated>
        <summary type="html"><![CDATA[Exploiting sparsity is a key technique in accelerating quantized
convolutional neural network (CNN) inference on mobile devices. Prior sparse
CNN accelerators largely exploit un-structured sparsity and achieve significant
speedups. Due to the unbounded, largely unpredictable sparsity patterns,
however, exploiting unstructured sparsity requires complicated hardware design
with significant energy and area overhead, which is particularly detrimental to
mobile/IoT inference scenarios where energy and area efficiency are crucial. We
propose to exploit structured sparsity, more specifically, Density Bound Block
(DBB) sparsity for both weights and activations. DBB block tensors bound the
maximum number of non-zeros per block. DBB thus exposes statically predictable
sparsity patterns that enable lean sparsity-exploiting hardware. We propose new
hardware primitives to implement DBB sparsity for (static) weights and
(dynamic) activations, respectively, with very low overheads. Building on top
of the primitives, we describe S2TA, a systolic array-based CNN accelerator
that exploits joint weight and activation DBB sparsity and new dimensions of
data reuse unavailable on the traditional systolic array. S2TA in 16nm achieves
more than 2x speedup and energy reduction compared to a strong baseline of a
systolic array with zero-value clock gating, over five popular CNN benchmarks.
Compared to two recent non-systolic sparse accelerators, Eyeriss v2 (65nm) and
SparTen (45nm), S2TA in 65nm uses about 2.2x and 3.1x less energy per
inference, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhi-Gang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1"&gt;Paul N. Whatmough&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuhao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1"&gt;Matthew Mattina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Process Boosting. (arXiv:2004.02653v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.02653</id>
        <link href="http://arxiv.org/abs/2004.02653"/>
        <updated>2021-07-19T00:49:07.312Z</updated>
        <summary type="html"><![CDATA[We introduce a novel way to combine boosting with Gaussian process and mixed
effects models. This allows for relaxing, first, the linearity assumption for
the mean function in Gaussian process and grouped random effects models in a
flexible non-parametric way and, second, the independence assumption made in
most boosting algorithms. The former is advantageous for predictive accuracy
and for avoiding model misspecifications. The latter is important for more
efficient learning of the mean function and for obtaining probabilistic
predictions. In addition, we present an extension that scales to large data
using a Vecchia approximation for the Gaussian process model relying on novel
results for covariance parameter inference. We obtain increased predictive
accuracy compared to existing approaches on several simulated and real-world
data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sigrist_F/0/1/0/all/0/1"&gt;Fabio Sigrist&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine-learning Kondo physics using variational autoencoders. (arXiv:2107.08013v1 [cond-mat.str-el])]]></title>
        <id>http://arxiv.org/abs/2107.08013</id>
        <link href="http://arxiv.org/abs/2107.08013"/>
        <updated>2021-07-19T00:49:07.294Z</updated>
        <summary type="html"><![CDATA[We employ variational autoencoders to extract physical insight from a dataset
of one-particle Anderson impurity model spectral functions. Autoencoders are
trained to find a low-dimensional, latent space representation that faithfully
characterizes each element of the training set, as measured by a reconstruction
error. Variational autoencoders, a probabilistic generalization of standard
autoencoders, further condition the learned latent space to promote highly
interpretable features. In our study, we find that the learned latent space
components strongly correlate with well known, but nontrivial, parameters that
characterize emergent behaviors in the Anderson impurity model. In particular,
one latent space component correlates with particle-hole asymmetry, while
another is in near one-to-one correspondence with the Kondo temperature, a
dynamically generated low-energy scale in the impurity model. With symbolic
regression, we model this component as a function of bare physical input
parameters and "rediscover" the non-perturbative formula for the Kondo
temperature. The machine learning pipeline we develop opens opportunities to
discover new domain knowledge in other physical systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Miles_C/0/1/0/all/0/1"&gt;Cole Miles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Carbone_M/0/1/0/all/0/1"&gt;Matthew R. Carbone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Sturm_E/0/1/0/all/0/1"&gt;Erica J. Sturm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Lu_D/0/1/0/all/0/1"&gt;Deyu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Weichselbaum_A/0/1/0/all/0/1"&gt;Andreas Weichselbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Barros_K/0/1/0/all/0/1"&gt;Kipton Barros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Konik_R/0/1/0/all/0/1"&gt;Robert M. Konik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlled AutoEncoders to Generate Faces from Voices. (arXiv:2107.07988v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07988</id>
        <link href="http://arxiv.org/abs/2107.07988"/>
        <updated>2021-07-19T00:49:07.288Z</updated>
        <summary type="html"><![CDATA[Multiple studies in the past have shown that there is a strong correlation
between human vocal characteristics and facial features. However, existing
approaches generate faces simply from voice, without exploring the set of
features that contribute to these observed correlations. A computational
methodology to explore this can be devised by rephrasing the question to: "how
much would a target face have to change in order to be perceived as the
originator of a source voice?" With this in perspective, we propose a framework
to morph a target face in response to a given voice in a way that facial
features are implicitly guided by learned voice-face correlation in this paper.
Our framework includes a guided autoencoder that converts one face to another,
controlled by a unique model-conditioning component called a gating controller
which modifies the reconstructed face based on input voice recordings. We
evaluate the framework on VoxCelab and VGGFace datasets through human subjects
and face retrieval. Various experiments demonstrate the effectiveness of our
proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Hao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lulan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guikang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1"&gt;Bhiksha Raj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rita Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Bayesian Sampling Using Normalizing Flows to Assist Markov Chain Monte Carlo Methods. (arXiv:2107.08001v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.08001</id>
        <link href="http://arxiv.org/abs/2107.08001"/>
        <updated>2021-07-19T00:49:07.282Z</updated>
        <summary type="html"><![CDATA[Normalizing flows can generate complex target distributions and thus show
promise in many applications in Bayesian statistics as an alternative or
complement to MCMC for sampling posteriors. Since no data set from the target
posterior distribution is available beforehand, the flow is typically trained
using the reverse Kullback-Leibler (KL) divergence that only requires samples
from a base distribution. This strategy may perform poorly when the posterior
is complicated and hard to sample with an untrained normalizing flow. Here we
explore a distinct training strategy, using the direct KL divergence as loss,
in which samples from the posterior are generated by (i) assisting a local MCMC
algorithm on the posterior with a normalizing flow to accelerate its mixing
rate and (ii) using the data generated this way to train the flow. The method
only requires a limited amount of \textit{a~priori} input about the posterior,
and can be used to estimate the evidence required for model validation, as we
illustrate on examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gabrie_M/0/1/0/all/0/1"&gt;Marylou Gabri&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rotskoff_G/0/1/0/all/0/1"&gt;Grant M. Rotskoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vanden_Eijnden_E/0/1/0/all/0/1"&gt;Eric Vanden-Eijnden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tracing Halpha Fibrils through Bayesian Deep Learning. (arXiv:2107.07886v1 [astro-ph.SR])]]></title>
        <id>http://arxiv.org/abs/2107.07886</id>
        <link href="http://arxiv.org/abs/2107.07886"/>
        <updated>2021-07-19T00:49:07.276Z</updated>
        <summary type="html"><![CDATA[We present a new deep learning method, dubbed FibrilNet, for tracing
chromospheric fibrils in Halpha images of solar observations. Our method
consists of a data pre-processing component that prepares training data from a
threshold-based tool, a deep learning model implemented as a Bayesian
convolutional neural network for probabilistic image segmentation with
uncertainty quantification to predict fibrils, and a post-processing component
containing a fibril-fitting algorithm to determine fibril orientations. The
FibrilNet tool is applied to high-resolution Halpha images from an active
region (AR 12665) collected by the 1.6 m Goode Solar Telescope (GST) equipped
with high-order adaptive optics at the Big Bear Solar Observatory (BBSO). We
quantitatively assess the FibrilNet tool, comparing its image segmentation
algorithm and fibril-fitting algorithm with those employed by the
threshold-based tool. Our experimental results and major findings are
summarized as follows. First, the image segmentation results (i.e., detected
fibrils) of the two tools are quite similar, demonstrating the good learning
capability of FibrilNet. Second, FibrilNet finds more accurate and smoother
fibril orientation angles than the threshold-based tool. Third, FibrilNet is
faster than the threshold-based tool and the uncertainty maps produced by
FibrilNet not only provide a quantitative way to measure the confidence on each
detected fibril, but also help identify fibril structures that are not detected
by the threshold-based tool but are inferred through machine learning. Finally,
we apply FibrilNet to full-disk Halpha images from other solar observatories
and additional high-resolution Halpha images collected by BBSO/GST,
demonstrating the tool's usability in diverse datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haodi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Jing_J/0/1/0/all/0/1"&gt;Ju Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiasheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jason T. L. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haimin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections. (arXiv:2107.07859v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07859</id>
        <link href="http://arxiv.org/abs/2107.07859"/>
        <updated>2021-07-19T00:49:07.270Z</updated>
        <summary type="html"><![CDATA[We propose Steadiness and Cohesiveness, two novel metrics to measure the
inter-cluster reliability of multidimensional projection (MDP), specifically
how well the inter-cluster structures are preserved between the original
high-dimensional space and the low-dimensional projection space. Measuring
inter-cluster reliability is crucial as it directly affects how well
inter-cluster tasks (e.g., identifying cluster relationships in the original
space from a projected view) can be conducted; however, despite the importance
of inter-cluster tasks, we found that previous metrics, such as Trustworthiness
and Continuity, fail to measure inter-cluster reliability. Our metrics consider
two aspects of the inter-cluster reliability: Steadiness measures the extent to
which clusters in the projected space form clusters in the original space, and
Cohesiveness measures the opposite. They extract random clusters with arbitrary
shapes and positions in one space and evaluate how much the clusters are
stretched or dispersed in the other space. Furthermore, our metrics can
quantify pointwise distortions, allowing for the visualization of inter-cluster
reliability in a projection, which we call a reliability map. Through
quantitative experiments, we verify that our metrics precisely capture the
distortions that harm inter-cluster reliability while previous metrics have
difficulty capturing the distortions. A case study also demonstrates that our
metrics and the reliability map 1) support users in selecting the proper
projection techniques or hyperparameters and 2) prevent misinterpretation while
performing inter-cluster tasks, thus allow an adequate identification of
inter-cluster structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1"&gt;Hyeon Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1"&gt;Hyung-Kwon Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1"&gt;Jaemin Jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngtaek Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1"&gt;Jinwook Seo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Property-aware Adaptive Relation Networks for Molecular Property Prediction. (arXiv:2107.07994v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07994</id>
        <link href="http://arxiv.org/abs/2107.07994"/>
        <updated>2021-07-19T00:49:07.265Z</updated>
        <summary type="html"><![CDATA[Molecular property prediction plays a fundamental role in drug discovery to
discover candidate molecules with target properties. However, molecular
property prediction is essentially a few-shot problem which makes it hard to
obtain regular models. In this paper, we propose a property-aware adaptive
relation networks (PAR) for the few-shot molecular property prediction problem.
In comparison to existing works, we leverage the facts that both substructures
and relationships among molecules are different considering various molecular
properties. Our PAR is compatible with existing graph-based molecular encoders,
and are further equipped with the ability to obtain property-aware molecular
embedding and model molecular relation graph adaptively. The resultant relation
graph also facilitates effective label propagation within each task. Extensive
experiments on benchmark molecular property prediction datasets show that our
method consistently outperforms state-of-the-art methods and is able to obtain
property-aware molecular embedding and model molecular relation graph properly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1"&gt;Abulikemu Abuduweili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Uncertainty-Aware, Shareable and Transparent Neural Network Architecture for Brain-Age Modeling. (arXiv:2107.07977v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07977</id>
        <link href="http://arxiv.org/abs/2107.07977"/>
        <updated>2021-07-19T00:49:07.248Z</updated>
        <summary type="html"><![CDATA[The deviation between chronological age and age predicted from neuroimaging
data has been identified as a sensitive risk-marker of cross-disorder brain
changes, growing into a cornerstone of biological age-research. However,
Machine Learning models underlying the field do not consider uncertainty,
thereby confounding results with training data density and variability. Also,
existing models are commonly based on homogeneous training sets, often not
independently validated, and cannot be shared due to data protection issues.
Here, we introduce an uncertainty-aware, shareable, and transparent Monte-Carlo
Dropout Composite-Quantile-Regression (MCCQR) Neural Network trained on
N=10,691 datasets from the German National Cohort. The MCCQR model provides
robust, distribution-free uncertainty quantification in high-dimensional
neuroimaging data, achieving lower error rates compared to existing models
across ten recruitment centers and in three independent validation samples
(N=4,004). In two examples, we demonstrate that it prevents spurious
associations and increases power to detect accelerated brain-aging. We make the
pre-trained model publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hahn_T/0/1/0/all/0/1"&gt;Tim Hahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ernsting_J/0/1/0/all/0/1"&gt;Jan Ernsting&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winter_N/0/1/0/all/0/1"&gt;Nils R. Winter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holstein_V/0/1/0/all/0/1"&gt;Vincent Holstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leenings_R/0/1/0/all/0/1"&gt;Ramona Leenings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beisemann_M/0/1/0/all/0/1"&gt;Marie Beisemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fisch_L/0/1/0/all/0/1"&gt;Lukas Fisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarink_K/0/1/0/all/0/1"&gt;Kelvin Sarink&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emden_D/0/1/0/all/0/1"&gt;Daniel Emden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Opel_N/0/1/0/all/0/1"&gt;Nils Opel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Redlich_R/0/1/0/all/0/1"&gt;Ronny Redlich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Repple_J/0/1/0/all/0/1"&gt;Jonathan Repple&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grotegerd_D/0/1/0/all/0/1"&gt;Dominik Grotegerd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meinert_S/0/1/0/all/0/1"&gt;Susanne Meinert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirsch_J/0/1/0/all/0/1"&gt;Jochen G. Hirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niendorf_T/0/1/0/all/0/1"&gt;Thoralf Niendorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Endemann_B/0/1/0/all/0/1"&gt;Beate Endemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bamberg_F/0/1/0/all/0/1"&gt;Fabian Bamberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kroncke_T/0/1/0/all/0/1"&gt;Thomas Kr&amp;#xf6;ncke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulow_R/0/1/0/all/0/1"&gt;Robin B&amp;#xfc;low&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Volzke_H/0/1/0/all/0/1"&gt;Henry V&amp;#xf6;lzke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stackelberg_O/0/1/0/all/0/1"&gt;Oyunbileg von Stackelberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sowade_R/0/1/0/all/0/1"&gt;Ramona Felizitas Sowade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Umutlu_L/0/1/0/all/0/1"&gt;Lale Umutlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_B/0/1/0/all/0/1"&gt;B&amp;#xf6;rge Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caspers_S/0/1/0/all/0/1"&gt;Svenja Caspers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Consortium_G/0/1/0/all/0/1"&gt;German National Cohort Study Center Consortium&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kugel_H/0/1/0/all/0/1"&gt;Harald Kugel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kircher_T/0/1/0/all/0/1"&gt;Tilo Kircher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risse_B/0/1/0/all/0/1"&gt;Benjamin Risse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaser_C/0/1/0/all/0/1"&gt;Christian Gaser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1"&gt;James H. Cole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dannlowski_U/0/1/0/all/0/1"&gt;Udo Dannlowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_K/0/1/0/all/0/1"&gt;Klaus Berger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Graph Topology Learning from Matrix-valued Time Series. (arXiv:2107.08020v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.08020</id>
        <link href="http://arxiv.org/abs/2107.08020"/>
        <updated>2021-07-19T00:49:07.242Z</updated>
        <summary type="html"><![CDATA[This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations), recording, over time, observations of multiple
measurements. From such data, we propose to learn, in an online fashion, a
graph that captures two aspects of dependency: one describing the sparse
spatial relationship between sensors, and the other characterizing the
measurement relationship. To this purpose, we introduce a novel multivariate
autoregressive model to infer the graph topology encoded in the coefficient
matrix which captures the sparse Granger causality dependency structure present
in such matrix-valued time series. We decompose the graph by imposing a
Kronecker sum structure on the coefficient matrix. We develop two online
approaches to learn the graph in a recursive way. The first one uses Wald test
for the projected OLS estimation, where we derive the asymptotic distribution
for the estimator. For the second one, we formalize a Lasso-type optimization
problem. We rely on homotopy algorithms to derive updating rules for estimating
the coefficient matrix. Furthermore, we provide an adaptive tuning procedure
for the regularization parameter. Numerical experiments using both synthetic
and real data, are performed to support the effectiveness of the proposed
learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yiye Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bigot_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xe9;mie Bigot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Maabout_S/0/1/0/all/0/1"&gt;Sofian Maabout&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty Prediction for Machine Learning Models of Material Properties. (arXiv:2107.07997v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07997</id>
        <link href="http://arxiv.org/abs/2107.07997"/>
        <updated>2021-07-19T00:49:07.236Z</updated>
        <summary type="html"><![CDATA[Uncertainty quantification in Artificial Intelligence (AI)-based predictions
of material properties is of immense importance for the success and reliability
of AI applications in material science. While confidence intervals are commonly
reported for machine learning (ML) models, prediction intervals, i.e., the
evaluation of the uncertainty on each prediction, are seldomly available. In
this work we compare 3 different approaches to obtain such individual
uncertainty, testing them on 12 ML-physical properties. Specifically, we
investigated using the Quantile loss function, machine learning the prediction
intervals directly and using Gaussian Processes. We identify each approachs
advantages and disadvantages and end up slightly favoring the modeling of the
individual uncertainties directly, as it is the easiest to fit and, in most
cases, minimizes over-and under-estimation of the predicted errors. All data
for training and testing were taken from the publicly available JARVIS-DFT
database, and the codes developed for computing the prediction intervals are
available through JARVIS-Tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tavazza_F/0/1/0/all/0/1"&gt;Francesca Tavazza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cost_B/0/1/0/all/0/1"&gt;Brian De Cost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_K/0/1/0/all/0/1"&gt;Kamal Choudhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Port-Hamiltonian Neural Networks for Learning Explicit Time-Dependent Dynamical Systems. (arXiv:2107.08024v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.08024</id>
        <link href="http://arxiv.org/abs/2107.08024"/>
        <updated>2021-07-19T00:49:07.219Z</updated>
        <summary type="html"><![CDATA[Accurately learning the temporal behavior of dynamical systems requires
models with well-chosen learning biases. Recent innovations embed the
Hamiltonian and Lagrangian formalisms into neural networks and demonstrate a
significant improvement over other approaches in predicting trajectories of
physical systems. These methods generally tackle autonomous systems that depend
implicitly on time or systems for which a control signal is known apriori.
Despite this success, many real world dynamical systems are non-autonomous,
driven by time-dependent forces and experience energy dissipation. In this
study, we address the challenge of learning from such non-autonomous systems by
embedding the port-Hamiltonian formalism into neural networks, a versatile
framework that can capture energy dissipation and time-dependent control
forces. We show that the proposed \emph{port-Hamiltonian neural network} can
efficiently learn the dynamics of nonlinear physical systems of practical
interest and accurately recover the underlying stationary Hamiltonian,
time-dependent force, and dissipative coefficient. A promising outcome of our
network is its ability to learn and predict chaotic systems such as the Duffing
equation, for which the trajectories are typically hard to learn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1"&gt;Shaan Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattheakis_M/0/1/0/all/0/1"&gt;Marios Mattheakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sondak_D/0/1/0/all/0/1"&gt;David Sondak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Protopapas_P/0/1/0/all/0/1"&gt;Pavlos Protopapas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nearest neighbor Methods and their Applications in Design of 5G & Beyond Wireless Networks. (arXiv:2107.07869v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2107.07869</id>
        <link href="http://arxiv.org/abs/2107.07869"/>
        <updated>2021-07-19T00:49:07.212Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an overview of Nearest neighbor (NN) methods, which
are frequently employed for solving classification problems using supervised
learning. The article concisely introduces the theoretical background,
algorithmic, and implementation aspects along with the key applications. From
an application standpoint, this article explores the challenges related to the
5G and beyond wireless networks which can be solved using NN classification
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1"&gt;Syed Ali Raza Zaidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite Basis Physics-Informed Neural Networks (FBPINNs): a scalable domain decomposition approach for solving differential equations. (arXiv:2107.07871v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2107.07871</id>
        <link href="http://arxiv.org/abs/2107.07871"/>
        <updated>2021-07-19T00:49:07.205Z</updated>
        <summary type="html"><![CDATA[Recently, physics-informed neural networks (PINNs) have offered a powerful
new paradigm for solving problems relating to differential equations. Compared
to classical numerical methods PINNs have several advantages, for example their
ability to provide mesh-free solutions of differential equations and their
ability to carry out forward and inverse modelling within the same optimisation
problem. Whilst promising, a key limitation to date is that PINNs have
struggled to accurately and efficiently solve problems with large domains
and/or multi-scale solutions, which is crucial for their real-world
application. Multiple significant and related factors contribute to this issue,
including the increasing complexity of the underlying PINN optimisation problem
as the problem size grows and the spectral bias of neural networks. In this
work we propose a new, scalable approach for solving large problems relating to
differential equations called Finite Basis PINNs (FBPINNs). FBPINNs are
inspired by classical finite element methods, where the solution of the
differential equation is expressed as the sum of a finite set of basis
functions with compact support. In FBPINNs neural networks are used to learn
these basis functions, which are defined over small, overlapping subdomains.
FBINNs are designed to address the spectral bias of neural networks by using
separate input normalisation over each subdomain, and reduce the complexity of
the underlying optimisation problem by using many smaller neural networks in a
parallel divide-and-conquer approach. Our numerical experiments show that
FBPINNs are effective in solving both small and larger, multi-scale problems,
outperforming standard PINNs in both accuracy and computational resources
required, potentially paving the way to the application of PINNs on large,
real-world problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Moseley_B/0/1/0/all/0/1"&gt;Ben Moseley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Markham_A/0/1/0/all/0/1"&gt;Andrew Markham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Nissen_Meyer_T/0/1/0/all/0/1"&gt;Tarje Nissen-Meyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive first-order methods revisited: Convex optimization without Lipschitz requirements. (arXiv:2107.08011v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.08011</id>
        <link href="http://arxiv.org/abs/2107.08011"/>
        <updated>2021-07-19T00:49:07.198Z</updated>
        <summary type="html"><![CDATA[We propose a new family of adaptive first-order methods for a class of convex
minimization problems that may fail to be Lipschitz continuous or smooth in the
standard sense. Specifically, motivated by a recent flurry of activity on
non-Lipschitz (NoLips) optimization, we consider problems that are continuous
or smooth relative to a reference Bregman function - as opposed to a global,
ambient norm (Euclidean or otherwise). These conditions encompass a wide range
of problems with singular objectives, such as Fisher markets, Poisson
tomography, D-design, and the like. In this setting, the application of
existing order-optimal adaptive methods - like UnixGrad or AcceleGrad - is not
possible, especially in the presence of randomness and uncertainty. The
proposed method - which we call adaptive mirror descent (AdaMir) - aims to
close this gap by concurrently achieving min-max optimal rates in problems that
are relatively continuous or smooth, including stochastic ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Antonakopoulos_K/0/1/0/all/0/1"&gt;Kimon Antonakopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mertikopoulos_P/0/1/0/all/0/1"&gt;Panayotis Mertikopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single Pass Entrywise-Transformed Low Rank Approximation. (arXiv:2107.07889v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.07889</id>
        <link href="http://arxiv.org/abs/2107.07889"/>
        <updated>2021-07-19T00:49:07.186Z</updated>
        <summary type="html"><![CDATA[In applications such as natural language processing or computer vision, one
is given a large $n \times d$ matrix $A = (a_{i,j})$ and would like to compute
a matrix decomposition, e.g., a low rank approximation, of a function $f(A) =
(f(a_{i,j}))$ applied entrywise to $A$. A very important special case is the
likelihood function $f\left( A \right ) = \log{\left( \left| a_{ij}\right|
+1\right)}$. A natural way to do this would be to simply apply $f$ to each
entry of $A$, and then compute the matrix decomposition, but this requires
storing all of $A$ as well as multiple passes over its entries. Recent work of
Liang et al.\ shows how to find a rank-$k$ factorization to $f(A)$ for an $n
\times n$ matrix $A$ using only $n \cdot \operatorname{poly}(\epsilon^{-1}k\log
n)$ words of memory, with overall error $10\|f(A)-[f(A)]_k\|_F^2 +
\operatorname{poly}(\epsilon/k) \|f(A)\|_{1,2}^2$, where $[f(A)]_k$ is the best
rank-$k$ approximation to $f(A)$ and $\|f(A)\|_{1,2}^2$ is the square of the
sum of Euclidean lengths of rows of $f(A)$. Their algorithm uses three passes
over the entries of $A$. The authors pose the open question of obtaining an
algorithm with $n \cdot \operatorname{poly}(\epsilon^{-1}k\log n)$ words of
memory using only a single pass over the entries of $A$. In this paper we
resolve this open question, obtaining the first single-pass algorithm for this
problem and for the same class of functions $f$ studied by Liang et al.
Moreover, our error is $\|f(A)-[f(A)]_k\|_F^2 + \operatorname{poly}(\epsilon/k)
\|f(A)\|_F^2$, where $\|f(A)\|_F^2$ is the sum of squares of Euclidean lengths
of rows of $f(A)$. Thus our error is significantly smaller, as it removes the
factor of $10$ and also $\|f(A)\|_F^2 \leq \|f(A)\|_{1,2}^2$. We also give an
algorithm for regression, pointing out an error in previous work, and
empirically validate our results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yifei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yiming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting IoT Device Identification. (arXiv:2107.07818v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.07818</id>
        <link href="http://arxiv.org/abs/2107.07818"/>
        <updated>2021-07-19T00:49:07.140Z</updated>
        <summary type="html"><![CDATA[Internet-of-Things (IoT) devices are known to be the source of many security
problems, and as such, they would greatly benefit from automated management.
This requires robustly identifying devices so that appropriate network security
policies can be applied. We address this challenge by exploring how to
accurately identify IoT devices based on their network behavior, while
leveraging approaches previously proposed by other researchers.

We compare the accuracy of four different previously proposed machine
learning models (tree-based and neural network-based) for identifying IoT
devices. We use packet trace data collected over a period of six months from a
large IoT test-bed. We show that, while all models achieve high accuracy when
evaluated on the same dataset as they were trained on, their accuracy degrades
over time, when evaluated on data collected outside the training set. We show
that on average the models' accuracy degrades after a couple of weeks by up to
40 percentage points (on average between 12 and 21 percentage points). We argue
that, in order to keep the models' accuracy at a high level, these need to be
continuously updated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kolcun_R/0/1/0/all/0/1"&gt;Roman Kolcun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popescu_D/0/1/0/all/0/1"&gt;Diana Andreea Popescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Safronov_V/0/1/0/all/0/1"&gt;Vadim Safronov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1"&gt;Poonam Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandalari_A/0/1/0/all/0/1"&gt;Anna Maria Mandalari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mortier_R/0/1/0/all/0/1"&gt;Richard Mortier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1"&gt;Hamed Haddadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Versatile modular neural locomotion control with fast learning. (arXiv:2107.07844v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.07844</id>
        <link href="http://arxiv.org/abs/2107.07844"/>
        <updated>2021-07-19T00:49:07.134Z</updated>
        <summary type="html"><![CDATA[Legged robots have significant potential to operate in highly unstructured
environments. The design of locomotion control is, however, still challenging.
Currently, controllers must be either manually designed for specific robots and
tasks, or automatically designed via machine learning methods that require long
training times and yield large opaque controllers. Drawing inspiration from
animal locomotion, we propose a simple yet versatile modular neural control
structure with fast learning. The key advantages of our approach are that
behavior-specific control modules can be added incrementally to obtain
increasingly complex emergent locomotion behaviors, and that neural connections
interfacing with existing modules can be quickly and automatically learned. In
a series of experiments, we show how eight modules can be quickly learned and
added to a base control module to obtain emergent adaptive behaviors allowing a
hexapod robot to navigate in complex environments. We also show that modules
can be added and removed during operation without affecting the functionality
of the remaining controller. Finally, the control approach was successfully
demonstrated on a physical hexapod robot. Taken together, our study reveals a
significant step towards fast automatic design of versatile neural locomotion
control for complex robotic systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thor_M/0/1/0/all/0/1"&gt;Mathias Thor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manoonpong_P/0/1/0/all/0/1"&gt;Poramate Manoonpong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Penalized Shared-parameter Algorithm for Estimating Optimal Dynamic Treatment Regimens. (arXiv:2107.07875v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07875</id>
        <link href="http://arxiv.org/abs/2107.07875"/>
        <updated>2021-07-19T00:49:07.126Z</updated>
        <summary type="html"><![CDATA[A dynamic treatment regimen (DTR) is a set of decision rules to personalize
treatments for an individual using their medical history. The Q-learning based
Q-shared algorithm has been used to develop DTRs that involve decision rules
shared across multiple stages of intervention. We show that the existing
Q-shared algorithm can suffer from non-convergence due to the use of linear
models in the Q-learning setup, and identify the condition in which Q-shared
fails. Leveraging properties from expansion-constrained ordinary least-squares,
we give a penalized Q-shared algorithm that not only converges in settings that
violate the condition, but can outperform the original Q-shared algorithm even
when the condition is satisfied. We give evidence for the proposed method in a
real-world application and several synthetic simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nalamada_T/0/1/0/all/0/1"&gt;Trikay Nalamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Shruti Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jahja_M/0/1/0/all/0/1"&gt;Maria Jahja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chakraborty_B/0/1/0/all/0/1"&gt;Bibhas Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghosh_P/0/1/0/all/0/1"&gt;Palash Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Causal Perspective on Meaningful and Robust Algorithmic Recourse. (arXiv:2107.07853v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07853</id>
        <link href="http://arxiv.org/abs/2107.07853"/>
        <updated>2021-07-19T00:49:07.073Z</updated>
        <summary type="html"><![CDATA[Algorithmic recourse explanations inform stakeholders on how to act to revert
unfavorable predictions. However, in general ML models do not predict well in
interventional distributions. Thus, an action that changes the prediction in
the desired way may not lead to an improvement of the underlying target. Such
recourse is neither meaningful nor robust to model refits. Extending the work
of Karimi et al. (2021), we propose meaningful algorithmic recourse (MAR) that
only recommends actions that improve both prediction and target. We justify
this selection constraint by highlighting the differences between model audit
and meaningful, actionable recourse explanations. Additionally, we introduce a
relaxation of MAR called effective algorithmic recourse (EAR), which, under
certain assumptions, yields meaningful recourse by only allowing interventions
on causes of the target.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Konig_G/0/1/0/all/0/1"&gt;Gunnar K&amp;#xf6;nig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Freiesleben_T/0/1/0/all/0/1"&gt;Timo Freiesleben&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Grosse_Wentrup_M/0/1/0/all/0/1"&gt;Moritz Grosse-Wentrup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Predictive Coding for Anomaly Detection. (arXiv:2107.07820v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07820</id>
        <link href="http://arxiv.org/abs/2107.07820"/>
        <updated>2021-07-19T00:49:07.065Z</updated>
        <summary type="html"><![CDATA[Reliable detection of anomalies is crucial when deploying machine learning
models in practice, but remains challenging due to the lack of labeled data. To
tackle this challenge, contrastive learning approaches are becoming
increasingly popular, given the impressive results they have achieved in
self-supervised representation learning settings. However, while most existing
contrastive anomaly detection and segmentation approaches have been applied to
images, none of them can use the contrastive losses directly for both anomaly
detection and segmentation. In this paper, we close this gap by making use of
the Contrastive Predictive Coding model (arXiv:1807.03748). We show that its
patch-wise contrastive loss can directly be interpreted as an anomaly score,
and how this allows for the creation of anomaly segmentation masks. The
resulting model achieves promising results for both anomaly detection and
segmentation on the challenging MVTec-AD dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1"&gt;Puck de Haan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1"&gt;Sindy L&amp;#xf6;we&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07791</id>
        <link href="http://arxiv.org/abs/2107.07791"/>
        <updated>2021-07-19T00:49:07.046Z</updated>
        <summary type="html"><![CDATA[We present a novel learning-based approach to graph representations of road
networks employing state-of-the-art graph convolutional neural networks. Our
approach is applied to realistic road networks of 17 cities from Open Street
Map. While edge features are crucial to generate descriptive graph
representations of road networks, graph convolutional networks usually rely on
node features only. We show that the highly representative edge features can
still be integrated into such networks by applying a line graph transformation.
We also propose a method for neighborhood sampling based on a topological
neighborhood composed of both local and global neighbors. We compare the
performance of learning representations using different types of neighborhood
aggregation functions in transductive and inductive tasks and in supervised and
unsupervised learning. Furthermore, we propose a novel aggregation approach,
Graph Attention Isomorphism Network, GAIN. Our results show that GAIN
outperforms state-of-the-art methods on the road type classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1"&gt;Zahra Gharaee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1"&gt;Shreyas Kowshik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1"&gt;Oliver Stromann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1"&gt;Michael Felsberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EGC2: Enhanced Graph Classification with Easy Graph Compression. (arXiv:2107.07737v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07737</id>
        <link href="http://arxiv.org/abs/2107.07737"/>
        <updated>2021-07-19T00:49:07.040Z</updated>
        <summary type="html"><![CDATA[Graph classification plays a significant role in network analysis. It also
faces potential security threat like adversarial attacks. Some defense methods
may sacrifice algorithm complexity for robustness like adversarial training,
while others may sacrifice the clean example performance such as
smoothing-based defense. Most of them are suffered from high-complexity or less
transferability. To address this problem, we proposed EGC$^2$, an enhanced
graph classification model with easy graph compression. EGC$^2$ captures the
relationship between features of different nodes by constructing feature graphs
and improving aggregate node-level representation. To achieve lower complexity
defense applied to various graph classification models, EGC$^2$ utilizes a
centrality-based edge importance index to compress graphs, filtering out
trivial structures and even adversarial perturbations of the input graphs, thus
improves its robustness. Experiments on seven benchmark datasets demonstrate
that the proposed feature read-out and graph compression mechanisms enhance the
robustness of various basic models, thus achieving the state-of-the-art
performance of accuracy and robustness in the threat of different adversarial
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jinyin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dunjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1"&gt;Zhaoyan Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1"&gt;Mingwei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yi Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Learning for Marked Temporal Point Processes. (arXiv:2107.07729v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07729</id>
        <link href="http://arxiv.org/abs/2107.07729"/>
        <updated>2021-07-19T00:49:07.033Z</updated>
        <summary type="html"><![CDATA[Temporal Point Processes (TPPs) are often used to represent the sequence of
events ordered as per the time of occurrence. Owing to their flexible nature,
TPPs have been used to model different scenarios and have shown applicability
in various real-world applications. While TPPs focus on modeling the event
occurrence, Marked Temporal Point Process (MTPP) focuses on modeling the
category/class of the event as well (termed as the marker). Research in MTPP
has garnered substantial attention over the past few years, with an extensive
focus on supervised algorithms. Despite the research focus, limited attention
has been given to the challenging problem of developing solutions in
semi-supervised settings, where algorithms have access to a mix of labeled and
unlabeled data. This research proposes a novel algorithm for Semi-supervised
Learning for Marked Temporal Point Processes (SSL-MTPP) applicable in such
scenarios. The proposed SSL-MTPP algorithm utilizes a combination of labeled
and unlabeled data for learning a robust marker prediction model. The proposed
algorithm utilizes an RNN-based Encoder-Decoder module for learning effective
representations of the time sequence. The efficacy of the proposed algorithm
has been demonstrated via multiple protocols on the Retweet dataset, where the
proposed SSL-MTPP demonstrates improved performance in comparison to the
traditional supervised learning approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1"&gt;Shivshankar Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_A/0/1/0/all/0/1"&gt;Anand Vir Singh Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Maneet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Karamjit Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recognizing bird species in diverse soundscapes under weak supervision. (arXiv:2107.07728v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.07728</id>
        <link href="http://arxiv.org/abs/2107.07728"/>
        <updated>2021-07-19T00:49:07.026Z</updated>
        <summary type="html"><![CDATA[We present a robust classification approach for avian vocalization in complex
and diverse soundscapes, achieving second place in the BirdCLEF2021 challenge.
We illustrate how to make full use of pre-trained convolutional neural
networks, by using an efficient modeling and training routine supplemented by
novel augmentation methods. Thereby, we improve the generalization of weakly
labeled crowd-sourced data to productive data collected by autonomous recording
units. As such, we illustrate how to progress towards an accurate automated
assessment of avian population which would enable global biodiversity
monitoring at scale, impossible by manual annotation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henkel_C/0/1/0/all/0/1"&gt;Christof Henkel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_P/0/1/0/all/0/1"&gt;Pascal Pfeiffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singer_P/0/1/0/all/0/1"&gt;Philipp Singer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-labelling Enhanced Media Bias Detection. (arXiv:2107.07705v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07705</id>
        <link href="http://arxiv.org/abs/2107.07705"/>
        <updated>2021-07-19T00:49:07.019Z</updated>
        <summary type="html"><![CDATA[Leveraging unlabelled data through weak or distant supervision is a
compelling approach to developing more effective text classification models.
This paper proposes a simple but effective data augmentation method, which
leverages the idea of pseudo-labelling to select samples from noisy distant
supervision annotation datasets. The result shows that the proposed method
improves the accuracy of biased news detection models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1"&gt;Qin Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1"&gt;Brian Mac Namee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1"&gt;Ruihai Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ScRAE: Deterministic Regularized Autoencoders with Flexible Priors for Clustering Single-cell Gene Expression Data. (arXiv:2107.07709v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07709</id>
        <link href="http://arxiv.org/abs/2107.07709"/>
        <updated>2021-07-19T00:49:07.002Z</updated>
        <summary type="html"><![CDATA[Clustering single-cell RNA sequence (scRNA-seq) data poses statistical and
computational challenges due to their high-dimensionality and data-sparsity,
also known as `dropout' events. Recently, Regularized Auto-Encoder (RAE) based
deep neural network models have achieved remarkable success in learning robust
low-dimensional representations. The basic idea in RAEs is to learn a
non-linear mapping from the high-dimensional data space to a low-dimensional
latent space and vice-versa, simultaneously imposing a distributional prior on
the latent space, which brings in a regularization effect. This paper argues
that RAEs suffer from the infamous problem of bias-variance trade-off in their
naive formulation. While a simple AE without a latent regularization results in
data over-fitting, a very strong prior leads to under-representation and thus
bad clustering. To address the above issues, we propose a modified RAE
framework (called the scRAE) for effective clustering of the single-cell RNA
sequencing data. scRAE consists of deterministic AE with a flexibly learnable
prior generator network, which is jointly trained with the AE. This facilitates
scRAE to trade-off better between the bias and variance in the latent space. We
demonstrate the efficacy of the proposed method through extensive
experimentation on several real-world single-cell Gene expression datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1"&gt;Arnab Kumar Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asnani_H/0/1/0/all/0/1"&gt;Himanshu Asnani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1"&gt;Parag Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1"&gt;Prathosh AP&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards an Interpretable Latent Space in Structured Models for Video Prediction. (arXiv:2107.07713v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07713</id>
        <link href="http://arxiv.org/abs/2107.07713"/>
        <updated>2021-07-19T00:49:06.996Z</updated>
        <summary type="html"><![CDATA[We focus on the task of future frame prediction in video governed by
underlying physical dynamics. We work with models which are object-centric,
i.e., explicitly work with object representations, and propagate a loss in the
latent space. Specifically, our research builds on recent work by Kipf et al.
\cite{kipf&al20}, which predicts the next state via contrastive learning of
object interactions in a latent space using a Graph Neural Network. We argue
that injecting explicit inductive bias in the model, in form of general
physical laws, can help not only make the model more interpretable, but also
improve the overall prediction of model. As a natural by-product, our model can
learn feature maps which closely resemble actual object positions in the image,
without having any explicit supervision about the object positions at the
training time. In comparison with earlier works \cite{jaques&al20}, which
assume a complete knowledge of the dynamics governing the motion in the form of
a physics engine, we rely only on the knowledge of general physical laws, such
as, world consists of objects, which have position and velocity. We propose an
additional decoder based loss in the pixel space, imposed in a curriculum
manner, to further refine the latent space predictions. Experiments in multiple
different settings demonstrate that while Kipf et al. model is effective at
capturing object interactions, our model can be significantly more effective at
localising objects, resulting in improved performance in 3 out of 4 domains
that we experiment with. Additionally, our model can learn highly intrepretable
feature maps, resembling actual object positions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1"&gt;Rushil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1"&gt;Vishal Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_Y/0/1/0/all/0/1"&gt;Yash Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yitao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1"&gt;Guy Van den Broeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1"&gt;Parag Singla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Online Control with Model Misspecification. (arXiv:2107.07732v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.07732</id>
        <link href="http://arxiv.org/abs/2107.07732"/>
        <updated>2021-07-19T00:49:06.989Z</updated>
        <summary type="html"><![CDATA[We study online control of an unknown nonlinear dynamical system that is
approximated by a time-invariant linear system with model misspecification. Our
study focuses on robustness, which measures how much deviation from the assumed
linear approximation can be tolerated while maintaining a bounded $\ell_2$-gain
compared to the optimal control in hindsight. Some models cannot be stabilized
even with perfect knowledge of their coefficients: the robustness is limited by
the minimal distance between the assumed dynamics and the set of unstabilizable
dynamics. Therefore it is necessary to assume a lower bound on this distance.
Under this assumption, and with full observation of the $d$ dimensional state,
we describe an efficient controller that attains $\Omega(\frac{1}{\sqrt{d}})$
robustness together with an $\ell_2$-gain whose dimension dependence is near
optimal. We also give an inefficient algorithm that attains constant robustness
independent of the dimension, with a finite but sub-optimal $\ell_2$-gain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ghai_U/0/1/0/all/0/1"&gt;Udaya Ghai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hazan_E/0/1/0/all/0/1"&gt;Elad Hazan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Megretski_A/0/1/0/all/0/1"&gt;Alexandre Megretski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeXtQSM -- A complete deep learning pipeline for data-consistent quantitative susceptibility mapping trained with hybrid data. (arXiv:2107.07752v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07752</id>
        <link href="http://arxiv.org/abs/2107.07752"/>
        <updated>2021-07-19T00:49:06.983Z</updated>
        <summary type="html"><![CDATA[Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great
potential in recent years, outperforming traditional non-learning approaches in
speed and accuracy. However, many of the current deep learning approaches are
not data consistent, require in vivo training data or do not solve all steps of
the QSM processing pipeline. Here we aim to overcome these limitations and
developed a framework to solve the QSM processing steps jointly. We developed a
new hybrid training data generation method that enables the end-to-end training
for solving background field correction and dipole inversion in a
data-consistent fashion using a variational network that combines the QSM model
term and a learned regularizer. We demonstrate that NeXtQSM overcomes the
limitations of previous model-agnostic deep learning methods and show that
NeXtQSM offers a complete deep learning based pipeline for computing robust,
fast and accurate quantitative susceptibility maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cognolato_F/0/1/0/all/0/1"&gt;Francesco Cognolato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+OBrien_K/0/1/0/all/0/1"&gt;Kieran O&amp;#x27;Brien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jin_J/0/1/0/all/0/1"&gt;Jin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Robinson_S/0/1/0/all/0/1"&gt;Simon Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Laun_F/0/1/0/all/0/1"&gt;Frederik B. Laun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barth_M/0/1/0/all/0/1"&gt;Markus Barth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bollmann_S/0/1/0/all/0/1"&gt;Steffen Bollmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Optimal Stationary Control of Linear Stochastic Systems. (arXiv:2107.07788v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2107.07788</id>
        <link href="http://arxiv.org/abs/2107.07788"/>
        <updated>2021-07-19T00:49:06.976Z</updated>
        <summary type="html"><![CDATA[This paper studies the optimal stationary control of continuous-time linear
stochastic systems with both additive and multiplicative noises, using
reinforcement learning techniques. Based on policy iteration, a novel
off-policy reinforcement learning algorithm, named optimistic
least-squares-based policy iteration, is proposed which is able to iteratively
find near-optimal policies of the optimal stationary control problem directly
from input/state data without explicitly identifying any system matrices,
starting from an initial admissible control policy. The solutions given by the
proposed optimistic least-squares-based policy iteration are proved to converge
to a small neighborhood of the optimal solution with probability one, under
mild conditions. The application of the proposed algorithm to a triple inverted
pendulum example validates its feasibility and effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pang_B/0/1/0/all/0/1"&gt;Bo Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhong-Ping Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of Blood Lactate Values in Critically Ill Patients: A Retrospective Multi-center Cohort Study. (arXiv:2107.07582v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2107.07582</id>
        <link href="http://arxiv.org/abs/2107.07582"/>
        <updated>2021-07-19T00:49:06.959Z</updated>
        <summary type="html"><![CDATA[Purpose. Elevations in initially obtained serum lactate levels are strong
predictors of mortality in critically ill patients. Identifying patients whose
serum lactate levels are more likely to increase can alert physicians to
intensify care and guide them in the frequency of tending the blood test. We
investigate whether machine learning models can predict subsequent serum
lactate changes.

Methods. We investigated serum lactate change prediction using the MIMIC-III
and eICU-CRD datasets in internal as well as external validation of the eICU
cohort on the MIMIC-III cohort. Three subgroups were defined based on the
initial lactate levels: i) normal group (<2 mmol/L), ii) mild group (2-4
mmol/L), and iii) severe group (>4 mmol/L). Outcomes were defined based on
increase or decrease of serum lactate levels between the groups. We also
performed sensitivity analysis by defining the outcome as lactate change of
>10% and furthermore investigated the influence of the time interval between
subsequent lactate measurements on predictive performance.

Results. The LSTM models were able to predict deterioration of serum lactate
values of MIMIC-III patients with an AUC of 0.77 (95% CI 0.762-0.771) for the
normal group, 0.77 (95% CI 0.768-0.772) for the mild group, and 0.85 (95% CI
0.840-0.851) for the severe group, with a slightly lower performance in the
external validation.

Conclusion. The LSTM demonstrated good discrimination of patients who had
deterioration in serum lactate levels. Clinical studies are needed to evaluate
whether utilization of a clinical decision support tool based on these results
could positively impact decision-making and patient outcomes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Mamandipoor_B/0/1/0/all/0/1"&gt;Behrooz Mamandipoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Yeung_W/0/1/0/all/0/1"&gt;Wesley Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Agha_Mir_Salim_L/0/1/0/all/0/1"&gt;Louis Agha-Mir-Salim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Stone_D/0/1/0/all/0/1"&gt;David J. Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Osmani_V/0/1/0/all/0/1"&gt;Venet Osmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Celi_L/0/1/0/all/0/1"&gt;Leo Anthony Celi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Fairness in Generative Models. (arXiv:2107.07754v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07754</id>
        <link href="http://arxiv.org/abs/2107.07754"/>
        <updated>2021-07-19T00:49:06.952Z</updated>
        <summary type="html"><![CDATA[Deep generative models have made much progress in improving training
stability and quality of generated data. Recently there has been increased
interest in the fairness of deep-generated data. Fairness is important in many
applications, e.g. law enforcement, as biases will affect efficacy. Central to
fair data generation are the fairness metrics for the assessment and evaluation
of different generative models. In this paper, we first review fairness metrics
proposed in previous works and highlight potential weaknesses. We then discuss
a performance benchmark framework along with the assessment of alternative
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1"&gt;Christopher T.H Teo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1"&gt;Ngai-Man Cheung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometric Value Iteration: Dynamic Error-Aware KL Regularization for Reinforcement Learning. (arXiv:2107.07659v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07659</id>
        <link href="http://arxiv.org/abs/2107.07659"/>
        <updated>2021-07-19T00:49:06.946Z</updated>
        <summary type="html"><![CDATA[The recent booming of entropy-regularized literature reveals that
Kullback-Leibler (KL) regularization brings advantages to Reinforcement
Learning (RL) algorithms by canceling out errors under mild assumptions.
However, existing analyses focus on fixed regularization with a constant
weighting coefficient and have not considered the case where the coefficient is
allowed to change dynamically. In this paper, we study the dynamic coefficient
scheme and present the first asymptotic error bound. Based on the dynamic
coefficient error bound, we propose an effective scheme to tune the coefficient
according to the magnitude of error in favor of more robust learning. On top of
this development, we propose a novel algorithm: Geometric Value Iteration (GVI)
that features a dynamic error-aware KL coefficient design aiming to mitigate
the impact of errors on the performance. Our experiments demonstrate that GVI
can effectively exploit the trade-off between learning speed and robustness
over uniform averaging of constant KL coefficient. The combination of GVI and
deep networks shows stable learning behavior even in the absence of a target
network where algorithms with a constant KL coefficient would greatly oscillate
or even fail to converge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kitamura_T/0/1/0/all/0/1"&gt;Toshinori Kitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingwei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1"&gt;Takamitsu Matsubara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active learning for online training in imbalanced data streams under cold start. (arXiv:2107.07724v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07724</id>
        <link href="http://arxiv.org/abs/2107.07724"/>
        <updated>2021-07-19T00:49:06.939Z</updated>
        <summary type="html"><![CDATA[Labeled data is essential in modern systems that rely on Machine Learning
(ML) for predictive modelling. Such systems may suffer from the cold-start
problem: supervised models work well but, initially, there are no labels, which
are costly or slow to obtain. This problem is even worse in imbalanced data
scenarios. Online financial fraud detection is an example where labeling is: i)
expensive, or ii) it suffers from long delays, if relying on victims filing
complaints. The latter may not be viable if a model has to be in place
immediately, so an option is to ask analysts to label events while minimizing
the number of annotations to control costs. We propose an Active Learning (AL)
annotation system for datasets with orders of magnitude of class imbalance, in
a cold start streaming scenario. We present a computationally efficient
Outlier-based Discriminative AL approach (ODAL) and design a novel 3-stage
sequence of AL labeling policies where it is used as warm-up. Then, we perform
empirical studies in four real world datasets, with various magnitudes of class
imbalance. The results show that our method can more quickly reach a high
performance model than standard AL policies. Its observed gains over random
sampling can reach 80% and be competitive with policies with an unlimited
annotation budget or additional historical data (with 1/10 to 1/50 of the
labels).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barata_R/0/1/0/all/0/1"&gt;Ricardo Barata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leite_M/0/1/0/all/0/1"&gt;Miguel Leite&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacheco_R/0/1/0/all/0/1"&gt;Ricardo Pacheco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sampaio_M/0/1/0/all/0/1"&gt;Marco O. P. Sampaio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ascensao_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Tiago Ascens&amp;#xe3;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1"&gt;Pedro Bizarro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MS-MDA: Multisource Marginal Distribution Adaptation for Cross-subject and Cross-session EEG Emotion Recognition. (arXiv:2107.07740v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07740</id>
        <link href="http://arxiv.org/abs/2107.07740"/>
        <updated>2021-07-19T00:49:06.933Z</updated>
        <summary type="html"><![CDATA[As an essential element for the diagnosis and rehabilitation of psychiatric
disorders, the electroencephalogram (EEG) based emotion recognition has
achieved significant progress due to its high precision and reliability.
However, one obstacle to practicality lies in the variability between subjects
and sessions. Although several studies have adopted domain adaptation (DA)
approaches to tackle this problem, most of them treat multiple EEG data from
different subjects and sessions together as a single source domain for
transfer, which either fails to satisfy the assumption of domain adaptation
that the source has a certain marginal distribution, or increases the
difficulty of adaptation. We therefore propose the multi-source marginal
distribution adaptation (MS-MDA) for EEG emotion recognition, which takes both
domain-invariant and domain-specific features into consideration. First, we
assume that different EEG data share the same low-level features, then we
construct independent branches for multiple EEG data source domains to adopt
one-to-one domain adaptation and extract domain-specific features. Finally, the
inference is made by multiple branches. We evaluate our method on SEED and
SEED-IV for recognizing three and four emotions, respectively. Experimental
results show that the MS-MDA outperforms the comparison methods and
state-of-the-art models in cross-session and cross-subject transfer scenarios
in our settings. Codes at https://github.com/VoiceBeer/MS-MDA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1"&gt;Ming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhunan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Cunhang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinpeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Huiguang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When does loss-based prioritization fail?. (arXiv:2107.07741v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07741</id>
        <link href="http://arxiv.org/abs/2107.07741"/>
        <updated>2021-07-19T00:49:06.895Z</updated>
        <summary type="html"><![CDATA[Not all examples are created equal, but standard deep neural network training
protocols treat each training point uniformly. Each example is propagated
forward and backward through the network the same amount of times, independent
of how much the example contributes to the learning protocol. Recent work has
proposed ways to accelerate training by deviating from this uniform treatment.
Popular methods entail up-weighting examples that contribute more to the loss
with the intuition that examples with low loss have already been learned by the
model, so their marginal value to the training procedure should be lower. This
view assumes that updating the model with high loss examples will be beneficial
to the model. However, this may not hold for noisy, real world data. In this
paper, we theorize and then empirically demonstrate that loss-based
acceleration methods degrade in scenarios with noisy and corrupted data. Our
work suggests measures of example difficulty need to correctly separate out
noise from other types of challenging examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1"&gt;Niel Teng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xinyu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Rosanne Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1"&gt;Sara Hooker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yosinski_J/0/1/0/all/0/1"&gt;Jason Yosinski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-preserving Spatiotemporal Scenario Generation of Renewable Energies: A Federated Deep Generative Learning Approach. (arXiv:2107.07738v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07738</id>
        <link href="http://arxiv.org/abs/2107.07738"/>
        <updated>2021-07-19T00:49:06.886Z</updated>
        <summary type="html"><![CDATA[Scenario generation is a fundamental and crucial tool for decision-making in
power systems with high-penetration renewables. Based on big historical data, a
novel federated deep generative learning framework, called Fed-LSGAN, is
proposed by integrating federated learning and least square generative
adversarial networks (LSGANs) for renewable scenario generation. Specifically,
federated learning learns a shared global model in a central server from
renewable sites at network edges, which enables the Fed-LSGAN to generate
scenarios in a privacy-preserving manner without sacrificing the generation
quality by transferring model parameters, rather than all data. Meanwhile, the
LSGANs-based deep generative model generates scenarios that conform to the
distribution of historical data through fully capturing the spatial-temporal
characteristics of renewable powers, which leverages the least squares loss
function to improve the training stability and generation quality. The
simulation results demonstrate that the proposal manages to generate
high-quality renewable scenarios and outperforms the state-of-the-art
centralized methods. Besides, an experiment with different federated learning
settings is designed and conducted to verify the robustness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiazheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-differentiable Ensemble Kalman Filters. (arXiv:2107.07687v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07687</id>
        <link href="http://arxiv.org/abs/2107.07687"/>
        <updated>2021-07-19T00:49:06.870Z</updated>
        <summary type="html"><![CDATA[Data assimilation is concerned with sequentially estimating a
temporally-evolving state. This task, which arises in a wide range of
scientific and engineering applications, is particularly challenging when the
state is high-dimensional and the state-space dynamics are unknown. This paper
introduces a machine learning framework for learning dynamical systems in data
assimilation. Our auto-differentiable ensemble Kalman filters (AD-EnKFs) blend
ensemble Kalman filters for state recovery with machine learning tools for
learning the dynamics. In doing so, AD-EnKFs leverage the ability of ensemble
Kalman filters to scale to high-dimensional states and the power of automatic
differentiation to train high-dimensional surrogate models for the dynamics.
Numerical results using the Lorenz-96 model show that AD-EnKFs outperform
existing methods that use expectation-maximization or particle filters to merge
data assimilation and machine learning. In addition, AD-EnKFs are easy to
implement and require minimal tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sanz_Alonso_D/0/1/0/all/0/1"&gt;Daniel Sanz-Alonso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Willett_R/0/1/0/all/0/1"&gt;Rebecca Willett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic insights on continual learning from fruit flies. (arXiv:2107.07617v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07617</id>
        <link href="http://arxiv.org/abs/2107.07617"/>
        <updated>2021-07-19T00:49:06.824Z</updated>
        <summary type="html"><![CDATA[Continual learning in computational systems is challenging due to
catastrophic forgetting. We discovered a two layer neural circuit in the fruit
fly olfactory system that addresses this challenge by uniquely combining sparse
coding and associative learning. In the first layer, odors are encoded using
sparse, high dimensional representations, which reduces memory interference by
activating non overlapping populations of neurons for different odors. In the
second layer, only the synapses between odor activated neurons and the output
neuron associated with the odor are modified during learning; the rest of the
weights are frozen to prevent unrelated memories from being overwritten. We
show empirically and analytically that this simple and lightweight algorithm
significantly boosts continual learning performance. The fly associative
learning algorithm is strikingly similar to the classic perceptron learning
algorithm, albeit two modifications, which we show are critical for reducing
catastrophic forgetting. Overall, fruit flies evolved an efficient lifelong
learning algorithm, and circuit mechanisms from neuroscience can be translated
to improve machine computation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1"&gt;Sanjoy Dasgupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navlakha_S/0/1/0/all/0/1"&gt;Saket Navlakha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECG-Adv-GAN: Detecting ECG Adversarial Examples with Conditional Generative Adversarial Networks. (arXiv:2107.07677v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07677</id>
        <link href="http://arxiv.org/abs/2107.07677"/>
        <updated>2021-07-19T00:49:06.794Z</updated>
        <summary type="html"><![CDATA[Electrocardiogram (ECG) acquisition requires an automated system and analysis
pipeline for understanding specific rhythm irregularities. Deep neural networks
have become a popular technique for tracing ECG signals, outperforming human
experts. Despite this, convolutional neural networks are susceptible to
adversarial examples that can misclassify ECG signals and decrease the model's
precision. Moreover, they do not generalize well on the out-of-distribution
dataset. The GAN architecture has been employed in recent works to synthesize
adversarial ECG signals to increase existing training data. However, they use a
disjointed CNN-based classification architecture to detect arrhythmia. Till
now, no versatile architecture has been proposed that can detect adversarial
examples and classify arrhythmia simultaneously. To alleviate this, we propose
a novel Conditional Generative Adversarial Network to simultaneously generate
ECG signals for different categories and detect cardiac abnormalities.
Moreover, the model is conditioned on class-specific ECG signals to synthesize
realistic adversarial examples. Consequently, we compare our architecture and
show how it outperforms other classification models in normal/abnormal ECG
signal detection by benchmarking real world and adversarial signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1"&gt;Khondker Fariha Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1"&gt;Sharif Amit Kamran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tavakkoli_A/0/1/0/all/0/1"&gt;Alireza Tavakkoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1"&gt;Lei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1"&gt;Daniel Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajasegarar_S/0/1/0/all/0/1"&gt;Sutharshan Rajasegarar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karmaker_C/0/1/0/all/0/1"&gt;Chandan Karmaker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Face Recognition System for Remote Employee Tracking. (arXiv:2107.07576v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07576</id>
        <link href="http://arxiv.org/abs/2107.07576"/>
        <updated>2021-07-19T00:49:06.772Z</updated>
        <summary type="html"><![CDATA[During the COVID-19 pandemic, most of the human-to-human interactions have
been stopped. To mitigate the spread of deadly coronavirus, many offices took
the initiative so that the employees can work from home. But, tracking the
employees and finding out if they are really performing what they were supposed
to turn out to be a serious challenge for all the companies and organizations
who are facilitating "Work From Home". To deal with the challenge effectively,
we came up with a solution to track the employees with face recognition. We
have been testing this system experimentally for our office. To train the face
recognition module, we used FaceNet with KNN using the Labeled Faces in the
Wild (LFW) dataset and achieved 97.8% accuracy. We integrated the trained model
into our central system, where the employees log their time. In this paper, we
discuss in brief the system we have been experimenting with and the pros and
cons of the system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1"&gt;Mohammad Sabik Irbaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1"&gt;MD Abdullah Al Nasim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferdous_R/0/1/0/all/0/1"&gt;Refat E Ferdous&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Contextual Anomaly Detection for Time Series. (arXiv:2107.07702v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07702</id>
        <link href="http://arxiv.org/abs/2107.07702"/>
        <updated>2021-07-19T00:49:06.765Z</updated>
        <summary type="html"><![CDATA[We introduce Neural Contextual Anomaly Detection (NCAD), a framework for
anomaly detection on time series that scales seamlessly from the unsupervised
to supervised setting, and is applicable to both univariate and multivariate
time series. This is achieved by effectively combining recent developments in
representation learning for multivariate time series, with techniques for deep
anomaly detection originally developed for computer vision that we tailor to
the time series setting. Our window-based approach facilitates learning the
boundary between normal and anomalous classes by injecting generic synthetic
anomalies into the available data. Moreover, our method can effectively take
advantage of all the available information, be it as domain knowledge, or as
training labels in the semi-supervised setting. We demonstrate empirically on
standard benchmark datasets that our approach obtains a state-of-the-art
performance in these settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carmona_C/0/1/0/all/0/1"&gt;Chris U. Carmona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aubet_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois-Xavier Aubet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flunkert_V/0/1/0/all/0/1"&gt;Valentin Flunkert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasthaus_J/0/1/0/all/0/1"&gt;Jan Gasthaus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving application performance with biased distributions of quantum states. (arXiv:2107.07642v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.07642</id>
        <link href="http://arxiv.org/abs/2107.07642"/>
        <updated>2021-07-19T00:49:06.759Z</updated>
        <summary type="html"><![CDATA[We consider the properties of a specific distribution of mixed quantum states
of arbitrary dimension that can be biased towards a specific mean purity. In
particular, we analyze mixtures of Haar-random pure states with
Dirichlet-distributed coefficients. We analytically derive the concentration
parameters required to match the mean purity of the Bures and Hilbert--Schmidt
distributions in any dimension. Numerical simulations suggest that this value
recovers the Hilbert--Schmidt distribution exactly, offering an alternative and
intuitive physical interpretation for ensembles of Hilbert--Schmidt-distributed
random quantum states. We then demonstrate how substituting these
Dirichlet-weighted Haar mixtures in place of the Bures and Hilbert--Schmidt
distributions results in measurable performance advantages in
machine-learning-based quantum state tomography systems and Bayesian quantum
state reconstruction. Finally, we experimentally characterize the distribution
of quantum states generated by both a cloud-accessed IBM quantum computer and
an in-house source of polarization-entangled photons. In each case, our method
can more closely match the underlying distribution than either Bures or
Hilbert--Schmidt distributed states for various experimental conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lohani_S/0/1/0/all/0/1"&gt;Sanjaya Lohani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lukens_J/0/1/0/all/0/1"&gt;Joseph M. Lukens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jones_D/0/1/0/all/0/1"&gt;Daniel E. Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Searles_T/0/1/0/all/0/1"&gt;Thomas A. Searles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Glasser_R/0/1/0/all/0/1"&gt;Ryan T. Glasser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kirby_B/0/1/0/all/0/1"&gt;Brian T. Kirby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intersectional Bias in Causal Language Models. (arXiv:2107.07691v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07691</id>
        <link href="http://arxiv.org/abs/2107.07691"/>
        <updated>2021-07-19T00:49:06.746Z</updated>
        <summary type="html"><![CDATA[To examine whether intersectional bias can be observed in language
generation, we examine \emph{GPT-2} and \emph{GPT-NEO} models, ranging in size
from 124 million to ~2.7 billion parameters. We conduct an experiment combining
up to three social categories - gender, religion and disability - into
unconditional or zero-shot prompts used to generate sentences that are then
analysed for sentiment. Our results confirm earlier tests conducted with
auto-regressive causal models, including the \emph{GPT} family of models. We
also illustrate why bias may be resistant to techniques that target single
categories (e.g. gender, religion and race), as it can also manifest, in often
subtle ways, in texts prompted by concatenated social categories. To address
these difficulties, we suggest technical and community-based approaches need to
combine to acknowledge and address complex and intersectional language model
bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Magee_L/0/1/0/all/0/1"&gt;Liam Magee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghahremanlou_L/0/1/0/all/0/1"&gt;Lida Ghahremanlou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soldatic_K/0/1/0/all/0/1"&gt;Karen Soldatic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robertson_S/0/1/0/all/0/1"&gt;Shanthi Robertson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual FUDGE: Form Understanding via Dynamic Graph Editing. (arXiv:2105.08194v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08194</id>
        <link href="http://arxiv.org/abs/2105.08194"/>
        <updated>2021-07-19T00:49:06.730Z</updated>
        <summary type="html"><![CDATA[We address the problem of form understanding: finding text entities and the
relationships/links between them in form images. The proposed FUDGE model
formulates this problem on a graph of text elements (the vertices) and uses a
Graph Convolutional Network to predict changes to the graph. The initial
vertices are detected text lines and do not necessarily correspond to the final
text entities, which can span multiple lines. Also, initial edges contain many
false-positive relationships. FUDGE edits the graph structure by combining text
segments (graph vertices) and pruning edges in an iterative fashion to obtain
the final text entities and relationships. While recent work in this area has
focused on leveraging large-scale pre-trained Language Models (LM), FUDGE
achieves almost the same level of entity linking performance on the FUNSD
dataset by learning only visual features from the (small) provided training
set. FUDGE can be applied on forms where text recognition is difficult (e.g.
degraded or historical forms) and on forms in resource-poor languages where
pre-training such LMs is challenging. FUDGE is state-of-the-art on the
historical NAF dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Davis_B/0/1/0/all/0/1"&gt;Brian Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morse%7F_B/0/1/0/all/0/1"&gt;Bryan Morse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price%7F_B/0/1/0/all/0/1"&gt;Brian Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tensmeyer%7F_C/0/1/0/all/0/1"&gt;Chris Tensmeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiginton_C/0/1/0/all/0/1"&gt;Curtis Wiginton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring inter-cluster similarities with Alpha Shape TRIangulation in loCal Subspaces (ASTRICS) facilitates visualization and clustering of high-dimensional data. (arXiv:2107.07603v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2107.07603</id>
        <link href="http://arxiv.org/abs/2107.07603"/>
        <updated>2021-07-19T00:49:06.719Z</updated>
        <summary type="html"><![CDATA[Clustering and visualizing high-dimensional (HD) data are important tasks in
a variety of fields. For example, in bioinformatics, they are crucial for
analyses of single-cell data such as mass cytometry (CyTOF) data. Some of the
most effective algorithms for clustering HD data are based on representing the
data by nodes in a graph, with edges connecting neighbouring nodes according to
some measure of similarity or distance. However, users of graph-based
algorithms are typically faced with the critical but challenging task of
choosing the value of an input parameter that sets the size of neighbourhoods
in the graph, e.g. the number of nearest neighbours to which to connect each
node or a threshold distance for connecting nodes. The burden on the user could
be alleviated by a measure of inter-node similarity that can have value 0 for
dissimilar nodes without requiring any user-defined parameters or thresholds.
This would determine the neighbourhoods automatically while still yielding a
sparse graph. To this end, I propose a new method called ASTRICS to measure
similarity between clusters of HD data points based on local dimensionality
reduction and triangulation of critical alpha shapes. I show that my ASTRICS
similarity measure can facilitate both clustering and visualization of HD data
by using it in Stage 2 of a three-stage pipeline: Stage 1 = perform an initial
clustering of the data by any method; Stage 2 = let graph nodes represent
initial clusters instead of individual data points and use ASTRICS to
automatically define edges between nodes; Stage 3 = use the graph for further
clustering and visualization. This trades the critical task of choosing a graph
neighbourhood size for the easier task of essentially choosing a resolution at
which to view the data. The graph and consequently downstream clustering and
visualization are then automatically adapted to the chosen resolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Scurll_J/0/1/0/all/0/1"&gt;Joshua M. Scurll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharing Pain: Using Domain Transfer Between Pain Types for Recognition of Sparse Pain Expressions in Horses. (arXiv:2105.10313v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10313</id>
        <link href="http://arxiv.org/abs/2105.10313"/>
        <updated>2021-07-19T00:49:06.704Z</updated>
        <summary type="html"><![CDATA[Orthopedic disorders are a common cause for euthanasia among horses, which
often could have been avoided with earlier detection. These conditions often
create varying degrees of subtle but long-term pain. It is challenging to train
a visual pain recognition method with video data depicting such pain, since the
resulting pain behavior also is subtle, sparsely appearing, and varying, making
it challenging for even an expert human labeler to provide accurate
ground-truth for the data. We show that transferring features from a dataset of
horses with acute nociceptive pain (where labeling is less ambiguous) can aid
the learning to recognize more complex orthopedic pain. Moreover, we present a
human expert baseline for the problem, as well as an extensive empirical study
of various domain transfer methods and of what is detected by the pain
recognition method trained on acute pain in the orthopedic dataset. Finally,
this is accompanied with a discussion around the challenges posed by real-world
animal behavior datasets and how best practices can be established for similar
fine-grained action recognition tasks. Our code is available at
https://github.com/sofiabroome/painface-recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1"&gt;Sofia Broom&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ask_K/0/1/0/all/0/1"&gt;Katrina Ask&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1"&gt;Maheen Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1"&gt;Pia Haubro Andersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1"&gt;Hedvig Kjellstr&amp;#xf6;m&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond In-Place Corruption: Insertion and Deletion In Denoising Probabilistic Models. (arXiv:2107.07675v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07675</id>
        <link href="http://arxiv.org/abs/2107.07675"/>
        <updated>2021-07-19T00:49:06.696Z</updated>
        <summary type="html"><![CDATA[Denoising diffusion probabilistic models (DDPMs) have shown impressive
results on sequence generation by iteratively corrupting each example and then
learning to map corrupted versions back to the original. However, previous work
has largely focused on in-place corruption, adding noise to each pixel or token
individually while keeping their locations the same. In this work, we consider
a broader class of corruption processes and denoising models over sequence data
that can insert and delete elements, while still being efficient to train and
sample from. We demonstrate that these models outperform standard in-place
models on an arithmetic sequence task, and that when trained on the text8
dataset they can be used to fix spelling errors without any fine-tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1"&gt;Daniel D. Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1"&gt;Jacob Austin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1"&gt;Rianne van den Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarlow_D/0/1/0/all/0/1"&gt;Daniel Tarlow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Application of Active Query K-Means in Text Classification. (arXiv:2107.07682v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07682</id>
        <link href="http://arxiv.org/abs/2107.07682"/>
        <updated>2021-07-19T00:49:06.689Z</updated>
        <summary type="html"><![CDATA[Active learning is a state-of-art machine learning approach to deal with an
abundance of unlabeled data. In the field of Natural Language Processing,
typically it is costly and time-consuming to have all the data annotated. This
inefficiency inspires out our application of active learning in text
classification. Traditional unsupervised k-means clustering is first modified
into a semi-supervised version in this research. Then, a novel attempt is
applied to further extend the algorithm into active learning scenario with
Penalized Min-Max-selection, so as to make limited queries that yield more
stable initial centroids. This method utilizes both the interactive query
results from users and the underlying distance representation. After tested on
a Chinese news dataset, it shows a consistent increase in accuracy while
lowering the cost in training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yukun Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Attack for Uncertainty Estimation: Identifying Critical Regions in Neural Networks. (arXiv:2107.07618v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07618</id>
        <link href="http://arxiv.org/abs/2107.07618"/>
        <updated>2021-07-19T00:49:06.684Z</updated>
        <summary type="html"><![CDATA[We propose a novel method to capture data points near decision boundary in
neural network that are often referred to a specific type of uncertainty. In
our approach, we sought to perform uncertainty estimation based on the idea of
adversarial attack method. In this paper, uncertainty estimates are derived
from the input perturbations, unlike previous studies that provide
perturbations on the model's parameters as in Bayesian approach. We are able to
produce uncertainty with couple of perturbations on the inputs. Interestingly,
we apply the proposed method to datasets derived from blockchain. We compare
the performance of model uncertainty with the most recent uncertainty methods.
We show that the proposed method has revealed a significant outperformance over
other methods and provided less risk to capture model uncertainty in machine
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alarab_I/0/1/0/all/0/1"&gt;Ismail Alarab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakoonwit_S/0/1/0/all/0/1"&gt;Simant Prakoonwit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlation detection in trees for partial graph alignment. (arXiv:2107.07623v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.07623</id>
        <link href="http://arxiv.org/abs/2107.07623"/>
        <updated>2021-07-19T00:49:06.669Z</updated>
        <summary type="html"><![CDATA[We consider alignment of sparse graphs, which consists in finding a mapping
between the nodes of two graphs which preserves most of the edges. Our approach
is to compare local structures in the two graphs, matching two nodes if their
neighborhoods are 'close enough': for correlated Erd\H{o}s-R\'enyi random
graphs, this problem can be locally rephrased in terms of testing whether a
pair of branching trees is drawn from either a product distribution, or a
correlated distribution. We design an optimal test for this problem which gives
rise to a message-passing algorithm for graph alignment, which provably returns
in polynomial time a positive fraction of correctly matched vertices, and a
vanishing fraction of mismatches. With an average degree $\lambda = O(1)$ in
the graphs, and a correlation parameter $s \in [0,1]$, this result holds with
$\lambda s$ large enough, and $1-s$ small enough, completing the recent
state-of-the-art diagram. Tighter conditions for determining whether partial
graph alignment (or correlation detection in trees) is feasible in polynomial
time are given in terms of Kullback-Leibler divergences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ganassali_L/0/1/0/all/0/1"&gt;Luca Ganassali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Massoulie_L/0/1/0/all/0/1"&gt;Laurent Massouli&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lelarge_M/0/1/0/all/0/1"&gt;Marc Lelarge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Globally Convergent Multilevel Training of Deep Residual Networks. (arXiv:2107.07572v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07572</id>
        <link href="http://arxiv.org/abs/2107.07572"/>
        <updated>2021-07-19T00:49:06.659Z</updated>
        <summary type="html"><![CDATA[We propose a globally convergent multilevel training method for deep residual
networks (ResNets). The devised method can be seen as a novel variant of the
recursive multilevel trust-region (RMTR) method, which operates in hybrid
(stochastic-deterministic) settings by adaptively adjusting mini-batch sizes
during the training. The multilevel hierarchy and the transfer operators are
constructed by exploiting a dynamical system's viewpoint, which interprets
forward propagation through the ResNet as a forward Euler discretization of an
initial value problem. In contrast to traditional training approaches, our
novel RMTR method also incorporates curvature information on all levels of the
multilevel hierarchy by means of the limited-memory SR1 method. The overall
performance and the convergence properties of our multilevel training method
are numerically investigated using examples from the field of classification
and regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kopanicakova_A/0/1/0/all/0/1"&gt;Alena Kopani&amp;#x10d;&amp;#xe1;kov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_R/0/1/0/all/0/1"&gt;Rolf Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Importance of Regularisation & Auxiliary Information in OOD Detection. (arXiv:2107.07564v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07564</id>
        <link href="http://arxiv.org/abs/2107.07564"/>
        <updated>2021-07-19T00:49:06.653Z</updated>
        <summary type="html"><![CDATA[Neural networks are often utilised in critical domain applications
(e.g.~self-driving cars, financial markets, and aerospace engineering), even
though they exhibit overconfident predictions for ambiguous inputs. This
deficiency demonstrates a fundamental flaw indicating that neural networks
often overfit on spurious correlations. To address this problem in this work we
present two novel objectives that improve the ability of a network to detect
out-of-distribution samples and therefore avoid overconfident predictions for
ambiguous inputs. We empirically demonstrate that our methods outperform the
baseline and perform better than the majority of existing approaches, while
performing competitively those that they don't outperform. Additionally, we
empirically demonstrate the robustness of our approach against common
corruptions and demonstrate the importance of regularisation and auxiliary
information in out-of-distribution detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitros_J/0/1/0/all/0/1"&gt;John Mitros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1"&gt;Brian Mac Namee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Channel Coding Benchmark for Meta-Learning. (arXiv:2107.07579v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07579</id>
        <link href="http://arxiv.org/abs/2107.07579"/>
        <updated>2021-07-19T00:49:06.648Z</updated>
        <summary type="html"><![CDATA[Meta-learning provides a popular and effective family of methods for
data-efficient learning of new tasks. However, several important issues in
meta-learning have proven hard to study thus far. For example, performance
degrades in real-world settings where meta-learners must learn from a wide and
potentially multi-modal distribution of training tasks; and when distribution
shift exists between meta-train and meta-test task distributions. These issues
are typically hard to study since the shape of task distributions, and shift
between them are not straightforward to measure or control in standard
benchmarks. We propose the channel coding problem as a benchmark for
meta-learning. Channel coding is an important practical application where task
distributions naturally arise, and fast adaptation to new tasks is practically
valuable. We use this benchmark to study several aspects of meta-learning,
including the impact of task distribution breadth and shift, which can be
controlled in the coding problem. Going forward, this benchmark provides a tool
for the community to study the capabilities and limitations of meta-learning,
and to drive research on practically robust and effective meta-learners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1"&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1"&gt;Rajesh Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyeji Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Da Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1"&gt;Nicholas Lane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1"&gt;Timothy Hospedales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07647</id>
        <link href="http://arxiv.org/abs/2107.07647"/>
        <updated>2021-07-19T00:49:06.633Z</updated>
        <summary type="html"><![CDATA[A novel energy-efficient edge computing paradigm is proposed for real-time
deep learning-based image upsampling applications. State-of-the-art deep
learning solutions for image upsampling are currently trained using either
resize or sub-pixel convolution to learn kernels that generate high fidelity
images with minimal artifacts. However, performing inference with these learned
convolution kernels requires memory-intensive feature map transformations that
dominate time and energy costs in real-time applications. To alleviate this
pressure on memory bandwidth, we confine the use of resize or sub-pixel
convolution to training in the cloud by transforming learned convolution
kernels to deconvolution kernels before deploying them for inference as a
functionally equivalent deconvolution. These kernel transformations, intended
as a one-time cost when shifting from training to inference, enable a systems
designer to use each algorithm in their optimal context by preserving the image
fidelity learned when training in the cloud while minimizing data transfer
penalties during inference at the edge. We also explore existing variants of
deconvolution inference algorithms and introduce a novel variant for
consideration. We analyze and compare the inference properties of
convolution-based upsampling algorithms using a quantitative model of incurred
time and energy costs and show that using deconvolution for inference at the
edge improves both system latency and energy efficiency when compared to their
sub-pixel or resize convolution counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1"&gt;Ian Colbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1"&gt;Ken Kreutz-Delgado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Srinjoy Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.07002</id>
        <link href="http://arxiv.org/abs/2107.07002"/>
        <updated>2021-07-19T00:49:06.626Z</updated>
        <summary type="html"><![CDATA[The world of empirical machine learning (ML) strongly relies on benchmarks in
order to determine the relative effectiveness of different algorithms and
methods. This paper proposes the notion of "a benchmark lottery" that describes
the overall fragility of the ML benchmarking process. The benchmark lottery
postulates that many factors, other than fundamental algorithmic superiority,
may lead to a method being perceived as superior. On multiple benchmark setups
that are prevalent in the ML community, we show that the relative performance
of algorithms may be altered significantly simply by choosing different
benchmark tasks, highlighting the fragility of the current paradigms and
potential fallacious interpretation derived from benchmarking ML methods. Given
that every benchmark makes a statement about what it perceives to be important,
we argue that this might lead to biased progress in the community. We discuss
the implications of the observed phenomena and provide recommendations on
mitigating them using multiple machine learning domains and communities as use
cases, including natural language processing, computer vision, information
retrieval, recommender systems, and reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1"&gt;Alexey A. Gritsenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhe Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1"&gt;Fernando Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SA-GD: Improved Gradient Descent Learning Strategy with Simulated Annealing. (arXiv:2107.07558v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07558</id>
        <link href="http://arxiv.org/abs/2107.07558"/>
        <updated>2021-07-19T00:49:06.611Z</updated>
        <summary type="html"><![CDATA[Gradient descent algorithm is the most utilized method when optimizing
machine learning issues. However, there exists many local minimums and saddle
points in the loss function, especially for high dimensional non-convex
optimization problems like deep learning. Gradient descent may make loss
function trapped in these local intervals which impedes further optimization,
resulting in poor generalization ability. This paper proposes the SA-GD
algorithm which introduces the thought of simulated annealing algorithm to
gradient descent. SA-GD method offers model the ability of mounting hills in
probability, tending to enable the model to jump out of these local areas and
converge to a optimal state finally. We took CNN models as an example and
tested the basic CNN models on various benchmark datasets. Compared to the
baseline models with traditional gradient descent algorithm, models with SA-GD
algorithm possess better generalization ability without sacrificing the
efficiency and stability of model convergence. In addition, SA-GD can be
utilized as an effective ensemble learning approach which improves the final
performance significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhicheng Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing. (arXiv:2106.03686v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03686</id>
        <link href="http://arxiv.org/abs/2106.03686"/>
        <updated>2021-07-19T00:49:06.597Z</updated>
        <summary type="html"><![CDATA[We address the detection of material defects, which are inside a layered
material structure using compressive sensing based multiple-input and
multiple-output (MIMO) wireless radar. Here, the strong clutter due to the
reflection of the layered structure's surface often makes the detection of the
defects challenging. Thus, sophisticated signal separation methods are required
for improved defect detection. In many scenarios, the number of defects that we
are interested in is limited and the signaling response of the layered
structure can be modeled as a low-rank structure. Therefore, we propose joint
rank and sparsity minimization for defect detection. In particular, we propose
a non-convex approach based on the iteratively reweighted nuclear and
$\ell_1-$norm (a double-reweighted approach) to obtain a higher accuracy
compared to the conventional nuclear norm and $\ell_1-$norm minimization. To
this end, an iterative algorithm is designed to estimate the low-rank and
sparse contributions. Further, we propose deep learning to learn the parameters
of the algorithm (i.e., algorithm unfolding) to improve the accuracy and the
speed of convergence of the algorithm. Our numerical results show that the
proposed approach outperforms the conventional approaches in terms of mean
square errors of the recovered low-rank and sparse components and the speed of
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Thanthrige_U/0/1/0/all/0/1"&gt;Udaya S.K.P. Miriya Thanthrige&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jung_P/0/1/0/all/0/1"&gt;Peter Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sezgin_A/0/1/0/all/0/1"&gt;Aydin Sezgin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Shot Neural Ensemble Architecture Search by Diversity-Guided Search Space Shrinking. (arXiv:2104.00597v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00597</id>
        <link href="http://arxiv.org/abs/2104.00597"/>
        <updated>2021-07-19T00:49:06.565Z</updated>
        <summary type="html"><![CDATA[Despite remarkable progress achieved, most neural architecture search (NAS)
methods focus on searching for one single accurate and robust architecture. To
further build models with better generalization capability and performance,
model ensemble is usually adopted and performs better than stand-alone models.
Inspired by the merits of model ensemble, we propose to search for multiple
diverse models simultaneously as an alternative way to find powerful models.
Searching for ensembles is non-trivial and has two key challenges: enlarged
search space and potentially more complexity for the searched model. In this
paper, we propose a one-shot neural ensemble architecture search (NEAS)
solution that addresses the two challenges. For the first challenge, we
introduce a novel diversity-based metric to guide search space shrinking,
considering both the potentiality and diversity of candidate operators. For the
second challenge, we enable a new search dimension to learn layer sharing among
different models for efficiency purposes. The experiments on ImageNet clearly
demonstrate that our solution can improve the supernet's capacity of ranking
ensemble architectures, and further lead to better search results. The
discovered architectures achieve superior performance compared with
state-of-the-arts such as MobileNetV3 and EfficientNet families under aligned
settings. Moreover, we evaluate the generalization ability and robustness of
our searched architecture on the COCO detection benchmark and achieve a 3.1%
improvement on AP compared with MobileNetV3. Codes and models are available at
https://github.com/researchmm/NEAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minghao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Houwen Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jianlong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1"&gt;Haibin Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CutDepth:Edge-aware Data Augmentation in Depth Estimation. (arXiv:2107.07684v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07684</id>
        <link href="http://arxiv.org/abs/2107.07684"/>
        <updated>2021-07-19T00:49:06.547Z</updated>
        <summary type="html"><![CDATA[It is difficult to collect data on a large scale in a monocular depth
estimation because the task requires the simultaneous acquisition of RGB images
and depths. Data augmentation is thus important to this task. However, there
has been little research on data augmentation for tasks such as monocular depth
estimation, where the transformation is performed pixel by pixel. In this
paper, we propose a data augmentation method, called CutDepth. In CutDepth,
part of the depth is pasted onto an input image during training. The method
extends variations data without destroying edge features. Experiments
objectively and subjectively show that the proposed method outperforms
conventional methods of data augmentation. The estimation accuracy is improved
with CutDepth even though there are few training data at long distances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_Y/0/1/0/all/0/1"&gt;Yasunori Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamashita_T/0/1/0/all/0/1"&gt;Takayoshi Yamashita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video. (arXiv:2011.10670v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10670</id>
        <link href="http://arxiv.org/abs/2011.10670"/>
        <updated>2021-07-19T00:49:06.497Z</updated>
        <summary type="html"><![CDATA[With the advancement in computer vision deep learning, systems now are able
to analyze an unprecedented amount of rich visual information from videos to
enable applications such as autonomous driving, socially-aware robot assistant
and public safety monitoring. Deciphering human behaviors to predict their
future paths/trajectories and what they would do from videos is important in
these applications. However, human trajectory prediction still remains a
challenging task, as scene semantics and human intent are difficult to model.
Many systems do not provide high-level semantic attributes to reason about
pedestrian future. This design hinders prediction performance in video data
from diverse domains and unseen scenarios. To enable optimal future human
behavioral forecasting, it is crucial for the system to be able to detect and
analyze human activities as well as scene semantics, passing informative
features to the subsequent prediction module for context understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Junwei Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accuracy Prediction with Non-neural Model for Neural Architecture Search. (arXiv:2007.04785v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04785</id>
        <link href="http://arxiv.org/abs/2007.04785"/>
        <updated>2021-07-19T00:49:06.477Z</updated>
        <summary type="html"><![CDATA[Neural architecture search (NAS) with an accuracy predictor that predicts the
accuracy of candidate architectures has drawn increasing attention due to its
simplicity and effectiveness. Previous works usually employ neural
network-based predictors which require more delicate design and are easy to
overfit. Considering that most architectures are represented as sequences of
discrete symbols which are more like tabular data and preferred by non-neural
predictors, in this paper, we study an alternative approach which uses
non-neural model for accuracy prediction. Specifically, as decision tree based
models can better handle tabular data, we leverage gradient boosting decision
tree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor
can achieve comparable (if not better) prediction accuracy than neural network
based predictors. Moreover, considering that a compact search space can ease
the search process, we propose to prune the search space gradually according to
important features derived from GBDT. In this way, NAS can be performed by
first pruning the search space and then searching a neural architecture, which
is more efficient and effective. Experiments on NASBench-101 and ImageNet
demonstrate the effectiveness of using GBDT as predictor for NAS: (1) On
NASBench-101, it is 22x, 8x, and 6x more sample efficient than random search,
regularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global
optimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further
achieves 23.4% top-1 error rate on ImageNet when enhanced with search space
pruning. Code is provided in the supplementary materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Renqian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Enhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information-Theoretic Visual Explanation for Black-Box Classifiers. (arXiv:2009.11150v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.11150</id>
        <link href="http://arxiv.org/abs/2009.11150"/>
        <updated>2021-07-19T00:49:06.470Z</updated>
        <summary type="html"><![CDATA[In this work, we attempt to explain the prediction of any black-box
classifier from an information-theoretic perspective. For each input feature,
we compare the classifier outputs with and without that feature using two
information-theoretic metrics. Accordingly, we obtain two attribution maps--an
information gain (IG) map and a point-wise mutual information (PMI) map. IG map
provides a class-independent answer to "How informative is each pixel?", and
PMI map offers a class-specific explanation of "How much does each pixel
support a specific class?" Compared to existing methods, our method improves
the correctness of the attribution maps in terms of a quantitative metric. We
also provide a detailed analysis of an ImageNet classifier using the proposed
method, and the code is available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jihun Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1"&gt;Eunji Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Siwon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Annotated High-Fidelity Images Containing Multiple Coherent Objects. (arXiv:2006.12150v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12150</id>
        <link href="http://arxiv.org/abs/2006.12150"/>
        <updated>2021-07-19T00:49:06.464Z</updated>
        <summary type="html"><![CDATA[Recent developments related to generative models have made it possible to
generate diverse high-fidelity images. In particular, layout-to-image
generation models have gained significant attention due to their capability to
generate realistic complex images containing distinct objects. These models are
generally conditioned on either semantic layouts or textual descriptions.
However, unlike natural images, providing auxiliary information can be
extremely hard in domains such as biomedical imaging and remote sensing. In
this work, we propose a multi-object generation framework that can synthesize
images with multiple objects without explicitly requiring their contextual
information during the generation process. Based on a vector-quantized
variational autoencoder (VQ-VAE) backbone, our model learns to preserve spatial
coherency within an image as well as semantic coherency between the objects and
the background through two powerful autoregressive priors: PixelSNAIL and
LayoutPixelSNAIL. While the PixelSNAIL learns the distribution of the latent
encodings of the VQ-VAE, the LayoutPixelSNAIL is used to specifically learn the
semantic distribution of the objects. An implicit advantage of our approach is
that the generated samples are accompanied by object-level annotations. We
demonstrate how coherency and fidelity are preserved with our method through
experiments on the Multi-MNIST and CLEVR datasets; thereby outperforming
state-of-the-art multi-object generative methods. The efficacy of our approach
is demonstrated through application on medical imaging datasets, where we show
that augmenting the training set with generated samples using our approach
improves the performance of existing models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cardenas_B/0/1/0/all/0/1"&gt;Bryan G. Cardenas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arya_D/0/1/0/all/0/1"&gt;Devanshu Arya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1"&gt;Deepak K. Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation Consolidation for Training Expert Students. (arXiv:2107.08039v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08039</id>
        <link href="http://arxiv.org/abs/2107.08039"/>
        <updated>2021-07-19T00:49:06.449Z</updated>
        <summary type="html"><![CDATA[Traditionally, distillation has been used to train a student model to emulate
the input/output functionality of a teacher. A more useful goal than emulation,
yet under-explored, is for the student to learn feature representations that
transfer well to future tasks. However, we observe that standard distillation
of task-specific teachers actually *reduces* the transferability of student
representations to downstream tasks. We show that a multi-head, multi-task
distillation method using an unlabeled proxy dataset and a generalist teacher
is sufficient to consolidate representations from task-specific teacher(s) and
improve downstream performance, outperforming the teacher(s) and the strong
baseline of ImageNet pretrained features. Our method can also combine the
representational knowledge of multiple teachers trained on one or multiple
domains into a single model, whose representation is improved on all teachers'
domain(s).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhizhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1"&gt;Charless Fowlkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1"&gt;Marzia Polito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1"&gt;Rahul Bhotika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Families In Wild Multimedia (FIW MM): A Multi-Modal Database for Recognizing Kinship. (arXiv:2007.14509v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.14509</id>
        <link href="http://arxiv.org/abs/2007.14509"/>
        <updated>2021-07-19T00:49:06.444Z</updated>
        <summary type="html"><![CDATA[Kinship is a soft biometric detectable in media with an abundance of
practical applications. Despite the difficulty of detecting kinship, annual
data challenges using still-images have consistently improved performances and
attracted new researchers. Now, systems reach performance levels unforeseeable
a decade ago, closing in on performances acceptable to deploy in practice.
Similar to other biometric tasks, we expect systems can benefit from additional
modalities. We hypothesize that adding modalities to FIW, which contains only
still-images, will improve performance. Thus, to narrow the gap between
research and reality and enhance the power of kinship recognition systems, we
extend FIW with multimedia (MM) data (i.e., video, audio, and text captions).
Specifically, we introduce the first publicly available multi-task MM kinship
dataset. To build FIW MM, we developed machinery to automatically collect,
annotate, and prepare the data, requiring minimal human input and no financial
cost. The proposed MM corpus allows the problem statements to be more realistic
template-based protocols. We show significant improvements in all benchmarks
with the added modalities. The results highlight edge cases to inspire future
research with different areas of improvement. FIW MM provides the data required
to increase the potential of automated systems to detect kinship in MM. It also
allows experts from diverse fields to collaborate in novel ways.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1"&gt;Joseph P. Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1"&gt;Zaid Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1"&gt;Ming Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yun Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Efficient GANs for Image Translation via Differentiable Masks and co-Attention Distillation. (arXiv:2011.08382v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08382</id>
        <link href="http://arxiv.org/abs/2011.08382"/>
        <updated>2021-07-19T00:49:06.438Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have been widely-used in image
translation, but their high computational and storage costs impede the
deployment on mobile devices. Prevalent methods for CNN compression cannot be
directly applied to GANs due to the complicated generator architecture and the
unstable adversarial training. To solve these, in this paper, we introduce a
novel GAN compression method, termed DMAD, by proposing a Differentiable Mask
and a co-Attention Distillation. The former searches for a light-weight
generator architecture in a training-adaptive manner. To overcome channel
inconsistency when pruning the residual connections, an adaptive cross-block
group sparsity is further incorporated. The latter simultaneously distills
informative attention maps from both the generator and discriminator of a
pre-trained model to the searched generator, effectively stabilizing the
adversarial training of our light-weight model. Experiments show that DMAD can
reduce the Multiply Accumulate Operations (MACs) of CycleGAN by 13$\times$ and
that of Pix2Pix by 4$\times$ while retaining a comparable performance against
the full model. Our code can be available at https://github.com/SJLeo/DMAD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaojie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1"&gt;Fei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1"&gt;Xudong Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingliang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongjian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is attention to bounding boxes all you need for pedestrian action prediction?. (arXiv:2107.08031v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08031</id>
        <link href="http://arxiv.org/abs/2107.08031"/>
        <updated>2021-07-19T00:49:06.431Z</updated>
        <summary type="html"><![CDATA[The human driver is no longer the only one concerned with the complexity of
the driving scenarios. Autonomous vehicles (AV) are similarly becoming involved
in the process. Nowadays, the development of AV in urban places underpins
essential safety concerns for vulnerable road users (VRUs) such as pedestrians.
Therefore, to make the roads safer, it is critical to classify and predict
their future behavior. In this paper, we present a framework based on multiple
variations of the Transformer models to reason attentively about the dynamic
evolution of the pedestrians' past trajectory and predict its future actions of
crossing or not crossing the street. We proved that using only bounding boxes
as input to our model can outperform the previous state-of-the-art models and
reach a prediction accuracy of 91 % and an F1-score of 0.83 on the PIE dataset
up to two seconds ahead in the future. In addition, we introduced a large-size
simulated dataset (CP2A) using CARLA for action prediction. Our model has
similarly reached high accuracy (91 %) and F1-score (0.91) on this dataset.
Interestingly, we showed that pre-training our Transformer model on the
simulated dataset and then fine-tuning it on the real dataset can be very
effective for the action prediction task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Achaji_L/0/1/0/all/0/1"&gt;Lina Achaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1"&gt;Julien Moreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fouqueray_T/0/1/0/all/0/1"&gt;Thibault Fouqueray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aioun_F/0/1/0/all/0/1"&gt;Francois Aioun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charpillet_F/0/1/0/all/0/1"&gt;Francois Charpillet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlled AutoEncoders to Generate Faces from Voices. (arXiv:2107.07988v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07988</id>
        <link href="http://arxiv.org/abs/2107.07988"/>
        <updated>2021-07-19T00:49:06.425Z</updated>
        <summary type="html"><![CDATA[Multiple studies in the past have shown that there is a strong correlation
between human vocal characteristics and facial features. However, existing
approaches generate faces simply from voice, without exploring the set of
features that contribute to these observed correlations. A computational
methodology to explore this can be devised by rephrasing the question to: "how
much would a target face have to change in order to be perceived as the
originator of a source voice?" With this in perspective, we propose a framework
to morph a target face in response to a given voice in a way that facial
features are implicitly guided by learned voice-face correlation in this paper.
Our framework includes a guided autoencoder that converts one face to another,
controlled by a unique model-conditioning component called a gating controller
which modifies the reconstructed face based on input voice recordings. We
evaluate the framework on VoxCelab and VGGFace datasets through human subjects
and face retrieval. Various experiments demonstrate the effectiveness of our
proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Hao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lulan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guikang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1"&gt;Bhiksha Raj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rita Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CCVS: Context-aware Controllable Video Synthesis. (arXiv:2107.08037v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08037</id>
        <link href="http://arxiv.org/abs/2107.08037"/>
        <updated>2021-07-19T00:49:06.418Z</updated>
        <summary type="html"><![CDATA[This presentation introduces a self-supervised learning approach to the
synthesis of new video clips from old ones, with several new key elements for
improved spatial resolution and realism: It conditions the synthesis process on
contextual information for temporal continuity and ancillary information for
fine control. The prediction model is doubly autoregressive, in the latent
space of an autoencoder for forecasting, and in image space for updating
contextual information, which is also used to enforce spatio-temporal
consistency through a learnable optical flow module. Adversarial training of
the autoencoder in the appearance and temporal domains is used to further
improve the realism of its output. A quantizer inserted between the encoder and
the transformer in charge of forecasting future frames in latent space (and its
inverse inserted between the transformer and the decoder) adds even more
flexibility by affording simple mechanisms for handling multimodal ancillary
information for controlling the synthesis process (eg, a few sample frames, an
audio track, a trajectory in image space) and taking into account the
intrinsically uncertain nature of the future by allowing multiple predictions.
Experiments with an implementation of the proposed approach give very good
qualitative and quantitative results on multiple tasks and standard benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moing_G/0/1/0/all/0/1"&gt;Guillaume Le Moing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1"&gt;Jean Ponce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1"&gt;Cordelia Schmid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Flow via Invertible nxn Convolution. (arXiv:1905.10170v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.10170</id>
        <link href="http://arxiv.org/abs/1905.10170"/>
        <updated>2021-07-19T00:49:06.400Z</updated>
        <summary type="html"><![CDATA[Flow-based generative models have recently become one of the most efficient
approaches to model data generation. Indeed, they are constructed with a
sequence of invertible and tractable transformations. Glow first introduced a
simple type of generative flow using an invertible $1 \times 1$ convolution.
However, the $1 \times 1$ convolution suffers from limited flexibility compared
to the standard convolutions. In this paper, we propose a novel invertible $n
\times n$ convolution approach that overcomes the limitations of the invertible
$1 \times 1$ convolution. In addition, our proposed network is not only
tractable and invertible but also uses fewer parameters than standard
convolutions. The experiments on CIFAR-10, ImageNet and Celeb-HQ datasets, have
shown that our invertible $n \times n$ convolution helps to improve the
performance of generative models significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Thanh-Dat Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1"&gt;Khoa Luu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1"&gt;Chi Nhan Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1"&gt;Ngan Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1"&gt;Minh-Triet Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[All the attention you need: Global-local, spatial-channel attention for image retrieval. (arXiv:2107.08000v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.08000</id>
        <link href="http://arxiv.org/abs/2107.08000"/>
        <updated>2021-07-19T00:49:06.393Z</updated>
        <summary type="html"><![CDATA[We address representation learning for large-scale instance-level image
retrieval. Apart from backbone, training pipelines and loss functions, popular
approaches have focused on different spatial pooling and attention mechanisms,
which are at the core of learning a powerful global image representation. There
are different forms of attention according to the interaction of elements of
the feature tensor (local and global) and the dimensions where it is applied
(spatial and channel). Unfortunately, each study addresses only one or two
forms of attention and applies it to different problems like classification,
detection or retrieval.

We present global-local attention module (GLAM), which is attached at the end
of a backbone network and incorporates all four forms of attention: local and
global, spatial and channel. We obtain a new feature tensor and, by spatial
pooling, we learn a powerful embedding for image retrieval. Focusing on global
descriptors, we provide empirical evidence of the interaction of all forms of
attention and improve the state of the art on standard benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chull Hwan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1"&gt;Hye Joo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1"&gt;Yannis Avrithis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Scale Recovery for Monocular Depth and Egomotion Estimation. (arXiv:2009.03787v4 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.03787</id>
        <link href="http://arxiv.org/abs/2009.03787"/>
        <updated>2021-07-19T00:49:06.379Z</updated>
        <summary type="html"><![CDATA[The self-supervised loss formulation for jointly training depth and egomotion
neural networks with monocular images is well studied and has demonstrated
state-of-the-art accuracy. One of the main limitations of this approach,
however, is that the depth and egomotion estimates are only determined up to an
unknown scale. In this paper, we present a novel scale recovery loss that
enforces consistency between a known camera height and the estimated camera
height, generating metric (scaled) depth and egomotion predictions. We show
that our proposed method is competitive with other scale recovery techniques
that require more information. Further, we demonstrate that our method
facilitates network retraining within new environments, whereas other
scale-resolving approaches are incapable of doing so. Notably, our egomotion
network is able to produce more accurate estimates than a similar method which
recovers scale at test time only.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagstaff_B/0/1/0/all/0/1"&gt;Brandon Wagstaff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1"&gt;Jonathan Kelly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lightness Modulated Deep Inverse Tone Mapping. (arXiv:2107.07907v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07907</id>
        <link href="http://arxiv.org/abs/2107.07907"/>
        <updated>2021-07-19T00:49:06.350Z</updated>
        <summary type="html"><![CDATA[Single-image HDR reconstruction or inverse tone mapping (iTM) is a
challenging task. In particular, recovering information in over-exposed regions
is extremely difficult because details in such regions are almost completely
lost. In this paper, we present a deep learning based iTM method that takes
advantage of the feature extraction and mapping power of deep convolutional
neural networks (CNNs) and uses a lightness prior to modulate the CNN to better
exploit observations in the surrounding areas of the over-exposed regions to
enhance the quality of HDR image reconstruction. Specifically, we introduce a
Hierarchical Synthesis Network (HiSN) for inferring a HDR image from a LDR
input and a Lightness Adpative Modulation Network (LAMN) to incorporate the the
lightness prior knowledge in the inferring process. The HiSN hierarchically
synthesizes the high-brightness component and the low-brightness component of
the HDR image whilst the LAMN uses a lightness adaptive mask that separates
detail-less saturated bright pixels from well-exposed lower light pixels to
enable HiSN to better infer the missing information, particularly in the
difficult over-exposed detail-less areas. We present experimental results to
demonstrate the effectiveness of the new technique based on quantitative
measures and visual comparisons. In addition, we present ablation studies of
HiSN and visualization of the activation maps inside LAMN to help gain a deeper
understanding of the internal working of the new iTM algorithm and explain why
it can achieve much improved performance over state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kanglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cao_G/0/1/0/all/0/1"&gt;Gaofeng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jiang Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qiu_G/0/1/0/all/0/1"&gt;Guoping Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In-Bed Person Monitoring Using Thermal Infrared Sensors. (arXiv:2107.07986v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.07986</id>
        <link href="http://arxiv.org/abs/2107.07986"/>
        <updated>2021-07-19T00:49:06.343Z</updated>
        <summary type="html"><![CDATA[The world is expecting an aging population and shortage of healthcare
professionals. This poses the problem of providing a safe and dignified life
for the elderly. Technological solutions involving cameras can contribute to
safety, comfort and efficient emergency responses, but they are invasive of
privacy. We use 'Griddy', a prototype with a Panasonic Grid-EYE, a
low-resolution infrared thermopile array sensor, which offers more privacy.
Mounted over a bed, it can determine if the user is on the bed or not without
human interaction. For this purpose, two datasets were captured, one (480
images) under constant conditions, and a second one (200 images) under
different variations such as use of a duvet, sleeping with a pet, or increased
room temperature. We test three machine learning algorithms: Support Vector
Machines (SVM), k-Nearest Neighbors (k-NN) and Neural Network (NN). With
10-fold cross validation, the highest accuracy in the main dataset is for both
SVM and k-NN (99%). The results with variable data show a lower reliability
under certain circumstances, highlighting the need of extra work to meet the
challenge of variations in the environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Josse_E/0/1/0/all/0/1"&gt;Elias Josse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nerborg_A/0/1/0/all/0/1"&gt;Amanda Nerborg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Diaz_K/0/1/0/all/0/1"&gt;Kevin Hernandez-Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1"&gt;Fernando Alonso-Fernandez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning to Ternary Hash Codes by Continuation. (arXiv:2107.07987v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07987</id>
        <link href="http://arxiv.org/abs/2107.07987"/>
        <updated>2021-07-19T00:49:06.308Z</updated>
        <summary type="html"><![CDATA[Recently, it has been observed that {0,1,-1}-ternary codes which are simply
generated from deep features by hard thresholding, tend to outperform
{-1,1}-binary codes in image retrieval. To obtain better ternary codes, we for
the first time propose to jointly learn the features with the codes by
appending a smoothed function to the networks. During training, the function
could evolve into a non-smoothed ternary function by a continuation method. The
method circumvents the difficulty of directly training discrete functions and
reduces the quantization errors of ternary codes. Experiments show that the
generated codes indeed could achieve higher retrieval accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingrui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weiyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Weizhi Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Bias in Visual Datasets. (arXiv:2107.07919v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07919</id>
        <link href="http://arxiv.org/abs/2107.07919"/>
        <updated>2021-07-19T00:49:06.301Z</updated>
        <summary type="html"><![CDATA[Computer Vision (CV) has achieved remarkable results, outperforming humans in
several tasks. Nonetheless, it may result in major discrimination if not dealt
with proper care. CV systems highly depend on the data they are fed with and
can learn and amplify biases within such data. Thus, both the problems of
understanding and discovering biases are of utmost importance. Yet, to date
there is no comprehensive survey on bias in visual datasets. To this end, this
work aims to: i) describe the biases that can affect visual datasets; ii)
review the literature on methods for bias discovery and quantification in
visual datasets; iii) discuss existing attempts to collect bias-aware visual
datasets. A key conclusion of our study is that the problem of bias discovery
and quantification in visual datasets is still open and there is room for
improvement in terms of both methods and the range of biases that can be
addressed; moreover, there is no such thing as a bias-free dataset, so
scientists and practitioners must become aware of the biases in their datasets
and make them explicit. To this end, we propose a checklist that can be used to
spot different types of bias during visual dataset collection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fabbrizzi_S/0/1/0/all/0/1"&gt;Simone Fabbrizzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1"&gt;Symeon Papadopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1"&gt;Eirini Ntoutsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1"&gt;Ioannis Kompatsiaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Discovery of Object Radiance Fields. (arXiv:2107.07905v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07905</id>
        <link href="http://arxiv.org/abs/2107.07905"/>
        <updated>2021-07-19T00:49:06.288Z</updated>
        <summary type="html"><![CDATA[We study the problem of inferring an object-centric scene representation from
a single image, aiming to derive a representation that explains the image
formation process, captures the scene's 3D nature, and is learned without
supervision. Most existing methods on scene decomposition lack one or more of
these characteristics, due to the fundamental challenge in integrating the
complex 3D-to-2D image formation process into powerful inference schemes like
deep networks. In this paper, we propose unsupervised discovery of Object
Radiance Fields (uORF), integrating recent progresses in neural 3D scene
representations and rendering with deep inference networks for unsupervised 3D
scene decomposition. Trained on multi-view RGB images without annotations, uORF
learns to decompose complex scenes with diverse, textured background from a
single image. We show that uORF performs well on unsupervised 3D scene
segmentation, novel view synthesis, and scene editing on three datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hong-Xing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas J. Guibas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiajun Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-based Vehicle Self-Localization with HD Feature Maps. (arXiv:2107.07787v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07787</id>
        <link href="http://arxiv.org/abs/2107.07787"/>
        <updated>2021-07-19T00:49:06.279Z</updated>
        <summary type="html"><![CDATA[We present a vehicle self-localization method using point-based deep neural
networks. Our approach processes measurements and point features, i.e.
landmarks, from a high-definition digital map to infer the vehicle's pose. To
learn the best association and incorporate local information between the point
sets, we propose an attention mechanism that matches the measurements to the
corresponding landmarks. Finally, we use this representation for the
point-cloud registration and the subsequent pose regression task. Furthermore,
we introduce a training simulation framework that artificially generates
measurements and landmarks to facilitate the deployment process and reduce the
cost of creating extensive datasets from real-world data. We evaluate our
method on our dataset, as well as an adapted version of the Kitti odometry
dataset, where we achieve superior performance compared to related approaches;
and additionally show dominant generalization capabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Engel_N/0/1/0/all/0/1"&gt;Nico Engel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1"&gt;Vasileios Belagiannis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1"&gt;Klaus Dietmayer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wasserstein Distances, Geodesics and Barycenters of Merge Trees. (arXiv:2107.07789v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2107.07789</id>
        <link href="http://arxiv.org/abs/2107.07789"/>
        <updated>2021-07-19T00:49:06.272Z</updated>
        <summary type="html"><![CDATA[This paper presents a unified computational framework for the estimation of
distances, geodesics and barycenters of merge trees. We extend recent work on
the edit distance [106] and introduce a new metric, called the Wasserstein
distance between merge trees, which is purposely designed to enable efficient
computations of geodesics and barycenters. Specifically, our new distance is
strictly equivalent to the L2-Wasserstein distance between extremum persistence
diagrams, but it is restricted to a smaller solution space, namely, the space
of rooted partial isomorphisms between branch decomposition trees. This enables
a simple extension of existing optimization frameworks [112] for geodesics and
barycenters from persistence diagrams to merge trees. We introduce a task-based
algorithm which can be generically applied to distance, geodesic, barycenter or
cluster computation. The task-based nature of our approach enables further
accelerations with shared-memory parallelism. Extensive experiments on public
ensembles and SciVis contest benchmarks demonstrate the efficiency of our
approach -- with barycenter computations in the orders of minutes for the
largest examples -- as well as its qualitative ability to generate
representative barycenter merge trees, visually summarizing the features of
interest found in the ensemble. We show the utility of our contributions with
dedicated visualization applications: feature tracking, temporal reduction and
ensemble clustering. We provide a lightweight C++ implementation that can be
used to reproduce our results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pont_M/0/1/0/all/0/1"&gt;Mathieu Pont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1"&gt;Jules Vidal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delon_J/0/1/0/all/0/1"&gt;Julie Delon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tierny_J/0/1/0/all/0/1"&gt;Julien Tierny&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient automated U-Net based tree crown delineation using UAV multi-spectral imagery on embedded devices. (arXiv:2107.07826v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07826</id>
        <link href="http://arxiv.org/abs/2107.07826"/>
        <updated>2021-07-19T00:49:06.255Z</updated>
        <summary type="html"><![CDATA[Delineation approaches provide significant benefits to various domains,
including agriculture, environmental and natural disasters monitoring. Most of
the work in the literature utilize traditional segmentation methods that
require a large amount of computational and storage resources. Deep learning
has transformed computer vision and dramatically improved machine translation,
though it requires massive dataset for training and significant resources for
inference. More importantly, energy-efficient embedded vision hardware
delivering real-time and robust performance is crucial in the aforementioned
application. In this work, we propose a U-Net based tree delineation method,
which is effectively trained using multi-spectral imagery but can then
delineate single-spectrum images. The deep architecture that also performs
localization, i.e., a class label corresponds to each pixel, has been
successfully used to allow training with a small set of segmented images. The
ground truth data were generated using traditional image denoising and
segmentation approaches. To be able to execute the proposed DNN efficiently in
embedded platforms designed for deep learning approaches, we employ traditional
model compression and acceleration methods. Extensive evaluation studies using
data collected from UAVs equipped with multi-spectral cameras demonstrate the
effectiveness of the proposed methods in terms of delineation accuracy and
execution efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blekos_K/0/1/0/all/0/1"&gt;Kostas Blekos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1"&gt;Stavros Nousias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1"&gt;Aris S Lalos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive Deep Video Dehazing without Explicit Alignment Estimation. (arXiv:2107.07837v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07837</id>
        <link href="http://arxiv.org/abs/2107.07837"/>
        <updated>2021-07-19T00:49:06.241Z</updated>
        <summary type="html"><![CDATA[To solve the issue of video dehazing, there are two main tasks to attain: how
to align adjacent frames to the reference frame; how to restore the reference
frame. Some papers adopt explicit approaches (e.g., the Markov random field,
optical flow, deformable convolution, 3D convolution) to align neighboring
frames with the reference frame in feature space or image space, they then use
various restoration methods to achieve the final dehazing results. In this
paper, we propose a progressive alignment and restoration method for video
dehazing. The alignment process aligns consecutive neighboring frames stage by
stage without using the optical flow estimation. The restoration process is not
only implemented under the alignment process but also uses a refinement network
to improve the dehazing performance of the whole network. The proposed networks
include four fusion networks and one refinement network. To decrease the
parameters of networks, three fusion networks in the first fusion stage share
the same parameters. Extensive experiments demonstrate that the proposed video
dehazing method achieves outstanding performance against the-state-of-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Runde Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Semi-supervised 3D Super-Resolution and Segmentation with Mixed Adversarial Gaussian Domain Adaptation. (arXiv:2107.07975v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07975</id>
        <link href="http://arxiv.org/abs/2107.07975"/>
        <updated>2021-07-19T00:49:06.224Z</updated>
        <summary type="html"><![CDATA[Optimising the analysis of cardiac structure and function requires accurate
3D representations of shape and motion. However, techniques such as cardiac
magnetic resonance imaging are conventionally limited to acquiring contiguous
cross-sectional slices with low through-plane resolution and potential
inter-slice spatial misalignment. Super-resolution in medical imaging aims to
increase the resolution of images but is conventionally trained on features
from low resolution datasets and does not super-resolve corresponding
segmentations. Here we propose a semi-supervised multi-task generative
adversarial network (Gemini-GAN) that performs joint super-resolution of the
images and their labels using a ground truth of high resolution 3D cines and
segmentations, while an unsupervised variational adversarial mixture
autoencoder (V-AMA) is used for continuous domain adaptation. Our proposed
approach is extensively evaluated on two transnational multi-ethnic populations
of 1,331 and 205 adults respectively, delivering an improvement on state of the
art methods in terms of Dice index, peak signal to noise ratio, and structural
similarity index measure. This framework also exceeds the performance of state
of the art generative domain adaptation models on external validation (Dice
index 0.81 vs 0.74 for the left ventricle). This demonstrates how joint
super-resolution and segmentation, trained on 3D ground-truth data with
cross-domain generalization, enables robust precision phenotyping in diverse
populations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Savioli_N/0/1/0/all/0/1"&gt;Nicolo Savioli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Marvao_A/0/1/0/all/0/1"&gt;Antonio de Marvao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1"&gt;Wenjia Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cook_S/0/1/0/all/0/1"&gt;Stuart A. Cook&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chin_C/0/1/0/all/0/1"&gt;Calvin W.L. Chin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+ORegan_D/0/1/0/all/0/1"&gt;Declan P. O&amp;#x27;Regan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Theoretical Analysis of Granulometry-based Roughness Measures on Cartosat DEMs. (arXiv:2107.07827v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07827</id>
        <link href="http://arxiv.org/abs/2107.07827"/>
        <updated>2021-07-19T00:49:06.208Z</updated>
        <summary type="html"><![CDATA[The study of water bodies such as rivers is an important problem in the
remote sensing community. A meaningful set of quantitative features reflecting
the geophysical properties help us better understand the formation and
evolution of rivers. Typically, river sub-basins are analysed using Cartosat
Digital Elevation Models (DEMs), obtained at regular time epochs. One of the
useful geophysical features of a river sub-basin is that of a roughness measure
on DEMs. However, to the best of our knowledge, there is not much literature
available on theoretical analysis of roughness measures. In this article, we
revisit the roughness measure on DEM data adapted from multiscale
granulometries in mathematical morphology, namely multiscale directional
granulometric index (MDGI). This measure was classically used to obtain
shape-size analysis in greyscale images. In earlier works, MDGIs were
introduced to capture the characteristic surficial roughness of a river
sub-basin along specific directions. Also, MDGIs can be efficiently computed
and are known to be useful features for classification of river sub-basins. In
this article, we provide a theoretical analysis of a MDGI. In particular, we
characterize non-trivial sufficient conditions on the structure of DEMs under
which MDGIs are invariant. These properties are illustrated with some
fictitious DEMs. We also provide connections to a discrete derivative of volume
of a DEM. Based on these connections, we provide intuition as to why a MDGI is
considered a roughness measure. Further, we experimentally illustrate on
Lower-Indus, Wardha, and Barmer river sub-basins that the proposed features
capture the characteristics of the river sub-basin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kannan_N/0/1/0/all/0/1"&gt;Nagajothi Kannan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danda_S/0/1/0/all/0/1"&gt;Sravan Danda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Challa_A/0/1/0/all/0/1"&gt;Aditya Challa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+S_D/0/1/0/all/0/1"&gt;Daya Sagar B S&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unpaired cross-modality educed distillation (CMEDL) applied to CT lung tumor segmentation. (arXiv:2107.07985v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07985</id>
        <link href="http://arxiv.org/abs/2107.07985"/>
        <updated>2021-07-19T00:49:06.202Z</updated>
        <summary type="html"><![CDATA[Accurate and robust segmentation of lung cancers from CTs is needed to more
accurately plan and deliver radiotherapy and to measure treatment response.
This is particularly difficult for tumors located close to mediastium, due to
low soft-tissue contrast. Therefore, we developed a new cross-modality educed
distillation (CMEDL) approach, using unpaired CT and MRI scans, whereby a
teacher MRI network guides a student CT network to extract features that signal
the difference between foreground and background. Our contribution eliminates
two requirements of distillation methods: (i) paired image sets by using an
image to image (I2I) translation and (ii) pre-training of the teacher network
with a large training set by using concurrent training of all networks. Our
framework uses an end-to-end trained unpaired I2I translation, teacher, and
student segmentation networks. Our framework can be combined with any I2I and
segmentation network. We demonstrate our framework's feasibility using 3
segmentation and 2 I2I methods. All networks were trained with 377 CT and 82
T2w MRI from different sets of patients. Ablation tests and different
strategies for incorporating MRI information into CT were performed. Accuracy
was measured using Dice similarity (DSC), surface Dice (sDSC), and Hausdorff
distance at the 95$^{th}$ percentile (HD95). The CMEDL approach was
significantly (p $<$ 0.001) more accurate than non-CMEDL methods,
quantitatively and visually. It produced the highest segmentation accuracy
(sDSC of 0.83 $\pm$ 0.16 and HD95 of 5.20 $\pm$ 6.86mm). CMEDL was also more
accurate than using either pMRI's or the combination of CT's with pMRI's for
segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jue Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rimner_A/0/1/0/all/0/1"&gt;Andreas Rimner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deasy_J/0/1/0/all/0/1"&gt;Joseph O. Deasy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Veeraraghavan_H/0/1/0/all/0/1"&gt;Harini Veeraraghavan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoReMi: First glance at a universal OMR dataset. (arXiv:2107.07786v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07786</id>
        <link href="http://arxiv.org/abs/2107.07786"/>
        <updated>2021-07-19T00:49:06.177Z</updated>
        <summary type="html"><![CDATA[The main challenges of Optical Music Recognition (OMR) come from the nature
of written music, its complexity and the difficulty of finding an appropriate
data representation. This paper provides a first look at DoReMi, an OMR dataset
that addresses these challenges, and a baseline object detection model to
assess its utility. Researchers often approach OMR following a set of small
stages, given that existing data often do not satisfy broader research. We
examine the possibility of changing this tendency by presenting more metadata.
Our approach complements existing research; hence DoReMi allows harmonisation
with two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated
using a music notation software and includes over 6400 printed sheet music
images with accompanying metadata useful in OMR research. Our dataset provides
OMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage
of OMR. We obtain 64% mean average precision (mAP) in object detection using
half of the data. Further work includes re-iterating through the creation
process to satisfy custom OMR models. While we do not assume to have solved the
main challenges in OMR, this dataset opens a new course of discussions that
would ultimately aid that goal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shatri_E/0/1/0/all/0/1"&gt;Elona Shatri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1"&gt;Gy&amp;#xf6;rgy Fazekas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Deep Domain Adaptation and Tiny Object Detection Challenges, Techniques and Datasets. (arXiv:2107.07927v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07927</id>
        <link href="http://arxiv.org/abs/2107.07927"/>
        <updated>2021-07-19T00:49:06.171Z</updated>
        <summary type="html"><![CDATA[This survey paper specially analyzed computer vision-based object detection
challenges and solutions by different techniques. We mainly highlighted object
detection by three different trending strategies, i.e., 1) domain adaptive deep
learning-based approaches (discrepancy-based, Adversarial-based,
Reconstruction-based, Hybrid). We examined general as well as tiny object
detection-related challenges and offered solutions by historical and
comparative analysis. In part 2) we mainly focused on tiny object detection
techniques (multi-scale feature learning, Data augmentation, Training strategy
(TS), Context-based detection, GAN-based detection). In part 3), To obtain
knowledge-able findings, we discussed different object detection methods, i.e.,
convolutions and convolutional neural networks (CNN), pooling operations with
trending types. Furthermore, we explained results with the help of some object
detection algorithms, i.e., R-CNN, Fast R-CNN, Faster R-CNN, YOLO, and SSD,
which are generally considered the base bone of CV, CNN, and OD. We performed
comparative analysis on different datasets such as MS-COCO, PASCAL VOC07,12,
and ImageNet to analyze results and present findings. At the end, we showed
future directions with existing challenges of the field. In the future, OD
methods and models can be analyzed for real-time object detection, tracking
strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muzammul_M/0/1/0/all/0/1"&gt;Muhammed Muzammul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rectifying the Shortcut Learning of Background: Shared Object Concentration for Few-Shot Image Recognition. (arXiv:2107.07746v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07746</id>
        <link href="http://arxiv.org/abs/2107.07746"/>
        <updated>2021-07-19T00:49:06.158Z</updated>
        <summary type="html"><![CDATA[Few-Shot image classification aims to utilize pretrained knowledge learned
from a large-scale dataset to tackle a series of downstream classification
tasks. Typically, each task involves only few training examples from brand-new
categories. This requires the pretraining models to focus on well-generalizable
knowledge, but ignore domain-specific information. In this paper, we observe
that image background serves as a source of domain-specific knowledge, which is
a shortcut for models to learn in the source dataset, but is harmful when
adapting to brand-new classes. To prevent the model from learning this shortcut
knowledge, we propose COSOC, a novel Few-Shot Learning framework, to
automatically figure out foreground objects at both pretraining and evaluation
stage. COSOC is a two-stage algorithm motivated by the observation that
foreground objects from different images within the same class share more
similar patterns than backgrounds. At the pretraining stage, for each class, we
cluster contrastive-pretrained features of randomly cropped image patches, such
that crops containing only foreground objects can be identified by a single
cluster. We then force the pretraining model to focus on found foreground
objects by a fusion sampling strategy; at the evaluation stage, among images in
each training class of any few-shot task, we seek for shared contents and
filter out background. The recognized foreground objects of each class are used
to match foreground of testing images. Extensive experiments tailored to
inductive FSL tasks on two benchmarks demonstrate the state-of-the-art
performance of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xu Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Longhui Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1"&gt;Liangjian Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinrong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lingxi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pose Normalization of Indoor Mapping Datasets Partially Compliant to the Manhattan World Assumption. (arXiv:2107.07778v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07778</id>
        <link href="http://arxiv.org/abs/2107.07778"/>
        <updated>2021-07-19T00:49:06.151Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel pose normalization method for indoor
mapping point clouds and triangle meshes that is robust against large fractions
of the indoor mapping geometries deviating from an ideal Manhattan World
structure. In the case of building structures that contain multiple Manhattan
World systems, the dominant Manhattan World structure supported by the largest
fraction of geometries is determined and used for alignment. In a first step, a
vertical alignment orienting a chosen axis to be orthogonal to horizontal floor
and ceiling surfaces is conducted. Subsequently, a rotation around the
resulting vertical axis is determined that aligns the dataset horizontally with
the coordinate axes. The proposed method is evaluated quantitatively against
several publicly available indoor mapping datasets. Our implementation of the
proposed procedure along with code for reproducing the evaluation will be made
available to the public upon acceptance for publication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hubner_P/0/1/0/all/0/1"&gt;Patrick H&amp;#xfc;bner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1"&gt;Martin Weinmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wursthorn_S/0/1/0/all/0/1"&gt;Sven Wursthorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hinz_S/0/1/0/all/0/1"&gt;Stefan Hinz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Predictive Coding for Anomaly Detection. (arXiv:2107.07820v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07820</id>
        <link href="http://arxiv.org/abs/2107.07820"/>
        <updated>2021-07-19T00:49:06.129Z</updated>
        <summary type="html"><![CDATA[Reliable detection of anomalies is crucial when deploying machine learning
models in practice, but remains challenging due to the lack of labeled data. To
tackle this challenge, contrastive learning approaches are becoming
increasingly popular, given the impressive results they have achieved in
self-supervised representation learning settings. However, while most existing
contrastive anomaly detection and segmentation approaches have been applied to
images, none of them can use the contrastive losses directly for both anomaly
detection and segmentation. In this paper, we close this gap by making use of
the Contrastive Predictive Coding model (arXiv:1807.03748). We show that its
patch-wise contrastive loss can directly be interpreted as an anomaly score,
and how this allows for the creation of anomaly segmentation masks. The
resulting model achieves promising results for both anomaly detection and
segmentation on the challenging MVTec-AD dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1"&gt;Puck de Haan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1"&gt;Sindy L&amp;#xf6;we&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Instance Learning with Auxiliary Task Weighting for Multiple Myeloma Classification. (arXiv:2107.07805v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07805</id>
        <link href="http://arxiv.org/abs/2107.07805"/>
        <updated>2021-07-19T00:49:06.122Z</updated>
        <summary type="html"><![CDATA[Whole body magnetic resonance imaging (WB-MRI) is the recommended modality
for diagnosis of multiple myeloma (MM). WB-MRI is used to detect sites of
disease across the entire skeletal system, but it requires significant
expertise and is time-consuming to report due to the great number of images. To
aid radiological reading, we propose an auxiliary task-based multiple instance
learning approach (ATMIL) for MM classification with the ability to localize
sites of disease. This approach is appealing as it only requires patient-level
annotations where an attention mechanism is used to identify local regions with
active disease. We borrow ideas from multi-task learning and define an
auxiliary task with adaptive reweighting to support and improve learning
efficiency in the presence of data scarcity. We validate our approach on both
synthetic and real multi-center clinical data. We show that the MIL attention
module provides a mechanism to localize bone regions while the adaptive
reweighting of the auxiliary task considerably improves the performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qaiser_T/0/1/0/all/0/1"&gt;Talha Qaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winzeck_S/0/1/0/all/0/1"&gt;Stefan Winzeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barfoot_T/0/1/0/all/0/1"&gt;Theodore Barfoot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barwick_T/0/1/0/all/0/1"&gt;Tara Barwick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doran_S/0/1/0/all/0/1"&gt;Simon J. Doran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1"&gt;Martin F. Kaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wedlake_L/0/1/0/all/0/1"&gt;Linda Wedlake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tunariu_N/0/1/0/all/0/1"&gt;Nina Tunariu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koh_D/0/1/0/all/0/1"&gt;Dow-Mu Koh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Messiou_C/0/1/0/all/0/1"&gt;Christina Messiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rockall_A/0/1/0/all/0/1"&gt;Andrea Rockall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeXtQSM -- A complete deep learning pipeline for data-consistent quantitative susceptibility mapping trained with hybrid data. (arXiv:2107.07752v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07752</id>
        <link href="http://arxiv.org/abs/2107.07752"/>
        <updated>2021-07-19T00:49:06.107Z</updated>
        <summary type="html"><![CDATA[Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great
potential in recent years, outperforming traditional non-learning approaches in
speed and accuracy. However, many of the current deep learning approaches are
not data consistent, require in vivo training data or do not solve all steps of
the QSM processing pipeline. Here we aim to overcome these limitations and
developed a framework to solve the QSM processing steps jointly. We developed a
new hybrid training data generation method that enables the end-to-end training
for solving background field correction and dipole inversion in a
data-consistent fashion using a variational network that combines the QSM model
term and a learned regularizer. We demonstrate that NeXtQSM overcomes the
limitations of previous model-agnostic deep learning methods and show that
NeXtQSM offers a complete deep learning based pipeline for computing robust,
fast and accurate quantitative susceptibility maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cognolato_F/0/1/0/all/0/1"&gt;Francesco Cognolato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+OBrien_K/0/1/0/all/0/1"&gt;Kieran O&amp;#x27;Brien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jin_J/0/1/0/all/0/1"&gt;Jin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Robinson_S/0/1/0/all/0/1"&gt;Simon Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Laun_F/0/1/0/all/0/1"&gt;Frederik B. Laun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barth_M/0/1/0/all/0/1"&gt;Markus Barth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bollmann_S/0/1/0/all/0/1"&gt;Steffen Bollmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting generative self-supervised learning for the assessment of biological images with lack of annotations: a COVID-19 case-study. (arXiv:2107.07761v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07761</id>
        <link href="http://arxiv.org/abs/2107.07761"/>
        <updated>2021-07-19T00:49:06.096Z</updated>
        <summary type="html"><![CDATA[Computer-aided analysis of biological images typically requires extensive
training on large-scale annotated datasets, which is not viable in many
situations. In this paper we present GAN-DL, a Discriminator Learner based on
the StyleGAN2 architecture, which we employ for self-supervised image
representation learning in the case of fluorescent biological images. We show
that Wasserstein Generative Adversarial Networks combined with linear Support
Vector Machines enable high-throughput compound screening based on raw images.
We demonstrate this by classifying active and inactive compounds tested for the
inhibition of SARS-CoV-2 infection in VERO and HRCE cell lines. In contrast to
previous methods, our deep learning based approach does not require any
annotation besides the one that is normally collected during the sample
preparation process. We test our technique on the RxRx19a Sars-CoV-2 image
collection. The dataset consists of fluorescent images that were generated to
assess the ability of regulatory-approved or in late-stage clinical trials
compound to modulate the in vitro infection from SARS-CoV-2 in both VERO and
HRCE cell lines. We show that our technique can be exploited not only for
classification tasks, but also to effectively derive a dose response curve for
the tested treatments, in a self-supervised manner. Lastly, we demonstrate its
generalization capabilities by successfully addressing a zero-shot learning
task, consisting in the categorization of four different cell types of the
RxRx1 fluorescent images collection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mascolini_A/0/1/0/all/0/1"&gt;Alessio Mascolini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cardamone_D/0/1/0/all/0/1"&gt;Dario Cardamone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ponzio_F/0/1/0/all/0/1"&gt;Francesco Ponzio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cataldo_S/0/1/0/all/0/1"&gt;Santa Di Cataldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ficarra_E/0/1/0/all/0/1"&gt;Elisa Ficarra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optical Inspection of the Silicon Micro-strip Sensors for the CBM Experiment employing Artificial Intelligence. (arXiv:2107.07714v1 [physics.ins-det])]]></title>
        <id>http://arxiv.org/abs/2107.07714</id>
        <link href="http://arxiv.org/abs/2107.07714"/>
        <updated>2021-07-19T00:49:06.076Z</updated>
        <summary type="html"><![CDATA[Optical inspection of 1191 silicon micro-strip sensors was performed using a
custom made optical inspection setup, employing a machine-learning based
approach for the defect analysis and subsequent quality assurance. Furthermore,
metrological control of the sensor's surface was performed. In this manuscript,
we present the analysis of various sensor surface defects. Among these are
implant breaks, p-stop breaks, aluminium strip opens, aluminium strip shorts,
surface scratches, double metallization layer defects, passivation layer
defects, bias resistor defects as well as dust particle identification. The
defect detection was done using the application of Convolutional Deep Neural
Networks (CDNNs). From this, defective strips and defect clusters were
identified, as well as a 2D map of the defects using their geometrical
positions on the sensor was performed. Based on the total number of defects
found on the sensor's surface, a method for the estimation of sensor's overall
quality grade and quality score was proposed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Lavrik_E/0/1/0/all/0/1"&gt;E. Lavrik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Shiroya_M/0/1/0/all/0/1"&gt;M. Shiroya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schmidt_H/0/1/0/all/0/1"&gt;H.R. Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Toia_A/0/1/0/all/0/1"&gt;A. Toia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Heuser_J/0/1/0/all/0/1"&gt;J.M. Heuser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07791</id>
        <link href="http://arxiv.org/abs/2107.07791"/>
        <updated>2021-07-19T00:49:06.068Z</updated>
        <summary type="html"><![CDATA[We present a novel learning-based approach to graph representations of road
networks employing state-of-the-art graph convolutional neural networks. Our
approach is applied to realistic road networks of 17 cities from Open Street
Map. While edge features are crucial to generate descriptive graph
representations of road networks, graph convolutional networks usually rely on
node features only. We show that the highly representative edge features can
still be integrated into such networks by applying a line graph transformation.
We also propose a method for neighborhood sampling based on a topological
neighborhood composed of both local and global neighbors. We compare the
performance of learning representations using different types of neighborhood
aggregation functions in transductive and inductive tasks and in supervised and
unsupervised learning. Furthermore, we propose a novel aggregation approach,
Graph Attention Isomorphism Network, GAIN. Our results show that GAIN
outperforms state-of-the-art methods on the road type classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1"&gt;Zahra Gharaee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1"&gt;Shreyas Kowshik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1"&gt;Oliver Stromann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1"&gt;Michael Felsberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning Framework for Remote Heart Rate Estimation Using Spatiotemporal Augmentation. (arXiv:2107.07695v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07695</id>
        <link href="http://arxiv.org/abs/2107.07695"/>
        <updated>2021-07-19T00:49:06.044Z</updated>
        <summary type="html"><![CDATA[Recent supervised deep learning methods have shown that heart rate can be
measured remotely using facial videos. However, the performance of these
supervised method are dependent on the availability of large-scale labelled
data and they have been limited to 2D deep learning architectures that do not
fully exploit the 3D spatiotemporal information. To solve this problem, we
present a novel 3D self-supervised spatiotemporal learning framework for remote
HR estimation on facial videos. Concretely, we propose a landmark-based spatial
augmentation which splits the face into several informative parts based on the
Shafer's dichromatic reflection model and a novel sparsity-based temporal
augmentation exploiting Nyquist-Shannon sampling theorem to enhance the signal
modelling ability. We evaluated our method on 3 public datasets and
outperformed other self-supervised methods and achieved competitive accuracy
with the state-of-the-art supervised methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_E/0/1/0/all/0/1"&gt;Euijoon Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jinman Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Appearance-Invariant Topometric Localization with New Place Awareness. (arXiv:2107.07707v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.07707</id>
        <link href="http://arxiv.org/abs/2107.07707"/>
        <updated>2021-07-19T00:49:06.038Z</updated>
        <summary type="html"><![CDATA[Probabilistic state-estimation approaches offer a principled foundation for
designing localization systems, because they naturally integrate sequences of
imperfect motion and exteroceptive sensor data. Recently, probabilistic
localization systems utilizing appearance-invariant visual place recognition
(VPR) methods as the primary exteroceptive sensor have demonstrated
state-of-the-art performance in the presence of substantial appearance change.
However, existing systems 1) do not fully utilize odometry data within the
motion models, and 2) are unable to handle route deviations, due to the
assumption that query traverses exactly repeat the mapping traverse. To address
these shortcomings, we present a new probabilistic topometric localization
system which incorporates full 3-dof odometry into the motion model and
furthermore, adds an "off-map" state within the state-estimation framework,
allowing query traverses which feature significant route detours from the
reference map to be successfully localized. We perform extensive evaluation on
multiple query traverses from the Oxford RobotCar dataset exhibiting both
significant appearance change and deviations from routes previously traversed.
In particular, we evaluate performance on two practically relevant localization
tasks: loop closure detection and global localization. Our approach achieves
major performance improvements over both existing and improved state-of-the-art
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Ming Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1"&gt;Tobias Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunderhauf_N/0/1/0/all/0/1"&gt;Niko S&amp;#xfc;nderhauf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1"&gt;Michael Milford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DANCE: DAta-Network Co-optimization for Efficient Segmentation Model Training and Inference. (arXiv:2107.07706v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07706</id>
        <link href="http://arxiv.org/abs/2107.07706"/>
        <updated>2021-07-19T00:49:06.031Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation for scene understanding is nowadays widely demanded,
raising significant challenges for the algorithm efficiency, especially its
applications on resource-limited platforms. Current segmentation models are
trained and evaluated on massive high-resolution scene images ("data level")
and suffer from the expensive computation arising from the required multi-scale
aggregation("network level"). In both folds, the computational and energy costs
in training and inference are notable due to the often desired large input
resolutions and heavy computational burden of segmentation models. To this end,
we propose DANCE, general automated DAta-Network Co-optimization for Efficient
segmentation model training and inference. Distinct from existing efficient
segmentation approaches that focus merely on light-weight network design, DANCE
distinguishes itself as an automated simultaneous data-network co-optimization
via both input data manipulation and network architecture slimming.
Specifically, DANCE integrates automated data slimming which adaptively
downsamples/drops input images and controls their corresponding contribution to
the training loss guided by the images' spatial complexity. Such a downsampling
operation, in addition to slimming down the cost associated with the input size
directly, also shrinks the dynamic range of input object and context scales,
therefore motivating us to also adaptively slim the network to match the
downsampled data. Extensive experiments and ablating studies (on four SOTA
segmentation models with three popular segmentation datasets under two training
settings) demonstrate that DANCE can achieve "all-win" towards efficient
segmentation(reduced training cost, less expensive inference, and better mean
Intersection-over-Union (mIoU)).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chaojian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wuyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yuchen Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yonggan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yingyan Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Violence Detection Using CNN-LSTM. (arXiv:2107.07578v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07578</id>
        <link href="http://arxiv.org/abs/2107.07578"/>
        <updated>2021-07-19T00:49:06.004Z</updated>
        <summary type="html"><![CDATA[Violence rates however have been brought down about 57% during the span of
the past 4 decades yet it doesn't change the way that the demonstration of
violence actually happens, unseen by the law. Violence can be mass controlled
sometimes by higher authorities, however, to hold everything in line one must
"Microgovern" over each movement occurring in every road of each square. To
address the butterfly effects impact in our setting, I made a unique model and
a theorized system to handle the issue utilizing deep learning. The model takes
the input of the CCTV video feeds and after drawing inference, recognizes if a
violent movement is going on. And hypothesized architecture aims towards
probability-driven computation of video feeds and reduces overhead from naively
computing for every CCTV video feeds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1"&gt;Mann Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Depth Estimation from Monocular Images and Sparse radar using Deep Ordinal Regression Network. (arXiv:2107.07596v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07596</id>
        <link href="http://arxiv.org/abs/2107.07596"/>
        <updated>2021-07-19T00:49:05.988Z</updated>
        <summary type="html"><![CDATA[We integrate sparse radar data into a monocular depth estimation model and
introduce a novel preprocessing method for reducing the sparseness and limited
field of view provided by radar. We explore the intrinsic error of different
radar modalities and show our proposed method results in more data points with
reduced error. We further propose a novel method for estimating dense depth
maps from monocular 2D images and sparse radar measurements using deep learning
based on the deep ordinal regression network by Fu et al. Radar data are
integrated by first converting the sparse 2D points to a height-extended 3D
measurement and then including it into the network using a late fusion
approach. Experiments are conducted on the nuScenes dataset. Our experiments
demonstrate state-of-the-art performance in both day and night scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lo_C/0/1/0/all/0/1"&gt;Chen-Chou Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vandewalle_P/0/1/0/all/0/1"&gt;Patrick Vandewalle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OdoViz: A 3D Odometry Visualization and Processing Tool. (arXiv:2107.07557v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07557</id>
        <link href="http://arxiv.org/abs/2107.07557"/>
        <updated>2021-07-19T00:49:05.971Z</updated>
        <summary type="html"><![CDATA[OdoViz is a reactive web-based tool for 3D visualization and processing of
autonomous vehicle datasets designed to support common tasks in visual place
recognition research. The system includes functionality for loading,
inspecting, visualizing, and processing GPS/INS poses, point clouds and camera
images. It supports a number of commonly used driving datasets and can be
adapted to load custom datasets with minimal effort. OdoViz's design consists
of a slim server to serve the datasets coupled with a rich client frontend.
This design supports multiple deployment configurations including single user
stand-alone installations, research group installations serving datasets
internally across a lab, or publicly accessible web-frontends for providing
online interfaces for exploring and interacting with datasets. The tool allows
viewing complete vehicle trajectories traversed at multiple different time
periods simultaneously, facilitating tasks such as sub-sampling, comparing and
finding pose correspondences both across and within sequences. This
significantly reduces the effort required in creating subsets of data from
existing datasets for machine learning tasks. Further to the above, the system
also supports adding custom extensions and plugins to extend the capabilities
of the software for other potential data management, visualization and
processing tasks. The platform has been open-sourced to promote its use and
encourage further contributions from the research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1"&gt;Saravanabalagi Ramachandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1"&gt;John McDonald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised 3D Hand-Object Pose Estimation via Pose Dictionary Learning. (arXiv:2107.07676v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07676</id>
        <link href="http://arxiv.org/abs/2107.07676"/>
        <updated>2021-07-19T00:49:05.963Z</updated>
        <summary type="html"><![CDATA[3D hand-object pose estimation is an important issue to understand the
interaction between human and environment. Current hand-object pose estimation
methods require detailed 3D labels, which are expensive and labor-intensive. To
tackle the problem of data collection, we propose a semi-supervised 3D
hand-object pose estimation method with two key techniques: pose dictionary
learning and an object-oriented coordinate system. The proposed pose dictionary
learning module can distinguish infeasible poses by reconstruction error,
enabling unlabeled data to provide supervision signals. The proposed
object-oriented coordinate system can make 3D estimations equivariant to the
camera perspective. Experiments are conducted on FPHA and HO-3D datasets. Our
method reduces estimation error by 19.5% / 24.9% for hands/objects compared to
straightforward use of labeled data on FPHA and outperforms several baseline
methods. Extensive experiments also validate the robustness of the proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zida Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07647</id>
        <link href="http://arxiv.org/abs/2107.07647"/>
        <updated>2021-07-19T00:49:05.921Z</updated>
        <summary type="html"><![CDATA[A novel energy-efficient edge computing paradigm is proposed for real-time
deep learning-based image upsampling applications. State-of-the-art deep
learning solutions for image upsampling are currently trained using either
resize or sub-pixel convolution to learn kernels that generate high fidelity
images with minimal artifacts. However, performing inference with these learned
convolution kernels requires memory-intensive feature map transformations that
dominate time and energy costs in real-time applications. To alleviate this
pressure on memory bandwidth, we confine the use of resize or sub-pixel
convolution to training in the cloud by transforming learned convolution
kernels to deconvolution kernels before deploying them for inference as a
functionally equivalent deconvolution. These kernel transformations, intended
as a one-time cost when shifting from training to inference, enable a systems
designer to use each algorithm in their optimal context by preserving the image
fidelity learned when training in the cloud while minimizing data transfer
penalties during inference at the edge. We also explore existing variants of
deconvolution inference algorithms and introduce a novel variant for
consideration. We analyze and compare the inference properties of
convolution-based upsampling algorithms using a quantitative model of incurred
time and energy costs and show that using deconvolution for inference at the
edge improves both system latency and energy efficiency when compared to their
sub-pixel or resize convolution counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1"&gt;Ian Colbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1"&gt;Ken Kreutz-Delgado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Srinjoy Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Face Recognition System for Remote Employee Tracking. (arXiv:2107.07576v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07576</id>
        <link href="http://arxiv.org/abs/2107.07576"/>
        <updated>2021-07-19T00:49:05.915Z</updated>
        <summary type="html"><![CDATA[During the COVID-19 pandemic, most of the human-to-human interactions have
been stopped. To mitigate the spread of deadly coronavirus, many offices took
the initiative so that the employees can work from home. But, tracking the
employees and finding out if they are really performing what they were supposed
to turn out to be a serious challenge for all the companies and organizations
who are facilitating "Work From Home". To deal with the challenge effectively,
we came up with a solution to track the employees with face recognition. We
have been testing this system experimentally for our office. To train the face
recognition module, we used FaceNet with KNN using the Labeled Faces in the
Wild (LFW) dataset and achieved 97.8% accuracy. We integrated the trained model
into our central system, where the employees log their time. In this paper, we
discuss in brief the system we have been experimenting with and the pros and
cons of the system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1"&gt;Mohammad Sabik Irbaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1"&gt;MD Abdullah Al Nasim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferdous_R/0/1/0/all/0/1"&gt;Refat E Ferdous&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pivot Through English: Reliably Answering Multilingual Questions without Document Retrieval. (arXiv:2012.14094v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14094</id>
        <link href="http://arxiv.org/abs/2012.14094"/>
        <updated>2021-07-19T00:49:05.908Z</updated>
        <summary type="html"><![CDATA[Existing methods for open-retrieval question answering in lower resource
languages (LRLs) lag significantly behind English. They not only suffer from
the shortcomings of non-English document retrieval, but are reliant on
language-specific supervision for either the task or translation. We formulate
a task setup more realistic to available resources, that circumvents document
retrieval to reliably transfer knowledge from English to lower resource
languages. Assuming a strong English question answering model or database, we
compare and analyze methods that pivot through English: to map foreign queries
to English and then English answers back to target language answers. Within
this task setup we propose Reranked Multilingual Maximal Inner Product Search
(RM-MIPS), akin to semantic similarity retrieval over the English training set
with reranking, which outperforms the strongest baselines by 2.7% on XQuAD and
6.2% on MKQA. Analysis demonstrates the particular efficacy of this strategy
over state-of-the-art alternatives in challenging settings: low-resource
languages, with extensive distractor data and query distribution misalignment.
Circumventing retrieval, our analysis shows this approach offers rapid answer
generation to almost any language off-the-shelf, without the need for any
additional training data in the target language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Montero_I/0/1/0/all/0/1"&gt;Ivan Montero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1"&gt;Shayne Longpre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1"&gt;Ni Lao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Andrew J. Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DuBois_C/0/1/0/all/0/1"&gt;Christopher DuBois&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation. (arXiv:2101.00419v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00419</id>
        <link href="http://arxiv.org/abs/2101.00419"/>
        <updated>2021-07-19T00:49:05.844Z</updated>
        <summary type="html"><![CDATA[We present Knowledge Enhanced Multimodal BART (KM-BART), which is a
Transformer-based sequence-to-sequence model capable of reasoning about
commonsense knowledge from multimodal inputs of images and texts. We adapt the
generative BART architecture to a multimodal model with visual and textual
inputs. We further develop novel pretraining tasks to improve the model
performance on the Visual Commonsense Generation (VCG) task. In particular, our
pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model
performance on the VCG task by leveraging commonsense knowledge from a large
language model pretrained on external commonsense knowledge graphs. To the best
of our knowledge, we are the first to propose a dedicated task for improving
model performance on the VCG task. Experimental results show that our model
reaches state-of-the-art performance on the VCG task by applying these novel
pretraining tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1"&gt;Yiran Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zai Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zhao Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakemeyer_G/0/1/0/all/0/1"&gt;Gerhard Lakemeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yunpu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1"&gt;Roger Wattenhofer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SlimIPL: Language-Model-Free Iterative Pseudo-Labeling. (arXiv:2010.11524v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11524</id>
        <link href="http://arxiv.org/abs/2010.11524"/>
        <updated>2021-07-19T00:49:05.837Z</updated>
        <summary type="html"><![CDATA[Recent results in end-to-end automatic speech recognition have demonstrated
the efficacy of pseudo-labeling for semi-supervised models trained both with
Connectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq)
losses. Iterative Pseudo-Labeling (IPL), which continuously trains a single
model using pseudo-labels iteratively re-generated as the model learns, has
been shown to further improve performance in ASR. We improve upon the IPL
algorithm: as the model learns, we propose to iteratively re-generate
transcriptions with hard labels (the most probable tokens), that is, without a
language model. We call this approach Language-Model-Free IPL (slimIPL) and
give a resultant training setup for low-resource settings with CTC-based
models. slimIPL features a dynamic cache for pseudo-labels which reduces
sensitivity to changes in relabeling hyperparameters and results in improves
training stability. slimIPL is also highly-efficient and requires 3.5-4x fewer
computational resources to converge than other state-of-the-art
semi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL
is competitive with self-supervised approaches, and is state-of-the-art with
100 hours of labeled audio without the use of a language model both at test
time and during pseudo-label generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1"&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiantong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1"&gt;Jacob Kahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1"&gt;Gabriel Synnaeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1"&gt;Ronan Collobert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Race, Racism, and Anti-Racism in NLP. (arXiv:2106.11410v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11410</id>
        <link href="http://arxiv.org/abs/2106.11410"/>
        <updated>2021-07-19T00:49:05.830Z</updated>
        <summary type="html"><![CDATA[Despite inextricable ties between race and language, little work has
considered race in NLP research and development. In this work, we survey 79
papers from the ACL anthology that mention race. These papers reveal various
types of race-related bias in all stages of NLP model development, highlighting
the need for proactive consideration of how NLP systems can uphold racial
hierarchies. However, persistent gaps in research on race and NLP remain: race
has been siloed as a niche topic and remains ignored in many NLP tasks; most
work operationalizes race as a fixed single-dimensional variable with a
ground-truth label, which risks reinforcing differences produced by historical
racism; and the voices of historically marginalized people are nearly absent in
NLP literature. By identifying where and how NLP literature has and has not
considered race, especially in comparison to related fields, our work calls for
inclusion and racial justice in NLP research practices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1"&gt;Anjalie Field&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blodgett_S/0/1/0/all/0/1"&gt;Su Lin Blodgett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waseem_Z/0/1/0/all/0/1"&gt;Zeerak Waseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal-aware Language Representation Learning From Crowdsourced Labels. (arXiv:2107.07958v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07958</id>
        <link href="http://arxiv.org/abs/2107.07958"/>
        <updated>2021-07-19T00:49:05.801Z</updated>
        <summary type="html"><![CDATA[Learning effective language representations from crowdsourced labels is
crucial for many real-world machine learning tasks. A challenging aspect of
this problem is that the quality of crowdsourced labels suffer high intra- and
inter-observer variability. Since the high-capacity deep neural networks can
easily memorize all disagreements among crowdsourced labels, directly applying
existing supervised language representation learning algorithms may yield
suboptimal solutions. In this paper, we propose \emph{TACMA}, a
\underline{t}emporal-\underline{a}ware language representation learning
heuristic for \underline{c}rowdsourced labels with \underline{m}ultiple
\underline{a}nnotators. The proposed approach (1) explicitly models the
intra-observer variability with attention mechanism; (2) computes and
aggregates per-sample confidence scores from multiple workers to address the
inter-observer disagreements. The proposed heuristic is extremely easy to
implement in around 5 lines of code. The proposed heuristic is evaluated on
four synthetic and four real-world data sets. The results show that our
approach outperforms a wide range of state-of-the-art baselines in terms of
prediction accuracy and AUC. To encourage the reproducible results, we make our
code publicly available at \url{https://github.com/CrowdsourcingMining/TACMA}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1"&gt;Yang Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiao Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenbiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zitao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[POS tagging, lemmatization and dependency parsing of West Frisian. (arXiv:2107.07974v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07974</id>
        <link href="http://arxiv.org/abs/2107.07974"/>
        <updated>2021-07-19T00:49:05.794Z</updated>
        <summary type="html"><![CDATA[We present a lemmatizer/POS-tagger/dependency parser for West Frisian using a
corpus of 44,714 words in 3,126 sentences that were annotated according to the
guidelines of Universal Dependency version 2. POS tags were assigned to words
by using a Dutch POS tagger that was applied to a literal word-by-word
translation, or to sentences of a Dutch parallel text. Best results were
obtained when using literal translations that were created by using the Frisian
translation program Oersetter. Morphologic and syntactic annotations were
generated on the basis of a literal Dutch translation as well. The performance
of the lemmatizer/tagger/annotator when it was trained using default parameters
was compared to the performance that was obtained when using the parameter
values that were used for training the LassySmall UD 2.5 corpus. A significant
improvement was found for `lemma'. The Frisian lemmatizer/PoS tagger/dependency
parser is released as a web app and as a web service.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heeringa_W/0/1/0/all/0/1"&gt;Wilbert Heeringa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouma_G/0/1/0/all/0/1"&gt;Gosse Bouma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofman_M/0/1/0/all/0/1"&gt;Martha Hofman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drenth_E/0/1/0/all/0/1"&gt;Eduard Drenth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wijffels_J/0/1/0/all/0/1"&gt;Jan Wijffels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velde_H/0/1/0/all/0/1"&gt;Hans Van de Velde&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Know Deeper: Knowledge-Conversation Cyclic Utilization Mechanism for Open-domain Dialogue Generation. (arXiv:2107.07771v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07771</id>
        <link href="http://arxiv.org/abs/2107.07771"/>
        <updated>2021-07-19T00:49:05.774Z</updated>
        <summary type="html"><![CDATA[End-to-End intelligent neural dialogue systems suffer from the problems of
generating inconsistent and repetitive responses. Existing dialogue models pay
attention to unilaterally incorporating personal knowledge into the dialog
while ignoring the fact that incorporating the personality-related conversation
information into personal knowledge taken as the bilateral information flow
boosts the quality of the subsequent conversation. Besides, it is indispensable
to control personal knowledge utilization over the conversation level. In this
paper, we propose a conversation-adaption multi-view persona aware response
generation model that aims at enhancing conversation consistency and
alleviating the repetition from two folds. First, we consider conversation
consistency from multiple views. From the view of the persona profile, we
design a novel interaction module that not only iteratively incorporates
personalized knowledge into each turn conversation but also captures the
personality-related information from conversation to enhance personalized
knowledge semantic representation. From the view of speaking style, we
introduce the speaking style vector and feed it into the decoder to keep the
speaking style consistency. To avoid conversation repetition, we devise a
coverage mechanism to keep track of the activation of personal knowledge
utilization. Experiments on both automatic and human evaluation verify the
superiority of our model over previous models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yajing Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yue Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1"&gt;Luxi Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuqiang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiangpeng Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Vulnerable Are Automatic Fake News Detection Methods to Adversarial Attacks?. (arXiv:2107.07970v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07970</id>
        <link href="http://arxiv.org/abs/2107.07970"/>
        <updated>2021-07-19T00:49:05.715Z</updated>
        <summary type="html"><![CDATA[As the spread of false information on the internet has increased dramatically
in recent years, more and more attention is being paid to automated fake news
detection. Some fake news detection methods are already quite successful.
Nevertheless, there are still many vulnerabilities in the detection algorithms.
The reason for this is that fake news publishers can structure and formulate
their texts in such a way that a detection algorithm does not expose this text
as fake news. This paper shows that it is possible to automatically attack
state-of-the-art models that have been trained to detect Fake News, making
these vulnerable. For this purpose, corresponding models were first trained
based on a dataset. Then, using Text-Attack, an attempt was made to manipulate
the trained models in such a way that previously correctly identified fake news
was classified as true news. The results show that it is possible to
automatically bypass Fake News detection mechanisms, leading to implications
concerning existing policy initiatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koenders_C/0/1/0/all/0/1"&gt;Camille Koenders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filla_J/0/1/0/all/0/1"&gt;Johannes Filla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1"&gt;Nicolai Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woloszyn_V/0/1/0/all/0/1"&gt;Vinicius Woloszyn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Rich Syntax for Better Knowledge Base Question Answering. (arXiv:2107.07940v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07940</id>
        <link href="http://arxiv.org/abs/2107.07940"/>
        <updated>2021-07-19T00:49:05.695Z</updated>
        <summary type="html"><![CDATA[Recent studies on Knowledge Base Question Answering (KBQA) have shown great
progress on this task via better question understanding. Previous works for
encoding questions mainly focus on the word sequences, but seldom consider the
information from syntactic trees.In this paper, we propose an approach to learn
syntax-based representations for KBQA. First, we encode path-based syntax by
considering the shortest dependency paths between keywords. Then, we propose
two encoding strategies to mode the information of whole syntactic trees to
obtain tree-based syntax. Finally, we combine both path-based and tree-based
syntax representations for KBQA. We conduct extensive experiments on a widely
used benchmark dataset and the experimental results show that our syntax-aware
systems can make full use of syntax information in different settings and
achieve state-of-the-art performance of KBQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengju Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Yonghui Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Muhua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenliang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Min Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-labelling Enhanced Media Bias Detection. (arXiv:2107.07705v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07705</id>
        <link href="http://arxiv.org/abs/2107.07705"/>
        <updated>2021-07-19T00:49:05.680Z</updated>
        <summary type="html"><![CDATA[Leveraging unlabelled data through weak or distant supervision is a
compelling approach to developing more effective text classification models.
This paper proposes a simple but effective data augmentation method, which
leverages the idea of pseudo-labelling to select samples from noisy distant
supervision annotation datasets. The result shows that the proposed method
improves the accuracy of biased news detection models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1"&gt;Qin Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1"&gt;Brian Mac Namee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1"&gt;Ruihai Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.07002</id>
        <link href="http://arxiv.org/abs/2107.07002"/>
        <updated>2021-07-19T00:49:05.656Z</updated>
        <summary type="html"><![CDATA[The world of empirical machine learning (ML) strongly relies on benchmarks in
order to determine the relative effectiveness of different algorithms and
methods. This paper proposes the notion of "a benchmark lottery" that describes
the overall fragility of the ML benchmarking process. The benchmark lottery
postulates that many factors, other than fundamental algorithmic superiority,
may lead to a method being perceived as superior. On multiple benchmark setups
that are prevalent in the ML community, we show that the relative performance
of algorithms may be altered significantly simply by choosing different
benchmark tasks, highlighting the fragility of the current paradigms and
potential fallacious interpretation derived from benchmarking ML methods. Given
that every benchmark makes a statement about what it perceives to be important,
we argue that this might lead to biased progress in the community. We discuss
the implications of the observed phenomena and provide recommendations on
mitigating them using multiple machine learning domains and communities as use
cases, including natural language processing, computer vision, information
retrieval, recommender systems, and reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1"&gt;Alexey A. Gritsenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhe Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1"&gt;Fernando Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Level Contrastive Learning for Few-Shot Problems. (arXiv:2107.07608v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07608</id>
        <link href="http://arxiv.org/abs/2107.07608"/>
        <updated>2021-07-19T00:49:05.614Z</updated>
        <summary type="html"><![CDATA[Contrastive learning is a discriminative approach that aims at grouping
similar samples closer and diverse samples far from each other. It it an
efficient technique to train an encoder generating distinguishable and
informative representations, and it may even increase the encoder's
transferability. Most current applications of contrastive learning benefit only
a single representation from the last layer of an encoder.In this paper, we
propose a multi-level contrasitive learning approach which applies contrastive
losses at different layers of an encoder to learn multiple representations from
the encoder. Afterward, an ensemble can be constructed to take advantage of the
multiple representations for the downstream tasks. We evaluated the proposed
method on few-shot learning problems and conducted experiments using the
mini-ImageNet and the tiered-ImageNet datasets. Our model achieved the new
state-of-the-art results for both datasets, comparing to previous regular,
ensemble, and contrastive learing (single-level) based approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? A Comprehensive Assessment for Catalan. (arXiv:2107.07903v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07903</id>
        <link href="http://arxiv.org/abs/2107.07903"/>
        <updated>2021-07-19T00:49:05.605Z</updated>
        <summary type="html"><![CDATA[Multilingual language models have been a crucial breakthrough as they
considerably reduce the need of data for under-resourced languages.
Nevertheless, the superiority of language-specific models has already been
proven for languages having access to large amounts of data. In this work, we
focus on Catalan with the aim to explore to what extent a medium-sized
monolingual language model is competitive with state-of-the-art large
multilingual models. For this, we: (1) build a clean, high-quality textual
Catalan corpus (CaText), the largest to date (but only a fraction of the usual
size of the previous work in monolingual language models), (2) train a
Transformer-based language model for Catalan (BERTa), and (3) devise a thorough
evaluation in a diversity of settings, comprising a complete array of
downstream tasks, namely, Part of Speech Tagging, Named Entity Recognition and
Classification, Text Classification, Question Answering, and Semantic Textual
Similarity, with most of the corresponding datasets being created ex novo. The
result is a new benchmark, the Catalan Language Understanding Benchmark (CLUB),
which we publish as an open resource, together with the clean textual corpus,
the language model, and the cleaning pipeline. Using state-of-the-art
multilingual models and a monolingual model trained only on Wikipedia as
baselines, we consistently observe the superiority of our model across tasks
and settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1"&gt;Jordi Armengol-Estap&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1"&gt;Casimiro Pio Carrino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1"&gt;Carlos Rodriguez-Penagos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonet_O/0/1/0/all/0/1"&gt;Ona de Gibert Bonet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1"&gt;Carme Armentano-Oller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1"&gt;Aitor Gonzalez-Agirre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melero_M/0/1/0/all/0/1"&gt;Maite Melero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1"&gt;Marta Villegas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Understand Child-directed and Adult-directed Speech. (arXiv:2005.02721v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.02721</id>
        <link href="http://arxiv.org/abs/2005.02721"/>
        <updated>2021-07-19T00:49:05.581Z</updated>
        <summary type="html"><![CDATA[Speech directed to children differs from adult-directed speech in linguistic
aspects such as repetition, word choice, and sentence length, as well as in
aspects of the speech signal itself, such as prosodic and phonemic variation.
Human language acquisition research indicates that child-directed speech helps
language learners. This study explores the effect of child-directed speech when
learning to extract semantic information from speech directly. We compare the
task performance of models trained on adult-directed speech (ADS) and
child-directed speech (CDS). We find indications that CDS helps in the initial
stages of learning, but eventually, models trained on ADS reach comparable task
performance, and generalize better. The results suggest that this is at least
partially due to linguistic rather than acoustic properties of the two
registers, as we see the same pattern when looking at models trained on
acoustically comparable synthetic speech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gelderloos_L/0/1/0/all/0/1"&gt;Lieke Gelderloos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1"&gt;Grzegorz Chrupa&amp;#x142;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alishahi_A/0/1/0/all/0/1"&gt;Afra Alishahi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multimodal Machine Learning Framework for Teacher Vocal Delivery Evaluation. (arXiv:2107.07956v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.07956</id>
        <link href="http://arxiv.org/abs/2107.07956"/>
        <updated>2021-07-19T00:49:05.566Z</updated>
        <summary type="html"><![CDATA[The quality of vocal delivery is one of the key indicators for evaluating
teacher enthusiasm, which has been widely accepted to be connected to the
overall course qualities. However, existing evaluation for vocal delivery is
mainly conducted with manual ratings, which faces two core challenges:
subjectivity and time-consuming. In this paper, we present a novel machine
learning approach that utilizes pairwise comparisons and a multimodal
orthogonal fusing algorithm to generate large-scale objective evaluation
results of the teacher vocal delivery in terms of fluency and passion. We
collect two datasets from real-world education scenarios and the experiment
results demonstrate the effectiveness of our algorithm. To encourage
reproducible results, we make our code public available at
\url{https://github.com/tal-ai/ML4VocalDelivery.git}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1"&gt;Yang Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenbiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zitao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Task Requirements Writing Evaluation via Machine Reading Comprehension. (arXiv:2107.07957v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07957</id>
        <link href="http://arxiv.org/abs/2107.07957"/>
        <updated>2021-07-19T00:49:05.545Z</updated>
        <summary type="html"><![CDATA[Task requirements (TRs) writing is an important question type in Key English
Test and Preliminary English Test. A TR writing question may include multiple
requirements and a high-quality essay must respond to each requirement
thoroughly and accurately. However, the limited teacher resources prevent
students from getting detailed grading instantly. The majority of existing
automatic essay scoring systems focus on giving a holistic score but rarely
provide reasons to support it. In this paper, we proposed an end-to-end
framework based on machine reading comprehension (MRC) to address this problem
to some extent. The framework not only detects whether an essay responds to a
requirement question, but clearly marks where the essay answers the question.
Our framework consists of three modules: question normalization module, ELECTRA
based MRC module and response locating module. We extensively explore
state-of-the-art MRC methods. Our approach achieves 0.93 accuracy score and
0.85 F1 score on a real-world educational dataset. To encourage reproducible
results, we make our code publicly available at
\url{https://github.com/aied2021TRMRC/AIED_2021_TRMRC_code}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shiting Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guowei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_P/0/1/0/all/0/1"&gt;Peilei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenbiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zitao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Application of Active Query K-Means in Text Classification. (arXiv:2107.07682v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07682</id>
        <link href="http://arxiv.org/abs/2107.07682"/>
        <updated>2021-07-19T00:49:05.525Z</updated>
        <summary type="html"><![CDATA[Active learning is a state-of-art machine learning approach to deal with an
abundance of unlabeled data. In the field of Natural Language Processing,
typically it is costly and time-consuming to have all the data annotated. This
inefficiency inspires out our application of active learning in text
classification. Traditional unsupervised k-means clustering is first modified
into a semi-supervised version in this research. Then, a novel attempt is
applied to further extend the algorithm into active learning scenario with
Penalized Min-Max-selection, so as to make limited queries that yield more
stable initial centroids. This method utilizes both the interactive query
results from users and the underlying distance representation. After tested on
a Chinese news dataset, it shows a consistent increase in accuracy while
lowering the cost in training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yukun Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intersectional Bias in Causal Language Models. (arXiv:2107.07691v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07691</id>
        <link href="http://arxiv.org/abs/2107.07691"/>
        <updated>2021-07-19T00:49:05.500Z</updated>
        <summary type="html"><![CDATA[To examine whether intersectional bias can be observed in language
generation, we examine \emph{GPT-2} and \emph{GPT-NEO} models, ranging in size
from 124 million to ~2.7 billion parameters. We conduct an experiment combining
up to three social categories - gender, religion and disability - into
unconditional or zero-shot prompts used to generate sentences that are then
analysed for sentiment. Our results confirm earlier tests conducted with
auto-regressive causal models, including the \emph{GPT} family of models. We
also illustrate why bias may be resistant to techniques that target single
categories (e.g. gender, religion and race), as it can also manifest, in often
subtle ways, in texts prompted by concatenated social categories. To address
these difficulties, we suggest technical and community-based approaches need to
combine to acknowledge and address complex and intersectional language model
bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Magee_L/0/1/0/all/0/1"&gt;Liam Magee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghahremanlou_L/0/1/0/all/0/1"&gt;Lida Ghahremanlou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soldatic_K/0/1/0/all/0/1"&gt;Karen Soldatic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robertson_S/0/1/0/all/0/1"&gt;Shanthi Robertson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-task Learning with Cross Attention for Keyword Spotting. (arXiv:2107.07634v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.07634</id>
        <link href="http://arxiv.org/abs/2107.07634"/>
        <updated>2021-07-19T00:49:05.441Z</updated>
        <summary type="html"><![CDATA[Keyword spotting (KWS) is an important technique for speech applications,
which enables users to activate devices by speaking a keyword phrase. Although
a phoneme classifier can be used for KWS, exploiting a large amount of
transcribed data for automatic speech recognition (ASR), there is a mismatch
between the training criterion (phoneme recognition) and the target task (KWS).
Recently, multi-task learning has been applied to KWS to exploit both ASR and
KWS training data. In this approach, an output of an acoustic model is split
into two branches for the two tasks, one for phoneme transcription trained with
the ASR data and one for keyword classification trained with the KWS data. In
this paper, we introduce a cross attention decoder in the multi-task learning
framework. Unlike the conventional multi-task learning approach with the simple
split of the output layer, the cross attention decoder summarizes information
from a phonetic encoder by performing cross attention between the encoder
outputs and a trainable query sequence to predict a confidence score for the
KWS task. Experimental results on KWS tasks show that the proposed approach
outperformed the conventional multi-task learning with split branches and a
bi-directional long short-team memory decoder by 12% on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Higuchi_T/0/1/0/all/0/1"&gt;Takuya Higuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Anmol Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dhir_C/0/1/0/all/0/1"&gt;Chandra Dhir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Goldfish Memory: Long-Term Open-Domain Conversation. (arXiv:2107.07567v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07567</id>
        <link href="http://arxiv.org/abs/2107.07567"/>
        <updated>2021-07-19T00:49:05.430Z</updated>
        <summary type="html"><![CDATA[Despite recent improvements in open-domain dialogue models, state of the art
models are trained and evaluated on short conversations with little context. In
contrast, the long-term conversation setting has hardly been studied. In this
work we collect and release a human-human dataset consisting of multiple chat
sessions whereby the speaking partners learn about each other's interests and
discuss the things they have learnt from past sessions. We show how existing
models trained on existing datasets perform poorly in this long-term
conversation setting in both automatic and human evaluations, and we study
long-context models that can perform much better. In particular, we find
retrieval-augmented methods and methods with an ability to summarize and recall
previous conversations outperform the standard encoder-decoder architectures
currently considered state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models. (arXiv:2107.07610v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07610</id>
        <link href="http://arxiv.org/abs/2107.07610"/>
        <updated>2021-07-19T00:49:05.402Z</updated>
        <summary type="html"><![CDATA[This paper improves the robustness of the pretrained language model BERT
against word substitution-based adversarial attacks by leveraging
self-supervised contrastive learning with adversarial perturbations. One
advantage of our method compared to previous works is that it is capable of
improving model robustness without using any labels. Additionally, we also
create an adversarial attack for word-level adversarial training on BERT. The
attack is efficient, allowing adversarial training for BERT on adversarial
examples generated on the fly during training. Experimental results on four
datasets show that our method improves the robustness of BERT against four
different word substitution-based adversarial attacks. Furthermore, to
understand why our method can improve the model robustness against adversarial
attacks, we study vector representations of clean examples and their
corresponding adversarial examples before and after applying our method. As our
method improves model robustness with unlabeled raw data, it opens up the
possibility of using large text datasets to train robust language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zhao Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yihan Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1"&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1"&gt;Roger Wattenhofer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TAPEX: Table Pre-training via Learning a Neural SQL Executor. (arXiv:2107.07653v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07653</id>
        <link href="http://arxiv.org/abs/2107.07653"/>
        <updated>2021-07-19T00:49:05.328Z</updated>
        <summary type="html"><![CDATA[Recent years pre-trained language models hit a success on modeling natural
language sentences and (semi-)structured tables. However, existing table
pre-training techniques always suffer from low data quality and low
pre-training efficiency. In this paper, we show that table pre-training can be
realized by learning a neural SQL executor over a synthetic corpus, which is
obtained by automatically synthesizing executable SQL queries. By pre-training
on the synthetic corpus, our approach TAPEX dramatically improves the
performance on downstream tasks, boosting existing language models by at most
19.5%. Meanwhile, TAPEX has remarkably high pre-training efficiency and yields
strong results when using a small pre-trained corpus. Experimental results
demonstrate that TAPEX outperforms previous table pre-training approaches by a
large margin, and our model achieves new state-of-the-art results on four
well-known datasets, including improving the WikiSQL denotation accuracy to
89.6% (+4.9%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the
SQA denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.6%
(+3.6%). Our work opens the way to reason over structured data by pre-training
on synthetic executable programs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiaqi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zeqi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1"&gt;Jian-guang Lou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.07002</id>
        <link href="http://arxiv.org/abs/2107.07002"/>
        <updated>2021-07-19T00:49:05.316Z</updated>
        <summary type="html"><![CDATA[The world of empirical machine learning (ML) strongly relies on benchmarks in
order to determine the relative effectiveness of different algorithms and
methods. This paper proposes the notion of "a benchmark lottery" that describes
the overall fragility of the ML benchmarking process. The benchmark lottery
postulates that many factors, other than fundamental algorithmic superiority,
may lead to a method being perceived as superior. On multiple benchmark setups
that are prevalent in the ML community, we show that the relative performance
of algorithms may be altered significantly simply by choosing different
benchmark tasks, highlighting the fragility of the current paradigms and
potential fallacious interpretation derived from benchmarking ML methods. Given
that every benchmark makes a statement about what it perceives to be important,
we argue that this might lead to biased progress in the community. We discuss
the implications of the observed phenomena and provide recommendations on
mitigating them using multiple machine learning domains and communities as use
cases, including natural language processing, computer vision, information
retrieval, recommender systems, and reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1"&gt;Alexey A. Gritsenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhe Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1"&gt;Fernando Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Internet-Augmented Dialogue Generation. (arXiv:2107.07566v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.07566</id>
        <link href="http://arxiv.org/abs/2107.07566"/>
        <updated>2021-07-19T00:49:05.279Z</updated>
        <summary type="html"><![CDATA[The largest store of continually updating knowledge on our planet can be
accessed via internet search. In this work we study giving access to this
information to conversational agents. Large language models, even though they
store an impressive amount of knowledge within their weights, are known to
hallucinate facts when generating dialogue (Shuster et al., 2021); moreover,
those facts are frozen in time at the point of model training. In contrast, we
propose an approach that learns to generate an internet search query based on
the context, and then conditions on the search results to finally generate a
response, a method that can employ up-to-the-minute relevant information. We
train and evaluate such models on a newly collected dataset of human-human
conversations whereby one of the speakers is given access to internet search
during knowledgedriven discussions in order to ground their responses. We find
that search-query based access of the internet in conversation provides
superior performance compared to existing approaches that either use no
augmentation or FAISS-based retrieval (Lewis et al., 2020).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1"&gt;Mojtaba Komeili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1"&gt;Kurt Shuster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIGIR 2021 E-Commerce Workshop Data Challenge. (arXiv:2104.09423v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09423</id>
        <link href="http://arxiv.org/abs/2104.09423"/>
        <updated>2021-07-19T00:49:05.195Z</updated>
        <summary type="html"><![CDATA[The 2021 SIGIR workshop on eCommerce is hosting the Coveo Data Challenge for
"In-session prediction for purchase intent and recommendations". The challenge
addresses the growing need for reliable predictions within the boundaries of a
shopping session, as customer intentions can be different depending on the
occasion. The need for efficient procedures for personalization is even clearer
if we consider the e-commerce landscape more broadly: outside of giant digital
retailers, the constraints of the problem are stricter, due to smaller user
bases and the realization that most users are not frequently returning
customers. We release a new session-based dataset including more than 30M
fine-grained browsing events (product detail, add, purchase), enriched by
linguistic behavior (queries made by shoppers, with items clicked and items not
clicked after the query) and catalog meta-data (images, text, pricing
information). On this dataset, we ask participants to showcase innovative
solutions for two open problems: a recommendation task (where a model is shown
some events at the start of a session, and it is asked to predict future
product interactions); an intent prediction task, where a model is shown a
session containing an add-to-cart event, and it is asked to predict whether the
item will be bought before the end of the session.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1"&gt;Jacopo Tagliabue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greco_C/0/1/0/all/0/1"&gt;Ciro Greco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_J/0/1/0/all/0/1"&gt;Jean-Francis Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bingqing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chia_P/0/1/0/all/0/1"&gt;Patrick John Chia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1"&gt;Federico Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cassani_G/0/1/0/all/0/1"&gt;Giovanni Cassani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling User Behaviour in Research Paper Recommendation System. (arXiv:2107.07831v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07831</id>
        <link href="http://arxiv.org/abs/2107.07831"/>
        <updated>2021-07-19T00:49:05.156Z</updated>
        <summary type="html"><![CDATA[User intention which often changes dynamically is considered to be an
important factor for modeling users in the design of recommendation systems.
Recent studies are starting to focus on predicting user intention (what users
want) beyond user preference (what users like). In this work, a user intention
model is proposed based on deep sequential topic analysis. The model predicts a
user's intention in terms of the topic of interest. The Hybrid Topic Model
(HTM) comprising Latent Dirichlet Allocation (LDA) and Word2Vec is proposed to
derive the topic of interest of users and the history of preferences. HTM finds
the true topics of papers estimating word-topic distribution which includes
syntactic and semantic correlations among words. Next, to model user intention,
a Long Short Term Memory (LSTM) based sequential deep learning model is
proposed. This model takes into account temporal context, namely the time
difference between clicks of two consecutive papers seen by a user. Extensive
experiments with the real-world research paper dataset indicate that the
proposed approach significantly outperforms the state-of-the-art methods.
Further, the proposed approach introduces a new road map to model a user
activity suitable for the design of a research paper recommendation system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1"&gt;Arpita Chaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samanta_D/0/1/0/all/0/1"&gt;Debasis Samanta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarma_M/0/1/0/all/0/1"&gt;Monalisa Sarma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Knowledge Graph Embedding and Their Applications. (arXiv:2107.07842v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07842</id>
        <link href="http://arxiv.org/abs/2107.07842"/>
        <updated>2021-07-19T00:49:05.141Z</updated>
        <summary type="html"><![CDATA[Knowledge Graph embedding provides a versatile technique for representing
knowledge. These techniques can be used in a variety of applications such as
completion of knowledge graph to predict missing information, recommender
systems, question answering, query expansion, etc. The information embedded in
Knowledge graph though being structured is challenging to consume in a
real-world application. Knowledge graph embedding enables the real-world
application to consume information to improve performance. Knowledge graph
embedding is an active research area. Most of the embedding methods focus on
structure-based information. Recent research has extended the boundary to
include text-based information and image-based information in entity embedding.
Efforts have been made to enhance the representation with context information.
This paper introduces growth in the field of KG embedding from simple
translation-based models to enrichment-based models. This paper includes the
utility of the Knowledge graph in real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1"&gt;Shivani Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luthra_T/0/1/0/all/0/1"&gt;Tarun Luthra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Ashima Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rajat Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoReMi: First glance at a universal OMR dataset. (arXiv:2107.07786v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07786</id>
        <link href="http://arxiv.org/abs/2107.07786"/>
        <updated>2021-07-19T00:49:05.131Z</updated>
        <summary type="html"><![CDATA[The main challenges of Optical Music Recognition (OMR) come from the nature
of written music, its complexity and the difficulty of finding an appropriate
data representation. This paper provides a first look at DoReMi, an OMR dataset
that addresses these challenges, and a baseline object detection model to
assess its utility. Researchers often approach OMR following a set of small
stages, given that existing data often do not satisfy broader research. We
examine the possibility of changing this tendency by presenting more metadata.
Our approach complements existing research; hence DoReMi allows harmonisation
with two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated
using a music notation software and includes over 6400 printed sheet music
images with accompanying metadata useful in OMR research. Our dataset provides
OMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage
of OMR. We obtain 64% mean average precision (mAP) in object detection using
half of the data. Further work includes re-iterating through the creation
process to satisfy custom OMR models. While we do not assume to have solved the
main challenges in OMR, this dataset opens a new course of discussions that
would ultimately aid that goal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shatri_E/0/1/0/all/0/1"&gt;Elona Shatri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1"&gt;Gy&amp;#xf6;rgy Fazekas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lightness Modulated Deep Inverse Tone Mapping. (arXiv:2107.07907v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07907</id>
        <link href="http://arxiv.org/abs/2107.07907"/>
        <updated>2021-07-19T00:49:04.940Z</updated>
        <summary type="html"><![CDATA[Single-image HDR reconstruction or inverse tone mapping (iTM) is a
challenging task. In particular, recovering information in over-exposed regions
is extremely difficult because details in such regions are almost completely
lost. In this paper, we present a deep learning based iTM method that takes
advantage of the feature extraction and mapping power of deep convolutional
neural networks (CNNs) and uses a lightness prior to modulate the CNN to better
exploit observations in the surrounding areas of the over-exposed regions to
enhance the quality of HDR image reconstruction. Specifically, we introduce a
Hierarchical Synthesis Network (HiSN) for inferring a HDR image from a LDR
input and a Lightness Adpative Modulation Network (LAMN) to incorporate the the
lightness prior knowledge in the inferring process. The HiSN hierarchically
synthesizes the high-brightness component and the low-brightness component of
the HDR image whilst the LAMN uses a lightness adaptive mask that separates
detail-less saturated bright pixels from well-exposed lower light pixels to
enable HiSN to better infer the missing information, particularly in the
difficult over-exposed detail-less areas. We present experimental results to
demonstrate the effectiveness of the new technique based on quantitative
measures and visual comparisons. In addition, we present ablation studies of
HiSN and visualization of the activation maps inside LAMN to help gain a deeper
understanding of the internal working of the new iTM algorithm and explain why
it can achieve much improved performance over state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kanglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cao_G/0/1/0/all/0/1"&gt;Gaofeng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jiang Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qiu_G/0/1/0/all/0/1"&gt;Guoping Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-labelling Enhanced Media Bias Detection. (arXiv:2107.07705v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07705</id>
        <link href="http://arxiv.org/abs/2107.07705"/>
        <updated>2021-07-19T00:49:04.905Z</updated>
        <summary type="html"><![CDATA[Leveraging unlabelled data through weak or distant supervision is a
compelling approach to developing more effective text classification models.
This paper proposes a simple but effective data augmentation method, which
leverages the idea of pseudo-labelling to select samples from noisy distant
supervision annotation datasets. The result shows that the proposed method
improves the accuracy of biased news detection models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1"&gt;Qin Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1"&gt;Brian Mac Namee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1"&gt;Ruihai Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Robust Dense Retrieval with Contrastive Dual Learning. (arXiv:2107.07773v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07773</id>
        <link href="http://arxiv.org/abs/2107.07773"/>
        <updated>2021-07-19T00:49:04.885Z</updated>
        <summary type="html"><![CDATA[Dense retrieval conducts text retrieval in the embedding space and has shown
many advantages compared to sparse retrieval. Existing dense retrievers
optimize representations of queries and documents with contrastive training and
map them to the embedding space. The embedding space is optimized by aligning
the matched query-document pairs and pushing the negative documents away from
the query. However, in such training paradigm, the queries are only optimized
to align to the documents and are coarsely positioned, leading to an
anisotropic query embedding space. In this paper, we analyze the embedding
space distributions and propose an effective training paradigm, Contrastive
Dual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained
query representations for dense retrieval. DANCE incorporates an additional
dual training object of query retrieval, inspired by the classic information
retrieval training axiom, query likelihood. With contrastive learning, the dual
training object of DANCE learns more tailored representations for queries and
documents to keep the embedding space smooth and uniform, thriving on the
ranking performance of DANCE on the MS MARCO document retrieval task. Different
from ANCE that only optimized with the document retrieval task, DANCE
concentrates the query embeddings closer to document representations while
making the document distribution more discriminative. Such concentrated query
embedding distribution assigns more uniform negative sampling probabilities to
queries and helps to sufficiently optimize query representations in the query
retrieval task. Our codes are released at https://github.com/thunlp/DANCE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yizhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhenghao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Chenyan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoReMi: First glance at a universal OMR dataset. (arXiv:2107.07786v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07786</id>
        <link href="http://arxiv.org/abs/2107.07786"/>
        <updated>2021-07-19T00:49:04.774Z</updated>
        <summary type="html"><![CDATA[The main challenges of Optical Music Recognition (OMR) come from the nature
of written music, its complexity and the difficulty of finding an appropriate
data representation. This paper provides a first look at DoReMi, an OMR dataset
that addresses these challenges, and a baseline object detection model to
assess its utility. Researchers often approach OMR following a set of small
stages, given that existing data often do not satisfy broader research. We
examine the possibility of changing this tendency by presenting more metadata.
Our approach complements existing research; hence DoReMi allows harmonisation
with two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated
using a music notation software and includes over 6400 printed sheet music
images with accompanying metadata useful in OMR research. Our dataset provides
OMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage
of OMR. We obtain 64% mean average precision (mAP) in object detection using
half of the data. Further work includes re-iterating through the creation
process to satisfy custom OMR models. While we do not assume to have solved the
main challenges in OMR, this dataset opens a new course of discussions that
would ultimately aid that goal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shatri_E/0/1/0/all/0/1"&gt;Elona Shatri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1"&gt;Gy&amp;#xf6;rgy Fazekas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Causal-Neural Connection: Expressiveness, Learnability, and Inference. (arXiv:2107.00793v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00793</id>
        <link href="http://arxiv.org/abs/2107.00793"/>
        <updated>2021-07-16T00:48:26.348Z</updated>
        <summary type="html"><![CDATA[One of the central elements of any causal inference is an object called
structural causal model (SCM), which represents a collection of mechanisms and
exogenous sources of random variation of the system under investigation (Pearl,
2000). An important property of many kinds of neural networks is universal
approximability: the ability to approximate any function to arbitrary
precision. Given this property, one may be tempted to surmise that a collection
of neural nets is capable of learning any SCM by training on data generated by
that SCM. In this paper, we show this is not the case by disentangling the
notions of expressivity and learnability. Specifically, we show that the causal
hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits
of what can be learned from data, still holds for neural models. For instance,
an arbitrarily complex and expressive neural net is unable to predict the
effects of interventions given observational data alone. Given this result, we
introduce a special type of SCM called a neural causal model (NCM), and
formalize a new type of inductive bias to encode structural constraints
necessary for performing causal inferences. Building on this new class of
models, we focus on solving two canonical tasks found in the literature known
as causal identification and estimation. Leveraging the neural toolbox, we
develop an algorithm that is both sufficient and necessary to determine whether
a causal effect can be learned from data (i.e., causal identifiability); it
then estimates the effect whenever identifiability holds (causal estimation).
Simulations corroborate the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1"&gt;Kevin Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kai-Zhan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1"&gt;Elias Bareinboim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12423</id>
        <link href="http://arxiv.org/abs/2106.12423"/>
        <updated>2021-07-16T00:48:26.334Z</updated>
        <summary type="html"><![CDATA[We observe that despite their hierarchical convolutional nature, the
synthesis process of typical generative adversarial networks depends on
absolute pixel coordinates in an unhealthy manner. This manifests itself as,
e.g., detail appearing to be glued to image coordinates instead of the surfaces
of depicted objects. We trace the root cause to careless signal processing that
causes aliasing in the generator network. Interpreting all signals in the
network as continuous, we derive generally applicable, small architectural
changes that guarantee that unwanted information cannot leak into the
hierarchical synthesis process. The resulting networks match the FID of
StyleGAN2 but differ dramatically in their internal representations, and they
are fully equivariant to translation and rotation even at subpixel scales. Our
results pave the way for generative models better suited for video and
animation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1"&gt;Tero Karras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1"&gt;Miika Aittala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1"&gt;Samuli Laine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1"&gt;Erik H&amp;#xe4;rk&amp;#xf6;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1"&gt;Janne Hellsten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1"&gt;Jaakko Lehtinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1"&gt;Timo Aila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks. (arXiv:2106.05825v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05825</id>
        <link href="http://arxiv.org/abs/2106.05825"/>
        <updated>2021-07-16T00:48:26.309Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) are employed in an increasing number of
applications, some of which are safety critical. Unfortunately, DNNs are known
to be vulnerable to so-called adversarial attacks that manipulate inputs to
cause incorrect results that can be beneficial to an attacker or damaging to
the victim. Multiple defenses have been proposed to increase the robustness of
DNNs. In general, these defenses have high overhead, some require
attack-specific re-training of the model or careful tuning to adapt to
different attacks.

This paper presents HASI, a hardware-accelerated defense that uses a process
we call stochastic inference to detect adversarial inputs. We show that by
carefully injecting noise into the model at inference time, we can
differentiate adversarial inputs from benign ones. HASI uses the output
distribution characteristics of noisy inference compared to a non-noisy
reference to detect adversarial inputs. We show an adversarial detection rate
of 86% when applied to VGG16 and 93% when applied to ResNet50, which exceeds
the detection rate of the state of the art approaches, with a much lower
overhead. We demonstrate two software/hardware-accelerated co-designs, which
reduces the performance impact of stochastic inference to 1.58X-2X relative to
the unprotected baseline, compared to 15X-20X overhead for a software-only GPU
implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1"&gt;Mohammad Hossein Samavatian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1"&gt;Saikat Majumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1"&gt;Kristin Barber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1"&gt;Radu Teodorescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concept drift detection and adaptation for federated and continual learning. (arXiv:2105.13309v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13309</id>
        <link href="http://arxiv.org/abs/2105.13309"/>
        <updated>2021-07-16T00:48:26.297Z</updated>
        <summary type="html"><![CDATA[Smart devices, such as smartphones, wearables, robots, and others, can
collect vast amounts of data from their environment. This data is suitable for
training machine learning models, which can significantly improve their
behavior, and therefore, the user experience. Federated learning is a young and
popular framework that allows multiple distributed devices to train deep
learning models collaboratively while preserving data privacy. Nevertheless,
this approach may not be optimal for scenarios where data distribution is
non-identical among the participants or changes over time, causing what is
known as concept drift. Little research has yet been done in this field, but
this kind of situation is quite frequent in real life and poses new challenges
to both continual and federated learning. Therefore, in this work, we present a
new method, called Concept-Drift-Aware Federated Averaging (CDA-FedAvg). Our
proposal is an extension of the most popular federated algorithm, Federated
Averaging (FedAvg), enhancing it for continual adaptation under concept drift.
We empirically demonstrate the weaknesses of regular FedAvg and prove that
CDA-FedAvg outperforms it in this type of scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Casado_F/0/1/0/all/0/1"&gt;Fernando E. Casado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lema_D/0/1/0/all/0/1"&gt;Dylan Lema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Criado_M/0/1/0/all/0/1"&gt;Marcos F. Criado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iglesias_R/0/1/0/all/0/1"&gt;Roberto Iglesias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Regueiro_C/0/1/0/all/0/1"&gt;Carlos V. Regueiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barro_S/0/1/0/all/0/1"&gt;Sen&amp;#xe9;n Barro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissimilarity Mixture Autoencoder for Deep Clustering. (arXiv:2006.08177v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08177</id>
        <link href="http://arxiv.org/abs/2006.08177"/>
        <updated>2021-07-16T00:48:26.226Z</updated>
        <summary type="html"><![CDATA[The dissimilarity mixture autoencoder (DMAE) is a neural network model for
feature-based clustering that incorporates a flexible dissimilarity function
and can be integrated into any kind of deep learning architecture. It
internally represents a dissimilarity mixture model (DMM) that extends
classical methods like K-Means, Gaussian mixture models, or Bregman clustering
to any convex and differentiable dissimilarity function through the
reinterpretation of probabilities as neural network representations. DMAE can
be integrated with deep learning architectures into end-to-end models, allowing
the simultaneous estimation of the clustering and neural network's parameters.
Experimental evaluation was performed on image and text clustering benchmark
datasets showing that DMAE is competitive in terms of unsupervised
classification accuracy and normalized mutual information. The source code with
the implementation of DMAE is publicly available at:
https://github.com/juselara1/dmae]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lara_J/0/1/0/all/0/1"&gt;Juan S. Lara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1"&gt;Fabio A. Gonz&amp;#xe1;lez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disambiguation of weak supervision with exponential convergence rates. (arXiv:2102.02789v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02789</id>
        <link href="http://arxiv.org/abs/2102.02789"/>
        <updated>2021-07-16T00:48:26.220Z</updated>
        <summary type="html"><![CDATA[Machine learning approached through supervised learning requires expensive
annotation of data. This motivates weakly supervised learning, where data are
annotated with incomplete yet discriminative information. In this paper, we
focus on partial labelling, an instance of weak supervision where, from a given
input, we are given a set of potential targets. We review a disambiguation
principle to recover full supervision from weak supervision, and propose an
empirical disambiguation algorithm. We prove exponential convergence rates of
our algorithm under classical learnability assumptions, and we illustrate the
usefulness of our method on practical examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cabannes_V/0/1/0/all/0/1"&gt;Vivien Cabannes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TreeBERT: A Tree-Based Pre-Trained Model for Programming Language. (arXiv:2105.12485v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12485</id>
        <link href="http://arxiv.org/abs/2105.12485"/>
        <updated>2021-07-16T00:48:26.207Z</updated>
        <summary type="html"><![CDATA[Source code can be parsed into the abstract syntax tree (AST) based on
defined syntax rules. However, in pre-training, little work has considered the
incorporation of tree structure into the learning process. In this paper, we
present TreeBERT, a tree-based pre-trained model for improving programming
language-oriented generation tasks. To utilize tree structure, TreeBERT
represents the AST corresponding to the code as a set of composition paths and
introduces node position embedding. The model is trained by tree masked
language modeling (TMLM) and node order prediction (NOP) with a hybrid
objective. TMLM uses a novel masking strategy designed according to the tree's
characteristics to help the model understand the AST and infer the missing
semantics of the AST. With NOP, TreeBERT extracts the syntactical structure by
learning the order constraints of nodes in AST. We pre-trained TreeBERT on
datasets covering multiple programming languages. On code summarization and
code documentation tasks, TreeBERT outperforms other pre-trained models and
state-of-the-art models designed for these tasks. Furthermore, TreeBERT
performs well when transferred to the pre-trained unseen programming language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xue Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhuoran Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1"&gt;Chen Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1"&gt;Lei Lyu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modelling Sovereign Credit Ratings: Evaluating the Accuracy and Driving Factors using Machine Learning Techniques. (arXiv:2101.12684v2 [q-fin.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12684</id>
        <link href="http://arxiv.org/abs/2101.12684"/>
        <updated>2021-07-16T00:48:26.183Z</updated>
        <summary type="html"><![CDATA[Sovereign credit ratings summarize the creditworthiness of countries. These
ratings have a large influence on the economy and the yields at which
governments can issue new debt. This paper investigates the use of a Multilayer
Perceptron (MLP), Classification and Regression Trees (CART), Support Vector
Machines (SVM), Na\"ive Bayes (NB), and an Ordered Logit (OL) model for the
prediction of sovereign credit ratings. We show that MLP is best suited for
predicting sovereign credit ratings, with a random cross-validated accuracy of
68%, followed by CART (59%), SVM (41%), NB (38%), and OL (33%). Investigation
of the determining factors shows that there is some heterogeneity in the
important variables across the models. However, the two models with the highest
out-of-sample predictive accuracy, MLP and CART, show a lot of similarities in
the influential variables, with regulatory quality, and GDP per capita as
common important variables. Consistent with economic theory, a higher
regulatory quality and/or GDP per capita are associated with a higher credit
rating.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Overes_B/0/1/0/all/0/1"&gt;Bart H.L. Overes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Wel_M/0/1/0/all/0/1"&gt;Michel van der Wel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04170</id>
        <link href="http://arxiv.org/abs/2105.04170"/>
        <updated>2021-07-16T00:48:26.178Z</updated>
        <summary type="html"><![CDATA[Recommender systems rely on user behavior data like ratings and clicks to
build personalization model. However, the collected data is observational
rather than experimental, causing various biases in the data which
significantly affect the learned model. Most existing work for recommendation
debiasing, such as the inverse propensity scoring and imputation approaches,
focuses on one or two specific biases, lacking the universal capacity that can
account for mixed or even unknown biases in the data. Towards this research
gap, we first analyze the origin of biases from the perspective of \textit{risk
discrepancy} that represents the difference between the expectation empirical
risk and the true risk. Remarkably, we derive a general learning framework that
well summarizes most existing debiasing strategies by specifying some
parameters of the general framework. This provides a valuable opportunity to
develop a universal solution for debiasing, e.g., by learning the debiasing
parameters from data. However, the training data lacks important signal of how
the data is biased and what the unbiased data looks like. To move this idea
forward, we propose \textit{AotoDebias} that leverages another (small) set of
uniform data to optimize the debiasing parameters by solving the bi-level
optimization problem with meta-learning. Through theoretical analyses, we
derive the generalization bound for AutoDebias and prove its ability to acquire
the appropriate debiasing strategy. Extensive experiments on two real datasets
and a simulated dataset demonstrated effectiveness of AutoDebias. The code is
available at \url{https://github.com/DongHande/AutoDebias}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hande Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1"&gt;Xin Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guli Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Keping Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bit More Bayesian: Domain-Invariant Learning with Uncertainty. (arXiv:2105.04030v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04030</id>
        <link href="http://arxiv.org/abs/2105.04030"/>
        <updated>2021-07-16T00:48:26.165Z</updated>
        <summary type="html"><![CDATA[Domain generalization is challenging due to the domain shift and the
uncertainty caused by the inaccessibility of target domain data. In this paper,
we address both challenges with a probabilistic framework based on variational
Bayesian inference, by incorporating uncertainty into neural network weights.
We couple domain invariance in a probabilistic formula with the variational
Bayesian inference. This enables us to explore domain-invariant learning in a
principled way. Specifically, we derive domain-invariant representations and
classifiers, which are jointly established in a two-layer Bayesian neural
network. We empirically demonstrate the effectiveness of our proposal on four
widely used cross-domain visual recognition benchmarks. Ablation studies
validate the synergistic benefits of our Bayesian treatment when jointly
learning domain-invariant representations and classifiers for domain
generalization. Further, our method consistently delivers state-of-the-art mean
accuracy on all benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1"&gt;Zehao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jiayi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1"&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G. M. Snoek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MCMC-driven importance samplers. (arXiv:2105.02579v3 [stat.CO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02579</id>
        <link href="http://arxiv.org/abs/2105.02579"/>
        <updated>2021-07-16T00:48:26.151Z</updated>
        <summary type="html"><![CDATA[Monte Carlo methods are the standard procedure for estimating complicated
integrals of multidimensional Bayesian posterior distributions. In this work,
we focus on LAIS, a class of adaptive importance samplers where Markov chain
Monte Carlo (MCMC) algorithms are employed to drive an underlying multiple
importance sampling (IS) scheme. Its power lies in the simplicity of the
layered framework: the upper layer locates proposal densities by means of MCMC
algorithms; while the lower layer handles the multiple IS scheme, in order to
compute the final estimators. The modular nature of LAIS allows for different
possible choices in the upper and lower layers, that will have different
performance and computational costs. In this work, we propose different
enhancements in order to increase the efficiency and reduce the computational
cost, of both upper and lower layers. The different variants are essential if
we aim to address computational challenges arising in real-world applications,
such as highly concentrated posterior distributions (due to large amounts of
data, etc.). Hamiltonian-driven importance samplers are presented and tested.
Furthermore, we introduce different strategies for designing cheaper schemes,
for instance, recycling samples generated in the upper layer and using them in
the final estimators in the lower layer. Numerical experiments show the
benefits of the proposed schemes as compared to the vanilla version of LAIS and
other benchmark methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Llorente_F/0/1/0/all/0/1"&gt;F. Llorente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Curbelo_E/0/1/0/all/0/1"&gt;E. Curbelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Martino_L/0/1/0/all/0/1"&gt;L. Martino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Elvira_V/0/1/0/all/0/1"&gt;V. Elvira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Delgado_D/0/1/0/all/0/1"&gt;D. Delgado&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00652</id>
        <link href="http://arxiv.org/abs/2107.00652"/>
        <updated>2021-07-16T00:48:26.119Z</updated>
        <summary type="html"><![CDATA[We present CSWin Transformer, an efficient and effective Transformer-based
backbone for general-purpose vision tasks. A challenging issue in Transformer
design is that global self-attention is very expensive to compute whereas local
self-attention often limits the field of interactions of each token. To address
this issue, we develop the Cross-Shaped Window self-attention mechanism for
computing self-attention in the horizontal and vertical stripes in parallel
that form a cross-shaped window, with each stripe obtained by splitting the
input feature into stripes of equal width. We provide a detailed mathematical
analysis of the effect of the stripe width and vary the stripe width for
different layers of the Transformer network which achieves strong modeling
capability while limiting the computation cost. We also introduce
Locally-enhanced Positional Encoding (LePE), which handles the local positional
information better than existing encoding schemes. LePE naturally supports
arbitrary input resolutions, and is thus especially effective and friendly for
downstream tasks. Incorporated with these designs and a hierarchical structure,
CSWin Transformer demonstrates competitive performance on common vision tasks.
Specifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra
training data or label, 53.9 box AP and 46.4 mask AP on the COCO detection
task, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing
previous state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and
+2.0 respectively under the similar FLOPs setting. By further pretraining on
the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K
and state-of-the-art segmentation performance on ADE20K with 55.7 mIoU. The
code and models will be available at
https://github.com/microsoft/CSWin-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaoyi Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1"&gt;Jianmin Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1"&gt;Nenghai Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1"&gt;Baining Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissimilarity Mixture Autoencoder for Deep Clustering. (arXiv:2006.08177v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08177</id>
        <link href="http://arxiv.org/abs/2006.08177"/>
        <updated>2021-07-16T00:48:26.114Z</updated>
        <summary type="html"><![CDATA[The dissimilarity mixture autoencoder (DMAE) is a neural network model for
feature-based clustering that incorporates a flexible dissimilarity function
and can be integrated into any kind of deep learning architecture. It
internally represents a dissimilarity mixture model (DMM) that extends
classical methods like K-Means, Gaussian mixture models, or Bregman clustering
to any convex and differentiable dissimilarity function through the
reinterpretation of probabilities as neural network representations. DMAE can
be integrated with deep learning architectures into end-to-end models, allowing
the simultaneous estimation of the clustering and neural network's parameters.
Experimental evaluation was performed on image and text clustering benchmark
datasets showing that DMAE is competitive in terms of unsupervised
classification accuracy and normalized mutual information. The source code with
the implementation of DMAE is publicly available at:
https://github.com/juselara1/dmae]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lara_J/0/1/0/all/0/1"&gt;Juan S. Lara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1"&gt;Fabio A. Gonz&amp;#xe1;lez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Dimension-Free Understanding of Adaptive Linear Control. (arXiv:2103.10620v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10620</id>
        <link href="http://arxiv.org/abs/2103.10620"/>
        <updated>2021-07-16T00:48:26.106Z</updated>
        <summary type="html"><![CDATA[We study the problem of adaptive control of the linear quadratic regulator
for systems in very high, or even infinite dimension. We demonstrate that while
sublinear regret requires finite dimensional inputs, the ambient state
dimension of the system need not be bounded in order to perform online control.
We provide the first regret bounds for LQR which hold for infinite dimensional
systems, replacing dependence on ambient dimension with more natural notions of
problem complexity. Our guarantees arise from a novel perturbation bound for
certainty equivalence which scales with the prediction error in estimating the
system parameters, without requiring consistent parameter recovery in more
stringent measures like the operator norm. When specialized to finite
dimensional settings, our bounds recover near optimal dimension and time
horizon dependence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Perdomo_J/0/1/0/all/0/1"&gt;Juan C. Perdomo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Simchowitz_M/0/1/0/all/0/1"&gt;Max Simchowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Alekh Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter Bartlett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoders and Ensembles for Task-Free Continual Learning. (arXiv:2105.13327v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13327</id>
        <link href="http://arxiv.org/abs/2105.13327"/>
        <updated>2021-07-16T00:48:26.099Z</updated>
        <summary type="html"><![CDATA[We present an architecture that is effective for continual learning in an
especially demanding setting, where task boundaries do not exist or are
unknown. Our architecture comprises an encoder, pre-trained on a separate
dataset, and an ensemble of simple one-layer classifiers. Two main innovations
are required to make this combination work. First, the provision of suitably
generic pre-trained encoders has been made possible thanks to recent progress
in self-supervised training methods. Second, pairing each classifier in the
ensemble with a key, where the key-space is identical to the latent space of
the encoder, allows them to be used collectively, yet selectively, via
k-nearest neighbour lookup. We show that models trained with the
encoders-and-ensembles architecture are state-of-the-art for the task-free
setting on standard image classification continual learning benchmarks, and
improve on prior state-of-the-art by a large margin in the most challenging
cases. We also show that the architecture learns well in a fully incremental
setting, where one class is learned at a time, and we demonstrate its
effectiveness in this setting with up to 100 classes. Finally, we show that the
architecture works in a task-free continual learning context where the data
distribution changes gradually, and existing approaches requiring knowledge of
task boundaries cannot be applied.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1"&gt;Murray Shanahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaplanis_C/0/1/0/all/0/1"&gt;Christos Kaplanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1"&gt;Jovana Mitrovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Almost Tight Approximation Algorithms for Explainable Clustering. (arXiv:2107.00774v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00774</id>
        <link href="http://arxiv.org/abs/2107.00774"/>
        <updated>2021-07-16T00:48:26.079Z</updated>
        <summary type="html"><![CDATA[Recently, due to an increasing interest for transparency in artificial
intelligence, several methods of explainable machine learning have been
developed with the simultaneous goal of accuracy and interpretability by
humans. In this paper, we study a recent framework of explainable clustering
first suggested by Dasgupta et al.~\cite{dasgupta2020explainable}.
Specifically, we focus on the $k$-means and $k$-medians problems and provide
nearly tight upper and lower bounds.

First, we provide an $O(\log k \log \log k)$-approximation algorithm for
explainable $k$-medians, improving on the best known algorithm of
$O(k)$~\cite{dasgupta2020explainable} and nearly matching the known
$\Omega(\log k)$ lower bound~\cite{dasgupta2020explainable}. In addition, in
low-dimensional spaces $d \ll \log k$, we show that our algorithm also provides
an $O(d \log^2 d)$-approximate solution for explainable $k$-medians. This
improves over the best known bound of $O(d \log k)$ for low
dimensions~\cite{laber2021explainable}, and is a constant for constant
dimensional spaces. To complement this, we show a nearly matching $\Omega(d)$
lower bound. Next, we study the $k$-means problem in this context and provide
an $O(k \log k)$-approximation algorithm for explainable $k$-means, improving
over the $O(k^2)$ bound of Dasgupta et al. and the $O(d k \log k)$ bound of
\cite{laber2021explainable}. To complement this we provide an almost tight
$\Omega(k)$ lower bound, improving over the $\Omega(\log k)$ lower bound of
Dasgupta et al. Given an approximate solution to the classic $k$-means and
$k$-medians, our algorithm for $k$-medians runs in time $O(kd \log^2 k )$ and
our algorithm for $k$-means runs in time $ O(k^2 d)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esfandiari_H/0/1/0/all/0/1"&gt;Hossein Esfandiari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1"&gt;Vahab Mirrokni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Shyam Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast rates in structured prediction. (arXiv:2102.00760v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00760</id>
        <link href="http://arxiv.org/abs/2102.00760"/>
        <updated>2021-07-16T00:48:26.072Z</updated>
        <summary type="html"><![CDATA[Discrete supervised learning problems such as classification are often
tackled by introducing a continuous surrogate problem akin to regression.
Bounding the original error, between estimate and solution, by the surrogate
error endows discrete problems with convergence rates already shown for
continuous instances. Yet, current approaches do not leverage the fact that
discrete problems are essentially predicting a discrete output when continuous
problems are predicting a continuous value. In this paper, we tackle this issue
for general structured prediction problems, opening the way to "super fast"
rates, that is, convergence rates for the excess risk faster than $n^{-1}$,
where $n$ is the number of observations, with even exponential rates with the
strongest assumptions. We first illustrate it for predictors based on nearest
neighbors, generalizing rates known for binary classification to any discrete
problem within the framework of structured prediction. We then consider kernel
ridge regression where we improve known rates in $n^{-1/4}$ to arbitrarily fast
rates, depending on a parameter characterizing the hardness of the problem,
thus allowing, under smoothness assumptions, to bypass the curse of
dimensionality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cabannes_V/0/1/0/all/0/1"&gt;Vivien Cabannes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Functional Model for Structure Learning and Parameter Estimation in Continuous Time Bayesian Network: An Application in Identifying Patterns of Multiple Chronic Conditions. (arXiv:2007.15847v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15847</id>
        <link href="http://arxiv.org/abs/2007.15847"/>
        <updated>2021-07-16T00:48:26.063Z</updated>
        <summary type="html"><![CDATA[Bayesian networks are powerful statistical models to study the probabilistic
relationships among set random variables with major applications in disease
modeling and prediction. Here, we propose a continuous time Bayesian network
with conditional dependencies, represented as Poisson regression, to model the
impact of exogenous variables on the conditional dependencies of the network.
We also propose an adaptive regularization method with an intuitive early
stopping feature based on density based clustering for efficient learning of
the structure and parameters of the proposed network. Using a dataset of
patients with multiple chronic conditions extracted from electronic health
records of the Department of Veterans Affairs we compare the performance of the
proposed approach with some of the existing methods in the literature for both
short-term (one-year ahead) and long-term (multi-year ahead) predictions. The
proposed approach provides a sparse intuitive representation of the complex
functional relationships between multiple chronic conditions. It also provides
the capability of analyzing multiple disease trajectories over time given any
combination of prior conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Faruqui_S/0/1/0/all/0/1"&gt;Syed Hasib Akhter Faruqui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alaeddini_A/0/1/0/all/0/1"&gt;Adel Alaeddini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaramillo_C/0/1/0/all/0/1"&gt;Carlos A. Jaramillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Space Partitions for Path Planning. (arXiv:2106.10544v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10544</id>
        <link href="http://arxiv.org/abs/2106.10544"/>
        <updated>2021-07-16T00:48:26.048Z</updated>
        <summary type="html"><![CDATA[Path planning, the problem of efficiently discovering high-reward
trajectories, often requires optimizing a high-dimensional and multimodal
reward function. Popular approaches like CEM and CMA-ES greedily focus on
promising regions of the search space and may get trapped in local maxima. DOO
and VOOT balance exploration and exploitation, but use space partitioning
strategies independent of the reward function to be optimized. Recently, LaMCTS
empirically learns to partition the search space in a reward-sensitive manner
for black-box optimization. In this paper, we develop a novel formal regret
analysis for when and why such an adaptive region partitioning scheme works. We
also propose a new path planning method PlaLaM which improves the function
value estimation within each sub-region, and uses a latent representation of
the search space. Empirically, PlaLaM outperforms existing path planning
methods in 2D navigation tasks, especially in the presence of
difficult-to-escape local optima, and shows benefits when plugged into
model-based RL with planning components such as PETS. These gains transfer to
highly multimodal real-world tasks, where we outperform strong baselines in
compiler phase ordering by up to 245% and in molecular design by up to 0.4 on
properties on a 0-1 scale. Code is available at
https://github.com/yangkevin2/plalam.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kevin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianjun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cummins_C/0/1/0/all/0/1"&gt;Chris Cummins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Brandon Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steiner_B/0/1/0/all/0/1"&gt;Benoit Steiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linnan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1"&gt;Dan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuandong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compute and memory efficient universal sound source separation. (arXiv:2103.02644v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02644</id>
        <link href="http://arxiv.org/abs/2103.02644"/>
        <updated>2021-07-16T00:48:26.042Z</updated>
        <summary type="html"><![CDATA[Recent progress in audio source separation lead by deep learning has enabled
many neural network models to provide robust solutions to this fundamental
estimation problem. In this study, we provide a family of efficient neural
network architectures for general purpose audio source separation while
focusing on multiple computational aspects that hinder the application of
neural networks in real-world scenarios. The backbone structure of this
convolutional network is the SUccessive DOwnsampling and Resampling of
Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is
performed through simple one-dimensional convolutions. This mechanism enables
our models to obtain high fidelity signal separation in a wide variety of
settings where variable number of sources are present and with limited
computational resources (e.g. floating point operations, memory footprint,
number of parameters and latency). Our experiments show that SuDoRM-RF models
perform comparably and even surpass several state-of-the-art benchmarks with
significantly higher computational resource requirements. The causal variation
of SuDoRM-RF is able to obtain competitive performance in real-time speech
separation of around 10dB scale-invariant signal-to-distortion ratio
improvement (SI-SDRi) while remaining up to 20 times faster than real-time on a
laptop device.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1"&gt;Efthymios Tzinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhepei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xilin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1"&gt;Paris Smaragdis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Learn Variational Semantic Memory. (arXiv:2010.10341v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10341</id>
        <link href="http://arxiv.org/abs/2010.10341"/>
        <updated>2021-07-16T00:48:26.030Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce variational semantic memory into meta-learning to
acquire long-term knowledge for few-shot learning. The variational semantic
memory accrues and stores semantic information for the probabilistic inference
of class prototypes in a hierarchical Bayesian framework. The semantic memory
is grown from scratch and gradually consolidated by absorbing information from
tasks it experiences. By doing so, it is able to accumulate long-term, general
knowledge that enables it to learn new concepts of objects. We formulate memory
recall as the variational inference of a latent memory variable from addressed
contents, which offers a principled way to adapt the knowledge to individual
tasks. Our variational semantic memory, as a new long-term memory module,
confers principled recall and update mechanisms that enable semantic
information to be efficiently accrued and adapted for few-shot learning.
Experiments demonstrate that the probabilistic modelling of prototypes achieves
a more informative representation of object classes compared to deterministic
vectors. The consistent new state-of-the-art performance on four benchmarks
shows the benefit of variational semantic memory in boosting few-shot
recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1"&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yingjun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Huan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1"&gt;Qiang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G. M. Snoek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GMAC: A Distributional Perspective on Actor-Critic Framework. (arXiv:2105.11366v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11366</id>
        <link href="http://arxiv.org/abs/2105.11366"/>
        <updated>2021-07-16T00:48:26.025Z</updated>
        <summary type="html"><![CDATA[In this paper, we devise a distributional framework on actor-critic as a
solution to distributional instability, action type restriction, and conflation
between samples and statistics. We propose a new method that minimizes the
Cram\'er distance with the multi-step Bellman target distribution generated
from a novel Sample-Replacement algorithm denoted SR($\lambda$), which learns
the correct value distribution under multiple Bellman operations.
Parameterizing a value distribution with Gaussian Mixture Model further
improves the efficiency and the performance of the method, which we name GMAC.
We empirically show that GMAC captures the correct representation of value
distributions and improves the performance of a conventional actor-critic
method with low computational cost, in both discrete and continuous action
spaces using Arcade Learning Environment (ALE) and PyBullet environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1"&gt;Daniel Wontae Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Younghoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chan Y. Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation Algorithms for Socially Fair Clustering. (arXiv:2103.02512v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02512</id>
        <link href="http://arxiv.org/abs/2103.02512"/>
        <updated>2021-07-16T00:48:26.007Z</updated>
        <summary type="html"><![CDATA[We present an $(e^{O(p)} \frac{\log \ell}{\log\log\ell})$-approximation
algorithm for socially fair clustering with the $\ell_p$-objective. In this
problem, we are given a set of points in a metric space. Each point belongs to
one (or several) of $\ell$ groups. The goal is to find a $k$-medians,
$k$-means, or, more generally, $\ell_p$-clustering that is simultaneously good
for all of the groups. More precisely, we need to find a set of $k$ centers $C$
so as to minimize the maximum over all groups $j$ of $\sum_{u \text{ in group
}j} d(u,C)^p$. The socially fair clustering problem was independently proposed
by Ghadiri, Samadi, and Vempala [2021] and Abbasi, Bhaskara, and
Venkatasubramanian [2021]. Our algorithm improves and generalizes their
$O(\ell)$-approximation algorithms for the problem. The natural LP relaxation
for the problem has an integrality gap of $\Omega(\ell)$. In order to obtain
our result, we introduce a strengthened LP relaxation and show that it has an
integrality gap of $\Theta(\frac{\log \ell}{\log\log\ell})$ for a fixed $p$.
Additionally, we present a bicriteria approximation algorithm, which
generalizes the bicriteria approximation of Abbasi et al. [2021].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makarychev_Y/0/1/0/all/0/1"&gt;Yury Makarychev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakilian_A/0/1/0/all/0/1"&gt;Ali Vakilian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification. (arXiv:2107.07511v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07511</id>
        <link href="http://arxiv.org/abs/2107.07511"/>
        <updated>2021-07-16T00:48:26.000Z</updated>
        <summary type="html"><![CDATA[Black-box machine learning learning methods are now routinely used in
high-risk settings, like medical diagnostics, which demand uncertainty
quantification to avoid consequential model failures. Distribution-free
uncertainty quantification (distribution-free UQ) is a user-friendly paradigm
for creating statistically rigorous confidence intervals/sets for such
predictions. Critically, the intervals/sets are valid without distributional
assumptions or model assumptions, with explicit guarantees with finitely many
datapoints. Moreover, they adapt to the difficulty of the input; when the input
example is difficult, the uncertainty intervals/sets are large, signaling that
the model might be wrong. Without much work, one can use distribution-free
methods on any underlying algorithm, such as a neural network, to produce
confidence sets guaranteed to contain the ground truth with a user-specified
probability, such as 90%. Indeed, the methods are easy-to-understand and
general, applying to many modern prediction problems arising in the fields of
computer vision, natural language processing, deep reinforcement learning, and
so on. This hands-on introduction is aimed at a reader interested in the
practical implementation of distribution-free UQ, including conformal
prediction and related methods, who is not necessarily a statistician. We will
include many explanatory illustrations, examples, and code samples in Python,
with PyTorch syntax. The goal is to provide the reader a working understanding
of distribution-free UQ, allowing them to put confidence intervals on their
algorithms, with one self-contained document.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1"&gt;Anastasios N. Angelopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1"&gt;Stephen Bates&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Neural Network Interpretability. (arXiv:2012.14261v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14261</id>
        <link href="http://arxiv.org/abs/2012.14261"/>
        <updated>2021-07-16T00:48:25.993Z</updated>
        <summary type="html"><![CDATA[Along with the great success of deep neural networks, there is also growing
concern about their black-box nature. The interpretability issue affects
people's trust on deep learning systems. It is also related to many ethical
problems, e.g., algorithmic discrimination. Moreover, interpretability is a
desired property for deep networks to become powerful tools in other research
fields, e.g., drug discovery and genomics. In this survey, we conduct a
comprehensive review of the neural network interpretability research. We first
clarify the definition of interpretability as it has been used in many
different contexts. Then we elaborate on the importance of interpretability and
propose a novel taxonomy organized along three dimensions: type of engagement
(passive vs. active interpretation approaches), the type of explanation, and
the focus (from local to global interpretability). This taxonomy provides a
meaningful 3D view of distribution of papers from the relevant literature as
two of the dimensions are not simply categorical but allow ordinal
subcategories. Finally, we summarize the existing interpretability evaluation
methods and suggest possible research directions inspired by our new taxonomy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tino_P/0/1/0/all/0/1"&gt;Peter Ti&amp;#x148;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ale&amp;#x161; Leonardis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Ke Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Investigation of Traffic Density Changes inside Wuhan during the COVID-19 Epidemic with GF-2 Time-Series Images. (arXiv:2006.16098v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16098</id>
        <link href="http://arxiv.org/abs/2006.16098"/>
        <updated>2021-07-16T00:48:25.974Z</updated>
        <summary type="html"><![CDATA[In order to mitigate the spread of COVID-19, Wuhan was the first city to
implement strict lockdown policy in 2020. Even though numerous researches have
discussed the travel restriction between cities and provinces, few studies
focus on the effect of transportation control inside the city due to the lack
of the measurement and available data in Wuhan. Since the public transports
have been shut down in the beginning of city lockdown, the change of traffic
density is a good indicator to reflect the intracity population flow.
Therefore, in this paper, we collected time-series high-resolution remote
sensing images with the resolution of 1m acquired before, during and after
Wuhan lockdown by GF-2 satellite. Vehicles on the road were extracted and
counted for the statistics of traffic density to reflect the changes of human
transmissions in the whole period of Wuhan lockdown. Open Street Map was used
to obtain observation road surfaces, and a vehicle detection method combing
morphology filter and deep learning was utilized to extract vehicles with the
accuracy of 62.56%. According to the experimental results, the traffic density
of Wuhan dropped with the percentage higher than 80%, and even higher than 90%
on main roads during city lockdown; after lockdown lift, the traffic density
recovered to the normal rate. Traffic density distributions also show the
obvious reduction and increase throughout the whole study area. The significant
reduction and recovery of traffic density indicates that the lockdown policy in
Wuhan show effectiveness in controlling human transmission inside the city, and
the city returned to normal after lockdown lift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yinong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Haonan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jingwen Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ru_L/0/1/0/all/0/1"&gt;Lixiang Ru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hongruixuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Bo Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liangpei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptable Agent Populations via a Generative Model of Policies. (arXiv:2107.07506v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07506</id>
        <link href="http://arxiv.org/abs/2107.07506"/>
        <updated>2021-07-16T00:48:25.958Z</updated>
        <summary type="html"><![CDATA[In the natural world, life has found innumerable ways to survive and often
thrive. Between and even within species, each individual is in some manner
unique, and this diversity lends adaptability and robustness to life. In this
work, we aim to learn a space of diverse and high-reward policies on any given
environment. To this end, we introduce a generative model of policies, which
maps a low-dimensional latent space to an agent policy space. Our method
enables learning an entire population of agent policies, without requiring the
use of separate policy parameters. Just as real world populations can adapt and
evolve via natural selection, our method is able to adapt to changes in our
environment solely by selecting for policies in latent space. We test our
generative model's capabilities in a variety of environments, including an
open-ended grid-world and a two-player soccer environment. Code,
visualizations, and additional experiments can be found at
https://kennyderek.github.io/adap/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Derek_K/0/1/0/all/0/1"&gt;Kenneth Derek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1"&gt;Phillip Isola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exact Partitioning of High-order Models with a Novel Convex Tensor Cone Relaxation. (arXiv:1911.02161v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.02161</id>
        <link href="http://arxiv.org/abs/1911.02161"/>
        <updated>2021-07-16T00:48:25.941Z</updated>
        <summary type="html"><![CDATA[In this paper we propose an algorithm for exact partitioning of high-order
models. We define a general class of $m$-degree Homogeneous Polynomial Models,
which subsumes several examples motivated from prior literature. Exact
partitioning can be formulated as a tensor optimization problem. We relax this
high-order combinatorial problem to a convex conic form problem. To this end,
we carefully define the Carath\'eodory symmetric tensor cone, and show its
convexity, and the convexity of its dual cone. This allows us to construct a
primal-dual certificate to show that the solution of the convex relaxation is
correct (equal to the unobserved true group assignment) and to analyze the
statistical upper bound of exact partitioning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ke_C/0/1/0/all/0/1"&gt;Chuyang Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honorio_J/0/1/0/all/0/1"&gt;Jean Honorio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Only Train Once: A One-Shot Neural Network Training And Pruning Framework. (arXiv:2107.07467v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07467</id>
        <link href="http://arxiv.org/abs/2107.07467"/>
        <updated>2021-07-16T00:48:25.932Z</updated>
        <summary type="html"><![CDATA[Structured pruning is a commonly used technique in deploying deep neural
networks (DNNs) onto resource-constrained devices. However, the existing
pruning methods are usually heuristic, task-specified, and require an extra
fine-tuning procedure. To overcome these limitations, we propose a framework
that compresses DNNs into slimmer architectures with competitive performances
and significant FLOPs reductions by Only-Train-Once (OTO). OTO contains two
keys: (i) we partition the parameters of DNNs into zero-invariant groups,
enabling us to prune zero groups without affecting the output; and (ii) to
promote zero groups, we then formulate a structured-sparsity optimization
problem and propose a novel optimization algorithm, Half-Space Stochastic
Projected Gradient (HSPG), to solve it, which outperforms the standard proximal
methods on group sparsity exploration and maintains comparable convergence. To
demonstrate the effectiveness of OTO, we train and compress full models
simultaneously from scratch without fine-tuning for inference speedup and
parameter reduction, and achieve state-of-the-art results on VGG16 for CIFAR10,
ResNet50 for CIFAR10/ImageNet and Bert for SQuAD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1"&gt;Bo Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1"&gt;Tianyu Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1"&gt;Biyi Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhihui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1"&gt;Luming Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yixin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1"&gt;Sheng Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1"&gt;Xiao Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Frank-Wolfe Adversarial Training. (arXiv:2012.12368v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12368</id>
        <link href="http://arxiv.org/abs/2012.12368"/>
        <updated>2021-07-16T00:48:25.925Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are easily fooled by small perturbations known as
adversarial attacks. Adversarial Training (AT) is a technique that
approximately solves a robust optimization problem to minimize the worst-case
loss and is widely regarded as the most effective defense against such attacks.
We develop a theoretical framework for adversarial training with FW
optimization (FW-AT) that reveals a geometric connection between the loss
landscape and the distortion of $\ell_\infty$ FW attacks (the attack's $\ell_2$
norm). Specifically, we show that high distortion of FW attacks is equivalent
to low variation along the attack path. It is then experimentally demonstrated
on various deep neural network architectures that $\ell_\infty$ attacks against
robust models achieve near maximal $\ell_2$ distortion. This mathematical
transparency differentiates FW from the more popular Projected Gradient Descent
(PGD) optimization. To demonstrate the utility of our theoretical framework we
develop FW-Adapt, a novel adversarial training algorithm which uses simple
distortion measure to adaptively change number of attack steps during training.
FW-Adapt provides strong robustness at lower training times in comparison to
PGD-AT for a variety of white-box and black-box attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1"&gt;Theodoros Tsiligkaridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1"&gt;Jay Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Cloud Completion by Learning Shape Priors. (arXiv:2008.00394v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00394</id>
        <link href="http://arxiv.org/abs/2008.00394"/>
        <updated>2021-07-16T00:48:25.918Z</updated>
        <summary type="html"><![CDATA[In view of the difficulty in reconstructing object details in point cloud
completion, we propose a shape prior learning method for object completion. The
shape priors include geometric information in both complete and the partial
point clouds. We design a feature alignment strategy to learn the shape prior
from complete points, and a coarse to fine strategy to incorporate partial
prior in the fine stage. To learn the complete objects prior, we first train a
point cloud auto-encoder to extract the latent embeddings from complete points.
Then we learn a mapping to transfer the point features from partial points to
that of the complete points by optimizing feature alignment losses. The feature
alignment losses consist of a L2 distance and an adversarial loss obtained by
Maximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2
distance optimizes the partial features towards the complete ones in the
feature space, and MMD-GAN decreases the statistical distance of two point
features in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art
performances on the point cloud completion task. Our code is available at
https://github.com/xiaogangw/point-cloud-completion-shape-prior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1"&gt;Marcelo H Ang Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gim Hee Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wireless Image Retrieval at the Edge. (arXiv:2007.10915v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10915</id>
        <link href="http://arxiv.org/abs/2007.10915"/>
        <updated>2021-07-16T00:48:25.911Z</updated>
        <summary type="html"><![CDATA[We study the image retrieval problem at the wireless edge, where an edge
device captures an image, which is then used to retrieve similar images from an
edge server. These can be images of the same person or a vehicle taken from
other cameras at different times and locations. Our goal is to maximize the
accuracy of the retrieval task under power and bandwidth constraints over the
wireless link. Due to the stringent delay constraint of the underlying
application, sending the whole image at a sufficient quality is not possible.
We propose two alternative schemes based on digital and analog communications,
respectively. In the digital approach, we first propose a deep neural network
(DNN) aided retrieval-oriented image compression scheme, whose output bit
sequence is transmitted over the channel using conventional channel codes. In
the analog joint source and channel coding (JSCC) approach, the feature vectors
are directly mapped into channel symbols. We evaluate both schemes on image
based re-identification (re-ID) tasks under different channel conditions,
including both static and fading channels. We show that the JSCC scheme
significantly increases the end-to-end accuracy, speeds up the encoding
process, and provides graceful degradation with channel conditions. The
proposed architecture is evaluated through extensive simulations on different
datasets and channel conditions, as well as through ablation studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1"&gt;Mikolaj Jankowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1"&gt;Krystian Mikolajczyk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Transferability in Wearable Sensor Systems. (arXiv:2003.07982v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.07982</id>
        <link href="http://arxiv.org/abs/2003.07982"/>
        <updated>2021-07-16T00:48:25.897Z</updated>
        <summary type="html"><![CDATA[Machine learning is used for inference and decision making in wearable sensor
systems. However, recent studies have found that machine learning algorithms
are easily fooled by the addition of adversarial perturbations to their inputs.
What is more interesting is that adversarial examples generated for one machine
learning system is also effective against other systems. This property of
adversarial examples is called transferability. In this work, we take the first
stride in studying adversarial transferability in wearable sensor systems from
the following perspectives: 1) transferability between machine learning
systems, 2) transferability across subjects, 3) transferability across sensor
body locations, and 4) transferability across datasets. We found strong
untargeted transferability in most cases. Targeted attacks were less successful
with success scores from $0\%$ to $80\%$. The transferability of adversarial
examples depends on many factors such as the inclusion of data from all
subjects, sensor body position, number of samples in the dataset, type of
learning algorithm, and the distribution of source and target system dataset.
The transferability of adversarial examples decreases sharply when the data
distribution of the source and target system becomes more distinct. We also
provide guidelines for the community for designing robust sensor systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sah_R/0/1/0/all/0/1"&gt;Ramesh Kumar Sah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghasemzadeh_H/0/1/0/all/0/1"&gt;Hassan Ghasemzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistent Online Gaussian Process Regression Without the Sample Complexity Bottleneck. (arXiv:2004.11094v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.11094</id>
        <link href="http://arxiv.org/abs/2004.11094"/>
        <updated>2021-07-16T00:48:25.890Z</updated>
        <summary type="html"><![CDATA[Gaussian processes provide a framework for nonlinear nonparametric Bayesian
inference widely applicable across science and engineering. Unfortunately,
their computational burden scales cubically with the training sample size,
which in the case that samples arrive in perpetuity, approaches infinity. This
issue necessitates approximations for use with streaming data, which to date
mostly lack convergence guarantees. Thus, we develop the first online Gaussian
process approximation that preserves convergence to the population posterior,
i.e., asymptotic posterior consistency, while ameliorating its intractable
complexity growth with the sample size. We propose an online compression scheme
that, following each a posteriori update, fixes an error neighborhood with
respect to the Hellinger metric centered at the current posterior, and greedily
tosses out past kernel dictionary elements until its boundary is hit. We call
the resulting method Parsimonious Online Gaussian Processes (POG). For
diminishing error radius, exact asymptotic consistency is preserved (Theorem
1(i)) at the cost of unbounded memory in the limit. On the other hand, for
constant error radius, POG converges to a neighborhood of the population
posterior (Theorem 1(ii))but with finite memory at-worst determined by the
metric entropy of the feature space (Theorem 2). Experimental results are
presented on several nonlinear regression problems which illuminates the merits
of this approach as compared with alternatives that fix the subspace dimension
defining the history of past points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1"&gt;Alec Koppel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pradhan_H/0/1/0/all/0/1"&gt;Hrusikesha Pradhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rajawat_K/0/1/0/all/0/1"&gt;Ketan Rajawat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Goodness-of-fit Test on the Number of Biclusters in a Relational Data Matrix. (arXiv:2102.11658v3 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11658</id>
        <link href="http://arxiv.org/abs/2102.11658"/>
        <updated>2021-07-16T00:48:25.882Z</updated>
        <summary type="html"><![CDATA[Biclustering is a method for detecting homogeneous submatrices in a given
observed matrix, and it is an effective tool for relational data analysis.
Although there are many studies that estimate the underlying bicluster
structure of a matrix, few have enabled us to determine the appropriate number
of biclusters in an observed matrix. Recently, a statistical test on the number
of biclusters has been proposed for a regular-grid bicluster structure, where
we assume that the latent bicluster structure can be represented by row-column
clustering. However, when the latent bicluster structure does not satisfy such
regular-grid assumption, the previous test requires a larger number of
biclusters than necessary (i.e., a finer bicluster structure than necessary)
for the null hypothesis to be accepted, which is not desirable in terms of
interpreting the accepted bicluster structure. In this study, we propose a new
statistical test on the number of biclusters that does not require the
regular-grid assumption and derive the asymptotic behavior of the proposed test
statistic in both null and alternative cases. We illustrate the effectiveness
of the proposed method by applying it to both synthetic and practical
relational data matrices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1"&gt;Chihiro Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[APo-VAE: Text Generation in Hyperbolic Space. (arXiv:2005.00054v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00054</id>
        <link href="http://arxiv.org/abs/2005.00054"/>
        <updated>2021-07-16T00:48:25.876Z</updated>
        <summary type="html"><![CDATA[Natural language often exhibits inherent hierarchical structure ingrained
with complex syntax and semantics. However, most state-of-the-art deep
generative models learn embeddings only in Euclidean vector space, without
accounting for this structural property of language. In this paper, we
investigate text generation in a hyperbolic latent space to learn continuous
hierarchical representations. An Adversarial Poincare Variational Autoencoder
(APo-VAE) is presented, where both the prior and variational posterior of
latent variables are defined over a Poincare ball via wrapped normal
distributions. By adopting the primal-dual formulation of KL divergence, an
adversarial learning procedure is introduced to empower robust model training.
Extensive experiments in language modeling and dialog-response generation tasks
demonstrate the winning effectiveness of the proposed APo-VAE model over VAEs
in Euclidean latent space, thanks to its superb capabilities in capturing
latent language hierarchies in hyperbolic space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1"&gt;Shuyang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chenyang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Rank Training of Deep Neural Networks for Emerging Memory Technology. (arXiv:2009.03887v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.03887</id>
        <link href="http://arxiv.org/abs/2009.03887"/>
        <updated>2021-07-16T00:48:25.859Z</updated>
        <summary type="html"><![CDATA[The recent success of neural networks for solving difficult decision tasks
has incentivized incorporating smart decision making "at the edge." However,
this work has traditionally focused on neural network inference, rather than
training, due to memory and compute limitations, especially in emerging
non-volatile memory systems, where writes are energetically costly and reduce
lifespan. Yet, the ability to train at the edge is becoming increasingly
important as it enables real-time adaptability to device drift and
environmental variation, user customization, and federated learning across
devices. In this work, we address two key challenges for training on edge
devices with non-volatile memory: low write density and low auxiliary memory.
We present a low-rank training scheme that addresses these challenges while
maintaining computational efficiency. We then demonstrate the technique on a
representative convolutional neural network across several adaptation problems,
where it out-performs standard SGD both in accuracy and in number of weight
writes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gural_A/0/1/0/all/0/1"&gt;Albert Gural&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadeau_P/0/1/0/all/0/1"&gt;Phillip Nadeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tikekar_M/0/1/0/all/0/1"&gt;Mehul Tikekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murmann_B/0/1/0/all/0/1"&gt;Boris Murmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Overview of Machine Learning-aided Optical Performance Monitoring Techniques. (arXiv:2107.07338v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07338</id>
        <link href="http://arxiv.org/abs/2107.07338"/>
        <updated>2021-07-16T00:48:25.825Z</updated>
        <summary type="html"><![CDATA[Future communication systems are faced with increased demand for high
capacity, dynamic bandwidth, reliability and heterogeneous traffic. To meet
these requirements, networks have become more complex and thus require new
design methods and monitoring techniques, as they evolve towards becoming
autonomous. Machine learning has come to the forefront in recent years as a
promising technology to aid in this evolution. Optical fiber communications can
already provide the high capacity required for most applications, however,
there is a need for increased scalability and adaptability to changing user
demands and link conditions. Accurate performance monitoring is an integral
part of this transformation. In this paper we review optical performance
monitoring techniques where machine learning algorithms have been applied.
Moreover, since alot of OPM depends on knowledge of the signal type, we also
review work for modulation format recognition and bitrate identification. We
additionally briefly introduce a neuromorphic approach to OPM as an emerging
technique that has only recently been applied to this domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tizikara_D/0/1/0/all/0/1"&gt;Dativa K. Tizikara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serugunda_J/0/1/0/all/0/1"&gt;Jonathan Serugunda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katumba_A/0/1/0/all/0/1"&gt;Andrew Katumba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis. (arXiv:2107.07373v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07373</id>
        <link href="http://arxiv.org/abs/2107.07373"/>
        <updated>2021-07-16T00:48:25.816Z</updated>
        <summary type="html"><![CDATA[We convert the DeepMind Mathematics Dataset into a reinforcement learning
environment by interpreting it as a program synthesis problem. Each action
taken in the environment adds an operator or an input into a discrete compute
graph. Graphs which compute correct answers yield positive reward, enabling the
optimization of a policy to construct compute graphs conditioned on problem
statements. Baseline models are trained using Double DQN on various subsets of
problem types, demonstrating the capability to learn to correctly construct
graphs despite the challenges of combinatorial explosion and noisy rewards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palermo_J/0/1/0/all/0/1"&gt;Joseph Palermo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Johnny Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Alok Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic analysis of solar cell optical performance using Gaussian processes. (arXiv:2107.07342v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07342</id>
        <link href="http://arxiv.org/abs/2107.07342"/>
        <updated>2021-07-16T00:48:25.802Z</updated>
        <summary type="html"><![CDATA[This work investigates application of different machine learning based
prediction methodologies to estimate the performance of silicon based textured
cells. Concept of confidence bound regions is introduced and advantages of this
concept are discussed in detail. Results show that reflection profiles and
depth dependent optical generation profiles can be accurately estimated using
Gaussian processes with exact knowledge of uncertainty in the prediction
values.It is also shown that cell design parameters can be estimated for a
desired performance metric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaiswal_R/0/1/0/all/0/1"&gt;Rahul Jaiswal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_Ramon_M/0/1/0/all/0/1"&gt;Manel Mart&amp;#xed;nez-Ram&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busani_T/0/1/0/all/0/1"&gt;Tito Busani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Do Not Need a Bigger Boat: Recommendations at Reasonable Scale in a (Mostly) Serverless and Open Stack. (arXiv:2107.07346v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07346</id>
        <link href="http://arxiv.org/abs/2107.07346"/>
        <updated>2021-07-16T00:48:25.796Z</updated>
        <summary type="html"><![CDATA[We argue that immature data pipelines are preventing a large portion of
industry practitioners from leveraging the latest research on recommender
systems. We propose our template data stack for machine learning at "reasonable
scale", and show how many challenges are solved by embracing a serverless
paradigm. Leveraging our experience, we detail how modern open source can
provide a pipeline processing terabytes of data with limited infrastructure
work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1"&gt;Jacopo Tagliabue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Newton-LESS: Sparsification without Trade-offs for the Sketched Newton Update. (arXiv:2107.07480v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.07480</id>
        <link href="http://arxiv.org/abs/2107.07480"/>
        <updated>2021-07-16T00:48:25.778Z</updated>
        <summary type="html"><![CDATA[In second-order optimization, a potential bottleneck can be computing the
Hessian matrix of the optimized function at every iteration. Randomized
sketching has emerged as a powerful technique for constructing estimates of the
Hessian which can be used to perform approximate Newton steps. This involves
multiplication by a random sketching matrix, which introduces a trade-off
between the computational cost of sketching and the convergence rate of the
optimization algorithm. A theoretically desirable but practically much too
expensive choice is to use a dense Gaussian sketching matrix, which produces
unbiased estimates of the exact Newton step and which offers strong
problem-independent convergence guarantees. We show that the Gaussian sketching
matrix can be drastically sparsified, significantly reducing the computational
cost of sketching, without substantially affecting its convergence properties.
This approach, called Newton-LESS, is based on a recently introduced sketching
technique: LEverage Score Sparsified (LESS) embeddings. We prove that
Newton-LESS enjoys nearly the same problem-independent local convergence rate
as Gaussian embeddings, not just up to constant factors but even down to lower
order terms, for a large class of optimization tasks. In particular, this leads
to a new state-of-the-art convergence result for an iterative least squares
solver. Finally, we extend LESS embeddings to include uniformly sparsified
random sign matrices which can be implemented efficiently and which perform
well in numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Derezinski_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Derezi&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lacotte_J/0/1/0/all/0/1"&gt;Jonathan Lacotte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Pilanci_M/0/1/0/all/0/1"&gt;Mert Pilanci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks. (arXiv:2107.07455v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07455</id>
        <link href="http://arxiv.org/abs/2107.07455"/>
        <updated>2021-07-16T00:48:25.772Z</updated>
        <summary type="html"><![CDATA[There has been significant research done on developing methods for improving
robustness to distributional shift and uncertainty estimation. In contrast,
only limited work has examined developing standard datasets and benchmarks for
assessing these approaches. Additionally, most work on uncertainty estimation
and robustness has developed new techniques based on small-scale regression or
image classification tasks. However, many tasks of practical interest have
different modalities, such as tabular data, audio, text, or sensor data, which
offer significant challenges involving regression and discrete or continuous
structured prediction. Thus, given the current state of the field, a
standardized large-scale dataset of tasks across a range of modalities affected
by distributional shifts is necessary. This will enable researchers to
meaningfully evaluate the plethora of recently developed uncertainty
quantification methods, as well as assessment criteria and state-of-the-art
baselines. In this work, we propose the \emph{Shifts Dataset} for evaluation of
uncertainty estimates and robustness to distributional shift. The dataset,
which has been collected from industrial sources and services, is composed of
three tasks, with each corresponding to a particular data modality: tabular
weather prediction, machine translation, and self-driving car (SDC) vehicle
motion prediction. All of these data modalities and tasks are affected by real,
`in-the-wild' distributional shifts and pose interesting challenges with
respect to uncertainty estimation. In this work we provide a description of the
dataset and baseline results for all tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1"&gt;Andrey Malinin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1"&gt;Neil Band&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chesnokov_G/0/1/0/all/0/1"&gt;German Chesnokov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1"&gt;Mark J. F. Gales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noskov_A/0/1/0/all/0/1"&gt;Alexey Noskov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ploskonosov_A/0/1/0/all/0/1"&gt;Andrey Ploskonosov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1"&gt;Liudmila Prokhorenkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Provilkov_I/0/1/0/all/0/1"&gt;Ivan Provilkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1"&gt;Vatsal Raina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1"&gt;Vyas Raina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmatova_M/0/1/0/all/0/1"&gt;Mariya Shmatova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tigas_P/0/1/0/all/0/1"&gt;Panos Tigas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yangel_B/0/1/0/all/0/1"&gt;Boris Yangel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Scoring Rule Design. (arXiv:2107.07420v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2107.07420</id>
        <link href="http://arxiv.org/abs/2107.07420"/>
        <updated>2021-07-16T00:48:25.765Z</updated>
        <summary type="html"><![CDATA[This paper introduces an optimization problem for proper scoring rule design.
Consider a principal who wants to collect an agent's prediction about an
unknown state. The agent can either report his prior prediction or access a
costly signal and report the posterior prediction. Given a collection of
possible distributions containing the agent's posterior prediction
distribution, the principal's objective is to design a bounded scoring rule to
maximize the agent's worst-case payoff increment between reporting his
posterior prediction and reporting his prior prediction.

We study two settings of such optimization for proper scoring rules: static
and asymptotic settings. In the static setting, where the agent can access one
signal, we propose an efficient algorithm to compute an optimal scoring rule
when the collection of distributions is finite. The agent can adaptively and
indefinitely refine his prediction in the asymptotic setting. We first consider
a sequence of collections of posterior distributions with vanishing covariance,
which emulates general estimators with large samples, and show the optimality
of the quadratic scoring rule. Then, when the agent's posterior distribution is
a Beta-Bernoulli process, we find that the log scoring rule is optimal. We also
prove the optimality of the log scoring rule over a smaller set of functions
for categorical distributions with Dirichlet priors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiling Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fang-Yi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Untrained DNN for Channel Estimation of RIS-Assisted Multi-User OFDM System with Hardware Impairments. (arXiv:2107.07423v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.07423</id>
        <link href="http://arxiv.org/abs/2107.07423"/>
        <updated>2021-07-16T00:48:25.725Z</updated>
        <summary type="html"><![CDATA[Reconfigurable intelligent surface (RIS) is an emerging technology for
improving performance in fifth-generation (5G) and beyond networks. Practically
channel estimation of RIS-assisted systems is challenging due to the passive
nature of the RIS. The purpose of this paper is to introduce a deep
learning-based, low complexity channel estimator for the RIS-assisted
multi-user single-input-multiple-output (SIMO) orthogonal frequency division
multiplexing (OFDM) system with hardware impairments. We propose an untrained
deep neural network (DNN) based on the deep image prior (DIP) network to
denoise the effective channel of the system obtained from the conventional
pilot-based least-square (LS) estimation and acquire a more accurate
estimation. We have shown that our proposed method has high performance in
terms of accuracy and low complexity compared to conventional methods. Further,
we have shown that the proposed estimator is robust to interference caused by
the hardware impairments at the transceiver and RIS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ginige_N/0/1/0/all/0/1"&gt;Nipuni Ginige&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Manosha_K/0/1/0/all/0/1"&gt;K. B. Shashika Manosha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajatheva_N/0/1/0/all/0/1"&gt;Nandana Rajatheva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Latva_aho_M/0/1/0/all/0/1"&gt;Matti Latva-aho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ApproxNet: Content and Contention-Aware Video Analytics System for Embedded Clients. (arXiv:1909.02068v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.02068</id>
        <link href="http://arxiv.org/abs/1909.02068"/>
        <updated>2021-07-16T00:48:25.719Z</updated>
        <summary type="html"><![CDATA[Videos take a lot of time to transport over the network, hence running
analytics on the live video on embedded or mobile devices has become an
important system driver. Considering that such devices, e.g., surveillance
cameras or AR/VR gadgets, are resource constrained, creating lightweight deep
neural networks (DNNs) for embedded devices is crucial. None of the current
approximation techniques for object classification DNNs can adapt to changing
runtime conditions, e.g., changes in resource availability on the device, the
content characteristics, or requirements from the user. In this paper, we
introduce ApproxNet, a video object classification system for embedded or
mobile clients. It enables novel dynamic approximation techniques to achieve
desired inference latency and accuracy trade-off under changing runtime
conditions. It achieves this by enabling two approximation knobs within a
single DNN model, rather than creating and maintaining an ensemble of models
(e.g., MCDNN [MobiSys-16]. We show that ApproxNet can adapt seamlessly at
runtime to these changes, provides low and stable latency for the image and
video frame classification problems, and show the improvement in accuracy and
latency over ResNet [CVPR-16], MCDNN [MobiSys-16], MobileNets [Google-17],
NestDNN [MobiCom-18], and MSDNet [ICLR-18].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ran Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Rakesh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pengcheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_P/0/1/0/all/0/1"&gt;Peter Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meghanath_G/0/1/0/all/0/1"&gt;Ganga Meghanath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaterji_S/0/1/0/all/0/1"&gt;Somali Chaterji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1"&gt;Subrata Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1"&gt;Saurabh Bagchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferring the Structure of Ordinary Differential Equations. (arXiv:2107.07345v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07345</id>
        <link href="http://arxiv.org/abs/2107.07345"/>
        <updated>2021-07-16T00:48:25.703Z</updated>
        <summary type="html"><![CDATA[Understanding physical phenomena oftentimes means understanding the
underlying dynamical system that governs observational measurements. While
accurate prediction can be achieved with black box systems, they often lack
interpretability and are less amenable for further expert investigation.
Alternatively, the dynamics can be analysed via symbolic regression. In this
paper, we extend the approach by (Udrescu et al., 2020) called AIFeynman to the
dynamic setting to perform symbolic regression on ODE systems based on
observations from the resulting trajectories. We compare this extension to
state-of-the-art approaches for symbolic regression empirically on several
dynamical systems for which the ground truth equations of increasing complexity
are available. Although the proposed approach performs best on this benchmark,
we observed difficulties of all the compared symbolic regression approaches on
more complex systems, such as Cart-Pole.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weilbach_J/0/1/0/all/0/1"&gt;Juliane Weilbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerwinn_S/0/1/0/all/0/1"&gt;Sebastian Gerwinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weilbach_C/0/1/0/all/0/1"&gt;Christian Weilbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1"&gt;Melih Kandemir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07436</id>
        <link href="http://arxiv.org/abs/2107.07436"/>
        <updated>2021-07-16T00:48:25.693Z</updated>
        <summary type="html"><![CDATA[Shapley values are widely used to explain black-box models, but they are
costly to calculate because they require many model evaluations. We introduce
FastSHAP, a method for estimating Shapley values in a single forward pass using
a learned explainer model. FastSHAP amortizes the cost of explaining many
inputs via a learning approach inspired by the Shapley value's weighted least
squares characterization, and it can be trained using standard stochastic
gradient optimization. We compare FastSHAP to existing estimation approaches,
revealing that it generates high-quality explanations with orders of magnitude
speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jethani_N/0/1/0/all/0/1"&gt;Neil Jethani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sudarshan_M/0/1/0/all/0/1"&gt;Mukund Sudarshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Covert_I/0/1/0/all/0/1"&gt;Ian Covert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1"&gt;Su-In Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized and Reliable Decision Sets: Enhancing Interpretability in Clinical Decision Support Systems. (arXiv:2107.07483v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2107.07483</id>
        <link href="http://arxiv.org/abs/2107.07483"/>
        <updated>2021-07-16T00:48:25.687Z</updated>
        <summary type="html"><![CDATA[In this study, we present a novel clinical decision support system and
discuss its interpretability-related properties. It combines a decision set of
rules with a machine learning scheme to offer global and local
interpretability. More specifically, machine learning is used to predict the
likelihood of each of those rules to be correct for a particular patient, which
may also contribute to better predictive performances. Moreover, the
reliability analysis of individual predictions is also addressed, contributing
to further personalized interpretability. The combination of these several
elements may be crucial to obtain the clinical stakeholders' trust, leading to
a better assessment of patients' conditions and improvement of the physicians'
decision-making.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Valente_F/0/1/0/all/0/1"&gt;Francisco Valente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Paredes_S/0/1/0/all/0/1"&gt;Sim&amp;#xe3;o Paredes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Henriques_J/0/1/0/all/0/1"&gt;Jorge Henriques&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High carbon stock mapping at large scale with optical satellite imagery and spaceborne LIDAR. (arXiv:2107.07431v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07431</id>
        <link href="http://arxiv.org/abs/2107.07431"/>
        <updated>2021-07-16T00:48:25.677Z</updated>
        <summary type="html"><![CDATA[The increasing demand for commodities is leading to changes in land use
worldwide. In the tropics, deforestation, which causes high carbon emissions
and threatens biodiversity, is often linked to agricultural expansion. While
the need for deforestation-free global supply chains is widely recognized,
making progress in practice remains a challenge. Here, we propose an automated
approach that aims to support conservation and sustainable land use planning
decisions by mapping tropical landscapes at large scale and high spatial
resolution following the High Carbon Stock (HCS) approach. A deep learning
approach is developed that estimates canopy height for each 10 m Sentinel-2
pixel by learning from sparse GEDI LIDAR reference data, achieving an overall
RMSE of 6.3 m. We show that these wall-to-wall maps of canopy top height are
predictive for classifying HCS forests and degraded areas with an overall
accuracy of 86 % and produce a first high carbon stock map for Indonesia,
Malaysia, and the Philippines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1"&gt;Nico Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1"&gt;Jan Dirk Wegner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Copula-Based Normalizing Flows. (arXiv:2107.07352v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07352</id>
        <link href="http://arxiv.org/abs/2107.07352"/>
        <updated>2021-07-16T00:48:25.658Z</updated>
        <summary type="html"><![CDATA[Normalizing flows, which learn a distribution by transforming the data to
samples from a Gaussian base distribution, have proven powerful density
approximations. But their expressive power is limited by this choice of the
base distribution. We, therefore, propose to generalize the base distribution
to a more elaborate copula distribution to capture the properties of the target
distribution more accurately. In a first empirical analysis, we demonstrate
that this replacement can dramatically improve the vanilla normalizing flows in
terms of flexibility, stability, and effectivity for heavy-tailed data. Our
results suggest that the improvements are related to an increased local
Lipschitz-stability of the learned flow.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laszkiewicz_M/0/1/0/all/0/1"&gt;Mike Laszkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1"&gt;Johannes Lederer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1"&gt;Asja Fischer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fixed Version of Quadratic Program in Gradient Episodic Memory. (arXiv:2107.07384v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07384</id>
        <link href="http://arxiv.org/abs/2107.07384"/>
        <updated>2021-07-16T00:48:25.640Z</updated>
        <summary type="html"><![CDATA[Gradient Episodic Memory is indeed a novel method for continual learning,
which solves new problems quickly without forgetting previously acquired
knowledge. However, in the process of studying the paper, we found there were
some problems in the proof of the dual problem of Quadratic Program, so here we
give our fixed version for this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiying Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explore and Control with Adversarial Surprise. (arXiv:2107.07394v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07394</id>
        <link href="http://arxiv.org/abs/2107.07394"/>
        <updated>2021-07-16T00:48:25.631Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) provides a framework for learning goal-directed
policies given user-specified rewards. However, since designing rewards often
requires substantial engineering effort, we are interested in the problem of
learning without rewards, where agents must discover useful behaviors in the
absence of task-specific incentives. Intrinsic motivation is a family of
unsupervised RL techniques which develop general objectives for an RL agent to
optimize that lead to better exploration or the discovery of skills. In this
paper, we propose a new unsupervised RL technique based on an adversarial game
which pits two policies against each other to compete over the amount of
surprise an RL agent experiences. The policies each take turns controlling the
agent. The Explore policy maximizes entropy, putting the agent into surprising
or unfamiliar situations. Then, the Control policy takes over and seeks to
recover from those situations by minimizing entropy. The game harnesses the
power of multi-agent competition to drive the agent to seek out increasingly
surprising parts of the environment while learning to gain mastery over them.
We show empirically that our method leads to the emergence of complex skills by
exhibiting clear phase transitions. Furthermore, we show both theoretically
(via a latent state space coverage argument) and empirically that our method
has the potential to be applied to the exploration of stochastic,
partially-observed environments. We show that Adversarial Surprise learns more
complex behaviors, and explores more effectively than competitive baselines,
outperforming intrinsic motivation methods based on active inference,
novelty-seeking (Random Network Distillation (RND)), and multi-agent
unsupervised RL (Asymmetric Self-Play (ASP)) in MiniGrid, Atari and VizDoom
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fickinger_A/0/1/0/all/0/1"&gt;Arnaud Fickinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1"&gt;Natasha Jaques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parajuli_S/0/1/0/all/0/1"&gt;Samyak Parajuli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1"&gt;Michael Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhinehart_N/0/1/0/all/0/1"&gt;Nicholas Rhinehart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1"&gt;Glen Berseth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1"&gt;Stuart Russell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration. (arXiv:2107.07410v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07410</id>
        <link href="http://arxiv.org/abs/2107.07410"/>
        <updated>2021-07-16T00:48:25.625Z</updated>
        <summary type="html"><![CDATA[Model-based Reinforcement Learning (RL) is a popular learning paradigm due to
its potential sample efficiency compared to model-free RL. However, existing
empirical model-based RL approaches lack the ability to explore. This work
studies a computationally and statistically efficient model-based algorithm for
both Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes
(MDPs). For both models, our algorithm guarantees polynomial sample complexity
and only uses access to a planning oracle. Experimentally, we first demonstrate
the flexibility and efficacy of our algorithm on a set of exploration
challenging control tasks where existing empirical model-based RL approaches
completely fail. We then show that our approach retains excellent performance
even in common dense reward control benchmarks that do not require heavy
exploration. Finally, we demonstrate that our method can also perform
reward-free exploration efficiently. Our code can be found at
https://github.com/yudasong/PCMLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yuda Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proceedings of the Sixteenth Workshop on Logical Frameworks and Meta-Languages: Theory and Practice. (arXiv:2107.07376v1 [cs.LO])]]></title>
        <id>http://arxiv.org/abs/2107.07376</id>
        <link href="http://arxiv.org/abs/2107.07376"/>
        <updated>2021-07-16T00:48:25.611Z</updated>
        <summary type="html"><![CDATA[Logical frameworks and meta-languages form a common substrate for
representing, implementing and reasoning about a wide variety of deductive
systems of interest in logic and computer science. Their design, implementation
and their use in reasoning tasks, ranging from the correctness of software to
the properties of formal systems, have been the focus of considerable research
over the last two decades. This workshop brings together designers,
implementors and practitioners to discuss various aspects impinging on the
structure and utility of logical frameworks, including the treatment of
variable binding, inductive and co-inductive reasoning techniques and the
expressiveness and lucidity of the reasoning process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pimentel_E/0/1/0/all/0/1"&gt;Elaine Pimentel&lt;/a&gt; (UFRN), &lt;a href="http://arxiv.org/find/cs/1/au:+Tassi_E/0/1/0/all/0/1"&gt;Enrico Tassi&lt;/a&gt; (Inria)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills. (arXiv:2107.07261v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07261</id>
        <link href="http://arxiv.org/abs/2107.07261"/>
        <updated>2021-07-16T00:48:25.604Z</updated>
        <summary type="html"><![CDATA[Models pre-trained with a language modeling objective possess ample world
knowledge and language skills, but are known to struggle in tasks that require
reasoning. In this work, we propose to leverage semi-structured tables, and
automatically generate at scale question-paragraph pairs, where answering the
question requires reasoning over multiple facts in the paragraph. We add a
pre-training step over this synthetic data, which includes examples that
require 16 different reasoning skills such as number comparison, conjunction,
and fact composition. To improve data efficiency, we propose sampling
strategies that focus training on reasoning skills the model is currently
lacking. We evaluate our approach on three reading comprehension datasets that
are focused on reasoning, and show that our model, PReasM, substantially
outperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling
examples based on current model errors leads to faster training and higher
overall performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1"&gt;Ori Yoran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talmor_A/0/1/0/all/0/1"&gt;Alon Talmor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1"&gt;Jonathan Berant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging wisdom of the crowds to improve consensus among radiologists by real time, blinded collaborations on a digital swarm platform. (arXiv:2107.07341v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.07341</id>
        <link href="http://arxiv.org/abs/2107.07341"/>
        <updated>2021-07-16T00:48:25.589Z</updated>
        <summary type="html"><![CDATA[Radiologists today play a key role in making diagnostic decisions and
labeling images for training A.I. algorithms. Low inter-reader reliability
(IRR) can be seen between experts when interpreting challenging cases. While
teams-based decisions are known to outperform individual decisions,
inter-personal biases often creep up in group interactions which limit
non-dominant participants from expressing true opinions. To overcome the dual
problems of low consensus and inter-personal bias, we explored a solution
modeled on biological swarms of bees. Two separate cohorts; three radiologists
and five radiology residents collaborated on a digital swarm platform in real
time and in a blinded fashion, grading meniscal lesions on knee MR exams. These
consensus votes were benchmarked against clinical (arthroscopy) and
radiological (senior-most radiologist) observations. The IRR of the consensus
votes was compared to the IRR of the majority and most confident votes of the
two cohorts.The radiologist cohort saw an improvement of 23% in IRR of swarm
votes over majority vote. Similar improvement of 23% in IRR in 3-resident swarm
votes over majority vote, was observed. The 5-resident swarm had an even higher
improvement of 32% in IRR over majority vote. Swarm consensus votes also
improved specificity by up to 50%. The swarm consensus votes outperformed
individual and majority vote decisions in both the radiologists and resident
cohorts. The 5-resident swarm had higher IRR than 3-resident swarm indicating
positive effect of increased swarm size. The attending and resident swarms also
outperformed predictions from a state-of-the-art A.I. algorithm. Utilizing a
digital swarm platform improved agreement and allows participants to express
judgement free intent, resulting in superior clinical performance and robust
A.I. training labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1"&gt;Rutwik Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astuto_B/0/1/0/all/0/1"&gt;Bruno Astuto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gleason_T/0/1/0/all/0/1"&gt;Tyler Gleason&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fletcher_W/0/1/0/all/0/1"&gt;Will Fletcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banaga_J/0/1/0/all/0/1"&gt;Justin Banaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sweetwood_K/0/1/0/all/0/1"&gt;Kevin Sweetwood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_A/0/1/0/all/0/1"&gt;Allen Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1"&gt;Rina Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGill_K/0/1/0/all/0/1"&gt;Kevin McGill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Link_T/0/1/0/all/0/1"&gt;Thomas Link&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crane_J/0/1/0/all/0/1"&gt;Jason Crane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedoia_V/0/1/0/all/0/1"&gt;Valentina Pedoia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1"&gt;Sharmila Majumdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A multi-schematic classifier-independent oversampling approach for imbalanced datasets. (arXiv:2107.07349v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07349</id>
        <link href="http://arxiv.org/abs/2107.07349"/>
        <updated>2021-07-16T00:48:25.573Z</updated>
        <summary type="html"><![CDATA[Over 85 oversampling algorithms, mostly extensions of the SMOTE algorithm,
have been built over the past two decades, to solve the problem of imbalanced
datasets. However, it has been evident from previous studies that different
oversampling algorithms have different degrees of efficiency with different
classifiers. With numerous algorithms available, it is difficult to decide on
an oversampling algorithm for a chosen classifier. Here, we overcome this
problem with a multi-schematic and classifier-independent oversampling
approach: ProWRAS(Proximity Weighted Random Affine Shadowsampling). ProWRAS
integrates the Localized Random Affine Shadowsampling (LoRAS)algorithm and the
Proximity Weighted Synthetic oversampling (ProWSyn) algorithm. By controlling
the variance of the synthetic samples, as well as a proximity-weighted
clustering system of the minority classdata, the ProWRAS algorithm improves
performance, compared to algorithms that generate synthetic samples through
modelling high dimensional convex spaces of the minority class. ProWRAS has
four oversampling schemes, each of which has its unique way to model the
variance of the generated data. Most importantly, the performance of ProWRAS
with proper choice of oversampling schemes, is independent of the classifier
used. We have benchmarked our newly developed ProWRAS algorithm against five
sate-of-the-art oversampling models and four different classifiers on 20
publicly available datasets. ProWRAS outperforms other oversampling algorithms
in a statistically significant way, in terms of both F1-score and Kappa-score.
Moreover, we have introduced a novel measure for classifier independence
I-score, and showed quantitatively that ProWRAS performs better, independent of
the classifier used. In practice, ProWRAS customizes synthetic sample
generation according to a classifier of choice and thereby reduces benchmarking
efforts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bej_S/0/1/0/all/0/1"&gt;Saptarshi Bej&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schultz_K/0/1/0/all/0/1"&gt;Kristian Schultz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1"&gt;Prashant Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolfien_M/0/1/0/all/0/1"&gt;Markus Wolfien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolkenhauer_O/0/1/0/all/0/1"&gt;Olaf Wolkenhauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Ant Swarm-Based Data Clustering. (arXiv:2107.07382v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.07382</id>
        <link href="http://arxiv.org/abs/2107.07382"/>
        <updated>2021-07-16T00:48:25.558Z</updated>
        <summary type="html"><![CDATA[Biologically inspired computing techniques are very effective and useful in
many areas of research including data clustering. Ant clustering algorithm is a
nature-inspired clustering technique which is extensively studied for over two
decades. In this study, we extend the ant clustering algorithm (ACA) to a
hybrid ant clustering algorithm (hACA). Specifically, we include a genetic
algorithm in standard ACA to extend the hybrid algorithm for better
performance. We also introduced novel pick up and drop off rules to speed up
the clustering performance. We study the performance of the hACA algorithm and
compare with standard ACA as a benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azam_M/0/1/0/all/0/1"&gt;Md Ali Azam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossen_A/0/1/0/all/0/1"&gt;Abir Hossen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Md Hafizur Rahman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting. (arXiv:2107.07240v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07240</id>
        <link href="http://arxiv.org/abs/2107.07240"/>
        <updated>2021-07-16T00:48:25.541Z</updated>
        <summary type="html"><![CDATA[We study the realistic potential of conducting backdoor attack against deep
neural networks (DNNs) during deployment stage. Specifically, our goal is to
design a deployment-stage backdoor attack algorithm that is both threatening
and realistically implementable. To this end, we propose Subnet Replacement
Attack (SRA), which is capable of embedding backdoor into DNNs by directly
modifying a limited number of model parameters. Considering the realistic
practicability, we abandon the strong white-box assumption widely adopted in
existing studies, instead, our algorithm works in a gray-box setting, where
architecture information of the victim model is available but the adversaries
do not have any knowledge of parameter values. The key philosophy underlying
our approach is -- given any neural network instance (regardless of its
specific parameter values) of a certain architecture, we can always embed a
backdoor into that model instance, by replacing a very narrow subnet of a
benign model (without backdoor) with a malicious backdoor subnet, which is
designed to be sensitive (fire large activation value) to a particular backdoor
trigger pattern.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1"&gt;Xiangyu Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jifeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chulin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Parameter Generators. (arXiv:2107.07110v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07110</id>
        <link href="http://arxiv.org/abs/2107.07110"/>
        <updated>2021-07-16T00:48:25.503Z</updated>
        <summary type="html"><![CDATA[We present a generic method for recurrently using the same parameters for
many different convolution layers to build a deep network. Specifically, for a
network, we create a recurrent parameter generator (RPG), from which the
parameters of each convolution layer are generated. Though using recurrent
models to build a deep convolutional neural network (CNN) is not entirely new,
our method achieves significant performance gain compared to the existing
works. We demonstrate how to build a one-layer neural network to achieve
similar performance compared to other traditional CNN models on various
applications and datasets. Such a method allows us to build an arbitrarily
complex neural network with any amount of parameters. For example, we build a
ResNet34 with model parameters reduced by more than $400$ times, which still
achieves $41.6\%$ ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG
can be applied at different scales, such as layers, blocks, or even
sub-networks. Specifically, we use the RPG to build a ResNet18 network with the
number of weights equivalent to one convolutional layer of a conventional
ResNet and show this model can achieve $67.2\%$ ImageNet top-1 accuracy. The
proposed method can be viewed as an inverse approach to model compression.
Rather than removing the unused parameters from a large model, it aims to
squeeze more information into a small number of parameters. Extensive
experiment results are provided to demonstrate the power of the proposed
recurrent parameter generator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiayun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yubei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Stella X. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1"&gt;Brian Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1"&gt;Yann LeCun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Input Dependent Sparse Gaussian Processes. (arXiv:2107.07281v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07281</id>
        <link href="http://arxiv.org/abs/2107.07281"/>
        <updated>2021-07-16T00:48:25.487Z</updated>
        <summary type="html"><![CDATA[Gaussian Processes (GPs) are Bayesian models that provide uncertainty
estimates associated to the predictions made. They are also very flexible due
to their non-parametric nature. Nevertheless, GPs suffer from poor scalability
as the number of training instances N increases. More precisely, they have a
cubic cost with respect to $N$. To overcome this problem, sparse GP
approximations are often used, where a set of $M \ll N$ inducing points is
introduced during training. The location of the inducing points is learned by
considering them as parameters of an approximate posterior distribution $q$.
Sparse GPs, combined with variational inference for inferring $q$, reduce the
training cost of GPs to $\mathcal{O}(M^3)$. Critically, the inducing points
determine the flexibility of the model and they are often located in regions of
the input space where the latent function changes. A limitation is, however,
that for some learning tasks a large number of inducing points may be required
to obtain a good prediction performance. To address this limitation, we propose
here to amortize the computation of the inducing points locations, as well as
the parameters of the variational posterior approximation q. For this, we use a
neural network that receives the observed data as an input and outputs the
inducing points locations and the parameters of $q$. We evaluate our method in
several experiments, showing that it performs similar or better than other
state-of-the-art sparse variational GP approaches. However, with our method the
number of inducing points is reduced drastically due to their dependency on the
input data. This makes our method scale to larger datasets and have faster
training and prediction times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jafrasteh_B/0/1/0/all/0/1"&gt;Bahram Jafrasteh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villacampa_Calvo_C/0/1/0/all/0/1"&gt;Carlos Villacampa-Calvo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_D/0/1/0/all/0/1"&gt;Daniel Hern&amp;#xe1;ndez-Lobato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Bayesian Learning with Metropolis-Adjusted Hamiltonian Monte Carlo. (arXiv:2107.07211v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07211</id>
        <link href="http://arxiv.org/abs/2107.07211"/>
        <updated>2021-07-16T00:48:25.480Z</updated>
        <summary type="html"><![CDATA[Federated learning performed by a decentralized networks of agents is
becoming increasingly important with the prevalence of embedded software on
autonomous devices. Bayesian approaches to learning benefit from offering more
information as to the uncertainty of a random quantity, and Langevin and
Hamiltonian methods are effective at realizing sampling from an uncertain
distribution with large parameter dimensions. Such methods have only recently
appeared in the decentralized setting, and either exclusively use stochastic
gradient Langevin and Hamiltonian Monte Carlo approaches that require a
diminishing stepsize to asymptotically sample from the posterior and are known
in practice to characterize uncertainty less faithfully than constant step-size
methods with a Metropolis adjustment, or assume strong convexity properties of
the potential function. We present the first approach to incorporating constant
stepsize Metropolis-adjusted HMC in the decentralized sampling framework, show
theoretical guarantees for consensus and probability distance to the posterior
stationary distribution, and demonstrate their effectiveness numerically on
standard real world problems, including decentralized learning of neural
networks which is known to be highly non-convex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kungurtsev_V/0/1/0/all/0/1"&gt;Vyacheslav Kungurtsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cobb_A/0/1/0/all/0/1"&gt;Adam Cobb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1"&gt;Tara Javidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalaian_B/0/1/0/all/0/1"&gt;Brian Jalaian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Topic Inference for Chest X-Ray Report Generation. (arXiv:2107.07314v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07314</id>
        <link href="http://arxiv.org/abs/2107.07314"/>
        <updated>2021-07-16T00:48:25.471Z</updated>
        <summary type="html"><![CDATA[Automating report generation for medical imaging promises to reduce workload
and assist diagnosis in clinical practice. Recent work has shown that deep
learning models can successfully caption natural images. However, learning from
medical data is challenging due to the diversity and uncertainty inherent in
the reports written by different radiologists with discrepant expertise and
experience. To tackle these challenges, we propose variational topic inference
for automatic report generation. Specifically, we introduce a set of topics as
latent variables to guide sentence generation by aligning image and language
modalities in a latent space. The topics are inferred in a conditional
variational inference framework, with each topic governing the generation of a
sentence in the report. Further, we adopt a visual attention module that
enables the model to attend to different locations in the image and generate
more informative descriptions. We conduct extensive experiments on two
benchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results
demonstrate that our proposed variational topic inference method can generate
novel reports rather than mere copies of reports used in training, while still
achieving comparable performance to state-of-the-art methods in terms of
standard language generation criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1"&gt;Ivona Najdenkoska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1"&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1"&gt;Marcel Worring&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning on a Data Diet: Finding Important Examples Early in Training. (arXiv:2107.07075v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07075</id>
        <link href="http://arxiv.org/abs/2107.07075"/>
        <updated>2021-07-16T00:48:25.464Z</updated>
        <summary type="html"><![CDATA[The recent success of deep learning has partially been driven by training
increasingly overparametrized networks on ever larger datasets. It is therefore
natural to ask: how much of the data is superfluous, which examples are
important for generalization, and how do we find them? In this work, we make
the striking observation that, on standard vision benchmarks, the initial loss
gradient norm of individual training examples, averaged over several weight
initializations, can be used to identify a smaller set of training data that is
important for generalization. Furthermore, after only a few epochs of training,
the information in gradient norms is reflected in the normed error--L2 distance
between the predicted probabilities and one hot labels--which can be used to
prune a significant fraction of the dataset without sacrificing test accuracy.
Based on this, we propose data pruning methods which use only local information
early in training, and connect them to recent work that prunes data by
discarding examples that are rarely forgotten over the course of training. Our
methods also shed light on how the underlying data distribution shapes the
training dynamics: they rank examples based on their importance for
generalization, detect noisy examples and identify subspaces of the model's
data representation that are relatively stable over training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1"&gt;Mansheej Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1"&gt;Surya Ganguli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1"&gt;Gintare Karolina Dziugaite&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parsimony-Enhanced Sparse Bayesian Learning for Robust Discovery of Partial Differential Equations. (arXiv:2107.07040v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07040</id>
        <link href="http://arxiv.org/abs/2107.07040"/>
        <updated>2021-07-16T00:48:25.449Z</updated>
        <summary type="html"><![CDATA[Robust physics discovery is of great interest for many scientific and
engineering fields. Inspired by the principle that a representative model is
the one simplest possible, a new model selection criteria considering both
model's Parsimony and Sparsity is proposed. A Parsimony Enhanced Sparse
Bayesian Learning (PeSBL) method is developed for discovering the governing
Partial Differential Equations (PDEs) of nonlinear dynamical systems. Compared
with the conventional Sparse Bayesian Learning (SBL) method, the PeSBL method
promotes parsimony of the learned model in addition to its sparsity. In this
method, the parsimony of model terms is evaluated using their locations in the
prescribed candidate library, for the first time, considering the increased
complexity with the power of polynomials and the order of spatial derivatives.
Subsequently, the model parameters are updated through Bayesian inference with
the raw data. This procedure aims to reduce the error associated with the
possible loss of information in data preprocessing and numerical
differentiation prior to sparse regression. Results of numerical case studies
indicate that the governing PDEs of many canonical dynamical systems can be
correctly identified using the proposed PeSBL method from highly noisy data (up
to 50% in the current study). Next, the proposed methodology is extended for
stochastic PDE learning where all parameters and modeling error are considered
as random variables. Hierarchical Bayesian Inference (HBI) is integrated with
the proposed framework for stochastic PDE learning from a population of
observations. Finally, the proposed PeSBL is demonstrated for system response
prediction with uncertainties and anomaly diagnosis. Codes of all demonstrated
examples in this study are available on the website: https://github.com/ymlasu.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepHyperion: Exploring the Feature Space of Deep Learning-Based Systems through Illumination Search. (arXiv:2107.06997v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06997</id>
        <link href="http://arxiv.org/abs/2107.06997"/>
        <updated>2021-07-16T00:48:25.429Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL) has been successfully applied to a wide range of
application domains, including safety-critical ones. Several DL testing
approaches have been recently proposed in the literature but none of them aims
to assess how different interpretable features of the generated inputs affect
the system's behaviour. In this paper, we resort to Illumination Search to find
the highest-performing test cases (i.e., misbehaving and closest to
misbehaving), spread across the cells of a map representing the feature space
of the system. We introduce a methodology that guides the users of our approach
in the tasks of identifying and quantifying the dimensions of the feature space
for a given domain. We developed DeepHyperion, a search-based tool for DL
systems that illuminates, i.e., explores at large, the feature space, by
providing developers with an interpretable feature map where automatically
generated inputs are placed along with information about the exposed
behaviours.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zohdinasab_T/0/1/0/all/0/1"&gt;Tahereh Zohdinasab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riccio_V/0/1/0/all/0/1"&gt;Vincenzo Riccio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gambi_A/0/1/0/all/0/1"&gt;Alessio Gambi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tonella_P/0/1/0/all/0/1"&gt;Paolo Tonella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Channel Auto-Encoders and a Novel Dataset for Learning Domain Invariant Representations of Histopathology Images. (arXiv:2107.07271v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07271</id>
        <link href="http://arxiv.org/abs/2107.07271"/>
        <updated>2021-07-16T00:48:25.382Z</updated>
        <summary type="html"><![CDATA[Domain shift is a problem commonly encountered when developing automated
histopathology pipelines. The performance of machine learning models such as
convolutional neural networks within automated histopathology pipelines is
often diminished when applying them to novel data domains due to factors
arising from differing staining and scanning protocols. The Dual-Channel
Auto-Encoder (DCAE) model was previously shown to produce feature
representations that are less sensitive to appearance variation introduced by
different digital slide scanners. In this work, the Multi-Channel Auto-Encoder
(MCAE) model is presented as an extension to DCAE which learns from more than
two domains of data. Additionally, a synthetic dataset is generated using
CycleGANs that contains aligned tissue images that have had their appearance
synthetically modified. Experimental results show that the MCAE model produces
feature representations that are less sensitive to inter-domain variations than
the comparative StaNoSA method when tested on the novel synthetic data.
Additionally, the MCAE and StaNoSA models are tested on a novel tissue
classification task. The results of this experiment show the MCAE model out
performs the StaNoSA model by 5 percentage-points in the f1-score. These
results show that the MCAE model is able to generalise better to novel data and
tasks than existing approaches by actively learning normalised feature
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Moyes_A/0/1/0/all/0/1"&gt;Andrew Moyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gault_R/0/1/0/all/0/1"&gt;Richard Gault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ming_J/0/1/0/all/0/1"&gt;Ji Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Crookes_D/0/1/0/all/0/1"&gt;Danny Crookes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relative Lipschitzness in Extragradient Methods and a Direct Recipe for Acceleration. (arXiv:2011.06572v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.06572</id>
        <link href="http://arxiv.org/abs/2011.06572"/>
        <updated>2021-07-16T00:48:25.365Z</updated>
        <summary type="html"><![CDATA[We show that standard extragradient methods (i.e. mirror prox and dual
extrapolation) recover optimal accelerated rates for first-order minimization
of smooth convex functions. To obtain this result we provide a fine-grained
characterization of the convergence rates of extragradient methods for solving
monotone variational inequalities in terms of a natural condition we call
relative Lipschitzness. We further generalize this framework to handle local
and randomized notions of relative Lipschitzness and thereby recover rates for
box-constrained $\ell_\infty$ regression based on area convexity and complexity
bounds achieved by accelerated (randomized) coordinate descent for smooth
convex function minimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cohen_M/0/1/0/all/0/1"&gt;Michael B. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sidford_A/0/1/0/all/0/1"&gt;Aaron Sidford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tian_K/0/1/0/all/0/1"&gt;Kevin Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mapping Learning Algorithms on Data, a useful step for optimizing performances and their comparison. (arXiv:2107.06981v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06981</id>
        <link href="http://arxiv.org/abs/2107.06981"/>
        <updated>2021-07-16T00:48:25.343Z</updated>
        <summary type="html"><![CDATA[In the paper, we propose a novel methodology to map learning algorithms on
data (performance map) in order to gain more insights in the distribution of
their performances across their parameter space. This methodology provides
useful information when selecting a learner's best configuration for the data
at hand, and it also enhances the comparison of learners across learning
contexts. In order to explain the proposed methodology, the study introduces
the notions of learning context, performance map, and high performance
function. It then applies these concepts to a variety of learning contexts to
show how their use can provide more insights in a learner's behavior, and can
enhance the comparison of learners across learning contexts. The study is
completed by an extensive experimental study describing how the proposed
methodology can be applied.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neri_F/0/1/0/all/0/1"&gt;Filippo Neri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07502</id>
        <link href="http://arxiv.org/abs/2107.07502"/>
        <updated>2021-07-16T00:48:25.336Z</updated>
        <summary type="html"><![CDATA[Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yiwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zetian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jason Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Leslie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peter Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Michelle A. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assign Hysteresis Parameter For Ericsson BTS Power Saving Algorithm Using Unsupervised Learning. (arXiv:2107.07412v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.07412</id>
        <link href="http://arxiv.org/abs/2107.07412"/>
        <updated>2021-07-16T00:48:25.316Z</updated>
        <summary type="html"><![CDATA[Gaza Strip suffers from a chronic electricity deficit that affects all
industries including the telecommunication field, so there is a need to
optimize and reduce power consumption of the telecommunication equipment. In
this paper we propose a new model that helps GSM radio frequency engineers to
choose the optimal value of hysteresis parameter for Ericsson BTS power saving
algorithm which aims to switch OFF unused frequency channels, our model is
based on unsupervised machine learning clustering K-means algorithm. By using
our model with BTS power saving algorithm we reduce number of active TRX by
20.9%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sahmoud_T/0/1/0/all/0/1"&gt;Thaer Sahmoud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ashor_W/0/1/0/all/0/1"&gt;Wesam Ashor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Vision Transformer with Squeeze and Excitation for Facial Expression Recognition. (arXiv:2107.03107v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03107</id>
        <link href="http://arxiv.org/abs/2107.03107"/>
        <updated>2021-07-16T00:48:25.308Z</updated>
        <summary type="html"><![CDATA[As various databases of facial expressions have been made accessible over the
last few decades, the Facial Expression Recognition (FER) task has gotten a lot
of interest. The multiple sources of the available databases raised several
challenges for facial recognition task. These challenges are usually addressed
by Convolution Neural Network (CNN) architectures. Different from CNN models, a
Transformer model based on attention mechanism has been presented recently to
address vision tasks. One of the major issue with Transformers is the need of a
large data for training, while most FER databases are limited compared to other
vision applications. Therefore, we propose in this paper to learn a vision
Transformer jointly with a Squeeze and Excitation (SE) block for FER task. The
proposed method is evaluated on different publicly available FER databases
including CK+, JAFFE,RAF-DB and SFEW. Experiments demonstrate that our model
outperforms state-of-the-art methods on CK+ and SFEW and achieves competitive
results on JAFFE and RAF-DB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aouayeb_M/0/1/0/all/0/1"&gt;Mouath Aouayeb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soladie_C/0/1/0/all/0/1"&gt;Catherine Soladie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kpalma_K/0/1/0/all/0/1"&gt;Kidiyo Kpalma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seguier_R/0/1/0/all/0/1"&gt;Renaud Seguier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Designing GANs: A Likelihood Ratio Approach. (arXiv:2002.00865v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00865</id>
        <link href="http://arxiv.org/abs/2002.00865"/>
        <updated>2021-07-16T00:48:25.301Z</updated>
        <summary type="html"><![CDATA[We are interested in the design of generative networks. The training of these
mathematical structures is mostly performed with the help of adversarial
(min-max) optimization problems. We propose a simple methodology for
constructing such problems assuring, at the same time, consistency of the
corresponding solution. We give characteristic examples developed by our
method, some of which can be recognized from other applications, and some are
introduced here for the first time. We present a new metric, the likelihood
ratio, that can be employed online to examine the convergence and stability
during the training of different Generative Adversarial Networks (GANs).
Finally, we compare various possibilities by applying them to well-known
datasets using neural networks of different configurations and sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Basioti_K/0/1/0/all/0/1"&gt;Kalliopi Basioti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moustakides_G/0/1/0/all/0/1"&gt;George V. Moustakides&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MCL-GAN: Generative Adversarial Networks with Multiple Specialized Discriminators. (arXiv:2107.07260v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07260</id>
        <link href="http://arxiv.org/abs/2107.07260"/>
        <updated>2021-07-16T00:48:25.283Z</updated>
        <summary type="html"><![CDATA[We propose a generative adversarial network with multiple discriminators,
where each discriminator is specialized to distinguish the subset of a real
dataset. This approach facilitates learning a generator coinciding with the
underlying data distribution and thus mitigates the chronic mode collapse
problem. From the inspiration of multiple choice learning, we guide each
discriminator to have expertise in the subset of the entire data and allow the
generator to find reasonable correspondences between the latent and real data
spaces automatically without supervision for training examples and the number
of discriminators. Despite the use of multiple discriminators, the backbone
networks are shared across the discriminators and the increase of training cost
is minimized. We demonstrate the effectiveness of our algorithm in the standard
datasets using multiple evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jinyoung Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bohyung Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FMNet: Latent Feature-wise Mapping Network for Cleaning up Noisy Micro-Doppler Spectrogram. (arXiv:2107.07312v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.07312</id>
        <link href="http://arxiv.org/abs/2107.07312"/>
        <updated>2021-07-16T00:48:25.277Z</updated>
        <summary type="html"><![CDATA[Micro-Doppler signatures contain considerable information about target
dynamics. However, the radar sensing systems are easily affected by noisy
surroundings, resulting in uninterpretable motion patterns on the micro-Doppler
spectrogram. Meanwhile, radar returns often suffer from multipath, clutter and
interference. These issues lead to difficulty in, for example motion feature
extraction, activity classification using micro Doppler signatures ($\mu$-DS),
etc. In this paper, we propose a latent feature-wise mapping strategy, called
Feature Mapping Network (FMNet), to transform measured spectrograms so that
they more closely resemble the output from a simulation under the same
conditions. Based on measured spectrogram and the matched simulated data, our
framework contains three parts: an Encoder which is used to extract latent
representations/features, a Decoder outputs reconstructed spectrogram according
to the latent features, and a Discriminator minimizes the distance of latent
features of measured and simulated data. We demonstrate the FMNet with six
activities data and two experimental scenarios, and final results show strong
enhanced patterns and can keep actual motion information to the greatest
extent. On the other hand, we also propose a novel idea which trains a
classifier with only simulated data and predicts new measured samples after
cleaning them up with the FMNet. From final classification results, we can see
significant improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1"&gt;Chong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenda Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vishwakarma_S/0/1/0/all/0/1"&gt;Shelly Vishwakarma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_F/0/1/0/all/0/1"&gt;Fangzhan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Julier_S/0/1/0/all/0/1"&gt;Simon Julier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chetty_K/0/1/0/all/0/1"&gt;Kevin Chetty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Accurate Human Activity Recognition for Embedded Devices Using Multi-level Distillation. (arXiv:2107.07331v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07331</id>
        <link href="http://arxiv.org/abs/2107.07331"/>
        <updated>2021-07-16T00:48:25.265Z</updated>
        <summary type="html"><![CDATA[Human activity recognition (HAR) based on IMU sensors is an essential domain
in ubiquitous computing. Because of the improving trend to deploy artificial
intelligence into IoT devices or smartphones, more researchers design the HAR
models for embedded devices. We propose a plug-and-play HAR modeling pipeline
with multi-level distillation to build deep convolutional HAR models with
native support of embedded devices. SMLDist consists of stage distillation,
memory distillation, and logits distillation, which covers all the information
flow of the deep models. Stage distillation constrains the learning direction
of the intermediate features. Memory distillation teaches the student models
how to explain and store the inner relationship between high-dimensional
features based on Hopfield networks. Logits distillation constructs distilled
logits by a smoothed conditional rule to keep the probable distribution and
improve the correctness of the soft target. We compare the performance of
accuracy, F1 macro score, and energy cost on the embedded platform of various
state-of-the-art HAR frameworks with a MobileNet V3 model built by SMLDist. The
produced model has well balance with robustness, efficiency, and accuracy.
SMLDist can also compress the models with minor performance loss in an equal
compression rate than other state-of-the-art knowledge distillation methods on
seven public datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Runze Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haiyong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Fang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xuechun Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhiqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yida Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training for temporal sparsity in deep neural networks, application in video processing. (arXiv:2107.07305v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07305</id>
        <link href="http://arxiv.org/abs/2107.07305"/>
        <updated>2021-07-16T00:48:25.258Z</updated>
        <summary type="html"><![CDATA[Activation sparsity improves compute efficiency and resource utilization in
sparsity-aware neural network accelerators. As the predominant operation in
DNNs is multiply-accumulate (MAC) of activations with weights to compute inner
products, skipping operations where (at least) one of the two operands is zero
can make inference more efficient in terms of latency and power. Spatial
sparsification of activations is a popular topic in DNN literature and several
methods have already been established to bias a DNN for it. On the other hand,
temporal sparsity is an inherent feature of bio-inspired spiking neural
networks (SNNs), which neuromorphic processing exploits for hardware
efficiency. Introducing and exploiting spatio-temporal sparsity, is a topic
much less explored in DNN literature, but in perfect resonance with the trend
in DNN, to shift from static signal processing to more streaming signal
processing. Towards this goal, in this paper we introduce a new DNN layer
(called Delta Activation Layer), whose sole purpose is to promote temporal
sparsity of activations during training. A Delta Activation Layer casts
temporal sparsity into spatial activation sparsity to be exploited when
performing sparse tensor multiplications in hardware. By employing delta
inference and ``the usual'' spatial sparsification heuristics during training,
the resulting model learns to exploit not only spatial but also temporal
activation sparsity (for a given input data distribution). One may use the
Delta Activation Layer either during vanilla training or during a refinement
phase. We have implemented Delta Activation Layer as an extension of the
standard Tensoflow-Keras library, and applied it to train deep neural networks
on the Human Action Recognition (UCF101) dataset. We report an almost 3x
improvement of activation sparsity, with recoverable loss of model accuracy
after longer training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yousefzadeh_A/0/1/0/all/0/1"&gt;Amirreza Yousefzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sifalakis_M/0/1/0/all/0/1"&gt;Manolis Sifalakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tournesol: A quest for a large, secure and trustworthy database of reliable human judgments. (arXiv:2107.07334v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.07334</id>
        <link href="http://arxiv.org/abs/2107.07334"/>
        <updated>2021-07-16T00:48:25.252Z</updated>
        <summary type="html"><![CDATA[Today's large-scale algorithms have become immensely influential, as they
recommend and moderate the content that billions of humans are exposed to on a
daily basis. They are the de-facto regulators of our societies' information
diet, from shaping opinions on public health to organizing groups for social
movements. This creates serious concerns, but also great opportunities to
promote quality information. Addressing the concerns and seizing the
opportunities is a challenging, enormous and fabulous endeavor, as intuitively
appealing ideas often come with unwanted {\it side effects}, and as it requires
us to think about what we deeply prefer.

Understanding how today's large-scale algorithms are built is critical to
determine what interventions will be most effective. Given that these
algorithms rely heavily on {\it machine learning}, we make the following key
observation: \emph{any algorithm trained on uncontrolled data must not be
trusted}. Indeed, a malicious entity could take control over the data, poison
it with dangerously manipulative fabricated inputs, and thereby make the
trained algorithm extremely unsafe. We thus argue that the first step towards
safe and ethical large-scale algorithms must be the collection of a large,
secure and trustworthy dataset of reliable human judgments.

To achieve this, we introduce \emph{Tournesol}, an open source platform
available at \url{https://tournesol.app}. Tournesol aims to collect a large
database of human judgments on what algorithms ought to widely recommend (and
what they ought to stop widely recommending). We outline the structure of the
Tournesol database, the key features of the Tournesol platform and the main
hurdles that must be overcome to make it a successful project. Most
importantly, we argue that, if successful, Tournesol may then serve as the
essential foundation for any safe and ethical large-scale algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1"&gt;L&amp;#xea;-Nguy&amp;#xea;n Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faucon_L/0/1/0/all/0/1"&gt;Louis Faucon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jungo_A/0/1/0/all/0/1"&gt;Aidan Jungo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Volodin_S/0/1/0/all/0/1"&gt;Sergei Volodin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papuc_D/0/1/0/all/0/1"&gt;Dalia Papuc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liossatos_O/0/1/0/all/0/1"&gt;Orfeas Liossatos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crulis_B/0/1/0/all/0/1"&gt;Ben Crulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tighanimine_M/0/1/0/all/0/1"&gt;Mariame Tighanimine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constantin_I/0/1/0/all/0/1"&gt;Isabela Constantin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kucherenko_A/0/1/0/all/0/1"&gt;Anastasiia Kucherenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maurer_A/0/1/0/all/0/1"&gt;Alexandre Maurer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grimberg_F/0/1/0/all/0/1"&gt;Felix Grimberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nitu_V/0/1/0/all/0/1"&gt;Vlad Nitu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vossen_C/0/1/0/all/0/1"&gt;Chris Vossen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rouault_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Rouault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Mhamdi_E/0/1/0/all/0/1"&gt;El-Mahdi El-Mhamdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A unified framework for bandit multiple testing. (arXiv:2107.07322v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07322</id>
        <link href="http://arxiv.org/abs/2107.07322"/>
        <updated>2021-07-16T00:48:25.245Z</updated>
        <summary type="html"><![CDATA[In bandit multiple hypothesis testing, each arm corresponds to a different
null hypothesis that we wish to test, and the goal is to design adaptive
algorithms that correctly identify large set of interesting arms (true
discoveries), while only mistakenly identifying a few uninteresting ones (false
discoveries). One common metric in non-bandit multiple testing is the false
discovery rate (FDR). We propose a unified, modular framework for bandit FDR
control that emphasizes the decoupling of exploration and summarization of
evidence. We utilize the powerful martingale-based concept of ``e-processes''
to ensure FDR control for arbitrary composite nulls, exploration rules and
stopping times in generic problem settings. In particular, valid FDR control
holds even if the reward distributions of the arms could be dependent, multiple
arms may be queried simultaneously, and multiple (cooperating or competing)
agents may be querying arms, covering combinatorial semi-bandit type settings
as well. Prior work has considered in great detail the setting where each arm's
reward distribution is independent and sub-Gaussian, and a single arm is
queried at each step. Our framework recovers matching sample complexity
guarantees in this special case, and performs comparably or better in practice.
For other settings, sample complexities will depend on the finer details of the
problem (composite nulls being tested, exploration algorithm, data dependence
structure, stopping rule) and we do not explore these; our contribution is to
show that the FDR guarantee is clean and entirely agnostic to these details.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Ziyu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruodu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutation is all you need. (arXiv:2107.07343v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07343</id>
        <link href="http://arxiv.org/abs/2107.07343"/>
        <updated>2021-07-16T00:48:25.203Z</updated>
        <summary type="html"><![CDATA[Neural architecture search (NAS) promises to make deep learning accessible to
non-experts by automating architecture engineering of deep neural networks.
BANANAS is one state-of-the-art NAS method that is embedded within the Bayesian
optimization framework. Recent experimental findings have demonstrated the
strong performance of BANANAS on the NAS-Bench-101 benchmark being determined
by its path encoding and not its choice of surrogate model. We present
experimental results suggesting that the performance of BANANAS on the
NAS-Bench-301 benchmark is determined by its acquisition function optimizer,
which minimally mutates the incumbent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_L/0/1/0/all/0/1"&gt;Lennart Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfisterer_F/0/1/0/all/0/1"&gt;Florian Pfisterer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_M/0/1/0/all/0/1"&gt;Martin Binder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1"&gt;Bernd Bischl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Framework for A Personalized Intelligent Assistant to Elderly People for Activities of Daily Living. (arXiv:2107.07344v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.07344</id>
        <link href="http://arxiv.org/abs/2107.07344"/>
        <updated>2021-07-16T00:48:25.194Z</updated>
        <summary type="html"><![CDATA[The increasing population of elderly people is associated with the need to
meet their increasing requirements and to provide solutions that can improve
their quality of life in a smart home. In addition to fear and anxiety towards
interfacing with systems; cognitive disabilities, weakened memory, disorganized
behavior and even physical limitations are some of the problems that elderly
people tend to face with increasing age. The essence of providing
technology-based solutions to address these needs of elderly people and to
create smart and assisted living spaces for the elderly; lies in developing
systems that can adapt by addressing their diversity and can augment their
performances in the context of their day to day goals. Therefore, this work
proposes a framework for development of a Personalized Intelligent Assistant to
help elderly people perform Activities of Daily Living (ADLs) in a smart and
connected Internet of Things (IoT) based environment. This Personalized
Intelligent Assistant can analyze different tasks performed by the user and
recommend activities by considering their daily routine, current affective
state and the underlining user experience. To uphold the efficacy of this
proposed framework, it has been tested on a couple of datasets for modelling an
average user and a specific user respectively. The results presented show that
the model achieves a performance accuracy of 73.12% when modelling a specific
user, which is considerably higher than its performance while modelling an
average user, this upholds the relevance for development and implementation of
this proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nirmalya Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chia Y. Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Randomized ReLU Activation for Uncertainty Estimation of Deep Neural Networks. (arXiv:2107.07197v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07197</id>
        <link href="http://arxiv.org/abs/2107.07197"/>
        <updated>2021-07-16T00:48:25.172Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have successfully learned useful data
representations in various tasks, however, assessing the reliability of these
representations remains a challenge. Deep Ensemble is widely considered the
state-of-the-art method for uncertainty estimation, but it is very expensive to
train and test. MC-Dropout is another alternative method, which is less
expensive but lacks the diversity of predictions. To get more diverse
predictions in less time, we introduce Randomized ReLU Activation (RRA)
framework. Under the framework, we propose two strategies, MC-DropReLU and
MC-RReLU, to estimate uncertainty. Instead of randomly dropping some neurons of
the network as in MC-Dropout, the RRA framework adds randomness to the
activation function module, making the outputs diverse. As far as we know, this
is the first attempt to add randomness to the activation function module to
generate predictive uncertainty. We analyze and compare the output diversity of
MC-Dropout and our method from the variance perspective and obtain the
relationship between the hyperparameters and output diversity in the two
methods. Moreover, our method is simple to implement and does not need to
modify the existing model. We experimentally validate the RRA framework on
three widely used datasets, CIFAR10, CIFAR100, and TinyImageNet. The
experiments demonstrate that our method has competitive performance but is more
favorable in training time and memory requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yufeng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1"&gt;Zhiqiang Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1"&gt;Tingsong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1"&gt;Wen Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Image Features Boost Housing Market Predictions?. (arXiv:2107.07148v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07148</id>
        <link href="http://arxiv.org/abs/2107.07148"/>
        <updated>2021-07-16T00:48:25.154Z</updated>
        <summary type="html"><![CDATA[The attractiveness of a property is one of the most interesting, yet
challenging, categories to model. Image characteristics are used to describe
certain attributes, and to examine the influence of visual factors on the price
or timeframe of the listing. In this paper, we propose a set of techniques for
the extraction of visual features for efficient numerical inclusion in
modern-day predictive algorithms. We discuss techniques such as Shannon's
entropy, calculating the center of gravity, employing image segmentation, and
using Convolutional Neural Networks. After comparing these techniques as
applied to a set of property-related images (indoor, outdoor, and satellite),
we conclude the following: (i) the entropy is the most efficient single-digit
visual measure for housing price prediction; (ii) image segmentation is the
most important visual feature for the prediction of housing lifespan; and (iii)
deep image features can be used to quantify interior characteristics and
contribute to captivation modeling. The set of 40 image features selected here
carries a significant amount of predictive power and outperforms some of the
strongest metadata predictors. Without any need to replace a human expert in a
real-estate appraisal process, we conclude that the techniques presented in
this paper can efficiently describe visible characteristics, thus introducing
perceived attractiveness as a quantitative measure into the predictive modeling
of housing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1"&gt;Zona Kostic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jevremovic_A/0/1/0/all/0/1"&gt;Aleksandar Jevremovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning for Recommendations at Grubhub. (arXiv:2107.07106v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07106</id>
        <link href="http://arxiv.org/abs/2107.07106"/>
        <updated>2021-07-16T00:48:25.137Z</updated>
        <summary type="html"><![CDATA[We propose a method to easily modify existing offline Recommender Systems to
run online using Transfer Learning. Online Learning for Recommender Systems has
two main advantages: quality and scale. Like many Machine Learning algorithms
in production if not regularly retrained will suffer from Concept Drift. A
policy that is updated frequently online can adapt to drift faster than a batch
system. This is especially true for user-interaction systems like recommenders
where the underlying distribution can shift drastically to follow user
behaviour. As a platform grows rapidly like Grubhub, the cost of running batch
training jobs becomes material. A shift from stateless batch learning offline
to stateful incremental learning online can recover, for example, at Grubhub,
up to a 45x cost savings and a +20% metrics increase. There are a few
challenges to overcome with the transition to online stateful learning, namely
convergence, non-stationary embeddings and off-policy evaluation, which we
explore from our experiences running this system in production.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Egg_A/0/1/0/all/0/1"&gt;Alex Egg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeFed: A Principled Decentralized and Privacy-Preserving Federated Learning Algorithm. (arXiv:2107.07171v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07171</id>
        <link href="http://arxiv.org/abs/2107.07171"/>
        <updated>2021-07-16T00:48:25.121Z</updated>
        <summary type="html"><![CDATA[Federated learning enables a large number of clients to participate in
learning a shared model while maintaining the training data stored in each
client, which protects data privacy and security. Till now, federated learning
frameworks are built in a centralized way, in which a central client is needed
for collecting and distributing information from every other client. This not
only leads to high communication pressure at the central client, but also
renders the central client highly vulnerable to failure and attack. Here we
propose a principled decentralized federated learning algorithm (DeFed), which
removes the central client in the classical Federated Averaging (FedAvg)
setting and only relies information transmission between clients and their
local neighbors. The proposed DeFed algorithm is proven to reach the global
minimum with a convergence rate of $O(1/T)$ when the loss function is smooth
and strongly convex, where $T$ is the number of iterations in gradient descent.
Finally, the proposed algorithm has been applied to a number of toy examples to
demonstrate its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Ye Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Ruijuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chuan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Maolin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_F/0/1/0/all/0/1"&gt;Feng Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xinlei Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MURAL: Meta-Learning Uncertainty-Aware Rewards for Outcome-Driven Reinforcement Learning. (arXiv:2107.07184v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07184</id>
        <link href="http://arxiv.org/abs/2107.07184"/>
        <updated>2021-07-16T00:48:25.115Z</updated>
        <summary type="html"><![CDATA[Exploration in reinforcement learning is a challenging problem: in the worst
case, the agent must search for reward states that could be hidden anywhere in
the state space. Can we define a more tractable class of RL problems, where the
agent is provided with examples of successful outcomes? In this problem
setting, the reward function can be obtained automatically by training a
classifier to categorize states as successful or not. If trained properly, such
a classifier can not only afford a reward function, but actually provide a
well-shaped objective landscape that both promotes progress toward good states
and provides a calibrated exploration bonus. In this work, we we show that an
uncertainty aware classifier can solve challenging reinforcement learning
problems by both encouraging exploration and provided directed guidance towards
positive outcomes. We propose a novel mechanism for obtaining these calibrated,
uncertainty-aware classifiers based on an amortized technique for computing the
normalized maximum likelihood (NML) distribution, also showing how these
techniques can be made computationally tractable by leveraging tools from
meta-learning. We show that the resulting algorithm has a number of intriguing
connections to both count-based exploration methods and prior algorithms for
learning reward functions, while also providing more effective guidance towards
the goal. We demonstrate that our algorithm solves a number of challenging
navigation and robotic manipulation tasks which prove difficult or impossible
for prior methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kevin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1"&gt;Ashwin Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1"&gt;Vitchyr Pong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aurick Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Justin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Backprop-Free Reinforcement Learning with Active Neural Generative Coding. (arXiv:2107.07046v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07046</id>
        <link href="http://arxiv.org/abs/2107.07046"/>
        <updated>2021-07-16T00:48:25.095Z</updated>
        <summary type="html"><![CDATA[In humans, perceptual awareness facilitates the fast recognition and
extraction of information from sensory input. This awareness largely depends on
how the human agent interacts with the environment. In this work, we propose
active neural generative coding, a computational framework for learning
action-driven generative models without backpropagation of errors (backprop) in
dynamic environments. Specifically, we develop an intelligent agent that
operates even with sparse rewards, drawing inspiration from the cognitive
theory of planning as inference. We demonstrate on several control problems, in
the online learning setting, that our proposed modeling framework performs
competitively with deep Q-learning models. The robust performance of our agent
offers promising evidence that a backprop-free approach for neural inference
and learning can drive goal-directed behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1"&gt;Alexander Ororbia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mali_A/0/1/0/all/0/1"&gt;Ankur Mali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Robust Deep Learning Workflow to Predict Multiphase Flow Behavior during Geological CO2 Sequestration Injection and Post-Injection Periods. (arXiv:2107.07274v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07274</id>
        <link href="http://arxiv.org/abs/2107.07274"/>
        <updated>2021-07-16T00:48:25.087Z</updated>
        <summary type="html"><![CDATA[This paper contributes to the development and evaluation of a deep learning
workflow that accurately and efficiently predicts the temporal-spatial
evolution of pressure and CO2 plumes during injection and post-injection
periods of geologic CO2 sequestration (GCS) operations. Based on a Fourier
Neuron Operator, the deep learning workflow takes input variables or features
including rock properties, well operational controls and time steps, and
predicts the state variables of pressure and CO2 saturation. To further improve
the predictive fidelity, separate deep learning models are trained for CO2
injection and post-injection periods due the difference in primary driving
force of fluid flow and transport during these two phases. We also explore
different combinations of features to predict the state variables. We use a
realistic example of CO2 injection and storage in a 3D heterogeneous saline
aquifer, and apply the deep learning workflow that is trained from
physics-based simulation data and emulate the physics process. Through this
numerical experiment, we demonstrate that using two separate deep learning
models to distinguish post-injection from injection period generates the most
accurate prediction of pressure, and a single deep learning model of the whole
GCS process including the cumulative injection volume of CO2 as a deep learning
feature, leads to the most accurate prediction of CO2 saturation. For the
post-injection period, it is key to use cumulative CO2 injection volume to
inform the deep learning models about the total carbon storage when predicting
either pressure or saturation. The deep learning workflow not only provides
high predictive fidelity across temporal and spatial scales, but also offers a
speedup of 250 times compared to full physics reservoir simulation, and thus
will be a significant predictive tool for engineers to manage the long term
process of GCS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bicheng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bailian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harp_D/0/1/0/all/0/1"&gt;Dylan Robert Harp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pawar_R/0/1/0/all/0/1"&gt;Rajesh J. Pawar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data. (arXiv:2105.04727v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04727</id>
        <link href="http://arxiv.org/abs/2105.04727"/>
        <updated>2021-07-16T00:48:25.080Z</updated>
        <summary type="html"><![CDATA[We propose FEDENHANCE, an unsupervised federated learning (FL) approach for
speech enhancement and separation with non-IID distributed data across multiple
clients. We simulate a real-world scenario where each client only has access to
a few noisy recordings from a limited and disjoint number of speakers (hence
non-IID). Each client trains their model in isolation using mixture invariant
training while periodically providing updates to a central server. Our
experiments show that our approach achieves competitive enhancement performance
compared to IID training on a single device and that we can further facilitate
the convergence speed and the overall performance using transfer learning on
the server-side. Moreover, we show that we can effectively combine updates from
clients trained locally with supervised and unsupervised losses. We also
release a new dataset LibriFSD50K and its creation recipe in order to
facilitate FL research for source separation problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1"&gt;Efthymios Tzinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casebeer_J/0/1/0/all/0/1"&gt;Jonah Casebeer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhepei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1"&gt;Paris Smaragdis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (arXiv:2107.06400v1 [cs.CL] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.06400</id>
        <link href="http://arxiv.org/abs/2107.06400"/>
        <updated>2021-07-16T00:48:25.059Z</updated>
        <summary type="html"><![CDATA[One of the stratagems used to deceive spam filters is to substitute vocables
with synonyms or similar words that turn the message unrecognisable by the
detection algorithms. In this paper we investigate whether the recent
development of language models sensitive to the semantics and context of words,
such as Google's BERT, may be useful to overcome this adversarial attack
(called "Mad-lib" as per the word substitution game). Using a dataset of 5572
SMS spam messages, we first established a baseline of detection performance
using widely known document representation models (BoW and TFIDF) and the novel
BERT model, coupled with a variety of classification algorithms (Decision Tree,
kNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we
built a thesaurus of the vocabulary contained in these messages, and set up a
Mad-lib attack experiment in which we modified each message of a held out
subset of data (not used in the baseline experiment) with different rates of
substitution of original words with synonyms from the thesaurus. Lastly, we
evaluated the detection performance of the three representation models (BoW,
TFIDF and BERT) coupled with the best classifier from the baseline experiment
(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)
in the original dataset, whereas the BERT model obtained 96%. On the other
hand, the Mad-lib attack experiment showed that BERT encodings manage to
maintain a similar BA performance of 96% with an average substitution rate of
1.82 words per message, and 95% with 3.34 words substituted per message. In
contrast, the BA performance of the BoW and TFIDF encoders dropped to chance.
These results hint at the potential advantage of BERT models to combat these
type of ingenious attacks, offsetting to some extent for the inappropriate use
of semantic relationships in language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rojas_Galeano_S/0/1/0/all/0/1"&gt;Sergio Rojas-Galeano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the expressivity of bi-Lipschitz normalizing flows. (arXiv:2107.07232v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07232</id>
        <link href="http://arxiv.org/abs/2107.07232"/>
        <updated>2021-07-16T00:48:25.051Z</updated>
        <summary type="html"><![CDATA[An invertible function is bi-Lipschitz if both the function and its inverse
have bounded Lipschitz constants. Nowadays, most Normalizing Flows are
bi-Lipschitz by design or by training to limit numerical errors (among other
things). In this paper, we discuss the expressivity of bi-Lipschitz Normalizing
Flows and identify several target distributions that are difficult to
approximate using such models. Then, we characterize the expressivity of
bi-Lipschitz Normalizing Flows by giving several lower bounds on the Total
Variation distance between these particularly unfavorable distributions and
their best possible approximation. Finally, we discuss potential remedies which
include using more complex latent distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verine_A/0/1/0/all/0/1"&gt;Alexandre Verine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Negrevergne_B/0/1/0/all/0/1"&gt;Benjamin Negrevergne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1"&gt;Fabrice Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chevaleyre_Y/0/1/0/all/0/1"&gt;Yann Chevaleyre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Environment Inference for Invariant Learning. (arXiv:2010.07249v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07249</id>
        <link href="http://arxiv.org/abs/2010.07249"/>
        <updated>2021-07-16T00:48:25.044Z</updated>
        <summary type="html"><![CDATA[Learning models that gracefully handle distribution shifts is central to
research on domain generalization, robust optimization, and fairness. A
promising formulation is domain-invariant learning, which identifies the key
issue of learning which features are domain-specific versus domain-invariant.
An important assumption in this area is that the training examples are
partitioned into "domains" or "environments". Our focus is on the more common
setting where such partitions are not provided. We propose EIIL, a general
framework for domain-invariant learning that incorporates Environment Inference
to directly infer partitions that are maximally informative for downstream
Invariant Learning. We show that EIIL outperforms invariant learning methods on
the CMNIST benchmark without using environment labels, and significantly
outperforms ERM on worst-group performance in the Waterbirds and CivilComments
datasets. Finally, we establish connections between EIIL and algorithmic
fairness, which enables EIIL to improve accuracy and calibration in a fair
prediction problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1"&gt;Elliot Creager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobsen_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rn-Henrik Jacobsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1"&gt;Richard Zemel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JKOnet: Proximal Optimal Transport Modeling of Population Dynamics. (arXiv:2106.06345v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06345</id>
        <link href="http://arxiv.org/abs/2106.06345"/>
        <updated>2021-07-16T00:48:25.020Z</updated>
        <summary type="html"><![CDATA[Consider a heterogeneous population of points evolving with time. While the
population evolves, both in size and nature, we can observe it periodically,
through snapshots taken at different timestamps. Each of these snapshots is
formed by sampling points from the population at that time, and then creating
features to recover point clouds. While these snapshots describe the
population's evolution on aggregate, they do not provide directly insights on
individual trajectories. This scenario is encountered in several applications,
notably single-cell genomics experiments, tracking of particles, or when
studying crowd motion. In this paper, we propose to model that dynamic as
resulting from the celebrated Jordan-Kinderlehrer-Otto (JKO) proximal scheme.
The JKO scheme posits that the configuration taken by a population at time $t$
is one that trades off a decrease w.r.t. an energy (the model we seek to learn)
penalized by an optimal transport distance w.r.t. the previous configuration.
To that end, we propose JKOnet, a neural architecture that combines an energy
model on measures, with (small) optimal displacements solved with input convex
neural networks (ICNN). We demonstrate the applicability of our model to
explain and predict population dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bunne_C/0/1/0/all/0/1"&gt;Charlotte Bunne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Papaxanthos_L/0/1/0/all/0/1"&gt;Laetitia Meng-Papaxanthos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1"&gt;Marco Cuturi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signal Processing on the Permutahedron: Tight Spectral Frames for Ranked Data Analysis. (arXiv:2103.04150v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04150</id>
        <link href="http://arxiv.org/abs/2103.04150"/>
        <updated>2021-07-16T00:48:25.014Z</updated>
        <summary type="html"><![CDATA[Ranked data sets, where m judges/voters specify a preference ranking of n
objects/candidates, are increasingly prevalent in contexts such as political
elections, computer vision, recommender systems, and bioinformatics. The vote
counts for each ranking can be viewed as an n! data vector lying on the
permutahedron, which is a Cayley graph of the symmetric group with vertices
labeled by permutations and an edge when two permutations differ by an adjacent
transposition. Leveraging combinatorial representation theory and recent
progress in signal processing on graphs, we investigate a novel, scalable
transform method to interpret and exploit structure in ranked data. We
represent data on the permutahedron using an overcomplete dictionary of atoms,
each of which captures both smoothness information about the data (typically
the focus of spectral graph decomposition methods in graph signal processing)
and structural information about the data (typically the focus of symmetry
decomposition methods from representation theory). These atoms have a more
naturally interpretable structure than any known basis for signals on the
permutahedron, and they form a Parseval frame, ensuring beneficial numerical
properties such as energy preservation. We develop specialized algorithms and
open software that take advantage of the symmetry and structure of the
permutahedron to improve the scalability of the proposed method, making it more
applicable to the high-dimensional ranked data found in applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yilin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+DeJong_J/0/1/0/all/0/1"&gt;Jennifer DeJong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Halverson_T/0/1/0/all/0/1"&gt;Tom Halverson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shuman_D/0/1/0/all/0/1"&gt;David I Shuman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A unified framework for non-negative matrix and tensor factorisations with a smoothed Wasserstein loss. (arXiv:2104.01708v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01708</id>
        <link href="http://arxiv.org/abs/2104.01708"/>
        <updated>2021-07-16T00:48:24.998Z</updated>
        <summary type="html"><![CDATA[Non-negative matrix and tensor factorisations are a classical tool for
finding low-dimensional representations of high-dimensional datasets. In
applications such as imaging, datasets can be regarded as distributions
supported on a space with metric structure. In such a setting, a loss function
based on the Wasserstein distance of optimal transportation theory is a natural
choice since it incorporates the underlying geometry of the data. We introduce
a general mathematical framework for computing non-negative factorisations of
both matrices and tensors with respect to an optimal transport loss. We derive
an efficient computational method for its solution using a convex dual
formulation, and demonstrate the applicability of this approach with several
numerical illustrations with both matrix and tensor-valued data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Stephen Y. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data vs classifiers, who wins?. (arXiv:2107.07451v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07451</id>
        <link href="http://arxiv.org/abs/2107.07451"/>
        <updated>2021-07-16T00:48:24.981Z</updated>
        <summary type="html"><![CDATA[The classification experiments covered by machine learning (ML) are composed
by two important parts: the data and the algorithm. As they are a fundamental
part of the problem, both must be considered when evaluating a model's
performance against a benchmark. The best classifiers need robust benchmarks to
be properly evaluated. For this, gold standard benchmarks such as OpenML-CC18
are used. However, data complexity is commonly not considered along with the
model during a performance evaluation. Recent studies employ Item Response
Theory (IRT) as a new approach to evaluating datasets and algorithms, capable
of evaluating both simultaneously. This work presents a new evaluation
methodology based on IRT and Glicko-2, jointly with the decodIRT tool developed
to guide the estimation of IRT in ML. It explores the IRT as a tool to evaluate
the OpenML-CC18 benchmark for its algorithmic evaluation capability and checks
if there is a subset of datasets more efficient than the original benchmark.
Several classifiers, from classics to ensemble, are also evaluated using the
IRT models. The Glicko-2 rating system was applied together with IRT to
summarize the innate ability and classifiers performance. It was noted that not
all OpenML-CC18 datasets are really useful for evaluating algorithms, where
only 10% were rated as being really difficult. Furthermore, it was verified the
existence of a more efficient subset containing only 50% of the original size.
While Randon Forest was singled out as the algorithm with the best innate
ability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cardoso_L/0/1/0/all/0/1"&gt;Lucas F. F. Cardoso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_V/0/1/0/all/0/1"&gt;Vitor C. A. Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frances_R/0/1/0/all/0/1"&gt;Regiane S. Kawasaki Franc&amp;#xea;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prudencio_R/0/1/0/all/0/1"&gt;Ricardo B. C. Prud&amp;#xea;ncio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alves_R/0/1/0/all/0/1"&gt;Ronnie C. O. Alves&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[xCos: An Explainable Cosine Metric for Face Verification Task. (arXiv:2003.05383v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.05383</id>
        <link href="http://arxiv.org/abs/2003.05383"/>
        <updated>2021-07-16T00:48:24.971Z</updated>
        <summary type="html"><![CDATA[We study the XAI (explainable AI) on the face recognition task, particularly
the face verification here. Face verification is a crucial task in recent days
and it has been deployed to plenty of applications, such as access control,
surveillance, and automatic personal log-on for mobile devices. With the
increasing amount of data, deep convolutional neural networks can achieve very
high accuracy for the face verification task. Beyond exceptional performances,
deep face verification models need more interpretability so that we can trust
the results they generate. In this paper, we propose a novel similarity metric,
called explainable cosine ($xCos$), that comes with a learnable module that can
be plugged into most of the verification models to provide meaningful
explanations. With the help of $xCos$, we can see which parts of the two input
faces are similar, where the model pays its attention to, and how the local
similarities are weighted to form the output $xCos$ score. We demonstrate the
effectiveness of our proposed method on LFW and various competitive benchmarks,
resulting in not only providing novel and desiring model interpretability for
face verification but also ensuring the accuracy as plugging into existing face
recognition models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yu-Sheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe-Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-An Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Siang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Ya-Liang Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[USCO-Solver: Solving Undetermined Stochastic Combinatorial Optimization Problems. (arXiv:2107.07508v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07508</id>
        <link href="http://arxiv.org/abs/2107.07508"/>
        <updated>2021-07-16T00:48:24.957Z</updated>
        <summary type="html"><![CDATA[Real-world decision-making systems are often subject to uncertainties that
have to be resolved through observational data. Therefore, we are frequently
confronted with combinatorial optimization problems of which the objective
function is unknown and thus has to be debunked using empirical evidence. In
contrast to the common practice that relies on a learning-and-optimization
strategy, we consider the regression between combinatorial spaces, aiming to
infer high-quality optimization solutions from samples of input-solution pairs
-- without the need to learn the objective function. Our main deliverable is a
universal solver that is able to handle abstract undetermined stochastic
combinatorial optimization problems. For learning foundations, we present
learning-error analysis under the PAC-Bayesian framework using a new
margin-based analysis. In empirical studies, we demonstrate our design using
proof-of-concept experiments, and compare it with other methods that are
potentially applicable. Overall, we obtain highly encouraging experimental
results for several classic combinatorial problems on both synthetic and
real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tong_G/0/1/0/all/0/1"&gt;Guangmo Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLSRIL-23: Cross Lingual Speech Representations for Indic Languages. (arXiv:2107.07402v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07402</id>
        <link href="http://arxiv.org/abs/2107.07402"/>
        <updated>2021-07-16T00:48:24.904Z</updated>
        <summary type="html"><![CDATA[We present a CLSRIL-23, a self supervised learning based audio pre-trained
model which learns cross lingual speech representations from raw audio across
23 Indic languages. It is built on top of wav2vec 2.0 which is solved by
training a contrastive task over masked latent speech representations and
jointly learns the quantization of latents shared across all languages. We
compare the language wise loss during pretraining to compare effects of
monolingual and multilingual pretraining. Performance on some downstream
fine-tuning tasks for speech recognition is also compared and our experiments
show that multilingual pretraining outperforms monolingual training, in terms
of learning speech representations which encodes phonetic similarity of
languages and also in terms of performance on down stream tasks. A decrease of
5% is observed in WER and 9.5% in CER when a multilingual pretrained model is
used for finetuning in Hindi. All the code models are also open sourced.
CLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio
data to facilitate research in speech recognition for Indic languages. We hope
that new state of the art systems will be created using the self supervised
approach, especially for low resources Indic languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Anirudh Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1"&gt;Harveen Singh Chadha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1"&gt;Priyanshi Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chimmwal_N/0/1/0/all/0/1"&gt;Neeraj Chimmwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1"&gt;Ankur Dhuriya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1"&gt;Rishabh Gaur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1"&gt;Vivek Raghavan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical graph neural nets can capture long-range interactions. (arXiv:2107.07432v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07432</id>
        <link href="http://arxiv.org/abs/2107.07432"/>
        <updated>2021-07-16T00:48:24.898Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) based on message passing between neighboring
nodes are known to be insufficient for capturing long-range interactions in
graphs. In this project we study hierarchical message passing models that
leverage a multi-resolution representation of a given graph. This facilitates
learning of features that span large receptive fields without loss of local
information, an aspect not studied in preceding work on hierarchical GNNs. We
introduce Hierarchical Graph Net (HGNet), which for any two connected nodes
guarantees existence of message-passing paths of at most logarithmic length
w.r.t. the input graph size. Yet, under mild assumptions, its internal
hierarchy maintains asymptotic size equivalent to that of the input graph. We
observe that our HGNet outperforms conventional stacking of GCN layers
particularly in molecular property prediction benchmarks. Finally, we propose
two benchmarking tasks designed to elucidate capability of GNNs to leverage
long-range interactions in graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rampasek_L/0/1/0/all/0/1"&gt;Ladislav Ramp&amp;#xe1;&amp;#x161;ek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1"&gt;Guy Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning-Based Analysis of Free-Text Keystroke Dynamics. (arXiv:2107.07409v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07409</id>
        <link href="http://arxiv.org/abs/2107.07409"/>
        <updated>2021-07-16T00:48:24.892Z</updated>
        <summary type="html"><![CDATA[The development of active and passive biometric authentication and
identification technology plays an increasingly important role in
cybersecurity. Keystroke dynamics can be used to analyze the way that a user
types based on various keyboard input. Previous work has shown that user
authentication and classification can be achieved based on keystroke dynamics.
In this research, we consider the problem of user classification based on
keystroke dynamics features collected from free-text. We implement and analyze
a novel a deep learning model that combines a convolutional neural network
(CNN) and a gated recurrent unit (GRU). We optimize the resulting model and
consider several relevant related problems. Our model is competitive with the
best results obtained in previous comparable research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Han-Chih Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1"&gt;Mark Stamp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SilGAN: Generating driving maneuvers for scenario-based software-in-the-loop testing. (arXiv:2107.07364v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2107.07364</id>
        <link href="http://arxiv.org/abs/2107.07364"/>
        <updated>2021-07-16T00:48:24.885Z</updated>
        <summary type="html"><![CDATA[Automotive software testing continues to rely largely upon expensive field
tests to ensure quality because alternatives like simulation-based testing are
relatively immature. As a step towards lowering reliance on field tests, we
present SilGAN, a deep generative model that eases specification, stimulus
generation, and automation of automotive software-in-the-loop testing. The
model is trained using data recorded from vehicles in the field. Upon training,
the model uses a concise specification for a driving scenario to generate
realistic vehicle state transitions that can occur during such a scenario. Such
authentic emulation of internal vehicle behavior can be used for rapid,
systematic and inexpensive testing of vehicle control software. In addition, by
presenting a targeted method for searching through the information learned by
the model, we show how a test objective like code coverage can be automated.
The data driven end-to-end testing pipeline that we present vastly expands the
scope and credibility of automotive simulation-based testing. This reduces time
to market while helping maintain required standards of quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parthasarathy_D/0/1/0/all/0/1"&gt;Dhasarathy Parthasarathy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johansson_A/0/1/0/all/0/1"&gt;Anton Johansson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expert Graphs: Synthesizing New Expertise via Collaboration. (arXiv:2107.07054v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07054</id>
        <link href="http://arxiv.org/abs/2107.07054"/>
        <updated>2021-07-16T00:48:24.879Z</updated>
        <summary type="html"><![CDATA[Consider multiple experts with overlapping expertise working on a
classification problem under uncertain input. What constitutes a consistent set
of opinions? How can we predict the opinions of experts on missing sub-domains?
In this paper, we define a framework of to analyze this problem, termed "expert
graphs." In an expert graph, vertices represent classes and edges represent
binary opinions on the topics of their vertices. We derive necessary conditions
for expert graph validity and use them to create "synthetic experts" which
describe opinions consistent with the observed opinions of other experts. We
show this framework to be equivalent to the well-studied linear ordering
polytope. We show our conditions are not sufficient for describing all expert
graphs on cliques, but are sufficient for cycles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazaheri_B/0/1/0/all/0/1"&gt;Bijan Mazaheri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Siddharth Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruck_J/0/1/0/all/0/1"&gt;Jehoshua Bruck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mid-flight Forecasting for CPA Lines in Online Advertising. (arXiv:2107.07494v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07494</id>
        <link href="http://arxiv.org/abs/2107.07494"/>
        <updated>2021-07-16T00:48:24.872Z</updated>
        <summary type="html"><![CDATA[For Verizon MediaDemand Side Platform(DSP), forecasting of ad campaign
performance not only feeds key information to the optimization server to allow
the system to operate on a high-performance mode, but also produces actionable
insights to the advertisers. In this paper, the forecasting problem for CPA
lines in the middle of the flight is investigated by taking the bidding
mechanism into account. The proposed methodology generates relationships
between various key performance metrics and optimization signals. It can also
be used to estimate the sensitivity of ad campaign performance metrics to the
adjustments of optimization signal, which is important to the design of a
campaign management system. The relationship between advertiser spends and
effective Cost Per Action(eCPA) is also characterized, which serves as a
guidance for mid-flight line adjustment to the advertisers. Several practical
issues in implementation, such as downsampling of the dataset, are also
discussed in the paper. At last, the forecasting results are validated against
actual deliveries and demonstrates promising accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+He_H/0/1/0/all/0/1"&gt;Hao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tian Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ren_L/0/1/0/all/0/1"&gt;Lihua Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karlsson_N/0/1/0/all/0/1"&gt;Niklas Karlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Flores_A/0/1/0/all/0/1"&gt;Aaron Flores&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Approximations for Thompson Sampling. (arXiv:2105.09232v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09232</id>
        <link href="http://arxiv.org/abs/2105.09232"/>
        <updated>2021-07-16T00:48:24.856Z</updated>
        <summary type="html"><![CDATA[We study the behavior of Thompson sampling from the perspective of weak
convergence. In the regime where the gaps between arm means scale as
$1/\sqrt{n}$ with the time horizon $n$, we show that the dynamics of Thompson
sampling evolve according to discrete versions of SDEs and random ODEs. As $n
\to \infty$, we show that the dynamics converge weakly to solutions of the
corresponding SDEs and random ODEs. (Recently, Wager and Xu (arXiv:2101.09855)
independently proposed this regime and developed similar SDE and random ODE
approximations for Thompson sampling in the multi-armed bandit setting.) Our
weak convergence theory, which covers both multi-armed and linear bandit
settings, is developed from first principles using the Continuous Mapping
Theorem and can be directly adapted to analyze other sampling-based bandit
algorithms, for example, algorithms using the bootstrap for exploration. We
also establish an invariance principle for multi-armed bandits with gaps
scaling as $1/\sqrt{n}$ -- for Thompson sampling and related algorithms
involving posterior approximation or the bootstrap, the weak diffusion limits
are in general the same regardless of the specifics of the reward distributions
or the choice of prior. In particular, as suggested by the classical
Bernstein-von Mises normal approximation for posterior distributions, the weak
diffusion limits generally coincide with the limit for normally-distributed
rewards and priors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lin Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glynn_P/0/1/0/all/0/1"&gt;Peter W. Glynn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Innovations Autoencoder and its Application in One-class Anomalous Sequence Detection. (arXiv:2106.12382v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12382</id>
        <link href="http://arxiv.org/abs/2106.12382"/>
        <updated>2021-07-16T00:48:24.848Z</updated>
        <summary type="html"><![CDATA[An innovations sequence of a time series is a sequence of independent and
identically distributed random variables with which the original time series
has a causal representation. The innovation at a time is statistically
independent of the history of the time series. As such, it represents the new
information contained at present but not in the past. Because of its simple
probability structure, an innovations sequence is the most efficient signature
of the original. Unlike the principle or independent component analysis
representations, an innovations sequence preserves not only the complete
statistical properties but also the temporal order of the original time series.
An long-standing open problem is to find a computationally tractable way to
extract an innovations sequence of non-Gaussian processes. This paper presents
a deep learning approach, referred to as Innovations Autoencoder (IAE), that
extracts innovations sequences using a causal convolutional neural network. An
application of IAE to the one-class anomalous sequence detection problem with
unknown anomaly and anomaly-free models is also presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tong_L/0/1/0/all/0/1"&gt;Lang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Significant Features for Few-Shot Learning using Dimensionality Reduction. (arXiv:2107.06992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06992</id>
        <link href="http://arxiv.org/abs/2107.06992"/>
        <updated>2021-07-16T00:48:24.841Z</updated>
        <summary type="html"><![CDATA[Few-shot learning is a relatively new technique that specializes in problems
where we have little amounts of data. The goal of these methods is to classify
categories that have not been seen before with just a handful of samples.
Recent approaches, such as metric learning, adopt the meta-learning strategy in
which we have episodic tasks conformed by support (training) data and query
(test) data. Metric learning methods have demonstrated that simple models can
achieve good performance by learning a similarity function to compare the
support and the query data. However, the feature space learned by a given
metric learning approach may not exploit the information given by a specific
few-shot task. In this work, we explore the use of dimension reduction
techniques as a way to find task-significant features helping to make better
predictions. We measure the performance of the reduced features by assigning a
score based on the intra-class and inter-class distance, and selecting a
feature reduction method in which instances of different classes are far away
and instances of the same class are close. This module helps to improve the
accuracy performance by allowing the similarity function, given by the metric
learning method, to have more discriminative features for the classification.
Our method outperforms the metric learning baselines in the miniImageNet
dataset by around 2% in accuracy performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_Ruiz_M/0/1/0/all/0/1"&gt;Mauricio Mendez-Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Zapata_I/0/1/0/all/0/1"&gt;Ivan Garcia Jorge Gonzalez-Zapata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1"&gt;Gilberto Ochoa-Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_A/0/1/0/all/0/1"&gt;Andres Mendez-Vazquez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-label Chaining with Imprecise Probabilities. (arXiv:2107.07443v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07443</id>
        <link href="http://arxiv.org/abs/2107.07443"/>
        <updated>2021-07-16T00:48:24.835Z</updated>
        <summary type="html"><![CDATA[We present two different strategies to extend the classical multi-label
chaining approach to handle imprecise probability estimates. These estimates
use convex sets of distributions (or credal sets) in order to describe our
uncertainty rather than a precise one. The main reasons one could have for
using such estimations are (1) to make cautious predictions (or no decision at
all) when a high uncertainty is detected in the chaining and (2) to make better
precise predictions by avoiding biases caused in early decisions in the
chaining. Through the use of the naive credal classifier, we propose efficient
procedures with theoretical justifications to solve both strategies. Our
experimental results on missing labels, which investigate how reliable these
predictions are in both approaches, indicate that our approaches produce
relevant cautiousness on those hard-to-predict instances where the precise
models fail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Alarcon_Y/0/1/0/all/0/1"&gt;Yonatan Carlos Carranza Alarc&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Destercke_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Destercke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Shift Detection: Localizing Which Features Have Shifted via Conditional Distribution Tests. (arXiv:2107.06929v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06929</id>
        <link href="http://arxiv.org/abs/2107.06929"/>
        <updated>2021-07-16T00:48:24.821Z</updated>
        <summary type="html"><![CDATA[While previous distribution shift detection approaches can identify if a
shift has occurred, these approaches cannot localize which specific features
have caused a distribution shift -- a critical step in diagnosing or fixing any
underlying issue. For example, in military sensor networks, users will want to
detect when one or more of the sensors has been compromised, and critically,
they will want to know which specific sensors might be compromised. Thus, we
first define a formalization of this problem as multiple conditional
distribution hypothesis tests and propose both non-parametric and parametric
statistical tests. For both efficiency and flexibility, we then propose to use
a test statistic based on the density model score function (i.e. gradient with
respect to the input) -- which can easily compute test statistics for all
dimensions in a single forward and backward pass. Any density model could be
used for computing the necessary statistics including deep density models such
as normalizing flows or autoregressive models. We additionally develop methods
for identifying when and where a shift occurs in multivariate time-series data
and show results for multiple scenarios using realistic attack models on both
simulated and real world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulinski_S/0/1/0/all/0/1"&gt;Sean Kulinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1"&gt;Saurabh Bagchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inouye_D/0/1/0/all/0/1"&gt;David I. Inouye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Neural Bandit: Provable Algorithm for Visual-aware Advertising. (arXiv:2107.07438v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07438</id>
        <link href="http://arxiv.org/abs/2107.07438"/>
        <updated>2021-07-16T00:48:24.807Z</updated>
        <summary type="html"><![CDATA[Online advertising is ubiquitous in web business. Image displaying is
considered as one of the most commonly used formats to interact with customers.
Contextual multi-armed bandit has shown success in the application of
advertising to solve the exploration-exploitation dilemma existed in the
recommendation procedure. Inspired by the visual-aware advertising, in this
paper, we propose a contextual bandit algorithm, where the convolutional neural
network (CNN) is utilized to learn the reward function along with an upper
confidence bound (UCB) for exploration. We also prove a near-optimal regret
bound $\tilde{\mathcal{O}}(\sqrt{T})$ when the network is over-parameterized
and establish strong connections with convolutional neural tangent kernel
(CNTK). Finally, we evaluate the empirical performance of the proposed
algorithm and show that it outperforms other state-of-the-art UCB-based bandit
algorithms on real-world image data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1"&gt;Yikun Ban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jingrui He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic Concept-based Explainable Reasoning. (arXiv:2107.07493v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07493</id>
        <link href="http://arxiv.org/abs/2107.07493"/>
        <updated>2021-07-16T00:48:24.799Z</updated>
        <summary type="html"><![CDATA[Recent research on graph neural network (GNN) models successfully applied
GNNs to classical graph algorithms and combinatorial optimisation problems.
This has numerous benefits, such as allowing applications of algorithms when
preconditions are not satisfied, or reusing learned models when sufficient
training data is not available or can't be generated. Unfortunately, a key
hindrance of these approaches is their lack of explainability, since GNNs are
black-box models that cannot be interpreted directly. In this work, we address
this limitation by applying existing work on concept-based explanations to GNN
models. We introduce concept-bottleneck GNNs, which rely on a modification to
the GNN readout mechanism. Using three case studies we demonstrate that: (i)
our proposed model is capable of accurately learning concepts and extracting
propositional formulas based on the learned concepts for each target class;
(ii) our concept-based GNN models achieve comparative performance with
state-of-the-art models; (iii) we can derive global graph concepts, without
explicitly providing any supervision on graph-level concepts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1"&gt;Dobrik Georgiev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1"&gt;Pietro Barbiero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1"&gt;Dmitry Kazhdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1"&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf2;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auditing for Diversity using Representative Examples. (arXiv:2107.07393v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.07393</id>
        <link href="http://arxiv.org/abs/2107.07393"/>
        <updated>2021-07-16T00:48:24.792Z</updated>
        <summary type="html"><![CDATA[Assessing the diversity of a dataset of information associated with people is
crucial before using such data for downstream applications. For a given
dataset, this often involves computing the imbalance or disparity in the
empirical marginal distribution of a protected attribute (e.g. gender, dialect,
etc.). However, real-world datasets, such as images from Google Search or
collections of Twitter posts, often do not have protected attributes labeled.
Consequently, to derive disparity measures for such datasets, the elements need
to hand-labeled or crowd-annotated, which are expensive processes.

We propose a cost-effective approach to approximate the disparity of a given
unlabeled dataset, with respect to a protected attribute, using a control set
of labeled representative examples. Our proposed algorithm uses the pairwise
similarity between elements in the dataset and elements in the control set to
effectively bootstrap an approximation to the disparity of the dataset.
Importantly, we show that using a control set whose size is much smaller than
the size of the dataset is sufficient to achieve a small approximation error.
Further, based on our theoretical framework, we also provide an algorithm to
construct adaptive control sets that achieve smaller approximation errors than
randomly chosen control sets. Simulations on two image datasets and one Twitter
dataset demonstrate the efficacy of our approach (using random and adaptive
control sets) in auditing the diversity of a wide variety of datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keswani_V/0/1/0/all/0/1"&gt;Vijay Keswani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celis_L/0/1/0/all/0/1"&gt;L. Elisa Celis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lockout: Sparse Regularization of Neural Networks. (arXiv:2107.07160v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07160</id>
        <link href="http://arxiv.org/abs/2107.07160"/>
        <updated>2021-07-16T00:48:24.785Z</updated>
        <summary type="html"><![CDATA[Many regression and classification procedures fit a parameterized function
$f(x;w)$ of predictor variables $x$ to data $\{x_{i},y_{i}\}_1^N$ based on some
loss criterion $L(y,f)$. Often, regularization is applied to improve accuracy
by placing a constraint $P(w)\leq t$ on the values of the parameters $w$.
Although efficient methods exist for finding solutions to these constrained
optimization problems for all values of $t\geq0$ in the special case when $f$
is a linear function, none are available when $f$ is non-linear (e.g. Neural
Networks). Here we present a fast algorithm that provides all such solutions
for any differentiable function $f$ and loss $L$, and any constraint $P$ that
is an increasing monotone function of the absolute value of each parameter.
Applications involving sparsity inducing regularization of arbitrary Neural
Networks are discussed. Empirical results indicate that these sparse solutions
are usually superior to their dense counterparts in both accuracy and
interpretability. This improvement in accuracy can often make Neural Networks
competitive with, and sometimes superior to, state-of-the-art methods in the
analysis of tabular data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valdes_G/0/1/0/all/0/1"&gt;Gilmer Valdes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arbelo_W/0/1/0/all/0/1"&gt;Wilmer Arbelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Interian_Y/0/1/0/all/0/1"&gt;Yannet Interian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedman_J/0/1/0/all/0/1"&gt;Jerome H. Friedman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoBERT-Zero: Evolving BERT Backbone from Scratch. (arXiv:2107.07445v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07445</id>
        <link href="http://arxiv.org/abs/2107.07445"/>
        <updated>2021-07-16T00:48:24.767Z</updated>
        <summary type="html"><![CDATA[Transformer-based pre-trained language models like BERT and its variants have
recently achieved promising performance in various natural language processing
(NLP) tasks. However, the conventional paradigm constructs the backbone by
purely stacking the manually designed global self-attention layers, introducing
inductive bias and thus leading to sub-optimal. In this work, we propose an
Operation-Priority Neural Architecture Search (OP-NAS) algorithm to
automatically search for promising hybrid backbone architectures. Our
well-designed search space (i) contains primitive math operations in the
intra-layer level to explore novel attention structures, and (ii) leverages
convolution blocks to be the supplementary for attention structure in the
inter-layer level to better learn local dependency. We optimize both the search
algorithm and evaluation of candidate models to boost the efficiency of our
proposed OP-NAS. Specifically, we propose Operation-Priority (OP) evolution
strategy to facilitate model search via balancing exploration and exploitation.
Furthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for
fast model evaluation. Extensive experiments show that the searched
architecture (named AutoBERT-Zero) significantly outperforms BERT and its
variants of different model capacities in various downstream tasks, proving the
architecture's transfer and generalization abilities. Remarkably,
AutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and
BERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE
test set. Code and pre-trained models will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jiahui Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+shi_H/0/1/0/all/0/1"&gt;Han shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip L.H. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable AI: current status and future directions. (arXiv:2107.07045v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07045</id>
        <link href="http://arxiv.org/abs/2107.07045"/>
        <updated>2021-07-16T00:48:24.755Z</updated>
        <summary type="html"><![CDATA[Explainable Artificial Intelligence (XAI) is an emerging area of research in
the field of Artificial Intelligence (AI). XAI can explain how AI obtained a
particular solution (e.g., classification or object detection) and can also
answer other "wh" questions. This explainability is not possible in traditional
AI. Explainability is essential for critical applications, such as defense,
health care, law and order, and autonomous driving vehicles, etc, where the
know-how is required for trust and transparency. A number of XAI techniques so
far have been purposed for such applications. This paper provides an overview
of these techniques from a multimedia (i.e., text, image, audio, and video)
point of view. The advantages and shortcomings of these techniques have been
discussed, and pointers to some future directions have also been provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gohel_P/0/1/0/all/0/1"&gt;Prashant Gohel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Priyanka Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohanty_M/0/1/0/all/0/1"&gt;Manoranjan Mohanty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous-variable neural-network quantum states and the quantum rotor model. (arXiv:2107.07105v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.07105</id>
        <link href="http://arxiv.org/abs/2107.07105"/>
        <updated>2021-07-16T00:48:24.740Z</updated>
        <summary type="html"><![CDATA[We initiate the study of neural-network quantum state algorithms for
analyzing continuous-variable lattice quantum systems in first quantization. A
simple family of continuous-variable trial wavefunctons is introduced which
naturally generalizes the restricted Boltzmann machine (RBM) wavefunction
introduced for analyzing quantum spin systems. By virtue of its simplicity, the
same variational Monte Carlo training algorithms that have been developed for
ground state determination and time evolution of spin systems have natural
analogues in the continuum. We offer a proof of principle demonstration in the
context of ground state determination of a stoquastic quantum rotor
Hamiltonian. Results are compared against those obtained from partial
differential equation (PDE) based scalable eigensolvers. This study serves as a
benchmark against which future investigation of continuous-variable neural
quantum states can be compared, and points to the need to consider deep network
architectures and more sophisticated training algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Stokes_J/0/1/0/all/0/1"&gt;James Stokes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+De_S/0/1/0/all/0/1"&gt;Saibal De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Veerapaneni_S/0/1/0/all/0/1"&gt;Shravan Veerapaneni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Carleo_G/0/1/0/all/0/1"&gt;Giuseppe Carleo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLEX: Unifying Evaluation for Few-Shot NLP. (arXiv:2107.07170v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07170</id>
        <link href="http://arxiv.org/abs/2107.07170"/>
        <updated>2021-07-16T00:48:24.711Z</updated>
        <summary type="html"><![CDATA[Few-shot NLP research is highly active, yet conducted in disjoint research
threads with evaluation suites that lack challenging-yet-realistic testing
setups and fail to employ careful experimental design. Consequently, the
community does not know which techniques perform best or even if they
outperform simple baselines. We formulate desiderata for an ideal few-shot NLP
benchmark and present FLEX, the first benchmark, public leaderboard, and
framework that provides unified, comprehensive measurement for few-shot NLP
techniques. FLEX incorporates and introduces new best practices for few-shot
evaluation, including measurement of four transfer settings, textual labels for
zero-shot evaluation, and a principled approach to benchmark design that
optimizes statistical accuracy while keeping evaluation costs accessible to
researchers without large compute resources. In addition, we present UniFew, a
simple yet strong prompt-based model for few-shot learning which unifies the
pretraining and finetuning prompt formats, eschewing complex machinery of
recent prompt-based approaches in adapting downstream task formats to language
model pretraining objectives. We demonstrate that despite simplicity UniFew
achieves results competitive with both popular meta-learning and prompt-based
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1"&gt;Jonathan Bragg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1"&gt;Arman Cohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1"&gt;Kyle Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1"&gt;Iz Beltagy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What underlies rapid learning and systematic generalization in humans. (arXiv:2107.06994v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06994</id>
        <link href="http://arxiv.org/abs/2107.06994"/>
        <updated>2021-07-16T00:48:24.704Z</updated>
        <summary type="html"><![CDATA[Despite the groundbreaking successes of neural networks, contemporary models
require extensive training with massive datasets and exhibit poor out-of-sample
generalization. One proposed solution is to build systematicity and
domain-specific constraints into the model, echoing the tenets of classical,
symbolic cognitive architectures. In this paper, we consider the limitations of
this approach by examining human adults' ability to learn an abstract reasoning
task from a brief instructional tutorial and explanatory feedback for incorrect
responses, demonstrating that human learning dynamics and ability to generalize
outside the range of the training examples differ drastically from those of a
representative neural network model, and that the model is brittle to changes
in features not anticipated by its authors. We present further evidence from
human data that the ability to consistently solve the puzzles was associated
with education, particularly basic mathematics education, and with the ability
to provide a reliably identifiable, valid description of the strategy used. We
propose that rapid learning and systematic generalization in humans may depend
on a gradual, experience-dependent process of learning-to-learn using
instructions and explanations to guide the construction of explicit abstract
rules that support generalizable inferences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nam_A/0/1/0/all/0/1"&gt;Andrew Joohun Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1"&gt;James L. McClelland&lt;/a&gt; (Stanford University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based Spectrum Sensing and Access in Cognitive Radios via Approximate POMDPs. (arXiv:2107.07049v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.07049</id>
        <link href="http://arxiv.org/abs/2107.07049"/>
        <updated>2021-07-16T00:48:24.684Z</updated>
        <summary type="html"><![CDATA[A novel LEarning-based Spectrum Sensing and Access (LESSA) framework is
proposed, wherein a cognitive radio (CR) learns a time-frequency correlation
model underlying spectrum occupancy of licensed users (LUs) in a radio
ecosystem; concurrently, it devises an approximately optimal spectrum sensing
and access policy under sensing constraints. A Baum-Welch algorithm is proposed
to learn a parametric Markov transition model of LU spectrum occupancy based on
noisy spectrum measurements. Spectrum sensing and access are cast as a
Partially-Observable Markov Decision Process, approximately optimized via
randomized point-based value iteration. Fragmentation, Hamming-distance state
filters and Monte-Carlo methods are proposed to alleviate the inherent
computational complexity, and a weighted reward metric to regulate the
trade-off between CR throughput and LU interference. Numerical evaluations
demonstrate that LESSA performs within 5 percent of a genie-aided upper bound
with foreknowledge of LU spectrum occupancy, and outperforms state-of-the-art
algorithms across the entire trade-off region: 71 percent over
correlation-based clustering, 26 percent over Neyman-Pearson detection, 6
percent over the Viterbi algorithm, and 9 percent over an adaptive Deep
Q-Network. LESSA is then extended to a distributed Multi-Agent setting
(MA-LESSA), by proposing novel neighbor discovery and channel access rank
allocation. MA-LESSA improves CR throughput by 43 percent over cooperative
TD-SARSA, 84 percent over cooperative greedy distributed learning, and 3x over
non-cooperative learning via g-statistics and ACKs. Finally, MA-LESSA is
implemented on the DARPA SC2 platform, manifesting superior performance over
competitors in a real-world TDWR-UNII WLAN emulation; its implementation
feasibility is further validated on a testbed of ESP32 radios, exhibiting 96
percent success probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Keshavamurthy_B/0/1/0/all/0/1"&gt;Bharath Keshavamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Michelusi_N/0/1/0/all/0/1"&gt;Nicolo Michelusi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hida-Mat\'ern Kernel. (arXiv:2107.07098v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07098</id>
        <link href="http://arxiv.org/abs/2107.07098"/>
        <updated>2021-07-16T00:48:24.677Z</updated>
        <summary type="html"><![CDATA[We present the class of Hida-Mat\'ern kernels, which is the canonical family
of covariance functions over the entire space of stationary Gauss-Markov
Processes. It extends upon Mat\'ern kernels, by allowing for flexible
construction of priors over processes with oscillatory components. Any
stationary kernel, including the widely used squared-exponential and spectral
mixture kernels, are either directly within this class or are appropriate
asymptotic limits, demonstrating the generality of this class. Taking advantage
of its Markovian nature we show how to represent such processes as state space
models using only the kernel and its derivatives. In turn this allows us to
perform Gaussian Process inference more efficiently and side step the usual
computational burdens. We also show how exploiting special properties of the
state space representation enables improved numerical stability in addition to
further reductions of computational complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dowling_M/0/1/0/all/0/1"&gt;Matthew Dowling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sokol_P/0/1/0/all/0/1"&gt;Piotr Sok&amp;#xf3;&amp;#x142;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Park_I/0/1/0/all/0/1"&gt;Il Memming Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating Memorization in Sample Selection for Learning with Noisy Labels. (arXiv:2107.07041v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07041</id>
        <link href="http://arxiv.org/abs/2107.07041"/>
        <updated>2021-07-16T00:48:24.619Z</updated>
        <summary type="html"><![CDATA[Because deep learning is vulnerable to noisy labels, sample selection
techniques, which train networks with only clean labeled data, have attracted a
great attention. However, if the labels are dominantly corrupted by few
classes, these noisy samples are called dominant-noisy-labeled samples, the
network also learns dominant-noisy-labeled samples rapidly via content-aware
optimization. In this study, we propose a compelling criteria to penalize
dominant-noisy-labeled samples intensively through class-wise penalty labels.
By averaging prediction confidences for the each observed label, we obtain
suitable penalty labels that have high values if the labels are largely
corrupted by some classes. Experiments were performed using benchmarks
(CIFAR-10, CIFAR-100, Tiny-ImageNet) and real-world datasets (ANIMAL-10N,
Clothing1M) to evaluate the proposed criteria in various scenarios with
different noise rates. Using the proposed sample selection, the learning
process of the network becomes significantly robust to noisy labels compared to
existing methods in several noise types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1"&gt;Kyeongbo Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junggi Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwak_Y/0/1/0/all/0/1"&gt;Youngchul Kwak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1"&gt;Young-Rae Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seong-Eun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1"&gt;Woo-Jin Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVCell: Standard Cell Layout in Advanced Technology Nodes with Reinforcement Learning. (arXiv:2107.07044v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07044</id>
        <link href="http://arxiv.org/abs/2107.07044"/>
        <updated>2021-07-16T00:48:24.611Z</updated>
        <summary type="html"><![CDATA[High quality standard cell layout automation in advanced technology nodes is
still challenging in the industry today because of complex design rules. In
this paper we introduce an automatic standard cell layout generator called
NVCell that can generate layouts with equal or smaller area for over 90% of
single row cells in an industry standard cell library on an advanced technology
node. NVCell leverages reinforcement learning (RL) to fix design rule
violations during routing and to generate efficient placements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Haoxing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fojtik_M/0/1/0/all/0/1"&gt;Matthew Fojtik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1"&gt;Brucek Khailany&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropic Inequality Constraints from $e$-separation Relations in Directed Acyclic Graphs with Hidden Variables. (arXiv:2107.07087v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07087</id>
        <link href="http://arxiv.org/abs/2107.07087"/>
        <updated>2021-07-16T00:48:24.603Z</updated>
        <summary type="html"><![CDATA[Directed acyclic graphs (DAGs) with hidden variables are often used to
characterize causal relations between variables in a system. When some
variables are unobserved, DAGs imply a notoriously complicated set of
constraints on the distribution of observed variables. In this work, we present
entropic inequality constraints that are implied by $e$-separation relations in
hidden variable DAGs with discrete observed variables. The constraints can
intuitively be understood to follow from the fact that the capacity of
variables along a causal pathway to convey information is restricted by their
entropy; e.g. at the extreme case, a variable with entropy $0$ can convey no
information. We show how these constraints can be used to learn about the true
causal model from an observed data distribution. In addition, we propose a
measure of causal influence called the minimal mediary entropy, and demonstrate
that it can augment traditional measures such as the average causal effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Finkelstein_N/0/1/0/all/0/1"&gt;Noam Finkelstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zjawin_B/0/1/0/all/0/1"&gt;Beata Zjawin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wolfe_E/0/1/0/all/0/1"&gt;Elie Wolfe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shpitser_I/0/1/0/all/0/1"&gt;Ilya Shpitser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Spekkens_R/0/1/0/all/0/1"&gt;Robert W. Spekkens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07058</id>
        <link href="http://arxiv.org/abs/2107.07058"/>
        <updated>2021-07-16T00:48:24.585Z</updated>
        <summary type="html"><![CDATA[Image smoothing is a fundamental procedure in applications of both computer
vision and graphics. The required smoothing properties can be different or even
contradictive among different tasks. Nevertheless, the inherent smoothing
nature of one smoothing operator is usually fixed and thus cannot meet the
various requirements of different applications. In this paper, we first
introduce the truncated Huber penalty function which shows strong flexibility
under different parameter settings. A generalized framework is then proposed
with the introduced truncated Huber penalty function. When combined with its
strong flexibility, our framework is able to achieve diverse smoothing natures
where contradictive smoothing behaviors can even be achieved. It can also yield
the smoothing behavior that can seldom be achieved by previous methods, and
superior performance is thus achieved in challenging cases. These together
enable our framework capable of a range of applications and able to outperform
the state-of-the-art approaches in several tasks, such as image detail
enhancement, clip-art compression artifacts removal, guided depth map
restoration, image texture removal, etc. In addition, an efficient numerical
solution is provided and its convergence is theoretically guaranteed even the
optimization framework is non-convex and non-smooth. A simple yet effective
approach is further proposed to reduce the computational cost of our method
while maintaining its performance. The effectiveness and superior performance
of our approach are validated through comprehensive experiments in a range of
applications. Our code is available at
https://github.com/wliusjtu/Generalized-Smoothing-Framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Michael Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuSaver: Neural Adaptive Power Consumption Optimization for Mobile Video Streaming. (arXiv:2107.07127v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07127</id>
        <link href="http://arxiv.org/abs/2107.07127"/>
        <updated>2021-07-16T00:48:24.579Z</updated>
        <summary type="html"><![CDATA[Video streaming services strive to support high-quality videos at higher
resolutions and frame rates to improve the quality of experience (QoE).
However, high-quality videos consume considerable amounts of energy on mobile
devices. This paper proposes NeuSaver, which reduces the power consumption of
mobile devices when streaming videos by applying an adaptive frame rate to each
video chunk without compromising user experience. NeuSaver generates an optimal
policy that determines the appropriate frame rate for each video chunk using
reinforcement learning (RL). The RL model automatically learns the policy that
maximizes the QoE goals based on previous observations. NeuSaver also uses an
asynchronous advantage actor-critic algorithm to reinforce the RL model quickly
and robustly. Streaming servers that support NeuSaver preprocesses videos into
segments with various frame rates, which is similar to the process of creating
videos with multiple bit rates in dynamic adaptive streaming over HTTP.
NeuSaver utilizes the commonly used H.264 video codec. We evaluated NeuSaver in
various experiments and a user study through four video categories along with
the state-of-the-art model. Our experiments showed that NeuSaver effectively
reduces the power consumption of mobile devices when streaming video by an
average of 16.14% and up to 23.12% while achieving high QoE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyoungjun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Myungchul Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1"&gt;Laihyuk Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-based Machine Learning for Fast SAT Solvers and Logic Synthesis. (arXiv:2107.07116v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.07116</id>
        <link href="http://arxiv.org/abs/2107.07116"/>
        <updated>2021-07-16T00:48:24.572Z</updated>
        <summary type="html"><![CDATA[CNF-based SAT and MaxSAT solvers are central to logic synthesis and
verification systems. The increasing popularity of these constraint problems in
electronic design automation encourages studies on different SAT problems and
their properties for further computational efficiency. There has been both
theoretical and practical success of modern Conflict-driven clause learning SAT
solvers, which allows solving very large industrial instances in a relatively
short amount of time. Recently, machine learning approaches provide a new
dimension to solving this challenging problem. Neural symbolic models could
serve as generic solvers that can be specialized for specific domains based on
data without any changes to the structure of the model. In this work, we
propose a one-shot model derived from the Transformer architecture to solve the
MaxSAT problem, which is the optimization version of SAT where the goal is to
satisfy the maximum number of clauses. Our model has a scale-free structure
which could process varying size of instances. We use meta-path and
self-attention mechanism to capture interactions among homogeneous nodes. We
adopt cross-attention mechanisms on the bipartite graph to capture interactions
among heterogeneous nodes. We further apply an iterative algorithm to our model
to satisfy additional clauses, enabling a solution approaching that of an
exact-SAT problem. The attention mechanisms leverage the parallelism for
speedup. Our evaluation indicates improved speedup compared to heuristic
approaches and improved completion rate compared to machine learning
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1"&gt;Feng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chonghan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bashar_M/0/1/0/all/0/1"&gt;Mohammad Khairul Bashar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_N/0/1/0/all/0/1"&gt;Nikhil Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1"&gt;Vijaykrishnan Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Bayesian Neural Networks with Functional Probabilistic Layers. (arXiv:2107.07014v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07014</id>
        <link href="http://arxiv.org/abs/2107.07014"/>
        <updated>2021-07-16T00:48:24.565Z</updated>
        <summary type="html"><![CDATA[Bayesian neural networks provide a direct and natural way to extend standard
deep neural networks to support probabilistic deep learning through the use of
probabilistic layers that, traditionally, encode weight (and bias) uncertainty.
In particular, hybrid Bayesian neural networks utilize standard deterministic
layers together with few probabilistic layers judicially positioned in the
networks for uncertainty estimation. A major aspect and benefit of Bayesian
inference is that priors, in principle, provide the means to encode prior
knowledge for use in inference and prediction. However, it is difficult to
specify priors on weights since the weights have no intuitive interpretation.
Further, the relationships of priors on weights to the functions computed by
networks are difficult to characterize. In contrast, functions are intuitive to
interpret and are direct since they map inputs to outputs. Therefore, it is
natural to specify priors on functions to encode prior knowledge, and to use
them in inference and prediction based on functions. To support this, we
propose hybrid Bayesian neural networks with functional probabilistic layers
that encode function (and activation) uncertainty. We discuss their foundations
in functional Bayesian inference, functional variational inference, sparse
Gaussian processes, and sparse variational Gaussian processes. We further
perform few proof-of-concept experiments using GPflus, a new library that
provides Gaussian process layers and supports their use with deterministic
Keras layers to form hybrid neural network and Gaussian process models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Daniel T. Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Short-term Hourly Streamflow Prediction with Graph Convolutional GRU Networks. (arXiv:2107.07039v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07039</id>
        <link href="http://arxiv.org/abs/2107.07039"/>
        <updated>2021-07-16T00:48:24.552Z</updated>
        <summary type="html"><![CDATA[The frequency and impact of floods are expected to increase due to climate
change. It is crucial to predict streamflow, consequently flooding, in order to
prepare and mitigate its consequences in terms of property damage and
fatalities. This paper presents a Graph Convolutional GRUs based model to
predict the next 36 hours of streamflow for a sensor location using the
upstream river network. As shown in experiment results, the model presented in
this study provides better performance than the persistence baseline and a
Convolutional Bidirectional GRU network for the selected study area in
short-term streamflow prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1"&gt;Muhammed Sit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demiray_B/0/1/0/all/0/1"&gt;Bekir Demiray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1"&gt;Ibrahim Demir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Overview and Experimental Study of Learning-based Optimization Algorithms for Vehicle Routing Problem. (arXiv:2107.07076v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07076</id>
        <link href="http://arxiv.org/abs/2107.07076"/>
        <updated>2021-07-16T00:48:24.544Z</updated>
        <summary type="html"><![CDATA[Vehicle routing problem (VRP) is a typical discrete combinatorial
optimization problem, and many models and algorithms have been proposed to
solve VRP and variants. Although existing approaches has contributed a lot to
the development of this field, these approaches either are limited in problem
size or need manual intervening in choosing parameters. To tackle these
difficulties, many studies consider learning-based optimization algorithms to
solve VRP. This paper reviews recent advances in this field and divides
relevant approaches into end-to-end approaches and step-by-step approaches. We
design three part experiments to justly evaluate performance of four
representative learning-based optimization algorithms and conclude that
combining heuristic search can effectively improve learning ability and sampled
efficiency of LBO models. Finally we point out that research trend of LBO
algorithms is to solve large-scale and multiple constraints problems from real
world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bingjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Guohua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yongming He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1"&gt;Mingfeng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1"&gt;Witold Pedrycz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principal component analysis for Gaussian process posteriors. (arXiv:2107.07115v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07115</id>
        <link href="http://arxiv.org/abs/2107.07115"/>
        <updated>2021-07-16T00:48:24.525Z</updated>
        <summary type="html"><![CDATA[This paper proposes an extension of principal component analysis for Gaussian
process posteriors denoted by GP-PCA. Since GP-PCA estimates a low-dimensional
space of GP posteriors, it can be used for meta-learning, which is a framework
for improving the precision of a new task by estimating a structure of a set of
tasks. The issue is how to define a structure of a set of GPs with an
infinite-dimensional parameter, such as coordinate system and a divergence. In
this study, we reduce the infiniteness of GP to the finite-dimensional case
under the information geometrical framework by considering a space of GP
posteriors that has the same prior. In addition, we propose an approximation
method of GP-PCA based on variational inference and demonstrate the
effectiveness of GP-PCA as meta-learning through experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ishibashi_H/0/1/0/all/0/1"&gt;Hideaki Ishibashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Akaho_S/0/1/0/all/0/1"&gt;Shotaro Akaho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeightScale: Interpreting Weight Change in Neural Networks. (arXiv:2107.07005v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07005</id>
        <link href="http://arxiv.org/abs/2107.07005"/>
        <updated>2021-07-16T00:48:24.513Z</updated>
        <summary type="html"><![CDATA[Interpreting the learning dynamics of neural networks can provide useful
insights into how networks learn and the development of better training and
design approaches. We present an approach to interpret learning in neural
networks by measuring relative weight change on a per layer basis and
dynamically aggregating emerging trends through combination of dimensionality
reduction and clustering which allows us to scale to very deep networks. We use
this approach to investigate learning in the context of vision tasks across a
variety of state-of-the-art networks and provide insights into the learning
behavior of these networks, including how task complexity affects layer-wise
learning in deeper layers of networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1"&gt;Ayush Manish Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tendle_A/0/1/0/all/0/1"&gt;Atharva Tendle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sikka_H/0/1/0/all/0/1"&gt;Harshvardhan Sikka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sahib Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying Component Function in Product Assemblies with Graph Neural Networks. (arXiv:2107.07042v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07042</id>
        <link href="http://arxiv.org/abs/2107.07042"/>
        <updated>2021-07-16T00:48:24.472Z</updated>
        <summary type="html"><![CDATA[Function is defined as the ensemble of tasks that enable the product to
complete the designed purpose. Functional tools, such as functional modeling,
offer decision guidance in the early phase of product design, where explicit
design decisions are yet to be made. Function-based design data is often sparse
and grounded in individual interpretation. As such, function-based design tools
can benefit from automatic function classification to increase data fidelity
and provide function representation models that enable function-based
intelligent design agents. Function-based design data is commonly stored in
manually generated design repositories. These design repositories are a
collection of expert knowledge and interpretations of function in product
design bounded by function-flow and component taxonomies. In this work, we
represent a structured taxonomy-based design repository as assembly-flow
graphs, then leverage a graph neural network (GNN) model to perform automatic
function classification. We support automated function classification by
learning from repository data to establish the ground truth of component
function assignment. Experimental results show that our GNN model achieves a
micro-average F${_1}$-score of 0.832 for tier 1 (broad), 0.756 for tier 2, and
0.783 for tier 3 (specific) functions. Given the imbalance of data features,
the results are encouraging. Our efforts in this paper can be a starting point
for more sophisticated applications in knowledge-based CAD systems and
Design-for-X consideration in function-based design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferrero_V/0/1/0/all/0/1"&gt;Vincenzo Ferrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassani_K/0/1/0/all/0/1"&gt;Kaveh Hassani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grandi_D/0/1/0/all/0/1"&gt;Daniele Grandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DuPont_B/0/1/0/all/0/1"&gt;Bryony DuPont&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07002</id>
        <link href="http://arxiv.org/abs/2107.07002"/>
        <updated>2021-07-16T00:48:24.464Z</updated>
        <summary type="html"><![CDATA[The world of empirical machine learning (ML) strongly relies on benchmarks in
order to determine the relative effectiveness of different algorithms and
methods. This paper proposes the notion of "a benchmark lottery" that describes
the overall fragility of the ML benchmarking process. The benchmark lottery
postulates that many factors, other than fundamental algorithmic superiority,
may lead to a method being perceived as superior. On multiple benchmark setups
that are prevalent in the ML community, we show that the relative performance
of algorithms may be altered significantly simply by choosing different
benchmark tasks, highlighting the fragility of the current paradigms and
potential fallacious interpretation derived from benchmarking ML methods. Given
that every benchmark makes a statement about what it perceives to be important,
we argue that this might lead to biased progress in the community. We discuss
the implications of the observed phenomena and provide recommendations on
mitigating them using multiple machine learning domains and communities as use
cases, including natural language processing, computer vision, information
retrieval, recommender systems, and reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1"&gt;Alexey A. Gritsenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhe Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1"&gt;Fernando Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition. (arXiv:2107.07029v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.07029</id>
        <link href="http://arxiv.org/abs/2107.07029"/>
        <updated>2021-07-16T00:48:24.445Z</updated>
        <summary type="html"><![CDATA[Deep learning work on musical instrument recognition has generally focused on
instrument classes for which we have abundant data. In this work, we exploit
hierarchical relationships between instruments in a few-shot learning setup to
enable classification of a wider set of musical instruments, given a few
examples at inference. We apply a hierarchical loss function to the training of
prototypical networks, combined with a method to aggregate prototypes
hierarchically, mirroring the structure of a predefined musical instrument
hierarchy. These extensions require no changes to the network architecture and
new levels can be easily added or removed. Compared to a non-hierarchical
few-shot baseline, our method leads to a significant increase in classification
accuracy and significant decrease mistake severity on instrument classes unseen
in training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_H/0/1/0/all/0/1"&gt;Hugo Flores Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aguilar_A/0/1/0/all/0/1"&gt;Aldo Aguilar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manilow_E/0/1/0/all/0/1"&gt;Ethan Manilow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pardo_B/0/1/0/all/0/1"&gt;Bryan Pardo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Teaching Size. (arXiv:2107.07038v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07038</id>
        <link href="http://arxiv.org/abs/2107.07038"/>
        <updated>2021-07-16T00:48:24.440Z</updated>
        <summary type="html"><![CDATA[Recent research in machine teaching has explored the instruction of any
concept expressed in a universal language. In this compositional context, new
experimental results have shown that there exist data teaching sets
surprisingly shorter than the concept description itself. However, there exists
a bound for those remarkable experimental findings through teaching size and
concept complexity that we further explore here. As concepts are rarely taught
in isolation we investigate the best configuration of concepts to teach a given
set of concepts, where those that have been acquired first can be reused for
the description of new ones. This new notion of conditional teaching size
uncovers new insights, such as the interposition phenomenon: certain prior
knowledge generates simpler compatible concepts that increase the teaching size
of the concept that we want to teach. This does not happen for conditional
Kolmogorov complexity. Furthermore, we provide an algorithm that constructs
optimal curricula based on interposition avoidance. This paper presents a
series of theoretical results, including their proofs, and some directions for
future work. New research possibilities in curriculum teaching in compositional
scenarios are now wide open to exploration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Piqueras_M/0/1/0/all/0/1"&gt;Manuel Garcia-Piqueras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Orallo_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Hern&amp;#xe1;ndez-Orallo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Rank Temporal Attention-Augmented Bilinear Network for financial time-series forecasting. (arXiv:2107.06995v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06995</id>
        <link href="http://arxiv.org/abs/2107.06995"/>
        <updated>2021-07-16T00:48:24.433Z</updated>
        <summary type="html"><![CDATA[Financial market analysis, especially the prediction of movements of stock
prices, is a challenging problem. The nature of financial time-series data,
being non-stationary and nonlinear, is the main cause of these challenges. Deep
learning models have led to significant performance improvements in many
problems coming from different domains, including prediction problems of
financial time-series data. Although the prediction performance is the main
goal of such models, dealing with ultra high-frequency data sets restrictions
in terms of the number of model parameters and its inference speed. The
Temporal Attention-Augmented Bilinear network was recently proposed as an
efficient and high-performing model for Limit Order Book time-series
forecasting. In this paper, we propose a low-rank tensor approximation of the
model to further reduce the number of trainable parameters and increase its
speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shabani_M/0/1/0/all/0/1"&gt;Mostafa Shabani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free-Text Keystroke Dynamics for User Authentication. (arXiv:2107.07009v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07009</id>
        <link href="http://arxiv.org/abs/2107.07009"/>
        <updated>2021-07-16T00:48:24.419Z</updated>
        <summary type="html"><![CDATA[In this research, we consider the problem of verifying user identity based on
keystroke dynamics obtained from free-text. We employ a novel feature
engineering method that generates image-like transition matrices. For this
image-like feature, a convolution neural network (CNN) with cutout achieves the
best results. A hybrid model consisting of a CNN and a recurrent neural network
(RNN) is also shown to outperform previous research in this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Han-Chih Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1"&gt;Mark Stamp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing. (arXiv:2107.06990v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06990</id>
        <link href="http://arxiv.org/abs/2107.06990"/>
        <updated>2021-07-16T00:48:24.413Z</updated>
        <summary type="html"><![CDATA[Automated writing evaluation systems can improve students' writing insofar as
students attend to the feedback provided and revise their essay drafts in ways
aligned with such feedback. Existing research on revision of argumentative
writing in such systems, however, has focused on the types of revisions
students make (e.g., surface vs. content) rather than the extent to which
revisions actually respond to the feedback provided and improve the essay. We
introduce an annotation scheme to capture the nature of sentence-level
revisions of evidence use and reasoning (the `RER' scheme) and apply it to 5th-
and 6th-grade students' argumentative essays. We show that reliable manual
annotation can be achieved and that revision annotations correlate with a
holistic assessment of essay improvement in line with the feedback provided.
Furthermore, we explore the feasibility of automatically classifying revisions
according to our scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Afrin_T/0/1/0/all/0/1"&gt;Tazin Afrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Elaine Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1"&gt;Diane Litman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsumura_L/0/1/0/all/0/1"&gt;Lindsay C. Matsumura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Correnti_R/0/1/0/all/0/1"&gt;Richard Correnti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Field Guide to Federated Optimization. (arXiv:2107.06917v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06917</id>
        <link href="http://arxiv.org/abs/2107.06917"/>
        <updated>2021-07-16T00:48:24.404Z</updated>
        <summary type="html"><![CDATA[Federated learning and analytics are a distributed approach for
collaboratively learning models (or statistics) from decentralized data,
motivated by and designed for privacy protection. The distributed learning
process can be formulated as solving federated optimization problems, which
emphasize communication efficiency, data heterogeneity, compatibility with
privacy and system requirements, and other constraints that are not primary
considerations in other problem settings. This paper provides recommendations
and guidelines on formulating, designing, evaluating and analyzing federated
optimization algorithms through concrete examples and practical implementation,
with a focus on conducting effective simulations to infer real-world
performance. The goal of this work is not to survey the current literature, but
to inspire researchers and practitioners to design federated learning
algorithms that can be used in various practical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1"&gt;Zachary Charles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1"&gt;Gauri Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McMahan_H/0/1/0/all/0/1"&gt;H. Brendan McMahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1"&gt;Blaise Aguera y Arcas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1"&gt;Maruan Al-Shedivat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andrew_G/0/1/0/all/0/1"&gt;Galen Andrew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1"&gt;Salman Avestimehr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1"&gt;Katharine Daly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Data_D/0/1/0/all/0/1"&gt;Deepesh Data&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1"&gt;Suhas Diggavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eichner_H/0/1/0/all/0/1"&gt;Hubert Eichner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadhikar_A/0/1/0/all/0/1"&gt;Advait Gadhikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1"&gt;Zachary Garrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girgis_A/0/1/0/all/0/1"&gt;Antonious M. Girgis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanzely_F/0/1/0/all/0/1"&gt;Filip Hanzely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hard_A/0/1/0/all/0/1"&gt;Andrew Hard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1"&gt;Chaoyang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horvath_S/0/1/0/all/0/1"&gt;Samuel Horvath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1"&gt;Zhouyuan Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ingerman_A/0/1/0/all/0/1"&gt;Alex Ingerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1"&gt;Tara Javidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1"&gt;Peter Kairouz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1"&gt;Satyen Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1"&gt;Sai Praneeth Karimireddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konecny_J/0/1/0/all/0/1"&gt;Jakub Konecny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1"&gt;Sanmi Koyejo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Luyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1"&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Hang Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_S/0/1/0/all/0/1"&gt;Sashank J. Reddi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richtarik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1"&gt;Karan Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1"&gt;Virginia Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1"&gt;Mahdi Soltanolkotabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1"&gt;Weikang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ananda Theertha Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1"&gt;Ameet Talwalkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1"&gt;Blake Woodworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shanshan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Felix X. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Honglin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Chunxiang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chen Zhu&lt;/a&gt;, et al. (1 additional author not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DAL: Feature Learning from Overt Speech to Decode Imagined Speech-based EEG Signals with Convolutional Autoencoder. (arXiv:2107.07064v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.07064</id>
        <link href="http://arxiv.org/abs/2107.07064"/>
        <updated>2021-07-16T00:48:24.396Z</updated>
        <summary type="html"><![CDATA[Brain-computer interface (BCI) is one of the tools which enables the
communication between humans and devices by reflecting intention and status of
humans. With the development of artificial intelligence, the interest in
communication between humans and drones using electroencephalogram (EEG) is
increased. Especially, in the case of controlling drone swarms such as
direction or formation, there are many advantages compared with controlling a
drone unit. Imagined speech is one of the endogenous BCI paradigms, which can
identify intentions of users. When conducting imagined speech, the users
imagine the pronunciation as if actually speaking. In contrast, overt speech is
a task in which the users directly pronounce the words. When controlling drone
swarms using imagined speech, complex commands can be delivered more
intuitively, but decoding performance is lower than that of other endogenous
BCI paradigms. We proposed the Deep-autoleaner (DAL) to learn EEG features of
overt speech for imagined speech-based EEG signals classification. To the best
of our knowledge, this study is the first attempt to use EEG features of overt
speech to decode imagined speech-based EEG signals with an autoencoder. A total
of eight subjects participated in the experiment. When classifying four words,
the average accuracy of the DAL was 48.41%. In addition, when comparing the
performance between w/o and w/ EEG features of overt speech, there was a
performance improvement of 7.42% when including EEG features of overt speech.
Hence, we demonstrated that EEG features of overt speech could improve the
decoding performance of imagined speech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dae-Hyeok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sung-Jin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-Aware Fusing and Tiling of Neural Networks for Accelerated Edge Inference. (arXiv:2107.06960v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06960</id>
        <link href="http://arxiv.org/abs/2107.06960"/>
        <updated>2021-07-16T00:48:24.390Z</updated>
        <summary type="html"><![CDATA[A rising research challenge is running costly machine learning (ML) networks
locally on resource-constrained edge devices. ML networks with large
convolutional layers can easily exceed available memory, increasing latency due
to excessive swapping. Previous memory reduction techniques such as pruning and
quantization reduce model accuracy and often require retraining. Alternatively,
distributed methods partition the convolutions into equivalent smaller
sub-computations, but the implementations introduce communication costs and
require a network of devices. However, a distributed partitioning approach can
also be used to run in a reduced memory footprint on a single device by
subdividing the network into smaller operations.

This report extends prior work on distributed partitioning using tiling and
fusing of convolutional layers into a memory-aware execution on a single
device. Our approach extends prior fusing strategies to allow for two groups of
convolutional layers that are fused and tiled independently. This approach
reduces overhead via data reuse, and reduces the memory footprint further. We
also propose a memory usage predictor coupled with a search algorithm to
provide fusing and tiling configurations for an arbitrary set of convolutional
layers. When applied to the YOLOv2 object detection network, results show that
our approach can run in less than half the memory, and with a speedup of up to
2.78 under severe memory constraints. Additionally, our algorithm will return a
configuration with a latency that is within 6% of the best latency measured in
a manual search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farley_J/0/1/0/all/0/1"&gt;Jackson Farley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerstlauer_A/0/1/0/all/0/1"&gt;Andreas Gerstlauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GGT: Graph-Guided Testing for Adversarial Sample Detection of Deep Neural Network. (arXiv:2107.07043v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07043</id>
        <link href="http://arxiv.org/abs/2107.07043"/>
        <updated>2021-07-16T00:48:24.382Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNN) are known to be vulnerable to adversarial samples,
the detection of which is crucial for the wide application of these DNN models.
Recently, a number of deep testing methods in software engineering were
proposed to find the vulnerability of DNN systems, and one of them, i.e., Model
Mutation Testing (MMT), was used to successfully detect various adversarial
samples generated by different kinds of adversarial attacks. However, the
mutated models in MMT are always huge in number (e.g., over 100 models) and
lack diversity (e.g., can be easily circumvented by high-confidence adversarial
samples), which makes it less efficient in real applications and less effective
in detecting high-confidence adversarial samples. In this study, we propose
Graph-Guided Testing (GGT) for adversarial sample detection to overcome these
aforementioned challenges. GGT generates pruned models with the guide of graph
characteristics, each of them has only about 5% parameters of the mutated model
in MMT, and graph guided models have higher diversity. The experiments on
CIFAR10 and SVHN validate that GGT performs much better than MMT with respect
to both effectiveness and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zuohui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Renxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1"&gt;Jingyang Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yue Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1"&gt;Xin Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shouling Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1"&gt;Qi Xuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoniu Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-informed generative neural network: an application to troposphere temperature prediction. (arXiv:2107.06991v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06991</id>
        <link href="http://arxiv.org/abs/2107.06991"/>
        <updated>2021-07-16T00:48:24.377Z</updated>
        <summary type="html"><![CDATA[The troposphere is one of the atmospheric layers where most weather phenomena
occur. Temperature variations in the troposphere, especially at 500 hPa, a
typical level of the middle troposphere, are significant indicators of future
weather changes. Numerical weather prediction is effective for temperature
prediction, but its computational complexity hinders a timely response. This
paper proposes a novel temperature prediction approach in framework
ofphysics-informed deep learning. The new model, called PGnet, builds upon a
generative neural network with a mask matrix. The mask is designed to
distinguish the low-quality predicted regions generated by the first physical
stage. The generative neural network takes the mask as prior for the
second-stage refined predictions. A mask-loss and a jump pattern strategy are
developed to train the generative neural network without accumulating errors
during making time-series predictions. Experiments on ERA5 demonstrate that
PGnet can generate more refined temperature predictions than the
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhihao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jie Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weikai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zheng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the impossibility of non-trivial accuracy under fairness constraints. (arXiv:2107.06944v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06944</id>
        <link href="http://arxiv.org/abs/2107.06944"/>
        <updated>2021-07-16T00:48:24.361Z</updated>
        <summary type="html"><![CDATA[One of the main concerns about fairness in machine learning (ML) is that, in
order to achieve it, one may have to renounce to some accuracy. Having this
trade-off in mind, Hardt et al. have proposed the notion of equal opportunities
(EO), designed so as to be compatible with accuracy. In fact, it can be shown
that if the source of input data is deterministic, the two notions go well
along with each other. In the probabilistic case, however, things change.

As we show, there are probabilistic data sources for which EO can only be
achieved at the total detriment of accuracy, i.e. among the models that achieve
EO, those whose prediction does not depend on the input have the highest
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pinzon_C/0/1/0/all/0/1"&gt;Carlos Pinz&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1"&gt;Catuscia Palamidessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1"&gt;Pablo Piantanida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valencia_F/0/1/0/all/0/1"&gt;Frank Valencia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HTLM: Hyper-Text Pre-Training and Prompting of Language Models. (arXiv:2107.06955v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06955</id>
        <link href="http://arxiv.org/abs/2107.06955"/>
        <updated>2021-07-16T00:48:24.349Z</updated>
        <summary type="html"><![CDATA[We introduce HTLM, a hyper-text language model trained on a large-scale web
crawl. Modeling hyper-text has a number of advantages: (1) it is easily
gathered at scale, (2) it provides rich document-level and end-task-adjacent
supervision (e.g. class and id attributes often encode document category
information), and (3) it allows for new structured prompting that follows the
established semantics of HTML (e.g. to do zero-shot summarization by infilling
title tags for a webpage that contains the input text). We show that
pretraining with a BART-style denoising loss directly on simplified HTML
provides highly effective transfer for a wide range of end tasks and
supervision levels. HTLM matches or exceeds the performance of comparably sized
text-only LMs for zero-shot prompting and fine-tuning for classification
benchmarks, while also setting new state-of-the-art performance levels for
zero-shot summarization. We also find that hyper-text prompts provide more
value to HTLM, in terms of data efficiency, than plain text prompts do for
existing LMs, and that HTLM is highly effective at auto-prompting itself, by
simply generating the most likely hyper-text formatting for any available
training data. We will release all code and models to support future HTLM
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1"&gt;Armen Aghajanyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1"&gt;Dmytro Okhonko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Mike Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1"&gt;Mandar Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1"&gt;Gargi Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elastic Graph Neural Networks. (arXiv:2107.06996v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06996</id>
        <link href="http://arxiv.org/abs/2107.06996"/>
        <updated>2021-07-16T00:48:24.332Z</updated>
        <summary type="html"><![CDATA[While many existing graph neural networks (GNNs) have been proven to perform
$\ell_2$-based graph smoothing that enforces smoothness globally, in this work
we aim to further enhance the local smoothness adaptivity of GNNs via
$\ell_1$-based graph smoothing. As a result, we introduce a family of GNNs
(Elastic GNNs) based on $\ell_1$ and $\ell_2$-based graph smoothing. In
particular, we propose a novel and general message passing scheme into GNNs.
This message passing algorithm is not only friendly to back-propagation
training but also achieves the desired smoothing properties with a theoretical
convergence guarantee. Experiments on semi-supervised learning tasks
demonstrate that the proposed Elastic GNNs obtain better adaptivity on
benchmark datasets and are significantly robust to graph adversarial attacks.
The implementation of Elastic GNNs is available at
\url{https://github.com/lxiaorui/ElasticGNN}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaorui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1"&gt;Wei Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiliang Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Quantifying the Carbon Emissions of Differentially Private Machine Learning. (arXiv:2107.06946v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.06946</id>
        <link href="http://arxiv.org/abs/2107.06946"/>
        <updated>2021-07-16T00:48:24.316Z</updated>
        <summary type="html"><![CDATA[In recent years, machine learning techniques utilizing large-scale datasets
have achieved remarkable performance. Differential privacy, by means of adding
noise, provides strong privacy guarantees for such learning algorithms. The
cost of differential privacy is often a reduced model accuracy and a lowered
convergence speed. This paper investigates the impact of differential privacy
on learning algorithms in terms of their carbon footprint due to either longer
run-times or failed experiments. Through extensive experiments, further
guidance is provided on choosing the noise levels which can strike a balance
between desired privacy levels and reduced carbon emissions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1"&gt;Rakshit Naidu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diddee_H/0/1/0/all/0/1"&gt;Harshita Diddee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulay_A/0/1/0/all/0/1"&gt;Ajinkya Mulay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vardhan_A/0/1/0/all/0/1"&gt;Aleti Vardhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1"&gt;Krithika Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamzam_A/0/1/0/all/0/1"&gt;Ahmed Zamzam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Conditioned Knowledge Distillation. (arXiv:2107.06993v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06993</id>
        <link href="http://arxiv.org/abs/2107.06993"/>
        <updated>2021-07-16T00:48:24.295Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel confidence conditioned knowledge distillation (CCKD)
scheme for transferring the knowledge from a teacher model to a student model
is proposed. Existing state-of-the-art methods employ fixed loss functions for
this purpose and ignore the different levels of information that need to be
transferred for different samples. In addition to that, these methods are also
inefficient in terms of data usage. CCKD addresses these issues by leveraging
the confidence assigned by the teacher model to the correct class to devise
sample-specific loss functions (CCKD-L formulation) and targets (CCKD-T
formulation). Further, CCKD improves the data efficiency by employing
self-regulation to stop those samples from participating in the distillation
process on which the student model learns faster. Empirical evaluations on
several benchmark datasets show that CCKD methods achieve at least as much
generalization performance levels as other state-of-the-art methods while being
data efficient in the process. Student models trained through CCKD methods do
not retain most of the misclassifications commited by the teacher model on the
training set. Distillation through CCKD methods improves the resilience of the
student models against adversarial attacks compared to the conventional KD
method. Experiments show at least 3% increase in performance against
adversarial attacks for the MNIST and the Fashion MNIST datasets, and at least
6% increase for the CIFAR10 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Sourav Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Suresh Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Perceptual Image Quality Assessment for Compression. (arXiv:2103.01114v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01114</id>
        <link href="http://arxiv.org/abs/2103.01114"/>
        <updated>2021-07-16T00:48:24.289Z</updated>
        <summary type="html"><![CDATA[Lossy Image compression is necessary for efficient storage and transfer of
data. Typically the trade-off between bit-rate and quality determines the
optimal compression level. This makes the image quality metric an integral part
of any imaging system. While the existing full-reference metrics such as PSNR
and SSIM may be less sensitive to perceptual quality, the recently introduced
learning methods may fail to generalize to unseen data. In this paper we
propose the largest image compression quality dataset to date with human
perceptual preferences, enabling the use of deep learning, and we develop a
full reference perceptual quality assessment metric for lossy image compression
that outperforms the existing state-of-the-art methods. We show that the
proposed model can effectively learn from thousands of examples available in
the new dataset, and consequently it generalizes better to other unseen
datasets of human perceptual preference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mier_J/0/1/0/all/0/1"&gt;Juan Carlos Mier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_E/0/1/0/all/0/1"&gt;Eddie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1"&gt;Hossein Talebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Feng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1"&gt;Peyman Milanfar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards quantifying information flows: relative entropy in deep neural networks and the renormalization group. (arXiv:2107.06898v1 [hep-th])]]></title>
        <id>http://arxiv.org/abs/2107.06898</id>
        <link href="http://arxiv.org/abs/2107.06898"/>
        <updated>2021-07-16T00:48:24.270Z</updated>
        <summary type="html"><![CDATA[We investigate the analogy between the renormalization group (RG) and deep
neural networks, wherein subsequent layers of neurons are analogous to
successive steps along the RG. In particular, we quantify the flow of
information by explicitly computing the relative entropy or Kullback-Leibler
divergence in both the one- and two-dimensional Ising models under decimation
RG, as well as in a feedforward neural network as a function of depth. We
observe qualitatively identical behavior characterized by the monotonic
increase to a parameter-dependent asymptotic value. On the quantum field theory
side, the monotonic increase confirms the connection between the relative
entropy and the c-theorem. For the neural networks, the asymptotic behavior may
have implications for various information maximization methods in machine
learning, as well as for disentangling compactness and generalizability.
Furthermore, while both the two-dimensional Ising model and the random neural
networks we consider exhibit non-trivial critical points, the relative entropy
appears insensitive to the phase structure of either system. In this sense,
more refined probes are required in order to fully elucidate the flow of
information in these models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-th/1/au:+Erdmenger_J/0/1/0/all/0/1"&gt;Johanna Erdmenger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Grosvenor_K/0/1/0/all/0/1"&gt;Kevin T. Grosvenor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Jefferson_R/0/1/0/all/0/1"&gt;Ro Jefferson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance of Bayesian linear regression in a model with mismatch. (arXiv:2107.06936v1 [math.PR])]]></title>
        <id>http://arxiv.org/abs/2107.06936</id>
        <link href="http://arxiv.org/abs/2107.06936"/>
        <updated>2021-07-16T00:48:24.262Z</updated>
        <summary type="html"><![CDATA[For a model of high-dimensional linear regression with random design, we
analyze the performance of an estimator given by the mean of a log-concave
Bayesian posterior distribution with gaussian prior. The model is mismatched in
the following sense: like the model assumed by the statistician, the
labels-generating process is linear in the input data, but both the classifier
ground-truth prior and gaussian noise variance are unknown to her. This
inference model can be rephrased as a version of the Gardner model in spin
glasses and, using the cavity method, we provide fixed point equations for
various overlap order parameters, yielding in particular an expression for the
mean-square reconstruction error on the classifier (under an assumption of
uniqueness of solutions). As a direct corollary we obtain an expression for the
free energy. Similar models have already been studied by Shcherbina and Tirozzi
and by Talagrand, but our arguments are more straightforward and some
assumptions are relaxed. An interesting consequence of our analysis is that in
the random design setting of ridge regression, the performance of the posterior
mean is independent of the noise variance (or "temperature") assumed by the
statistician, and matches the one of the usual (zero temperature) ridge
estimator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Barbier_J/0/1/0/all/0/1"&gt;Jean Barbier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei-Kuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Panchenko_D/0/1/0/all/0/1"&gt;Dmitry Panchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Saenz_M/0/1/0/all/0/1"&gt;Manuel S&amp;#xe1;enz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos. (arXiv:2104.11452v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11452</id>
        <link href="http://arxiv.org/abs/2104.11452"/>
        <updated>2021-07-16T00:48:24.247Z</updated>
        <summary type="html"><![CDATA[Markerless motion capture and understanding of professional non-daily human
movements is an important yet unsolved task, which suffers from complex motion
patterns and severe self-occlusion, especially for the monocular setting. In
this paper, we propose SportsCap -- the first approach for simultaneously
capturing 3D human motions and understanding fine-grained actions from
monocular challenging sports video input. Our approach utilizes the semantic
and temporally structured sub-motion prior in the embedding space for motion
capture and understanding in a data-driven multi-task manner. To enable robust
capture under complex motion patterns, we propose an effective motion embedding
module to recover both the implicit motion embedding and explicit 3D motion
details via a corresponding mapping function as well as a sub-motion
classifier. Based on such hybrid motion information, we introduce a
multi-stream spatial-temporal Graph Convolutional Network(ST-GCN) to predict
the fine-grained semantic action attributes, and adopt a semantic attribute
mapping block to assemble various correlated action attributes into a
high-level action label for the overall detailed understanding of the whole
sequence, so as to enable various applications like action assessment or motion
scoring. Comprehensive experiments on both public and our proposed datasets
show that with a challenging monocular sports video input, our novel approach
not only significantly improves the accuracy of 3D human motion capture, but
also recovers accurate fine-grained semantic action attributes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1"&gt;Anqi Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yuexin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jingyi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. (arXiv:2107.06925v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.06925</id>
        <link href="http://arxiv.org/abs/2107.06925"/>
        <updated>2021-07-16T00:48:24.229Z</updated>
        <summary type="html"><![CDATA[Training large deep learning models at scale is very challenging. This paper
proposes Chimera, a novel pipeline parallelism scheme which combines
bidirectional pipelines for efficiently training large-scale models. Chimera is
a synchronous approach and therefore no loss of accuracy, which is more
convergence-friendly than asynchronous approaches. Compared with the latest
synchronous pipeline approach, Chimera reduces the number of bubbles by up to
50%; benefiting from the sophisticated scheduling of bidirectional pipelines,
Chimera has a more balanced activation memory consumption. Evaluations are
conducted on Transformer based language models. For a GPT-2 model with 1.3
billion parameters running on 2,048 GPU nodes of the Piz Daint supercomputer,
Chimera improves the training throughput by 1.16x-2.34x over the
state-of-the-art synchronous and asynchronous pipeline approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shigang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1"&gt;Torsten Hoefler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Failures in Out-of-Distribution Detection with Deep Generative Models. (arXiv:2107.06908v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06908</id>
        <link href="http://arxiv.org/abs/2107.06908"/>
        <updated>2021-07-16T00:48:24.223Z</updated>
        <summary type="html"><![CDATA[Deep generative models (DGMs) seem a natural fit for detecting
out-of-distribution (OOD) inputs, but such models have been shown to assign
higher probabilities or densities to OOD images than images from the training
distribution. In this work, we explain why this behavior should be attributed
to model misestimation. We first prove that no method can guarantee performance
beyond random chance without assumptions on which out-distributions are
relevant. We then interrogate the typical set hypothesis, the claim that
relevant out-distributions can lie in high likelihood regions of the data
distribution, and that OOD detection should be defined based on the data
distribution's typical set. We highlight the consequences implied by assuming
support overlap between in- and out-distributions, as well as the arbitrariness
of the typical set for OOD detection. Our results suggest that estimation error
is a more plausible explanation than the misalignment between likelihood-based
OOD detection and out-distributions of interest, and we illustrate how even
minimal estimation error can lead to OOD detection failures, yielding
implications for future work in deep generative modeling and OOD detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lily H. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_M/0/1/0/all/0/1"&gt;Mark Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements. (arXiv:2107.06943v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06943</id>
        <link href="http://arxiv.org/abs/2107.06943"/>
        <updated>2021-07-16T00:48:24.213Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an end-to-end multi-task neural network called
FetalNet with an attention mechanism and stacked module for spatio-temporal
fetal ultrasound scan video analysis. Fetal biometric measurement is a standard
examination during pregnancy used for the fetus growth monitoring and
estimation of gestational age and fetal weight. The main goal in fetal
ultrasound scan video analysis is to find proper standard planes to measure the
fetal head, abdomen and femur. Due to natural high speckle noise and shadows in
ultrasound data, medical expertise and sonographic experience are required to
find the appropriate acquisition plane and perform accurate measurements of the
fetus. In addition, existing computer-aided methods for fetal US biometric
measurement address only one single image frame without considering temporal
features. To address these shortcomings, we propose an end-to-end multi-task
neural network for spatio-temporal ultrasound scan video analysis to
simultaneously localize, classify and measure the fetal body parts. We propose
a new encoder-decoder segmentation architecture that incorporates a
classification branch. Additionally, we employ an attention mechanism with a
stacked module to learn salient maps to suppress irrelevant US regions and
efficient scan plane localization. We trained on the fetal ultrasound video
comes from routine examinations of 700 different patients. Our method called
FetalNet outperforms existing state-of-the-art methods in both classification
and segmentation in fetal ultrasound video recordings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plotka_S/0/1/0/all/0/1"&gt;Szymon P&amp;#x142;otka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wlodarczyk_T/0/1/0/all/0/1"&gt;Tomasz W&amp;#x142;odarczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klasa_A/0/1/0/all/0/1"&gt;Adam Klasa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipa_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Lipa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1"&gt;Arkadiusz Sitek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1"&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double-Uncertainty Assisted Spatial and Temporal Regularization Weighting for Learning-based Registration. (arXiv:2107.02433v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02433</id>
        <link href="http://arxiv.org/abs/2107.02433"/>
        <updated>2021-07-16T00:48:24.188Z</updated>
        <summary type="html"><![CDATA[In order to tackle the difficulty associated with the ill-posed nature of the
image registration problem, researchers use regularization to constrain the
solution space. For most learning-based registration approaches, the
regularization usually has a fixed weight and only constrains the spatial
transformation. Such convention has two limitations: (1) The regularization
strength of a specific image pair should be associated with the content of the
images, thus the ``one value fits all'' scheme is not ideal; (2) Only spatially
regularizing the transformation (but overlooking the temporal consistency of
different estimations) may not be the best strategy to cope with the
ill-posedness. In this study, we propose a mean-teacher based registration
framework. This framework incorporates an additional \textit{temporal
regularization} term by encouraging the teacher model's temporal ensemble
prediction to be consistent with that of the student model. At each training
step, it also automatically adjusts the weights of the \textit{spatial
regularization} and the \textit{temporal regularization} by taking account of
the transformation uncertainty and appearance uncertainty derived from the
perturbed teacher model. We perform experiments on multi- and uni-modal
registration tasks, and the results show that our strategy outperforms the
traditional and learning-based benchmark methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1"&gt;Donghuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jiangpeng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jagadeesan_J/0/1/0/all/0/1"&gt;Jayender Jagadeesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1"&gt;William Wells III&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frisken_S/0/1/0/all/0/1"&gt;Sarah Frisken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1"&gt;Raymond Kai-yu Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12423</id>
        <link href="http://arxiv.org/abs/2106.12423"/>
        <updated>2021-07-16T00:48:24.167Z</updated>
        <summary type="html"><![CDATA[We observe that despite their hierarchical convolutional nature, the
synthesis process of typical generative adversarial networks depends on
absolute pixel coordinates in an unhealthy manner. This manifests itself as,
e.g., detail appearing to be glued to image coordinates instead of the surfaces
of depicted objects. We trace the root cause to careless signal processing that
causes aliasing in the generator network. Interpreting all signals in the
network as continuous, we derive generally applicable, small architectural
changes that guarantee that unwanted information cannot leak into the
hierarchical synthesis process. The resulting networks match the FID of
StyleGAN2 but differ dramatically in their internal representations, and they
are fully equivariant to translation and rotation even at subpixel scales. Our
results pave the way for generative models better suited for video and
animation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1"&gt;Tero Karras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1"&gt;Miika Aittala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1"&gt;Samuli Laine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1"&gt;Erik H&amp;#xe4;rk&amp;#xf6;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1"&gt;Janne Hellsten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1"&gt;Jaakko Lehtinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1"&gt;Timo Aila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00652</id>
        <link href="http://arxiv.org/abs/2107.00652"/>
        <updated>2021-07-16T00:48:24.161Z</updated>
        <summary type="html"><![CDATA[We present CSWin Transformer, an efficient and effective Transformer-based
backbone for general-purpose vision tasks. A challenging issue in Transformer
design is that global self-attention is very expensive to compute whereas local
self-attention often limits the field of interactions of each token. To address
this issue, we develop the Cross-Shaped Window self-attention mechanism for
computing self-attention in the horizontal and vertical stripes in parallel
that form a cross-shaped window, with each stripe obtained by splitting the
input feature into stripes of equal width. We provide a detailed mathematical
analysis of the effect of the stripe width and vary the stripe width for
different layers of the Transformer network which achieves strong modeling
capability while limiting the computation cost. We also introduce
Locally-enhanced Positional Encoding (LePE), which handles the local positional
information better than existing encoding schemes. LePE naturally supports
arbitrary input resolutions, and is thus especially effective and friendly for
downstream tasks. Incorporated with these designs and a hierarchical structure,
CSWin Transformer demonstrates competitive performance on common vision tasks.
Specifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra
training data or label, 53.9 box AP and 46.4 mask AP on the COCO detection
task, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing
previous state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and
+2.0 respectively under the similar FLOPs setting. By further pretraining on
the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K
and state-of-the-art segmentation performance on ADE20K with 55.7 mIoU. The
code and models will be available at
https://github.com/microsoft/CSWin-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaoyi Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1"&gt;Jianmin Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1"&gt;Nenghai Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1"&gt;Baining Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expectation-Maximization Regularized Deep Learning for Weakly Supervised Tumor Segmentation for Glioblastoma. (arXiv:2101.08757v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08757</id>
        <link href="http://arxiv.org/abs/2101.08757"/>
        <updated>2021-07-16T00:48:24.154Z</updated>
        <summary type="html"><![CDATA[We present an Expectation-Maximization (EM) Regularized Deep Learning
(EMReDL) model for weakly supervised tumor segmentation. The proposed framework
is tailored to glioblastoma, a type of malignant tumor characterized by its
diffuse infiltration into the surrounding brain tissue, which poses significant
challenge to treatment target and tumor burden estimation using conventional
structural MRI. Although physiological MRI provides more specific information
regarding tumor infiltration, the relatively low resolution hinders a precise
full annotation. This has motivated us to develop a weakly supervised deep
learning solution that exploits the partial labelled tumor regions.

EMReDL contains two components: a physiological prior prediction model and
EM-regularized segmentation model. The physiological prior prediction model
exploits the physiological MRI by training a classifier to generate a
physiological prior map. This map is passed to the segmentation model for
regularization using the EM algorithm. We evaluated the model on a glioblastoma
dataset with the pre-operative multiparametric and recurrence MRI available.
EMReDL showed to effectively segment the infiltrated tumor from the partially
labelled region of potential infiltration. The segmented core tumor and
infiltrated tumor demonstrated high consistency with the tumor burden labelled
by experts. The performance comparisons showed that EMReDL achieved higher
accuracy than published state-of-the-art models. On MR spectroscopy, the
segmented region displayed more aggressive features than other partial labelled
region. The proposed model can be generalized to other segmentation tasks that
rely on partial labels, with the CNN architecture flexible in the framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wenjian Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yiran Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Price_S/0/1/0/all/0/1"&gt;Stephen J. Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RoRD: Rotation-Robust Descriptors and Orthographic Views for Local Feature Matching. (arXiv:2103.08573v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08573</id>
        <link href="http://arxiv.org/abs/2103.08573"/>
        <updated>2021-07-16T00:48:24.116Z</updated>
        <summary type="html"><![CDATA[The use of local detectors and descriptors in typical computer vision
pipelines work well until variations in viewpoint and appearance change become
extreme. Past research in this area has typically focused on one of two
approaches to this challenge: the use of projections into spaces more suitable
for feature matching under extreme viewpoint changes, and attempting to learn
features that are inherently more robust to viewpoint change. In this paper, we
present a novel framework that combines learning of invariant descriptors
through data augmentation and orthographic viewpoint projection. We propose
rotation-robust local descriptors, learnt through training data augmentation
based on rotation homographies, and a correspondence ensemble technique that
combines vanilla feature correspondences with those obtained through
rotation-robust features. Using a range of benchmark datasets as well as
contributing a new bespoke dataset for this research domain, we evaluate the
effectiveness of the proposed approach on key tasks including pose estimation
and visual place recognition. Our system outperforms a range of baseline and
state-of-the-art techniques, including enabling higher levels of place
recognition precision across opposing place viewpoints and achieves
practically-useful performance levels even under extreme viewpoint changes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parihar_U/0/1/0/all/0/1"&gt;Udit Singh Parihar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gujarathi_A/0/1/0/all/0/1"&gt;Aniket Gujarathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_K/0/1/0/all/0/1"&gt;Kinal Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tourani_S/0/1/0/all/0/1"&gt;Satyajit Tourani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Sourav Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1"&gt;Michael Milford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1"&gt;K. Madhava Krishna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asynchronous Multi-View SLAM. (arXiv:2101.06562v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06562</id>
        <link href="http://arxiv.org/abs/2101.06562"/>
        <updated>2021-07-16T00:48:24.104Z</updated>
        <summary type="html"><![CDATA[Existing multi-camera SLAM systems assume synchronized shutters for all
cameras, which is often not the case in practice. In this work, we propose a
generalized multi-camera SLAM formulation which accounts for asynchronous
sensor observations. Our framework integrates a continuous-time motion model to
relate information across asynchronous multi-frames during tracking, local
mapping, and loop closing. For evaluation, we collected AMV-Bench, a
challenging new SLAM dataset covering 482 km of driving recorded using our
asynchronous multi-camera robotic platform. AMV-Bench is over an order of
magnitude larger than previous multi-view HD outdoor SLAM datasets, and covers
diverse and challenging motions and environments. Our experiments emphasize the
necessity of asynchronous sensor modeling, and show that the use of multiple
cameras is critical towards robust and accurate SLAM in challenging outdoor
scenes. For additional information, please see the project website at:
https://www.cs.toronto.edu/~ajyang/amv-slam]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;Anqi Joyce Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1"&gt;Can Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barsan_I/0/1/0/all/0/1"&gt;Ioan Andrei B&amp;#xe2;rsan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1"&gt;Raquel Urtasun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shenlong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generator Versus Segmentor: Pseudo-healthy Synthesis. (arXiv:2009.05722v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05722</id>
        <link href="http://arxiv.org/abs/2009.05722"/>
        <updated>2021-07-16T00:48:24.080Z</updated>
        <summary type="html"><![CDATA[This paper investigates the problem of pseudo-healthy synthesis that is
defined as synthesizing a subject-specific pathology-free image from a
pathological one. Recent approaches based on Generative Adversarial Network
(GAN) have been developed for this task. However, these methods will inevitably
fall into the trade-off between preserving the subject-specific identity and
generating healthy-like appearances. To overcome this challenge, we propose a
novel adversarial training regime, Generator versus Segmentor (GVS), to
alleviate this trade-off by a divide-and-conquer strategy. We further consider
the deteriorating generalization performance of the segmentor throughout the
training and develop a pixel-wise weighted loss by muting the well-transformed
pixels to promote it. Moreover, we propose a new metric to measure how healthy
the synthetic images look. The qualitative and quantitative experiments on the
public dataset BraTS demonstrate that the proposed method outperforms the
existing methods. Besides, we also certify the effectiveness of our method on
datasets LiTS. Our implementation and pre-trained networks are publicly
available at https://github.com/Au3C2/Generator-Versus-Segmentor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yunlong_Z/0/1/0/all/0/1"&gt;Zhang Yunlong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chenxin_L/0/1/0/all/0/1"&gt;Li Chenxin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_L/0/1/0/all/0/1"&gt;Lin Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liyan_S/0/1/0/all/0/1"&gt;Sun Liyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yihong_Z/0/1/0/all/0/1"&gt;Zhuang Yihong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1"&gt;Huang Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xinghao_D/0/1/0/all/0/1"&gt;Ding Xinghao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiaoqing_L/0/1/0/all/0/1"&gt;Liu Xiaoqing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yizhou_Y/0/1/0/all/0/1"&gt;Yu Yizhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging. (arXiv:2008.02766v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02766</id>
        <link href="http://arxiv.org/abs/2008.02766"/>
        <updated>2021-07-16T00:48:24.064Z</updated>
        <summary type="html"><![CDATA[Saliency maps have become a widely used method to make deep learning models
more interpretable by providing post-hoc explanations of classifiers through
identification of the most pertinent areas of the input medical image. They are
increasingly being used in medical imaging to provide clinically plausible
explanations for the decisions the neural network makes. However, the utility
and robustness of these visualization maps has not yet been rigorously examined
in the context of medical imaging. We posit that trustworthiness in this
context requires 1) localization utility, 2) sensitivity to model weight
randomization, 3) repeatability, and 4) reproducibility. Using the localization
information available in two large public radiology datasets, we quantify the
performance of eight commonly used saliency map approaches for the above
criteria using area under the precision-recall curves (AUPRC) and structural
similarity index (SSIM), comparing their performance to various baseline
measures. Using our framework to quantify the trustworthiness of saliency maps,
we show that all eight saliency map techniques fail at least one of the
criteria and are, in most cases, less trustworthy when compared to the
baselines. We suggest that their usage in the high-risk domain of medical
imaging warrants additional scrutiny and recommend that detection or
segmentation models be used if localization is the desired output of the
network. Additionally, to promote reproducibility of our findings, we provide
the code we used for all tests performed in this work at this link:
https://github.com/QTIM-Lab/Assessing-Saliency-Maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arun_N/0/1/0/all/0/1"&gt;Nishanth Arun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaw_N/0/1/0/all/0/1"&gt;Nathan Gaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Praveer Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Ken Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1"&gt;Mehak Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bryan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoebel_K/0/1/0/all/0/1"&gt;Katharina Hoebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sharut Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_J/0/1/0/all/0/1"&gt;Jay Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gidwani_M/0/1/0/all/0/1"&gt;Mishka Gidwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adebayo_J/0/1/0/all/0/1"&gt;Julius Adebayo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Matthew D. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1"&gt;Jayashree Kalpathy-Cramer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StyleFusion: A Generative Model for Disentangling Spatial Segments. (arXiv:2107.07437v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07437</id>
        <link href="http://arxiv.org/abs/2107.07437"/>
        <updated>2021-07-16T00:48:24.050Z</updated>
        <summary type="html"><![CDATA[We present StyleFusion, a new mapping architecture for StyleGAN, which takes
as input a number of latent codes and fuses them into a single style code.
Inserting the resulting style code into a pre-trained StyleGAN generator
results in a single harmonized image in which each semantic region is
controlled by one of the input latent codes. Effectively, StyleFusion yields a
disentangled representation of the image, providing fine-grained control over
each region of the generated image. Moreover, to help facilitate global control
over the generated image, a special input latent code is incorporated into the
fused representation. StyleFusion operates in a hierarchical manner, where each
level is tasked with learning to disentangle a pair of image regions (e.g., the
car body and wheels). The resulting learned disentanglement allows one to
modify both local, fine-grained semantics (e.g., facial features) as well as
more global features (e.g., pose and background), providing improved
flexibility in the synthesis process. As a natural extension, StyleFusion
enables one to perform semantically-aware cross-image mixing of regions that
are not necessarily aligned. Finally, we demonstrate how StyleFusion can be
paired with existing editing techniques to more faithfully constrain the edit
to the user's region of interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kafri_O/0/1/0/all/0/1"&gt;Omer Kafri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patashnik_O/0/1/0/all/0/1"&gt;Or Patashnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alaluf_Y/0/1/0/all/0/1"&gt;Yuval Alaluf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[xCos: An Explainable Cosine Metric for Face Verification Task. (arXiv:2003.05383v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.05383</id>
        <link href="http://arxiv.org/abs/2003.05383"/>
        <updated>2021-07-16T00:48:24.013Z</updated>
        <summary type="html"><![CDATA[We study the XAI (explainable AI) on the face recognition task, particularly
the face verification here. Face verification is a crucial task in recent days
and it has been deployed to plenty of applications, such as access control,
surveillance, and automatic personal log-on for mobile devices. With the
increasing amount of data, deep convolutional neural networks can achieve very
high accuracy for the face verification task. Beyond exceptional performances,
deep face verification models need more interpretability so that we can trust
the results they generate. In this paper, we propose a novel similarity metric,
called explainable cosine ($xCos$), that comes with a learnable module that can
be plugged into most of the verification models to provide meaningful
explanations. With the help of $xCos$, we can see which parts of the two input
faces are similar, where the model pays its attention to, and how the local
similarities are weighted to form the output $xCos$ score. We demonstrate the
effectiveness of our proposed method on LFW and various competitive benchmarks,
resulting in not only providing novel and desiring model interpretability for
face verification but also ensuring the accuracy as plugging into existing face
recognition models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yu-Sheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe-Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-An Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Siang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Ya-Liang Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged Robots. (arXiv:2107.07243v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.07243</id>
        <link href="http://arxiv.org/abs/2107.07243"/>
        <updated>2021-07-16T00:48:23.997Z</updated>
        <summary type="html"><![CDATA[We present VILENS (Visual Inertial Lidar Legged Navigation System), an
odometry system for legged robots based on factor graphs. The key novelty is
the tight fusion of four different sensor modalities to achieve reliable
operation when the individual sensors would otherwise produce degenerate
estimation. To minimize leg odometry drift, we extend the robot's state with a
linear velocity bias term which is estimated online. This bias is only
observable because of the tight fusion of this preintegrated velocity factor
with vision, lidar, and IMU factors. Extensive experimental validation on the
ANYmal quadruped robots is presented, for a total duration of 2 h and 1.8 km
traveled. The experiments involved dynamic locomotion over loose rocks, slopes,
and mud; these included perceptual challenges, such as dark and dusty
underground caverns or open, feature-deprived areas, as well as mobility
challenges such as slipping and terrain deformation. We show an average
improvement of 62% translational and 51% rotational errors compared to a
state-of-the-art loosely coupled approach. To demonstrate its robustness,
VILENS was also integrated with a perceptive controller and a local path
planner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wisth_D/0/1/0/all/0/1"&gt;David Wisth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camurri_M/0/1/0/all/0/1"&gt;Marco Camurri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallon_M/0/1/0/all/0/1"&gt;Maurice Fallon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Cloud Completion by Learning Shape Priors. (arXiv:2008.00394v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00394</id>
        <link href="http://arxiv.org/abs/2008.00394"/>
        <updated>2021-07-16T00:48:23.969Z</updated>
        <summary type="html"><![CDATA[In view of the difficulty in reconstructing object details in point cloud
completion, we propose a shape prior learning method for object completion. The
shape priors include geometric information in both complete and the partial
point clouds. We design a feature alignment strategy to learn the shape prior
from complete points, and a coarse to fine strategy to incorporate partial
prior in the fine stage. To learn the complete objects prior, we first train a
point cloud auto-encoder to extract the latent embeddings from complete points.
Then we learn a mapping to transfer the point features from partial points to
that of the complete points by optimizing feature alignment losses. The feature
alignment losses consist of a L2 distance and an adversarial loss obtained by
Maximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2
distance optimizes the partial features towards the complete ones in the
feature space, and MMD-GAN decreases the statistical distance of two point
features in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art
performances on the point cloud completion task. Our code is available at
https://github.com/xiaogangw/point-cloud-completion-shape-prior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1"&gt;Marcelo H Ang Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gim Hee Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07502</id>
        <link href="http://arxiv.org/abs/2107.07502"/>
        <updated>2021-07-16T00:48:23.960Z</updated>
        <summary type="html"><![CDATA[Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yiwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zetian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jason Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Leslie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peter Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Michelle A. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutually improved endoscopic image synthesis and landmark detection in unpaired image-to-image translation. (arXiv:2107.06941v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06941</id>
        <link href="http://arxiv.org/abs/2107.06941"/>
        <updated>2021-07-16T00:48:23.952Z</updated>
        <summary type="html"><![CDATA[The CycleGAN framework allows for unsupervised image-to-image translation of
unpaired data. In a scenario of surgical training on a physical surgical
simulator, this method can be used to transform endoscopic images of phantoms
into images which more closely resemble the intra-operative appearance of the
same surgical target structure. This can be viewed as a novel augmented reality
approach, which we coined Hyperrealism in previous work. In this use case, it
is of paramount importance to display objects like needles, sutures or
instruments consistent in both domains while altering the style to a more
tissue-like appearance. Segmentation of these objects would allow for a direct
transfer, however, contouring of these, partly tiny and thin foreground objects
is cumbersome and perhaps inaccurate. Instead, we propose to use landmark
detection on the points when sutures pass into the tissue. This objective is
directly incorporated into a CycleGAN framework by treating the performance of
pre-trained detector models as an additional optimization goal. We show that a
task defined on these sparse landmark labels improves consistency of synthesis
by the generator network in both domains. Comparing a baseline CycleGAN
architecture to our proposed extension (DetCycleGAN), mean precision (PPV)
improved by +61.32, mean sensitivity (TPR) by +37.91, and mean F1 score by
+0.4743. Furthermore, it could be shown that by dataset fusion, generated
intra-operative images can be leveraged as additional training data for the
detection network itself. The data is released within the scope of the AdaptOR
MICCAI Challenge 2021 at https://adaptor2021.github.io/, and code at
https://github.com/Cardio-AI/detcyclegan_pytorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharan_L/0/1/0/all/0/1"&gt;Lalith Sharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romano_G/0/1/0/all/0/1"&gt;Gabriele Romano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehler_S/0/1/0/all/0/1"&gt;Sven Koehler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelm_H/0/1/0/all/0/1"&gt;Halvar Kelm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karck_M/0/1/0/all/0/1"&gt;Matthias Karck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simone_R/0/1/0/all/0/1"&gt;Raffaele De Simone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engelhardt_S/0/1/0/all/0/1"&gt;Sandy Engelhardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Frank-Wolfe Adversarial Training. (arXiv:2012.12368v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12368</id>
        <link href="http://arxiv.org/abs/2012.12368"/>
        <updated>2021-07-16T00:48:23.934Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are easily fooled by small perturbations known as
adversarial attacks. Adversarial Training (AT) is a technique that
approximately solves a robust optimization problem to minimize the worst-case
loss and is widely regarded as the most effective defense against such attacks.
We develop a theoretical framework for adversarial training with FW
optimization (FW-AT) that reveals a geometric connection between the loss
landscape and the distortion of $\ell_\infty$ FW attacks (the attack's $\ell_2$
norm). Specifically, we show that high distortion of FW attacks is equivalent
to low variation along the attack path. It is then experimentally demonstrated
on various deep neural network architectures that $\ell_\infty$ attacks against
robust models achieve near maximal $\ell_2$ distortion. This mathematical
transparency differentiates FW from the more popular Projected Gradient Descent
(PGD) optimization. To demonstrate the utility of our theoretical framework we
develop FW-Adapt, a novel adversarial training algorithm which uses simple
distortion measure to adaptively change number of attack steps during training.
FW-Adapt provides strong robustness at lower training times in comparison to
PGD-AT for a variety of white-box and black-box attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1"&gt;Theodoros Tsiligkaridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1"&gt;Jay Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommending best course of treatment based on similarities of prognostic markers\thanks{All authors contributed equally. (arXiv:2107.07500v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07500</id>
        <link href="http://arxiv.org/abs/2107.07500"/>
        <updated>2021-07-16T00:48:23.918Z</updated>
        <summary type="html"><![CDATA[With the advancement in the technology sector spanning over every field, a
huge influx of information is inevitable. Among all the opportunities that the
advancements in the technology have brought, one of them is to propose
efficient solutions for data retrieval. This means that from an enormous pile
of data, the retrieval methods should allow the users to fetch the relevant and
recent data over time. In the field of entertainment and e-commerce,
recommender systems have been functioning to provide the aforementioned.
Employing the same systems in the medical domain could definitely prove to be
useful in variety of ways. Following this context, the goal of this paper is to
propose collaborative filtering based recommender system in the healthcare
sector to recommend remedies based on the symptoms experienced by the patients.
Furthermore, a new dataset is developed consisting of remedies concerning
various diseases to address the limited availability of the data. The proposed
recommender system accepts the prognostic markers of a patient as the input and
generates the best remedy course. With several experimental trials, the
proposed model achieved promising results in recommending the possible remedy
for given prognostic markers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sudhanshu/0/1/0/all/0/1"&gt;Sudhanshu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Level generation and style enhancement -- deep learning for game development overview. (arXiv:2107.07397v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07397</id>
        <link href="http://arxiv.org/abs/2107.07397"/>
        <updated>2021-07-16T00:48:23.875Z</updated>
        <summary type="html"><![CDATA[We present practical approaches of using deep learning to create and enhance
level maps and textures for video games -- desktop, mobile, and web. We aim to
present new possibilities for game developers and level artists. The task of
designing levels and filling them with details is challenging. It is both
time-consuming and takes effort to make levels rich, complex, and with a
feeling of being natural. Fortunately, recent progress in deep learning
provides new tools to accompany level designers and visual artists. Moreover,
they offer a way to generate infinite worlds for game replayability and adjust
educational games to players' needs. We present seven approaches to create
level maps, each using statistical methods, machine learning, or deep learning.
In particular, we include:

- Generative Adversarial Networks for creating new images from existing
examples (e.g. ProGAN).

- Super-resolution techniques for upscaling images while preserving crisp
detail (e.g. ESRGAN).

- Neural style transfer for changing visual themes.

- Image translation - turning semantic maps into images (e.g. GauGAN).

- Semantic segmentation for turning images into semantic masks (e.g. U-Net).

- Unsupervised semantic segmentation for extracting semantic features (e.g.
Tile2Vec).

- Texture synthesis - creating large patterns based on a smaller sample (e.g.
InGAN).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Migdal_P/0/1/0/all/0/1"&gt;Piotr Migda&amp;#x142;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olechno_B/0/1/0/all/0/1"&gt;Bart&amp;#x142;omiej Olechno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Podgorski_B/0/1/0/all/0/1"&gt;B&amp;#x142;a&amp;#x17c;ej Podg&amp;#xf3;rski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Channel Auto-Encoders and a Novel Dataset for Learning Domain Invariant Representations of Histopathology Images. (arXiv:2107.07271v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07271</id>
        <link href="http://arxiv.org/abs/2107.07271"/>
        <updated>2021-07-16T00:48:23.866Z</updated>
        <summary type="html"><![CDATA[Domain shift is a problem commonly encountered when developing automated
histopathology pipelines. The performance of machine learning models such as
convolutional neural networks within automated histopathology pipelines is
often diminished when applying them to novel data domains due to factors
arising from differing staining and scanning protocols. The Dual-Channel
Auto-Encoder (DCAE) model was previously shown to produce feature
representations that are less sensitive to appearance variation introduced by
different digital slide scanners. In this work, the Multi-Channel Auto-Encoder
(MCAE) model is presented as an extension to DCAE which learns from more than
two domains of data. Additionally, a synthetic dataset is generated using
CycleGANs that contains aligned tissue images that have had their appearance
synthetically modified. Experimental results show that the MCAE model produces
feature representations that are less sensitive to inter-domain variations than
the comparative StaNoSA method when tested on the novel synthetic data.
Additionally, the MCAE and StaNoSA models are tested on a novel tissue
classification task. The results of this experiment show the MCAE model out
performs the StaNoSA model by 5 percentage-points in the f1-score. These
results show that the MCAE model is able to generalise better to novel data and
tasks than existing approaches by actively learning normalised feature
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Moyes_A/0/1/0/all/0/1"&gt;Andrew Moyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gault_R/0/1/0/all/0/1"&gt;Richard Gault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ming_J/0/1/0/all/0/1"&gt;Ji Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Crookes_D/0/1/0/all/0/1"&gt;Danny Crookes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MeNToS: Tracklets Association with a Space-Time Memory Network. (arXiv:2107.07067v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07067</id>
        <link href="http://arxiv.org/abs/2107.07067"/>
        <updated>2021-07-16T00:48:23.860Z</updated>
        <summary type="html"><![CDATA[We propose a method for multi-object tracking and segmentation (MOTS) that
does not require fine-tuning or per benchmark hyperparameter selection. The
proposed method addresses particularly the data association problem. Indeed,
the recently introduced HOTA metric, that has a better alignment with the human
visual assessment by evenly balancing detections and associations quality, has
shown that improvements are still needed for data association. After creating
tracklets using instance segmentation and optical flow, the proposed method
relies on a space-time memory network (STM) developed for one-shot video object
segmentation to improve the association of tracklets with temporal gaps. To the
best of our knowledge, our method, named MeNToS, is the first to use the STM
network to track object masks for MOTS. We took the 4th place in the RobMOTS
challenge. The project page is https://mehdimiah.com/mentos.html.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miah_M/0/1/0/all/0/1"&gt;Mehdi Miah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1"&gt;Guillaume-Alexandre Bilodeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1"&gt;Nicolas Saunier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-image Full-body Human Relighting. (arXiv:2107.07259v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07259</id>
        <link href="http://arxiv.org/abs/2107.07259"/>
        <updated>2021-07-16T00:48:23.853Z</updated>
        <summary type="html"><![CDATA[We present a single-image data-driven method to automatically relight images
with full-body humans in them. Our framework is based on a realistic scene
decomposition leveraging precomputed radiance transfer (PRT) and spherical
harmonics (SH) lighting. In contrast to previous work, we lift the assumptions
on Lambertian materials and explicitly model diffuse and specular reflectance
in our data. Moreover, we introduce an additional light-dependent residual term
that accounts for errors in the PRT-based image reconstruction. We propose a
new deep learning architecture, tailored to the decomposition performed in PRT,
that is trained using a combination of L1, logarithmic, and rendering losses.
Our model outperforms the state of the art for full-body human relighting both
with synthetic images and photographs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lagunas_M/0/1/0/all/0/1"&gt;Manuel Lagunas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jimei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1"&gt;Ruben Villegas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1"&gt;Zhixin Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masia_B/0/1/0/all/0/1"&gt;Belen Masia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_D/0/1/0/all/0/1"&gt;Diego Gutierrez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Topic Inference for Chest X-Ray Report Generation. (arXiv:2107.07314v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07314</id>
        <link href="http://arxiv.org/abs/2107.07314"/>
        <updated>2021-07-16T00:48:23.836Z</updated>
        <summary type="html"><![CDATA[Automating report generation for medical imaging promises to reduce workload
and assist diagnosis in clinical practice. Recent work has shown that deep
learning models can successfully caption natural images. However, learning from
medical data is challenging due to the diversity and uncertainty inherent in
the reports written by different radiologists with discrepant expertise and
experience. To tackle these challenges, we propose variational topic inference
for automatic report generation. Specifically, we introduce a set of topics as
latent variables to guide sentence generation by aligning image and language
modalities in a latent space. The topics are inferred in a conditional
variational inference framework, with each topic governing the generation of a
sentence in the report. Further, we adopt a visual attention module that
enables the model to attend to different locations in the image and generate
more informative descriptions. We conduct extensive experiments on two
benchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results
demonstrate that our proposed variational topic inference method can generate
novel reports rather than mere copies of reports used in training, while still
achieving comparable performance to state-of-the-art methods in terms of
standard language generation criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1"&gt;Ivona Najdenkoska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1"&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1"&gt;Marcel Worring&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What and When to Look?: Temporal Span Proposal Network for Video Visual Relation Detection. (arXiv:2107.07154v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07154</id>
        <link href="http://arxiv.org/abs/2107.07154"/>
        <updated>2021-07-16T00:48:23.829Z</updated>
        <summary type="html"><![CDATA[Identifying relations between objects is central to understanding the scene.
While several works have been proposed for relation modeling in the image
domain, there have been many constraints in the video domain due to challenging
dynamics of spatio-temporal interactions (e.g., Between which objects are there
an interaction? When do relations occur and end?). To date, two representative
methods have been proposed to tackle Video Visual Relation Detection (VidVRD):
segment-based and window-based. We first point out the limitations these two
methods have and propose Temporal Span Proposal Network (TSPN), a novel method
with two advantages in terms of efficiency and effectiveness. 1) TSPN tells
what to look: it sparsifies relation search space by scoring relationness
(i.e., confidence score for the existence of a relation between pair of
objects) of object pair. 2) TSPN tells when to look: it leverages the full
video context to simultaneously predict the temporal span and categories of the
entire relations. TSPN demonstrates its effectiveness by achieving new
state-of-the-art by a significant margin on two VidVRD benchmarks
(ImageNet-VidVDR and VidOR) while also showing lower time complexity than
existing methods - in particular, twice as efficient as a popular segment-based
approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1"&gt;Sangmin Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1"&gt;Junhyug Noh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kangil Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neighbor-view Enhanced Model for Vision and Language Navigation. (arXiv:2107.07201v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07201</id>
        <link href="http://arxiv.org/abs/2107.07201"/>
        <updated>2021-07-16T00:48:23.822Z</updated>
        <summary type="html"><![CDATA[Vision and Language Navigation (VLN) requires an agent to navigate to a
target location by following natural language instructions. Most of existing
works represent a navigation candidate by the feature of the corresponding
single view where the candidate lies in. However, an instruction may mention
landmarks out of the single view as references, which might lead to failures of
textual-visual matching of existing methods. In this work, we propose a
multi-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate
visual contexts from neighbor views for better textual-visual matching.
Specifically, our NvEM utilizes a subject module and a reference module to
collect contexts from neighbor views. The subject module fuses neighbor views
at a global level, and the reference module fuses neighbor objects at a local
level. Subjects and references are adaptively determined via attention
mechanisms. Our model also includes an action module to utilize the strong
orientation guidance (e.g., ``turn left'') in instructions. Each module
predicts navigation action separately and their weighted sum is used for
predicting the final action. Extensive experimental results demonstrate the
effectiveness of the proposed method on the R2R and R4R benchmarks against
several state-of-the-art navigators, and NvEM even beats some pre-training
ones. Our code is available at https://github.com/MarSaKi/NvEM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1"&gt;Dong An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yuankai Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tieniu Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amodal segmentation just like doing a jigsaw. (arXiv:2107.07464v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07464</id>
        <link href="http://arxiv.org/abs/2107.07464"/>
        <updated>2021-07-16T00:48:23.816Z</updated>
        <summary type="html"><![CDATA[Amodal segmentation is a new direction of instance segmentation while
considering the segmentation of the visible and occluded parts of the instance.
The existing state-of-the-art method uses multi-task branches to predict the
amodal part and the visible part separately and subtract the visible part from
the amodal part to obtain the occluded part. However, the amodal part contains
visible information. Therefore, the separated prediction method will generate
duplicate information. Different from this method, we propose a method of
amodal segmentation based on the idea of the jigsaw. The method uses multi-task
branches to predict the two naturally decoupled parts of visible and occluded,
which is like getting two matching jigsaw pieces. Then put the two jigsaw
pieces together to get the amodal part. This makes each branch focus on the
modeling of the object. And we believe that there are certain rules in the
occlusion relationship in the real world. This is a kind of occlusion context
information. This jigsaw method can better model the occlusion relationship and
use the occlusion context information, which is important for amodal
segmentation. Experiments on two widely used amodally annotated datasets prove
that our method exceeds existing state-of-the-art methods. The source code of
this work will be made public soon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xunli Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianqin Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Conditioned Knowledge Distillation. (arXiv:2107.06993v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06993</id>
        <link href="http://arxiv.org/abs/2107.06993"/>
        <updated>2021-07-16T00:48:23.808Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel confidence conditioned knowledge distillation (CCKD)
scheme for transferring the knowledge from a teacher model to a student model
is proposed. Existing state-of-the-art methods employ fixed loss functions for
this purpose and ignore the different levels of information that need to be
transferred for different samples. In addition to that, these methods are also
inefficient in terms of data usage. CCKD addresses these issues by leveraging
the confidence assigned by the teacher model to the correct class to devise
sample-specific loss functions (CCKD-L formulation) and targets (CCKD-T
formulation). Further, CCKD improves the data efficiency by employing
self-regulation to stop those samples from participating in the distillation
process on which the student model learns faster. Empirical evaluations on
several benchmark datasets show that CCKD methods achieve at least as much
generalization performance levels as other state-of-the-art methods while being
data efficient in the process. Student models trained through CCKD methods do
not retain most of the misclassifications commited by the teacher model on the
training set. Distillation through CCKD methods improves the resilience of the
student models against adversarial attacks compared to the conventional KD
method. Experiments show at least 3% increase in performance against
adversarial attacks for the MNIST and the Fashion MNIST datasets, and at least
6% increase for the CIFAR10 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Sourav Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Suresh Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Attacks on Multi-task Visual Perception for Autonomous Driving. (arXiv:2107.07449v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07449</id>
        <link href="http://arxiv.org/abs/2107.07449"/>
        <updated>2021-07-16T00:48:23.791Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have accomplished impressive success in various
applications, including autonomous driving perception tasks, in recent years.
On the other hand, current deep neural networks are easily fooled by
adversarial attacks. This vulnerability raises significant concerns,
particularly in safety-critical applications. As a result, research into
attacking and defending DNNs has gained much coverage. In this work, detailed
adversarial attacks are applied on a diverse multi-task visual perception deep
network across distance estimation, semantic segmentation, motion detection,
and object detection. The experiments consider both white and black box attacks
for targeted and un-targeted cases, while attacking a task and inspecting the
effect on all the others, in addition to inspecting the effect of applying a
simple defense method. We conclude this paper by comparing and discussing the
experimental results, proposing insights and future work. The visualizations of
the attacks are available at https://youtu.be/R3JUV41aiPY.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sobh_I/0/1/0/all/0/1"&gt;Ibrahim Sobh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamed_A/0/1/0/all/0/1"&gt;Ahmed Hamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Varun Ravi Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Conditional Adaptation for Recognizing Unseen Classes in Unseen Domains. (arXiv:2107.07497v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07497</id>
        <link href="http://arxiv.org/abs/2107.07497"/>
        <updated>2021-07-16T00:48:23.785Z</updated>
        <summary type="html"><![CDATA[Recent progress towards designing models that can generalize to unseen
domains (i.e domain generalization) or unseen classes (i.e zero-shot learning)
has embarked interest towards building models that can tackle both domain-shift
and semantic shift simultaneously (i.e zero-shot domain generalization). For
models to generalize to unseen classes in unseen domains, it is crucial to
learn feature representation that preserves class-level (domain-invariant) as
well as domain-specific information. Motivated from the success of generative
zero-shot approaches, we propose a feature generative framework integrated with
a COntext COnditional Adaptive (COCOA) Batch-Normalization to seamlessly
integrate class-level semantic and domain-specific information. The generated
visual features better capture the underlying data distribution enabling us to
generalize to unseen classes and domains at test-time. We thoroughly evaluate
and analyse our approach on established large-scale benchmark - DomainNet and
demonstrate promising performance over baselines and state-of-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mangla_P/0/1/0/all/0/1"&gt;Puneet Mangla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandhok_S/0/1/0/all/0/1"&gt;Shivam Chandhok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A modular U-Net for automated segmentation of X-ray tomography images in composite materials. (arXiv:2107.07468v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07468</id>
        <link href="http://arxiv.org/abs/2107.07468"/>
        <updated>2021-07-16T00:48:23.779Z</updated>
        <summary type="html"><![CDATA[X-ray Computed Tomography (XCT) techniques have evolved to a point that
high-resolution data can be acquired so fast that classic segmentation methods
are prohibitively cumbersome, demanding automated data pipelines capable of
dealing with non-trivial 3D images. Deep learning has demonstrated success in
many image processing tasks, including material science applications, showing a
promising alternative for a humanfree segmentation pipeline. In this paper a
modular interpretation of UNet (Modular U-Net) is proposed and trained to
segment 3D tomography images of a three-phased glass fiber-reinforced Polyamide
66. We compare 2D and 3D versions of our model, finding that the former is
slightly better than the latter. We observe that human-comparable results can
be achievied even with only 10 annotated layers and using a shallow U-Net
yields better results than a deeper one. As a consequence, Neural Network (NN)
show indeed a promising venue to automate XCT data processing pipelines needing
no human, adhoc intervention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bertoldo_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o P C Bertoldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Decenciere_E/0/1/0/all/0/1"&gt;Etienne Decenci&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ryckelynck_D/0/1/0/all/0/1"&gt;David Ryckelynck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Proudhon_H/0/1/0/all/0/1"&gt;Henry Proudhon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Anomaly Instance Segmentation for Baggage Threat Recognition. (arXiv:2107.07333v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07333</id>
        <link href="http://arxiv.org/abs/2107.07333"/>
        <updated>2021-07-16T00:48:23.766Z</updated>
        <summary type="html"><![CDATA[Identifying potential threats concealed within the baggage is of prime
concern for the security staff. Many researchers have developed frameworks that
can detect baggage threats from X-ray scans. However, to the best of our
knowledge, all of these frameworks require extensive training on large-scale
and well-annotated datasets, which are hard to procure in the real world. This
paper presents a novel unsupervised anomaly instance segmentation framework
that recognizes baggage threats, in X-ray scans, as anomalies without requiring
any ground truth labels. Furthermore, thanks to its stylization capacity, the
framework is trained only once, and at the inference stage, it detects and
extracts contraband items regardless of their scanner specifications. Our
one-staged approach initially learns to reconstruct normal baggage content via
an encoder-decoder network utilizing a proposed stylization loss function. The
model subsequently identifies the abnormal regions by analyzing the disparities
within the original and the reconstructed scans. The anomalous regions are then
clustered and post-processed to fit a bounding box for their localization. In
addition, an optional classifier can also be appended with the proposed
framework to recognize the categories of these extracted anomalies. A thorough
evaluation of the proposed system on four public baggage X-ray datasets,
without any re-training, demonstrates that it achieves competitive performance
as compared to the conventional fully supervised methods (i.e., the mean
average precision score of 0.7941 on SIXray, 0.8591 on GDXray, 0.7483 on
OPIXray, and 0.5439 on COMPASS-XP dataset) while outperforming state-of-the-art
semi-supervised and unsupervised baggage threat detection frameworks by 67.37%,
32.32%, 47.19%, and 45.81% in terms of F1 score across SIXray, GDXray, OPIXray,
and COMPASS-XP datasets, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1"&gt;Taimur Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1"&gt;Samet Akcay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1"&gt;Naoufel Werghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07436</id>
        <link href="http://arxiv.org/abs/2107.07436"/>
        <updated>2021-07-16T00:48:23.748Z</updated>
        <summary type="html"><![CDATA[Shapley values are widely used to explain black-box models, but they are
costly to calculate because they require many model evaluations. We introduce
FastSHAP, a method for estimating Shapley values in a single forward pass using
a learned explainer model. FastSHAP amortizes the cost of explaining many
inputs via a learning approach inspired by the Shapley value's weighted least
squares characterization, and it can be trained using standard stochastic
gradient optimization. We compare FastSHAP to existing estimation approaches,
revealing that it generates high-quality explanations with orders of magnitude
speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jethani_N/0/1/0/all/0/1"&gt;Neil Jethani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sudarshan_M/0/1/0/all/0/1"&gt;Mukund Sudarshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Covert_I/0/1/0/all/0/1"&gt;Ian Covert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1"&gt;Su-In Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning based Food Instance Segmentation using Synthetic Data. (arXiv:2107.07191v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07191</id>
        <link href="http://arxiv.org/abs/2107.07191"/>
        <updated>2021-07-16T00:48:23.735Z</updated>
        <summary type="html"><![CDATA[In the process of intelligently segmenting foods in images using deep neural
networks for diet management, data collection and labeling for network training
are very important but labor-intensive tasks. In order to solve the
difficulties of data collection and annotations, this paper proposes a food
segmentation method applicable to real-world through synthetic data. To perform
food segmentation on healthcare robot systems, such as meal assistance robot
arm, we generate synthetic data using the open-source 3D graphics software
Blender placing multiple objects on meal plate and train Mask R-CNN for
instance segmentation. Also, we build a data collection system and verify our
segmentation model on real-world food data. As a result, on our real-world
dataset, the model trained only synthetic data is available to segment food
instances that are not trained with 52.2% mask AP@all, and improve performance
by +6.4%p after fine-tuning comparing to the model trained from scratch. In
addition, we also confirm the possibility and performance improvement on the
public dataset for fair analysis. Our code and pre-trained weights are
avaliable online at: https://github.com/gist-ailab/Food-Instance-Segmentation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;D. Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;J. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;J. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;K. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training for temporal sparsity in deep neural networks, application in video processing. (arXiv:2107.07305v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07305</id>
        <link href="http://arxiv.org/abs/2107.07305"/>
        <updated>2021-07-16T00:48:23.725Z</updated>
        <summary type="html"><![CDATA[Activation sparsity improves compute efficiency and resource utilization in
sparsity-aware neural network accelerators. As the predominant operation in
DNNs is multiply-accumulate (MAC) of activations with weights to compute inner
products, skipping operations where (at least) one of the two operands is zero
can make inference more efficient in terms of latency and power. Spatial
sparsification of activations is a popular topic in DNN literature and several
methods have already been established to bias a DNN for it. On the other hand,
temporal sparsity is an inherent feature of bio-inspired spiking neural
networks (SNNs), which neuromorphic processing exploits for hardware
efficiency. Introducing and exploiting spatio-temporal sparsity, is a topic
much less explored in DNN literature, but in perfect resonance with the trend
in DNN, to shift from static signal processing to more streaming signal
processing. Towards this goal, in this paper we introduce a new DNN layer
(called Delta Activation Layer), whose sole purpose is to promote temporal
sparsity of activations during training. A Delta Activation Layer casts
temporal sparsity into spatial activation sparsity to be exploited when
performing sparse tensor multiplications in hardware. By employing delta
inference and ``the usual'' spatial sparsification heuristics during training,
the resulting model learns to exploit not only spatial but also temporal
activation sparsity (for a given input data distribution). One may use the
Delta Activation Layer either during vanilla training or during a refinement
phase. We have implemented Delta Activation Layer as an extension of the
standard Tensoflow-Keras library, and applied it to train deep neural networks
on the Human Action Recognition (UCF101) dataset. We report an almost 3x
improvement of activation sparsity, with recoverable loss of model accuracy
after longer training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yousefzadeh_A/0/1/0/all/0/1"&gt;Amirreza Yousefzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sifalakis_M/0/1/0/all/0/1"&gt;Manolis Sifalakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN. (arXiv:2107.07224v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07224</id>
        <link href="http://arxiv.org/abs/2107.07224"/>
        <updated>2021-07-16T00:48:23.715Z</updated>
        <summary type="html"><![CDATA[Generative adversarial models (GANs) continue to produce advances in terms of
the visual quality of still images, as well as the learning of temporal
correlations. However, few works manage to combine these two interesting
capabilities for the synthesis of video content: Most methods require an
extensive training dataset in order to learn temporal correlations, while being
rather limited in the resolution and visual quality of their output frames. In
this paper, we present a novel approach to the video synthesis problem that
helps to greatly improve visual quality and drastically reduce the amount of
training data and resources necessary for generating video content. Our
formulation separates the spatial domain, in which individual frames are
synthesized, from the temporal domain, in which motion is generated. For the
spatial domain we make use of a pre-trained StyleGAN network, the latent space
of which allows control over the appearance of the objects it was trained for.
The expressive power of this model allows us to embed our training videos in
the StyleGAN latent space. Our temporal architecture is then trained not on
sequences of RGB frames, but on sequences of StyleGAN latent codes. The
advantageous properties of the StyleGAN space simplify the discovery of
temporal correlations. We demonstrate that it suffices to train our temporal
architecture on only 10 minutes of footage of 1 subject for about 6 hours.
After training, our model can not only generate new portrait videos for the
training subject, but also for any random subject which can be embedded in the
StyleGAN space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fox_G/0/1/0/all/0/1"&gt;Gereon Fox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1"&gt;Ayush Tewari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1"&gt;Mohamed Elgharib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Neural Bandit: Provable Algorithm for Visual-aware Advertising. (arXiv:2107.07438v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07438</id>
        <link href="http://arxiv.org/abs/2107.07438"/>
        <updated>2021-07-16T00:48:23.709Z</updated>
        <summary type="html"><![CDATA[Online advertising is ubiquitous in web business. Image displaying is
considered as one of the most commonly used formats to interact with customers.
Contextual multi-armed bandit has shown success in the application of
advertising to solve the exploration-exploitation dilemma existed in the
recommendation procedure. Inspired by the visual-aware advertising, in this
paper, we propose a contextual bandit algorithm, where the convolutional neural
network (CNN) is utilized to learn the reward function along with an upper
confidence bound (UCB) for exploration. We also prove a near-optimal regret
bound $\tilde{\mathcal{O}}(\sqrt{T})$ when the network is over-parameterized
and establish strong connections with convolutional neural tangent kernel
(CNTK). Finally, we evaluate the empirical performance of the proposed
algorithm and show that it outperforms other state-of-the-art UCB-based bandit
algorithms on real-world image data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1"&gt;Yikun Ban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jingrui He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DynaDog+T: A Parametric Animal Model for Synthetic Canine Image Generation. (arXiv:2107.07330v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07330</id>
        <link href="http://arxiv.org/abs/2107.07330"/>
        <updated>2021-07-16T00:48:23.691Z</updated>
        <summary type="html"><![CDATA[Synthetic data is becoming increasingly common for training computer vision
models for a variety of tasks. Notably, such data has been applied in tasks
related to humans such as 3D pose estimation where data is either difficult to
create or obtain in realistic settings. Comparatively, there has been less work
into synthetic animal data and it's uses for training models. Consequently, we
introduce a parametric canine model, DynaDog+T, for generating synthetic canine
images and data which we use for a common computer vision task, binary
segmentation, which would otherwise be difficult due to the lack of available
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deane_J/0/1/0/all/0/1"&gt;Jake Deane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kearney_S/0/1/0/all/0/1"&gt;Sinead Kearney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kwang In Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cosker_D/0/1/0/all/0/1"&gt;Darren Cosker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High carbon stock mapping at large scale with optical satellite imagery and spaceborne LIDAR. (arXiv:2107.07431v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07431</id>
        <link href="http://arxiv.org/abs/2107.07431"/>
        <updated>2021-07-16T00:48:23.675Z</updated>
        <summary type="html"><![CDATA[The increasing demand for commodities is leading to changes in land use
worldwide. In the tropics, deforestation, which causes high carbon emissions
and threatens biodiversity, is often linked to agricultural expansion. While
the need for deforestation-free global supply chains is widely recognized,
making progress in practice remains a challenge. Here, we propose an automated
approach that aims to support conservation and sustainable land use planning
decisions by mapping tropical landscapes at large scale and high spatial
resolution following the High Carbon Stock (HCS) approach. A deep learning
approach is developed that estimates canopy height for each 10 m Sentinel-2
pixel by learning from sparse GEDI LIDAR reference data, achieving an overall
RMSE of 6.3 m. We show that these wall-to-wall maps of canopy top height are
predictive for classifying HCS forests and degraded areas with an overall
accuracy of 86 % and produce a first high carbon stock map for Indonesia,
Malaysia, and the Philippines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1"&gt;Nico Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1"&gt;Jan Dirk Wegner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Framework for A Personalized Intelligent Assistant to Elderly People for Activities of Daily Living. (arXiv:2107.07344v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.07344</id>
        <link href="http://arxiv.org/abs/2107.07344"/>
        <updated>2021-07-16T00:48:23.649Z</updated>
        <summary type="html"><![CDATA[The increasing population of elderly people is associated with the need to
meet their increasing requirements and to provide solutions that can improve
their quality of life in a smart home. In addition to fear and anxiety towards
interfacing with systems; cognitive disabilities, weakened memory, disorganized
behavior and even physical limitations are some of the problems that elderly
people tend to face with increasing age. The essence of providing
technology-based solutions to address these needs of elderly people and to
create smart and assisted living spaces for the elderly; lies in developing
systems that can adapt by addressing their diversity and can augment their
performances in the context of their day to day goals. Therefore, this work
proposes a framework for development of a Personalized Intelligent Assistant to
help elderly people perform Activities of Daily Living (ADLs) in a smart and
connected Internet of Things (IoT) based environment. This Personalized
Intelligent Assistant can analyze different tasks performed by the user and
recommend activities by considering their daily routine, current affective
state and the underlining user experience. To uphold the efficacy of this
proposed framework, it has been tested on a couple of datasets for modelling an
average user and a specific user respectively. The results presented show that
the model achieves a performance accuracy of 73.12% when modelling a specific
user, which is considerably higher than its performance while modelling an
average user, this upholds the relevance for development and implementation of
this proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nirmalya Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chia Y. Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Automatic Natural Image Matting. (arXiv:2107.07235v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07235</id>
        <link href="http://arxiv.org/abs/2107.07235"/>
        <updated>2021-07-16T00:48:23.587Z</updated>
        <summary type="html"><![CDATA[Automatic image matting (AIM) refers to estimating the soft foreground from
an arbitrary natural image without any auxiliary input like trimap, which is
useful for image editing. Prior methods try to learn semantic features to aid
the matting process while being limited to images with salient opaque
foregrounds such as humans and animals. In this paper, we investigate the
difficulties when extending them to natural images with salient
transparent/meticulous foregrounds or non-salient foregrounds. To address the
problem, a novel end-to-end matting network is proposed, which can predict a
generalized trimap for any image of the above types as a unified semantic
representation. Simultaneously, the learned semantic features guide the matting
network to focus on the transition areas via an attention mechanism. We also
construct a test set AIM-500 that contains 500 diverse natural images covering
all types along with manually labeled alpha mattes, making it feasible to
benchmark the generalization ability of AIM models. Results of the experiments
demonstrate that our network trained on available composite matting datasets
outperforms existing methods both objectively and subjectively. The source code
and dataset are available at https://github.com/JizhiziLi/AIM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jizhizi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Image Cropping. (arXiv:2107.07153v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07153</id>
        <link href="http://arxiv.org/abs/2107.07153"/>
        <updated>2021-07-16T00:48:23.563Z</updated>
        <summary type="html"><![CDATA[Automatic image cropping techniques are commonly used to enhance the
aesthetic quality of an image; they do it by detecting the most beautiful or
the most salient parts of the image and removing the unwanted content to have a
smaller image that is more visually pleasing. In this thesis, I introduce an
additional dimension to the problem of cropping, semantics. I argue that image
cropping can also enhance the image's relevancy for a given entity by using the
semantic information contained in the image. I call this problem, Semantic
Image Cropping. To support my argument, I provide a new dataset containing 100
images with at least two different entities per image and four ground truth
croppings collected using Amazon Mechanical Turk. I use this dataset to show
that state-of-the-art cropping algorithms that only take into account
aesthetics do not perform well in the problem of semantic image cropping.
Additionally, I provide a new deep learning system that takes not just
aesthetics but also semantics into account to generate image croppings, and I
evaluate its performance using my new semantic cropping dataset, showing that
using the semantic information of an image can help to produce better
croppings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Corcoll_O/0/1/0/all/0/1"&gt;Oriol Corcoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Lambertian Priors into Surface Normals Measurement. (arXiv:2107.07192v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07192</id>
        <link href="http://arxiv.org/abs/2107.07192"/>
        <updated>2021-07-16T00:48:23.554Z</updated>
        <summary type="html"><![CDATA[The goal of photometric stereo is to measure the precise surface normal of a
3D object from observations with various shading cues. However, non-Lambertian
surfaces influence the measurement accuracy due to irregular shading cues.
Despite deep neural networks have been employed to simulate the performance of
non-Lambertian surfaces, the error in specularities, shadows, and crinkle
regions is hard to be reduced. In order to address this challenge, we here
propose a photometric stereo network that incorporates Lambertian priors to
better measure the surface normal. In this paper, we use the initial normal
under the Lambertian assumption as the prior information to refine the normal
measurement, instead of solely applying the observed shading cues to deriving
the surface normal. Our method utilizes the Lambertian information to
reparameterize the network weights and the powerful fitting ability of deep
neural networks to correct these errors caused by general reflectance
properties. Our explorations include: the Lambertian priors (1) reduce the
learning hypothesis space, making our method learn the mapping in the same
surface normal space and improving the accuracy of learning, and (2) provides
the differential features learning, improving the surfaces reconstruction of
details. Extensive experiments verify the effectiveness of the proposed
Lambertian prior photometric stereo network in accurate surface normal
measurement, on the challenging benchmark dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1"&gt;Yakun Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jian_M/0/1/0/all/0/1"&gt;Muwei Jian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shaoxiang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yingyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Junyu Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Sparse Interaction Graphs of Partially Observed Pedestrians for Trajectory Prediction. (arXiv:2107.07056v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.07056</id>
        <link href="http://arxiv.org/abs/2107.07056"/>
        <updated>2021-07-16T00:48:23.530Z</updated>
        <summary type="html"><![CDATA[Multi-pedestrian trajectory prediction is an indispensable safety element of
autonomous systems that interact with crowds in unstructured environments. Many
recent efforts have developed trajectory prediction algorithms with focus on
understanding social norms behind pedestrian motions. Yet we observe these
works usually hold two assumptions that prevent them from being smoothly
applied to robot applications: positions of all pedestrians are consistently
tracked; the target agent pays attention to all pedestrians in the scene. The
first assumption leads to biased interaction modeling with incomplete
pedestrian data, and the second assumption introduces unnecessary disturbances
and leads to the freezing robot problem. Thus, we propose Gumbel Social
Transformer, in which an Edge Gumbel Selector samples a sparse interaction
graph of partially observed pedestrians at each time step. A Node Transformer
Encoder and a Masked LSTM encode the pedestrian features with the sampled
sparse graphs to predict trajectories. We demonstrate that our model overcomes
the potential problems caused by the assumptions, and our approach outperforms
the related works in benchmark evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruohua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1"&gt;Kazuki Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1"&gt;Katherine Driggs-Campbell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Parameter Generators. (arXiv:2107.07110v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07110</id>
        <link href="http://arxiv.org/abs/2107.07110"/>
        <updated>2021-07-16T00:48:23.523Z</updated>
        <summary type="html"><![CDATA[We present a generic method for recurrently using the same parameters for
many different convolution layers to build a deep network. Specifically, for a
network, we create a recurrent parameter generator (RPG), from which the
parameters of each convolution layer are generated. Though using recurrent
models to build a deep convolutional neural network (CNN) is not entirely new,
our method achieves significant performance gain compared to the existing
works. We demonstrate how to build a one-layer neural network to achieve
similar performance compared to other traditional CNN models on various
applications and datasets. Such a method allows us to build an arbitrarily
complex neural network with any amount of parameters. For example, we build a
ResNet34 with model parameters reduced by more than $400$ times, which still
achieves $41.6\%$ ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG
can be applied at different scales, such as layers, blocks, or even
sub-networks. Specifically, we use the RPG to build a ResNet18 network with the
number of weights equivalent to one convolutional layer of a conventional
ResNet and show this model can achieve $67.2\%$ ImageNet top-1 accuracy. The
proposed method can be viewed as an inverse approach to model compression.
Rather than removing the unused parameters from a large model, it aims to
squeeze more information into a small number of parameters. Extensive
experiment results are provided to demonstrate the power of the proposed
recurrent parameter generator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiayun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yubei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Stella X. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1"&gt;Brian Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1"&gt;Yann LeCun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lidar Light Scattering Augmentation (LISA): Physics-based Simulation of Adverse Weather Conditions for 3D Object Detection. (arXiv:2107.07004v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07004</id>
        <link href="http://arxiv.org/abs/2107.07004"/>
        <updated>2021-07-16T00:48:23.517Z</updated>
        <summary type="html"><![CDATA[Lidar-based object detectors are critical parts of the 3D perception pipeline
in autonomous navigation systems such as self-driving cars. However, they are
known to be sensitive to adverse weather conditions such as rain, snow and fog
due to reduced signal-to-noise ratio (SNR) and signal-to-background ratio
(SBR). As a result, lidar-based object detectors trained on data captured in
normal weather tend to perform poorly in such scenarios. However, collecting
and labelling sufficient training data in a diverse range of adverse weather
conditions is laborious and prohibitively expensive. To address this issue, we
propose a physics-based approach to simulate lidar point clouds of scenes in
adverse weather conditions. These augmented datasets can then be used to train
lidar-based detectors to improve their all-weather reliability. Specifically,
we introduce a hybrid Monte-Carlo based approach that treats (i) the effects of
large particles by placing them randomly and comparing their back reflected
power against the target, and (ii) attenuation effects on average through
calculation of scattering efficiencies from the Mie theory and particle size
distributions. Retraining networks with this augmented data improves mean
average precision evaluated on real world rainy scenes and we observe greater
improvement in performance with our model relative to existing models from the
literature. Furthermore, we evaluate recent state-of-the-art detectors on the
simulated weather conditions and present an in-depth analysis of their
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kilic_V/0/1/0/all/0/1"&gt;Velat Kilic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hegde_D/0/1/0/all/0/1"&gt;Deepti Hegde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sindagi_V/0/1/0/all/0/1"&gt;Vishwanath Sindagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1"&gt;A. Brinton Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_M/0/1/0/all/0/1"&gt;Mark A. Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Applying the Case Difference Heuristic to Learn Adaptations from Deep Network Features. (arXiv:2107.07095v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.07095</id>
        <link href="http://arxiv.org/abs/2107.07095"/>
        <updated>2021-07-16T00:48:23.509Z</updated>
        <summary type="html"><![CDATA[The case difference heuristic (CDH) approach is a knowledge-light method for
learning case adaptation knowledge from the case base of a case-based reasoning
system. Given a pair of cases, the CDH approach attributes the difference in
their solutions to the difference in the problems they solve, and generates
adaptation rules to adjust solutions accordingly when a retrieved case and new
query have similar problem differences. As an alternative to learning
adaptation rules, several researchers have applied neural networks to learn to
predict solution differences from problem differences. Previous work on such
approaches has assumed that the feature set describing problems is predefined.
This paper investigates a two-phase process combining deep learning for feature
extraction and neural network based adaptation learning from extracted
features. Its performance is demonstrated in a regression task on an image
data: predicting age given the image of a face. Results show that the combined
process can successfully learn adaptation knowledge applicable to nonsymbolic
differences in cases. The CBR system achieves slightly lower performance
overall than a baseline deep network regressor, but better performance than the
baseline on novel queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaomeng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Ziwei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leake_D/0/1/0/all/0/1"&gt;David Leake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xizi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1"&gt;David Crandall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COAST: COntrollable Arbitrary-Sampling NeTwork for Compressive Sensing. (arXiv:2107.07225v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07225</id>
        <link href="http://arxiv.org/abs/2107.07225"/>
        <updated>2021-07-16T00:48:23.418Z</updated>
        <summary type="html"><![CDATA[Recent deep network-based compressive sensing (CS) methods have achieved
great success. However, most of them regard different sampling matrices as
different independent tasks and need to train a specific model for each target
sampling matrix. Such practices give rise to inefficiency in computing and
suffer from poor generalization ability. In this paper, we propose a novel
COntrollable Arbitrary-Sampling neTwork, dubbed COAST, to solve CS problems of
arbitrary-sampling matrices (including unseen sampling matrices) with one
single model. Under the optimization-inspired deep unfolding framework, our
COAST exhibits good interpretability. In COAST, a random projection
augmentation (RPA) strategy is proposed to promote the training diversity in
the sampling space to enable arbitrary sampling, and a controllable proximal
mapping module (CPMM) and a plug-and-play deblocking (PnP-D) strategy are
further developed to dynamically modulate the network features and effectively
eliminate the blocking artifacts, respectively. Extensive experiments on widely
used benchmark datasets demonstrate that our proposed COAST is not only able to
handle arbitrary sampling matrices with one single model but also to achieve
state-of-the-art performance with fast speed. The source code is available on
https://github.com/jianzhangcs/COAST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_D/0/1/0/all/0/1"&gt;Di You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jingfen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Siwei Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STAR: Sparse Transformer-based Action Recognition. (arXiv:2107.07089v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07089</id>
        <link href="http://arxiv.org/abs/2107.07089"/>
        <updated>2021-07-16T00:48:23.377Z</updated>
        <summary type="html"><![CDATA[The cognitive system for human action and behavior has evolved into a deep
learning regime, and especially the advent of Graph Convolution Networks has
transformed the field in recent years. However, previous works have mainly
focused on over-parameterized and complex models based on dense graph
convolution networks, resulting in low efficiency in training and inference.
Meanwhile, the Transformer architecture-based model has not yet been well
explored for cognitive application in human action and behavior estimation.
This work proposes a novel skeleton-based human action recognition model with
sparse attention on the spatial dimension and segmented linear attention on the
temporal dimension of data. Our model can also process the variable length of
video clips grouped as a single batch. Experiments show that our model can
achieve comparable performance while utilizing much less trainable parameters
and achieve high speed in training and inference. Experiments show that our
model achieves 4~18x speedup and 1/7~1/15 model size compared with the baseline
models at competitive accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1"&gt;Feng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chonghan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1"&gt;Liang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yizhou Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1"&gt;Tianyi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muralidhar_S/0/1/0/all/0/1"&gt;Shivran Muralidhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1"&gt;Tian Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1"&gt;Vijaykrishnan Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying Component Function in Product Assemblies with Graph Neural Networks. (arXiv:2107.07042v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07042</id>
        <link href="http://arxiv.org/abs/2107.07042"/>
        <updated>2021-07-16T00:48:23.370Z</updated>
        <summary type="html"><![CDATA[Function is defined as the ensemble of tasks that enable the product to
complete the designed purpose. Functional tools, such as functional modeling,
offer decision guidance in the early phase of product design, where explicit
design decisions are yet to be made. Function-based design data is often sparse
and grounded in individual interpretation. As such, function-based design tools
can benefit from automatic function classification to increase data fidelity
and provide function representation models that enable function-based
intelligent design agents. Function-based design data is commonly stored in
manually generated design repositories. These design repositories are a
collection of expert knowledge and interpretations of function in product
design bounded by function-flow and component taxonomies. In this work, we
represent a structured taxonomy-based design repository as assembly-flow
graphs, then leverage a graph neural network (GNN) model to perform automatic
function classification. We support automated function classification by
learning from repository data to establish the ground truth of component
function assignment. Experimental results show that our GNN model achieves a
micro-average F${_1}$-score of 0.832 for tier 1 (broad), 0.756 for tier 2, and
0.783 for tier 3 (specific) functions. Given the imbalance of data features,
the results are encouraging. Our efforts in this paper can be a starting point
for more sophisticated applications in knowledge-based CAD systems and
Design-for-X consideration in function-based design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferrero_V/0/1/0/all/0/1"&gt;Vincenzo Ferrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassani_K/0/1/0/all/0/1"&gt;Kaveh Hassani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grandi_D/0/1/0/all/0/1"&gt;Daniele Grandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DuPont_B/0/1/0/all/0/1"&gt;Bryony DuPont&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient and Small Convolutional Neural Network for Pest Recognition -- ExquisiteNet. (arXiv:2107.07167v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07167</id>
        <link href="http://arxiv.org/abs/2107.07167"/>
        <updated>2021-07-16T00:48:23.363Z</updated>
        <summary type="html"><![CDATA[Nowadays, due to the rapid population expansion, food shortage has become a
critical issue. In order to stabilizing the food source production, preventing
crops from being attacked by pests is very important. In generally, farmers use
pesticides to kill pests, however, improperly using pesticides will also kill
some insects which is beneficial to crops, such as bees. If the number of bees
is too few, the supplement of food in the world will be in short. Besides,
excessive pesticides will seriously pollute the environment. Accordingly,
farmers need a machine which can automatically recognize the pests. Recently,
deep learning is popular because its effectiveness in the field of image
classification. In this paper, we propose a small and efficient model called
ExquisiteNet to complete the task of recognizing the pests and we expect to
apply our model on mobile devices. ExquisiteNet mainly consists of two blocks.
One is double fusion with squeeze-and-excitation-bottleneck block (DFSEB
block), and the other is max feature expansion block (ME block). ExquisiteNet
only has 0.98M parameters and its computing speed is very fast almost the same
as SqueezeNet. In order to evaluate our model's performance, we test our model
on a benchmark pest dataset called IP102. Compared to many state-of-the-art
models, such as ResNet101, ShuffleNetV2, MobileNetV3-large and EfficientNet
etc., our model achieves higher accuracy, that is, 52.32% on the test set of
IP102 without any data augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shi-Yao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chung-Yen Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Retrieval and Localization in Large Art Collections using Deep Multi-Style Feature Fusion and Iterative Voting. (arXiv:2107.06935v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06935</id>
        <link href="http://arxiv.org/abs/2107.06935"/>
        <updated>2021-07-16T00:48:23.356Z</updated>
        <summary type="html"><![CDATA[The search for specific objects or motifs is essential to art history as both
assist in decoding the meaning of artworks. Digitization has produced large art
collections, but manual methods prove to be insufficient to analyze them. In
the following, we introduce an algorithm that allows users to search for image
regions containing specific motifs or objects and find similar regions in an
extensive dataset, helping art historians to analyze large digitized art
collections. Computer vision has presented efficient methods for visual
instance retrieval across photographs. However, applied to art collections,
they reveal severe deficiencies because of diverse motifs and massive domain
shifts induced by differences in techniques, materials, and styles. In this
paper, we present a multi-style feature fusion approach that successfully
reduces the domain gap and improves retrieval results without labelled data or
curated image collections. Our region-based voting with GPU-accelerated
approximate nearest-neighbour search allows us to find and localize even small
motifs within an extensive dataset in a few seconds. We obtain state-of-the-art
results on the Brueghel dataset and demonstrate its generalization to
inhomogeneous collections with a large number of distractors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ufer_N/0/1/0/all/0/1"&gt;Nikolai Ufer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_S/0/1/0/all/0/1"&gt;Sabine Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07058</id>
        <link href="http://arxiv.org/abs/2107.07058"/>
        <updated>2021-07-16T00:48:23.350Z</updated>
        <summary type="html"><![CDATA[Image smoothing is a fundamental procedure in applications of both computer
vision and graphics. The required smoothing properties can be different or even
contradictive among different tasks. Nevertheless, the inherent smoothing
nature of one smoothing operator is usually fixed and thus cannot meet the
various requirements of different applications. In this paper, we first
introduce the truncated Huber penalty function which shows strong flexibility
under different parameter settings. A generalized framework is then proposed
with the introduced truncated Huber penalty function. When combined with its
strong flexibility, our framework is able to achieve diverse smoothing natures
where contradictive smoothing behaviors can even be achieved. It can also yield
the smoothing behavior that can seldom be achieved by previous methods, and
superior performance is thus achieved in challenging cases. These together
enable our framework capable of a range of applications and able to outperform
the state-of-the-art approaches in several tasks, such as image detail
enhancement, clip-art compression artifacts removal, guided depth map
restoration, image texture removal, etc. In addition, an efficient numerical
solution is provided and its convergence is theoretically guaranteed even the
optimization framework is non-convex and non-smooth. A simple yet effective
approach is further proposed to reduce the computational cost of our method
while maintaining its performance. The effectiveness and superior performance
of our approach are validated through comprehensive experiments in a range of
applications. Our code is available at
https://github.com/wliusjtu/Generalized-Smoothing-Framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Michael Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diff-Net: Image Feature Difference based High-Definition Map Change Detection. (arXiv:2107.07030v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07030</id>
        <link href="http://arxiv.org/abs/2107.07030"/>
        <updated>2021-07-16T00:48:23.333Z</updated>
        <summary type="html"><![CDATA[Up-to-date High-Definition (HD) maps are essential for self-driving cars. To
achieve constantly updated HD maps, we present a deep neural network (DNN),
Diff-Net, to detect changes in them. Compared to traditional methods based on
object detectors, the essential design in our work is a parallel feature
difference calculation structure that infers map changes by comparing features
extracted from the camera and rasterized images. To generate these rasterized
images, we project map elements onto images in the camera view, yielding
meaningful map representations that can be consumed by a DNN accordingly. As we
formulate the change detection task as an object detection problem, we leverage
the anchor-based structure that predicts bounding boxes with different change
status categories. Furthermore, rather than relying on single frame input, we
introduce a spatio-temporal fusion module that fuses features from history
frames into the current, thus improving the overall performance. Finally, we
comprehensively validate our method's effectiveness using freshly collected
datasets. Results demonstrate that our Diff-Net achieves better performance
than the baseline methods and is ready to be integrated into a map production
pipeline maintaining an up-to-date HD map.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shengjie Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaoqing Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Ning Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shiyu Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Significant Features for Few-Shot Learning using Dimensionality Reduction. (arXiv:2107.06992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06992</id>
        <link href="http://arxiv.org/abs/2107.06992"/>
        <updated>2021-07-16T00:48:23.325Z</updated>
        <summary type="html"><![CDATA[Few-shot learning is a relatively new technique that specializes in problems
where we have little amounts of data. The goal of these methods is to classify
categories that have not been seen before with just a handful of samples.
Recent approaches, such as metric learning, adopt the meta-learning strategy in
which we have episodic tasks conformed by support (training) data and query
(test) data. Metric learning methods have demonstrated that simple models can
achieve good performance by learning a similarity function to compare the
support and the query data. However, the feature space learned by a given
metric learning approach may not exploit the information given by a specific
few-shot task. In this work, we explore the use of dimension reduction
techniques as a way to find task-significant features helping to make better
predictions. We measure the performance of the reduced features by assigning a
score based on the intra-class and inter-class distance, and selecting a
feature reduction method in which instances of different classes are far away
and instances of the same class are close. This module helps to improve the
accuracy performance by allowing the similarity function, given by the metric
learning method, to have more discriminative features for the classification.
Our method outperforms the metric learning baselines in the miniImageNet
dataset by around 2% in accuracy performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_Ruiz_M/0/1/0/all/0/1"&gt;Mauricio Mendez-Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Zapata_I/0/1/0/all/0/1"&gt;Ivan Garcia Jorge Gonzalez-Zapata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1"&gt;Gilberto Ochoa-Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_A/0/1/0/all/0/1"&gt;Andres Mendez-Vazquez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements. (arXiv:2107.06943v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06943</id>
        <link href="http://arxiv.org/abs/2107.06943"/>
        <updated>2021-07-16T00:48:23.318Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an end-to-end multi-task neural network called
FetalNet with an attention mechanism and stacked module for spatio-temporal
fetal ultrasound scan video analysis. Fetal biometric measurement is a standard
examination during pregnancy used for the fetus growth monitoring and
estimation of gestational age and fetal weight. The main goal in fetal
ultrasound scan video analysis is to find proper standard planes to measure the
fetal head, abdomen and femur. Due to natural high speckle noise and shadows in
ultrasound data, medical expertise and sonographic experience are required to
find the appropriate acquisition plane and perform accurate measurements of the
fetus. In addition, existing computer-aided methods for fetal US biometric
measurement address only one single image frame without considering temporal
features. To address these shortcomings, we propose an end-to-end multi-task
neural network for spatio-temporal ultrasound scan video analysis to
simultaneously localize, classify and measure the fetal body parts. We propose
a new encoder-decoder segmentation architecture that incorporates a
classification branch. Additionally, we employ an attention mechanism with a
stacked module to learn salient maps to suppress irrelevant US regions and
efficient scan plane localization. We trained on the fetal ultrasound video
comes from routine examinations of 700 different patients. Our method called
FetalNet outperforms existing state-of-the-art methods in both classification
and segmentation in fetal ultrasound video recordings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plotka_S/0/1/0/all/0/1"&gt;Szymon P&amp;#x142;otka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wlodarczyk_T/0/1/0/all/0/1"&gt;Tomasz W&amp;#x142;odarczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klasa_A/0/1/0/all/0/1"&gt;Adam Klasa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipa_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Lipa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1"&gt;Arkadiusz Sitek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1"&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VAD-free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording. (arXiv:2107.07509v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.07509</id>
        <link href="http://arxiv.org/abs/2107.07509"/>
        <updated>2021-07-16T00:48:23.310Z</updated>
        <summary type="html"><![CDATA[In this work, we propose novel decoding algorithms to enable streaming
automatic speech recognition (ASR) on unsegmented long-form recordings without
voice activity detection (VAD), based on monotonic chunkwise attention (MoChA)
with an auxiliary connectionist temporal classification (CTC) objective. We
propose a block-synchronous beam search decoding to take advantage of efficient
batched output-synchronous and low-latency input-synchronous searches. We also
propose a VAD-free inference algorithm that leverages CTC probabilities to
determine a suitable timing to reset the model states to tackle the
vulnerability to long-form data. Experimental evaluations demonstrate that the
block-synchronous decoding achieves comparable accuracy to the
label-synchronous one. Moreover, the VAD-free inference can recognize long-form
speech robustly for up to a few hours.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1"&gt;Hirofumi Inaguma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1"&gt;Tatsuya Kawahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surgical Instruction Generation with Transformers. (arXiv:2107.06964v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06964</id>
        <link href="http://arxiv.org/abs/2107.06964"/>
        <updated>2021-07-16T00:48:23.301Z</updated>
        <summary type="html"><![CDATA[Automatic surgical instruction generation is a prerequisite towards
intra-operative context-aware surgical assistance. However, generating
instructions from surgical scenes is challenging, as it requires jointly
understanding the surgical activity of current view and modelling relationships
between visual information and textual description. Inspired by the neural
machine translation and imaging captioning tasks in open domain, we introduce a
transformer-backboned encoder-decoder network with self-critical reinforcement
learning to generate instructions from surgical images. We evaluate the
effectiveness of our method on DAISI dataset, which includes 290 procedures
from various medical disciplines. Our approach outperforms the existing
baseline over all caption evaluation metrics. The results demonstrate the
benefits of the encoder-decoder structure backboned by transformer in handling
multimodal context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinglu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1"&gt;Yinyu Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;Jian Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian Jun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Image Features Boost Housing Market Predictions?. (arXiv:2107.07148v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07148</id>
        <link href="http://arxiv.org/abs/2107.07148"/>
        <updated>2021-07-16T00:48:23.284Z</updated>
        <summary type="html"><![CDATA[The attractiveness of a property is one of the most interesting, yet
challenging, categories to model. Image characteristics are used to describe
certain attributes, and to examine the influence of visual factors on the price
or timeframe of the listing. In this paper, we propose a set of techniques for
the extraction of visual features for efficient numerical inclusion in
modern-day predictive algorithms. We discuss techniques such as Shannon's
entropy, calculating the center of gravity, employing image segmentation, and
using Convolutional Neural Networks. After comparing these techniques as
applied to a set of property-related images (indoor, outdoor, and satellite),
we conclude the following: (i) the entropy is the most efficient single-digit
visual measure for housing price prediction; (ii) image segmentation is the
most important visual feature for the prediction of housing lifespan; and (iii)
deep image features can be used to quantify interior characteristics and
contribute to captivation modeling. The set of 40 image features selected here
carries a significant amount of predictive power and outperforms some of the
strongest metadata predictors. Without any need to replace a human expert in a
real-estate appraisal process, we conclude that the techniques presented in
this paper can efficiently describe visible characteristics, thus introducing
perceived attractiveness as a quantitative measure into the predictive modeling
of housing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1"&gt;Zona Kostic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jevremovic_A/0/1/0/all/0/1"&gt;Aleksandar Jevremovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Compact CNNs for Image Classification using Dynamic-coded Filter Fusion. (arXiv:2107.06916v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06916</id>
        <link href="http://arxiv.org/abs/2107.06916"/>
        <updated>2021-07-16T00:48:23.278Z</updated>
        <summary type="html"><![CDATA[The mainstream approach for filter pruning is usually either to force a
hard-coded importance estimation upon a computation-heavy pretrained model to
select "important" filters, or to impose a hyperparameter-sensitive sparse
constraint on the loss objective to regularize the network training. In this
paper, we present a novel filter pruning method, dubbed dynamic-coded filter
fusion (DCFF), to derive compact CNNs in a computation-economical and
regularization-free manner for efficient image classification. Each filter in
our DCFF is firstly given an inter-similarity distribution with a temperature
parameter as a filter proxy, on top of which, a fresh Kullback-Leibler
divergence based dynamic-coded criterion is proposed to evaluate the filter
importance. In contrast to simply keeping high-score filters in other methods,
we propose the concept of filter fusion, i.e., the weighted averages using the
assigned proxies, as our preserved filters. We obtain a one-hot
inter-similarity distribution as the temperature parameter approaches infinity.
Thus, the relative importance of each filter can vary along with the training
of the compact CNN, leading to dynamically changeable fused filters without
both the dependency on the pretrained model and the introduction of sparse
constraints. Extensive experiments on classification benchmarks demonstrate the
superiority of our DCFF over the compared counterparts. For example, our DCFF
derives a compact VGGNet-16 with only 72.77M FLOPs and 1.06M parameters while
reaching top-1 accuracy of 93.47% on CIFAR-10. A compact ResNet-50 is obtained
with 63.8% FLOPs and 58.6% parameter reductions, retaining 75.60% top-1
accuracy on ILSVRC-2012. Our code, narrower models and training logs are
available at https://github.com/lmbxmu/DCFF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bohong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1"&gt;Fei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wei Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR. (arXiv:2107.00635v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00635</id>
        <link href="http://arxiv.org/abs/2107.00635"/>
        <updated>2021-07-16T00:48:23.271Z</updated>
        <summary type="html"><![CDATA[While attention-based encoder-decoder (AED) models have been successfully
extended to the online variants for streaming automatic speech recognition
(ASR), such as monotonic chunkwise attention (MoChA), the models still have a
large label emission latency because of the unconstrained end-to-end training
objective. Previous works tackled this problem by leveraging alignment
information to control the timing to emit tokens during training. In this work,
we propose a simple alignment-free regularization method, StableEmit, to
encourage MoChA to emit tokens earlier. StableEmit discounts the selection
probabilities in hard monotonic attention for token boundary detection by a
constant factor and regularizes them to recover the total attention mass during
training. As a result, the scale of the selection probabilities is increased,
and the values can reach a threshold for token emission earlier, leading to a
reduction of emission latency and deletion errors. Moreover, StableEmit can be
combined with methods that constraint alignments to further improve the
accuracy and latency. Experimental evaluations with LSTM and Conformer encoders
demonstrate that StableEmit significantly reduces the recognition errors and
the emission latency simultaneously. We also show that the use of alignment
information is complementary in both metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1"&gt;Hirofumi Inaguma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1"&gt;Tatsuya Kawahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06912</id>
        <link href="http://arxiv.org/abs/2107.06912"/>
        <updated>2021-07-16T00:48:23.264Z</updated>
        <summary type="html"><![CDATA[Connecting Vision and Language plays an essential role in Generative
Intelligence. For this reason, in the last few years, a large research effort
has been devoted to image captioning, i.e. the task of describing images with
syntactically and semantically meaningful sentences. Starting from 2015 the
task has generally been addressed with pipelines composed of a visual encoding
step and a language model for text generation. During these years, both
components have evolved considerably through the exploitation of object
regions, attributes, and relationships and the introduction of multi-modal
connections, fully-attentive approaches, and BERT-like early-fusion strategies.
However, regardless of the impressive results obtained, research in image
captioning has not reached a conclusive answer yet. This work aims at providing
a comprehensive overview and categorization of image captioning approaches,
from visual encoding and text generation to training strategies, used datasets,
and evaluation metrics. In this respect, we quantitatively compare many
relevant state-of-the-art approaches to identify the most impactful technical
innovations in image captioning architectures and training strategies.
Moreover, many variants of the problem and its open challenges are analyzed and
discussed. The final goal of this work is to serve as a tool for understanding
the existing state-of-the-art and highlighting the future directions for an
area of research where Computer Vision and Natural Language Processing can find
an optimal synergy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1"&gt;Matteo Stefanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1"&gt;Marcella Cornia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1"&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1"&gt;Silvia Cascianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1"&gt;Giuseppe Fiameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data. (arXiv:2105.04727v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04727</id>
        <link href="http://arxiv.org/abs/2105.04727"/>
        <updated>2021-07-16T00:48:23.256Z</updated>
        <summary type="html"><![CDATA[We propose FEDENHANCE, an unsupervised federated learning (FL) approach for
speech enhancement and separation with non-IID distributed data across multiple
clients. We simulate a real-world scenario where each client only has access to
a few noisy recordings from a limited and disjoint number of speakers (hence
non-IID). Each client trains their model in isolation using mixture invariant
training while periodically providing updates to a central server. Our
experiments show that our approach achieves competitive enhancement performance
compared to IID training on a single device and that we can further facilitate
the convergence speed and the overall performance using transfer learning on
the server-side. Moreover, we show that we can effectively combine updates from
clients trained locally with supervised and unsupervised losses. We also
release a new dataset LibriFSD50K and its creation recipe in order to
facilitate FL research for source separation problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1"&gt;Efthymios Tzinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casebeer_J/0/1/0/all/0/1"&gt;Jonah Casebeer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhepei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1"&gt;Paris Smaragdis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Passive attention in artificial neural networks predicts human visual selectivity. (arXiv:2107.07013v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07013</id>
        <link href="http://arxiv.org/abs/2107.07013"/>
        <updated>2021-07-16T00:48:23.237Z</updated>
        <summary type="html"><![CDATA[Developments in machine learning interpretability techniques over the past
decade have provided new tools to observe the image regions that are most
informative for classification and localization in artificial neural networks
(ANNs). Are the same regions similarly informative to human observers? Using
data from 78 new experiments and 6,610 participants, we show that passive
attention techniques reveal a significant overlap with human visual selectivity
estimates derived from 6 distinct behavioral tasks including visual
discrimination, spatial localization, recognizability, free-viewing,
cued-object search, and saliency search fixations. We find that input
visualizations derived from relatively simple ANN architectures probed using
guided backpropagation methods are the best predictors of a shared component in
the joint variability of the human measures. We validate these correlational
results with causal manipulations using recognition experiments. We show that
images masked with ANN attention maps were easier for humans to classify than
control masks in a speeded recognition experiment. Similarly, we find that
recognition performance in the same ANN models was likewise influenced by
masking input images using human visual selectivity maps. This work contributes
a new approach to evaluating the biological and psychological validity of
leading ANNs as models of human vision: by examining their similarities and
differences in terms of their visual selectivity to the information contained
in images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Langlois_T/0/1/0/all/0/1"&gt;Thomas A. Langlois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;H. Charles Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1"&gt;Erin Grant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1"&gt;Ishita Dasgupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1"&gt;Thomas L. Griffiths&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1"&gt;Nori Jacoby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07502</id>
        <link href="http://arxiv.org/abs/2107.07502"/>
        <updated>2021-07-16T00:48:23.230Z</updated>
        <summary type="html"><![CDATA[Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yiwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zetian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jason Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Leslie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peter Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Michelle A. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Learning-Based Approach for Improving Generalization Capability of Machine Reading Comprehension Systems. (arXiv:2107.00368v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00368</id>
        <link href="http://arxiv.org/abs/2107.00368"/>
        <updated>2021-07-16T00:48:23.222Z</updated>
        <summary type="html"><![CDATA[Machine Reading Comprehension (MRC) is an active field in natural language
processing with many successful developed models in recent years. Despite their
high in-distribution accuracy, these models suffer from two issues: high
training cost and low out-of-distribution accuracy. Even though some approaches
have been presented to tackle the generalization problem, they have high,
intolerable training costs. In this paper, we investigate the effect of
ensemble learning approach to improve generalization of MRC systems without
retraining a big model. After separately training the base models with
different structures on different datasets, they are ensembled using weighting
and stacking approaches in probabilistic and non-probabilistic settings. Three
configurations are investigated including heterogeneous, homogeneous, and
hybrid on eight datasets and six state-of-the-art models. We identify the
important factors in the effectiveness of ensemble methods. Also, we compare
the robustness of ensemble and fine-tuned models against data distribution
shifts. The experimental results show the effectiveness and robustness of the
ensemble approach in improving the out-of-distribution accuracy of MRC systems,
especially when the base models are similar in accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baradaran_R/0/1/0/all/0/1"&gt;Razieh Baradaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1"&gt;Hossein Amirkhani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark. (arXiv:2107.07498v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07498</id>
        <link href="http://arxiv.org/abs/2107.07498"/>
        <updated>2021-07-16T00:48:23.183Z</updated>
        <summary type="html"><![CDATA[Pretrained Language Models (PLMs) have achieved tremendous success in natural
language understanding tasks. While different learning schemes -- fine-tuning,
zero-shot and few-shot learning -- have been widely explored and compared for
languages such as English, there is comparatively little work in Chinese to
fairly and comprehensively evaluate and compare these methods. This work first
introduces Chinese Few-shot Learning Evaluation Benchmark (FewCLUE), the first
comprehensive small sample evaluation benchmark in Chinese. It includes nine
tasks, ranging from single-sentence and sentence-pair classification tasks to
machine reading comprehension tasks. Given the high variance of the few-shot
learning performance, we provide multiple training/validation sets to
facilitate a more accurate and stable evaluation of few-shot modeling. An
unlabeled training set with up to 20,000 additional samples per task is
provided, allowing researchers to explore better ways of using unlabeled
samples. Next, we implement a set of state-of-the-art (SOTA) few-shot learning
methods (including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their
performance with fine-tuning and zero-shot learning schemes on the newly
constructed FewCLUE benchmark.Our results show that: 1) all five few-shot
learning methods exhibit better performance than fine-tuning or zero-shot
learning; 2) among the five methods, PET is the best performing few-shot
method; 3) few-shot learning performance is highly dependent on the specific
task. Our benchmark and code are available at
https://github.com/CLUEbenchmark/FewCLUE]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaojing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1"&gt;Chenyang Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuanwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Hu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Huilin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1"&gt;Guoao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xiang Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hai Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Potential UAV Landing Sites Detection through Digital Elevation Models Analysis. (arXiv:2107.06921v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06921</id>
        <link href="http://arxiv.org/abs/2107.06921"/>
        <updated>2021-07-16T00:48:23.157Z</updated>
        <summary type="html"><![CDATA[In this paper, a simple technique for Unmanned Aerial Vehicles (UAVs)
potential landing site detection using terrain information through
identification of flat areas, is presented. The algorithm utilizes digital
elevation models (DEM) that represent the height distribution of an area. Flat
areas which constitute appropriate landing zones for UAVs in normal or
emergency situations result by thresholding the image gradient magnitude of the
digital surface model (DSM). The proposed technique also uses connected
components evaluation on the thresholded gradient image in order to discover
connected regions of sufficient size for landing. Moreover, man-made structures
and vegetation areas are detected and excluded from the potential landing
sites. Quantitative performance evaluation of the proposed landing site
detection algorithm in a number of areas on real world and synthetic datasets,
accompanied by a comparison with a state-of-the-art algorithm, proves its
efficiency and superiority.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kakaletsis_E/0/1/0/all/0/1"&gt;Efstratios Kakaletsis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolaidis_N/0/1/0/all/0/1"&gt;Nikos Nikolaidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Triage and diagnosis of COVID-19 from medical social media. (arXiv:2103.11850v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11850</id>
        <link href="http://arxiv.org/abs/2103.11850"/>
        <updated>2021-07-16T00:48:23.138Z</updated>
        <summary type="html"><![CDATA[Objective: This study aims to develop an end-to-end natural language
processing pipeline for triage and diagnosis of COVID-19 from patient-authored
social media posts, in order to provide researchers and other interested
parties with additional information on the symptoms, severity and prevalence of
the disease. Materials and Methods: The text processing pipeline first extracts
COVID-19 symptoms and related concepts such as severity, duration, negations,
and body parts from patients posts using conditional random fields. An
unsupervised rule-based algorithm is then applied to establish relations
between concepts in the next step of the pipeline. The extracted concepts and
relations are subsequently used to construct two different vector
representations of each post. These vectors are applied separately to build
support vector machine learning models to triage patients into three categories
and diagnose them for COVID-19. Results: We report that macro- and
micro-averaged F1 scores in the range of 71-96% and 61-87%, respectively, for
the triage and diagnosis of COVID-19, when the models are trained on human
labelled data. Our experimental results indicate that similar performance can
be achieved when the models are trained using predicted labels from concept
extraction and rule-based classifiers, thus yielding end-to-end machine
learning. Also, we highlight important features uncovered by our diagnostic
machine learning models and compare them with the most frequent symptoms
revealed in another COVID-19 dataset. In particular, we found that the most
important features are not always the most frequent ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1"&gt;Abul Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levene_M/0/1/0/all/0/1"&gt;Mark Levene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_D/0/1/0/all/0/1"&gt;David Weston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fromson_R/0/1/0/all/0/1"&gt;Renate Fromson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koslover_N/0/1/0/all/0/1"&gt;Nicolas Koslover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levene_T/0/1/0/all/0/1"&gt;Tamara Levene&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compute and memory efficient universal sound source separation. (arXiv:2103.02644v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02644</id>
        <link href="http://arxiv.org/abs/2103.02644"/>
        <updated>2021-07-16T00:48:23.079Z</updated>
        <summary type="html"><![CDATA[Recent progress in audio source separation lead by deep learning has enabled
many neural network models to provide robust solutions to this fundamental
estimation problem. In this study, we provide a family of efficient neural
network architectures for general purpose audio source separation while
focusing on multiple computational aspects that hinder the application of
neural networks in real-world scenarios. The backbone structure of this
convolutional network is the SUccessive DOwnsampling and Resampling of
Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is
performed through simple one-dimensional convolutions. This mechanism enables
our models to obtain high fidelity signal separation in a wide variety of
settings where variable number of sources are present and with limited
computational resources (e.g. floating point operations, memory footprint,
number of parameters and latency). Our experiments show that SuDoRM-RF models
perform comparably and even surpass several state-of-the-art benchmarks with
significantly higher computational resource requirements. The causal variation
of SuDoRM-RF is able to obtain competitive performance in real-time speech
separation of around 10dB scale-invariant signal-to-distortion ratio
improvement (SI-SDRi) while remaining up to 20 times faster than real-time on a
laptop device.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1"&gt;Efthymios Tzinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhepei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xilin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1"&gt;Paris Smaragdis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VT-SSum: A Benchmark Dataset for Video Transcript Segmentation and Summarization. (arXiv:2106.05606v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05606</id>
        <link href="http://arxiv.org/abs/2106.05606"/>
        <updated>2021-07-16T00:48:23.072Z</updated>
        <summary type="html"><![CDATA[Video transcript summarization is a fundamental task for video understanding.
Conventional approaches for transcript summarization are usually built upon the
summarization data for written language such as news articles, while the domain
discrepancy may degrade the model performance on spoken text. In this paper, we
present VT-SSum, a benchmark dataset with spoken language for video transcript
segmentation and summarization, which includes 125K transcript-summary pairs
from 9,616 videos. VT-SSum takes advantage of the videos from VideoLectures.NET
by leveraging the slides content as the weak supervision to generate the
extractive summary for video transcripts. Experiments with a state-of-the-art
deep learning approach show that the model trained with VT-SSum brings a
significant improvement on the AMI spoken text summarization benchmark. VT-SSum
is publicly available at https://github.com/Dod-o/VT-SSum to support the future
research of video transcript segmentation and summarization tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1"&gt;Tengchao Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Lei Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasilijevic_M/0/1/0/all/0/1"&gt;Momcilo Vasilijevic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills. (arXiv:2107.07261v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07261</id>
        <link href="http://arxiv.org/abs/2107.07261"/>
        <updated>2021-07-16T00:48:22.991Z</updated>
        <summary type="html"><![CDATA[Models pre-trained with a language modeling objective possess ample world
knowledge and language skills, but are known to struggle in tasks that require
reasoning. In this work, we propose to leverage semi-structured tables, and
automatically generate at scale question-paragraph pairs, where answering the
question requires reasoning over multiple facts in the paragraph. We add a
pre-training step over this synthetic data, which includes examples that
require 16 different reasoning skills such as number comparison, conjunction,
and fact composition. To improve data efficiency, we propose sampling
strategies that focus training on reasoning skills the model is currently
lacking. We evaluate our approach on three reading comprehension datasets that
are focused on reasoning, and show that our model, PReasM, substantially
outperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling
examples based on current model errors leads to faster training and higher
overall performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1"&gt;Ori Yoran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talmor_A/0/1/0/all/0/1"&gt;Alon Talmor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1"&gt;Jonathan Berant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tailor: Generating and Perturbing Text with Semantic Controls. (arXiv:2107.07150v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07150</id>
        <link href="http://arxiv.org/abs/2107.07150"/>
        <updated>2021-07-16T00:48:22.906Z</updated>
        <summary type="html"><![CDATA[Making controlled perturbations is essential for various tasks (e.g., data
augmentation), but building task-specific generators can be expensive. We
introduce Tailor, a task-agnostic generation system that perturbs text in a
semantically-controlled way. With unlikelihood training, we design Tailor's
generator to follow a series of control codes derived from semantic roles.
Through modifications of these control codes, Tailor can produce fine-grained
perturbations. We implement a set of operations on control codes that can be
composed into complex perturbation strategies, and demonstrate their
effectiveness in three distinct applications: First, Tailor facilitates the
construction of high-quality contrast sets that are lexically diverse, and less
biased than original task test data. Second, paired with automated labeling
heuristics, Tailor helps improve model generalization through data
augmentation: We obtain an average gain of 1.73 on an NLI challenge set by
perturbing just 5% of training data. Third, without any finetuning overhead,
Tailor's perturbations effectively improve compositionality in fine-grained
style transfer, outperforming fine-tuned baselines on 6 transfers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1"&gt;Alexis Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tongshuang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1"&gt;Matthew E. Peters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1"&gt;Matt Gardner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLEX: Unifying Evaluation for Few-Shot NLP. (arXiv:2107.07170v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07170</id>
        <link href="http://arxiv.org/abs/2107.07170"/>
        <updated>2021-07-16T00:48:22.899Z</updated>
        <summary type="html"><![CDATA[Few-shot NLP research is highly active, yet conducted in disjoint research
threads with evaluation suites that lack challenging-yet-realistic testing
setups and fail to employ careful experimental design. Consequently, the
community does not know which techniques perform best or even if they
outperform simple baselines. We formulate desiderata for an ideal few-shot NLP
benchmark and present FLEX, the first benchmark, public leaderboard, and
framework that provides unified, comprehensive measurement for few-shot NLP
techniques. FLEX incorporates and introduces new best practices for few-shot
evaluation, including measurement of four transfer settings, textual labels for
zero-shot evaluation, and a principled approach to benchmark design that
optimizes statistical accuracy while keeping evaluation costs accessible to
researchers without large compute resources. In addition, we present UniFew, a
simple yet strong prompt-based model for few-shot learning which unifies the
pretraining and finetuning prompt formats, eschewing complex machinery of
recent prompt-based approaches in adapting downstream task formats to language
model pretraining objectives. We demonstrate that despite simplicity UniFew
achieves results competitive with both popular meta-learning and prompt-based
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1"&gt;Jonathan Bragg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1"&gt;Arman Cohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1"&gt;Kyle Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1"&gt;Iz Beltagy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning for Text Classification with Multi-source Noise Simulation and Hard Example Mining. (arXiv:2107.07113v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07113</id>
        <link href="http://arxiv.org/abs/2107.07113"/>
        <updated>2021-07-16T00:48:22.816Z</updated>
        <summary type="html"><![CDATA[Many real-world applications involve the use of Optical Character Recognition
(OCR) engines to transform handwritten images into transcripts on which
downstream Natural Language Processing (NLP) models are applied. In this
process, OCR engines may introduce errors and inputs to downstream NLP models
become noisy. Despite that pre-trained models achieve state-of-the-art
performance in many NLP benchmarks, we prove that they are not robust to noisy
texts generated by real OCR engines. This greatly limits the application of NLP
models in real-world scenarios. In order to improve model performance on noisy
OCR transcripts, it is natural to train the NLP model on labelled noisy texts.
However, in most cases there are only labelled clean texts. Since there is no
handwritten pictures corresponding to the text, it is impossible to directly
use the recognition model to obtain noisy labelled data. Human resources can be
employed to copy texts and take pictures, but it is extremely expensive
considering the size of data for model training. Consequently, we are
interested in making NLP models intrinsically robust to OCR errors in a low
resource manner. We propose a novel robust training framework which 1) employs
simple but effective methods to directly simulate natural OCR noises from clean
texts and 2) iteratively mines the hard examples from a large number of
simulated samples for optimal performance. 3) To make our model learn
noise-invariant representations, a stability loss is employed. Experiments on
three real-world datasets show that the proposed framework boosts the
robustness of pre-trained models by a large margin. We believe that this work
can greatly promote the application of NLP models in actual scenarios, although
the algorithm we use is simple and straightforward. We make our codes and three
datasets publicly
available\footnote{https://github.com/tal-ai/Robust-learning-MSSHEM}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guowei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenbiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1"&gt;Weiping Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zitao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features. (arXiv:2107.06963v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06963</id>
        <link href="http://arxiv.org/abs/2107.06963"/>
        <updated>2021-07-16T00:48:22.800Z</updated>
        <summary type="html"><![CDATA[Knowledge-grounded dialogue systems are intended to convey information that
is based on evidence provided in a given source text. We discuss the challenges
of training a generative neural dialogue model for such systems that is
controlled to stay faithful to the evidence. Existing datasets contain a mix of
conversational responses that are faithful to selected evidence as well as more
subjective or chit-chat style responses. We propose different evaluation
measures to disentangle these different styles of responses by quantifying the
informativeness and objectivity. At training time, additional inputs based on
these evaluation measures are given to the dialogue model. At generation time,
these additional inputs act as stylistic controls that encourage the model to
generate responses that are faithful to the provided evidence. We also
investigate the usage of additional controls at decoding time using resampling
techniques. In addition to automatic metrics, we perform a human evaluation
study where raters judge the output of these controlled generation models to be
generally more objective and faithful to the evidence compared to baseline
dialogue systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rashkin_H/0/1/0/all/0/1"&gt;Hannah Rashkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1"&gt;David Reitter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomar_G/0/1/0/all/0/1"&gt;Gaurav Singh Tomar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Dipanjan Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoBERT-Zero: Evolving BERT Backbone from Scratch. (arXiv:2107.07445v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07445</id>
        <link href="http://arxiv.org/abs/2107.07445"/>
        <updated>2021-07-16T00:48:22.784Z</updated>
        <summary type="html"><![CDATA[Transformer-based pre-trained language models like BERT and its variants have
recently achieved promising performance in various natural language processing
(NLP) tasks. However, the conventional paradigm constructs the backbone by
purely stacking the manually designed global self-attention layers, introducing
inductive bias and thus leading to sub-optimal. In this work, we propose an
Operation-Priority Neural Architecture Search (OP-NAS) algorithm to
automatically search for promising hybrid backbone architectures. Our
well-designed search space (i) contains primitive math operations in the
intra-layer level to explore novel attention structures, and (ii) leverages
convolution blocks to be the supplementary for attention structure in the
inter-layer level to better learn local dependency. We optimize both the search
algorithm and evaluation of candidate models to boost the efficiency of our
proposed OP-NAS. Specifically, we propose Operation-Priority (OP) evolution
strategy to facilitate model search via balancing exploration and exploitation.
Furthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for
fast model evaluation. Extensive experiments show that the searched
architecture (named AutoBERT-Zero) significantly outperforms BERT and its
variants of different model capacities in various downstream tasks, proving the
architecture's transfer and generalization abilities. Remarkably,
AutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and
BERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE
test set. Code and pre-trained models will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jiahui Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+shi_H/0/1/0/all/0/1"&gt;Han shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip L.H. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLSRIL-23: Cross Lingual Speech Representations for Indic Languages. (arXiv:2107.07402v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07402</id>
        <link href="http://arxiv.org/abs/2107.07402"/>
        <updated>2021-07-16T00:48:22.760Z</updated>
        <summary type="html"><![CDATA[We present a CLSRIL-23, a self supervised learning based audio pre-trained
model which learns cross lingual speech representations from raw audio across
23 Indic languages. It is built on top of wav2vec 2.0 which is solved by
training a contrastive task over masked latent speech representations and
jointly learns the quantization of latents shared across all languages. We
compare the language wise loss during pretraining to compare effects of
monolingual and multilingual pretraining. Performance on some downstream
fine-tuning tasks for speech recognition is also compared and our experiments
show that multilingual pretraining outperforms monolingual training, in terms
of learning speech representations which encodes phonetic similarity of
languages and also in terms of performance on down stream tasks. A decrease of
5% is observed in WER and 9.5% in CER when a multilingual pretrained model is
used for finetuning in Hindi. All the code models are also open sourced.
CLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio
data to facilitate research in speech recognition for Indic languages. We hope
that new state of the art systems will be created using the self supervised
approach, especially for low resources Indic languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Anirudh Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1"&gt;Harveen Singh Chadha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1"&gt;Priyanshi Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chimmwal_N/0/1/0/all/0/1"&gt;Neeraj Chimmwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1"&gt;Ankur Dhuriya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1"&gt;Rishabh Gaur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1"&gt;Vivek Raghavan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spanish Language Models. (arXiv:2107.07253v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07253</id>
        <link href="http://arxiv.org/abs/2107.07253"/>
        <updated>2021-07-16T00:48:22.751Z</updated>
        <summary type="html"><![CDATA[This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as
well as the corresponding performance evaluations. Both models were pre-trained
using the largest Spanish corpus known to date, with a total of 570GB of clean
and deduplicated text processed for this work, compiled from the web crawlings
performed by the National Library of Spain from 2009 to 2019.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1"&gt;Asier Guti&amp;#xe9;rrez-Fandi&amp;#xf1;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1"&gt;Jordi Armengol-Estap&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1"&gt;Marc P&amp;#xe0;mies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1"&gt;Joan Llop-Palao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silveira_Ocampo_J/0/1/0/all/0/1"&gt;Joaqu&amp;#xed;n Silveira-Ocampo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1"&gt;Casimiro Pio Carrino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1"&gt;Aitor Gonzalez-Agirre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1"&gt;Carme Armentano-Oller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1"&gt;Carlos Rodriguez-Penagos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1"&gt;Marta Villegas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing. (arXiv:2107.06990v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06990</id>
        <link href="http://arxiv.org/abs/2107.06990"/>
        <updated>2021-07-16T00:48:22.730Z</updated>
        <summary type="html"><![CDATA[Automated writing evaluation systems can improve students' writing insofar as
students attend to the feedback provided and revise their essay drafts in ways
aligned with such feedback. Existing research on revision of argumentative
writing in such systems, however, has focused on the types of revisions
students make (e.g., surface vs. content) rather than the extent to which
revisions actually respond to the feedback provided and improve the essay. We
introduce an annotation scheme to capture the nature of sentence-level
revisions of evidence use and reasoning (the `RER' scheme) and apply it to 5th-
and 6th-grade students' argumentative essays. We show that reliable manual
annotation can be achieved and that revision annotations correlate with a
holistic assessment of essay improvement in line with the feedback provided.
Furthermore, we explore the feasibility of automatically classifying revisions
according to our scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Afrin_T/0/1/0/all/0/1"&gt;Tazin Afrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Elaine Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1"&gt;Diane Litman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsumura_L/0/1/0/all/0/1"&gt;Lindsay C. Matsumura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Correnti_R/0/1/0/all/0/1"&gt;Richard Correnti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Task Learning based Online Dialogic Instruction Detection with Pre-trained Language Models. (arXiv:2107.07119v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07119</id>
        <link href="http://arxiv.org/abs/2107.07119"/>
        <updated>2021-07-16T00:48:22.722Z</updated>
        <summary type="html"><![CDATA[In this work, we study computational approaches to detect online dialogic
instructions, which are widely used to help students understand learning
materials, and build effective study habits. This task is rather challenging
due to the widely-varying quality and pedagogical styles of dialogic
instructions. To address these challenges, we utilize pre-trained language
models, and propose a multi-task paradigm which enhances the ability to
distinguish instances of different classes by enlarging the margin between
categories via contrastive loss. Furthermore, we design a strategy to fully
exploit the misclassified examples during the training stage. Extensive
experiments on a real-world online educational data set demonstrate that our
approach achieves superior performance compared to representative baselines. To
encourage reproducible results, we make our implementation online available at
\url{https://github.com/AIED2021/multitask-dialogic-instruction}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1"&gt;Yang Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenbiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luckin_R/0/1/0/all/0/1"&gt;Rose Luckin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zitao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task. (arXiv:2107.06959v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06959</id>
        <link href="http://arxiv.org/abs/2107.06959"/>
        <updated>2021-07-16T00:48:22.698Z</updated>
        <summary type="html"><![CDATA[In this paper, we describe our end-to-end multilingual speech translation
system submitted to the IWSLT 2021 evaluation campaign on the Multilingual
Speech Translation shared task. Our system is built by leveraging transfer
learning across modalities, tasks and languages. First, we leverage
general-purpose multilingual modules pretrained with large amounts of
unlabelled and labelled data. We further enable knowledge transfer from the
text task to the speech task by training two tasks jointly. Finally, our
multilingual model is finetuned on speech translation task-specific data to
achieve the best translation results. Experimental results show our system
outperforms the reported systems, including both end-to-end and cascaded based
approaches, by a large margin.

In some translation directions, our speech translation results evaluated on
the public Multilingual TEDx test set are even comparable with the ones from a
strong text-to-text translation system, which uses the oracle speech
transcripts as input.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yun Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1"&gt;Hongyu Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1"&gt;Juan Pino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1"&gt;Holger Schwenk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1"&gt;Naman Goyal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wordcraft: a Human-AI Collaborative Editor for Story Writing. (arXiv:2107.07430v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07430</id>
        <link href="http://arxiv.org/abs/2107.07430"/>
        <updated>2021-07-16T00:48:22.690Z</updated>
        <summary type="html"><![CDATA[As neural language models grow in effectiveness, they are increasingly being
applied in real-world settings. However these applications tend to be limited
in the modes of interaction they support. In this extended abstract, we propose
Wordcraft, an AI-assisted editor for story writing in which a writer and a
dialog system collaborate to write a story. Our novel interface uses few-shot
learning and the natural affordances of conversation to support a variety of
interactions. Our editor provides a sandbox for writers to probe the boundaries
of transformer-based language models and paves the way for future
human-in-the-loop training pipelines and novel evaluation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1"&gt;Andy Coenen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Luke Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1"&gt;Daphne Ippolito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1"&gt;Emily Reif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1"&gt;Ann Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Security in McAdams Coefficient-Based Speaker Anonymization by Watermarking Method. (arXiv:2107.07223v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.07223</id>
        <link href="http://arxiv.org/abs/2107.07223"/>
        <updated>2021-07-16T00:48:22.664Z</updated>
        <summary type="html"><![CDATA[Speaker anonymization aims to suppress speaker individuality to protect
privacy in speech while preserving the other aspects, such as speech content.
One effective solution for anonymization is to modify the McAdams coefficient.
In this work, we propose a method to improve the security for speaker
anonymization based on the McAdams coefficient by using a speech watermarking
approach. The proposed method consists of two main processes: one for embedding
and one for detection. In embedding process, two different McAdams coefficients
represent binary bits ``0" and ``1". The watermarked speech is then obtained by
frame-by-frame bit inverse switching. Subsequently, the detection process is
carried out by a power spectrum comparison. We conducted objective evaluations
with reference to the VoicePrivacy 2020 Challenge (VP2020) and of the speech
watermarking with reference to the Information Hiding Challenge (IHC) and found
that our method could satisfy the blind detection, inaudibility, and robustness
requirements in watermarking. It also significantly improved the anonymization
performance in comparison to the secondary baseline system in VP2020.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mawalim_C/0/1/0/all/0/1"&gt;Candy Olivia Mawalim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unoki_M/0/1/0/all/0/1"&gt;Masashi Unoki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HTLM: Hyper-Text Pre-Training and Prompting of Language Models. (arXiv:2107.06955v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06955</id>
        <link href="http://arxiv.org/abs/2107.06955"/>
        <updated>2021-07-16T00:48:22.621Z</updated>
        <summary type="html"><![CDATA[We introduce HTLM, a hyper-text language model trained on a large-scale web
crawl. Modeling hyper-text has a number of advantages: (1) it is easily
gathered at scale, (2) it provides rich document-level and end-task-adjacent
supervision (e.g. class and id attributes often encode document category
information), and (3) it allows for new structured prompting that follows the
established semantics of HTML (e.g. to do zero-shot summarization by infilling
title tags for a webpage that contains the input text). We show that
pretraining with a BART-style denoising loss directly on simplified HTML
provides highly effective transfer for a wide range of end tasks and
supervision levels. HTLM matches or exceeds the performance of comparably sized
text-only LMs for zero-shot prompting and fine-tuning for classification
benchmarks, while also setting new state-of-the-art performance levels for
zero-shot summarization. We also find that hyper-text prompts provide more
value to HTLM, in terms of data efficiency, than plain text prompts do for
existing LMs, and that HTLM is highly effective at auto-prompting itself, by
simply generating the most likely hyper-text formatting for any available
training data. We will release all code and models to support future HTLM
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1"&gt;Armen Aghajanyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1"&gt;Dmytro Okhonko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Mike Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1"&gt;Mandar Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1"&gt;Gargi Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving ESL Sentence Completion Questions via Pre-trained Neural Language Models. (arXiv:2107.07122v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07122</id>
        <link href="http://arxiv.org/abs/2107.07122"/>
        <updated>2021-07-16T00:48:22.592Z</updated>
        <summary type="html"><![CDATA[Sentence completion (SC) questions present a sentence with one or more blanks
that need to be filled in, three to five possible words or phrases as options.
SC questions are widely used for students learning English as a Second Language
(ESL) and building computational approaches to automatically solve such
questions is beneficial to language learners. In this work, we propose a neural
framework to solve SC questions in English examinations by utilizing
pre-trained language models. We conduct extensive experiments on a real-world
K-12 ESL SC question dataset and the results demonstrate the superiority of our
model in terms of prediction accuracy. Furthermore, we run precision-recall
trade-off analysis to discuss the practical issues when deploying it in
real-life scenarios. To encourage reproducible results, we make our code
publicly available at \url{https://github.com/AIED2021/ESL-SentenceCompletion}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiongqiong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jiafu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1"&gt;Qiang Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenbiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Feng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zitao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07502</id>
        <link href="http://arxiv.org/abs/2107.07502"/>
        <updated>2021-07-16T00:48:22.536Z</updated>
        <summary type="html"><![CDATA[Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yiwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zetian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jason Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Leslie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peter Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Michelle A. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Transmission Scheme and Coded Content Placement in Cluster-centric UAV-aided Cellular Networks. (arXiv:2101.11787v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11787</id>
        <link href="http://arxiv.org/abs/2101.11787"/>
        <updated>2021-07-16T00:48:22.525Z</updated>
        <summary type="html"><![CDATA[Recently, as a consequence of the COVID-19 pandemic, dependence on
telecommunication for remote working and telemedicine has significantly
increased. In cellular networks, incorporation of Unmanned Aerial Vehicles
(UAVs) can result in enhanced connectivity for outdoor users due to the high
probability of establishing Line of Sight (LoS) links. The UAV's limited
battery life and its signal attenuation in indoor areas, however, make it
inefficient to manage users' requests in indoor environments. Referred to as
the Cluster centric and Coded UAV-aided Femtocaching (CCUF) framework, the
network's coverage in both indoor and outdoor environments increases via a
two-phase clustering for FAPs' formation and UAVs' deployment. First objective
is to increase the content diversity. In this context, we propose a coded
content placement in a cluster-centric cellular network, which is integrated
with the Coordinated Multi-Point (CoMP) to mitigate the inter-cell interference
in edge areas. Then, we compute, experimentally, the number of coded contents
to be stored in each caching node to increase the cache-hit ratio,
Signal-to-Interference-plus-Noise Ratio (SINR), and cache diversity and
decrease the users' access delay and cache redundancy for different content
popularity profiles. Capitalizing on clustering, our second objective is to
assign the best caching node to indoor/outdoor users for managing their
requests. In this regard, we define the movement speed of ground users as the
decision metric of the transmission scheme for serving outdoor users' requests
to avoid frequent handovers between FAPs and increase the battery life of UAVs.
Simulation results illustrate that the proposed CCUF implementation increases
the cache hit-ratio, SINR, and cache diversity and decrease the users' access
delay, cache redundancy and UAVs' energy consumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+HajiAkhondi_Meybodi_Z/0/1/0/all/0/1"&gt;Zohreh HajiAkhondi-Meybodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1"&gt;Arash Mohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abouei_J/0/1/0/all/0/1"&gt;Jamshid Abouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_M/0/1/0/all/0/1"&gt;Ming Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1"&gt;Konstantinos N. Plataniotis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06912</id>
        <link href="http://arxiv.org/abs/2107.06912"/>
        <updated>2021-07-16T00:48:22.510Z</updated>
        <summary type="html"><![CDATA[Connecting Vision and Language plays an essential role in Generative
Intelligence. For this reason, in the last few years, a large research effort
has been devoted to image captioning, i.e. the task of describing images with
syntactically and semantically meaningful sentences. Starting from 2015 the
task has generally been addressed with pipelines composed of a visual encoding
step and a language model for text generation. During these years, both
components have evolved considerably through the exploitation of object
regions, attributes, and relationships and the introduction of multi-modal
connections, fully-attentive approaches, and BERT-like early-fusion strategies.
However, regardless of the impressive results obtained, research in image
captioning has not reached a conclusive answer yet. This work aims at providing
a comprehensive overview and categorization of image captioning approaches,
from visual encoding and text generation to training strategies, used datasets,
and evaluation metrics. In this respect, we quantitatively compare many
relevant state-of-the-art approaches to identify the most impactful technical
innovations in image captioning architectures and training strategies.
Moreover, many variants of the problem and its open challenges are analyzed and
discussed. The final goal of this work is to serve as a tool for understanding
the existing state-of-the-art and highlighting the future directions for an
area of research where Computer Vision and Natural Language Processing can find
an optimal synergy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1"&gt;Matteo Stefanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1"&gt;Marcella Cornia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1"&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1"&gt;Silvia Cascianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1"&gt;Giuseppe Fiameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transition-based Bubble Parsing: Improvements on Coordination Structure Prediction. (arXiv:2107.06905v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06905</id>
        <link href="http://arxiv.org/abs/2107.06905"/>
        <updated>2021-07-16T00:48:22.501Z</updated>
        <summary type="html"><![CDATA[We propose a transition-based bubble parser to perform coordination structure
identification and dependency-based syntactic analysis simultaneously. Bubble
representations were proposed in the formal linguistics literature decades ago;
they enhance dependency trees by encoding coordination boundaries and internal
relationships within coordination structures explicitly. In this paper, we
introduce a transition system and neural models for parsing these
bubble-enhanced structures. Experimental results on the English Penn Treebank
and the English GENIA corpus show that our parsers beat previous
state-of-the-art approaches on the task of coordination structure prediction,
especially for the subset of sentences with complex coordination structures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1"&gt;Tianze Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1"&gt;Lillian Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TGIF: Tree-Graph Integrated-Format Parser for Enhanced UD with Two-Stage Generic- to Individual-Language Finetuning. (arXiv:2107.06907v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06907</id>
        <link href="http://arxiv.org/abs/2107.06907"/>
        <updated>2021-07-16T00:48:22.340Z</updated>
        <summary type="html"><![CDATA[We present our contribution to the IWPT 2021 shared task on parsing into
enhanced Universal Dependencies. Our main system component is a hybrid
tree-graph parser that integrates (a) predictions of spanning trees for the
enhanced graphs with (b) additional graph edges not present in the spanning
trees. We also adopt a finetuning strategy where we first train a
language-generic parser on the concatenation of data from all available
languages, and then, in a second step, finetune on each individual language
separately. Additionally, we develop our own complete set of pre-processing
modules relevant to the shared task, including tokenization, sentence
segmentation, and multiword token expansion, based on pre-trained XLM-R models
and our own pre-training of character-level language models. Our submission
reaches a macro-average ELAS of 89.24 on the test set. It ranks top among all
teams, with a margin of more than 2 absolute ELAS over the next best-performing
submission, and best score on 16 out of 17 languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1"&gt;Tianze Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1"&gt;Lillian Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-modal Variational Auto-encoder for Content-based Micro-video Background Music Recommendation. (arXiv:2107.07268v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.07268</id>
        <link href="http://arxiv.org/abs/2107.07268"/>
        <updated>2021-07-16T00:48:22.242Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for
content-based micro-video background music recommendation. CMVAE is a
hierarchical Bayesian generative model that matches relevant background music
to a micro-video by projecting these two multimodal inputs into a shared
low-dimensional latent space, where the alignment of two corresponding
embeddings of a matched video-music pair is achieved by cross-generation.
Moreover, the multimodal information is fused by the product-of-experts (PoE)
principle, where the semantic information in visual and textual modalities of
the micro-video are weighted according to their variance estimations such that
the modality with a lower noise level is given more weights. Therefore, the
micro-video latent variables contain less irrelevant information that results
in a more robust model generalization. Furthermore, we establish a large-scale
content-based micro-video background music recommendation dataset, TT-150k,
composed of approximately 3,000 different background music clips associated to
150,000 micro-videos from different users. Extensive experiments on the
established TT-150k dataset demonstrate the effectiveness of the proposed
method. A qualitative assessment of CMVAE by visualizing some recommendation
results is also included.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jing Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yaochen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiayi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenzhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-modal Variational Auto-encoder for Content-based Micro-video Background Music Recommendation. (arXiv:2107.07268v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.07268</id>
        <link href="http://arxiv.org/abs/2107.07268"/>
        <updated>2021-07-16T00:48:22.212Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for
content-based micro-video background music recommendation. CMVAE is a
hierarchical Bayesian generative model that matches relevant background music
to a micro-video by projecting these two multimodal inputs into a shared
low-dimensional latent space, where the alignment of two corresponding
embeddings of a matched video-music pair is achieved by cross-generation.
Moreover, the multimodal information is fused by the product-of-experts (PoE)
principle, where the semantic information in visual and textual modalities of
the micro-video are weighted according to their variance estimations such that
the modality with a lower noise level is given more weights. Therefore, the
micro-video latent variables contain less irrelevant information that results
in a more robust model generalization. Furthermore, we establish a large-scale
content-based micro-video background music recommendation dataset, TT-150k,
composed of approximately 3,000 different background music clips associated to
150,000 micro-videos from different users. Extensive experiments on the
established TT-150k dataset demonstrate the effectiveness of the proposed
method. A qualitative assessment of CMVAE by visualizing some recommendation
results is also included.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jing Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yaochen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiayi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenzhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommending best course of treatment based on similarities of prognostic markers\thanks{All authors contributed equally. (arXiv:2107.07500v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07500</id>
        <link href="http://arxiv.org/abs/2107.07500"/>
        <updated>2021-07-16T00:48:22.196Z</updated>
        <summary type="html"><![CDATA[With the advancement in the technology sector spanning over every field, a
huge influx of information is inevitable. Among all the opportunities that the
advancements in the technology have brought, one of them is to propose
efficient solutions for data retrieval. This means that from an enormous pile
of data, the retrieval methods should allow the users to fetch the relevant and
recent data over time. In the field of entertainment and e-commerce,
recommender systems have been functioning to provide the aforementioned.
Employing the same systems in the medical domain could definitely prove to be
useful in variety of ways. Following this context, the goal of this paper is to
propose collaborative filtering based recommender system in the healthcare
sector to recommend remedies based on the symptoms experienced by the patients.
Furthermore, a new dataset is developed consisting of remedies concerning
various diseases to address the limited availability of the data. The proposed
recommender system accepts the prognostic markers of a patient as the input and
generates the best remedy course. With several experimental trials, the
proposed model achieved promising results in recommending the possible remedy
for given prognostic markers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sudhanshu/0/1/0/all/0/1"&gt;Sudhanshu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-detecting groups based on textual similarity for group recommendations. (arXiv:2107.07284v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07284</id>
        <link href="http://arxiv.org/abs/2107.07284"/>
        <updated>2021-07-16T00:48:22.160Z</updated>
        <summary type="html"><![CDATA[In general, recommender systems are designed to provide personalized items to
a user. But in few cases, items are recommended for a group, and the challenge
is to aggregate the individual user preferences to infer the recommendation to
a group. It is also important to consider the similarity of characteristics
among the members of a group to generate a better recommendation. Members of an
automatically identified group will have similar characteristics, and reaching
a consensus with a decision-making process is preferable in this case. It
requires users-items and their rating interactions over a utility matrix to
auto-detect the groups in group recommendations. We may not overlook other
intrinsic information to form a group. The textual information also plays a
pivotal role in user clustering. In this paper, we auto-detect the groups based
on the textual similarity of the metadata (review texts). We consider the order
in user preferences in our models. We have conducted extensive experiments over
two real-world datasets to check the efficacy of the proposed models. We have
also conducted a competitive comparison with a baseline model to show the
improvements in the quality of recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_C/0/1/0/all/0/1"&gt;Chintoo Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdary_C/0/1/0/all/0/1"&gt;C. Ravindranath Chowdary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Next-item Recommendations in Short Sessions. (arXiv:2107.07453v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07453</id>
        <link href="http://arxiv.org/abs/2107.07453"/>
        <updated>2021-07-16T00:48:22.148Z</updated>
        <summary type="html"><![CDATA[The changing preferences of users towards items trigger the emergence of
session-based recommender systems (SBRSs), which aim to model the dynamic
preferences of users for next-item recommendations. However, most of the
existing studies on SBRSs are based on long sessions only for recommendations,
ignoring short sessions, though short sessions, in fact, account for a large
proportion in most of the real-world datasets. As a result, the applicability
of existing SBRSs solutions is greatly reduced. In a short session, quite
limited contextual information is available, making the next-item
recommendation very challenging. To this end, in this paper, inspired by the
success of few-shot learning (FSL) in effectively learning a model with limited
instances, we formulate the next-item recommendation as an FSL problem.
Accordingly, following the basic idea of a representative approach for FSL,
i.e., meta-learning, we devise an effective SBRS called INter-SEssion
collaborative Recommender netTwork (INSERT) for next-item recommendations in
short sessions. With the carefully devised local module and global module,
INSERT is able to learn an optimal preference representation of the current
user in a given short session. In particular, in the global module, a similar
session retrieval network (SSRN) is designed to find out the sessions similar
to the current short session from the historical sessions of both the current
user and other users, respectively. The obtained similar sessions are then
utilized to complement and optimize the preference representation learned from
the current short session by the local module for more accurate next-item
recommendations in this short session. Extensive experiments conducted on two
real-world datasets demonstrate the superiority of our proposed INSERT over the
state-of-the-art SBRSs when making next-item recommendations in short sessions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1"&gt;Wenzhuo Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shoujin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shengsheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuSaver: Neural Adaptive Power Consumption Optimization for Mobile Video Streaming. (arXiv:2107.07127v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07127</id>
        <link href="http://arxiv.org/abs/2107.07127"/>
        <updated>2021-07-16T00:48:22.137Z</updated>
        <summary type="html"><![CDATA[Video streaming services strive to support high-quality videos at higher
resolutions and frame rates to improve the quality of experience (QoE).
However, high-quality videos consume considerable amounts of energy on mobile
devices. This paper proposes NeuSaver, which reduces the power consumption of
mobile devices when streaming videos by applying an adaptive frame rate to each
video chunk without compromising user experience. NeuSaver generates an optimal
policy that determines the appropriate frame rate for each video chunk using
reinforcement learning (RL). The RL model automatically learns the policy that
maximizes the QoE goals based on previous observations. NeuSaver also uses an
asynchronous advantage actor-critic algorithm to reinforce the RL model quickly
and robustly. Streaming servers that support NeuSaver preprocesses videos into
segments with various frame rates, which is similar to the process of creating
videos with multiple bit rates in dynamic adaptive streaming over HTTP.
NeuSaver utilizes the commonly used H.264 video codec. We evaluated NeuSaver in
various experiments and a user study through four video categories along with
the state-of-the-art model. Our experiments showed that NeuSaver effectively
reduces the power consumption of mobile devices when streaming video by an
average of 16.14% and up to 23.12% while achieving high QoE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyoungjun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Myungchul Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1"&gt;Laihyuk Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scene-adaptive Knowledge Distillation for Sequential Recommendation via Differentiable Architecture Search. (arXiv:2107.07173v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07173</id>
        <link href="http://arxiv.org/abs/2107.07173"/>
        <updated>2021-07-16T00:48:22.123Z</updated>
        <summary type="html"><![CDATA[Sequential recommender systems (SRS) have become a research hotspot due to
its power in modeling user dynamic interests and sequential behavioral
patterns. To maximize model expressive ability, a default choice is to apply a
larger and deeper network architecture, which, however, often brings high
network latency when generating online recommendations. Naturally, we argue
that compressing the heavy recommendation models into middle- or light- weight
neural networks is of great importance for practical production systems. To
realize such a goal, we propose AdaRec, a knowledge distillation (KD) framework
which compresses knowledge of a teacher model into a student model adaptively
according to its recommendation scene by using differentiable Neural
Architecture Search (NAS). Specifically, we introduce a target-oriented
distillation loss to guide the structure search process for finding the student
network architecture, and a cost-sensitive loss as constraints for model size,
which achieves a superior trade-off between recommendation effectiveness and
efficiency. In addition, we leverage Earth Mover's Distance (EMD) to realize
many-to-many layer mapping during knowledge distillation, which enables each
intermediate student layer to learn from other intermediate teacher layers
adaptively. Extensive experiments on real-world recommendation datasets
demonstrate that our model achieves competitive or better accuracy with notable
inference speedup comparing to strong counterparts, while discovering diverse
neural architectures for sequential recommender models under different
recommendation scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1"&gt;Fajie Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaxi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Min Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengming Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sketching sounds: an exploratory study on sound-shape associations. (arXiv:2107.07360v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.07360</id>
        <link href="http://arxiv.org/abs/2107.07360"/>
        <updated>2021-07-16T00:48:22.090Z</updated>
        <summary type="html"><![CDATA[Sound synthesiser controls typically correspond to technical parameters of
signal processing algorithms rather than intuitive sound descriptors that
relate to human perception of sound. This makes it difficult to realise sound
ideas in a straightforward way. Cross-modal mappings, for example between
gestures and sound, have been suggested as a more intuitive control mechanism.
A large body of research shows consistency in human associations between sounds
and shapes. However, the use of drawings to drive sound synthesis has not been
explored to its full extent. This paper presents an exploratory study that
asked participants to sketch visual imagery of sounds with a monochromatic
digital drawing interface, with the aim to identify different representational
approaches and determine whether timbral sound characteristics can be
communicated reliably through visual sketches. Results imply that the
development of a synthesiser exploiting sound-shape associations is feasible,
but a larger and more focused dataset is needed in followup studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lobbers_S/0/1/0/all/0/1"&gt;Sebastian L&amp;#xf6;bbers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1"&gt;Mathieu Barthet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1"&gt;Gy&amp;#xf6;rgy Fazekas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04170</id>
        <link href="http://arxiv.org/abs/2105.04170"/>
        <updated>2021-07-16T00:48:22.054Z</updated>
        <summary type="html"><![CDATA[Recommender systems rely on user behavior data like ratings and clicks to
build personalization model. However, the collected data is observational
rather than experimental, causing various biases in the data which
significantly affect the learned model. Most existing work for recommendation
debiasing, such as the inverse propensity scoring and imputation approaches,
focuses on one or two specific biases, lacking the universal capacity that can
account for mixed or even unknown biases in the data. Towards this research
gap, we first analyze the origin of biases from the perspective of \textit{risk
discrepancy} that represents the difference between the expectation empirical
risk and the true risk. Remarkably, we derive a general learning framework that
well summarizes most existing debiasing strategies by specifying some
parameters of the general framework. This provides a valuable opportunity to
develop a universal solution for debiasing, e.g., by learning the debiasing
parameters from data. However, the training data lacks important signal of how
the data is biased and what the unbiased data looks like. To move this idea
forward, we propose \textit{AotoDebias} that leverages another (small) set of
uniform data to optimize the debiasing parameters by solving the bi-level
optimization problem with meta-learning. Through theoretical analyses, we
derive the generalization bound for AutoDebias and prove its ability to acquire
the appropriate debiasing strategy. Extensive experiments on two real datasets
and a simulated dataset demonstrated effectiveness of AutoDebias. The code is
available at \url{https://github.com/DongHande/AutoDebias}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hande Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1"&gt;Xin Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guli Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Keping Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning for Recommendations at Grubhub. (arXiv:2107.07106v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07106</id>
        <link href="http://arxiv.org/abs/2107.07106"/>
        <updated>2021-07-16T00:48:21.994Z</updated>
        <summary type="html"><![CDATA[We propose a method to easily modify existing offline Recommender Systems to
run online using Transfer Learning. Online Learning for Recommender Systems has
two main advantages: quality and scale. Like many Machine Learning algorithms
in production if not regularly retrained will suffer from Concept Drift. A
policy that is updated frequently online can adapt to drift faster than a batch
system. This is especially true for user-interaction systems like recommenders
where the underlying distribution can shift drastically to follow user
behaviour. As a platform grows rapidly like Grubhub, the cost of running batch
training jobs becomes material. A shift from stateless batch learning offline
to stateful incremental learning online can recover, for example, at Grubhub,
up to a 45x cost savings and a +20% metrics increase. There are a few
challenges to overcome with the transition to online stateful learning, namely
convergence, non-stationary embeddings and off-policy evaluation, which we
explore from our experiences running this system in production.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Egg_A/0/1/0/all/0/1"&gt;Alex Egg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-optimal inference in adaptive linear regression. (arXiv:2107.02266v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02266</id>
        <link href="http://arxiv.org/abs/2107.02266"/>
        <updated>2021-07-15T01:59:05.080Z</updated>
        <summary type="html"><![CDATA[When data is collected in an adaptive manner, even simple methods like
ordinary least squares can exhibit non-normal asymptotic behavior. As an
undesirable consequence, hypothesis tests and confidence intervals based on
asymptotic normality can lead to erroneous results. We propose an online
debiasing estimator to correct these distributional anomalies in least squares
estimation. Our proposed method takes advantage of the covariance structure
present in the dataset and provides sharper estimates in directions for which
more information has accrued. We establish an asymptotic normality property for
our proposed online debiasing estimator under mild conditions on the data
collection process, and provide asymptotically exact confidence intervals. We
additionally prove a minimax lower bound for the adaptive linear regression
problem, thereby providing a baseline by which to compare estimators. There are
various conditions under which our proposed estimator achieves the minimax
lower bound up to logarithmic factors. We demonstrate the usefulness of our
theory via applications to multi-armed bandit, autoregressive time series
estimation, and active learning with exploration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Khamaru_K/0/1/0/all/0/1"&gt;Koulik Khamaru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Deshpande_Y/0/1/0/all/0/1"&gt;Yash Deshpande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mackey_L/0/1/0/all/0/1"&gt;Lester Mackey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wainwright_M/0/1/0/all/0/1"&gt;Martin J. Wainwright&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors. (arXiv:2009.13370v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13370</id>
        <link href="http://arxiv.org/abs/2009.13370"/>
        <updated>2021-07-15T01:59:04.915Z</updated>
        <summary type="html"><![CDATA[This paper estimates free energy, average mutual information, and minimum
mean square error (MMSE) of a linear model under two assumptions: (1) the
source is generated by a Markov chain, (2) the source is generated via a hidden
Markov model. Our estimates are based on the replica method in statistical
physics. We show that under the posterior mean estimator, the linear model with
Markov sources or hidden Markov sources is decoupled into single-input AWGN
channels with state information available at both encoder and decoder where the
state distribution follows the left Perron-Frobenius eigenvector with unit
Manhattan norm of the stochastic matrix of Markov chains. Numerical results
show that the free energies and MSEs obtained via the replica method closely
approximate to their counterparts achieved by the Metropolis-Hastings algorithm
or some well-known approximate message passing algorithms in the research
literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1"&gt;Lan V. Truong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08038</id>
        <link href="http://arxiv.org/abs/2106.08038"/>
        <updated>2021-07-15T01:59:04.856Z</updated>
        <summary type="html"><![CDATA[Averaging predictions over a set of models -- an ensemble -- is widely used
to improve predictive performance and uncertainty estimation of deep learning
models. At the same time, many machine learning systems, such as search,
matching, and recommendation systems, heavily rely on embeddings.
Unfortunately, due to misalignment of features of independently trained models,
embeddings, cannot be improved with a naive deep ensemble like approach. In
this work, we look at the ensembling of representations and propose mean
embeddings with test-time augmentation (MeTTA) simple yet well-performing
recipe for ensembling representations. Empirically we demonstrate that MeTTA
significantly boosts the quality of linear evaluation on ImageNet for both
supervised and self-supervised models. Even more exciting, we draw connections
between MeTTA, image retrieval, and transformation invariant models. We believe
that spreading the success of ensembles to inference higher-quality
representations is the important step that will open many new applications of
ensembling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1"&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1"&gt;Andrei Atanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning in weakly nonlinear systems: A Case study on Significant wave heights. (arXiv:2105.08583v3 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08583</id>
        <link href="http://arxiv.org/abs/2105.08583"/>
        <updated>2021-07-15T01:59:04.842Z</updated>
        <summary type="html"><![CDATA[This paper proposes a machine learning method based on the Extra Trees (ET)
algorithm for forecasting Significant Wave Heights in oceanic waters. To derive
multiple features from the CDIP buoys, which make point measurements, we first
nowcast various parameters and then forecast them at 30-min intervals. The
proposed algorithm has Scatter Index (SI), Bias, Correlation Coefficient, Root
Mean Squared Error (RMSE) of 0.130, -0.002, 0.97, and 0.14, respectively, for
one day ahead prediction and 0.110, -0.001, 0.98, and 0.122, respectively, for
14-day ahead prediction on the testing dataset. While other state-of-the-art
methods can only forecast up to 120 hours ahead, we extend it further to 14
days. Our proposed setup includes spectral features, hv-block cross-validation,
and stringent QC criteria. The proposed algorithm performs significantly better
than the state-of-the-art methods commonly used for significant wave height
forecasting for one-day ahead prediction. Moreover, the improved performance of
the proposed machine learning method compared to the numerical methods shows
that this performance can be extended to even longer periods allowing for early
prediction of significant wave heights in oceanic waters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1"&gt;Pujan Pokhrel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving the Kolmogorov PDE by means of deep learning. (arXiv:1806.00421v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1806.00421</id>
        <link href="http://arxiv.org/abs/1806.00421"/>
        <updated>2021-07-15T01:59:04.827Z</updated>
        <summary type="html"><![CDATA[Stochastic differential equations (SDEs) and the Kolmogorov partial
differential equations (PDEs) associated to them have been widely used in
models from engineering, finance, and the natural sciences. In particular, SDEs
and Kolmogorov PDEs, respectively, are highly employed in models for the
approximative pricing of financial derivatives. Kolmogorov PDEs and SDEs,
respectively, can typically not be solved explicitly and it has been and still
is an active topic of research to design and analyze numerical methods which
are able to approximately solve Kolmogorov PDEs and SDEs, respectively. Nearly
all approximation methods for Kolmogorov PDEs in the literature suffer under
the curse of dimensionality or only provide approximations of the solution of
the PDE at a single fixed space-time point. In this paper we derive and propose
a numerical approximation method which aims to overcome both of the above
mentioned drawbacks and intends to deliver a numerical approximation of the
Kolmogorov PDE on an entire region $[a,b]^d$ without suffering from the curse
of dimensionality. Numerical results on examples including the heat equation,
the Black-Scholes model, the stochastic Lorenz equation, and the Heston model
suggest that the proposed approximation algorithm is quite effective in high
dimensions in terms of both accuracy and speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Beck_C/0/1/0/all/0/1"&gt;Christian Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Becker_S/0/1/0/all/0/1"&gt;Sebastian Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Grohs_P/0/1/0/all/0/1"&gt;Philipp Grohs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jaafari_N/0/1/0/all/0/1"&gt;Nor Jaafari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jentzen_A/0/1/0/all/0/1"&gt;Arnulf Jentzen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational graph convolutional networks for predicting blood-brain barrier penetration of drug molecules. (arXiv:2107.06773v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2107.06773</id>
        <link href="http://arxiv.org/abs/2107.06773"/>
        <updated>2021-07-15T01:59:04.821Z</updated>
        <summary type="html"><![CDATA[The evaluation of the BBB penetrating ability of drug molecules is a critical
step in brain drug development. Computational prediction based on machine
learning has proved to be an efficient way to conduct the evaluation. However,
performance of the established models has been limited by their incapability of
dealing with the interactions between drugs and proteins, which play an
important role in the mechanism behind BBB penetrating behaviors. To address
this issue, we employed the relational graph convolutional network (RGCN) to
handle the drug-protein (denoted by the encoding gene) relations as well as the
features of each individual drug. In addition, drug-drug similarity was also
introduced to connect structurally similar drugs in the graph. The RGCN model
was initially trained without input of any drug features. And the performance
was already promising, demonstrating the significant role of the
drug-protein/drug-drug relations in the prediction of BBB permeability.
Moreover, molecular embeddings from a pre-trained knowledge graph were used as
the drug features to further enhance the predictive ability of the model.
Finally, the best performing RGCN model was built with a large number of
unlabeled drugs integrated into the graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaoqian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yejin Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Designing Machine Learning Pipeline Toolkit for AutoML Surrogate Modeling Optimization. (arXiv:2107.01253v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01253</id>
        <link href="http://arxiv.org/abs/2107.01253"/>
        <updated>2021-07-15T01:59:04.805Z</updated>
        <summary type="html"><![CDATA[The pipeline optimization problem in machine learning requires simultaneous
optimization of pipeline structures and parameter adaptation of their elements.
Having an elegant way to express these structures can help lessen the
complexity in the management and analysis of their performances together with
the different choices of optimization strategies. With these issues in mind, we
created the AutoMLPipeline (AMLP) toolkit which facilitates the creation and
evaluation of complex machine learning pipeline structures using simple
expressions. We use AMLP to find optimal pipeline signatures, datamine them,
and use these datamined features to speed-up learning and prediction. We
formulated a two-stage pipeline optimization with surrogate modeling in AMLP
which outperforms other AutoML approaches with a 4-hour time budget in less
than 5 minutes of AMLP computation time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palmes_P/0/1/0/all/0/1"&gt;Paulito P. Palmes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kishimoto_A/0/1/0/all/0/1"&gt;Akihiro Kishimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marinescu_R/0/1/0/all/0/1"&gt;Radu Marinescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1"&gt;Parikshit Ram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daly_E/0/1/0/all/0/1"&gt;Elizabeth Daly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DIT4BEARs Smart Roads Internship. (arXiv:2107.06755v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06755</id>
        <link href="http://arxiv.org/abs/2107.06755"/>
        <updated>2021-07-15T01:59:04.797Z</updated>
        <summary type="html"><![CDATA[The research internship at UiT - The Arctic University of Norway was offered
for our team being the winner of the 'Smart Roads - Winter Road Maintenance
2021' Hackathon. The internship commenced on 3 May 2021 and ended on 21 May
2021 with meetings happening twice each week. In spite of having different
nationalities and educational backgrounds, we both interns tried to collaborate
as a team as much as possible. The most alluring part was working on this
project made us realize the critical conditions faced by the arctic people,
where it was hard to gain such a unique experience from our residence. We
developed and implemented several deep learning models to classify the states
(dry, moist, wet, icy, snowy, slushy). Depending upon the best model, the
weather forecast app will predict the state taking the Ta, Tsurf, Height,
Speed, Water, etc. into consideration. The crucial part was to define a safety
metric which is the product of the accident rates based on friction and the
accident rates based on states. We developed a regressor that will predict the
safety metric depending upon the state obtained from the classifier and the
friction obtained from the sensor data. A pathfinding algorithm has been
designed using the sensor data, open street map data, weather data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jahin_M/0/1/0/all/0/1"&gt;Md. Abrar Jahin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krutsylo_A/0/1/0/all/0/1"&gt;Andrii Krutsylo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations. (arXiv:2003.08938v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08938</id>
        <link href="http://arxiv.org/abs/2003.08938"/>
        <updated>2021-07-15T01:59:04.765Z</updated>
        <summary type="html"><![CDATA[A deep reinforcement learning (DRL) agent observes its states through
observations, which may contain natural measurement errors or adversarial
noises. Since the observations deviate from the true states, they can mislead
the agent into making suboptimal actions. Several works have shown this
vulnerability via adversarial attacks, but existing approaches on improving the
robustness of DRL under this setting have limited success and lack for
theoretical principles. We show that naively applying existing techniques on
improving robustness for classification tasks, like adversarial training, is
ineffective for many RL tasks. We propose the state-adversarial Markov decision
process (SA-MDP) to study the fundamental properties of this problem, and
develop a theoretically principled policy regularization which can be applied
to a large family of DRL algorithms, including proximal policy optimization
(PPO), deep deterministic policy gradient (DDPG) and deep Q networks (DQN), for
both discrete and continuous action control problems. We significantly improve
the robustness of PPO, DDPG and DQN agents under a suite of strong white box
adversarial attacks, including new attacks of our own. Additionally, we find
that a robust policy noticeably improves DRL performance even without an
adversary in a number of environments. Our code is available at
https://github.com/chenhongge/StateAdvDRL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hongge Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boning_D/0/1/0/all/0/1"&gt;Duane Boning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Autoencoder-based Error-bounded Compression for Scientific Data. (arXiv:2105.11730v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11730</id>
        <link href="http://arxiv.org/abs/2105.11730"/>
        <updated>2021-07-15T01:59:04.751Z</updated>
        <summary type="html"><![CDATA[Error-bounded lossy compression is becoming an indispensable technique for
the success of today's scientific projects with vast volumes of data produced
during the simulations or instrument data acquisitions. Not only can it
significantly reduce data size, but it also can control the compression errors
based on user-specified error bounds. Autoencoder (AE) models have been widely
used in image compression, but few AE-based compression approaches support
error-bounding features, which are highly required by scientific applications.
To address this issue, we explore using convolutional autoencoders to improve
error-bounded lossy compression for scientific data, with the following three
key contributions. (1) We provide an in-depth investigation of the
characteristics of various autoencoder models and develop an error-bounded
autoencoder-based framework in terms of the SZ model. (2) We optimize the
compression quality for main stages in our designed AE-based error-bounded
compression framework, fine-tuning the block sizes and latent sizes and also
optimizing the compression efficiency of latent vectors. (3) We evaluate our
proposed solution using five real-world scientific datasets and comparing them
with six other related works. Experiments show that our solution exhibits a
very competitive compression quality from among all the compressors in our
tests. In absolute terms, it can obtain a much better compression quality (100%
~ 800% improvement in compression ratio with the same data distortion) compared
with SZ2.1 and ZFP in cases with a high compression ratio.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Di_S/0/1/0/all/0/1"&gt;Sheng Di&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1"&gt;Sian Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dingwen Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zizhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cappello_F/0/1/0/all/0/1"&gt;Franck Cappello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Physics-Informed Deep Learning Paradigm for Car-Following Models. (arXiv:2012.13376v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13376</id>
        <link href="http://arxiv.org/abs/2012.13376"/>
        <updated>2021-07-15T01:59:04.732Z</updated>
        <summary type="html"><![CDATA[Car-following behavior has been extensively studied using physics-based
models, such as the Intelligent Driver Model. These models successfully
interpret traffic phenomena observed in the real-world but may not fully
capture the complex cognitive process of driving. Deep learning models, on the
other hand, have demonstrated their power in capturing observed traffic
phenomena but require a large amount of driving data to train. This paper aims
to develop a family of neural network based car-following models that are
informed by physics-based models, which leverage the advantage of both
physics-based (being data-efficient and interpretable) and deep learning based
(being generalizable) models. We design physics-informed deep learning
car-following (PIDL-CF) architectures encoded with two popular physics-based
models - IDM and OVM, on which acceleration is predicted for four traffic
regimes: acceleration, deceleration, cruising, and emergency braking. Two types
of PIDL-CFM problems are studied, one to predict acceleration only and the
other to jointly predict acceleration and discover model parameters. We also
demonstrate the superior performance of PIDL with the Next Generation
SIMulation (NGSIM) dataset over baselines, especially when the training data is
sparse. The results demonstrate the superior performance of neural networks
informed by physics over those without. The developed PIDL-CF framework holds
the potential for system identification of driving models and for the
development of driving-based controls for automated vehicles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Z/0/1/0/all/0/1"&gt;Zhaobin Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1"&gt;Xuan Di&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1"&gt;Rongye Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale News Classification using BERT Language Model: Spark NLP Approach. (arXiv:2107.06785v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06785</id>
        <link href="http://arxiv.org/abs/2107.06785"/>
        <updated>2021-07-15T01:59:04.696Z</updated>
        <summary type="html"><![CDATA[The rise of big data analytics on top of NLP increases the computational
burden for text processing at scale. The problems faced in NLP are very high
dimensional text, so it takes a high computation resource. The MapReduce allows
parallelization of large computations and can improve the efficiency of text
processing. This research aims to study the effect of big data processing on
NLP tasks based on a deep learning approach. We classify a big text of news
topics with fine-tuning BERT used pre-trained models. Five pre-trained models
with a different number of parameters were used in this study. To measure the
efficiency of this method, we compared the performance of the BERT with the
pipelines from Spark NLP. The result shows that BERT without Spark NLP gives
higher accuracy compared to BERT with Spark NLP. The accuracy average and
training time of all models using BERT is 0.9187 and 35 minutes while using
BERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will
take more computation resources and need a longer time to complete the tasks.
However, the accuracy of BERT with Spark NLP only decreased by an average of
5.7%, while the training time was reduced significantly by 62.9% compared to
BERT without Spark NLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1"&gt;Kuncahyo Setyo Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1"&gt;Kuncahyo Setyo Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1"&gt;Novanto Yudistira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Optimization of Deep CNN for Image Denoising Using LSTM. (arXiv:2107.06845v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06845</id>
        <link href="http://arxiv.org/abs/2107.06845"/>
        <updated>2021-07-15T01:59:04.688Z</updated>
        <summary type="html"><![CDATA[The recent application of deep learning (DL) to various tasks has seen the
performance of classical techniques surpassed by their DL-based counterparts.
As a result, DL has equally seen application in the removal of noise from
images. In particular, the use of deep feed-forward convolutional neural
networks (DnCNNs) has been investigated for denoising. It utilizes advances in
DL techniques such as deep architecture, residual learning, and batch
normalization to achieve better denoising performance when compared with the
other classical state-of-the-art denoising algorithms. However, its deep
architecture resulted in a huge set of trainable parameters. Meta-optimization
is a training approach of enabling algorithms to learn to train themselves by
themselves. Training algorithms using meta-optimizers have been shown to enable
algorithms to achieve better performance when compared to the classical
gradient descent-based training approach. In this work, we investigate the
application of the meta-optimization training approach to the DnCNN denoising
algorithm to enhance its denoising capability. Our preliminary experiments on
simpler algorithms reveal the prospects of utilizing the meta-optimization
training approach towards the enhancement of the DnCNN denoising capability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1"&gt;Basit O. Alawode&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alfarraj_M/0/1/0/all/0/1"&gt;Motaz Alfarraj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient, Simple and Automated Negative Sampling for Knowledge Graph Embedding. (arXiv:2010.14227v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14227</id>
        <link href="http://arxiv.org/abs/2010.14227"/>
        <updated>2021-07-15T01:59:04.682Z</updated>
        <summary type="html"><![CDATA[Negative sampling, which samples negative triplets from non-observed ones in
knowledge graph (KG), is an essential step in KG embedding. Recently,
generative adversarial network (GAN), has been introduced in negative sampling.
By sampling negative triplets with large gradients, these methods avoid the
problem of vanishing gradient and thus obtain better performance. However, they
make the original model more complex and harder to train. In this paper,
motivated by the observation that negative triplets with large gradients are
important but rare, we propose to directly keep track of them with the cache.
In this way, our method acts as a "distilled" version of previous GAN-based
methods, which does not waste training time on additional parameters to fit the
full distribution of negative triplets. However, how to sample from and update
the cache are two critical questions. We propose to solve these issues by
automated machine learning techniques. The automated version also covers
GAN-based methods as special cases. Theoretical explanation of NSCaching is
also provided, justifying the superior over fixed sampling scheme. Besides, we
further extend NSCaching with skip-gram model for graph embedding. Finally,
extensive experiments show that our method can gain significant improvements on
various KG embedding models and the skip-gram model, and outperforms the
state-of-the-art negative sampling methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1"&gt;Quanming Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lei Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Do You Want Your Greedy: Simultaneous or Repeated?. (arXiv:2009.13998v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13998</id>
        <link href="http://arxiv.org/abs/2009.13998"/>
        <updated>2021-07-15T01:59:04.656Z</updated>
        <summary type="html"><![CDATA[We present SimultaneousGreedys, a deterministic algorithm for constrained
submodular maximization. At a high level, the algorithm maintains $\ell$
solutions and greedily updates them in a simultaneous fashion.
SimultaneousGreedys achieves the tightest known approximation guarantees for
both $k$-extendible systems and the more general $k$-systems, which are
$(k+1)^2/k = k + \mathcal{O}(1)$ and $(1 + \sqrt{k+2})^2 = k +
\mathcal{O}(\sqrt{k})$, respectively. This is in contrast to previous
algorithms, which are designed to provide tight approximation guarantees in one
setting, but not both. We also improve the analysis of RepeatedGreedy, showing
that it achieves an approximation ratio of $k + \mathcal{O}(\sqrt{k})$ for
$k$-systems when allowed to run for $\mathcal{O}(\sqrt{k})$ iterations, an
improvement in both the runtime and approximation over previous analyses. We
demonstrate that both algorithms may be modified to run in nearly linear time
with an arbitrarily small loss in the approximation.

Both SimultaneousGreedys and RepeatedGreedy are flexible enough to
incorporate the intersection of $m$ additional knapsack constraints, while
retaining similar approximation guarantees: both algorithms yield an
approximation guarantee of roughly $k + 2m + \mathcal{O}(\sqrt{k+m})$ for
$k$-systems and SimultaneousGreedys enjoys an improved approximation guarantee
of $k+2m + \mathcal{O}(\sqrt{m})$ for $k$-extendible systems. To complement our
algorithmic contributions, we provide a hardness result which states that no
algorithm making polynomially many oracle queries can achieve an approximation
better than $k + 1/2 + \varepsilon$. We also present SubmodularGreedy.jl, a
Julia package which implements these algorithms and may be downloaded at
https://github.com/crharshaw/SubmodularGreedy.jl . Finally, we test the
effectiveness of these algorithms on real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_M/0/1/0/all/0/1"&gt;Moran Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harshaw_C/0/1/0/all/0/1"&gt;Christopher Harshaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1"&gt;Amin Karbasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Sorting Networks for Scalable Sorting and Ranking Supervision. (arXiv:2105.04019v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04019</id>
        <link href="http://arxiv.org/abs/2105.04019"/>
        <updated>2021-07-15T01:59:04.650Z</updated>
        <summary type="html"><![CDATA[Sorting and ranking supervision is a method for training neural networks
end-to-end based on ordering constraints. That is, the ground truth order of
sets of samples is known, while their absolute values remain unsupervised. For
that, we propose differentiable sorting networks by relaxing their pairwise
conditional swap operations. To address the problems of vanishing gradients and
extensive blurring that arise with larger numbers of layers, we propose mapping
activations to regions with moderate gradients. We consider odd-even as well as
bitonic sorting networks, which outperform existing relaxations of the sorting
operation. We show that bitonic sorting networks can achieve stable training on
large input sets of up to 1024 elements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1"&gt;Felix Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgelt_C/0/1/0/all/0/1"&gt;Christian Borgelt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1"&gt;Hilde Kuehne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deussen_O/0/1/0/all/0/1"&gt;Oliver Deussen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Structural Causal Model for MR Images of Multiple Sclerosis. (arXiv:2103.03158v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03158</id>
        <link href="http://arxiv.org/abs/2103.03158"/>
        <updated>2021-07-15T01:59:04.644Z</updated>
        <summary type="html"><![CDATA[Precision medicine involves answering counterfactual questions such as "Would
this patient respond better to treatment A or treatment B?" These types of
questions are causal in nature and require the tools of causal inference to be
answered, e.g., with a structural causal model (SCM). In this work, we develop
an SCM that models the interaction between demographic information, disease
covariates, and magnetic resonance (MR) images of the brain for people with
multiple sclerosis. Inference in the SCM generates counterfactual images that
show what an MR image of the brain would look like if demographic or disease
covariates are changed. These images can be used for modeling disease
progression or used for image processing tasks where controlling for
confounders is necessary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reinhold_J/0/1/0/all/0/1"&gt;Jacob C. Reinhold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carass_A/0/1/0/all/0/1"&gt;Aaron Carass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1"&gt;Jerry L. Prince&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core-set Sampling for Efficient Neural Architecture Search. (arXiv:2107.06869v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06869</id>
        <link href="http://arxiv.org/abs/2107.06869"/>
        <updated>2021-07-15T01:59:04.638Z</updated>
        <summary type="html"><![CDATA[Neural architecture search (NAS), an important branch of automatic machine
learning, has become an effective approach to automate the design of deep
learning models. However, the major issue in NAS is how to reduce the large
search time imposed by the heavy computational burden. While most recent
approaches focus on pruning redundant sets or developing new search
methodologies, this paper attempts to formulate the problem based on the data
curation manner. Our key strategy is to search the architecture using
summarized data distribution, i.e., core-set. Typically, many NAS algorithms
separate searching and training stages, and the proposed core-set methodology
is only used in search stage, thus their performance degradation can be
minimized. In our experiments, we were able to save overall computational time
from 30.8 hours to 3.5 hours, 8.8x reduction, on a single RTX 3090 GPU without
sacrificing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shim_J/0/1/0/all/0/1"&gt;Jae-hun Shim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1"&gt;Kyeongbo Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;Suk-Ju Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interaction-Grounded Learning. (arXiv:2106.04887v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04887</id>
        <link href="http://arxiv.org/abs/2106.04887"/>
        <updated>2021-07-15T01:59:04.623Z</updated>
        <summary type="html"><![CDATA[Consider a prosthetic arm, learning to adapt to its user's control signals.
We propose Interaction-Grounded Learning for this novel setting, in which a
learner's goal is to interact with the environment with no grounding or
explicit reward to optimize its policies. Such a problem evades common RL
solutions which require an explicit reward. The learning agent observes a
multidimensional context vector, takes an action, and then observes a
multidimensional feedback vector. This multidimensional feedback vector has no
explicit reward information. In order to succeed, the algorithm must learn how
to evaluate the feedback vector to discover a latent reward signal, with which
it can ground its policies without supervision. We show that in an
Interaction-Grounded Learning setting, with certain natural assumptions, a
learner can discover the latent reward and ground its policy for successful
interaction. We provide theoretical guarantees and a proof-of-concept empirical
evaluation to demonstrate the effectiveness of our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tengyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1"&gt;John Langford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1"&gt;Paul Mineiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1"&gt;Ida Momennejad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel approach for modelling and classifying sit-to-stand kinematics using inertial sensors. (arXiv:2107.06859v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06859</id>
        <link href="http://arxiv.org/abs/2107.06859"/>
        <updated>2021-07-15T01:59:04.617Z</updated>
        <summary type="html"><![CDATA[Sit-to-stand transitions are an important part of activities of daily living
and play a key role in functional mobility in humans. The sit-to-stand movement
is often affected in older adults due to frailty and in patients with motor
impairments such as Parkinson's disease leading to falls. Studying kinematics
of sit-to-stand transitions can provide insight in assessment, monitoring and
developing rehabilitation strategies for the affected populations. We propose a
three-segment body model for estimating sit-to-stand kinematics using only two
wearable inertial sensors, placed on the shank and back. Reducing the number of
sensors to two instead of one per body segment facilitates monitoring and
classifying movements over extended periods, making it more comfortable to wear
while reducing the power requirements of sensors. We applied this model on 10
younger healthy adults (YH), 12 older healthy adults (OH) and 12 people with
Parkinson's disease (PwP). We have achieved this by incorporating unique
sit-to-stand classification technique using unsupervised learning in the model
based reconstruction of angular kinematics using extended Kalman filter. Our
proposed model showed that it was possible to successfully estimate thigh
kinematics despite not measuring the thigh motion with inertial sensor. We
classified sit-to-stand transitions, sitting and standing states with the
accuracies of 98.67%, 94.20% and 91.41% for YH, OH and PwP respectively. We
have proposed a novel integrated approach of modelling and classification for
estimating the body kinematics during sit-to-stand motion and successfully
applied it on YH, OH and PwP groups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wairagkar_M/0/1/0/all/0/1"&gt;Maitreyee Wairagkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villeneuve_E/0/1/0/all/0/1"&gt;Emma Villeneuve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_R/0/1/0/all/0/1"&gt;Rachel King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janko_B/0/1/0/all/0/1"&gt;Balazs Janko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnett_M/0/1/0/all/0/1"&gt;Malcolm Burnett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashburn_A/0/1/0/all/0/1"&gt;Ann Ashburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1"&gt;Veena Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sherratt_R/0/1/0/all/0/1"&gt;R. Simon Sherratt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holderbaum_W/0/1/0/all/0/1"&gt;William Holderbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harwin_W/0/1/0/all/0/1"&gt;William Harwin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Model and Data Driven Algorithm for Online Learning of Any-to-Any Path Loss Maps. (arXiv:2107.06677v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.06677</id>
        <link href="http://arxiv.org/abs/2107.06677"/>
        <updated>2021-07-15T01:59:04.609Z</updated>
        <summary type="html"><![CDATA[Learning any-to-any (A2A) path loss maps, where the objective is the
reconstruction of path loss between any two given points in a map, might be a
key enabler for many applications that rely on device-to-device (D2D)
communication. Such applications include machine-type communications (MTC) or
vehicle-to-vehicle (V2V) communications. Current approaches for learning A2A
maps are either model-based methods, or pure data-driven methods. Model-based
methods have the advantage that they can generate reliable estimations with low
computational complexity, but they cannot exploit information coming from data.
Pure data-driven methods can achieve good performance without assuming any
physical model, but their complexity and their lack of robustness is not
acceptable for many applications. In this paper, we propose a novel hybrid
model and data-driven approach that fuses information obtained from datasets
and models in an online fashion. To that end, we leverage the framework of
stochastic learning to deal with the sequential arrival of samples and propose
an online algorithm that alternatively and sequentially minimizes the original
non-convex problem. A proof of convergence is presented, along with experiments
based firstly on synthetic data, and secondly on a more realistic dataset for
V2X, with both experiments showing promising results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gutierrez_Estevez_M/0/1/0/all/0/1"&gt;M. A. Gutierrez-Estevez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kasparick_M/0/1/0/all/0/1"&gt;Martin Kasparick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cavalvante_R/0/1/0/all/0/1"&gt;Renato L. G. Cavalvante&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stanczak_S/0/1/0/all/0/1"&gt;S&amp;#x142;awomir Sta&amp;#x144;czak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition. (arXiv:2104.00120v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00120</id>
        <link href="http://arxiv.org/abs/2104.00120"/>
        <updated>2021-07-15T01:59:04.603Z</updated>
        <summary type="html"><![CDATA[Stream fusion, also known as system combination, is a common technique in
automatic speech recognition for traditional hybrid hidden Markov model
approaches, yet mostly unexplored for modern deep neural network end-to-end
model architectures. Here, we investigate various fusion techniques for the
all-attention-based encoder-decoder architecture known as the transformer,
striving to achieve optimal fusion by investigating different fusion levels in
an example single-microphone setting with fusion of standard magnitude and
phase features. We introduce a novel multi-encoder learning method that
performs a weighted combination of two encoder-decoder multi-head attention
outputs only during training. Employing then only the magnitude feature encoder
in inference, we are able to show consistent improvement on Wall Street Journal
(WSJ) with language model and on Librispeech, without increase in runtime or
parameters. Combining two such multi-encoder trained models by a simple late
fusion in inference, we achieve state-of-the-art performance for
transformer-based models on WSJ with a significant WER reduction of 19%
relative compared to the current benchmark approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lohrenz_T/0/1/0/all/0/1"&gt;Timo Lohrenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhengyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fingscheidt_T/0/1/0/all/0/1"&gt;Tim Fingscheidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissecting Supervised Contrastive Learning. (arXiv:2102.08817v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08817</id>
        <link href="http://arxiv.org/abs/2102.08817"/>
        <updated>2021-07-15T01:59:04.598Z</updated>
        <summary type="html"><![CDATA[Minimizing cross-entropy over the softmax scores of a linear map composed
with a high-capacity encoder is arguably the most popular choice for training
neural networks on supervised learning tasks. However, recent works show that
one can directly optimize the encoder instead, to obtain equally (or even more)
discriminative representations via a supervised variant of a contrastive
objective. In this work, we address the question whether there are fundamental
differences in the sought-for representation geometry in the output space of
the encoder at minimal loss. Specifically, we prove, under mild assumptions,
that both losses attain their minimum once the representations of each class
collapse to the vertices of a regular simplex, inscribed in a hypersphere. We
provide empirical evidence that this configuration is attained in practice and
that reaching a close-to-optimal state typically indicates good generalization
performance. Yet, the two losses show remarkably different optimization
behavior. The number of iterations required to perfectly fit to data scales
superlinearly with the amount of randomly flipped labels for the supervised
contrastive loss. This is in contrast to the approximately linear scaling
previously reported for networks trained with cross-entropy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Graf_F/0/1/0/all/0/1"&gt;Florian Graf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hofer_C/0/1/0/all/0/1"&gt;Christoph D. Hofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Niethammer_M/0/1/0/all/0/1"&gt;Marc Niethammer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kwitt_R/0/1/0/all/0/1"&gt;Roland Kwitt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clusterability as an Alternative to Anchor Points When Learning with Noisy Labels. (arXiv:2102.05291v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05291</id>
        <link href="http://arxiv.org/abs/2102.05291"/>
        <updated>2021-07-15T01:59:04.592Z</updated>
        <summary type="html"><![CDATA[The label noise transition matrix, characterizing the probabilities of a
training instance being wrongly annotated, is crucial to designing popular
solutions to learning with noisy labels. Existing works heavily rely on finding
"anchor points" or their approximates, defined as instances belonging to a
particular class almost surely. Nonetheless, finding anchor points remains a
non-trivial task, and the estimation accuracy is also often throttled by the
number of available anchor points. In this paper, we propose an alternative
option to the above task. Our main contribution is the discovery of an
efficient estimation procedure based on a clusterability condition. We prove
that with clusterable representations of features, using up to third-order
consensuses of noisy labels among neighbor representations is sufficient to
estimate a unique transition matrix. Compared with methods using anchor points,
our approach uses substantially more instances and benefits from a much better
sample complexity. We demonstrate the estimation accuracy and advantages of our
estimates using both synthetic noisy labels (on CIFAR-10/100) and real
human-level noisy labels (on Clothing1M and our self-collected human-annotated
CIFAR-10). Our code and human-level noisy CIFAR-10 labels are available at
https://github.com/UCSC-REAL/HOC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhaowei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yiwen Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differential-Critic GAN: Generating What You Want by a Cue of Preferences. (arXiv:2107.06700v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06700</id>
        <link href="http://arxiv.org/abs/2107.06700"/>
        <updated>2021-07-15T01:59:04.558Z</updated>
        <summary type="html"><![CDATA[This paper proposes Differential-Critic Generative Adversarial Network
(DiCGAN) to learn the distribution of user-desired data when only partial
instead of the entire dataset possesses the desired property, which generates
desired data that meets user's expectations and can assist in designing
biological products with desired properties. Existing approaches select the
desired samples first and train regular GANs on the selected samples to derive
the user-desired data distribution. However, the selection of the desired data
relies on an expert criterion and supervision over the entire dataset. DiCGAN
introduces a differential critic that can learn the preference direction from
the pairwise preferences, which is amateur knowledge and can be defined on part
of the training data. The resultant critic guides the generation of the desired
data instead of the whole data. Specifically, apart from the Wasserstein GAN
loss, a ranking loss of the pairwise preferences is defined over the critic. It
endows the difference of critic values between each pair of samples with the
pairwise preference relation. The higher critic value indicates that the sample
is preferred by the user. Thus training the generative model for higher critic
values encourages the generation of user-preferred samples. Extensive
experiments show that our DiCGAN achieves state-of-the-art performance in
learning the user-desired data distributions, especially in the cases of
insufficient desired data and limited supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yinghua Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yuangang Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor W.Tsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xin Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Contrastive Learning. (arXiv:2106.15499v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15499</id>
        <link href="http://arxiv.org/abs/2106.15499"/>
        <updated>2021-07-15T01:59:04.552Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel contrastive learning framework, coined as
Self-Contrastive (SelfCon) Learning, that self-contrasts within multiple
outputs from the different levels of a network. We confirmed that SelfCon loss
guarantees the lower bound of mutual information (MI) between the intermediate
and last representations. Besides, we empirically showed, via various MI
estimators, that SelfCon loss highly correlates to the increase of MI and
better classification performance. In our experiments, SelfCon surpasses
supervised contrastive (SupCon) learning without the need for a multi-viewed
batch and with the cheaper computational cost. Especially on ResNet-18, we
achieved top-1 classification accuracy of 76.45% for the CIFAR-100 dataset,
which is 2.87% and 4.36% higher than SupCon and cross-entropy loss,
respectively. We found that mitigating both vanishing gradient and overfitting
issue makes our method outperform the counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1"&gt;Sangmin Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungnyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1"&gt;Jongwoo Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gihun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noh_S/0/1/0/all/0/1"&gt;Seungjong Noh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Se-Young Yun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Slow-Growing Trees. (arXiv:2103.01926v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01926</id>
        <link href="http://arxiv.org/abs/2103.01926"/>
        <updated>2021-07-15T01:59:04.532Z</updated>
        <summary type="html"><![CDATA[Random Forest's performance can be matched by a single slow-growing tree
(SGT), which uses a learning rate to tame CART's greedy algorithm. SGT exploits
the view that CART is an extreme case of an iterative weighted least square
procedure. Moreover, a unifying view of Boosted Trees (BT) and Random Forests
(RF) is presented. Greedy ML algorithms' outcomes can be improved using either
"slow learning" or diversification. SGT applies the former to estimate a single
deep tree, and Booging (bagging stochastic BT with a high learning rate) uses
the latter with additive shallow trees. The performance of this tree ensemble
quaternity (Booging, BT, SGT, RF) is assessed on simulated and real regression
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Coulombe_P/0/1/0/all/0/1"&gt;Philippe Goulet Coulombe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Learning of Pinball TWSVM using Privileged Information and its applications. (arXiv:2107.06744v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06744</id>
        <link href="http://arxiv.org/abs/2107.06744"/>
        <updated>2021-07-15T01:59:04.461Z</updated>
        <summary type="html"><![CDATA[In any learning framework, an expert knowledge always plays a crucial role.
But, in the field of machine learning, the knowledge offered by an expert is
rarely used. Moreover, machine learning algorithms (SVM based) generally use
hinge loss function which is sensitive towards the noise. Thus, in order to get
the advantage from an expert knowledge and to reduce the sensitivity towards
the noise, in this paper, we propose privileged information based Twin Pinball
Support Vector Machine classifier (Pin-TWSVMPI) where expert's knowledge is in
the form of privileged information. The proposed Pin-TWSVMPI incorporates
privileged information by using correcting function so as to obtain two
nonparallel decision hyperplanes. Further, in order to make computations more
efficient and fast, we use Sequential Minimal Optimization (SMO) technique for
obtaining the classifier and have also shown its application for Pedestrian
detection and Handwritten digit recognition. Further, for UCI datasets, we
first implement a procedure which extracts privileged information from the
features of the dataset which are then further utilized by Pin-TWSVMPI that
leads to enhancement in classification accuracy with comparatively lesser
computational time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rastogi_R/0/1/0/all/0/1"&gt;Reshma Rastogi&lt;/a&gt; (nee. Khemchandani), &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1"&gt;Aman Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty Modeling of Emerging Device-based Computing-in-Memory Neural Accelerators with Application to Neural Architecture Search. (arXiv:2107.06871v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2107.06871</id>
        <link href="http://arxiv.org/abs/2107.06871"/>
        <updated>2021-07-15T01:59:04.454Z</updated>
        <summary type="html"><![CDATA[Emerging device-based Computing-in-memory (CiM) has been proved to be a
promising candidate for high-energy efficiency deep neural network (DNN)
computations. However, most emerging devices suffer uncertainty issues,
resulting in a difference between actual data stored and the weight value it is
designed to be. This leads to an accuracy drop from trained models to actually
deployed platforms. In this work, we offer a thorough analysis of the effect of
such uncertainties-induced changes in DNN models. To reduce the impact of
device uncertainties, we propose UAE, an uncertainty-aware Neural Architecture
Search scheme to identify a DNN model that is both accurate and robust against
device uncertainties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zheyu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1"&gt;Da-Cheng Juan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaobo Sharon Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yiyu Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Adaptive Multi-Intention Inverse Reinforcement Learning. (arXiv:2107.06692v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06692</id>
        <link href="http://arxiv.org/abs/2107.06692"/>
        <updated>2021-07-15T01:59:04.448Z</updated>
        <summary type="html"><![CDATA[This paper presents a deep Inverse Reinforcement Learning (IRL) framework
that can learn an a priori unknown number of nonlinear reward functions from
unlabeled experts' demonstrations. For this purpose, we employ the tools from
Dirichlet processes and propose an adaptive approach to simultaneously account
for both complex and unknown number of reward functions. Using the conditional
maximum entropy principle, we model the experts' multi-intention behaviors as a
mixture of latent intention distributions and derive two algorithms to estimate
the parameters of the deep reward network along with the number of experts'
intentions from unlabeled demonstrations. The proposed algorithms are evaluated
on three benchmarks, two of which have been specifically extended in this study
for multi-intention IRL, and compared with well-known baselines. We demonstrate
through several experiments the advantages of our algorithms over the existing
approaches and the benefits of online inferring, rather than fixing beforehand,
the number of expert's intentions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bighashdel_A/0/1/0/all/0/1"&gt;Ariyan Bighashdel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meletis_P/0/1/0/all/0/1"&gt;Panagiotis Meletis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jancura_P/0/1/0/all/0/1"&gt;Pavol Jancura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubbelman_G/0/1/0/all/0/1"&gt;Gijs Dubbelman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for Machine Learning of Model Error in Dynamical Systems. (arXiv:2107.06658v1 [math.DS])]]></title>
        <id>http://arxiv.org/abs/2107.06658</id>
        <link href="http://arxiv.org/abs/2107.06658"/>
        <updated>2021-07-15T01:59:04.443Z</updated>
        <summary type="html"><![CDATA[The development of data-informed predictive models for dynamical systems is
of widespread interest in many disciplines. We present a unifying framework for
blending mechanistic and machine-learning approaches to identify dynamical
systems from data. We compare pure data-driven learning with hybrid models
which incorporate imperfect domain knowledge. We cast the problem in both
continuous- and discrete-time, for problems in which the model error is
memoryless and in which it has significant memory, and we compare data-driven
and hybrid approaches experimentally. Our formulation is agnostic to the chosen
machine learning model.

Using Lorenz '63 and Lorenz '96 Multiscale systems, we find that hybrid
methods substantially outperform solely data-driven approaches in terms of data
hunger, demands for model complexity, and overall predictive performance. We
also find that, while a continuous-time framing allows for robustness to
irregular sampling and desirable domain-interpretability, a discrete-time
framing can provide similar or better predictive performance, especially when
data are undersampled and the vector field cannot be resolved.

We study model error from the learning theory perspective, defining excess
risk and generalization error; for a linear model of the error used to learn
about ergodic dynamical systems, both errors are bounded by terms that diminish
with the square-root of T. We also illustrate scenarios that benefit from
modeling with memory, proving that continuous-time recurrent neural networks
(RNNs) can, in principle, learn memory-dependent model error and reconstruct
the original system arbitrarily well; numerical results depict challenges in
representing memory by this approach. We also connect RNNs to reservoir
computing and thereby relate the learning of memory-dependent error to recent
work on supervised learning between Banach spaces using random features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Levine_M/0/1/0/all/0/1"&gt;Matthew E. Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1"&gt;Andrew M. Stuart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reactive Long Horizon Task Execution via Visual Skill and Precondition Models. (arXiv:2011.08694v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08694</id>
        <link href="http://arxiv.org/abs/2011.08694"/>
        <updated>2021-07-15T01:59:04.437Z</updated>
        <summary type="html"><![CDATA[Zero-shot execution of unseen robotic tasks is important to allowing robots
to perform a wide variety of tasks in human environments, but collecting the
amounts of data necessary to train end-to-end policies in the real-world is
often infeasible. We describe an approach for sim-to-real training that can
accomplish unseen robotic tasks using models learned in simulation to ground
components of a simple task planner. We learn a library of parameterized
skills, along with a set of predicates-based preconditions and termination
conditions, entirely in simulation. We explore a block-stacking task because it
has a clear structure, where multiple skills must be chained together, but our
methods are applicable to a wide range of other problems and domains, and can
transfer from simulation to the real-world with no fine tuning. The system is
able to recognize failures and accomplish long-horizon tasks from perceptual
input, which is critical for real-world execution. We evaluate our proposed
approach in both simulation and in the real-world, showing an increase in
success rate from 91.6% to 98% in simulation and from 10% to 80% success rate
in the real-world as compared with naive baselines. For experiment videos
including both real-world and simulation, see:
https://www.youtube.com/playlist?list=PL-oD0xHUngeLfQmpngYkGFZarstfPOXqX]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Shohin Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1"&gt;Chris Paxton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1"&gt;Arsalan Mousavian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fishman_A/0/1/0/all/0/1"&gt;Adam Fishman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Likhachev_M/0/1/0/all/0/1"&gt;Maxim Likhachev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Self-Training for Semi-Supervised Audio Recognition. (arXiv:2107.06877v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06877</id>
        <link href="http://arxiv.org/abs/2107.06877"/>
        <updated>2021-07-15T01:59:04.431Z</updated>
        <summary type="html"><![CDATA[Federated Learning is a distributed machine learning paradigm dealing with
decentralized and personal datasets. Since data reside on devices like
smartphones and virtual assistants, labeling is entrusted to the clients, or
labels are extracted in an automated way. Specifically, in the case of audio
data, acquiring semantic annotations can be prohibitively expensive and
time-consuming. As a result, an abundance of audio data remains unlabeled and
unexploited on users' devices. Most existing federated learning approaches
focus on supervised learning without harnessing the unlabeled data. In this
work, we study the problem of semi-supervised learning of audio models via
self-training in conjunction with federated learning. We propose FedSTAR to
exploit large-scale on-device unlabeled data to improve the generalization of
audio recognition models. We further demonstrate that self-supervised
pre-trained models can accelerate the training of on-device models,
significantly improving convergence to within fewer training rounds. We conduct
experiments on diverse public audio classification datasets and investigate the
performance of our models under varying percentages of labeled and unlabeled
data. Notably, we show that with as little as 3% labeled data available,
FedSTAR on average can improve the recognition rate by 13.28% compared to the
fully supervised federated model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsouvalas_V/0/1/0/all/0/1"&gt;Vasileios Tsouvalas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1"&gt;Aaqib Saeed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozcelebi_T/0/1/0/all/0/1"&gt;Tanir Ozcelebi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Detection in the DCT Domain: is Luminance the Solution?. (arXiv:2006.05732v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05732</id>
        <link href="http://arxiv.org/abs/2006.05732"/>
        <updated>2021-07-15T01:59:04.425Z</updated>
        <summary type="html"><![CDATA[Object detection in images has reached unprecedented performances. The
state-of-the-art methods rely on deep architectures that extract salient
features and predict bounding boxes enclosing the objects of interest. These
methods essentially run on RGB images. However, the RGB images are often
compressed by the acquisition devices for storage purpose and transfer
efficiency. Hence, their decompression is required for object detectors. To
gain in efficiency, this paper proposes to take advantage of the compressed
representation of images to carry out object detection usable in constrained
resources conditions.

Specifically, we focus on JPEG images and propose a thorough analysis of
detection architectures newly designed in regard of the peculiarities of the
JPEG norm. This leads to a $\times 1.7$ speed up in comparison with a standard
RGB-based architecture, while only reducing the detection performance by 5.5%.
Additionally, our empirical findings demonstrate that only part of the
compressed JPEG information, namely the luminance component, may be required to
match detection accuracy of the full input methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deguerre_B/0/1/0/all/0/1"&gt;Benjamin Deguerre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatelain_C/0/1/0/all/0/1"&gt;Clement Chatelain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasso_G/0/1/0/all/0/1"&gt;Gilles Gasso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous vs. Discrete Optimization of Deep Neural Networks. (arXiv:2107.06608v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06608</id>
        <link href="http://arxiv.org/abs/2107.06608"/>
        <updated>2021-07-15T01:59:04.408Z</updated>
        <summary type="html"><![CDATA[Existing analyses of optimization in deep learning are either continuous,
focusing on (variants of) gradient flow, or discrete, directly treating
(variants of) gradient descent. Gradient flow is amenable to theoretical
analysis, but is stylized and disregards computational efficiency. The extent
to which it represents gradient descent is an open question in deep learning
theory. The current paper studies this question. Viewing gradient descent as an
approximate numerical solution to the initial value problem of gradient flow,
we find that the degree of approximation depends on the curvature along the
latter's trajectory. We then show that over deep neural networks with
homogeneous activations, gradient flow trajectories enjoy favorable curvature,
suggesting they are well approximated by gradient descent. This finding allows
us to translate an analysis of gradient flow over deep linear neural networks
into a guarantee that gradient descent efficiently converges to global minimum
almost surely under random initialization. Experiments suggest that over simple
deep neural networks, gradient descent with conventional step size is indeed
close to the continuous limit. We hypothesize that the theory of gradient flows
will be central to unraveling mysteries behind deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elkabetz_O/0/1/0/all/0/1"&gt;Omer Elkabetz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1"&gt;Nadav Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Practicality of Deterministic Epistemic Uncertainty. (arXiv:2107.00649v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00649</id>
        <link href="http://arxiv.org/abs/2107.00649"/>
        <updated>2021-07-15T01:59:04.389Z</updated>
        <summary type="html"><![CDATA[A set of novel approaches for estimating epistemic uncertainty in deep neural
networks with a single forward pass has recently emerged as a valid alternative
to Bayesian Neural Networks. On the premise of informative representations,
these deterministic uncertainty methods (DUMs) achieve strong performance on
detecting out-of-distribution (OOD) data while adding negligible computational
costs at inference time. However, it remains unclear whether DUMs are well
calibrated and can seamlessly scale to real-world applications - both
prerequisites for their practical deployment. To this end, we first provide a
taxonomy of DUMs, evaluate their calibration under continuous distributional
shifts and their performance on OOD detection for image classification tasks.
Then, we extend the most promising approaches to semantic segmentation. We find
that, while DUMs scale to realistic vision tasks and perform well on OOD
detection, the practicality of current methods is undermined by poor
calibration under realistic distributional shifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Postels_J/0/1/0/all/0/1"&gt;Janis Postels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segu_M/0/1/0/all/0/1"&gt;Mattia Segu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1"&gt;Tao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fisher Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extreme Precipitation Seasonal Forecast Using a Transformer Neural Network. (arXiv:2107.06846v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06846</id>
        <link href="http://arxiv.org/abs/2107.06846"/>
        <updated>2021-07-15T01:59:04.368Z</updated>
        <summary type="html"><![CDATA[An impact of climate change is the increase in frequency and intensity of
extreme precipitation events. However, confidently predicting the likelihood of
extreme precipitation at seasonal scales remains an outstanding challenge.
Here, we present an approach to forecasting the quantiles of the maximum daily
precipitation in each week up to six months ahead using the temporal fusion
transformer (TFT) model. Through experiments in two regions, we compare TFT
predictions with those of two baselines: climatology and a calibrated ECMWF
SEAS5 ensemble forecast (S5). Our results show that, in terms of quantile risk
at six month lead time, the TFT predictions significantly outperform those from
S5 and show an overall small improvement compared to climatology. The TFT also
responds positively to departures from normal that climatology cannot.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Civitarese_D/0/1/0/all/0/1"&gt;Daniel Salles Civitarese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szwarcman_D/0/1/0/all/0/1"&gt;Daniela Szwarcman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zadrozny_B/0/1/0/all/0/1"&gt;Bianca Zadrozny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watson_C/0/1/0/all/0/1"&gt;Campbell Watson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conservative Objective Models for Effective Offline Model-Based Optimization. (arXiv:2107.06882v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06882</id>
        <link href="http://arxiv.org/abs/2107.06882"/>
        <updated>2021-07-15T01:59:04.363Z</updated>
        <summary type="html"><![CDATA[Computational design problems arise in a number of settings, from synthetic
biology to computer architectures. In this paper, we aim to solve data-driven
model-based optimization (MBO) problems, where the goal is to find a design
input that maximizes an unknown objective function provided access to only a
static dataset of prior experiments. Such data-driven optimization procedures
are the only practical methods in many real-world domains where active data
collection is expensive (e.g., when optimizing over proteins) or dangerous
(e.g., when optimizing over aircraft designs). Typical methods for MBO that
optimize the design against a learned model suffer from distributional shift:
it is easy to find a design that "fools" the model into predicting a high
value. To overcome this, we propose conservative objective models (COMs), a
method that learns a model of the objective function that lower bounds the
actual value of the ground-truth objective on out-of-distribution inputs, and
uses it for optimization. Structurally, COMs resemble adversarial training
methods used to overcome adversarial examples. COMs are simple to implement and
outperform a number of existing methods on a wide range of MBO problems,
including optimizing protein sequences, robot morphologies, neural network
weights, and superconducting materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trabucco_B/0/1/0/all/0/1"&gt;Brandon Trabucco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Aviral Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1"&gt;Xinyang Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Deep Distribution Network for Bid Shading in First-Price Auctions. (arXiv:2107.06650v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2107.06650</id>
        <link href="http://arxiv.org/abs/2107.06650"/>
        <updated>2021-07-15T01:59:04.356Z</updated>
        <summary type="html"><![CDATA[Since 2019, most ad exchanges and sell-side platforms (SSPs), in the online
advertising industry, shifted from second to first price auctions. Due to the
fundamental difference between these auctions, demand-side platforms (DSPs)
have had to update their bidding strategies to avoid bidding unnecessarily high
and hence overpaying. Bid shading was proposed to adjust the bid price intended
for second-price auctions, in order to balance cost and winning probability in
a first-price auction setup. In this study, we introduce a novel deep
distribution network for optimal bidding in both open (non-censored) and closed
(censored) online first-price auctions. Offline and online A/B testing results
show that our algorithm outperforms previous state-of-art algorithms in terms
of both surplus and effective cost per action (eCPX) metrics. Furthermore, the
algorithm is optimized in run-time and has been deployed into VerizonMedia DSP
as production algorithm, serving hundreds of billions of bid requests per day.
Online A/B test shows that advertiser's ROI are improved by +2.4%, +2.4%, and
+8.6% for impression based (CPM), click based (CPC), and conversion based (CPA)
campaigns respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tian Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Hao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shengjun Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlsson_N/0/1/0/all/0/1"&gt;Niklas Karlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shetty_B/0/1/0/all/0/1"&gt;Bharatbhushan Shetty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitts_B/0/1/0/all/0/1"&gt;Brendan Kitts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gligorijevic_D/0/1/0/all/0/1"&gt;Djordje Gligorijevic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gultekin_S/0/1/0/all/0/1"&gt;San Gultekin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_T/0/1/0/all/0/1"&gt;Tingyu Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1"&gt;Junwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianlong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flores_A/0/1/0/all/0/1"&gt;Aaron Flores&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Feature Selection via Transferring Supervised Knowledge. (arXiv:1908.03464v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.03464</id>
        <link href="http://arxiv.org/abs/1908.03464"/>
        <updated>2021-07-15T01:59:04.348Z</updated>
        <summary type="html"><![CDATA[Feature selection, an effective technique for dimensionality reduction, plays
an important role in many machine learning systems. Supervised knowledge can
significantly improve the performance. However, faced with the rapid growth of
newly emerging concepts, existing supervised methods might easily suffer from
the scarcity and validity of labeled data for training. In this paper, the
authors study the problem of zero-shot feature selection (i.e., building a
feature selection model that generalizes well to "unseen" concepts with limited
training data of "seen" concepts). Specifically, they adopt class-semantic
descriptions (i.e., attributes) as supervision for feature selection, so as to
utilize the supervised knowledge transferred from the seen concepts. For more
reliable discriminative features, they further propose the
center-characteristic loss which encourages the selected features to capture
the central characteristics of seen concepts. Extensive experiments conducted
on various real-world datasets demonstrate the effectiveness of the method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiao Wang&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tingzhang Zhao&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaojun Ye&lt;/a&gt; (2) ((1) Department of Computer Science, University of Science and Technology Beijing (2) School of Software, Tsinghua University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MINIROCKET: A Very Fast (Almost) Deterministic Transform for Time Series Classification. (arXiv:2012.08791v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08791</id>
        <link href="http://arxiv.org/abs/2012.08791"/>
        <updated>2021-07-15T01:59:04.340Z</updated>
        <summary type="html"><![CDATA[Until recently, the most accurate methods for time series classification were
limited by high computational complexity. ROCKET achieves state-of-the-art
accuracy with a fraction of the computational expense of most existing methods
by transforming input time series using random convolutional kernels, and using
the transformed features to train a linear classifier. We reformulate ROCKET
into a new method, MINIROCKET, making it up to 75 times faster on larger
datasets, and making it almost deterministic (and optionally, with additional
computational expense, fully deterministic), while maintaining essentially the
same accuracy. Using this method, it is possible to train and test a classifier
on all of 109 datasets from the UCR archive to state-of-the-art accuracy in
less than 10 minutes. MINIROCKET is significantly faster than any other method
of comparable accuracy (including ROCKET), and significantly more accurate than
any other method of even roughly-similar computational expense. As such, we
suggest that MINIROCKET should now be considered and used as the default
variant of ROCKET.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dempster_A/0/1/0/all/0/1"&gt;Angus Dempster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1"&gt;Daniel F. Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1"&gt;Geoffrey I. Webb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More. (arXiv:2107.06876v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06876</id>
        <link href="http://arxiv.org/abs/2107.06876"/>
        <updated>2021-07-15T01:59:04.324Z</updated>
        <summary type="html"><![CDATA[The current best practice for computing optimal transport (OT) is via entropy
regularization and Sinkhorn iterations. This algorithm runs in quadratic time
as it requires the full pairwise cost matrix, which is prohibitively expensive
for large sets of objects. In this work we propose two effective log-linear
time approximations of the cost matrix: First, a sparse approximation based on
locality-sensitive hashing (LSH) and, second, a Nystr\"om approximation with
LSH-based sparse corrections, which we call locally corrected Nystr\"om (LCN).
These approximations enable general log-linear time algorithms for
entropy-regularized OT that perform well even for the complex, high-dimensional
spaces common in deep learning. We analyse these approximations theoretically
and evaluate them experimentally both directly and end-to-end as a component
for real-world applications. Using our approximations for unsupervised word
embedding alignment enables us to speed up a state-of-the-art method by a
factor of 3 while also improving the accuracy by 3.1 percentage points without
any additional model changes. For graph distance regression we propose the
graph transport network (GTN), which combines graph neural networks (GNNs) with
enhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales
log-linearly in the number of nodes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klicpera_J/0/1/0/all/0/1"&gt;Johannes Klicpera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lienen_M/0/1/0/all/0/1"&gt;Marten Lienen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DULA: A Differentiable Ergonomics Model for Postural Optimization in Physical HRI. (arXiv:2107.06875v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.06875</id>
        <link href="http://arxiv.org/abs/2107.06875"/>
        <updated>2021-07-15T01:59:04.318Z</updated>
        <summary type="html"><![CDATA[Ergonomics and human comfort are essential concerns in physical human-robot
interaction applications. Defining an accurate and easy-to-use ergonomic
assessment model stands as an important step in providing feedback for postural
correction to improve operator health and comfort. In order to enable efficient
computation, previously proposed automated ergonomic assessment and correction
tools make approximations or simplifications to gold-standard assessment tools
used by ergonomists in practice. In order to retain assessment quality, while
improving computational considerations, we introduce DULA, a differentiable and
continuous ergonomics model learned to replicate the popular and scientifically
validated RULA assessment. We show that DULA provides assessment comparable to
RULA while providing computational benefits. We highlight DULA's strength in a
demonstration of gradient-based postural optimization for a simulated
teleoperation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yazdani_A/0/1/0/all/0/1"&gt;Amir Yazdani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novin_R/0/1/0/all/0/1"&gt;Roya Sabbagh Novin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merryweather_A/0/1/0/all/0/1"&gt;Andrew Merryweather&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hermans_T/0/1/0/all/0/1"&gt;Tucker Hermans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness in Ranking under Uncertainty. (arXiv:2107.06720v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06720</id>
        <link href="http://arxiv.org/abs/2107.06720"/>
        <updated>2021-07-15T01:59:04.312Z</updated>
        <summary type="html"><![CDATA[Fairness has emerged as an important consideration in algorithmic
decision-making. Unfairness occurs when an agent with higher merit obtains a
worse outcome than an agent with lower merit. Our central point is that a
primary cause of unfairness is uncertainty. A principal or algorithm making
decisions never has access to the agents' true merit, and instead uses proxy
features that only imperfectly predict merit (e.g., GPA, star ratings,
recommendation letters). None of these ever fully capture an agent's merit; yet
existing approaches have mostly been defining fairness notions directly based
on observed features and outcomes.

Our primary point is that it is more principled to acknowledge and model the
uncertainty explicitly. The role of observed features is to give rise to a
posterior distribution of the agents' merits. We use this viewpoint to define a
notion of approximate fairness in ranking. We call an algorithm $\phi$-fair
(for $\phi \in [0,1]$) if it has the following property for all agents $x$ and
all $k$: if agent $x$ is among the top $k$ agents with respect to merit with
probability at least $\rho$ (according to the posterior merit distribution),
then the algorithm places the agent among the top $k$ agents in its ranking
with probability at least $\phi \rho$.

We show how to compute rankings that optimally trade off approximate fairness
against utility to the principal. In addition to the theoretical
characterization, we present an empirical analysis of the potential impact of
the approach in simulation studies. For real-world validation, we applied the
approach in the context of a paper recommendation system that we built and
fielded at a large conference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Ashudeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kempe_D/0/1/0/all/0/1"&gt;David Kempe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1"&gt;Thorsten Joachims&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thinkback: Task-SpecificOut-of-Distribution Detection. (arXiv:2107.06668v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06668</id>
        <link href="http://arxiv.org/abs/2107.06668"/>
        <updated>2021-07-15T01:59:04.306Z</updated>
        <summary type="html"><![CDATA[The increased success of Deep Learning (DL) has recently sparked large-scale
deployment of DL models in many diverse industry segments. Yet, a crucial
weakness of supervised model is the inherent difficulty in handling
out-of-distribution samples, i.e., samples belonging to classes that were not
presented to the model at training time. We propose in this paper a novel way
to formulate the out-of-distribution detection problem, tailored for DL models.
Our method does not require fine tuning process on training data, yet is
significantly more accurate than the state of the art for out-of-distribution
detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lixuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1"&gt;Dario Rossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Closure Models for Dynamical Systems. (arXiv:2012.13869v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13869</id>
        <link href="http://arxiv.org/abs/2012.13869"/>
        <updated>2021-07-15T01:59:04.300Z</updated>
        <summary type="html"><![CDATA[Complex dynamical systems are used for predictions in many domains. Because
of computational costs, models are truncated, coarsened, or aggregated. As the
neglected and unresolved terms become important, the utility of model
predictions diminishes. We develop a novel, versatile, and rigorous methodology
to learn non-Markovian closure parameterizations for known-physics/low-fidelity
models using data from high-fidelity simulations. The new "neural closure
models" augment low-fidelity models with neural delay differential equations
(nDDEs), motivated by the Mori-Zwanzig formulation and the inherent delays in
complex dynamical systems. We demonstrate that neural closures efficiently
account for truncated modes in reduced-order-models, capture the effects of
subgrid-scale processes in coarse models, and augment the simplification of
complex biological and physical-biogeochemical models. We find that using
non-Markovian over Markovian closures improves long-term prediction accuracy
and requires smaller networks. We derive adjoint equations and network
architectures needed to efficiently implement the new discrete and distributed
nDDEs, for any time-integration schemes and allowing nonuniformly-spaced
temporal training data. The performance of discrete over distributed delays in
closure models is explained using information theory, and we find an optimal
amount of past information for a specified architecture. Finally, we analyze
computational complexity and explain the limited additional cost due to neural
closure models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abhinav Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lermusiaux_P/0/1/0/all/0/1"&gt;Pierre F.J. Lermusiaux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOSNet: Deep Learning based Objective Assessment for Voice Conversion. (arXiv:1904.08352v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.08352</id>
        <link href="http://arxiv.org/abs/1904.08352"/>
        <updated>2021-07-15T01:59:04.294Z</updated>
        <summary type="html"><![CDATA[Existing objective evaluation metrics for voice conversion (VC) are not
always correlated with human perception. Therefore, training VC models with
such criteria may not effectively improve naturalness and similarity of
converted speech. In this paper, we propose deep learning-based assessment
models to predict human ratings of converted speech. We adopt the convolutional
and recurrent neural network models to build a mean opinion score (MOS)
predictor, termed as MOSNet. The proposed models are tested on large-scale
listening test results of the Voice Conversion Challenge (VCC) 2018.
Experimental results show that the predicted scores of the proposed MOSNet are
highly correlated with human MOS ratings at the system level while being fairly
correlated with human MOS ratings at the utterance level. Meanwhile, we have
modified MOSNet to predict the similarity scores, and the preliminary results
show that the predicted scores are also fairly correlated with human ratings.
These results confirm that the proposed models could be used as a computational
evaluator to measure the MOS of VC systems to reduce the need for expensive
human rating.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1"&gt;Chen-Chou Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1"&gt;Szu-Wei Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wen-Chin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1"&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1"&gt;Yu Tsao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hsin-Min Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT Fine-Tuning for Sentiment Analysis on Indonesian Mobile Apps Reviews. (arXiv:2107.06802v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06802</id>
        <link href="http://arxiv.org/abs/2107.06802"/>
        <updated>2021-07-15T01:59:04.278Z</updated>
        <summary type="html"><![CDATA[User reviews have an essential role in the success of the developed mobile
apps. User reviews in the textual form are unstructured data, creating a very
high complexity when processed for sentiment analysis. Previous approaches that
have been used often ignore the context of reviews. In addition, the relatively
small data makes the model overfitting. A new approach, BERT, has been
introduced as a transfer learning model with a pre-trained model that has
previously been trained to have a better context representation. This study
examines the effectiveness of fine-tuning BERT for sentiment analysis using two
different pre-trained models. Besides the multilingual pre-trained model, we
use the pre-trained model that only has been trained in Indonesian. The dataset
used is Indonesian user reviews of the ten best apps in 2020 in Google Play
sites. We also perform hyper-parameter tuning to find the optimum trained
model. Two training data labeling approaches were also tested to determine the
effectiveness of the model, which is score-based and lexicon-based. The
experimental results show that pre-trained models trained in Indonesian have
better average accuracy on lexicon-based data. The pre-trained Indonesian model
highest accuracy is 84%, with 25 epochs and a training time of 24 minutes.
These results are better than all of the machine learning and multilingual
pre-trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1"&gt;Kuncahyo Setyo Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukmadewa_A/0/1/0/all/0/1"&gt;Anantha Yullian Sukmadewa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DW_H/0/1/0/all/0/1"&gt;Haftittah Wuswilahaken DW&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachtiar_F/0/1/0/all/0/1"&gt;Fitra Abdurrachman Bachtiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1"&gt;Novanto Yudistira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Programming of Reaction-Diffusion Patterns. (arXiv:2107.06862v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.06862</id>
        <link href="http://arxiv.org/abs/2107.06862"/>
        <updated>2021-07-15T01:59:04.272Z</updated>
        <summary type="html"><![CDATA[Reaction-Diffusion (RD) systems provide a computational framework that
governs many pattern formation processes in nature. Current RD system design
practices boil down to trial-and-error parameter search. We propose a
differentiable optimization method for learning the RD system parameters to
perform example-based texture synthesis on a 2D plane. We do this by
representing the RD system as a variant of Neural Cellular Automata and using
task-specific differentiable loss functions. RD systems generated by our method
exhibit robust, non-trivial 'life-like' behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mordvintsev_A/0/1/0/all/0/1"&gt;Alexander Mordvintsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Randazzo_E/0/1/0/all/0/1"&gt;Ettore Randazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niklasson_E/0/1/0/all/0/1"&gt;Eyvind Niklasson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Stochastic Optimization: New Results in Convex and Non-Convex Settings. (arXiv:2107.05585v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05585</id>
        <link href="http://arxiv.org/abs/2107.05585"/>
        <updated>2021-07-15T01:59:04.266Z</updated>
        <summary type="html"><![CDATA[We study differentially private stochastic optimization in convex and
non-convex settings. For the convex case, we focus on the family of non-smooth
generalized linear losses (GLLs). Our algorithm for the $\ell_2$ setting
achieves optimal excess population risk in near-linear time, while the best
known differentially private algorithms for general convex losses run in
super-linear time. Our algorithm for the $\ell_1$ setting has nearly-optimal
excess population risk $\tilde{O}\big(\sqrt{\frac{\log{d}}{n}}\big)$, and
circumvents the dimension dependent lower bound of [AFKT21] for general
non-smooth convex losses. In the differentially private non-convex setting, we
provide several new algorithms for approximating stationary points of the
population risk. For the $\ell_1$-case with smooth losses and polyhedral
constraint, we provide the first nearly dimension independent rate, $\tilde
O\big(\frac{\log^{2/3}{d}}{{n^{1/3}}}\big)$ in linear time. For the constrained
$\ell_2$-case, with smooth losses, we obtain a linear-time algorithm with rate
$\tilde O\big(\frac{1}{n^{3/10}d^{1/10}}+\big(\frac{d}{n^2}\big)^{1/5}\big)$.
Finally, for the $\ell_2$-case we provide the first method for {\em non-smooth
weakly convex} stochastic optimization with rate $\tilde
O\big(\frac{1}{n^{1/4}}+\big(\frac{d}{n^2}\big)^{1/6}\big)$ which matches the
best existing non-private algorithm when $d= O(\sqrt{n})$. We also extend all
our results above for the non-convex $\ell_2$ setting to the $\ell_p$ setting,
where $1 < p \leq 2$, with only polylogarithmic (in the dimension) overhead in
the rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bassily_R/0/1/0/all/0/1"&gt;Raef Bassily&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzman_C/0/1/0/all/0/1"&gt;Crist&amp;#xf3;bal Guzm&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menart_M/0/1/0/all/0/1"&gt;Michael Menart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RCDNet: An Interpretable Rain Convolutional Dictionary Network for Single Image Deraining. (arXiv:2107.06808v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06808</id>
        <link href="http://arxiv.org/abs/2107.06808"/>
        <updated>2021-07-15T01:59:04.260Z</updated>
        <summary type="html"><![CDATA[As a common weather, rain streaks adversely degrade the image quality. Hence,
removing rains from an image has become an important issue in the field. To
handle such an ill-posed single image deraining task, in this paper, we
specifically build a novel deep architecture, called rain convolutional
dictionary network (RCDNet), which embeds the intrinsic priors of rain streaks
and has clear interpretability. In specific, we first establish a RCD model for
representing rain streaks and utilize the proximal gradient descent technique
to design an iterative algorithm only containing simple operators for solving
the model. By unfolding it, we then build the RCDNet in which every network
module has clear physical meanings and corresponds to each operation involved
in the algorithm. This good interpretability greatly facilitates an easy
visualization and analysis on what happens inside the network and why it works
well in inference process. Moreover, taking into account the domain gap issue
in real scenarios, we further design a novel dynamic RCDNet, where the rain
kernels can be dynamically inferred corresponding to input rainy images and
then help shrink the space for rain layer estimation with few rain maps so as
to ensure a fine generalization performance in the inconsistent scenarios of
rain types between training and testing data. By end-to-end training such an
interpretable network, all involved rain kernels and proximal operators can be
automatically extracted, faithfully characterizing the features of both rain
and clean background layers, and thus naturally lead to better deraining
performance. Comprehensive experiments substantiate the superiority of our
method, especially on its well generality to diverse testing scenarios and good
interpretability for all its modules. Code is available in
\emph{\url{https://github.com/hongwang01/DRCDNet}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_Q/0/1/0/all/0/1"&gt;Qi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Criminal Charge Prediction and Its Algorithmic Bias via Quantum-Inspired Complex Valued Networks. (arXiv:2106.13456v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13456</id>
        <link href="http://arxiv.org/abs/2106.13456"/>
        <updated>2021-07-15T01:59:04.254Z</updated>
        <summary type="html"><![CDATA[While predictive policing has become increasingly common in assisting with
decisions in the criminal justice system, the use of these results is still
controversial. Some software based on deep learning lacks accuracy (e.g., in
F-1), and importantly many decision processes are not transparent, causing
doubt about decision bias, such as perceived racial and age disparities. This
paper addresses bias issues with post-hoc explanations to provide a trustable
prediction of whether a person will receive future criminal charges given one's
previous criminal records by learning temporal behavior patterns over twenty
years. Bi-LSTM relieves the vanishing gradient problem, attentional mechanisms
allow learning and interpretation of feature importance, and complex-valued
networks inspired quantum physics to facilitate a certain level of transparency
in modeling the decision process. Our approach shows a consistent and reliable
prediction precision and recall on a real-life dataset. Our analysis of the
importance of each input feature shows the critical causal impact on
decision-making, suggesting that criminal histories are statistically
significant factors, while identifiers, such as race and age, are not. Finally,
our algorithm indicates that a suspect tends to rather than suddenly increase
crime severity level over time gradually.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Abdul Rafae Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varsanyi_P/0/1/0/all/0/1"&gt;Peter Varsanyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pabreja_R/0/1/0/all/0/1"&gt;Rachit Pabreja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Mixture of Experts. (arXiv:2107.06724v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06724</id>
        <link href="http://arxiv.org/abs/2107.06724"/>
        <updated>2021-07-15T01:59:04.236Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has emerged as the predominant approach for
collaborative training of neural network models across multiple users, without
the need to gather the data at a central location. One of the important
challenges in this setting is data heterogeneity, i.e. different users have
different data characteristics. For this reason, training and using a single
global model might be suboptimal when considering the performance of each of
the individual user's data. In this work, we tackle this problem via Federated
Mixture of Experts, FedMix, a framework that allows us to train an ensemble of
specialized models. FedMix adaptively selects and trains a user-specific
selection of the ensemble members. We show that users with similar data
characteristics select the same members and therefore share statistical
strength while mitigating the effect of non-i.i.d data. Empirically, we show
through an extensive experimental evaluation that FedMix improves performance
compared to using a single global model across a variety of different sources
of non-i.i.d.-ness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reisser_M/0/1/0/all/0/1"&gt;Matthias Reisser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Louizos_C/0/1/0/all/0/1"&gt;Christos Louizos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1"&gt;Efstratios Gavves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M5 Competition Uncertainty: Overdispersion, distributional forecasting, GAMLSS and beyond. (arXiv:2107.06675v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.06675</id>
        <link href="http://arxiv.org/abs/2107.06675"/>
        <updated>2021-07-15T01:59:04.229Z</updated>
        <summary type="html"><![CDATA[The M5 competition uncertainty track aims for probabilistic forecasting of
sales of thousands of Walmart retail goods. We show that the M5 competition
data faces strong overdispersion and sporadic demand, especially zero demand.
We discuss resulting modeling issues concerning adequate probabilistic
forecasting of such count data processes. Unfortunately, the majority of
popular prediction methods used in the M5 competition (e.g. lightgbm and
xgboost GBMs) fails to address the data characteristics due to the considered
objective functions. The distributional forecasting provides a suitable
modeling approach for to the overcome those problems. The GAMLSS framework
allows flexible probabilistic forecasting using low dimensional distributions.
We illustrate, how the GAMLSS approach can be applied for the M5 competition
data by modeling the location and scale parameter of various distributions,
e.g. the negative binomial distribution. Finally, we discuss software packages
for distributional modeling and their drawback, like the R package gamlss with
its package extensions, and (deep) distributional forecasting libraries such as
TensorFlow Probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ziel_F/0/1/0/all/0/1"&gt;Florian Ziel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modelling Neuronal Behaviour with Time Series Regression: Recurrent Neural Networks on C. Elegans Data. (arXiv:2107.06762v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2107.06762</id>
        <link href="http://arxiv.org/abs/2107.06762"/>
        <updated>2021-07-15T01:59:04.223Z</updated>
        <summary type="html"><![CDATA[Given the inner complexity of the human nervous system, insight into the
dynamics of brain activity can be gained from understanding smaller and simpler
organisms, such as the nematode C. Elegans. The behavioural and structural
biology of these organisms is well-known, making them prime candidates for
benchmarking modelling and simulation techniques. In these complex neuronal
collections, classical, white-box modelling techniques based on intrinsic
structural or behavioural information are either unable to capture the profound
nonlinearities of the neuronal response to different stimuli or generate
extremely complex models, which are computationally intractable. In this paper
we show how the nervous system of C. Elegans can be modelled and simulated with
data-driven models using different neural network architectures. Specifically,
we target the use of state of the art recurrent neural networks architectures
such as LSTMs and GRUs and compare these architectures in terms of their
properties and their accuracy as well as the complexity of the resulting
models. We show that GRU models with a hidden layer size of 4 units are able to
accurately reproduce with high accuracy the system's response to very different
stimuli.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Mestre_G/0/1/0/all/0/1"&gt;Gon&amp;#xe7;alo Mestre&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/q-bio/1/au:+Barbulescu_R/0/1/0/all/0/1"&gt;Ruxandra Barbulescu&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/q-bio/1/au:+Oliveira_A/0/1/0/all/0/1"&gt;Arlindo L. Oliveira&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/q-bio/1/au:+Silveira_L/0/1/0/all/0/1"&gt;L. Miguel Silveira&lt;/a&gt; (1 and 2) ((1) INESC-ID, Rua Alves Redol 9, 1000-029 Lisboa, (2) IST Tecnico Lisboa, Universidade de Lisboa, Av. Rovisco Pais 1, 1049-001 Lisboa)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Potential of Low-bit Training of Convolutional Neural Networks. (arXiv:2006.02804v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02804</id>
        <link href="http://arxiv.org/abs/2006.02804"/>
        <updated>2021-07-15T01:59:04.217Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a low-bit training framework for convolutional
neural networks, which is built around a novel multi-level scaling (MLS) tensor
format. Our framework focuses on reducing the energy consumption of convolution
operations by quantizing all the convolution operands to low bit-width format.
Specifically, we propose the MLS tensor format, in which the element-wise
bit-width can be largely reduced. Then, we describe the dynamic quantization
and the low-bit tensor convolution arithmetic to leverage the MLS tensor format
efficiently. Experiments show that our framework achieves a superior trade-off
between the accuracy and the bit-width than previous low-bit training
frameworks. For training a variety of models on CIFAR-10, using 1-bit mantissa
and 2-bit exponent is adequate to keep the accuracy loss within $1\%$. And on
larger datasets like ImageNet, using 4-bit mantissa and 2-bit exponent is
adequate to keep the accuracy loss within $1\%$. Through the energy consumption
simulation of the computing units, we can estimate that training a variety of
models with our framework could achieve $8.3\sim10.2\times$ and
$1.9\sim2.3\times$ higher energy efficiency than training with full-precision
and 8-bit floating-point arithmetic, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1"&gt;Kai Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1"&gt;Xuefei Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1"&gt;Guohao Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhenhua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tianchen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shulin Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Huazhong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Initial Pools for Deep Active Learning. (arXiv:2011.14696v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14696</id>
        <link href="http://arxiv.org/abs/2011.14696"/>
        <updated>2021-07-15T01:59:04.210Z</updated>
        <summary type="html"><![CDATA[Active Learning (AL) techniques aim to minimize the training data required to
train a model for a given task. Pool-based AL techniques start with a small
initial labeled pool and then iteratively pick batches of the most informative
samples for labeling. Generally, the initial pool is sampled randomly and
labeled to seed the AL iterations. While recent studies have focused on
evaluating the robustness of various query functions in AL, little to no
attention has been given to the design of the initial labeled pool for deep
active learning. Given the recent successes of learning representations in
self-supervised/unsupervised ways, we study if an intelligently sampled initial
labeled pool can improve deep AL performance. We investigate the effect of
intelligently sampled initial labeled pools, including the use of
self-supervised and unsupervised strategies, on deep AL methods. The setup,
hypotheses, methodology, and implementation details were evaluated by peer
review before experiments were conducted. Experimental results could not
conclusively prove that intelligently sampled initial pools are better for AL
than random initial pools in the long run, although a Variational
Autoencoder-based initial pool sampling strategy showed interesting trends that
merit deeper investigation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1"&gt;Akshay L Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1"&gt;Sai Vikas Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1"&gt;Chaitanya Devaguptapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A LightGBM based Forecasting of Dominant Wave Periods in Oceanic Waters. (arXiv:2105.08721v4 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08721</id>
        <link href="http://arxiv.org/abs/2105.08721"/>
        <updated>2021-07-15T01:59:04.193Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a Light Gradient Boosting (LightGBM) to forecast
dominant wave periods in oceanic waters. First, we use the data collected from
CDIP buoys and apply various data filtering methods. The data filtering methods
allow us to obtain a high-quality dataset for training and validation purposes.
We then extract various wave-based features like wave heights, periods,
skewness, kurtosis, etc., and atmospheric features like humidity, pressure, and
air temperature for the buoys. Afterward, we train algorithms that use LightGBM
and Extra Trees through a hv-block cross-validation scheme to forecast dominant
wave periods for up to 30 days ahead. LightGBM has the R2 score of 0.94, 0.94,
and 0.94 for 1-day ahead, 15-day ahead, and 30-day ahead prediction. Similarly,
Extra Trees (ET) has an R2 score of 0.88, 0.86, and 0.85 for 1-day ahead,
15-day ahead, and 30 day ahead prediction. In case of the test dataset,
LightGBM has R2 score of 0.94, 0.94, and 0.94 for 1-day ahead, 15-day ahead and
30-day ahead prediction. ET has R2 score of 0.88, 0.86, and 0.85 for 1-day
ahead, 15-day ahead, and 30-day ahead prediction. A similar R2 score for both
training and the test dataset suggests that the machine learning models
developed in this paper are robust. Since the LightGBM algorithm outperforms ET
for all the windows tested, it is taken as the final algorithm. Note that the
performance of both methods does not decrease significantly as the forecast
horizon increases. Likewise, the proposed method outperforms the numerical
approaches included in this paper in the test dataset. For 1 day ahead
prediction, the proposed algorithm has SI, Bias, CC, and RMSE of 0.09, 0.00,
0.97, and 1.78 compared to 0.268, 0.40, 0.63, and 2.18 for the European Centre
for Medium-range Weather Forecasts (ECMWF) model, which outperforms all the
other methods in the test dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1"&gt;Pujan Pokhrel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Tensor Train Parameterization of Deep Learning Layers. (arXiv:2103.04217v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04217</id>
        <link href="http://arxiv.org/abs/2103.04217"/>
        <updated>2021-07-15T01:59:04.186Z</updated>
        <summary type="html"><![CDATA[We study low-rank parameterizations of weight matrices with embedded spectral
properties in the Deep Learning context. The low-rank property leads to
parameter efficiency and permits taking computational shortcuts when computing
mappings. Spectral properties are often subject to constraints in optimization
problems, leading to better models and stability of optimization. We start by
looking at the compact SVD parameterization of weight matrices and identifying
redundancy sources in the parameterization. We further apply the Tensor Train
(TT) decomposition to the compact SVD components, and propose a non-redundant
differentiable parameterization of fixed TT-rank tensor manifolds, termed the
Spectral Tensor Train Parameterization (STTP). We demonstrate the effects of
neural network compression in the image classification setting and both
compression and improved training stability in the generative adversarial
training setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1"&gt;Anton Obukhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1"&gt;Maxim Rakhuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1"&gt;Alexander Liniger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1"&gt;Stamatios Georgoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Dengxin Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance Analysis on Machine Learning-Based Channel Estimation. (arXiv:1911.03886v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.03886</id>
        <link href="http://arxiv.org/abs/1911.03886"/>
        <updated>2021-07-15T01:59:04.178Z</updated>
        <summary type="html"><![CDATA[Recently, machine learning-based channel estimation has attracted much
attention. The performance of machine learning-based estimation has been
validated by simulation experiments. However, little attention has been paid to
the theoretical performance analysis. In this paper, we investigate the mean
square error (MSE) performance of machine learning-based estimation. Hypothesis
testing is employed to analyze its MSE upper bound. Furthermore, we build a
statistical model for hypothesis testing, which holds when the linear learning
module with a low input dimension is used in machine learning-based channel
estimation, and derive a clear analytical relation between the size of the
training data and performance. Then, we simulate the machine learning-based
channel estimation in orthogonal frequency division multiplexing (OFDM) systems
to verify our analysis results. Finally, the design considerations for the
situation where only limited training data is available are discussed. In this
situation, our analysis results can be applied to assess the performance and
support the design of machine learning-based channel estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mei_K/0/1/0/all/0/1"&gt;Kai Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaochen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajatheva_N/0/1/0/all/0/1"&gt;Nandana Rajatheva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jibo Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymptotic Behavior of Adversarial Training in Binary Classification. (arXiv:2010.13275v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13275</id>
        <link href="http://arxiv.org/abs/2010.13275"/>
        <updated>2021-07-15T01:59:04.172Z</updated>
        <summary type="html"><![CDATA[It has been consistently reported that many machine learning models are
susceptible to adversarial attacks i.e., small additive adversarial
perturbations applied to data points can cause misclassification. Adversarial
training using empirical risk minimization is considered to be the
state-of-the-art method for defense against adversarial attacks. Despite being
successful in practice, several problems in understanding generalization
performance of adversarial training remain open. In this paper, we derive
precise theoretical predictions for the performance of adversarial training in
binary classification. We consider the high-dimensional regime where the
dimension of data grows with the size of the training data-set at a constant
ratio. Our results provide exact asymptotics for standard and adversarial test
errors of the estimators obtained by adversarial training with $\ell_q$-norm
bounded perturbations ($q \ge 1$) for both discriminative binary models and
generative Gaussian-mixture models with correlated features. Furthermore, we
use these sharp predictions to uncover several intriguing observations on the
role of various parameters including the over-parameterization ratio, the data
model, and the attack budget on the adversarial and standard errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Taheri_H/0/1/0/all/0/1"&gt;Hossein Taheri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pedarsani_R/0/1/0/all/0/1"&gt;Ramtin Pedarsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thrampoulidis_C/0/1/0/all/0/1"&gt;Christos Thrampoulidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks. (arXiv:2107.06661v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.06661</id>
        <link href="http://arxiv.org/abs/2107.06661"/>
        <updated>2021-07-15T01:59:04.166Z</updated>
        <summary type="html"><![CDATA[In high-dimensional state spaces, the usefulness of Reinforcement Learning
(RL) is limited by the problem of exploration. This issue has been addressed
using potential-based reward shaping (PB-RS) previously. In the present work,
we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the
strict optimality guarantees of PB-RS to a guarantee of preserved long-term
behavior. Being less restrictive, FV-RS allows for reward shaping functions
that are even better suited for improving the sample efficiency of RL
algorithms. In particular, we consider settings in which the agent has access
to an approximate plan. Here, we use examples of simulated robotic manipulation
tasks to demonstrate that plan-based FV-RS can indeed significantly improve the
sample efficiency of RL over plan-based PB-RS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_I/0/1/0/all/0/1"&gt;Ingmar Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oguz_O/0/1/0/all/0/1"&gt;Ozgur S. Oguz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toussaint_M/0/1/0/all/0/1"&gt;Marc Toussaint&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Parallel Algorithm for Sinkhorn Word-Movers Distance and Its Performance on PIUMA and Xeon CPU. (arXiv:2107.06433v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.06433</id>
        <link href="http://arxiv.org/abs/2107.06433"/>
        <updated>2021-07-15T01:59:04.149Z</updated>
        <summary type="html"><![CDATA[The Word Movers Distance (WMD) measures the semantic dissimilarity between
two text documents by computing the cost of optimally moving all words of a
source/query document to the most similar words of a target document. Computing
WMD between two documents is costly because it requires solving an optimization
problem that costs $O (V^3 \log(V)) $ where $V$ is the number of unique words
in the document. Fortunately, WMD can be framed as an Earth Mover's Distance
(EMD) for which the algorithmic complexity can be reduced to $O(V^2)$ by adding
an entropy penalty to the optimization problem and solving it using the
Sinkhorn-Knopp algorithm. Additionally, the computation can be made highly
parallel by computing the WMD of a single query document against multiple
target documents at once, for example by finding whether a given tweet is
similar to any other tweets of a given day.

In this paper, we first present a shared-memory parallel Sinkhorn-Knopp
algorithm to compute the WMD of one document against many other documents by
adopting the $ O(V^2)$ EMD algorithm. We then algorithmically transform the
original $O(V^2)$ dense compute-heavy version into an equivalent sparse one
which is mapped onto the new Intel Programmable Integrated Unified Memory
Architecture (PIUMA) system. The WMD parallel implementation achieves 67x
speedup on 96 cores across 4 NUMA sockets of an Intel Cascade Lake system. We
also show that PIUMA cores are around 1.2-2.6x faster than Xeon cores on
Sinkhorn-WMD and also provide better strong scaling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tithi_J/0/1/0/all/0/1"&gt;Jesmin Jahan Tithi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrini_F/0/1/0/all/0/1"&gt;Fabrizio Petrini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Large Language Models Trained on Code. (arXiv:2107.03374v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03374</id>
        <link href="http://arxiv.org/abs/2107.03374"/>
        <updated>2021-07-15T01:59:04.143Z</updated>
        <summary type="html"><![CDATA[We introduce Codex, a GPT language model fine-tuned on publicly available
code from GitHub, and study its Python code-writing capabilities. A distinct
production version of Codex powers GitHub Copilot. On HumanEval, a new
evaluation set we release to measure functional correctness for synthesizing
programs from docstrings, our model solves 28.8% of the problems, while GPT-3
solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling
from the model is a surprisingly effective strategy for producing working
solutions to difficult prompts. Using this method, we solve 70.2% of our
problems with 100 samples per problem. Careful investigation of our model
reveals its limitations, including difficulty with docstrings describing long
chains of operations and with binding operations to variables. Finally, we
discuss the potential broader impacts of deploying powerful code generation
technologies, covering safety, security, and economics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mark Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tworek_J/0/1/0/all/0/1"&gt;Jerry Tworek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jun_H/0/1/0/all/0/1"&gt;Heewoo Jun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Q/0/1/0/all/0/1"&gt;Qiming Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_H/0/1/0/all/0/1"&gt;Henrique Ponde de Oliveira Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1"&gt;Jared Kaplan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_H/0/1/0/all/0/1"&gt;Harri Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burda_Y/0/1/0/all/0/1"&gt;Yuri Burda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1"&gt;Nicholas Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockman_G/0/1/0/all/0/1"&gt;Greg Brockman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1"&gt;Alex Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1"&gt;Raul Puri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1"&gt;Gretchen Krueger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrov_M/0/1/0/all/0/1"&gt;Michael Petrov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khlaaf_H/0/1/0/all/0/1"&gt;Heidy Khlaaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sastry_G/0/1/0/all/0/1"&gt;Girish Sastry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishkin_P/0/1/0/all/0/1"&gt;Pamela Mishkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_B/0/1/0/all/0/1"&gt;Brooke Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gray_S/0/1/0/all/0/1"&gt;Scott Gray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryder_N/0/1/0/all/0/1"&gt;Nick Ryder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavlov_M/0/1/0/all/0/1"&gt;Mikhail Pavlov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Power_A/0/1/0/all/0/1"&gt;Alethea Power&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaiser_L/0/1/0/all/0/1"&gt;Lukasz Kaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bavarian_M/0/1/0/all/0/1"&gt;Mohammad Bavarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winter_C/0/1/0/all/0/1"&gt;Clemens Winter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tillet_P/0/1/0/all/0/1"&gt;Philippe Tillet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Such_F/0/1/0/all/0/1"&gt;Felipe Petroski Such&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cummings_D/0/1/0/all/0/1"&gt;Dave Cummings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plappert_M/0/1/0/all/0/1"&gt;Matthias Plappert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chantzis_F/0/1/0/all/0/1"&gt;Fotios Chantzis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_E/0/1/0/all/0/1"&gt;Elizabeth Barnes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herbert_Voss_A/0/1/0/all/0/1"&gt;Ariel Herbert-Voss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guss_W/0/1/0/all/0/1"&gt;William Hebgen Guss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nichol_A/0/1/0/all/0/1"&gt;Alex Nichol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paino_A/0/1/0/all/0/1"&gt;Alex Paino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tezak_N/0/1/0/all/0/1"&gt;Nikolas Tezak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babuschkin_I/0/1/0/all/0/1"&gt;Igor Babuschkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balaji_S/0/1/0/all/0/1"&gt;Suchir Balaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Shantanu Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saunders_W/0/1/0/all/0/1"&gt;William Saunders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hesse_C/0/1/0/all/0/1"&gt;Christopher Hesse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carr_A/0/1/0/all/0/1"&gt;Andrew N. Carr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1"&gt;Jan Leike&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achiam_J/0/1/0/all/0/1"&gt;Josh Achiam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1"&gt;Vedant Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morikawa_E/0/1/0/all/0/1"&gt;Evan Morikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1"&gt;Alec Radford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knight_M/0/1/0/all/0/1"&gt;Matthew Knight&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1"&gt;Miles Brundage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murati_M/0/1/0/all/0/1"&gt;Mira Murati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayer_K/0/1/0/all/0/1"&gt;Katie Mayer&lt;/a&gt;, et al. (6 additional authors not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Round Active Learning. (arXiv:2107.06703v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06703</id>
        <link href="http://arxiv.org/abs/2107.06703"/>
        <updated>2021-07-15T01:59:04.135Z</updated>
        <summary type="html"><![CDATA[Active learning (AL) aims at reducing labeling effort by identifying the most
valuable unlabeled data points from a large pool. Traditional AL frameworks
have two limitations: First, they perform data selection in a multi-round
manner, which is time-consuming and impractical. Second, they usually assume
that there are a small amount of labeled data points available in the same
domain as the data in the unlabeled pool. Recent work proposes a solution for
one-round active learning based on data utility learning and optimization,
which fixes the first issue but still requires the initially labeled data
points in the same domain. In this paper, we propose $\mathrm{D^2ULO}$ as a
solution that solves both issues. Specifically, $\mathrm{D^2ULO}$ leverages the
idea of domain adaptation (DA) to train a data utility model which can
effectively predict the utility for any given unlabeled data in the target
domain once labeled. The trained data utility model can then be used to select
high-utility data and at the same time, provide an estimate for the utility of
the selected data. Our algorithm does not rely on any feedback from annotators
in the target domain and hence, can be used to perform zero-round active
learning or warm-start existing multi-round active learning strategies. Our
experiments show that $\mathrm{D^2ULO}$ outperforms the existing
state-of-the-art AL strategies equipped with domain adaptation over various
domain shift settings (e.g., real-to-real data and synthetic-to-real data).
Particularly, $\mathrm{D^2ULO}$ are applicable to the scenario where source and
target labels have mismatches, which is not supported by the existing works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Si Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ruoxi Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Waveform Learning Through Joint Optimization of Pulse and Constellation Shaping. (arXiv:2106.15158v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15158</id>
        <link href="http://arxiv.org/abs/2106.15158"/>
        <updated>2021-07-15T01:59:04.130Z</updated>
        <summary type="html"><![CDATA[As communication systems are foreseen to enable new services such as joint
communication and sensing and utilize parts of the sub-THz spectrum, the design
of novel waveforms that can support these emerging applications becomes
increasingly challenging. We present in this work an end-to-end learning
approach to design waveforms through joint learning of pulse shaping and
constellation geometry, together with a neural network (NN)-based receiver.
Optimization is performed to maximize an achievable information rate, while
satisfying constraints on out-of-band emission and power envelope. Our results
show that the proposed approach enables up to orders of magnitude smaller
adjacent channel leakage ratios (ACLRs) with peak-to-average power ratios
(PAPRs) competitive with traditional filters, without significant loss of
information rate on an additive white Gaussian noise (AWGN) channel, and no
additional complexity at the transmitter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1"&gt;Fay&amp;#xe7;al Ait Aoudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1"&gt;Jakob Hoydis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental limits and algorithms for sparse linear regression with sublinear sparsity. (arXiv:2101.11156v4 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11156</id>
        <link href="http://arxiv.org/abs/2101.11156"/>
        <updated>2021-07-15T01:59:04.124Z</updated>
        <summary type="html"><![CDATA[We establish exact asymptotic expressions for the normalized mutual
information and minimum mean-square-error (MMSE) of sparse linear regression in
the sub-linear sparsity regime. Our result is achieved by a generalization of
the adaptive interpolation method in Bayesian inference for linear regimes to
sub-linear ones. A modification of the well-known approximate message passing
algorithm to approach the MMSE fundamental limit is also proposed, and its
state evolution is rigorously analyzed. Our results show that the traditional
linear assumption between the signal dimension and number of observations in
the replica and adaptive interpolation methods is not necessary for sparse
signals. They also show how to modify the existing well-known AMP algorithms
for linear regimes to sub-linear ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1"&gt;Lan V. Truong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented Tensor Decomposition with Stochastic Optimization. (arXiv:2106.07900v3 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07900</id>
        <link href="http://arxiv.org/abs/2106.07900"/>
        <updated>2021-07-15T01:59:04.103Z</updated>
        <summary type="html"><![CDATA[Tensor decompositions are powerful tools for dimensionality reduction and
feature interpretation of multidimensional data such as signals. Existing
tensor decomposition objectives (e.g., Frobenius norm) are designed for fitting
raw data under statistical assumptions, which may not align with downstream
classification tasks. Also, real-world tensor data are usually high-ordered and
have large dimensions with millions or billions of entries. Thus, it is
expensive to decompose the whole tensor with traditional algorithms. In
practice, raw tensor data also contains redundant information while data
augmentation techniques may be used to smooth out noise in samples. This paper
addresses the above challenges by proposing augmented tensor decomposition
(ATD), which effectively incorporates data augmentations to boost downstream
classification. To reduce the memory footprint of the decomposition, we propose
a stochastic algorithm that updates the factor matrices in a batch fashion. We
evaluate ATD on multiple signal datasets. It shows comparable or better
performance (e.g., up to 15% in accuracy) over self-supervised and autoencoder
baselines with less than 5% of model parameters, achieves 0.6% ~ 1.3% accuracy
gain over other tensor-based baselines, and reduces the memory footprint by 9X
when compared to standard tensor decomposition algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chaoqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Qian_C/0/1/0/all/0/1"&gt;Cheng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Singh_N/0/1/0/all/0/1"&gt;Navjot Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Cao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Westover_M/0/1/0/all/0/1"&gt;M Brandon Westover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Solomonik_E/0/1/0/all/0/1"&gt;Edgar Solomonik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jimeng Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepMutants: Training neural bug detectors with contextual mutations. (arXiv:2107.06657v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2107.06657</id>
        <link href="http://arxiv.org/abs/2107.06657"/>
        <updated>2021-07-15T01:59:04.096Z</updated>
        <summary type="html"><![CDATA[Learning-based bug detectors promise to find bugs in large code bases by
exploiting natural hints such as names of variables and functions or comments.
Still, existing techniques tend to underperform when presented with realistic
bugs. We believe bug detector learning to currently suffer from a lack of
realistic defective training examples. In fact, real world bugs are scarce
which has driven existing methods to train on artificially created and mostly
unrealistic mutants. In this work, we propose a novel contextual mutation
operator which incorporates knowledge about the mutation context to dynamically
inject natural and more realistic faults into code. Our approach employs a
masked language model to produce a context-dependent distribution over feasible
token replacements. The evaluation shows that sampling from a language model
does not only produce mutants which more accurately represent real bugs but
also lead to better performing bug detectors, both on artificial benchmarks and
on real world source code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Richter_C/0/1/0/all/0/1"&gt;Cedric Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wehrheim_H/0/1/0/all/0/1"&gt;Heike Wehrheim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalisation in Neural Networks Does not Require Feature Overlap. (arXiv:2107.06872v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.06872</id>
        <link href="http://arxiv.org/abs/2107.06872"/>
        <updated>2021-07-15T01:59:04.091Z</updated>
        <summary type="html"><![CDATA[That shared features between train and test data are required for
generalisation in artificial neural networks has been a common assumption of
both proponents and critics of these models. Here, we show that convolutional
architectures avoid this limitation by applying them to two well known
challenges, based on learning the identity function and learning rules
governing sequences of words. In each case, successful performance on the test
set requires generalising to features that were not present in the training
data, which is typically not feasible for standard connectionist models.
However, our experiments demonstrate that neural networks can succeed on such
problems when they incorporate the weight sharing employed by convolutional
architectures. In the image processing domain, such architectures are intended
to reflect the symmetry under spatial translations of the natural world that
such images depict. We discuss the role of symmetry in the two tasks and its
connection to generalisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_J/0/1/0/all/0/1"&gt;Jeff Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowers_J/0/1/0/all/0/1"&gt;Jeffrey S. Bowers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. (arXiv:2103.05630v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05630</id>
        <link href="http://arxiv.org/abs/2103.05630"/>
        <updated>2021-07-15T01:59:04.085Z</updated>
        <summary type="html"><![CDATA[The rapid progress of photorealistic synthesis techniques has reached at a
critical point where the boundary between real and manipulated images starts to
blur. Thus, benchmarking and advancing digital forgery analysis have become a
pressing issue. However, existing face forgery datasets either have limited
diversity or only support coarse-grained analysis. To counter this emerging
threat, we construct the ForgeryNet dataset, an extremely large face forgery
dataset with unified annotations in image- and video-level data across four
tasks: 1) Image Forgery Classification, including two-way (real / fake),
three-way (real / fake with identity-replaced forgery approaches / fake with
identity-remained forgery approaches), and n-way (real and 15 respective
forgery approaches) classification. 2) Spatial Forgery Localization, which
segments the manipulated area of fake images compared to their corresponding
source real images. 3) Video Forgery Classification, which re-defines the
video-level forgery classification with manipulated frames in random positions.
This task is important because attackers in real world are free to manipulate
any target frame. and 4) Temporal Forgery Localization, to localize the
temporal segments which are manipulated. ForgeryNet is by far the largest
publicly available deep face forgery dataset in terms of data-scale (2.9
million images, 221,247 videos), manipulations (7 image-level approaches, 8
video-level approaches), perturbations (36 independent and more mixed
perturbations) and annotations (6.3 million classification labels, 2.9 million
manipulated area annotations and 221,247 temporal forgery segment labels). We
perform extensive benchmarking and studies of existing face forensics methods
and obtain several valuable observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yinan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_B/0/1/0/all/0/1"&gt;Bei Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yichun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1"&gt;Guojun Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Luchuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1"&gt;Lu Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1"&gt;Jing Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothed functional-based gradient algorithms for off-policy reinforcement learning: A non-asymptotic viewpoint. (arXiv:2101.02137v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02137</id>
        <link href="http://arxiv.org/abs/2101.02137"/>
        <updated>2021-07-15T01:59:04.078Z</updated>
        <summary type="html"><![CDATA[We propose two policy gradient algorithms for solving the problem of control
in an off-policy reinforcement learning (RL) context. Both algorithms
incorporate a smoothed functional (SF) based gradient estimation scheme. The
first algorithm is a straightforward combination of importance sampling-based
off-policy evaluation with SF-based gradient estimation. The second algorithm,
inspired by the stochastic variance-reduced gradient (SVRG) algorithm,
incorporates variance reduction in the update iteration. For both algorithms,
we derive non-asymptotic bounds that establish convergence to an approximate
stationary point. From these results, we infer that the first algorithm
converges at a rate that is comparable to the well-known REINFORCE algorithm in
an off-policy RL context, while the second algorithm exhibits an improved rate
of convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vijayan_N/0/1/0/all/0/1"&gt;Nithia Vijayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+A_P/0/1/0/all/0/1"&gt;Prashanth L. A&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Auction: End-to-End Learning of Auction Mechanisms for E-Commerce Advertising. (arXiv:2106.03593v2 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03593</id>
        <link href="http://arxiv.org/abs/2106.03593"/>
        <updated>2021-07-15T01:59:04.062Z</updated>
        <summary type="html"><![CDATA[In e-commerce advertising, it is crucial to jointly consider various
performance metrics, e.g., user experience, advertiser utility, and platform
revenue. Traditional auction mechanisms, such as GSP and VCG auctions, can be
suboptimal due to their fixed allocation rules to optimize a single performance
metric (e.g., revenue or social welfare). Recently, data-driven auctions,
learned directly from auction outcomes to optimize multiple performance
metrics, have attracted increasing research interests. However, the procedure
of auction mechanisms involves various discrete calculation operations, making
it challenging to be compatible with continuous optimization pipelines in
machine learning. In this paper, we design \underline{D}eep \underline{N}eural
\underline{A}uctions (DNAs) to enable end-to-end auction learning by proposing
a differentiable model to relax the discrete sorting operation, a key component
in auctions. We optimize the performance metrics by developing deep models to
efficiently extract contexts from auctions, providing rich features for auction
design. We further integrate the game theoretical conditions within the model
design, to guarantee the stability of the auctions. DNAs have been successfully
deployed in the e-commerce advertising system at Taobao. Experimental
evaluation results on both large-scale data set as well as online A/B test
demonstrated that DNAs significantly outperformed other mechanisms widely
adopted in industry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhilin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhenzhe Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1"&gt;Hongtao Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1"&gt;Da Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dagui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guihai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ATTACC the Quadratic Bottleneck of Attention Layers. (arXiv:2107.06419v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06419</id>
        <link href="http://arxiv.org/abs/2107.06419"/>
        <updated>2021-07-15T01:59:04.056Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms form the backbone of state-of-the-art machine learning
models for a variety of tasks. Deploying them on deep neural network (DNN)
accelerators, however, is prohibitively challenging especially under long
sequences. Operators in attention layers exhibit limited reuse and quadratic
growth in memory footprint, leading to severe memory-boundedness. This paper
introduces a new attention-tailored dataflow, termed FLAT, which leverages
operator fusion, loop-nest optimizations, and interleaved execution. It
increases the effective memory bandwidth by efficiently utilizing the
high-bandwidth, low-capacity on-chip buffer and thus achieves better run time
and compute resource utilization. We term FLAT-compatible accelerators ATTACC.
In our evaluation, ATTACC achieves 1.94x and 1.76x speedup and 49% and 42% of
energy reduction comparing to state-of-the-art edge and cloud accelerators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kao_S/0/1/0/all/0/1"&gt;Sheng-Chun Kao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1"&gt;Suvinay Subramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_G/0/1/0/all/0/1"&gt;Gaurav Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1"&gt;Tushar Krishna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disparity Between Batches as a Signal for Early Stopping. (arXiv:2107.06665v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06665</id>
        <link href="http://arxiv.org/abs/2107.06665"/>
        <updated>2021-07-15T01:59:04.050Z</updated>
        <summary type="html"><![CDATA[We propose a metric for evaluating the generalization ability of deep neural
networks trained with mini-batch gradient descent. Our metric, called gradient
disparity, is the $\ell_2$ norm distance between the gradient vectors of two
mini-batches drawn from the training set. It is derived from a probabilistic
upper bound on the difference between the classification errors over a given
mini-batch, when the network is trained on this mini-batch and when the network
is trained on another mini-batch of points sampled from the same dataset. We
empirically show that gradient disparity is a very promising early-stopping
criterion (i) when data is limited, as it uses all the samples for training and
(ii) when available data has noisy labels, as it signals overfitting better
than the validation data. Furthermore, we show in a wide range of experimental
settings that gradient disparity is strongly related to the generalization
error between the training and test sets, and that it is also very informative
about the level of label noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Forouzesh_M/0/1/0/all/0/1"&gt;Mahsa Forouzesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thiran_P/0/1/0/all/0/1"&gt;Patrick Thiran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Spiking Dynamics with Spatial-temporal Feature Normalization in Graph Learning. (arXiv:2107.06865v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.06865</id>
        <link href="http://arxiv.org/abs/2107.06865"/>
        <updated>2021-07-15T01:59:04.044Z</updated>
        <summary type="html"><![CDATA[Biological spiking neurons with intrinsic dynamics underlie the powerful
representation and learning capabilities of the brain for processing multimodal
information in complex environments. Despite recent tremendous progress in
spiking neural networks (SNNs) for handling Euclidean-space tasks, it still
remains challenging to exploit SNNs in processing non-Euclidean-space data
represented by graph data, mainly due to the lack of effective modeling
framework and useful training techniques. Here we present a general spike-based
modeling framework that enables the direct training of SNNs for graph learning.
Through spatial-temporal unfolding for spiking data flows of node features, we
incorporate graph convolution filters into spiking dynamics and formalize a
synergistic learning paradigm. Considering the unique features of spike
representation and spiking dynamics, we propose a spatial-temporal feature
normalization (STFN) technique suitable for SNN to accelerate convergence. We
instantiate our methods into two spiking graph models, including graph
convolution SNNs and graph attention SNNs, and validate their performance on
three node-classification benchmarks, including Cora, Citeseer, and Pubmed. Our
model can achieve comparable performance with the state-of-the-art graph neural
network (GNN) models with much lower computation costs, demonstrating great
benefits for the execution on neuromorphic hardware and prompting neuromorphic
applications in graphical scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingkun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yujie Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1"&gt;Lei Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Faqiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guoqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1"&gt;Jing Pei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextual Games: Multi-Agent Learning with Side Information. (arXiv:2107.06327v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2107.06327</id>
        <link href="http://arxiv.org/abs/2107.06327"/>
        <updated>2021-07-15T01:59:04.038Z</updated>
        <summary type="html"><![CDATA[We formulate the novel class of contextual games, a type of repeated games
driven by contextual information at each round. By means of kernel-based
regularity assumptions, we model the correlation between different contexts and
game outcomes and propose a novel online (meta) algorithm that exploits such
correlations to minimize the contextual regret of individual players. We define
game-theoretic notions of contextual Coarse Correlated Equilibria (c-CCE) and
optimal contextual welfare for this new class of games and show that c-CCEs and
optimal welfare can be approached whenever players' contextual regrets vanish.
Finally, we empirically validate our results in a traffic routing experiment,
where our algorithm leads to better performance and higher welfare compared to
baselines that do not exploit the available contextual information or the
correlations present in the game.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sessa_P/0/1/0/all/0/1"&gt;Pier Giuseppe Sessa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bogunovic_I/0/1/0/all/0/1"&gt;Ilija Bogunovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamgarpour_M/0/1/0/all/0/1"&gt;Maryam Kamgarpour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AID-Purifier: A Light Auxiliary Network for Boosting Adversarial Defense. (arXiv:2107.06456v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06456</id>
        <link href="http://arxiv.org/abs/2107.06456"/>
        <updated>2021-07-15T01:59:04.021Z</updated>
        <summary type="html"><![CDATA[We propose an AID-purifier that can boost the robustness of
adversarially-trained networks by purifying their inputs. AID-purifier is an
auxiliary network that works as an add-on to an already trained main
classifier. To keep it computationally light, it is trained as a discriminator
with a binary cross-entropy loss. To obtain additionally useful information
from the adversarial examples, the architecture design is closely related to
information maximization principles where two layers of the main classification
network are piped to the auxiliary network. To assist the iterative
optimization procedure of purification, the auxiliary network is trained with
AVmixup. AID-purifier can be used together with other purifiers such as
PixelDefend for an extra enhancement. The overall results indicate that the
best performing adversarially-trained networks can be enhanced by the best
performing purification networks, where AID-purifier is a competitive candidate
that is light and robust.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1"&gt;Duhun Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1"&gt;Eunjung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1"&gt;Wonjong Rhee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-time simulations with high fidelity on quantum hardware. (arXiv:2102.04313v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04313</id>
        <link href="http://arxiv.org/abs/2102.04313"/>
        <updated>2021-07-15T01:59:04.014Z</updated>
        <summary type="html"><![CDATA[Moderate-size quantum computers are now publicly accessible over the cloud,
opening the exciting possibility of performing dynamical simulations of quantum
systems. However, while rapidly improving, these devices have short coherence
times, limiting the depth of algorithms that may be successfully implemented.
Here we demonstrate that, despite these limitations, it is possible to
implement long-time, high fidelity simulations on current hardware.
Specifically, we simulate an XY-model spin chain on the Rigetti and IBM quantum
computers, maintaining a fidelity of at least 0.9 for over 600 time steps. This
is a factor of 150 longer than is possible using the iterated Trotter method.
Our simulations are performed using a new algorithm that we call the fixed
state Variational Fast Forwarding (fsVFF) algorithm. This algorithm decreases
the circuit depth and width required for a quantum simulation by finding an
approximate diagonalization of a short time evolution unitary. Crucially, fsVFF
only requires finding a diagonalization on the subspace spanned by the initial
state, rather than on the total Hilbert space as with previous methods,
substantially reducing the required resources. We further demonstrate the
viability of fsVFF through large numerical implementations of the algorithm, as
well as an analysis of its noise resilience and the scaling of simulation
errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gibbs_J/0/1/0/all/0/1"&gt;Joe Gibbs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gili_K/0/1/0/all/0/1"&gt;Kaitlin Gili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Holmes_Z/0/1/0/all/0/1"&gt;Zo&amp;#xeb; Holmes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Commeau_B/0/1/0/all/0/1"&gt;Benjamin Commeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Arrasmith_A/0/1/0/all/0/1"&gt;Andrew Arrasmith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cincio_L/0/1/0/all/0/1"&gt;Lukasz Cincio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1"&gt;Patrick J. Coles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sornborger_A/0/1/0/all/0/1"&gt;Andrew Sornborger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Only Write Thrice: Creating Documents, Computational Notebooks and Presentations From a Single Source. (arXiv:2107.06639v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2107.06639</id>
        <link href="http://arxiv.org/abs/2107.06639"/>
        <updated>2021-07-15T01:59:04.007Z</updated>
        <summary type="html"><![CDATA[Academic trade requires juggling multiple variants of the same content
published in different formats: manuscripts, presentations, posters and
computational notebooks. The need to track versions to accommodate for the
write--review--rebut--revise life-cycle adds another layer of complexity. We
propose to significantly reduce this burden by maintaining a single source
document in a version-controlled environment (such as git), adding
functionality to generate a collection of output formats popular in academia.
To this end, we utilise various open-source tools from the Jupyter scientific
computing ecosystem and operationalise selected software engineering concepts.
We offer a proof-of-concept workflow that composes Jupyter Book (an online
document), Jupyter Notebook (a computational narrative) and reveal.js slides
from a single markdown source file. Hosted on GitHub, our approach supports
change tracking and versioning, as well as a transparent review process based
on the underlying code issue management infrastructure. An exhibit of our
workflow can be previewed at https://so-cool.github.io/you-only-write-thrice/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sokol_K/0/1/0/all/0/1"&gt;Kacper Sokol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flach_P/0/1/0/all/0/1"&gt;Peter Flach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Parallel Model Selection for Deep Learning Systems. (arXiv:2107.06469v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.06469</id>
        <link href="http://arxiv.org/abs/2107.06469"/>
        <updated>2021-07-15T01:59:03.992Z</updated>
        <summary type="html"><![CDATA[As deep learning becomes more expensive, both in terms of time and compute,
inefficiencies in machine learning (ML) training prevent practical usage of
state-of-the-art models for most users. The newest model architectures are
simply too large to be fit onto a single processor. To address the issue, many
ML practitioners have turned to model parallelism as a method of distributing
the computational requirements across several devices. Unfortunately, the
sequential nature of neural networks causes very low efficiency and device
utilization in model parallel training jobs. We propose a new form of "shard
parallelism" combining task and model parallelism, then package it into a
framework we name Hydra. Hydra recasts the problem of model parallelism in the
multi-model context to produce a fine-grained parallel workload of independent
model shards, rather than independent models. This new parallel design promises
dramatic speedups relative to the traditional model parallelism paradigm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagrecha_K/0/1/0/all/0/1"&gt;Kabir Nagrecha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interval Universal Approximation for Neural Networks. (arXiv:2007.06093v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06093</id>
        <link href="http://arxiv.org/abs/2007.06093"/>
        <updated>2021-07-15T01:59:03.984Z</updated>
        <summary type="html"><![CDATA[To verify safety and robustness of neural networks, researchers have
successfully applied abstract interpretation, primarily using the interval
abstract domain. In this paper, we study the theoretical power and limits of
the interval domain for neural-network verification.

First, we introduce the interval universal approximation (IUA) theorem. IUA
shows that neural networks not only can approximate any continuous function $f$
(universal approximation) as we have known for decades, but we can find a
neural network, using any well-behaved activation function, whose interval
bounds are an arbitrarily close approximation of the set semantics of $f$ (the
result of applying $f$ to a set of inputs). We call this notion of
approximation interval approximation. Our theorem generalizes the recent result
of Baader et al. (2020) from ReLUs to a rich class of activation functions that
we call squashable functions. Additionally, the IUA theorem implies that we can
always construct provably robust neural networks under $\ell_\infty$-norm using
almost any practical activation function.

Second, we study the computational complexity of constructing neural networks
that are amenable to precise interval analysis. This is a crucial question, as
our constructive proof of IUA is exponential in the size of the approximation
domain. We boil this question down to the problem of approximating the range of
a neural network with squashable activation functions. We show that the range
approximation problem (RA) is a $\Delta_2$-intermediate problem, which is
strictly harder than $\mathsf{NP}$-complete problems, assuming
$\mathsf{coNP}\not\subset \mathsf{NP}$. As a result, IUA is an inherently hard
problem: No matter what abstract domain or computational tools we consider to
achieve interval approximation, there is no efficient construction of such a
universal approximator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albarghouthi_A/0/1/0/all/0/1"&gt;Aws Albarghouthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakriya_G/0/1/0/all/0/1"&gt;Gautam Prakriya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text-to-hashtag Generation using Seq2seq Learning. (arXiv:2102.00904v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00904</id>
        <link href="http://arxiv.org/abs/2102.00904"/>
        <updated>2021-07-15T01:59:03.938Z</updated>
        <summary type="html"><![CDATA[In this paper, we studied whether models based on BiLSTM and BERT can predict
hashtags in Brazilian Portuguese for Ecommerce websites. Hashtags have a
sizable financial impact on Ecommerce. We processed a corpus of Ecommerce
reviews as inputs, and predicted hashtags as outputs. We evaluated the results
using four quantitative metrics: NIST, BLEU, METEOR and a crowdsourced score. A
word cloud was used as a qualitative metric. While all computer-generated
metrics (NIST, BLEU and METEOR) indicated bad results, the crowdsourced results
produced amazing scores. We concluded that the texts predicted by the neural
networks are very promising for use as hashtags for products on Ecommerce
websites. The code for this work is available at
https://github.com/augustocamargo/text-to-hashtag.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Camargo_A/0/1/0/all/0/1"&gt;Augusto Camargo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_W/0/1/0/all/0/1"&gt;Wesley Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peressim_F/0/1/0/all/0/1"&gt;Felipe Peressim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_A/0/1/0/all/0/1"&gt;Alan Barzilay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finger_M/0/1/0/all/0/1"&gt;Marcelo Finger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Higgs Boson Classification: Brain-inspired BCPNN Learning with StreamBrain. (arXiv:2107.06676v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06676</id>
        <link href="http://arxiv.org/abs/2107.06676"/>
        <updated>2021-07-15T01:59:03.932Z</updated>
        <summary type="html"><![CDATA[One of the most promising approaches for data analysis and exploration of
large data sets is Machine Learning techniques that are inspired by brain
models. Such methods use alternative learning rules potentially more
efficiently than established learning rules. In this work, we focus on the
potential of brain-inspired ML for exploiting High-Performance Computing (HPC)
resources to solve ML problems: we discuss the BCPNN and an HPC implementation,
called StreamBrain, its computational cost, suitability to HPC systems. As an
example, we use StreamBrain to analyze the Higgs Boson dataset from High Energy
Physics and discriminate between background and signal classes in collisions of
high-energy particle colliders. Overall, we reach up to 69.15% accuracy and
76.4% Area Under the Curve (AUC) performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Svedin_M/0/1/0/all/0/1"&gt;Martin Svedin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Podobas_A/0/1/0/all/0/1"&gt;Artur Podobas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1"&gt;Steven W. D. Chien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markidis_S/0/1/0/all/0/1"&gt;Stefano Markidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TEACHING -- Trustworthy autonomous cyber-physical applications through human-centred intelligence. (arXiv:2107.06543v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.06543</id>
        <link href="http://arxiv.org/abs/2107.06543"/>
        <updated>2021-07-15T01:59:03.922Z</updated>
        <summary type="html"><![CDATA[This paper discusses the perspective of the H2020 TEACHING project on the
next generation of autonomous applications running in a distributed and highly
heterogeneous environment comprising both virtual and physical resources
spanning the edge-cloud continuum. TEACHING puts forward a human-centred vision
leveraging the physiological, emotional, and cognitive state of the users as a
driver for the adaptation and optimization of the autonomous applications. It
does so by building a distributed, embedded and federated learning system
complemented by methods and tools to enforce its dependability, security and
privacy preservation. The paper discusses the main concepts of the TEACHING
approach and singles out the main AI-related research challenges associated
with it. Further, we provide a discussion of the design choices for the
TEACHING system to tackle the aforementioned challenges]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akarmazyan_S/0/1/0/all/0/1"&gt;Siranush Akarmazyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armengaud_E/0/1/0/all/0/1"&gt;Eric Armengaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacco_M/0/1/0/all/0/1"&gt;Manlio Bacco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bravos_G/0/1/0/all/0/1"&gt;George Bravos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calandra_C/0/1/0/all/0/1"&gt;Calogero Calandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_E/0/1/0/all/0/1"&gt;Emanuele Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1"&gt;Antonio Carta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cassara_P/0/1/0/all/0/1"&gt;Pietro Cassara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coppola_M/0/1/0/all/0/1"&gt;Massimo Coppola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davalas_C/0/1/0/all/0/1"&gt;Charalampos Davalas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dazzi_P/0/1/0/all/0/1"&gt;Patrizio Dazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Degennaro_M/0/1/0/all/0/1"&gt;Maria Carmela Degennaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarli_D/0/1/0/all/0/1"&gt;Daniele Di Sarli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dobaj_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Dobaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1"&gt;Claudio Gallicchio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girbal_S/0/1/0/all/0/1"&gt;Sylvain Girbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gotta_A/0/1/0/all/0/1"&gt;Alberto Gotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Groppo_R/0/1/0/all/0/1"&gt;Riccardo Groppo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1"&gt;Vincenzo Lomonaco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macher_G/0/1/0/all/0/1"&gt;Georg Macher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazzei_D/0/1/0/all/0/1"&gt;Daniele Mazzei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mencagli_G/0/1/0/all/0/1"&gt;Gabriele Mencagli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michail_D/0/1/0/all/0/1"&gt;Dimitrios Michail&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1"&gt;Alessio Micheli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peroglio_R/0/1/0/all/0/1"&gt;Roberta Peroglio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petroni_S/0/1/0/all/0/1"&gt;Salvatore Petroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potenza_R/0/1/0/all/0/1"&gt;Rosaria Potenza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pourdanesh_F/0/1/0/all/0/1"&gt;Farank Pourdanesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sardianos_C/0/1/0/all/0/1"&gt;Christos Sardianos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tserpes_K/0/1/0/all/0/1"&gt;Konstantinos Tserpes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliabo_F/0/1/0/all/0/1"&gt;Fulvio Tagliab&amp;#xf2;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valtl_J/0/1/0/all/0/1"&gt;Jakob Valtl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varlamis_I/0/1/0/all/0/1"&gt;Iraklis Varlamis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veledar_O/0/1/0/all/0/1"&gt;Omar Veledar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Referring Transformer: A One-step Approach to Multi-task Visual Grounding. (arXiv:2106.03089v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03089</id>
        <link href="http://arxiv.org/abs/2106.03089"/>
        <updated>2021-07-15T01:59:03.915Z</updated>
        <summary type="html"><![CDATA[As an important step towards visual reasoning, visual grounding (e.g., phrase
localization, referring expression comprehension/segmentation) has been widely
explored Previous approaches to referring expression comprehension (REC) or
segmentation (RES) either suffer from limited performance, due to a two-stage
setup, or require the designing of complex task-specific one-stage
architectures. In this paper, we propose a simple one-stage multi-task
framework for visual grounding tasks. Specifically, we leverage a transformer
architecture, where two modalities are fused in a visual-lingual encoder. In
the decoder, the model learns to generate contextualized lingual queries which
are then decoded and used to directly regress the bounding box and produce a
segmentation mask for the corresponding referred regions. With this simple but
highly contextualized model, we outperform state-of-the-arts methods by a large
margin on both REC and RES tasks. We also show that a simple pre-training
schedule (on an external dataset) further improves the performance. Extensive
experiments and ablations illustrate that our model benefits greatly from
contextualized information and multi-task training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Muchen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1"&gt;Leonid Sigal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Many-to-Many Voice Conversion based Feature Disentanglement using Variational Autoencoder. (arXiv:2107.06642v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.06642</id>
        <link href="http://arxiv.org/abs/2107.06642"/>
        <updated>2021-07-15T01:59:03.909Z</updated>
        <summary type="html"><![CDATA[Voice conversion is a challenging task which transforms the voice
characteristics of a source speaker to a target speaker without changing
linguistic content. Recently, there have been many works on many-to-many Voice
Conversion (VC) based on Variational Autoencoder (VAEs) achieving good results,
however, these methods lack the ability to disentangle speaker identity and
linguistic content to achieve good performance on unseen speaker scenarios. In
this paper, we propose a new method based on feature disentanglement to tackle
many to many voice conversion. The method has the capability to disentangle
speaker identity and linguistic content from utterances, it can convert from
many source speakers to many target speakers with a single autoencoder network.
Moreover, it naturally deals with the unseen target speaker scenarios. We
perform both objective and subjective evaluations to show the competitive
performance of our proposed method compared with other state-of-the-art models
in terms of naturalness and target speaker similarity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Luong_M/0/1/0/all/0/1"&gt;Manh Luong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tran_V/0/1/0/all/0/1"&gt;Viet Anh Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oblivious sketching for logistic regression. (arXiv:2107.06615v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.06615</id>
        <link href="http://arxiv.org/abs/2107.06615"/>
        <updated>2021-07-15T01:59:03.883Z</updated>
        <summary type="html"><![CDATA[What guarantees are possible for solving logistic regression in one pass over
a data stream? To answer this question, we present the first data oblivious
sketch for logistic regression. Our sketch can be computed in input sparsity
time over a turnstile data stream and reduces the size of a $d$-dimensional
data set from $n$ to only $\operatorname{poly}(\mu d\log n)$ weighted points,
where $\mu$ is a useful parameter which captures the complexity of compressing
the data. Solving (weighted) logistic regression on the sketch gives an $O(\log
n)$-approximation to the original problem on the full data set. We also show
how to obtain an $O(1)$-approximation with slight modifications. Our sketches
are fast, simple, easy to implement, and our experiments demonstrate their
practicality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Munteanu_A/0/1/0/all/0/1"&gt;Alexander Munteanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omlor_S/0/1/0/all/0/1"&gt;Simon Omlor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimality of the Johnson-Lindenstrauss Dimensionality Reduction for Practical Measures. (arXiv:2107.06626v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.06626</id>
        <link href="http://arxiv.org/abs/2107.06626"/>
        <updated>2021-07-15T01:59:03.876Z</updated>
        <summary type="html"><![CDATA[It is well known that the Johnson-Lindenstrauss dimensionality reduction
method is optimal for worst case distortion. While in practice many other
methods and heuristics are used, not much is known in terms of bounds on their
performance. The question of whether the JL method is optimal for practical
measures of distortion was recently raised in \cite{BFN19} (NeurIPS'19). They
provided upper bounds on its quality for a wide range of practical measures and
showed that indeed these are best possible in many cases. Yet, some of the most
important cases, including the fundamental case of average distortion were left
open. In particular, they show that the JL transform has $1+\epsilon$ average
distortion for embedding into $k$-dimensional Euclidean space, where
$k=O(1/\eps^2)$, and for more general $q$-norms of distortion, $k =
O(\max\{1/\eps^2,q/\eps\})$, whereas tight lower bounds were established only
for large values of $q$ via reduction to the worst case.

In this paper we prove that these bounds are best possible for any
dimensionality reduction method, for any $1 \leq q \leq O(\frac{\log (2\eps^2
n)}{\eps})$ and $\epsilon \geq \frac{1}{\sqrt{n}}$, where $n$ is the size of
the subset of Euclidean space.

Our results imply that the JL method is optimal for various distortion
measures commonly used in practice, such as {\it stress, energy} and {\it
relative error}. We prove that if any of these measures is bounded by $\eps$
then $k=\Omega(1/\eps^2)$, for any $\epsilon \geq \frac{1}{\sqrt{n}}$, matching
the upper bounds of \cite{BFN19} and extending their tightness results for the
full range moment analysis.

Our results may indicate that the JL dimensionality reduction method should
be considered more often in practical applications, and the bounds we provide
for its quality should be served as a measure for comparison when evaluating
the performance of other methods and heuristics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bartal_Y/0/1/0/all/0/1"&gt;Yair Bartal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fandina_O/0/1/0/all/0/1"&gt;Ora Nova Fandina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1"&gt;Kasper Green Larsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnability of Learning Performance and Its Application to Data Valuation. (arXiv:2107.06336v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06336</id>
        <link href="http://arxiv.org/abs/2107.06336"/>
        <updated>2021-07-15T01:59:03.863Z</updated>
        <summary type="html"><![CDATA[For most machine learning (ML) tasks, evaluating learning performance on a
given dataset requires intensive computation. On the other hand, the ability to
efficiently estimate learning performance may benefit a wide spectrum of
applications, such as active learning, data quality management, and data
valuation. Recent empirical studies show that for many common ML models, one
can accurately learn a parametric model that predicts learning performance for
any given input datasets using a small amount of samples. However, the
theoretical underpinning of the learnability of such performance prediction
models is still missing. In this work, we develop the first theoretical
analysis of the ML performance learning problem. We propose a relaxed notion
for submodularity that can well describe the behavior of learning performance
as a function of input datasets. We give a learning algorithm that achieves a
constant-factor approximation under certain assumptions. Further, we give a
learning algorithm that achieves arbitrarily small error based on a newly
derived structural result. We then discuss a natural, important use case of
learning performance learning -- data valuation, which is known to suffer
computational challenges due to the requirement of estimating learning
performance for many data combinations. We show that performance learning can
significantly improve the accuracy of data valuation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ruoxi Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distance-based Hyperspherical Classification for Multi-source Open-Set Domain Adaptation. (arXiv:2107.02067v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02067</id>
        <link href="http://arxiv.org/abs/2107.02067"/>
        <updated>2021-07-15T01:59:03.857Z</updated>
        <summary type="html"><![CDATA[Vision systems trained in closed-world scenarios will inevitably fail when
presented with new environmental conditions, new data distributions and novel
classes at deployment time. How to move towards open-world learning is a long
standing research question, but the existing solutions mainly focus on specific
aspects of the problem (single domain Open-Set, multi-domain Closed-Set), or
propose complex strategies which combine multiple losses and manually tuned
hyperparameters. In this work we tackle multi-source Open-Set domain adaptation
by introducing HyMOS: a straightforward supervised model that exploits the
power of contrastive learning and the properties of its hyperspherical feature
space to correctly predict known labels on the target, while rejecting samples
belonging to any unknown class. HyMOS includes a tailored data balancing to
enforce cross-source alignment and introduces style transfer among the instance
transformations of contrastive learning for source-target adaptation, avoiding
the risk of negative transfer. Finally a self-training strategy refines the
model without the need for handcrafted thresholds. We validate our method over
three challenging datasets and provide an extensive quantitative and
qualitative experimental analysis. The obtained results show that HyMOS
outperforms several Open-Set and universal domain adaptation approaches,
defining the new state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bucci_S/0/1/0/all/0/1"&gt;Silvia Bucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borlino_F/0/1/0/all/0/1"&gt;Francesco Cappio Borlino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1"&gt;Barbara Caputo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1"&gt;Tatiana Tommasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Note on Learning Rare Events in Molecular Dynamics using LSTM and Transformer. (arXiv:2107.06573v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.06573</id>
        <link href="http://arxiv.org/abs/2107.06573"/>
        <updated>2021-07-15T01:59:03.848Z</updated>
        <summary type="html"><![CDATA[Recurrent neural networks for language models like long short-term memory
(LSTM) have been utilized as a tool for modeling and predicting long term
dynamics of complex stochastic molecular systems. Recently successful examples
on learning slow dynamics by LSTM are given with simulation data of low
dimensional reaction coordinate. However, in this report we show that the
following three key factors significantly affect the performance of language
model learning, namely dimensionality of reaction coordinates, temporal
resolution and state partition. When applying recurrent neural networks to
molecular dynamics simulation trajectories of high dimensionality, we find that
rare events corresponding to the slow dynamics might be obscured by other
faster dynamics of the system, and cannot be efficiently learned. Under such
conditions, we find that coarse graining the conformational space into
metastable states and removing recrossing events when estimating transition
probabilities between states could greatly help improve the accuracy of slow
dynamics learning in molecular dynamics. Moreover, we also explore other models
like Transformer, which do not show superior performance than LSTM in
overcoming these issues. Therefore, to learn rare events of slow molecular
dynamics by LSTM and Transformer, it is critical to choose proper temporal
resolution (i.e., saving intervals of MD simulation trajectories) and state
partition in high resolution data, since deep neural network models might not
automatically disentangle slow dynamics from fast dynamics when both are
present in data influencing each other.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenqi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Siqin Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuhui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuan Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Much Can CLIP Benefit Vision-and-Language Tasks?. (arXiv:2107.06383v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06383</id>
        <link href="http://arxiv.org/abs/2107.06383"/>
        <updated>2021-07-15T01:59:03.826Z</updated>
        <summary type="html"><![CDATA[Most existing Vision-and-Language (V&L) models rely on pre-trained visual
encoders, using a relatively small set of manually-annotated data (as compared
to web-crawled data), to perceive the visual world. However, it has been
observed that large-scale pretraining usually can result in better
generalization performance, e.g., CLIP (Contrastive Language-Image
Pre-training), trained on a massive amount of image-caption pairs, has shown a
strong zero-shot capability on various vision tasks. To further study the
advantage brought by CLIP, we propose to use CLIP as the visual encoder in
various V&L models in two typical scenarios: 1) plugging CLIP into
task-specific fine-tuning; 2) combining CLIP with V&L pre-training and
transferring to downstream tasks. We show that CLIP significantly outperforms
widely-used visual encoders trained with in-domain annotated data, such as
BottomUp-TopDown. We achieve competitive or better results on diverse V&L
tasks, while establishing new state-of-the-art results on Visual Question
Answering, Visual Entailment, and V&L Navigation tasks. We release our code at
https://github.com/clip-vil/CLIP-ViL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Sheng Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liunian Harold Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1"&gt;Anna Rohrbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IFedAvg: Interpretable Data-Interoperability for Federated Learning. (arXiv:2107.06580v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06580</id>
        <link href="http://arxiv.org/abs/2107.06580"/>
        <updated>2021-07-15T01:59:03.817Z</updated>
        <summary type="html"><![CDATA[Recently, the ever-growing demand for privacy-oriented machine learning has
motivated researchers to develop federated and decentralized learning
techniques, allowing individual clients to train models collaboratively without
disclosing their private datasets. However, widespread adoption has been
limited in domains relying on high levels of user trust, where assessment of
data compatibility is essential. In this work, we define and address low
interoperability induced by underlying client data inconsistencies in federated
learning for tabular data. The proposed method, iFedAvg, builds on federated
averaging adding local element-wise affine layers to allow for a personalized
and granular understanding of the collaborative learning process. Thus,
enabling the detection of outlier datasets in the federation and also learning
the compensation for local data distribution shifts without sharing any
original data. We evaluate iFedAvg using several public benchmarks and a
previously unstudied collection of real-world datasets from the 2014 - 2016
West African Ebola epidemic, jointly forming the largest such dataset in the
world. In all evaluations, iFedAvg achieves competitive average performance
with negligible overhead. It additionally shows substantial improvement on
outlier clients, highlighting increased robustness to individual dataset
shifts. Most importantly, our method provides valuable client-specific insights
at a fine-grained level to guide interoperable federated learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roschewitz_D/0/1/0/all/0/1"&gt;David Roschewitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartley_M/0/1/0/all/0/1"&gt;Mary-Anne Hartley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corinzia_L/0/1/0/all/0/1"&gt;Luca Corinzia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Jigsaw Learning for Cartoon Face Recognition. (arXiv:2107.06532v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06532</id>
        <link href="http://arxiv.org/abs/2107.06532"/>
        <updated>2021-07-15T01:59:03.810Z</updated>
        <summary type="html"><![CDATA[Cartoon face recognition is challenging as they typically have smooth color
regions and emphasized edges, the key to recognize cartoon faces is to
precisely perceive their sparse and critical shape patterns. However, it is
quite difficult to learn a shape-oriented representation for cartoon face
recognition with convolutional neural networks (CNNs). To mitigate this issue,
we propose the GraphJigsaw that constructs jigsaw puzzles at various stages in
the classification network and solves the puzzles with the graph convolutional
network (GCN) in a progressive manner. Solving the puzzles requires the model
to spot the shape patterns of the cartoon faces as the texture information is
quite limited. The key idea of GraphJigsaw is constructing a jigsaw puzzle by
randomly shuffling the intermediate convolutional feature maps in the spatial
dimension and exploiting the GCN to reason and recover the correct layout of
the jigsaw fragments in a self-supervised manner. The proposed GraphJigsaw
avoids training the classification model with the deconstructed images that
would introduce noisy patterns and are harmful for the final classification.
Specially, GraphJigsaw can be incorporated at various stages in a top-down
manner within the classification model, which facilitates propagating the
learned shape patterns gradually. GraphJigsaw does not rely on any extra manual
annotation during the training process and incorporates no extra computation
burden at inference time. Both quantitative and qualitative experimental
results have verified the feasibility of our proposed GraphJigsaw, which
consistently outperforms other face recognition or jigsaw-based methods on two
popular cartoon face datasets with considerable improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lao_L/0/1/0/all/0/1"&gt;Lingjie Lao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shiguang Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Style-Restricted GAN: Multi-Modal Translation with Style Restriction Using Generative Adversarial Networks. (arXiv:2105.07621v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07621</id>
        <link href="http://arxiv.org/abs/2105.07621"/>
        <updated>2021-07-15T01:59:03.804Z</updated>
        <summary type="html"><![CDATA[Unpaired image-to-image translation using Generative Adversarial Networks
(GAN) is successful in converting images among multiple domains. Moreover,
recent studies have shown a way to diversify the outputs of the generator.
However, since there are no restrictions on how the generator diversifies the
results, it is likely to translate some unexpected features. In this paper, we
propose Style-Restricted GAN (SRGAN) to demonstrate the importance of
controlling the encoded features used in style diversifying process. More
specifically, instead of KL divergence loss, we adopt three new losses to
restrict the distribution of the encoded features: batch KL divergence loss,
correlation loss, and histogram imitation loss. Further, the encoder is
pre-trained with classification tasks before being used in translation process.
The study reports quantitative as well as qualitative results with Precision,
Recall, Density, and Coverage. The proposed three losses lead to the
enhancement of the level of diversity compared to the conventional KL loss. In
particular, SRGAN is found to be successful in translating with higher
diversity and without changing the class-unrelated features in the CelebA face
dataset. To conclude, the importance of the encoded features being
well-regulated was proven with two experiments. Our implementation is available
at https://github.com/shinshoji01/Style-Restricted_GAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Inoue_S/0/1/0/all/0/1"&gt;Sho Inoue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonsalves_T/0/1/0/all/0/1"&gt;Tad Gonsalves&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative and reproducible benchmarks for comprehensive evaluation of machine learning classifiers. (arXiv:2107.06475v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06475</id>
        <link href="http://arxiv.org/abs/2107.06475"/>
        <updated>2021-07-15T01:59:03.790Z</updated>
        <summary type="html"><![CDATA[Understanding the strengths and weaknesses of machine learning (ML)
algorithms is crucial for determine their scope of application. Here, we
introduce the DIverse and GENerative ML Benchmark (DIGEN) - a collection of
synthetic datasets for comprehensive, reproducible, and interpretable
benchmarking of machine learning algorithms for classification of binary
outcomes. The DIGEN resource consists of 40 mathematical functions which map
continuous features to discrete endpoints for creating synthetic datasets.
These 40 functions were discovered using a heuristic algorithm designed to
maximize the diversity of performance among multiple popular machine learning
algorithms thus providing a useful test suite for evaluating and comparing new
methods. Access to the generative functions facilitates understanding of why a
method performs poorly compared to other algorithms thus providing ideas for
improvement. The resource with extensive documentation and analyses is
open-source and available on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orzechowski_P/0/1/0/all/0/1"&gt;Patryk Orzechowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1"&gt;Jason H. Moore&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shortest-Path Constrained Reinforcement Learning for Sparse Reward Tasks. (arXiv:2107.06405v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06405</id>
        <link href="http://arxiv.org/abs/2107.06405"/>
        <updated>2021-07-15T01:59:03.772Z</updated>
        <summary type="html"><![CDATA[We propose the k-Shortest-Path (k-SP) constraint: a novel constraint on the
agent's trajectory that improves the sample efficiency in sparse-reward MDPs.
We show that any optimal policy necessarily satisfies the k-SP constraint.
Notably, the k-SP constraint prevents the policy from exploring state-action
pairs along the non-k-SP trajectories (e.g., going back and forth). However, in
practice, excluding state-action pairs may hinder the convergence of RL
algorithms. To overcome this, we propose a novel cost function that penalizes
the policy violating SP constraint, instead of completely excluding it. Our
numerical experiment in a tabular RL setting demonstrates that the SP
constraint can significantly reduce the trajectory space of policy. As a
result, our constraint enables more sample efficient learning by suppressing
redundant exploration and exploitation. Our experiments on MiniGrid, DeepMind
Lab, Atari, and Fetch show that the proposed method significantly improves
proximal policy optimization (PPO) and outperforms existing novelty-seeking
exploration methods including count-based exploration even in continuous
control tasks, indicating that it improves the sample efficiency by preventing
the agent from taking redundant actions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1"&gt;Sungryull Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sungtae Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jongwook Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seijen_H/0/1/0/all/0/1"&gt;Harm van Seijen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fatemi_M/0/1/0/all/0/1"&gt;Mehdi Fatemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Honglak Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning. (arXiv:2107.06501v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06501</id>
        <link href="http://arxiv.org/abs/2107.06501"/>
        <updated>2021-07-15T01:59:03.766Z</updated>
        <summary type="html"><![CDATA[High-level representation-guided pixel denoising and adversarial training are
independent solutions to enhance the robustness of CNNs against adversarial
attacks by pre-processing input data and re-training models, respectively. Most
recently, adversarial training techniques have been widely studied and improved
while the pixel denoising-based method is getting less attractive. However, it
is still questionable whether there exists a more advanced pixel
denoising-based method and whether the combination of the two solutions
benefits each other. To this end, we first comprehensively investigate two
kinds of pixel denoising methods for adversarial robustness enhancement (i.e.,
existing additive-based and unexplored filtering-based methods) under the loss
functions of image-level and semantic-level restorations, respectively, showing
that pixel-wise filtering can obtain much higher image quality (e.g., higher
PSNR) as well as higher robustness (e.g., higher accuracy on adversarial
examples) than existing pixel-wise additive-based method. However, we also
observe that the robustness results of the filtering-based method rely on the
perturbation amplitude of adversarial examples used for training. To address
this problem, we propose predictive perturbation-aware pixel-wise filtering,
where dual-perturbation filtering and an uncertainty-aware fusion module are
designed and employed to automatically perceive the perturbation amplitude
during the training and testing process. The proposed method is termed as
AdvFilter. Moreover, we combine adversarial pixel denoising methods with three
adversarial training-based methods, hinting that considering data and models
jointly is able to achieve more robust CNNs. The experiments conduct on
NeurIPS-2017DEV, SVHN, and CIFAR10 datasets and show the advantages over
enhancing CNNs' robustness, high generalization to different models, and noise
levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yihao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qing Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1"&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_W/0/1/0/all/0/1"&gt;Weikai Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1"&gt;Geguang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface. (arXiv:2107.06393v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06393</id>
        <link href="http://arxiv.org/abs/2107.06393"/>
        <updated>2021-07-15T01:59:03.756Z</updated>
        <summary type="html"><![CDATA[Modeling complex phenomena typically involves the use of both discrete and
continuous variables. Such a setting applies across a wide range of problems,
from identifying trends in time-series data to performing effective
compositional scene understanding in images. Here, we propose Hybrid Memoised
Wake-Sleep (HMWS), an algorithm for effective inference in such hybrid
discrete-continuous models. Prior approaches to learning suffer as they need to
perform repeated expensive inner-loop discrete inference. We build on a recent
approach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by
memoising discrete variables, and extend it to allow for a principled and
effective way to handle continuous variables by learning a separate recognition
model used for importance-sampling based approximate inference and
marginalization. We evaluate HMWS in the GP-kernel learning and 3D scene
understanding domains, and show that it outperforms current state-of-the-art
inference methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Tuan Anh Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1"&gt;Katherine M. Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hewitt_L/0/1/0/all/0/1"&gt;Luke Hewitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1"&gt;Kevin Ellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1"&gt;Siddharth N&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1"&gt;Samuel J. Gershman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting Thermoacoustic Instabilities in Liquid Propellant Rocket Engines Using Multimodal Bayesian Deep Learning. (arXiv:2107.06396v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2107.06396</id>
        <link href="http://arxiv.org/abs/2107.06396"/>
        <updated>2021-07-15T01:59:03.748Z</updated>
        <summary type="html"><![CDATA[The 100 MW cryogenic liquid oxygen/hydrogen multi-injector combustor BKD
operated by the DLR Institute of Space Propulsion is a research platform that
allows the study of thermoacoustic instabilities under realistic conditions,
representative of small upper stage rocket engines. We use data from BKD
experimental campaigns in which the static chamber pressure and fuel-oxidizer
ratio are varied such that the first tangential mode of the combustor is
excited under some conditions. We train an autoregressive Bayesian neural
network model to forecast the amplitude of the dynamic pressure time series,
inputting multiple sensor measurements (injector pressure/ temperature
measurements, static chamber pressure, high-frequency dynamic pressure
measurements, high-frequency OH* chemiluminescence measurements) and future
flow rate control signals. The Bayesian nature of our algorithms allows us to
work with a dataset whose size is restricted by the expense of each
experimental run, without making overconfident extrapolations. We find that the
networks are able to accurately forecast the evolution of the pressure
amplitude and anticipate instability events on unseen experimental runs 500
milliseconds in advance. We compare the predictive accuracy of multiple models
using different combinations of sensor inputs. We find that the high-frequency
dynamic pressure signal is particularly informative. We also use the technique
of integrated gradients to interpret the influence of different sensor inputs
on the model prediction. The negative log-likelihood of data points in the test
dataset indicates that predictive uncertainties are well-characterized by our
Bayesian model and simulating a sensor failure event results as expected in a
dramatic increase in the epistemic component of the uncertainty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Sengupta_U/0/1/0/all/0/1"&gt;Ushnish Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Waxenegger_Wilfing_G/0/1/0/all/0/1"&gt;G&amp;#xfc;nther Waxenegger-Wilfing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hardi_J/0/1/0/all/0/1"&gt;Justin Hardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Juniper_M/0/1/0/all/0/1"&gt;Matthew P. Juniper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Beyond Linear RL: Sample Efficient Neural Function Approximation. (arXiv:2107.06466v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06466</id>
        <link href="http://arxiv.org/abs/2107.06466"/>
        <updated>2021-07-15T01:59:03.741Z</updated>
        <summary type="html"><![CDATA[Deep Reinforcement Learning (RL) powered by neural net approximation of the Q
function has had enormous empirical success. While the theory of RL has
traditionally focused on linear function approximation (or eluder dimension)
approaches, little is known about nonlinear RL with neural net approximations
of the Q functions. This is the focus of this work, where we study function
approximation with two-layer neural networks (considering both ReLU and
polynomial activation functions). Our first result is a computationally and
statistically efficient algorithm in the generative model setting under
completeness for two-layer neural networks. Our second result considers this
setting but under only realizability of the neural net function class. Here,
assuming deterministic dynamics, the sample complexity scales linearly in the
algebraic dimension. In all cases, our results significantly improve upon what
can be attained with linear (or eluder dimension) methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"&gt;Baihe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaixuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham M. Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1"&gt;Qi Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Runzhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaqi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectrum Gaussian Processes Based On Tunable Basis Functions. (arXiv:2107.06473v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.06473</id>
        <link href="http://arxiv.org/abs/2107.06473"/>
        <updated>2021-07-15T01:59:03.734Z</updated>
        <summary type="html"><![CDATA[Spectral approximation and variational inducing learning for the Gaussian
process are two popular methods to reduce computational complexity. However, in
previous research, those methods always tend to adopt the orthonormal basis
functions, such as eigenvectors in the Hilbert space, in the spectrum method,
or decoupled orthogonal components in the variational framework. In this paper,
inspired by quantum physics, we introduce a novel basis function, which is
tunable, local and bounded, to approximate the kernel function in the Gaussian
process. There are two adjustable parameters in these functions, which control
their orthogonality to each other and limit their boundedness. And we conduct
extensive experiments on open-source datasets to testify its performance.
Compared to several state-of-the-art methods, it turns out that the proposed
method can obtain satisfactory or even better results, especially with poorly
chosen kernel functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wenqi Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_G/0/1/0/all/0/1"&gt;Guanlin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jiang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ping_Y/0/1/0/all/0/1"&gt;Yang Ping&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indonesia's Fake News Detection using Transformer Network. (arXiv:2107.06796v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06796</id>
        <link href="http://arxiv.org/abs/2107.06796"/>
        <updated>2021-07-15T01:59:03.710Z</updated>
        <summary type="html"><![CDATA[Fake news is a problem faced by society in this era. It is not rare for fake
news to cause provocation and problem for the people. Indonesia, as a country
with the 4th largest population, has a problem in dealing with fake news. More
than 30% of rural and urban population are deceived by this fake news problem.
As we have been studying, there is only few literatures on preventing the
spread of fake news in Bahasa Indonesia. So, this research is conducted to
prevent these problems. The dataset used in this research was obtained from a
news portal that identifies fake news, turnbackhoax.id. Using Web Scrapping on
this page, we got 1116 data consisting of valid news and fake news. The dataset
can be accessed at https://github.com/JibranFawaid/turnbackhoax-dataset. This
dataset will be combined with other available datasets. The methods used are
CNN, BiLSTM, Hybrid CNN-BiLSTM, and BERT with Transformer Network. This
research shows that the BERT method with Transformer Network has the best
results with an accuracy of up to 90%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awalina_A/0/1/0/all/0/1"&gt;Aisyah Awalina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fawaid_J/0/1/0/all/0/1"&gt;Jibran Fawaid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krisnabayu_R/0/1/0/all/0/1"&gt;Rifky Yunus Krisnabayu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1"&gt;Novanto Yudistira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06825</id>
        <link href="http://arxiv.org/abs/2107.06825"/>
        <updated>2021-07-15T01:59:03.703Z</updated>
        <summary type="html"><![CDATA[We introduce a generalization to the lottery ticket hypothesis in which the
notion of "sparsity" is relaxed by choosing an arbitrary basis in the space of
parameters. We present evidence that the original results reported for the
canonical basis continue to hold in this broader setting. We describe how
structured pruning methods, including pruning units or factorizing
fully-connected layers into products of low-rank matrices, can be cast as
particular instances of this "generalized" lottery ticket hypothesis. The
investigations reported here are preliminary and are provided to encourage
further research along this direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1"&gt;Ibrahim Alabdulmohsin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1"&gt;Larisa Markeeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1"&gt;Daniel Keysers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1"&gt;Ilya Tolstikhin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CNN-Cap: Effective Convolutional Neural Network Based Capacitance Models for Full-Chip Parasitic Extraction. (arXiv:2107.06511v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06511</id>
        <link href="http://arxiv.org/abs/2107.06511"/>
        <updated>2021-07-15T01:59:03.695Z</updated>
        <summary type="html"><![CDATA[Accurate capacitance extraction is becoming more important for designing
integrated circuits under advanced process technology. The pattern matching
based full-chip extraction methodology delivers fast computational speed, but
suffers from large error, and tedious efforts on building capacitance models of
the increasing structure patterns. In this work, we propose an effective method
for building convolutional neural network (CNN) based capacitance models
(called CNN-Cap) for two-dimensional (2-D) structures in full-chip capacitance
extraction. With a novel grid-based data representation, the proposed method is
able to model the pattern with a variable number of conductors, so that largely
reduce the number of patterns. Based on the ability of ResNet architecture on
capturing spatial information and the proposed training skills, the obtained
CNN-Cap exhibits much better performance over the multilayer perception neural
network based capacitance model while being more versatile. Extensive
experiments on a 55nm and a 15nm process technologies have demonstrated that
the error of total capacitance produced with CNN-Cap is always within 1.3% and
the error of produced coupling capacitance is less than 10% in over 99.5%
probability. CNN-Cap runs more than 4000X faster than 2-D field solver on a GPU
server, while it consumes negligible memory compared to the look-up table based
capacitance model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Dingcheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wenjian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuanbo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1"&gt;Wenjie Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Convolutional Neural Network Approach to the Classification of Engineering Models. (arXiv:2107.06481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06481</id>
        <link href="http://arxiv.org/abs/2107.06481"/>
        <updated>2021-07-15T01:59:03.667Z</updated>
        <summary type="html"><![CDATA[This paper presents a deep learning approach for the classification of
Engineering (CAD) models using Convolutional Neural Networks (CNNs). Owing to
the availability of large annotated datasets and also enough computational
power in the form of GPUs, many deep learning-based solutions for object
classification have been proposed of late, especially in the domain of images
and graphical models. Nevertheless, very few solutions have been proposed for
the task of functional classification of CAD models. Hence, for this research,
CAD models have been collected from Engineering Shape Benchmark (ESB), National
Design Repository (NDR) and augmented with newer models created using a
modelling software to form a dataset - 'CADNET'. It is proposed to use a
residual network architecture for CADNET, inspired by the popular ResNet. A
weighted Light Field Descriptor (LFD) scheme is chosen as the method of feature
extraction, and the generated images are fed as inputs to the CNN. The problem
of class imbalance in the dataset is addressed using a class weights approach.
Experiments have been conducted with other signatures such as geodesic distance
etc. using deep networks as well as other network architectures on the CADNET.
The LFD-based CNN approach using the proposed network architecture, along with
gradient boosting yielded the best classification accuracy on CADNET.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1"&gt;Bharadwaj Manda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhaskare_P/0/1/0/all/0/1"&gt;Pranjal Bhaskare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1"&gt;Ramanathan Muthuganapathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Evaluation Methods for the Causal Effect of Recommendations. (arXiv:2107.06630v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06630</id>
        <link href="http://arxiv.org/abs/2107.06630"/>
        <updated>2021-07-15T01:59:03.658Z</updated>
        <summary type="html"><![CDATA[Evaluating the causal effect of recommendations is an important objective
because the causal effect on user interactions can directly leads to an
increase in sales and user engagement. To select an optimal recommendation
model, it is common to conduct A/B testing to compare model performance.
However, A/B testing of causal effects requires a large number of users, making
such experiments costly and risky. We therefore propose the first interleaving
methods that can efficiently compare recommendation models in terms of causal
effects. In contrast to conventional interleaving methods, we measure the
outcomes of both items on an interleaved list and items not on the interleaved
list, since the causal effect is the difference between outcomes with and
without recommendations. To ensure that the evaluations are unbiased, we either
select items with equal probability or weight the outcomes using inverse
propensity scores. We then verify the unbiasedness and efficiency of online
evaluation methods through simulated online experiments. The results indicate
that our proposed methods are unbiased and that they have superior efficiency
to A/B testing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sato_M/0/1/0/all/0/1"&gt;Masahiro Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-free Reinforcement Learning for Robust Locomotion Using Trajectory Optimization for Exploration. (arXiv:2107.06629v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.06629</id>
        <link href="http://arxiv.org/abs/2107.06629"/>
        <updated>2021-07-15T01:59:03.650Z</updated>
        <summary type="html"><![CDATA[In this work we present a general, two-stage reinforcement learning approach
for going from a single demonstration trajectory to a robust policy that can be
deployed on hardware without any additional training. The demonstration is used
in the first stage as a starting point to facilitate initial exploration. In
the second stage, the relevant task reward is optimized directly and a policy
robust to environment uncertainties is computed. We demonstrate and examine in
detail performance and robustness of our approach on highly dynamic hopping and
bounding tasks on a real quadruped robot.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bogdanovic_M/0/1/0/all/0/1"&gt;Miroslav Bogdanovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khadiv_M/0/1/0/all/0/1"&gt;Majid Khadiv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Righetti_L/0/1/0/all/0/1"&gt;Ludovic Righetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Analysis of Visual COVID-19 Features from Chest Radiographs. (arXiv:2107.06618v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06618</id>
        <link href="http://arxiv.org/abs/2107.06618"/>
        <updated>2021-07-15T01:59:03.643Z</updated>
        <summary type="html"><![CDATA[Chest radiography has been a recommended procedure for patient triaging and
resource management in intensive care units (ICUs) throughout the COVID-19
pandemic. The machine learning efforts to augment this workflow have been long
challenged due to deficiencies in reporting, model evaluation, and failure mode
analysis. To address some of those shortcomings, we model radiological features
with a human-interpretable class hierarchy that aligns with the radiological
decision process. Also, we propose the use of a data-driven error analysis
methodology to uncover the blind spots of our model, providing further
transparency on its clinical utility. For example, our experiments show that
model failures highly correlate with ICU imaging conditions and with the
inherent difficulty in distinguishing certain types of radiological features.
Also, our hierarchical interpretation and analysis facilitates the comparison
with respect to radiologists' findings and inter-variability, which in return
helps us to better assess the clinical applicability of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bannur_S/0/1/0/all/0/1"&gt;Shruthi Bannur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oktay_O/0/1/0/all/0/1"&gt;Ozan Oktay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bernhardt_M/0/1/0/all/0/1"&gt;Melanie Bernhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schwaighofer_A/0/1/0/all/0/1"&gt;Anton Schwaighofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1"&gt;Rajesh Jena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nushi_B/0/1/0/all/0/1"&gt;Besmira Nushi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wadhwani_S/0/1/0/all/0/1"&gt;Sharan Wadhwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nori_A/0/1/0/all/0/1"&gt;Aditya Nori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Natarajan_K/0/1/0/all/0/1"&gt;Kal Natarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ashraf_S/0/1/0/all/0/1"&gt;Shazad Ashraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alvarez_Valle_J/0/1/0/all/0/1"&gt;Javier Alvarez-Valle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Castro_D/0/1/0/all/0/1"&gt;Daniel C. Castro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Pothole Detection Using Deep Learning. (arXiv:2107.06356v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06356</id>
        <link href="http://arxiv.org/abs/2107.06356"/>
        <updated>2021-07-15T01:59:03.628Z</updated>
        <summary type="html"><![CDATA[Roads are connecting line between different places, and used daily. Roads'
periodic maintenance keeps them safe and functional. Detecting and reporting
the existence of potholes to responsible departments can help in eliminating
them. This study deployed and tested on different deep learning architecture to
detect potholes. The images used for training were collected by cellphone
mounted on the windshield of the car, in addition to many images downloaded
from the internet to increase the size and variability of the database. Second,
various object detection algorithms are employed and compared to detect
potholes in real-time like SDD-TensorFlow, YOLOv3Darknet53 and YOLOv4Darknet53.
YOLOv4 achieved the best performance with 81% recall, 85% precision and 85.39%
mean Average Precision (mAP). The speed of processing was 20 frame per second.
The system was able to detect potholes from a range on 100 meters away from the
camera. The system can increase the safety of drivers and improve the
performance of self-driving cars by detecting pothole time ahead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaghouri_A/0/1/0/all/0/1"&gt;Anas Al Shaghouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alkhatib_R/0/1/0/all/0/1"&gt;Rami Alkhatib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berjaoui_S/0/1/0/all/0/1"&gt;Samir Berjaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Distance Measure for Privacy-preserving Process Mining based on Feature Learning. (arXiv:2107.06578v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.06578</id>
        <link href="http://arxiv.org/abs/2107.06578"/>
        <updated>2021-07-15T01:59:03.611Z</updated>
        <summary type="html"><![CDATA[To enable process analysis based on an event log without compromising the
privacy of individuals involved in process execution, a log may be anonymized.
Such anonymization strives to transform a log so that it satisfies provable
privacy guarantees, while largely maintaining its utility for process analysis.
Existing techniques perform anonymization using simple, syntactic measures to
identify suitable transformation operations. This way, the semantics of the
activities referenced by the events in a trace are neglected, potentially
leading to transformations in which events of unrelated activities are merged.
To avoid this and incorporate the semantics of activities during anonymization,
we propose to instead incorporate a distance measure based on feature learning.
Specifically, we show how embeddings of events enable the definition of a
distance measure for traces to guide event log anonymization. Our experiments
with real-world data indicate that anonymization using this measure, compared
to a syntactic one, yields logs that are closer to the original log in various
dimensions and, hence, have higher utility for process analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosel_F/0/1/0/all/0/1"&gt;Fabian R&amp;#xf6;sel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fahrenkrog_Petersen_S/0/1/0/all/0/1"&gt;Stephan A. Fahrenkrog-Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aa_H/0/1/0/all/0/1"&gt;Han van der Aa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weidlich_M/0/1/0/all/0/1"&gt;Matthias Weidlich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition. (arXiv:2107.06546v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06546</id>
        <link href="http://arxiv.org/abs/2107.06546"/>
        <updated>2021-07-15T01:59:03.596Z</updated>
        <summary type="html"><![CDATA[We present the visually-grounded language modelling track that was introduced
in the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the
new track and discuss participation rules in detail. We also present the two
baseline systems that were developed for this track.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alishahia_A/0/1/0/all/0/1"&gt;Afra Alishahia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1"&gt;Grzegorz Chrupa&amp;#x142;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cristia_A/0/1/0/all/0/1"&gt;Alejandrina Cristia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1"&gt;Emmanuel Dupoux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Higy_B/0/1/0/all/0/1"&gt;Bertrand Higy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavechin_M/0/1/0/all/0/1"&gt;Marvin Lavechin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1"&gt;Okko R&amp;#xe4;s&amp;#xe4;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chen Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deduplicating Training Data Makes Language Models Better. (arXiv:2107.06499v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06499</id>
        <link href="http://arxiv.org/abs/2107.06499"/>
        <updated>2021-07-15T01:59:03.560Z</updated>
        <summary type="html"><![CDATA[We find that existing language modeling datasets contain many near-duplicate
examples and long repetitive substrings. As a result, over 1% of the unprompted
output of language models trained on these datasets is copied verbatim from the
training data. We develop two tools that allow us to deduplicate training
datasets -- for example removing from C4 a single 61 word English sentence that
is repeated over 60,000 times. Deduplication allows us to train models that
emit memorized text ten times less frequently and require fewer train steps to
achieve the same or better accuracy. We can also reduce train-test overlap,
which affects over 4% of the validation set of standard datasets, thus allowing
for more accurate evaluation. We release code for reproducing our work and
performing dataset deduplication at
https://github.com/google-research/deduplicate-text-datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Katherine Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1"&gt;Daphne Ippolito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nystrom_A/0/1/0/all/0/1"&gt;Andrew Nystrom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chiyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1"&gt;Douglas Eck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1"&gt;Chris Callison-Burch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[For high-dimensional hierarchical models, consider exchangeability of effects across covariates instead of across datasets. (arXiv:2107.06428v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2107.06428</id>
        <link href="http://arxiv.org/abs/2107.06428"/>
        <updated>2021-07-15T01:59:03.550Z</updated>
        <summary type="html"><![CDATA[Hierarchical Bayesian methods enable information sharing across multiple
related regression problems. While standard practice is to model regression
parameters (effects) as (1) exchangeable across datasets and (2) correlated to
differing degrees across covariates, we show that this approach exhibits poor
statistical performance when the number of covariates exceeds the number of
datasets. For instance, in statistical genetics, we might regress dozens of
traits (defining datasets) for thousands of individuals (responses) on up to
millions of genetic variants (covariates). When an analyst has more covariates
than datasets, we argue that it is often more natural to instead model effects
as (1) exchangeable across covariates and (2) correlated to differing degrees
across datasets. To this end, we propose a hierarchical model expressing our
alternative perspective. We devise an empirical Bayes estimator for learning
the degree of correlation between datasets. We develop theory that demonstrates
that our method outperforms the classic approach when the number of covariates
dominates the number of datasets, and corroborate this result empirically on
several high-dimensional multiple regression and classification problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Trippe_B/0/1/0/all/0/1"&gt;Brian L. Trippe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Finucane_H/0/1/0/all/0/1"&gt;Hilary K. Finucane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Broderick_T/0/1/0/all/0/1"&gt;Tamara Broderick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry and Generalization: Eigenvalues as predictors of where a network will fail to generalize. (arXiv:2107.06386v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06386</id>
        <link href="http://arxiv.org/abs/2107.06386"/>
        <updated>2021-07-15T01:59:03.539Z</updated>
        <summary type="html"><![CDATA[We study the deformation of the input space by a trained autoencoder via the
Jacobians of the trained weight matrices. In doing so, we prove bounds for the
mean squared errors for points in the input space, under assumptions regarding
the orthogonality of the eigenvectors. We also show that the trace and the
product of the eigenvalues of the Jacobian matrices is a good predictor of the
MSE on test points. This is a dataset independent means of testing an
autoencoder's ability to generalize on new input. Namely, no knowledge of the
dataset on which the network was trained is needed, only the parameters of the
trained model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwala_S/0/1/0/all/0/1"&gt;Susama Agarwala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dees_B/0/1/0/all/0/1"&gt;Benjamin Dees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gearhart_A/0/1/0/all/0/1"&gt;Andrew Gearhart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lowman_C/0/1/0/all/0/1"&gt;Corey Lowman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Foes of Neural Network's Data Efficiency Among Unnecessary Input Dimensions. (arXiv:2107.06409v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06409</id>
        <link href="http://arxiv.org/abs/2107.06409"/>
        <updated>2021-07-15T01:59:03.528Z</updated>
        <summary type="html"><![CDATA[Datasets often contain input dimensions that are unnecessary to predict the
output label, e.g. background in object recognition, which lead to more
trainable parameters. Deep Neural Networks (DNNs) are robust to increasing the
number of parameters in the hidden layers, but it is unclear whether this holds
true for the input layer. In this letter, we investigate the impact of
unnecessary input dimensions on a central issue of DNNs: their data efficiency,
ie. the amount of examples needed to achieve certain generalization
performance. Our results show that unnecessary input dimensions that are
task-unrelated substantially degrade data efficiency. This highlights the need
for mechanisms that remove {task-unrelated} dimensions to enable data
efficiency gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1"&gt;Vanessa D&amp;#x27;Amario&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Sanjana Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1"&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safer Reinforcement Learning through Transferable Instinct Networks. (arXiv:2107.06686v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06686</id>
        <link href="http://arxiv.org/abs/2107.06686"/>
        <updated>2021-07-15T01:59:03.517Z</updated>
        <summary type="html"><![CDATA[Random exploration is one of the main mechanisms through which reinforcement
learning (RL) finds well-performing policies. However, it can lead to
undesirable or catastrophic outcomes when learning online in safety-critical
environments. In fact, safe learning is one of the major obstacles towards
real-world agents that can learn during deployment. One way of ensuring that
agents respect hard limitations is to explicitly configure boundaries in which
they can operate. While this might work in some cases, we do not always have
clear a-priori information which states and actions can lead dangerously close
to hazardous states. Here, we present an approach where an additional policy
can override the main policy and offer a safer alternative action. In our
instinct-regulated RL (IR^2L) approach, an "instinctual" network is trained to
recognize undesirable situations, while guarding the learning policy against
entering them. The instinct network is pre-trained on a single task where it is
safe to make mistakes, and transferred to environments in which learning a new
task safely is critical. We demonstrate IR^2L in the OpenAI Safety gym domain,
in which it receives a significantly lower number of safety violations during
training than a baseline RL approach while reaching similar task performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grbic_D/0/1/0/all/0/1"&gt;Djordje Grbic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1"&gt;Sebastian Risi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Associative Memory. (arXiv:2107.06446v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.06446</id>
        <link href="http://arxiv.org/abs/2107.06446"/>
        <updated>2021-07-15T01:59:03.494Z</updated>
        <summary type="html"><![CDATA[Dense Associative Memories or Modern Hopfield Networks have many appealing
properties of associative memory. They can do pattern completion, store a large
number of memories, and can be described using a recurrent neural network with
a degree of biological plausibility and rich feedback between the neurons. At
the same time, up until now all the models of this class have had only one
hidden layer, and have only been formulated with densely connected network
architectures, two aspects that hinder their machine learning applications.
This paper tackles this gap and describes a fully recurrent model of
associative memory with an arbitrary large number of layers, some of which can
be locally connected (convolutional), and a corresponding energy function that
decreases on the dynamical trajectory of the neurons' activations. The memories
of the full network are dynamically "assembled" using primitives encoded in the
synaptic weights of the lower layers, with the "assembling rules" encoded in
the synaptic weights of the higher layers. In addition to the bottom-up
propagation of information, typical of commonly used feedforward neural
networks, the model described has rich top-down feedback from higher layers
that help the lower-layer neurons to decide on their response to the input
stimuli.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krotov_D/0/1/0/all/0/1"&gt;Dmitry Krotov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tourbillon: a Physically Plausible Neural Architecture. (arXiv:2107.06424v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06424</id>
        <link href="http://arxiv.org/abs/2107.06424"/>
        <updated>2021-07-15T01:59:03.480Z</updated>
        <summary type="html"><![CDATA[In a physical neural system, backpropagation is faced with a number of
obstacles including: the need for labeled data, the violation of the locality
learning principle, the need for symmetric connections, and the lack of
modularity. Tourbillon is a new architecture that addresses all these
limitations. At its core, it consists of a stack of circular autoencoders
followed by an output layer. The circular autoencoders are trained in
self-supervised mode by recirculation algorithms and the top layer in
supervised mode by stochastic gradient descent, with the option of propagating
error information through the entire stack using non-symmetric connections.
While the Tourbillon architecture is meant primarily to address physical
constraints, and not to improve current engineering applications of deep
learning, we demonstrate its viability on standard benchmark datasets including
MNIST, Fashion MNIST, and CIFAR10. We show that Tourbillon can achieve
comparable performance to models trained with backpropagation and outperform
models that are trained with other physically plausible algorithms, such as
feedback alignment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tavakoli_M/0/1/0/all/0/1"&gt;Mohammadamin Tavakoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1"&gt;Pierre Baldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadowski_P/0/1/0/all/0/1"&gt;Peter Sadowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlated Stochastic Block Models: Exact Graph Matching with Applications to Recovering Communities. (arXiv:2107.06767v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2107.06767</id>
        <link href="http://arxiv.org/abs/2107.06767"/>
        <updated>2021-07-15T01:59:03.461Z</updated>
        <summary type="html"><![CDATA[We consider the task of learning latent community structure from multiple
correlated networks. First, we study the problem of learning the latent vertex
correspondence between two edge-correlated stochastic block models, focusing on
the regime where the average degree is logarithmic in the number of vertices.
We derive the precise information-theoretic threshold for exact recovery: above
the threshold there exists an estimator that outputs the true correspondence
with probability close to 1, while below it no estimator can recover the true
correspondence with probability bounded away from 0. As an application of our
results, we show how one can exactly recover the latent communities using
multiple correlated graphs in parameter regimes where it is
information-theoretically impossible to do so using just a single graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Racz_M/0/1/0/all/0/1"&gt;Miklos Z. Racz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sridhar_A/0/1/0/all/0/1"&gt;Anirudh Sridhar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zeroth and First Order Stochastic Frank-Wolfe Algorithms for Constrained Optimization. (arXiv:2107.06534v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.06534</id>
        <link href="http://arxiv.org/abs/2107.06534"/>
        <updated>2021-07-15T01:59:03.453Z</updated>
        <summary type="html"><![CDATA[This paper considers stochastic convex optimization problems with two sets of
constraints: (a) deterministic constraints on the domain of the optimization
variable, which are difficult to project onto; and (b) deterministic or
stochastic constraints that admit efficient projection. Problems of this form
arise frequently in the context of semidefinite programming as well as when
various NP-hard problems are solved approximately via semidefinite relaxation.
Since projection onto the first set of constraints is difficult, it becomes
necessary to explore projection-free algorithms, such as the stochastic
Frank-Wolfe (FW) algorithm. On the other hand, the second set of constraints
cannot be handled in the same way, and must be incorporated as an indicator
function within the objective function, thereby complicating the application of
FW methods. Similar problems have been studied before, and solved using
first-order stochastic FW algorithms by applying homotopy and Nesterov's
smoothing techniques to the indicator function. This work improves upon these
existing results and puts forth momentum-based first-order methods that yield
improved convergence rates, at par with the best known rates for problems
without the second set of constraints. Zeroth-order variants of the proposed
algorithms are also developed and again improve upon the state-of-the-art rate
results. The efficacy of the proposed algorithms is tested on relevant
applications of sparse matrix estimation, clustering via semidefinite
relaxation, and uniform sparsest cut problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Akhtar_Z/0/1/0/all/0/1"&gt;Zeeshan Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rajawat_K/0/1/0/all/0/1"&gt;Ketan Rajawat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Transformer Pruning. (arXiv:2104.08500v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08500</id>
        <link href="http://arxiv.org/abs/2104.08500"/>
        <updated>2021-07-15T01:59:03.447Z</updated>
        <summary type="html"><![CDATA[Vision transformer has achieved competitive performance on a variety of
computer vision applications. However, their storage, run-time memory, and
computational demands are hindering the deployment to mobile devices. Here we
present a vision transformer pruning approach, which identifies the impacts of
dimensions in each layer of transformer and then executes pruning accordingly.
By encouraging dimension-wise sparsity in the transformer, important dimensions
automatically emerge. A great number of dimensions with small importance scores
can be discarded to achieve a high pruning ratio without significantly
compromising accuracy. The pipeline for vision transformer pruning is as
follows: 1) training with sparsity regularization; 2) pruning dimensions of
linear projections; 3) fine-tuning. The reduced parameters and FLOPs ratios of
the proposed algorithm are well evaluated and analyzed on ImageNet dataset to
demonstrate the effectiveness of our proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Mingjian Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yehui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MESS: Manifold Embedding Motivated Super Sampling. (arXiv:2107.06566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06566</id>
        <link href="http://arxiv.org/abs/2107.06566"/>
        <updated>2021-07-15T01:59:03.441Z</updated>
        <summary type="html"><![CDATA[Many approaches in the field of machine learning and data analysis rely on
the assumption that the observed data lies on lower-dimensional manifolds. This
assumption has been verified empirically for many real data sets. To make use
of this manifold assumption one generally requires the manifold to be locally
sampled to a certain density such that features of the manifold can be
observed. However, for increasing intrinsic dimensionality of a data set the
required data density introduces the need for very large data sets, resulting
in one of the many faces of the curse of dimensionality. To combat the
increased requirement for local data density we propose a framework to generate
virtual data points that faithful to an approximate embedding function
underlying the manifold observable in the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thordsen_E/0/1/0/all/0/1"&gt;Erik Thordsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_E/0/1/0/all/0/1"&gt;Erich Schubert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Contextual Bandits: Learning How Behavior Evolves over Time. (arXiv:2107.06317v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06317</id>
        <link href="http://arxiv.org/abs/2107.06317"/>
        <updated>2021-07-15T01:59:03.424Z</updated>
        <summary type="html"><![CDATA[Understanding an agent's priorities by observing their behavior is critical
for transparency and accountability in decision processes, such as in
healthcare. While conventional approaches to policy learning almost invariably
assume stationarity in behavior, this is hardly true in practice: Medical
practice is constantly evolving, and clinical professionals are constantly
fine-tuning their priorities. We desire an approach to policy learning that
provides (1) interpretable representations of decision-making, accounts for (2)
non-stationarity in behavior, as well as operating in an (3) offline manner.
First, we model the behavior of learning agents in terms of contextual bandits,
and formalize the problem of inverse contextual bandits (ICB). Second, we
propose two algorithms to tackle ICB, each making varying degrees of
assumptions regarding the agent's learning strategy. Finally, through both real
and simulated data for liver transplantations, we illustrate the applicability
and explainability of our method, as well as validating its accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huyuk_A/0/1/0/all/0/1"&gt;Alihan H&amp;#xfc;y&amp;#xfc;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jarrett_D/0/1/0/all/0/1"&gt;Daniel Jarrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1"&gt;Mihaela van der Schaar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Abnormal Behavior with Self-Supervised Gaze Estimation. (arXiv:2107.06530v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06530</id>
        <link href="http://arxiv.org/abs/2107.06530"/>
        <updated>2021-07-15T01:59:03.417Z</updated>
        <summary type="html"><![CDATA[Due to the recent outbreak of COVID-19, many classes, exams, and meetings
have been conducted non-face-to-face. However, the foundation for video
conferencing solutions is still insufficient. So this technology has become an
important issue. In particular, these technologies are essential for
non-face-to-face testing, and technology dissemination is urgent. In this
paper, we present a single video conferencing solution using gaze estimation in
preparation for these problems. Gaze is an important cue for the tasks such as
analysis of human behavior. Hence, numerous studies have been proposed to solve
gaze estimation using deep learning, which is one of the most prominent methods
up to date. We use these gaze estimation methods to detect abnormal behavior of
video conferencing participants. Our contribution is as follows. i) We find and
apply the optimal network for the gaze estimation method and apply a
self-supervised method to improve accuracy. ii) For anomaly detection, we
present a new dataset that aggregates the values of a new gaze, head pose, etc.
iii) We train newly created data on Multi Layer Perceptron (MLP) models to
detect anomaly behavior based on deep learning. We demonstrate the robustness
of our method through experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suneung-Kim/0/1/0/all/0/1"&gt;Suneung-Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributionally Robust Policy Learning via Adversarial Environment Generation. (arXiv:2107.06353v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.06353</id>
        <link href="http://arxiv.org/abs/2107.06353"/>
        <updated>2021-07-15T01:59:03.411Z</updated>
        <summary type="html"><![CDATA[Our goal is to train control policies that generalize well to unseen
environments. Inspired by the Distributionally Robust Optimization (DRO)
framework, we propose DRAGEN - Distributionally Robust policy learning via
Adversarial Generation of ENvironments - for iteratively improving robustness
of policies to realistic distribution shifts by generating adversarial
environments. The key idea is to learn a generative model for environments
whose latent variables capture cost-predictive and realistic variations in
environments. We perform DRO with respect to a Wasserstein ball around the
empirical distribution of environments by generating realistic adversarial
environments via gradient ascent on the latent space. We demonstrate strong
Out-of-Distribution (OoD) generalization in simulation for (i) swinging up a
pendulum with onboard vision and (ii) grasping realistic 2D/3D objects.
Grasping experiments on hardware demonstrate better sim2real performance
compared to domain randomization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_A/0/1/0/all/0/1"&gt;Allen Z. Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1"&gt;Anirudha Majumdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Networks are Surprisingly Reversible: A Baseline for Zero-Shot Inversion. (arXiv:2107.06304v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06304</id>
        <link href="http://arxiv.org/abs/2107.06304"/>
        <updated>2021-07-15T01:59:03.404Z</updated>
        <summary type="html"><![CDATA[Understanding the behavior and vulnerability of pre-trained deep neural
networks (DNNs) can help to improve them. Analysis can be performed via
reversing the network's flow to generate inputs from internal representations.
Most existing work relies on priors or data-intensive optimization to invert a
model, yet struggles to scale to deep architectures and complex datasets. This
paper presents a zero-shot direct model inversion framework that recovers the
input to the trained model given only the internal representation. The crux of
our method is to inverse the DNN in a divide-and-conquer manner while
re-syncing the inverted layers via cycle-consistency guidance with the help of
synthesized data. As a result, we obtain a single feed-forward model capable of
inversion with a single forward pass without seeing any real data of the
original task. With the proposed approach, we scale zero-shot direct inversion
to deep architectures and complex datasets. We empirically show that modern
classification models on ImageNet can, surprisingly, be inverted, allowing an
approximate recovery of the original 224x224px images from a representation
after more than 20 layers. Moreover, inversion of generators in GANs unveils
latent code of a given synthesized face image at 128x128px, which can even, in
turn, improve defective synthesized images from GANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongxu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1"&gt;Jan Kautz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1"&gt;Pavlo Molchanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Hierarchical Federated Learning for IoT Heterogeneous Systems with Imbalanced Data. (arXiv:2107.06548v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06548</id>
        <link href="http://arxiv.org/abs/2107.06548"/>
        <updated>2021-07-15T01:59:03.386Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a distributed learning methodology that allows
multiple nodes to cooperatively train a deep learning model, without the need
to share their local data. It is a promising solution for telemonitoring
systems that demand intensive data collection, for detection, classification,
and prediction of future events, from different locations while maintaining a
strict privacy constraint. Due to privacy concerns and critical communication
bottlenecks, it can become impractical to send the FL updated models to a
centralized server. Thus, this paper studies the potential of hierarchical FL
in IoT heterogeneous systems and propose an optimized solution for user
assignment and resource allocation on multiple edge nodes. In particular, this
work focuses on a generic class of machine learning models that are trained
using gradient-descent-based schemes while considering the practical
constraints of non-uniformly distributed data across different users. We
evaluate the proposed system using two real-world datasets, and we show that it
outperforms state-of-the-art FL solutions. In particular, our numerical results
highlight the effectiveness of our approach and its ability to provide 4-6%
increase in the classification accuracy, with respect to hierarchical FL
schemes that consider distance-based user assignment. Furthermore, the proposed
approach could significantly accelerate FL training and reduce communication
overhead by providing 75-85% reduction in the communication rounds between edge
nodes and the centralized server, for the same model accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdellatif_A/0/1/0/all/0/1"&gt;Alaa Awad Abdellatif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mhaisen_N/0/1/0/all/0/1"&gt;Naram Mhaisen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Amr Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1"&gt;Mohsen Guizani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawy_Z/0/1/0/all/0/1"&gt;Zaher Dawy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasreddine_W/0/1/0/all/0/1"&gt;Wassim Nasreddine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BRIMA: low-overhead BRowser-only IMage Annotation tool (Preprint). (arXiv:2107.06351v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06351</id>
        <link href="http://arxiv.org/abs/2107.06351"/>
        <updated>2021-07-15T01:59:03.380Z</updated>
        <summary type="html"><![CDATA[Image annotation and large annotated datasets are crucial parts within the
Computer Vision and Artificial Intelligence fields.At the same time, it is
well-known and acknowledged by the research community that the image annotation
process is challenging, time-consuming and hard to scale. Therefore, the
researchers and practitioners are always seeking ways to perform the
annotations easier, faster, and at higher quality. Even though several widely
used tools exist and the tools' landscape evolved considerably, most of the
tools still require intricate technical setups and high levels of technical
savviness from its operators and crowdsource contributors.

In order to address such challenges, we develop and present BRIMA -- a
flexible and open-source browser extension that allows BRowser-only IMage
Annotation at considerably lower overheads. Once added to the browser, it
instantly allows the user to annotate images easily and efficiently directly
from the browser without any installation or setup on the client-side. It also
features cross-browser and cross-platform functionality thus presenting itself
as a neat tool for researchers within the Computer Vision, Artificial
Intelligence, and privacy-related fields.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lahtinen_T/0/1/0/all/0/1"&gt;Tuomo Lahtinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turtiainen_H/0/1/0/all/0/1"&gt;Hannu Turtiainen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costin_A/0/1/0/all/0/1"&gt;Andrei Costin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Feature Selection via Transferring Supervised Knowledge. (arXiv:1908.03464v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.03464</id>
        <link href="http://arxiv.org/abs/1908.03464"/>
        <updated>2021-07-15T01:59:03.372Z</updated>
        <summary type="html"><![CDATA[Feature selection, an effective technique for dimensionality reduction, plays
an important role in many machine learning systems. Supervised knowledge can
significantly improve the performance. However, faced with the rapid growth of
newly emerging concepts, existing supervised methods might easily suffer from
the scarcity and validity of labeled data for training. In this paper, the
authors study the problem of zero-shot feature selection (i.e., building a
feature selection model that generalizes well to "unseen" concepts with limited
training data of "seen" concepts). Specifically, they adopt class-semantic
descriptions (i.e., attributes) as supervision for feature selection, so as to
utilize the supervised knowledge transferred from the seen concepts. For more
reliable discriminative features, they further propose the
center-characteristic loss which encourages the selected features to capture
the central characteristics of seen concepts. Extensive experiments conducted
on various real-world datasets demonstrate the effectiveness of the method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiao Wang&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tingzhang Zhao&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaojun Ye&lt;/a&gt; (2) ((1) Department of Computer Science, University of Science and Technology Beijing (2) School of Software, Tsinghua University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Reinforcement Learning Based Stochastic Driver Behavior Learning. (arXiv:2107.06344v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06344</id>
        <link href="http://arxiv.org/abs/2107.06344"/>
        <updated>2021-07-15T01:59:03.329Z</updated>
        <summary type="html"><![CDATA[Drivers have unique and rich driving behaviors when operating vehicles in
traffic. This paper presents a novel driver behavior learning approach that
captures the uniqueness and richness of human driver behavior in realistic
driving scenarios. A stochastic inverse reinforcement learning (SIRL) approach
is proposed to learn a distribution of cost function, which represents the
richness of the human driver behavior with a given set of driver-specific
demonstrations. Evaluations are conducted on the realistic driving data
collected from the 3D driver-in-the-loop driving simulation. The results show
that the learned stochastic driver model is capable of expressing the richness
of the human driving strategies under different realistic driving scenarios.
Compared to the deterministic baseline driver model, the results reveal that
the proposed stochastic driver behavior model can better replicate the driver's
unique and rich driving strategies in a variety of traffic conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozkan_M/0/1/0/all/0/1"&gt;Mehmet Fatih Ozkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yao Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Performance Analysis of the Adversarial System Variant Approximation Method to Quantify Process Model Generalization. (arXiv:2107.06319v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06319</id>
        <link href="http://arxiv.org/abs/2107.06319"/>
        <updated>2021-07-15T01:59:03.321Z</updated>
        <summary type="html"><![CDATA[Process mining algorithms discover a process model from an event log. The
resulting process model is supposed to describe all possible event sequences of
the underlying system. Generalization is a process model quality dimension of
interest. A generalization metric should quantify the extent to which a process
model represents the observed event sequences contained in the event log and
the unobserved event sequences of the system. Most of the available metrics in
the literature cannot properly quantify the generalization of a process model.
A recently published method [1] called Adversarial System Variant Approximation
leverages Generative Adversarial Networks to approximate the underlying event
sequence distribution of a system from an event log. While this method
demonstrated performance gains over existing methods in measuring the
generalization of process models, its experimental evaluations have been
performed under ideal conditions. This paper experimentally investigates the
performance of Adversarial System Variant Approximation under non-ideal
conditions such as biased and limited event logs. Moreover, experiments are
performed to investigate the originally proposed sampling hyperparameter value
of the method on its performance to measure the generalization. The results
confirm the need to raise awareness about the working conditions of the
Adversarial System Variant Approximation method. The outcomes of this paper
also serve to initiate future research directions.

[1] Theis, Julian, and Houshang Darabi. "Adversarial System Variant
Approximation to Quantify Process Model Generalization." IEEE Access 8 (2020):
194410-194427.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Theis_J/0/1/0/all/0/1"&gt;Julian Theis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mokhtarian_I/0/1/0/all/0/1"&gt;Ilia Mokhtarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darabi_H/0/1/0/all/0/1"&gt;Houshang Darabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-isomorphic Inter-modality Graph Alignment and Synthesis for Holistic Brain Mapping. (arXiv:2107.06281v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2107.06281</id>
        <link href="http://arxiv.org/abs/2107.06281"/>
        <updated>2021-07-15T01:59:03.285Z</updated>
        <summary type="html"><![CDATA[Brain graph synthesis marked a new era for predicting a target brain graph
from a source one without incurring the high acquisition cost and processing
time of neuroimaging data. However, existing multi-modal graph synthesis
frameworks have several limitations. First, they mainly focus on generating
graphs from the same domain (intra-modality), overlooking the rich multimodal
representations of brain connectivity (inter-modality). Second, they can only
handle isomorphic graph generation tasks, limiting their generalizability to
synthesizing target graphs with a different node size and topological structure
from those of the source one. More importantly, both target and source domains
might have different distributions, which causes a domain fracture between them
(i.e., distribution misalignment). To address such challenges, we propose an
inter-modality aligner of non-isomorphic graphs (IMANGraphNet) framework to
infer a target graph modality based on a given modality. Our three core
contributions lie in (i) predicting a target graph (e.g., functional) from a
source graph (e.g., morphological) based on a novel graph generative
adversarial network (gGAN); (ii) using non-isomorphic graphs for both source
and target domains with a different number of nodes, edges and structure; and
(iii) enforcing the predicted target distribution to match that of the ground
truth graphs using a graph autoencoder to relax the designed loss oprimization.
To handle the unstable behavior of gGAN, we design a new Ground
Truth-Preserving (GT-P) loss function to guide the generator in learning the
topological structure of ground truth brain graphs. Our comprehensive
experiments on predicting functional from morphological graphs demonstrate the
outperformance of IMANGraphNet in comparison with its variants. This can be
further leveraged for integrative and holistic brain mapping in health and
disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Mhiri_I/0/1/0/all/0/1"&gt;Islem Mhiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Nebli_A/0/1/0/all/0/1"&gt;Ahmed Nebli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Mahjoub_M/0/1/0/all/0/1"&gt;Mohamed Ali Mahjoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1"&gt;Islem Rekik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09124</id>
        <link href="http://arxiv.org/abs/2104.09124"/>
        <updated>2021-07-15T01:59:03.232Z</updated>
        <summary type="html"><![CDATA[While self-supervised representation learning (SSL) has received widespread
attention from the community, recent research argue that its performance will
suffer a cliff fall when the model size decreases. The current method mainly
relies on contrastive learning to train the network and in this work, we
propose a simple yet effective Distilled Contrastive Learning (DisCo) to ease
the issue by a large margin. Specifically, we find the final embedding obtained
by the mainstream SSL methods contains the most fruitful information, and
propose to distill the final embedding to maximally transmit a teacher's
knowledge to a lightweight model by constraining the last embedding of the
student to be consistent with that of the teacher. In addition, in the
experiment, we find that there exists a phenomenon termed Distilling BottleNeck
and present to enlarge the embedding dimension to alleviate this problem. Our
method does not introduce any extra parameter to lightweight models during
deployment. Experimental results demonstrate that our method achieves the
state-of-the-art on all lightweight models. Particularly, when
ResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear
result of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,
but the number of parameters of EfficientNet-B0 is only 9.4%/16.3% of
ResNet-101/ResNet-50.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yuting Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1"&gt;Jia-Xin Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaowei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xing Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08038</id>
        <link href="http://arxiv.org/abs/2106.08038"/>
        <updated>2021-07-15T01:59:03.226Z</updated>
        <summary type="html"><![CDATA[Averaging predictions over a set of models -- an ensemble -- is widely used
to improve predictive performance and uncertainty estimation of deep learning
models. At the same time, many machine learning systems, such as search,
matching, and recommendation systems, heavily rely on embeddings.
Unfortunately, due to misalignment of features of independently trained models,
embeddings, cannot be improved with a naive deep ensemble like approach. In
this work, we look at the ensembling of representations and propose mean
embeddings with test-time augmentation (MeTTA) simple yet well-performing
recipe for ensembling representations. Empirically we demonstrate that MeTTA
significantly boosts the quality of linear evaluation on ImageNet for both
supervised and self-supervised models. Even more exciting, we draw connections
between MeTTA, image retrieval, and transformation invariant models. We believe
that spreading the success of ensembles to inference higher-quality
representations is the important step that will open many new applications of
ensembling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1"&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1"&gt;Andrei Atanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias. (arXiv:2106.03348v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03348</id>
        <link href="http://arxiv.org/abs/2106.03348"/>
        <updated>2021-07-15T01:59:03.220Z</updated>
        <summary type="html"><![CDATA[Transformers have shown great potential in various computer vision tasks
owing to their strong capability in modeling long-range dependency using the
self-attention mechanism. Nevertheless, vision transformers treat an image as
1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in
modeling local visual structures and dealing with scale variance.
Alternatively, they require large-scale training data and longer training
schedules to learn the IB implicitly. In this paper, we propose a novel Vision
Transformer Advanced by Exploring intrinsic IB from convolutions, ie, ViTAE.
Technically, ViTAE has several spatial pyramid reduction modules to downsample
and embed the input image into tokens with rich multi-scale context by using
multiple convolutions with different dilation rates. In this way, it acquires
an intrinsic scale invariance IB and is able to learn robust feature
representation for objects at various scales. Moreover, in each transformer
layer, ViTAE has a convolution block in parallel to the multi-head
self-attention module, whose features are fused and fed into the feed-forward
network. Consequently, it has the intrinsic locality IB and is able to learn
local features and global dependencies collaboratively. Experiments on ImageNet
as well as downstream tasks prove the superiority of ViTAE over the baseline
transformer and concurrent works. Source code and pretrained models will be
available at GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yufei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Person-MinkUNet: 3D Person Detection with LiDAR Point Cloud. (arXiv:2107.06780v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06780</id>
        <link href="http://arxiv.org/abs/2107.06780"/>
        <updated>2021-07-15T01:59:03.202Z</updated>
        <summary type="html"><![CDATA[In this preliminary work we attempt to apply submanifold sparse convolution
to the task of 3D person detection. In particular, we present Person-MinkUNet,
a single-stage 3D person detection network based on Minkowski Engine with U-Net
architecture. The network achieves a 76.4% average precision (AP) on the JRDB
3D detection benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1"&gt;Dan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1"&gt;Bastian Leibe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[About Explicit Variance Minimization: Training Neural Networks for Medical Imaging With Limited Data Annotations. (arXiv:2105.14117v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14117</id>
        <link href="http://arxiv.org/abs/2105.14117"/>
        <updated>2021-07-15T01:59:03.195Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning methods for computer vision have demonstrated the
effectiveness of pre-training feature representations, resulting in
well-generalizing Deep Neural Networks, even if the annotated data are limited.
However, representation learning techniques require a significant amount of
time for model training, with most of it time spent on precise hyper-parameter
optimization and selection of augmentation techniques. We hypothesized that if
the annotated dataset has enough morphological diversity to capture the general
population's as is common in medical imaging, for example, due to conserved
similarities of tissue mythologies, the variance error of the trained model is
the prevalent component of the Bias-Variance Trade-off. We propose the Variance
Aware Training (VAT) method that exploits this property by introducing the
variance error into the model loss function, i.e., enabling minimizing the
variance explicitly. Additionally, we provide the theoretical formulation and
proof of the proposed method to aid in interpreting the approach. Our method
requires selecting only one hyper-parameter and was able to match or improve
the state-of-the-art performance of self-supervised methods while achieving an
order of magnitude reduction in the GPU training time. We validated VAT on
three medical imaging datasets from diverse domains and various learning
objectives. These included a Magnetic Resonance Imaging (MRI) dataset for the
heart semantic segmentation (MICCAI 2017 ACDC challenge), fundus photography
dataset for ordinary regression of diabetic retinopathy progression (Kaggle
2019 APTOS Blindness Detection challenge), and classification of
histopathologic scans of lymph node sections (PatchCamelyon dataset).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shubin_D/0/1/0/all/0/1"&gt;Dmitrii Shubin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eytan_D/0/1/0/all/0/1"&gt;Danny Eytan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodfellow_S/0/1/0/all/0/1"&gt;Sebastian D. Goodfellow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Pothole Detection Using Deep Learning. (arXiv:2107.06356v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06356</id>
        <link href="http://arxiv.org/abs/2107.06356"/>
        <updated>2021-07-15T01:59:03.187Z</updated>
        <summary type="html"><![CDATA[Roads are connecting line between different places, and used daily. Roads'
periodic maintenance keeps them safe and functional. Detecting and reporting
the existence of potholes to responsible departments can help in eliminating
them. This study deployed and tested on different deep learning architecture to
detect potholes. The images used for training were collected by cellphone
mounted on the windshield of the car, in addition to many images downloaded
from the internet to increase the size and variability of the database. Second,
various object detection algorithms are employed and compared to detect
potholes in real-time like SDD-TensorFlow, YOLOv3Darknet53 and YOLOv4Darknet53.
YOLOv4 achieved the best performance with 81% recall, 85% precision and 85.39%
mean Average Precision (mAP). The speed of processing was 20 frame per second.
The system was able to detect potholes from a range on 100 meters away from the
camera. The system can increase the safety of drivers and improve the
performance of self-driving cars by detecting pothole time ahead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaghouri_A/0/1/0/all/0/1"&gt;Anas Al Shaghouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alkhatib_R/0/1/0/all/0/1"&gt;Rami Alkhatib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berjaoui_S/0/1/0/all/0/1"&gt;Samir Berjaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSFNet:Multi-scale features network for monocular depth estimation. (arXiv:2107.06445v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06445</id>
        <link href="http://arxiv.org/abs/2107.06445"/>
        <updated>2021-07-15T01:59:03.179Z</updated>
        <summary type="html"><![CDATA[In recent years, monocular depth estimation is applied to understand the
surrounding 3D environment and has made great progress. However, there is an
ill-posed problem on how to gain depth information directly from a single
image. With the rapid development of deep learning, this problem is possible to
be solved. Although more and more approaches are proposed one after another,
most of existing methods inevitably lost details due to continuous downsampling
when mapping from RGB space to depth space. To the end, we design a Multi-scale
Features Network (MSFNet), which consists of Enhanced Diverse Attention (EDA)
module and Upsample-Stage Fusion (USF) module. The EDA module employs the
spatial attention method to learn significant spatial information, while USF
module complements low-level detail information with high-level semantic
information from the perspective of multi-scale feature fusion to improve the
predicted effect. In addition, since the simple samples are always trained to a
better effect first, the hard samples are difficult to converge. Therefore, we
design a batch-loss to assign large loss factors to the harder samples in a
batch. Experiments on NYU-Depth V2 dataset and KITTI dataset demonstrate that
our proposed approach is more competitive with the state-of-the-art methods in
both qualitative and quantitative evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pei_M/0/1/0/all/0/1"&gt;Meiqi Pei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RCLC: ROI-based joint conventional and learning video compression. (arXiv:2107.06492v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.06492</id>
        <link href="http://arxiv.org/abs/2107.06492"/>
        <updated>2021-07-15T01:59:03.173Z</updated>
        <summary type="html"><![CDATA[COVID-19 leads to the high demand for remote interactive systems ever seen.
One of the key elements of these systems is video streaming, which requires a
very high network bandwidth due to its specific real-time demand, especially
with high-resolution video. Existing video compression methods are struggling
in the trade-off between video quality and the speed requirement. Addressed
that the background information rarely changes in most remote meeting cases, we
introduce a Region-Of-Interests (ROI) based video compression framework (named
RCLC) that leverages the cutting-edge learning-based and conventional
technologies. In RCLC, each coming frame is marked as a background-updating
(BU) or ROI-updating (RU) frame. By applying the conventional video codec, the
BU frame is compressed with low-quality and high-compression, while the ROI
from RU-frame is compressed with high-quality and low-compression. The
learning-based methods are applied to detect the ROI, blend background-ROI, and
enhance video quality. The experimental results show that our RCLC can reduce
up to 32.55\% BD-rate for the ROI region compared to H.265 video codec under a
similar compression time with 1080p resolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1"&gt;Trinh Man Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinjia Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Lies: Optical Adversarial Attack. (arXiv:2106.09908v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09908</id>
        <link href="http://arxiv.org/abs/2106.09908"/>
        <updated>2021-07-15T01:59:03.157Z</updated>
        <summary type="html"><![CDATA[A significant amount of work has been done on adversarial attacks that inject
imperceptible noise to images to deteriorate the image classification
performance of deep models. However, most of the existing studies consider
attacks in the digital (pixel) domain where an image acquired by an image
sensor with sampling and quantization has been recorded. This paper, for the
first time, introduces an optical adversarial attack, which physically alters
the light field information arriving at the image sensor so that the
classification model yields misclassification. More specifically, we modulate
the phase of the light in the Fourier domain using a spatial light modulator
placed in the photographic system. The operative parameters of the modulator
are obtained by gradient-based optimization to maximize cross-entropy and
minimize distortions. We present experiments based on both simulation and a
real hardware optical system, from which the feasibility of the proposed
optical attack is demonstrated. It is also verified that the proposed attack is
completely different from common optical-domain distortions such as spherical
aberration, defocus, and astigmatism in terms of both perturbation patterns and
classification results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyulim Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;JeongSoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Seungri Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jun-Ho Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_C/0/1/0/all/0/1"&gt;Chulmin Joo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jong-Seok Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Event Camera Calibration. (arXiv:2107.06749v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06749</id>
        <link href="http://arxiv.org/abs/2107.06749"/>
        <updated>2021-07-15T01:59:03.151Z</updated>
        <summary type="html"><![CDATA[Camera calibration is an important prerequisite towards the solution of 3D
computer vision problems. Traditional methods rely on static images of a
calibration pattern. This raises interesting challenges towards the practical
usage of event cameras, which notably require image change to produce
sufficient measurements. The current standard for event camera calibration
therefore consists of using flashing patterns. They have the advantage of
simultaneously triggering events in all reprojected pattern feature locations,
but it is difficult to construct or use such patterns in the field. We present
the first dynamic event camera calibration algorithm. It calibrates directly
from events captured during relative motion between camera and calibration
pattern. The method is propelled by a novel feature extraction mechanism for
calibration patterns, and leverages existing calibration tools before
optimizing all parameters through a multi-segment continuous-time formulation.
As demonstrated through our results on real data, the obtained calibration
method is highly convenient and reliably calibrates from data sequences
spanning less than 10 seconds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yifu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1"&gt;Laurent Kneip&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. (arXiv:2103.05630v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05630</id>
        <link href="http://arxiv.org/abs/2103.05630"/>
        <updated>2021-07-15T01:59:03.144Z</updated>
        <summary type="html"><![CDATA[The rapid progress of photorealistic synthesis techniques has reached at a
critical point where the boundary between real and manipulated images starts to
blur. Thus, benchmarking and advancing digital forgery analysis have become a
pressing issue. However, existing face forgery datasets either have limited
diversity or only support coarse-grained analysis. To counter this emerging
threat, we construct the ForgeryNet dataset, an extremely large face forgery
dataset with unified annotations in image- and video-level data across four
tasks: 1) Image Forgery Classification, including two-way (real / fake),
three-way (real / fake with identity-replaced forgery approaches / fake with
identity-remained forgery approaches), and n-way (real and 15 respective
forgery approaches) classification. 2) Spatial Forgery Localization, which
segments the manipulated area of fake images compared to their corresponding
source real images. 3) Video Forgery Classification, which re-defines the
video-level forgery classification with manipulated frames in random positions.
This task is important because attackers in real world are free to manipulate
any target frame. and 4) Temporal Forgery Localization, to localize the
temporal segments which are manipulated. ForgeryNet is by far the largest
publicly available deep face forgery dataset in terms of data-scale (2.9
million images, 221,247 videos), manipulations (7 image-level approaches, 8
video-level approaches), perturbations (36 independent and more mixed
perturbations) and annotations (6.3 million classification labels, 2.9 million
manipulated area annotations and 221,247 temporal forgery segment labels). We
perform extensive benchmarking and studies of existing face forensics methods
and obtain several valuable observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yinan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_B/0/1/0/all/0/1"&gt;Bei Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yichun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1"&gt;Guojun Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Luchuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1"&gt;Lu Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1"&gt;Jing Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data. (arXiv:2107.06777v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06777</id>
        <link href="http://arxiv.org/abs/2107.06777"/>
        <updated>2021-07-15T01:59:03.136Z</updated>
        <summary type="html"><![CDATA[One of the most pressing problems in the automated analysis of historical
documents is the availability of annotated training data. In this paper, we
propose a novel method for the synthesis of training data for semantic
segmentation of document images. We utilize clusters found in intermediate
features of a StyleGAN generator for the synthesis of RGB and label images at
the same time. Our model can be applied to any dataset of scanned documents
without the need for manual annotation of individual images, as each model is
custom-fit to the dataset. In our experiments, we show that models trained on
our synthetic data can reach competitive performance on open benchmark datasets
for line segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bartz_C/0/1/0/all/0/1"&gt;Christian Bartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratz_H/0/1/0/all/0/1"&gt;Hendrik R&amp;#xe4;tz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haojin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_J/0/1/0/all/0/1"&gt;Joseph Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1"&gt;Christoph Meinel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faces in the Wild: Efficient Gender Recognition in Surveillance Conditions. (arXiv:2107.06847v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06847</id>
        <link href="http://arxiv.org/abs/2107.06847"/>
        <updated>2021-07-15T01:59:03.128Z</updated>
        <summary type="html"><![CDATA[Soft biometrics inference in surveillance scenarios is a topic of interest
for various applications, particularly in security-related areas. However, soft
biometric analysis is not extensively reported in wild conditions. In
particular, previous works on gender recognition report their results in face
datasets, with relatively good image quality and frontal poses. Given the
uncertainty of the availability of the facial region in wild conditions, we
consider that these methods are not adequate for surveillance settings. To
overcome these limitations, we: 1) present frontal and wild face versions of
three well-known surveillance datasets; and 2) propose a model that effectively
and dynamically combines facial and body information, which makes it suitable
for gender recognition in wild conditions. The frontal and wild face datasets
derive from widely used Pedestrian Attribute Recognition (PAR) sets (PETA,
PA-100K, and RAP), using a pose-based approach to filter the frontal samples
and facial regions. This approach retrieves the facial region of images with
varying image/subject conditions, where the state-of-the-art face detectors
often fail. Our model combines facial and body information through a learnable
fusion matrix and a channel-attention sub-network, focusing on the most
influential body parts according to the specific image/subject features. We
compare it with five PAR methods, consistently obtaining state-of-the-art
results on gender recognition, and reducing the prediction errors by up to 24%
in frontal samples. The announced PAR datasets versions and model serve as the
basis for wild soft biometrics classification and are available in
https://github.com/Tiago-Roxo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roxo_T/0/1/0/all/0/1"&gt;Tiago Roxo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Proenca_H/0/1/0/all/0/1"&gt;Hugo Proen&amp;#xe7;a&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Pose Transfer with Dynamic Details using Neural Video Rendering. (arXiv:2106.14132v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14132</id>
        <link href="http://arxiv.org/abs/2106.14132"/>
        <updated>2021-07-15T01:59:03.111Z</updated>
        <summary type="html"><![CDATA[Pose transfer of human videos aims to generate a high fidelity video of a
target person imitating actions of a source person. A few studies have made
great progress either through image translation with deep latent features or
neural rendering with explicit 3D features. However, both of them rely on large
amounts of training data to generate realistic results, and the performance
degrades on more accessible internet videos due to insufficient training
frames. In this paper, we demonstrate that the dynamic details can be preserved
even trained from short monocular videos. Overall, we propose a neural video
rendering framework coupled with an image-translation-based dynamic details
generation network (D2G-Net), which fully utilizes both the stability of
explicit 3D features and the capacity of learning components. To be specific, a
novel texture representation is presented to encode both the static and
pose-varying appearance characteristics, which is then mapped to the image
space and rendered as a detail-rich frame in the neural rendering stage.
Moreover, we introduce a concise temporal loss in the training stage to
suppress the detail flickering that is made more visible due to high-quality
dynamic details generated by our method. Through extensive comparisons, we
demonstrate that our neural human video renderer is capable of achieving both
clearer dynamic details and more robust performance even on accessible short
videos with only 2k - 4k frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yang-tian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hao-zhi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Yu-kun Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lin Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking Outside the Window: Wide-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2106.15754v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15754</id>
        <link href="http://arxiv.org/abs/2106.15754"/>
        <updated>2021-07-15T01:59:03.100Z</updated>
        <summary type="html"><![CDATA[Long-range context information is crucial for the semantic segmentation of
High-Resolution (HR) Remote Sensing Images (RSIs). The image cropping
operations, commonly used for training neural networks, limit the perception of
long-range context information in large RSIs. To break this limitation, we
propose a Wide-Context Network (WiCoNet) for the semantic segmentation of HR
RSIs. In the WiCoNet, apart from a conventional feature extraction network that
aggregates the local information, an extra context branch is designed to
explicitly model the spatial information in a larger image area. The
information between the two branches is communicated through a Context
Transformer, which is a novel design derived from the Vision Transformer to
model the long-range context correlations. Ablation studies and comparative
experiments conducted on several benchmark datasets prove the effectiveness of
the proposed method. In addition, we present a new Beijing Land-Use (BLU)
dataset. This is a large-scale HR satellite dataset provided with high-quality
and fine-grained reference labels, which can boost future studies in this
field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"&gt;Lei Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shaofu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1"&gt;Xiaojie Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuebin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1"&gt;Lorenzo Bruzzone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Structural Causal Model for MR Images of Multiple Sclerosis. (arXiv:2103.03158v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03158</id>
        <link href="http://arxiv.org/abs/2103.03158"/>
        <updated>2021-07-15T01:59:03.094Z</updated>
        <summary type="html"><![CDATA[Precision medicine involves answering counterfactual questions such as "Would
this patient respond better to treatment A or treatment B?" These types of
questions are causal in nature and require the tools of causal inference to be
answered, e.g., with a structural causal model (SCM). In this work, we develop
an SCM that models the interaction between demographic information, disease
covariates, and magnetic resonance (MR) images of the brain for people with
multiple sclerosis. Inference in the SCM generates counterfactual images that
show what an MR image of the brain would look like if demographic or disease
covariates are changed. These images can be used for modeling disease
progression or used for image processing tasks where controlling for
confounders is necessary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reinhold_J/0/1/0/all/0/1"&gt;Jacob C. Reinhold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carass_A/0/1/0/all/0/1"&gt;Aaron Carass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1"&gt;Jerry L. Prince&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Developmental Stage Classification of EmbryosUsing Two-Stream Neural Network with Linear-Chain Conditional Random Field. (arXiv:2107.06360v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06360</id>
        <link href="http://arxiv.org/abs/2107.06360"/>
        <updated>2021-07-15T01:59:03.089Z</updated>
        <summary type="html"><![CDATA[The developmental process of embryos follows a monotonic order. An embryo can
progressively cleave from one cell to multiple cells and finally transform to
morula and blastocyst. For time-lapse videos of embryos, most existing
developmental stage classification methods conduct per-frame predictions using
an image frame at each time step. However, classification using only images
suffers from overlapping between cells and imbalance between stages. Temporal
information can be valuable in addressing this problem by capturing movements
between neighboring frames. In this work, we propose a two-stream model for
developmental stage classification. Unlike previous methods, our two-stream
model accepts both temporal and image information. We develop a linear-chain
conditional random field (CRF) on top of neural network features extracted from
the temporal and image streams to make use of both modalities. The linear-chain
CRF formulation enables tractable training of global sequential models over
multiple frames while also making it possible to inject monotonic development
order constraints into the learning process explicitly. We demonstrate our
algorithm on two time-lapse embryo video datasets: i) mouse and ii) human
embryo datasets. Our method achieves 98.1 % and 80.6 % for mouse and human
embryo stage classification, respectively. Our approach will enable more
profound clinical and biological studies and suggests a new direction for
developmental stage classification by utilizing temporal information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lukyanenko_S/0/1/0/all/0/1"&gt;Stanislav Lukyanenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1"&gt;Won-Dong Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1"&gt;Donglai Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Struyven_R/0/1/0/all/0/1"&gt;Robbert Struyven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leahy_B/0/1/0/all/0/1"&gt;Brian Leahy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Helen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1"&gt;Alexander Rush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Yosef_D/0/1/0/all/0/1"&gt;Dalit Ben-Yosef&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Needleman_D/0/1/0/all/0/1"&gt;Daniel Needleman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1"&gt;Hanspeter Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Guided Mixup for Semi-Supervised Domain Adaptation without Source Data. (arXiv:2107.06707v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06707</id>
        <link href="http://arxiv.org/abs/2107.06707"/>
        <updated>2021-07-15T01:59:03.083Z</updated>
        <summary type="html"><![CDATA[Present domain adaptation methods usually perform explicit representation
alignment by simultaneously accessing the source data and target data. However,
the source data are not always available due to the privacy preserving
consideration or bandwidth limitation. Source-free domain adaptation aims to
solve the above problem by performing domain adaptation without accessing the
source data. The adaptation paradigm is receiving more and more attention in
recent years, and multiple works have been proposed for unsupervised
source-free domain adaptation. However, without utilizing any supervised signal
and source data at the adaptation stage, the optimization of the target model
is unstable and fragile. To alleviate the problem, we focus on semi-supervised
domain adaptation under source-free setting. More specifically, we propose
uncertainty-guided Mixup to reduce the representation's intra-domain
discrepancy and perform inter-domain alignment without directly accessing the
source data. Finally, we conduct extensive semi-supervised domain adaptation
experiments on various datasets. Our method outperforms the recent
semi-supervised baselines and the unsupervised variant also achieves
competitive performance. The experiment codes will be released in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1"&gt;Ning Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1"&gt;Jiajun Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sheng Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition. (arXiv:2107.06538v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.06538</id>
        <link href="http://arxiv.org/abs/2107.06538"/>
        <updated>2021-07-15T01:59:03.066Z</updated>
        <summary type="html"><![CDATA[Fine-grained image recognition is challenging because discriminative clues
are usually fragmented, whether from a single image or multiple images. Despite
their significant improvements, most existing methods still focus on the most
discriminative parts from a single image, ignoring informative details in other
regions and lacking consideration of clues from other associated images. In
this paper, we analyze the difficulties of fine-grained image recognition from
a new perspective and propose a transformer architecture with the peak
suppression module and knowledge guidance module, which respects the
diversification of discriminative features in a single image and the
aggregation of discriminative clues among multiple images. Specifically, the
peak suppression module first utilizes a linear projection to convert the input
image into sequential tokens. It then blocks the token based on the attention
response generated by the transformer encoder. This module penalizes the
attention to the most discriminative parts in the feature learning process,
therefore, enhancing the information exploitation of the neglected regions. The
knowledge guidance module compares the image-based representation generated
from the peak suppression module with the learnable knowledge embedding set to
obtain the knowledge response coefficients. Afterwards, it formalizes the
knowledge learning as a classification problem using response coefficients as
the classification scores. Knowledge embeddings and image-based representations
are updated during training so that the knowledge embedding includes
discriminative clues for different images. Finally, we incorporate the acquired
knowledge embeddings into the image-based representations as comprehensive
representations, leading to significantly higher performance. Extensive
evaluations on the six popular datasets demonstrate the advantage of the
proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinda Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lili Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SurgeonAssist-Net: Towards Context-Aware Head-Mounted Display-Based Augmented Reality for Surgical Guidance. (arXiv:2107.06397v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06397</id>
        <link href="http://arxiv.org/abs/2107.06397"/>
        <updated>2021-07-15T01:59:03.060Z</updated>
        <summary type="html"><![CDATA[We present SurgeonAssist-Net: a lightweight framework making
action-and-workflow-driven virtual assistance, for a set of predefined surgical
tasks, accessible to commercially available optical see-through head-mounted
displays (OST-HMDs). On a widely used benchmark dataset for laparoscopic
surgical workflow, our implementation competes with state-of-the-art approaches
in prediction accuracy for automated task recognition, and yet requires 7.4x
fewer parameters, 10.2x fewer floating point operations per second (FLOPS), is
7.0x faster for inference on a CPU, and is capable of near real-time
performance on the Microsoft HoloLens 2 OST-HMD. To achieve this, we make use
of an efficient convolutional neural network (CNN) backbone to extract
discriminative features from image data, and a low-parameter recurrent neural
network (RNN) architecture to learn long-term temporal dependencies. To
demonstrate the feasibility of our approach for inference on the HoloLens 2 we
created a sample dataset that included video of several surgical tasks recorded
from a user-centric point-of-view. After training, we deployed our model and
cataloged its performance in an online simulated surgical scenario for the
prediction of the current surgical task. The utility of our approach is
explored in the discussion of several relevant clinical use-cases. Our code is
publicly available at https://github.com/doughtmw/surgeon-assist-net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Doughty_M/0/1/0/all/0/1"&gt;Mitchell Doughty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Karan Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghugre_N/0/1/0/all/0/1"&gt;Nilesh R. Ghugre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DVMN: Dense Validity Mask Network for Depth Completion. (arXiv:2107.06709v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06709</id>
        <link href="http://arxiv.org/abs/2107.06709"/>
        <updated>2021-07-15T01:59:03.055Z</updated>
        <summary type="html"><![CDATA[LiDAR depth maps provide environmental guidance in a variety of applications.
However, such depth maps are typically sparse and insufficient for complex
tasks such as autonomous navigation. State of the art methods use image guided
neural networks for dense depth completion. We develop a guided convolutional
neural network focusing on gathering dense and valid information from sparse
depth maps. To this end, we introduce a novel layer with spatially variant and
content-depended dilation to include additional data from sparse input.
Furthermore, we propose a sparsity invariant residual bottleneck block. We
evaluate our Dense Validity Mask Network (DVMN) on the KITTI depth completion
benchmark and achieve state of the art results. At the time of submission, our
network is the leading method using sparsity invariant convolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reichardt_L/0/1/0/all/0/1"&gt;Laurenz Reichardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mangat_P/0/1/0/all/0/1"&gt;Patrick Mangat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1"&gt;Oliver Wasenm&amp;#xfc;ller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Image Compression with Discretized Gaussian-Laplacian-Logistic Mixture Model and Concatenated Residual Modules. (arXiv:2107.06463v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06463</id>
        <link href="http://arxiv.org/abs/2107.06463"/>
        <updated>2021-07-15T01:59:03.048Z</updated>
        <summary type="html"><![CDATA[Recently deep learning-based image compression methods have achieved
significant achievements and gradually outperformed traditional approaches
including the latest standard Versatile Video Coding (VVC) in both PSNR and
MS-SSIM metrics. Two key components of learned image compression frameworks are
the entropy model of the latent representations and the encoding/decoding
network architectures. Various models have been proposed, such as
autoregressive, softmax, logistic mixture, Gaussian mixture, and Laplacian.
Existing schemes only use one of these models. However, due to the vast
diversity of images, it is not optimal to use one model for all images, even
different regions of one image. In this paper, we propose a more flexible
discretized Gaussian-Laplacian-Logistic mixture model (GLLMM) for the latent
representations, which can adapt to different contents in different images and
different regions of one image more accurately. Besides, in the
encoding/decoding network design part, we propose a concatenated residual
blocks (CRB), where multiple residual blocks are serially connected with
additional shortcut connections. The CRB can improve the learning ability of
the network, which can further improve the compression performance.
Experimental results using the Kodak and Tecnick datasets show that the
proposed scheme outperforms all the state-of-the-art learning-based methods and
existing compression standards including VVC intra coding (4:4:4 and 4:2:0) in
terms of the PSNR and MS-SSIM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1"&gt;Haisheng Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_F/0/1/0/all/0/1"&gt;Feng Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jianping Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1"&gt;Bing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Akbari_M/0/1/0/all/0/1"&gt;Mohammad Akbari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jie Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guohe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tu_C/0/1/0/all/0/1"&gt;Chengjie Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1"&gt;Jingning Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Multi-Modal Alignment for Whole Body Medical Imaging. (arXiv:2107.06652v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06652</id>
        <link href="http://arxiv.org/abs/2107.06652"/>
        <updated>2021-07-15T01:59:03.042Z</updated>
        <summary type="html"><![CDATA[This paper explores the use of self-supervised deep learning in medical
imaging in cases where two scan modalities are available for the same subject.
Specifically, we use a large publicly-available dataset of over 20,000 subjects
from the UK Biobank with both whole body Dixon technique magnetic resonance
(MR) scans and also dual-energy x-ray absorptiometry (DXA) scans. We make three
contributions: (i) We introduce a multi-modal image-matching contrastive
framework, that is able to learn to match different-modality scans of the same
subject with high accuracy. (ii) Without any adaption, we show that the
correspondences learnt during this contrastive training step can be used to
perform automatic cross-modal scan registration in a completely unsupervised
manner. (iii) Finally, we use these registrations to transfer segmentation maps
from the DXA scans to the MR scans where they are used to train a network to
segment anatomical regions without requiring ground-truth MR examples. To aid
further research, our code will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Windsor_R/0/1/0/all/0/1"&gt;Rhydian Windsor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1"&gt;Amir Jamaludin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1"&gt;Timor Kadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Human Motion Prediction via A Bayesian Neural Network. (arXiv:2107.06564v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.06564</id>
        <link href="http://arxiv.org/abs/2107.06564"/>
        <updated>2021-07-15T01:59:03.027Z</updated>
        <summary type="html"><![CDATA[Human motion prediction is an important and challenging topic that has
promising prospects in efficient and safe human-robot-interaction systems.
Currently, the majority of the human motion prediction algorithms are based on
deterministic models, which may lead to risky decisions for robots. To solve
this problem, we propose a probabilistic model for human motion prediction in
this paper. The key idea of our approach is to extend the conventional
deterministic motion prediction neural network to a Bayesian one. On one hand,
our model could generate several future motions when given an observed motion
sequence. On the other hand, by calculating the Epistemic Uncertainty and the
Heteroscedastic Aleatoric Uncertainty, our model could tell the robot if the
observation has been seen before and also give the optimal result among all
possible predictions. We extensively validate our approach on a large scale
benchmark dataset Human3.6m. The experiments show that our approach performs
better than deterministic methods. We further evaluate our approach in a
Human-Robot-Interaction (HRI) scenario. The experimental results show that our
approach makes the interaction more efficient and safer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xingyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1"&gt;Xuguang Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nanning Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence in PET: an Industry Perspective. (arXiv:2107.06747v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06747</id>
        <link href="http://arxiv.org/abs/2107.06747"/>
        <updated>2021-07-15T01:59:03.021Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence (AI) has significant potential to positively impact
and advance medical imaging, including positron emission tomography (PET)
imaging applications. AI has the ability to enhance and optimize all aspects of
the PET imaging chain from patient scheduling, patient setup, protocoling, data
acquisition, detector signal processing, reconstruction, image processing and
interpretation. AI poses industry-specific challenges which will need to be
addressed and overcome to maximize the future potentials of AI in PET. This
paper provides an overview of these industry-specific challenges for the
development, standardization, commercialization, and clinical adoption of AI,
and explores the potential enhancements to PET imaging brought on by AI in the
near future. In particular, the combination of on-demand image reconstruction,
AI, and custom designed data processing workflows may open new possibilities
for innovation which would positively impact the industry and ultimately
patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1"&gt;Arkadiusz Sitek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sangtae Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asma_E/0/1/0/all/0/1"&gt;Evren Asma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandler_A/0/1/0/all/0/1"&gt;Adam Chandler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ihsani_A/0/1/0/all/0/1"&gt;Alvin Ihsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prevrhal_S/0/1/0/all/0/1"&gt;Sven Prevrhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmim_A/0/1/0/all/0/1"&gt;Arman Rahmim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saboury_B/0/1/0/all/0/1"&gt;Babak Saboury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thielemans_K/0/1/0/all/0/1"&gt;Kris Thielemans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative and reproducible benchmarks for comprehensive evaluation of machine learning classifiers. (arXiv:2107.06475v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06475</id>
        <link href="http://arxiv.org/abs/2107.06475"/>
        <updated>2021-07-15T01:59:03.015Z</updated>
        <summary type="html"><![CDATA[Understanding the strengths and weaknesses of machine learning (ML)
algorithms is crucial for determine their scope of application. Here, we
introduce the DIverse and GENerative ML Benchmark (DIGEN) - a collection of
synthetic datasets for comprehensive, reproducible, and interpretable
benchmarking of machine learning algorithms for classification of binary
outcomes. The DIGEN resource consists of 40 mathematical functions which map
continuous features to discrete endpoints for creating synthetic datasets.
These 40 functions were discovered using a heuristic algorithm designed to
maximize the diversity of performance among multiple popular machine learning
algorithms thus providing a useful test suite for evaluating and comparing new
methods. Access to the generative functions facilitates understanding of why a
method performs poorly compared to other algorithms thus providing ideas for
improvement. The resource with extensive documentation and analyses is
open-source and available on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orzechowski_P/0/1/0/all/0/1"&gt;Patryk Orzechowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1"&gt;Jason H. Moore&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Convolutional Neural Network Approach to the Classification of Engineering Models. (arXiv:2107.06481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06481</id>
        <link href="http://arxiv.org/abs/2107.06481"/>
        <updated>2021-07-15T01:59:03.008Z</updated>
        <summary type="html"><![CDATA[This paper presents a deep learning approach for the classification of
Engineering (CAD) models using Convolutional Neural Networks (CNNs). Owing to
the availability of large annotated datasets and also enough computational
power in the form of GPUs, many deep learning-based solutions for object
classification have been proposed of late, especially in the domain of images
and graphical models. Nevertheless, very few solutions have been proposed for
the task of functional classification of CAD models. Hence, for this research,
CAD models have been collected from Engineering Shape Benchmark (ESB), National
Design Repository (NDR) and augmented with newer models created using a
modelling software to form a dataset - 'CADNET'. It is proposed to use a
residual network architecture for CADNET, inspired by the popular ResNet. A
weighted Light Field Descriptor (LFD) scheme is chosen as the method of feature
extraction, and the generated images are fed as inputs to the CNN. The problem
of class imbalance in the dataset is addressed using a class weights approach.
Experiments have been conducted with other signatures such as geodesic distance
etc. using deep networks as well as other network architectures on the CADNET.
The LFD-based CNN approach using the proposed network architecture, along with
gradient boosting yielded the best classification accuracy on CADNET.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1"&gt;Bharadwaj Manda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhaskare_P/0/1/0/all/0/1"&gt;Pranjal Bhaskare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1"&gt;Ramanathan Muthuganapathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Temporal Action Detection with Transformer. (arXiv:2106.10271v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10271</id>
        <link href="http://arxiv.org/abs/2106.10271"/>
        <updated>2021-07-15T01:59:03.002Z</updated>
        <summary type="html"><![CDATA[Temporal action detection (TAD) aims to determine the semantic label and the
boundaries of every action instance in an untrimmed video. It is a fundamental
and challenging task in video understanding and significant progress has been
made. Previous methods involve multiple stages or networks and hand-designed
rules or operations, which fall short in efficiency and flexibility. In this
paper, we propose an end-to-end framework for TAD upon Transformer, termed
\textit{TadTR}, which maps a set of learnable embeddings to action instances in
parallel. TadTR is able to adaptively extract temporal context information
required for making action predictions, by selectively attending to a sparse
set of snippets in a video. As a result, it simplifies the pipeline of TAD and
requires lower computation cost than previous detectors, while preserving
remarkable detection performance. TadTR achieves state-of-the-art performance
on HACS Segments (+3.35% average mAP). As a single-network detector, TadTR runs
10$\times$ faster than its comparable competitor. It outperforms existing
single-network detectors by a large margin on THUMOS14 (+5.0% average mAP) and
ActivityNet (+7.53% average mAP). When combined with other detectors, it
reports 54.1% mAP at IoU=0.5 on THUMOS14, and 34.55% average mAP on
ActivityNet-1.3. Our code will be released at
\url{https://github.com/xlliu7/TadTR}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaolong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qimeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1"&gt;Song Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiang Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Neural Rendering for Image Hazing. (arXiv:2107.06681v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06681</id>
        <link href="http://arxiv.org/abs/2107.06681"/>
        <updated>2021-07-15T01:59:02.996Z</updated>
        <summary type="html"><![CDATA[Image hazing aims to render a hazy image from a given clean one, which could
be applied to a variety of practical applications such as gaming, filming,
photographic filtering, and image dehazing. To generate plausible haze, we
study two less-touched but challenging problems in hazy image rendering,
namely, i) how to estimate the transmission map from a single image without
auxiliary information, and ii) how to adaptively learn the airlight from
exemplars, i.e., unpaired real hazy images. To this end, we propose a neural
rendering method for image hazing, dubbed as HazeGEN. To be specific, HazeGEN
is a knowledge-driven neural network which estimates the transmission map by
leveraging a new prior, i.e., there exists the structure similarity (e.g.,
contour and luminance) between the transmission map and the input clean image.
To adaptively learn the airlight, we build a neural module based on another new
prior, i.e., the rendered hazy image and the exemplar are similar in the
airlight distribution. To the best of our knowledge, this could be the first
attempt to deeply rendering hazy images in an unsupervised fashion. Comparing
with existing haze generation methods, HazeGEN renders the hazy images in an
unsupervised, learnable, and controllable manner, thus avoiding the
labor-intensive efforts in paired data collection and the domain-shift issue in
haze generation. Extensive experiments show the promising performance of our
method comparing with some baselines in both qualitative and quantitative
comparisons. The code will be released on GitHub after acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boyun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yijie Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1"&gt;Peng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1"&gt;Jiancheng Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xi Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth. (arXiv:2104.14540v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14540</id>
        <link href="http://arxiv.org/abs/2104.14540"/>
        <updated>2021-07-15T01:59:02.978Z</updated>
        <summary type="html"><![CDATA[Self-supervised monocular depth estimation networks are trained to predict
scene depth using nearby frames as a supervision signal during training.
However, for many applications, sequence information in the form of video
frames is also available at test time. The vast majority of monocular networks
do not make use of this extra signal, thus ignoring valuable information that
could be used to improve the predicted depth. Those that do, either use
computationally expensive test-time refinement techniques or off-the-shelf
recurrent networks, which only indirectly make use of the geometric information
that is inherently available.

We propose ManyDepth, an adaptive approach to dense depth estimation that can
make use of sequence information at test time, when it is available. Taking
inspiration from multi-view stereo, we propose a deep end-to-end cost volume
based approach that is trained using self-supervision only. We present a novel
consistency loss that encourages the network to ignore the cost volume when it
is deemed unreliable, e.g. in the case of moving objects, and an augmentation
scheme to cope with static cameras. Our detailed experiments on both KITTI and
Cityscapes show that we outperform all published self-supervised baselines,
including those that use single or multiple frames at test time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watson_J/0/1/0/all/0/1"&gt;Jamie Watson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1"&gt;Oisin Mac Aodha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1"&gt;Victor Prisacariu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brostow_G/0/1/0/all/0/1"&gt;Gabriel Brostow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Firman_M/0/1/0/all/0/1"&gt;Michael Firman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GREN: Graph-Regularized Embedding Network for Weakly-Supervised Disease Localization in X-ray images. (arXiv:2107.06442v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06442</id>
        <link href="http://arxiv.org/abs/2107.06442"/>
        <updated>2021-07-15T01:59:02.971Z</updated>
        <summary type="html"><![CDATA[Locating diseases in chest X-ray images with few careful annotations saves
large human effort. Recent works approached this task with innovative
weakly-supervised algorithms such as multi-instance learning (MIL) and class
activation maps (CAM), however, these methods often yield inaccurate or
incomplete regions. One of the reasons is the neglection of the pathological
implications hidden in the relationship across anatomical regions within each
image and the relationship across images. In this paper, we argue that the
cross-region and cross-image relationship, as contextual and compensating
information, is vital to obtain more consistent and integral regions. To model
the relationship, we propose the Graph Regularized Embedding Network (GREN),
which leverages the intra-image and inter-image information to locate diseases
on chest X-ray images. GREN uses a pre-trained U-Net to segment the lung lobes,
and then models the intra-image relationship between the lung lobes using an
intra-image graph to compare different regions. Meanwhile, the relationship
between in-batch images is modeled by an inter-image graph to compare multiple
images. This process mimics the training and decision-making process of a
radiologist: comparing multiple regions and images for diagnosis. In order for
the deep embedding layers of the neural network to retain structural
information (important in the localization task), we use the Hash coding and
Hamming distance to compute the graphs, which are used as regularizers to
facilitate training. By means of this, our approach achieves the
state-of-the-art result on NIH chest X-ray dataset for weakly-supervised
disease localization. Our codes are accessible online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_B/0/1/0/all/0/1"&gt;Baolian Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Gangming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1"&gt;Chaowei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1"&gt;Chengwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinpeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Huiguang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1"&gt;Licheng Jiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Analysis of Visual COVID-19 Features from Chest Radiographs. (arXiv:2107.06618v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06618</id>
        <link href="http://arxiv.org/abs/2107.06618"/>
        <updated>2021-07-15T01:59:02.965Z</updated>
        <summary type="html"><![CDATA[Chest radiography has been a recommended procedure for patient triaging and
resource management in intensive care units (ICUs) throughout the COVID-19
pandemic. The machine learning efforts to augment this workflow have been long
challenged due to deficiencies in reporting, model evaluation, and failure mode
analysis. To address some of those shortcomings, we model radiological features
with a human-interpretable class hierarchy that aligns with the radiological
decision process. Also, we propose the use of a data-driven error analysis
methodology to uncover the blind spots of our model, providing further
transparency on its clinical utility. For example, our experiments show that
model failures highly correlate with ICU imaging conditions and with the
inherent difficulty in distinguishing certain types of radiological features.
Also, our hierarchical interpretation and analysis facilitates the comparison
with respect to radiologists' findings and inter-variability, which in return
helps us to better assess the clinical applicability of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bannur_S/0/1/0/all/0/1"&gt;Shruthi Bannur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oktay_O/0/1/0/all/0/1"&gt;Ozan Oktay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bernhardt_M/0/1/0/all/0/1"&gt;Melanie Bernhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schwaighofer_A/0/1/0/all/0/1"&gt;Anton Schwaighofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1"&gt;Rajesh Jena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nushi_B/0/1/0/all/0/1"&gt;Besmira Nushi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wadhwani_S/0/1/0/all/0/1"&gt;Sharan Wadhwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nori_A/0/1/0/all/0/1"&gt;Aditya Nori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Natarajan_K/0/1/0/all/0/1"&gt;Kal Natarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ashraf_S/0/1/0/all/0/1"&gt;Shazad Ashraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alvarez_Valle_J/0/1/0/all/0/1"&gt;Javier Alvarez-Valle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Castro_D/0/1/0/all/0/1"&gt;Daniel C. Castro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Abnormal Behavior with Self-Supervised Gaze Estimation. (arXiv:2107.06530v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06530</id>
        <link href="http://arxiv.org/abs/2107.06530"/>
        <updated>2021-07-15T01:59:02.959Z</updated>
        <summary type="html"><![CDATA[Due to the recent outbreak of COVID-19, many classes, exams, and meetings
have been conducted non-face-to-face. However, the foundation for video
conferencing solutions is still insufficient. So this technology has become an
important issue. In particular, these technologies are essential for
non-face-to-face testing, and technology dissemination is urgent. In this
paper, we present a single video conferencing solution using gaze estimation in
preparation for these problems. Gaze is an important cue for the tasks such as
analysis of human behavior. Hence, numerous studies have been proposed to solve
gaze estimation using deep learning, which is one of the most prominent methods
up to date. We use these gaze estimation methods to detect abnormal behavior of
video conferencing participants. Our contribution is as follows. i) We find and
apply the optimal network for the gaze estimation method and apply a
self-supervised method to improve accuracy. ii) For anomaly detection, we
present a new dataset that aggregates the values of a new gaze, head pose, etc.
iii) We train newly created data on Multi Layer Perceptron (MLP) models to
detect anomaly behavior based on deep learning. We demonstrate the robustness
of our method through experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suneung-Kim/0/1/0/all/0/1"&gt;Suneung-Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Speed and High-Quality Text-to-Lip Generation. (arXiv:2107.06831v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.06831</id>
        <link href="http://arxiv.org/abs/2107.06831"/>
        <updated>2021-07-15T01:59:02.942Z</updated>
        <summary type="html"><![CDATA[As a key component of talking face generation, lip movements generation
determines the naturalness and coherence of the generated talking face video.
Prior literature mainly focuses on speech-to-lip generation while there is a
paucity in text-to-lip (T2L) generation. T2L is a challenging task and existing
end-to-end works depend on the attention mechanism and autoregressive (AR)
decoding manner. However, the AR decoding manner generates current lip frame
conditioned on frames generated previously, which inherently hinders the
inference speed, and also has a detrimental effect on the quality of generated
lip frames due to error propagation. This encourages the research of parallel
T2L generation. In this work, we propose a novel parallel decoding model for
high-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we
predict the duration of the encoded linguistic features and model the target
lip frames conditioned on the encoded linguistic features with their duration
in a non-autoregressive manner. Furthermore, we incorporate the structural
similarity index loss and adversarial learning to improve perceptual quality of
generated lip frames and alleviate the blurry prediction problem. Extensive
experiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L
generates lip movements with competitive quality compared with the
state-of-the-art AR T2L model DualLip and exceeds the baseline AR model
TransformerT2L by a notable margin benefiting from the mitigation of the error
propagation problem; and 2) exhibits distinct superiority in inference speed
(an average speedup of 19$\times$ than DualLip on TCD-TIMIT).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhiying Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhou Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels. (arXiv:2105.03857v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03857</id>
        <link href="http://arxiv.org/abs/2105.03857"/>
        <updated>2021-07-15T01:59:02.936Z</updated>
        <summary type="html"><![CDATA[Detection faults in seismic data is a crucial step for seismic structural
interpretation, reservoir characterization and well placement. Some recent
works regard it as an image segmentation task. The task of image segmentation
requires huge labels, especially 3D seismic data, which has a complex structure
and lots of noise. Therefore, its annotation requires expert experience and a
huge workload. In this study, we present lambda-BCE and lambda-smooth L1loss to
effectively train 3D-CNN by some slices from 3D seismic data, so that the model
can learn the segmentation of 3D seismic data from a few 2D slices. In order to
fully extract information from limited data and suppress seismic noise, we
propose an attention module that can be used for active supervision training
and embedded in the network. The attention heatmap label is generated by the
original label, and letting it supervise the attention module using the
lambda-smooth L1loss. The experiment demonstrates the effectiveness of our loss
function, the method can extract 3D seismic features from a few 2D slice
labels. And it also shows the advanced performance of the attention module,
which can significantly suppress the noise in the seismic data while increasing
the model's sensitivity to the foreground. Finally, on the public test set, we
only use the 2D slice labels training that accounts for 3.3% of the 3D volume
label, and achieve similar performance to the 3D volume label training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1"&gt;YiMin Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kewen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianbing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1"&gt;Yingjie Xi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Initial Pools for Deep Active Learning. (arXiv:2011.14696v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14696</id>
        <link href="http://arxiv.org/abs/2011.14696"/>
        <updated>2021-07-15T01:59:02.929Z</updated>
        <summary type="html"><![CDATA[Active Learning (AL) techniques aim to minimize the training data required to
train a model for a given task. Pool-based AL techniques start with a small
initial labeled pool and then iteratively pick batches of the most informative
samples for labeling. Generally, the initial pool is sampled randomly and
labeled to seed the AL iterations. While recent studies have focused on
evaluating the robustness of various query functions in AL, little to no
attention has been given to the design of the initial labeled pool for deep
active learning. Given the recent successes of learning representations in
self-supervised/unsupervised ways, we study if an intelligently sampled initial
labeled pool can improve deep AL performance. We investigate the effect of
intelligently sampled initial labeled pools, including the use of
self-supervised and unsupervised strategies, on deep AL methods. The setup,
hypotheses, methodology, and implementation details were evaluated by peer
review before experiments were conducted. Experimental results could not
conclusively prove that intelligently sampled initial pools are better for AL
than random initial pools in the long run, although a Variational
Autoencoder-based initial pool sampling strategy showed interesting trends that
merit deeper investigation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1"&gt;Akshay L Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1"&gt;Sai Vikas Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1"&gt;Chaitanya Devaguptapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressive Representations of Weather Scenes for Strategic Air Traffic Flow Management. (arXiv:2107.06394v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06394</id>
        <link href="http://arxiv.org/abs/2107.06394"/>
        <updated>2021-07-15T01:59:02.923Z</updated>
        <summary type="html"><![CDATA[Terse representation of high-dimensional weather scene data is explored, in
support of strategic air traffic flow management objectives. Specifically, we
consider whether aviation-relevant weather scenes are compressible, in the
sense that each scene admits a possibly-different sparse representation in a
basis of interest. Here, compression of weather scenes extracted from METAR
data (including temperature, flight categories, and visibility profiles for the
contiguous United States) is examined, for the graph-spectral basis. The scenes
are found to be compressible, with 75-95% of the scene content captured using
0.5-4% of the basis vectors. Further, the dominant basis vectors for each scene
are seen to identify time-varying spatial characteristics of the weather, and
reconstruction from the compressed representation is demonstrated. Finally,
potential uses of the compressive representations in strategic TFM design are
briefly scoped.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Sandip Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graphhopper: Multi-Hop Scene Graph Reasoning for Visual Question Answering. (arXiv:2107.06325v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06325</id>
        <link href="http://arxiv.org/abs/2107.06325"/>
        <updated>2021-07-15T01:59:02.917Z</updated>
        <summary type="html"><![CDATA[Visual Question Answering (VQA) is concerned with answering free-form
questions about an image. Since it requires a deep semantic and linguistic
understanding of the question and the ability to associate it with various
objects that are present in the image, it is an ambitious task and requires
multi-modal reasoning from both computer vision and natural language
processing. We propose Graphhopper, a novel method that approaches the task by
integrating knowledge graph reasoning, computer vision, and natural language
processing techniques. Concretely, our method is based on performing
context-driven, sequential reasoning based on the scene entities and their
semantic and spatial relationships. As a first step, we derive a scene graph
that describes the objects in the image, as well as their attributes and their
mutual relationships. Subsequently, a reinforcement learning agent is trained
to autonomously navigate in a multi-hop manner over the extracted scene graph
to generate reasoning paths, which are the basis for deriving answers. We
conduct an experimental study on the challenging dataset GQA, based on both
manually curated and automatically generated scene graphs. Our results show
that we keep up with a human performance on manually curated scene graphs.
Moreover, we find that Graphhopper outperforms another state-of-the-art scene
graph reasoning model on both manually curated and automatically generated
scene graphs by a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koner_R/0/1/0/all/0/1"&gt;Rajat Koner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hildebrandt_M/0/1/0/all/0/1"&gt;Marcel Hildebrandt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Deepan Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Detection in the DCT Domain: is Luminance the Solution?. (arXiv:2006.05732v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05732</id>
        <link href="http://arxiv.org/abs/2006.05732"/>
        <updated>2021-07-15T01:59:02.911Z</updated>
        <summary type="html"><![CDATA[Object detection in images has reached unprecedented performances. The
state-of-the-art methods rely on deep architectures that extract salient
features and predict bounding boxes enclosing the objects of interest. These
methods essentially run on RGB images. However, the RGB images are often
compressed by the acquisition devices for storage purpose and transfer
efficiency. Hence, their decompression is required for object detectors. To
gain in efficiency, this paper proposes to take advantage of the compressed
representation of images to carry out object detection usable in constrained
resources conditions.

Specifically, we focus on JPEG images and propose a thorough analysis of
detection architectures newly designed in regard of the peculiarities of the
JPEG norm. This leads to a $\times 1.7$ speed up in comparison with a standard
RGB-based architecture, while only reducing the detection performance by 5.5%.
Additionally, our empirical findings demonstrate that only part of the
compressed JPEG information, namely the luminance component, may be required to
match detection accuracy of the full input methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deguerre_B/0/1/0/all/0/1"&gt;Benjamin Deguerre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatelain_C/0/1/0/all/0/1"&gt;Clement Chatelain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasso_G/0/1/0/all/0/1"&gt;Gilles Gasso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Neural Human Performance Rendering from Sparse RGBD Videos. (arXiv:2107.06505v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06505</id>
        <link href="http://arxiv.org/abs/2107.06505"/>
        <updated>2021-07-15T01:59:02.895Z</updated>
        <summary type="html"><![CDATA[Recent neural rendering approaches for human activities achieve remarkable
view synthesis results, but still rely on dense input views or dense training
with all the capture frames, leading to deployment difficulty and inefficient
training overload. However, existing advances will be ill-posed if the input is
both spatially and temporally sparse. To fill this gap, in this paper we
propose a few-shot neural human rendering approach (FNHR) from only sparse RGBD
inputs, which exploits the temporal and spatial redundancy to generate
photo-realistic free-view output of human activities. Our FNHR is trained only
on the key-frames which expand the motion manifold in the input sequences. We
introduce a two-branch neural blending to combine the neural point render and
classical graphics texturing pipeline, which integrates reliable observations
over sparse key-frames. Furthermore, we adopt a patch-based adversarial
training process to make use of the local redundancy and avoids over-fitting to
the key-frames, which generates fine-detailed rendering results. Extensive
experiments demonstrate the effectiveness of our approach to generate
high-quality free view-point results for challenging human performances under
the sparse setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1"&gt;Anqi Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haimin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Minye Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jingyi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lan Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AID-Purifier: A Light Auxiliary Network for Boosting Adversarial Defense. (arXiv:2107.06456v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06456</id>
        <link href="http://arxiv.org/abs/2107.06456"/>
        <updated>2021-07-15T01:59:02.889Z</updated>
        <summary type="html"><![CDATA[We propose an AID-purifier that can boost the robustness of
adversarially-trained networks by purifying their inputs. AID-purifier is an
auxiliary network that works as an add-on to an already trained main
classifier. To keep it computationally light, it is trained as a discriminator
with a binary cross-entropy loss. To obtain additionally useful information
from the adversarial examples, the architecture design is closely related to
information maximization principles where two layers of the main classification
network are piped to the auxiliary network. To assist the iterative
optimization procedure of purification, the auxiliary network is trained with
AVmixup. AID-purifier can be used together with other purifiers such as
PixelDefend for an extra enhancement. The overall results indicate that the
best performing adversarially-trained networks can be enhanced by the best
performing purification networks, where AID-purifier is a competitive candidate
that is light and robust.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1"&gt;Duhun Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1"&gt;Eunjung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1"&gt;Wonjong Rhee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Ultrasound Frame to Volume Registration. (arXiv:2107.06449v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06449</id>
        <link href="http://arxiv.org/abs/2107.06449"/>
        <updated>2021-07-15T01:59:02.883Z</updated>
        <summary type="html"><![CDATA[Fusing intra-operative 2D transrectal ultrasound (TRUS) image with
pre-operative 3D magnetic resonance (MR) volume to guide prostate biopsy can
significantly increase the yield. However, such a multimodal 2D/3D registration
problem is a very challenging task. In this paper, we propose an end-to-end
frame-to-volume registration network (FVR-Net), which can efficiently bridge
the previous research gaps by aligning a 2D TRUS frame with a 3D TRUS volume
without requiring hardware tracking. The proposed FVR-Net utilizes a
dual-branch feature extraction module to extract the information from TRUS
frame and volume to estimate transformation parameters. We also introduce a
differentiable 2D slice sampling module which allows gradients backpropagating
from an unsupervised image similarity loss for content correspondence learning.
Our model shows superior efficiency for real-time interventional guidance with
highly competitive registration accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hengtao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xuanang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1"&gt;Sheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wood_B/0/1/0/all/0/1"&gt;Bradford J. Wood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1"&gt;Pingkun Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning. (arXiv:2107.06501v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06501</id>
        <link href="http://arxiv.org/abs/2107.06501"/>
        <updated>2021-07-15T01:59:02.877Z</updated>
        <summary type="html"><![CDATA[High-level representation-guided pixel denoising and adversarial training are
independent solutions to enhance the robustness of CNNs against adversarial
attacks by pre-processing input data and re-training models, respectively. Most
recently, adversarial training techniques have been widely studied and improved
while the pixel denoising-based method is getting less attractive. However, it
is still questionable whether there exists a more advanced pixel
denoising-based method and whether the combination of the two solutions
benefits each other. To this end, we first comprehensively investigate two
kinds of pixel denoising methods for adversarial robustness enhancement (i.e.,
existing additive-based and unexplored filtering-based methods) under the loss
functions of image-level and semantic-level restorations, respectively, showing
that pixel-wise filtering can obtain much higher image quality (e.g., higher
PSNR) as well as higher robustness (e.g., higher accuracy on adversarial
examples) than existing pixel-wise additive-based method. However, we also
observe that the robustness results of the filtering-based method rely on the
perturbation amplitude of adversarial examples used for training. To address
this problem, we propose predictive perturbation-aware pixel-wise filtering,
where dual-perturbation filtering and an uncertainty-aware fusion module are
designed and employed to automatically perceive the perturbation amplitude
during the training and testing process. The proposed method is termed as
AdvFilter. Moreover, we combine adversarial pixel denoising methods with three
adversarial training-based methods, hinting that considering data and models
jointly is able to achieve more robust CNNs. The experiments conduct on
NeurIPS-2017DEV, SVHN, and CIFAR10 datasets and show the advantages over
enhancing CNNs' robustness, high generalization to different models, and noise
levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yihao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qing Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1"&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_W/0/1/0/all/0/1"&gt;Weikai Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1"&gt;Geguang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDC: Piecewise Depth Completion utilizing Superpixels. (arXiv:2107.06711v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06711</id>
        <link href="http://arxiv.org/abs/2107.06711"/>
        <updated>2021-07-15T01:59:02.861Z</updated>
        <summary type="html"><![CDATA[Depth completion from sparse LiDAR and high-resolution RGB data is one of the
foundations for autonomous driving techniques. Current approaches often rely on
CNN-based methods with several known drawbacks: flying pixel at depth
discontinuities, overfitting to both a given data set as well as error metric,
and many more. Thus, we propose our novel Piecewise Depth Completion (PDC),
which works completely without deep learning. PDC segments the RGB image into
superpixels corresponding the regions with similar depth value. Superpixels
corresponding to same objects are gathered using a cost map. At the end, we
receive detailed depth images with state of the art accuracy. In our
evaluation, we can show both the influence of the individual proposed
processing steps and the overall performance of our method on the challenging
KITTI dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teutscher_D/0/1/0/all/0/1"&gt;Dennis Teutscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mangat_P/0/1/0/all/0/1"&gt;Patrick Mangat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1"&gt;Oliver Wasenm&amp;#xfc;ller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06825</id>
        <link href="http://arxiv.org/abs/2107.06825"/>
        <updated>2021-07-15T01:59:02.855Z</updated>
        <summary type="html"><![CDATA[We introduce a generalization to the lottery ticket hypothesis in which the
notion of "sparsity" is relaxed by choosing an arbitrary basis in the space of
parameters. We present evidence that the original results reported for the
canonical basis continue to hold in this broader setting. We describe how
structured pruning methods, including pruning units or factorizing
fully-connected layers into products of low-rank matrices, can be cast as
particular instances of this "generalized" lottery ticket hypothesis. The
investigations reported here are preliminary and are provided to encourage
further research along this direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1"&gt;Ibrahim Alabdulmohsin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1"&gt;Larisa Markeeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1"&gt;Daniel Keysers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1"&gt;Ilya Tolstikhin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning based Novel View Synthesis. (arXiv:2107.06812v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06812</id>
        <link href="http://arxiv.org/abs/2107.06812"/>
        <updated>2021-07-15T01:59:02.849Z</updated>
        <summary type="html"><![CDATA[Predicting novel views of a scene from real-world images has always been a
challenging task. In this work, we propose a deep convolutional neural network
(CNN) which learns to predict novel views of a scene from given collection of
images. In comparison to prior deep learning based approaches, which can handle
only a fixed number of input images to predict novel view, proposed approach
works with different numbers of input images. The proposed model explicitly
performs feature extraction and matching from a given pair of input images and
estimates, at each pixel, the probability distribution (pdf) over possible
depth levels in the scene. This pdf is then used for estimating the novel view.
The model estimates multiple predictions of novel view, one estimate per input
image pair, from given image collection. The model also estimates an occlusion
mask and combines multiple novel view estimates in to a single optimal
prediction. The finite number of depth levels used in the analysis may cause
occasional blurriness in the estimated view. We mitigate this issue with simple
multi-resolution analysis which improves the quality of the estimates. We
substantiate the performance on different datasets and show competitive
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+More_A/0/1/0/all/0/1"&gt;Amit More&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Subhasis Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Hypothesis Transfer for Source-Free Domain Adaptation. (arXiv:2107.06735v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06735</id>
        <link href="http://arxiv.org/abs/2107.06735"/>
        <updated>2021-07-15T01:59:02.843Z</updated>
        <summary type="html"><![CDATA[Domain Adaptation has been widely used to deal with the distribution shift in
vision, language, multimedia etc. Most domain adaptation methods learn
domain-invariant features with data from both domains available. However, such
a strategy might be infeasible in practice when source data are unavailable due
to data-privacy concerns. To address this issue, we propose a novel adaptation
method via hypothesis transfer without accessing source data at adaptation
stage. In order to fully use the limited target data, a semi-supervised mutual
enhancement method is proposed, in which entropy minimization and augmented
label propagation are used iteratively to perform inter-domain and intra-domain
alignments. Compared with state-of-the-art methods, the experimental results on
three public datasets demonstrate that our method gets up to 19.9% improvements
on semi-supervised adaptation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1"&gt;Ning Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1"&gt;Jiajun Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Lixian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Jun Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xifeng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Foes of Neural Network's Data Efficiency Among Unnecessary Input Dimensions. (arXiv:2107.06409v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06409</id>
        <link href="http://arxiv.org/abs/2107.06409"/>
        <updated>2021-07-15T01:59:02.836Z</updated>
        <summary type="html"><![CDATA[Datasets often contain input dimensions that are unnecessary to predict the
output label, e.g. background in object recognition, which lead to more
trainable parameters. Deep Neural Networks (DNNs) are robust to increasing the
number of parameters in the hidden layers, but it is unclear whether this holds
true for the input layer. In this letter, we investigate the impact of
unnecessary input dimensions on a central issue of DNNs: their data efficiency,
ie. the amount of examples needed to achieve certain generalization
performance. Our results show that unnecessary input dimensions that are
task-unrelated substantially degrade data efficiency. This highlights the need
for mechanisms that remove {task-unrelated} dimensions to enable data
efficiency gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1"&gt;Vanessa D&amp;#x27;Amario&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Sanjana Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1"&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-isomorphic Inter-modality Graph Alignment and Synthesis for Holistic Brain Mapping. (arXiv:2107.06281v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2107.06281</id>
        <link href="http://arxiv.org/abs/2107.06281"/>
        <updated>2021-07-15T01:59:02.828Z</updated>
        <summary type="html"><![CDATA[Brain graph synthesis marked a new era for predicting a target brain graph
from a source one without incurring the high acquisition cost and processing
time of neuroimaging data. However, existing multi-modal graph synthesis
frameworks have several limitations. First, they mainly focus on generating
graphs from the same domain (intra-modality), overlooking the rich multimodal
representations of brain connectivity (inter-modality). Second, they can only
handle isomorphic graph generation tasks, limiting their generalizability to
synthesizing target graphs with a different node size and topological structure
from those of the source one. More importantly, both target and source domains
might have different distributions, which causes a domain fracture between them
(i.e., distribution misalignment). To address such challenges, we propose an
inter-modality aligner of non-isomorphic graphs (IMANGraphNet) framework to
infer a target graph modality based on a given modality. Our three core
contributions lie in (i) predicting a target graph (e.g., functional) from a
source graph (e.g., morphological) based on a novel graph generative
adversarial network (gGAN); (ii) using non-isomorphic graphs for both source
and target domains with a different number of nodes, edges and structure; and
(iii) enforcing the predicted target distribution to match that of the ground
truth graphs using a graph autoencoder to relax the designed loss oprimization.
To handle the unstable behavior of gGAN, we design a new Ground
Truth-Preserving (GT-P) loss function to guide the generator in learning the
topological structure of ground truth brain graphs. Our comprehensive
experiments on predicting functional from morphological graphs demonstrate the
outperformance of IMANGraphNet in comparison with its variants. This can be
further leveraged for integrative and holistic brain mapping in health and
disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Mhiri_I/0/1/0/all/0/1"&gt;Islem Mhiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Nebli_A/0/1/0/all/0/1"&gt;Ahmed Nebli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Mahjoub_M/0/1/0/all/0/1"&gt;Mohamed Ali Mahjoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1"&gt;Islem Rekik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments. (arXiv:2011.04408v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04408</id>
        <link href="http://arxiv.org/abs/2011.04408"/>
        <updated>2021-07-15T01:59:02.821Z</updated>
        <summary type="html"><![CDATA[Different environments pose a great challenge on the outdoor robust visual
perception for long-term autonomous driving and the generalization of
learning-based algorithms on different environmental effects is still an open
problem. Although monocular depth prediction has been well studied recently,
there is few work focusing on the robust learning-based depth prediction across
different environments, e.g., changing illumination and seasons, owing to the
lack of such a multi-environment real-world dataset and benchmark. To this end,
the first cross-season monocular depth prediction dataset and benchmark
SeasonDepth (available on https://seasondepth.github.io/) is built based on CMU
Visual Localization dataset. To benchmark the depth estimation performance
under different environments, we investigate representative and recent
state-of-the-art open-source supervised, self-supervised and domain adaptation
depth prediction methods from KITTI benchmark using several newly-formulated
metrics. Through extensive experimental evaluation on the proposed dataset, the
influence of multiple environments on performance and robustness is analyzed
both qualitatively and quantitatively, showing that the long-term monocular
depth prediction is far from solved even with fine-tuning. We further give
promising avenues that self-supervised training and stereo geometry constraint
help to enhance the robustness to changing environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hanjiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Baoquan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Zhijian Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hesheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Attention Generative Adversarial Network for Remote Sensing Image Super-Resolution. (arXiv:2107.06536v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06536</id>
        <link href="http://arxiv.org/abs/2107.06536"/>
        <updated>2021-07-15T01:59:02.814Z</updated>
        <summary type="html"><![CDATA[Image super-resolution (SR) methods can generate remote sensing images with
high spatial resolution without increasing the cost, thereby providing a
feasible way to acquire high-resolution remote sensing images, which are
difficult to obtain due to the high cost of acquisition equipment and complex
weather. Clearly, image super-resolution is a severe ill-posed problem.
Fortunately, with the development of deep learning, the powerful fitting
ability of deep neural networks has solved this problem to some extent. In this
paper, we propose a network based on the generative adversarial network (GAN)
to generate high resolution remote sensing images, named the multi-attention
generative adversarial network (MA-GAN). We first designed a GAN-based
framework for the image SR task. The core to accomplishing the SR task is the
image generator with post-upsampling that we designed. The main body of the
generator contains two blocks; one is the pyramidal convolution in the
residual-dense block (PCRDB), and the other is the attention-based upsample
(AUP) block. The attentioned pyramidal convolution (AttPConv) in the PCRDB
block is a module that combines multi-scale convolution and channel attention
to automatically learn and adjust the scaling of the residuals for better
results. The AUP block is a module that combines pixel attention (PA) to
perform arbitrary multiples of upsampling. These two blocks work together to
help generate better quality images. For the loss function, we design a loss
function based on pixel loss and introduce both adversarial loss and feature
loss to guide the generator learning. We have compared our method with several
state-of-the-art methods on a remote sensing scene image dataset, and the
experimental results consistently demonstrate the effectiveness of the proposed
MA-GAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Meng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhihao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiasong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiuping Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Sen Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Generalizability in Limited-Angle CT Reconstruction with Sinogram Extrapolation. (arXiv:2103.05255v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05255</id>
        <link href="http://arxiv.org/abs/2103.05255"/>
        <updated>2021-07-15T01:59:02.789Z</updated>
        <summary type="html"><![CDATA[Computed tomography (CT) reconstruction from X-ray projections acquired
within a limited angle range is challenging, especially when the angle range is
extremely small. Both analytical and iterative models need more projections for
effective modeling. Deep learning methods have gained prevalence due to their
excellent reconstruction performances, but such success is mainly limited
within the same dataset and does not generalize across datasets with different
distributions. Hereby we propose ExtraPolationNetwork for limited-angle CT
reconstruction via the introduction of a sinogram extrapolation module, which
is theoretically justified. The module complements extra sinogram information
and boots model generalizability. Extensive experimental results show that our
reconstruction model achieves state-of-the-art performance on NIH-AAPM dataset,
similar to existing approaches. More importantly, we show that using such a
sinogram extrapolation module significantly improves the generalization
capability of the model on unseen datasets (e.g., COVID-19 and LIDC datasets)
when compared to existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1"&gt;Ce Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haimiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shang_K/0/1/0/all/0/1"&gt;Kun Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yuanyuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dong_B/0/1/0/all/0/1"&gt;Bin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1"&gt;S. Kevin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Label Generalized Zero Shot Learning for the Classification of Disease in Chest Radiographs. (arXiv:2107.06563v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06563</id>
        <link href="http://arxiv.org/abs/2107.06563"/>
        <updated>2021-07-15T01:59:02.775Z</updated>
        <summary type="html"><![CDATA[Despite the success of deep neural networks in chest X-ray (CXR) diagnosis,
supervised learning only allows the prediction of disease classes that were
seen during training. At inference, these networks cannot predict an unseen
disease class. Incorporating a new class requires the collection of labeled
data, which is not a trivial task, especially for less frequently-occurring
diseases. As a result, it becomes inconceivable to build a model that can
diagnose all possible disease classes. Here, we propose a multi-label
generalized zero shot learning (CXR-ML-GZSL) network that can simultaneously
predict multiple seen and unseen diseases in CXR images. Given an input image,
CXR-ML-GZSL learns a visual representation guided by the input's corresponding
semantics extracted from a rich medical text corpus. Towards this ambitious
goal, we propose to map both visual and semantic modalities to a latent feature
space using a novel learning objective. The objective ensures that (i) the most
relevant labels for the query image are ranked higher than irrelevant labels,
(ii) the network learns a visual representation that is aligned with its
semantics in the latent feature space, and (iii) the mapped semantics preserve
their original inter-class representation. The network is end-to-end trainable
and requires no independent pre-training for the offline feature extractor.
Experiments on the NIH Chest X-ray dataset show that our network outperforms
two strong baselines in terms of recall, precision, f1 score, and area under
the receiver operating characteristic curve. Our code is publicly available at:
https://github.com/nyuad-cai/CXR-ML-GZSL.git]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hayat_N/0/1/0/all/0/1"&gt;Nasir Hayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lashen_H/0/1/0/all/0/1"&gt;Hazem Lashen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1"&gt;Farah E. Shamout&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3rd Place Solution for Short-video Face Parsing Challenge. (arXiv:2106.07409v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07409</id>
        <link href="http://arxiv.org/abs/2106.07409"/>
        <updated>2021-07-15T01:59:02.759Z</updated>
        <summary type="html"><![CDATA[This is a short technical report introducing the solution of Team Rat for
Short-video Parsing Face Parsing Track of The 3rd Person in Context (PIC)
Workshop and Challenge at CVPR 2021.

In this report, we propose an Edge-Aware Network (EANet) that uses edge
information to refine the segmentation edge. To further obtain the finer edge
results, we introduce edge attention loss that only compute cross entropy on
the edges, it can effectively reduce the classification error around edge and
get more smooth boundary. Benefiting from the edge information and edge
attention loss, the proposed EANet achieves 86.16\% accuracy in the Short-video
Face Parsing track of the 3rd Person in Context (PIC) Workshop and Challenge,
ranked the third place.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_X/0/1/0/all/0/1"&gt;Xiaofei Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiangtao Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Tensor Train Parameterization of Deep Learning Layers. (arXiv:2103.04217v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04217</id>
        <link href="http://arxiv.org/abs/2103.04217"/>
        <updated>2021-07-15T01:59:02.743Z</updated>
        <summary type="html"><![CDATA[We study low-rank parameterizations of weight matrices with embedded spectral
properties in the Deep Learning context. The low-rank property leads to
parameter efficiency and permits taking computational shortcuts when computing
mappings. Spectral properties are often subject to constraints in optimization
problems, leading to better models and stability of optimization. We start by
looking at the compact SVD parameterization of weight matrices and identifying
redundancy sources in the parameterization. We further apply the Tensor Train
(TT) decomposition to the compact SVD components, and propose a non-redundant
differentiable parameterization of fixed TT-rank tensor manifolds, termed the
Spectral Tensor Train Parameterization (STTP). We demonstrate the effects of
neural network compression in the image classification setting and both
compression and improved training stability in the generative adversarial
training setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1"&gt;Anton Obukhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1"&gt;Maxim Rakhuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1"&gt;Alexander Liniger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1"&gt;Stamatios Georgoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Dengxin Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiSTF: Bilateral-Branch Self-Training Framework for Semi-Supervised Large-scale Fine-Grained Recognition. (arXiv:2107.06768v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06768</id>
        <link href="http://arxiv.org/abs/2107.06768"/>
        <updated>2021-07-15T01:59:02.736Z</updated>
        <summary type="html"><![CDATA[Semi-supervised Fine-Grained Recognition is a challenge task due to the
difficulty of data imbalance, high inter-class similarity and domain mismatch.
Recent years, this field has witnessed great progress and many methods has
gained great performance. However, these methods can hardly generalize to the
large-scale datasets, such as Semi-iNat, as they are prone to suffer from noise
in unlabeled data and the incompetence for learning features from imbalanced
fine-grained data. In this work, we propose Bilateral-Branch Self-Training
Framework (BiSTF), a simple yet effective framework to improve existing
semi-supervised learning methods on class-imbalanced and domain-shifted
fine-grained data. By adjusting the update frequency through stochastic epoch
update, BiSTF iteratively retrains a baseline SSL model with a labeled set
expanded by selectively adding pseudo-labeled samples from an unlabeled set,
where the distribution of pseudo-labeled samples are the same as the labeled
data. We show that BiSTF outperforms the existing state-of-the-art SSL
algorithm on Semi-iNat dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1"&gt;Guochen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Q/0/1/0/all/0/1"&gt;Qiang Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization with Pseudo-Domain Label for Face Anti-Spoofing. (arXiv:2107.06552v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06552</id>
        <link href="http://arxiv.org/abs/2107.06552"/>
        <updated>2021-07-15T01:59:02.730Z</updated>
        <summary type="html"><![CDATA[Face anti-spoofing (FAS) plays an important role in protecting face
recognition systems from face representation attacks. Many recent studies in
FAS have approached this problem with domain generalization technique. Domain
generalization aims to increase generalization performance to better detect
various types of attacks and unseen attacks. However, previous studies in this
area have defined each domain simply as an anti-spoofing datasets and focused
on developing learning techniques. In this paper, we proposed a method that
enables network to judge its domain by itself with the clustered convolutional
feature statistics from intermediate layers of the network, without labeling
domains as datasets. We obtained pseudo-domain labels by not only using the
network extracting features, but also using depth estimators, which were
previously used only as an auxiliary task in FAS. In our experiments, we
trained with three datasets and evaluated the performance with the remaining
one dataset to demonstrate the effectiveness of the proposed method by
conducting a total of four sets of experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young Eun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Networks are Surprisingly Reversible: A Baseline for Zero-Shot Inversion. (arXiv:2107.06304v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06304</id>
        <link href="http://arxiv.org/abs/2107.06304"/>
        <updated>2021-07-15T01:59:02.724Z</updated>
        <summary type="html"><![CDATA[Understanding the behavior and vulnerability of pre-trained deep neural
networks (DNNs) can help to improve them. Analysis can be performed via
reversing the network's flow to generate inputs from internal representations.
Most existing work relies on priors or data-intensive optimization to invert a
model, yet struggles to scale to deep architectures and complex datasets. This
paper presents a zero-shot direct model inversion framework that recovers the
input to the trained model given only the internal representation. The crux of
our method is to inverse the DNN in a divide-and-conquer manner while
re-syncing the inverted layers via cycle-consistency guidance with the help of
synthesized data. As a result, we obtain a single feed-forward model capable of
inversion with a single forward pass without seeing any real data of the
original task. With the proposed approach, we scale zero-shot direct inversion
to deep architectures and complex datasets. We empirically show that modern
classification models on ImageNet can, surprisingly, be inverted, allowing an
approximate recovery of the original 224x224px images from a representation
after more than 20 layers. Moreover, inversion of generators in GANs unveils
latent code of a given synthesized face image at 128x128px, which can even, in
turn, improve defective synthesized images from GANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongxu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1"&gt;Jan Kautz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1"&gt;Pavlo Molchanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BRIMA: low-overhead BRowser-only IMage Annotation tool (Preprint). (arXiv:2107.06351v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06351</id>
        <link href="http://arxiv.org/abs/2107.06351"/>
        <updated>2021-07-15T01:59:02.718Z</updated>
        <summary type="html"><![CDATA[Image annotation and large annotated datasets are crucial parts within the
Computer Vision and Artificial Intelligence fields.At the same time, it is
well-known and acknowledged by the research community that the image annotation
process is challenging, time-consuming and hard to scale. Therefore, the
researchers and practitioners are always seeking ways to perform the
annotations easier, faster, and at higher quality. Even though several widely
used tools exist and the tools' landscape evolved considerably, most of the
tools still require intricate technical setups and high levels of technical
savviness from its operators and crowdsource contributors.

In order to address such challenges, we develop and present BRIMA -- a
flexible and open-source browser extension that allows BRowser-only IMage
Annotation at considerably lower overheads. Once added to the browser, it
instantly allows the user to annotate images easily and efficiently directly
from the browser without any installation or setup on the client-side. It also
features cross-browser and cross-platform functionality thus presenting itself
as a neat tool for researchers within the Computer Vision, Artificial
Intelligence, and privacy-related fields.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lahtinen_T/0/1/0/all/0/1"&gt;Tuomo Lahtinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turtiainen_H/0/1/0/all/0/1"&gt;Hannu Turtiainen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costin_A/0/1/0/all/0/1"&gt;Andrei Costin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface. (arXiv:2107.06393v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06393</id>
        <link href="http://arxiv.org/abs/2107.06393"/>
        <updated>2021-07-15T01:59:02.702Z</updated>
        <summary type="html"><![CDATA[Modeling complex phenomena typically involves the use of both discrete and
continuous variables. Such a setting applies across a wide range of problems,
from identifying trends in time-series data to performing effective
compositional scene understanding in images. Here, we propose Hybrid Memoised
Wake-Sleep (HMWS), an algorithm for effective inference in such hybrid
discrete-continuous models. Prior approaches to learning suffer as they need to
perform repeated expensive inner-loop discrete inference. We build on a recent
approach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by
memoising discrete variables, and extend it to allow for a principled and
effective way to handle continuous variables by learning a separate recognition
model used for importance-sampling based approximate inference and
marginalization. We evaluate HMWS in the GP-kernel learning and 3D scene
understanding domains, and show that it outperforms current state-of-the-art
inference methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Tuan Anh Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1"&gt;Katherine M. Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hewitt_L/0/1/0/all/0/1"&gt;Luke Hewitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1"&gt;Kevin Ellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1"&gt;Siddharth N&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1"&gt;Samuel J. Gershman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Much Can CLIP Benefit Vision-and-Language Tasks?. (arXiv:2107.06383v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06383</id>
        <link href="http://arxiv.org/abs/2107.06383"/>
        <updated>2021-07-15T01:59:02.694Z</updated>
        <summary type="html"><![CDATA[Most existing Vision-and-Language (V&L) models rely on pre-trained visual
encoders, using a relatively small set of manually-annotated data (as compared
to web-crawled data), to perceive the visual world. However, it has been
observed that large-scale pretraining usually can result in better
generalization performance, e.g., CLIP (Contrastive Language-Image
Pre-training), trained on a massive amount of image-caption pairs, has shown a
strong zero-shot capability on various vision tasks. To further study the
advantage brought by CLIP, we propose to use CLIP as the visual encoder in
various V&L models in two typical scenarios: 1) plugging CLIP into
task-specific fine-tuning; 2) combining CLIP with V&L pre-training and
transferring to downstream tasks. We show that CLIP significantly outperforms
widely-used visual encoders trained with in-domain annotated data, such as
BottomUp-TopDown. We achieve competitive or better results on diverse V&L
tasks, while establishing new state-of-the-art results on Visual Question
Answering, Visual Entailment, and V&L Navigation tasks. We release our code at
https://github.com/clip-vil/CLIP-ViL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Sheng Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liunian Harold Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1"&gt;Anna Rohrbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDMapNet: An Online HD Map Construction and Evaluation Framework. (arXiv:2107.06307v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06307</id>
        <link href="http://arxiv.org/abs/2107.06307"/>
        <updated>2021-07-15T01:59:02.688Z</updated>
        <summary type="html"><![CDATA[High-definition map (HD map) construction is a crucial problem for autonomous
driving. This problem typically involves collecting high-quality point clouds,
fusing multiple point clouds of the same scene, annotating map elements, and
updating maps constantly. This pipeline, however, requires a vast amount of
human efforts and resources which limits its scalability. Additionally,
traditional HD maps are coupled with centimeter-level accurate localization
which is unreliable in many scenarios. In this paper, we argue that online map
learning, which dynamically constructs the HD maps based on local sensor
observations, is a more scalable way to provide semantic and geometry priors to
self-driving vehicles than traditional pre-annotated HD maps. Meanwhile, we
introduce an online map learning method, titled HDMapNet. It encodes image
features from surrounding cameras and/or point clouds from LiDAR, and predicts
vectorized map elements in the bird's-eye view. We benchmark HDMapNet on the
nuScenes dataset and show that in all settings, it performs better than
baseline methods. Of note, our fusion-based HDMapNet outperforms existing
methods by more than 50% in all metrics. To accelerate future research, we
develop customized metrics to evaluate map learning performance, including both
semantic-level and instance-level ones. By introducing this method and metrics,
we invite the community to study this novel map learning problem. We will
release our code and evaluation kit to facilitate future development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yilun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Programming of Reaction-Diffusion Patterns. (arXiv:2107.06862v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.06862</id>
        <link href="http://arxiv.org/abs/2107.06862"/>
        <updated>2021-07-15T01:59:02.641Z</updated>
        <summary type="html"><![CDATA[Reaction-Diffusion (RD) systems provide a computational framework that
governs many pattern formation processes in nature. Current RD system design
practices boil down to trial-and-error parameter search. We propose a
differentiable optimization method for learning the RD system parameters to
perform example-based texture synthesis on a 2D plane. We do this by
representing the RD system as a variant of Neural Cellular Automata and using
task-specific differentiable loss functions. RD systems generated by our method
exhibit robust, non-trivial 'life-like' behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mordvintsev_A/0/1/0/all/0/1"&gt;Alexander Mordvintsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Randazzo_E/0/1/0/all/0/1"&gt;Ettore Randazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niklasson_E/0/1/0/all/0/1"&gt;Eyvind Niklasson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Algebraic Recombination for Compositional Generalization. (arXiv:2107.06516v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06516</id>
        <link href="http://arxiv.org/abs/2107.06516"/>
        <updated>2021-07-15T01:59:02.545Z</updated>
        <summary type="html"><![CDATA[Neural sequence models exhibit limited compositional generalization ability
in semantic parsing tasks. Compositional generalization requires algebraic
recombination, i.e., dynamically recombining structured expressions in a
recursive manner. However, most previous studies mainly concentrate on
recombining lexical units, which is an important but not sufficient part of
algebraic recombination. In this paper, we propose LeAR, an end-to-end neural
model to learn algebraic recombination for compositional generalization. The
key insight is to model the semantic parsing task as a homomorphism between a
latent syntactic algebra and a semantic algebra, thus encouraging algebraic
recombination. Specifically, we learn two modules jointly: a Composer for
producing latent syntax, and an Interpreter for assigning semantic operations.
Experiments on two realistic and comprehensive compositional generalization
benchmarks demonstrate the effectiveness of our model. The source code is
publicly available at https://github.com/microsoft/ContextualSP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chenyao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1"&gt;Shengnan An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zeqi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1"&gt;Jian-Guang Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1"&gt;Lijie Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nanning Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reference Knowledgeable Network for Machine Reading Comprehension. (arXiv:2012.03709v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03709</id>
        <link href="http://arxiv.org/abs/2012.03709"/>
        <updated>2021-07-15T01:59:02.527Z</updated>
        <summary type="html"><![CDATA[Multi-choice Machine Reading Comprehension (MRC) as a challenge requires
model to select the most appropriate answer from a set of candidates given
passage and question. Most of the existing researches focus on the modeling of
the task datasets without explicitly referring to external fine-grained
knowledge sources, which is supposed to greatly make up the deficiency of the
given passage. Thus we propose a novel reference-based knowledge enhancement
model called Reference Knowledgeable Network (RekNet), which refines critical
information from the passage and quote explicit knowledge in necessity. In
detail, RekNet refines fine-grained critical information and defines it as
Reference Span, then quotes explicit knowledge quadruples by the co-occurrence
information of Reference Span and candidates. The proposed RekNet is evaluated
on three multi-choice MRC benchmarks: RACE, DREAM and Cosmos QA, which shows
consistent and remarkable performance improvement with observable statistical
significance level over strong baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yilin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation. (arXiv:2107.06779v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06779</id>
        <link href="http://arxiv.org/abs/2107.06779"/>
        <updated>2021-07-15T01:59:02.521Z</updated>
        <summary type="html"><![CDATA[Emotion recognition in conversation (ERC) is a crucial component in affective
dialogue systems, which helps the system understand users' emotions and
generate empathetic responses. However, most works focus on modeling speaker
and contextual information primarily on the textual modality or simply
leveraging multimodal information through feature concatenation. In order to
explore a more effective way of utilizing both multimodal and long-distance
contextual information, we propose a new model based on multimodal fused graph
convolutional network, MMGCN, in this work. MMGCN can not only make use of
multimodal dependencies effectively, but also leverage speaker information to
model inter-speaker and intra-speaker dependency. We evaluate our proposed
model on two public benchmark datasets, IEMOCAP and MELD, and the results prove
the effectiveness of MMGCN, which outperforms other SOTA methods by a
significant margin under the multimodal conversation setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jingwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuchen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jinming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indonesia's Fake News Detection using Transformer Network. (arXiv:2107.06796v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06796</id>
        <link href="http://arxiv.org/abs/2107.06796"/>
        <updated>2021-07-15T01:59:02.515Z</updated>
        <summary type="html"><![CDATA[Fake news is a problem faced by society in this era. It is not rare for fake
news to cause provocation and problem for the people. Indonesia, as a country
with the 4th largest population, has a problem in dealing with fake news. More
than 30% of rural and urban population are deceived by this fake news problem.
As we have been studying, there is only few literatures on preventing the
spread of fake news in Bahasa Indonesia. So, this research is conducted to
prevent these problems. The dataset used in this research was obtained from a
news portal that identifies fake news, turnbackhoax.id. Using Web Scrapping on
this page, we got 1116 data consisting of valid news and fake news. The dataset
can be accessed at https://github.com/JibranFawaid/turnbackhoax-dataset. This
dataset will be combined with other available datasets. The methods used are
CNN, BiLSTM, Hybrid CNN-BiLSTM, and BERT with Transformer Network. This
research shows that the BERT method with Transformer Network has the best
results with an accuracy of up to 90%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awalina_A/0/1/0/all/0/1"&gt;Aisyah Awalina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fawaid_J/0/1/0/all/0/1"&gt;Jibran Fawaid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krisnabayu_R/0/1/0/all/0/1"&gt;Rifky Yunus Krisnabayu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1"&gt;Novanto Yudistira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ParCourE: A Parallel Corpus Explorer fora Massively Multilingual Corpus. (arXiv:2107.06632v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06632</id>
        <link href="http://arxiv.org/abs/2107.06632"/>
        <updated>2021-07-15T01:59:02.507Z</updated>
        <summary type="html"><![CDATA[With more than 7000 languages worldwide, multilingual natural language
processing (NLP) is essential both from an academic and commercial perspective.
Researching typological properties of languages is fundamental for progress in
multilingual NLP. Examples include assessing language similarity for effective
transfer learning, injecting inductive biases into machine learning models or
creating resources such as dictionaries and inflection tables. We provide
ParCourE, an online tool that allows to browse a word-aligned parallel corpus,
covering 1334 languages. We give evidence that this is useful for typological
research. ParCourE can be set up for any parallel corpus and can thus be used
for typological research on other corpora as well as for exploring their
quality and properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Imani_A/0/1/0/all/0/1"&gt;Ayyoob Imani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1"&gt;Masoud Jalili Sabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1"&gt;Philipp Dufter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cysouw_M/0/1/0/all/0/1"&gt;Michael Cysouw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1"&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deduplicating Training Data Makes Language Models Better. (arXiv:2107.06499v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06499</id>
        <link href="http://arxiv.org/abs/2107.06499"/>
        <updated>2021-07-15T01:59:02.453Z</updated>
        <summary type="html"><![CDATA[We find that existing language modeling datasets contain many near-duplicate
examples and long repetitive substrings. As a result, over 1% of the unprompted
output of language models trained on these datasets is copied verbatim from the
training data. We develop two tools that allow us to deduplicate training
datasets -- for example removing from C4 a single 61 word English sentence that
is repeated over 60,000 times. Deduplication allows us to train models that
emit memorized text ten times less frequently and require fewer train steps to
achieve the same or better accuracy. We can also reduce train-test overlap,
which affects over 4% of the validation set of standard datasets, thus allowing
for more accurate evaluation. We release code for reproducing our work and
performing dataset deduplication at
https://github.com/google-research/deduplicate-text-datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Katherine Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1"&gt;Daphne Ippolito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nystrom_A/0/1/0/all/0/1"&gt;Andrew Nystrom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chiyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1"&gt;Douglas Eck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1"&gt;Chris Callison-Burch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text. (arXiv:2107.06483v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06483</id>
        <link href="http://arxiv.org/abs/2107.06483"/>
        <updated>2021-07-15T01:59:02.435Z</updated>
        <summary type="html"><![CDATA[Generating code-switched text is a problem of growing interest, especially
given the scarcity of corpora containing large volumes of real code-switched
text. In this work, we adapt a state-of-the-art neural machine translation
model to generate Hindi-English code-switched sentences starting from
monolingual Hindi sentences. We outline a carefully designed curriculum of
pretraining steps, including the use of synthetic code-switched text, that
enable the model to generate high-quality code-switched text. Using text
generated from our model as data augmentation, we show significant reductions
in perplexity on a language modeling task, compared to using text from other
generative models of CS text. We also show improvements using our text for a
downstream code-switched natural language inference task. Our generated text is
further subjected to a rigorous evaluation using a human evaluation study and a
range of objective metrics, where we show performance comparable (and sometimes
even superior) to code-switched text obtained via crowd workers who are native
Hindi speakers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tarunesh_I/0/1/0/all/0/1"&gt;Ishan Tarunesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Syamantak Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Importance-based Neuron Allocation for Multilingual Neural Machine Translation. (arXiv:2107.06569v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06569</id>
        <link href="http://arxiv.org/abs/2107.06569"/>
        <updated>2021-07-15T01:59:02.429Z</updated>
        <summary type="html"><![CDATA[Multilingual neural machine translation with a single model has drawn much
attention due to its capability to deal with multiple languages. However, the
current multilingual translation paradigm often makes the model tend to
preserve the general knowledge, but ignore the language-specific knowledge.
Some previous works try to solve this problem by adding various kinds of
language-specific modules to the model, but they suffer from the parameter
explosion problem and require specialized manual design. To solve these
problems, we propose to divide the model neurons into general and
language-specific parts based on their importance across languages. The general
part is responsible for preserving the general knowledge and participating in
the translation of all the languages, while the language-specific part is
responsible for preserving the language-specific knowledge and participating in
the translation of some specific languages. Experimental results on several
language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the
effectiveness and universality of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wanying Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shuhao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1"&gt;Dong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT Fine-Tuning for Sentiment Analysis on Indonesian Mobile Apps Reviews. (arXiv:2107.06802v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06802</id>
        <link href="http://arxiv.org/abs/2107.06802"/>
        <updated>2021-07-15T01:59:02.403Z</updated>
        <summary type="html"><![CDATA[User reviews have an essential role in the success of the developed mobile
apps. User reviews in the textual form are unstructured data, creating a very
high complexity when processed for sentiment analysis. Previous approaches that
have been used often ignore the context of reviews. In addition, the relatively
small data makes the model overfitting. A new approach, BERT, has been
introduced as a transfer learning model with a pre-trained model that has
previously been trained to have a better context representation. This study
examines the effectiveness of fine-tuning BERT for sentiment analysis using two
different pre-trained models. Besides the multilingual pre-trained model, we
use the pre-trained model that only has been trained in Indonesian. The dataset
used is Indonesian user reviews of the ten best apps in 2020 in Google Play
sites. We also perform hyper-parameter tuning to find the optimum trained
model. Two training data labeling approaches were also tested to determine the
effectiveness of the model, which is score-based and lexicon-based. The
experimental results show that pre-trained models trained in Indonesian have
better average accuracy on lexicon-based data. The pre-trained Indonesian model
highest accuracy is 84%, with 25 epochs and a training time of 24 minutes.
These results are better than all of the machine learning and multilingual
pre-trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1"&gt;Kuncahyo Setyo Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukmadewa_A/0/1/0/all/0/1"&gt;Anantha Yullian Sukmadewa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DW_H/0/1/0/all/0/1"&gt;Haftittah Wuswilahaken DW&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachtiar_F/0/1/0/all/0/1"&gt;Fitra Abdurrachman Bachtiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1"&gt;Novanto Yudistira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composing Conversational Negation. (arXiv:2107.06820v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06820</id>
        <link href="http://arxiv.org/abs/2107.06820"/>
        <updated>2021-07-15T01:59:02.397Z</updated>
        <summary type="html"><![CDATA[Negation in natural language does not follow Boolean logic and is therefore
inherently difficult to model. In particular, it takes into account the broader
understanding of what is being negated. In previous work, we proposed a
framework for negation of words that accounts for `worldly context'. In this
paper, we extend that proposal now accounting for the compositional structure
inherent in language, within the DisCoCirc framework. We compose the negations
of single words to capture the negation of sentences. We also describe how to
model the negation of words whose meanings evolve in the text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaikh_R/0/1/0/all/0/1"&gt;Razin A. Shaikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_L/0/1/0/all/0/1"&gt;Lia Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodatz_B/0/1/0/all/0/1"&gt;Benjamin Rodatz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1"&gt;Bob Coecke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale News Classification using BERT Language Model: Spark NLP Approach. (arXiv:2107.06785v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06785</id>
        <link href="http://arxiv.org/abs/2107.06785"/>
        <updated>2021-07-15T01:59:02.381Z</updated>
        <summary type="html"><![CDATA[The rise of big data analytics on top of NLP increases the computational
burden for text processing at scale. The problems faced in NLP are very high
dimensional text, so it takes a high computation resource. The MapReduce allows
parallelization of large computations and can improve the efficiency of text
processing. This research aims to study the effect of big data processing on
NLP tasks based on a deep learning approach. We classify a big text of news
topics with fine-tuning BERT used pre-trained models. Five pre-trained models
with a different number of parameters were used in this study. To measure the
efficiency of this method, we compared the performance of the BERT with the
pipelines from Spark NLP. The result shows that BERT without Spark NLP gives
higher accuracy compared to BERT with Spark NLP. The accuracy average and
training time of all models using BERT is 0.9187 and 35 minutes while using
BERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will
take more computation resources and need a longer time to complete the tasks.
However, the accuracy of BERT with Spark NLP only decreased by an average of
5.7%, while the training time was reduced significantly by 62.9% compared to
BERT without Spark NLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1"&gt;Kuncahyo Setyo Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1"&gt;Kuncahyo Setyo Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1"&gt;Novanto Yudistira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to make qubits speak. (arXiv:2107.06776v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.06776</id>
        <link href="http://arxiv.org/abs/2107.06776"/>
        <updated>2021-07-15T01:59:02.375Z</updated>
        <summary type="html"><![CDATA[This is a story about making quantum computers speak, and doing so in a
quantum-native, compositional and meaning-aware manner. Recently we did
question-answering with an actual quantum computer. We explain what we did,
stress that this was all done in terms of pictures, and provide many pointers
to the related literature. In fact, besides natural language, many other things
can be implemented in a quantum-native, compositional and meaning-aware manner,
and we provide the reader with some indications of that broader pictorial
landscape, including our account on the notion of compositionality. We also
provide some guidance for the actual execution, so that the reader can give it
a go as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Coecke_B/0/1/0/all/0/1"&gt;Bob Coecke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Felice_G/0/1/0/all/0/1"&gt;Giovanni de Felice&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Meichanetzidis_K/0/1/0/all/0/1"&gt;Konstantinos Meichanetzidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Toumi_A/0/1/0/all/0/1"&gt;Alexis Toumi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tortured phrases: A dubious writing style emerging in science. Evidence of critical issues affecting established journals. (arXiv:2107.06751v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2107.06751</id>
        <link href="http://arxiv.org/abs/2107.06751"/>
        <updated>2021-07-15T01:59:02.369Z</updated>
        <summary type="html"><![CDATA[Probabilistic text generators have been used to produce fake scientific
papers for more than a decade. Such nonsensical papers are easily detected by
both human and machine. Now more complex AI-powered generation techniques
produce texts indistinguishable from that of humans and the generation of
scientific texts from a few keywords has been documented. Our study introduces
the concept of tortured phrases: unexpected weird phrases in lieu of
established ones, such as 'counterfeit consciousness' instead of 'artificial
intelligence.' We combed the literature for tortured phrases and study one
reputable journal where these concentrated en masse. Hypothesising the use of
advanced language models we ran a detector on the abstracts of recent articles
of this journal and on several control sets. The pairwise comparisons reveal a
concentration of abstracts flagged as 'synthetic' in the journal. We also
highlight irregularities in its operation, such as abrupt changes in editorial
timelines. We substantiate our call for investigation by analysing several
individual dubious articles, stressing questionable features: tortured writing
style, citation of non-existent literature, and unacknowledged image reuse.
Surprisingly, some websites offer to rewrite texts for free, generating
gobbledegook full of tortured phrases. We believe some authors used rewritten
texts to pad their manuscripts. We wish to raise the awareness on publications
containing such questionable AI-generated or rewritten texts that passed (poor)
peer review. Deception with synthetic texts threatens the integrity of the
scientific literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cabanac_G/0/1/0/all/0/1"&gt;Guillaume Cabanac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labbe_C/0/1/0/all/0/1"&gt;Cyril Labb&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magazinov_A/0/1/0/all/0/1"&gt;Alexander Magazinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition. (arXiv:2107.06546v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06546</id>
        <link href="http://arxiv.org/abs/2107.06546"/>
        <updated>2021-07-15T01:59:02.362Z</updated>
        <summary type="html"><![CDATA[We present the visually-grounded language modelling track that was introduced
in the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the
new track and discuss participation rules in detail. We also present the two
baseline systems that were developed for this track.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alishahia_A/0/1/0/all/0/1"&gt;Afra Alishahia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1"&gt;Grzegorz Chrupa&amp;#x142;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cristia_A/0/1/0/all/0/1"&gt;Alejandrina Cristia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1"&gt;Emmanuel Dupoux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Higy_B/0/1/0/all/0/1"&gt;Bertrand Higy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavechin_M/0/1/0/all/0/1"&gt;Marvin Lavechin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1"&gt;Okko R&amp;#xe4;s&amp;#xe4;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chen Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Training of Acoustic Encoders for Speech Recognition. (arXiv:2106.08960v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08960</id>
        <link href="http://arxiv.org/abs/2106.08960"/>
        <updated>2021-07-15T01:59:02.356Z</updated>
        <summary type="html"><![CDATA[On-device speech recognition requires training models of different sizes for
deploying on devices with various computational budgets. When building such
different models, we can benefit from training them jointly to take advantage
of the knowledge shared between them. Joint training is also efficient since it
reduces the redundancy in the training procedure's data handling operations. We
propose a method for collaboratively training acoustic encoders of different
sizes for speech recognition. We use a sequence transducer setup where
different acoustic encoders share a common predictor and joiner modules. The
acoustic encoders are also trained using co-distillation through an auxiliary
task for frame level chenone prediction, along with the transducer loss. We
perform experiments using the LibriSpeech corpus and demonstrate that the
collaboratively trained acoustic encoders can provide up to a 11% relative
improvement in the word error rate on both the test partitions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagaraja_V/0/1/0/all/0/1"&gt;Varun Nagaraja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yangyang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1"&gt;Ganesh Venkatesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1"&gt;Ozlem Kalinli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1"&gt;Michael L. Seltzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1"&gt;Vikas Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling. (arXiv:2106.01040v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01040</id>
        <link href="http://arxiv.org/abs/2106.01040"/>
        <updated>2021-07-15T01:59:02.349Z</updated>
        <summary type="html"><![CDATA[Transformer is important for text modeling. However, it has difficulty in
handling long documents due to the quadratic complexity with input text length.
In order to handle this problem, we propose a hierarchical interactive
Transformer (Hi-Transformer) for efficient and effective long document
modeling. Hi-Transformer models documents in a hierarchical way, i.e., first
learns sentence representations and then learns document representations. It
can effectively reduce the complexity and meanwhile capture global document
context in the modeling of each sentence. More specifically, we first use a
sentence Transformer to learn the representations of each sentence. Then we use
a document Transformer to model the global document context from these sentence
representations. Next, we use another sentence Transformer to enhance sentence
modeling using the global document context. Finally, we use hierarchical
pooling method to obtain document embedding. Extensive experiments on three
benchmark datasets validate the efficiency and effectiveness of Hi-Transformer
in long document modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1"&gt;Tao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03158</id>
        <link href="http://arxiv.org/abs/2107.03158"/>
        <updated>2021-07-15T01:59:02.329Z</updated>
        <summary type="html"><![CDATA[Data augmentation, the artificial creation of training data for machine
learning by transformations, is a widely studied research field across machine
learning disciplines. While it is useful for increasing the generalization
capabilities of a model, it can also address many other challenges and
problems, from overcoming a limited amount of training data over regularizing
the objective to limiting the amount data used to protect privacy. Based on a
precise description of the goals and applications of data augmentation (C1) and
a taxonomy for existing works (C2), this survey is concerned with data
augmentation methods for textual classification and aims to achieve a concise
and comprehensive overview for researchers and practitioners (C3). Derived from
the taxonomy, we divided more than 100 methods into 12 different groupings and
provide state-of-the-art references expounding which methods are highly
promising (C4). Finally, research perspectives that may constitute a building
block for future work are given (C5).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1"&gt;Markus Bayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufhold_M/0/1/0/all/0/1"&gt;Marc-Andr&amp;#xe9; Kaufhold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1"&gt;Christian Reuter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annotation Inconsistency and Entity Bias in MultiWOZ. (arXiv:2105.14150v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14150</id>
        <link href="http://arxiv.org/abs/2105.14150"/>
        <updated>2021-07-15T01:59:02.318Z</updated>
        <summary type="html"><![CDATA[MultiWOZ is one of the most popular multi-domain task-oriented dialog
datasets, containing 10K+ annotated dialogs covering eight domains. It has been
widely accepted as a benchmark for various dialog tasks, e.g., dialog state
tracking (DST), natural language generation (NLG), and end-to-end (E2E) dialog
modeling. In this work, we identify an overlooked issue with dialog state
annotation inconsistencies in the dataset, where a slot type is tagged
inconsistently across similar dialogs leading to confusion for DST modeling. We
propose an automated correction for this issue, which is present in a whopping
70% of the dialogs. Additionally, we notice that there is significant entity
bias in the dataset (e.g., "cambridge" appears in 50% of the destination cities
in the train domain). The entity bias can potentially lead to named entity
memorization in generative models, which may go unnoticed as the test set
suffers from a similar entity bias as well. We release a new test set with all
entities replaced with unseen entities. Finally, we benchmark joint goal
accuracy (JGA) of the state-of-the-art DST baselines on these modified versions
of the data. Our experiments show that the annotation inconsistency corrections
lead to 7-10% improvement in JGA. On the other hand, we observe a 29% drop in
JGA when models are evaluated on the new test set with unseen entities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1"&gt;Kun Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1"&gt;Ahmad Beirami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouhan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1"&gt;Ankita De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1"&gt;Alborz Geramifard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1"&gt;Chinnadhurai Sankar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition. (arXiv:2104.00120v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00120</id>
        <link href="http://arxiv.org/abs/2104.00120"/>
        <updated>2021-07-15T01:59:02.309Z</updated>
        <summary type="html"><![CDATA[Stream fusion, also known as system combination, is a common technique in
automatic speech recognition for traditional hybrid hidden Markov model
approaches, yet mostly unexplored for modern deep neural network end-to-end
model architectures. Here, we investigate various fusion techniques for the
all-attention-based encoder-decoder architecture known as the transformer,
striving to achieve optimal fusion by investigating different fusion levels in
an example single-microphone setting with fusion of standard magnitude and
phase features. We introduce a novel multi-encoder learning method that
performs a weighted combination of two encoder-decoder multi-head attention
outputs only during training. Employing then only the magnitude feature encoder
in inference, we are able to show consistent improvement on Wall Street Journal
(WSJ) with language model and on Librispeech, without increase in runtime or
parameters. Combining two such multi-encoder trained models by a simple late
fusion in inference, we achieve state-of-the-art performance for
transformer-based models on WSJ with a significant WER reduction of 19%
relative compared to the current benchmark approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lohrenz_T/0/1/0/all/0/1"&gt;Timo Lohrenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhengyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fingscheidt_T/0/1/0/all/0/1"&gt;Tim Fingscheidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text-to-hashtag Generation using Seq2seq Learning. (arXiv:2102.00904v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00904</id>
        <link href="http://arxiv.org/abs/2102.00904"/>
        <updated>2021-07-15T01:59:02.301Z</updated>
        <summary type="html"><![CDATA[In this paper, we studied whether models based on BiLSTM and BERT can predict
hashtags in Brazilian Portuguese for Ecommerce websites. Hashtags have a
sizable financial impact on Ecommerce. We processed a corpus of Ecommerce
reviews as inputs, and predicted hashtags as outputs. We evaluated the results
using four quantitative metrics: NIST, BLEU, METEOR and a crowdsourced score. A
word cloud was used as a qualitative metric. While all computer-generated
metrics (NIST, BLEU and METEOR) indicated bad results, the crowdsourced results
produced amazing scores. We concluded that the texts predicted by the neural
networks are very promising for use as hashtags for products on Ecommerce
websites. The code for this work is available at
https://github.com/augustocamargo/text-to-hashtag.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Camargo_A/0/1/0/all/0/1"&gt;Augusto Camargo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_W/0/1/0/all/0/1"&gt;Wesley Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peressim_F/0/1/0/all/0/1"&gt;Felipe Peressim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_A/0/1/0/all/0/1"&gt;Alan Barzilay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finger_M/0/1/0/all/0/1"&gt;Marcelo Finger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More. (arXiv:2107.06876v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06876</id>
        <link href="http://arxiv.org/abs/2107.06876"/>
        <updated>2021-07-15T01:59:02.294Z</updated>
        <summary type="html"><![CDATA[The current best practice for computing optimal transport (OT) is via entropy
regularization and Sinkhorn iterations. This algorithm runs in quadratic time
as it requires the full pairwise cost matrix, which is prohibitively expensive
for large sets of objects. In this work we propose two effective log-linear
time approximations of the cost matrix: First, a sparse approximation based on
locality-sensitive hashing (LSH) and, second, a Nystr\"om approximation with
LSH-based sparse corrections, which we call locally corrected Nystr\"om (LCN).
These approximations enable general log-linear time algorithms for
entropy-regularized OT that perform well even for the complex, high-dimensional
spaces common in deep learning. We analyse these approximations theoretically
and evaluate them experimentally both directly and end-to-end as a component
for real-world applications. Using our approximations for unsupervised word
embedding alignment enables us to speed up a state-of-the-art method by a
factor of 3 while also improving the accuracy by 3.1 percentage points without
any additional model changes. For graph distance regression we propose the
graph transport network (GTN), which combines graph neural networks (GNNs) with
enhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales
log-linearly in the number of nodes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klicpera_J/0/1/0/all/0/1"&gt;Johannes Klicpera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lienen_M/0/1/0/all/0/1"&gt;Marten Lienen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["How to best say it?" : Translating Directives in Machine Language into Natural Language in the Blocks World. (arXiv:2107.06886v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06886</id>
        <link href="http://arxiv.org/abs/2107.06886"/>
        <updated>2021-07-15T01:59:02.276Z</updated>
        <summary type="html"><![CDATA[We propose a method to generate optimal natural language for block placement
directives generated by a machine's planner during human-agent interactions in
the blocks world. A non user-friendly machine directive, e.g., move(ObjId,
toPos), is transformed into visually and contextually grounded referring
expressions that are much easier for the user to comprehend. We describe an
algorithm that progressively and generatively transforms the machine's
directive in ECI (Elementary Composable Ideas)-space, generating many
alternative versions of the directive. We then define a cost function to
evaluate the ease of comprehension of these alternatives and select the best
option. The parameters for this cost function were derived empirically from a
user study that measured utterance-to-action timings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sujeong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamrakar_A/0/1/0/all/0/1"&gt;Amir Tamrakar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linking Health News to Research Literature. (arXiv:2107.06472v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.06472</id>
        <link href="http://arxiv.org/abs/2107.06472"/>
        <updated>2021-07-15T01:59:02.269Z</updated>
        <summary type="html"><![CDATA[Accurately linking news articles to scientific research works is a critical
component in a number of applications, such as measuring the social impact of a
research work and detecting inaccuracies or distortions in science news.
Although the lack of links between news and literature has been a challenge in
these applications, it is a relatively unexplored research problem. In this
paper we designed and evaluated a new approach that consists of (1) augmenting
latest named-entity recognition techniques to extract various metadata, and (2)
designing a new elastic search engine that can facilitate the use of enriched
metadata queries. To evaluate our approach, we constructed two datasets of
paired news articles and research papers: one is used for training models to
extract metadata, and the other for evaluation. Our experiments showed that the
new approach performed significantly better than a baseline approach used by
altmetric.com (0.89 vs 0.32 in terms of top-1 accuracy). To further demonstrate
the effectiveness of the approach, we also conducted a study on 37,600
health-related press releases published on EurekAlert!, which showed that our
approach was able to identify the corresponding research papers with a top-1
accuracy of at least 0.97.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bei Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WebMIaS on Docker: Deploying Math-Aware Search in a Single Line of Code. (arXiv:2106.00411v2 [cs.DL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00411</id>
        <link href="http://arxiv.org/abs/2106.00411"/>
        <updated>2021-07-15T01:59:02.249Z</updated>
        <summary type="html"><![CDATA[Math informational retrieval (MIR) search engines are absent in the
wide-spread production use, even though documents in the STEM fields contain
many mathematical formulae, which are sometimes more important than text for
understanding. We have developed and open-sourced the WebMIaS MIR search engine
that has been successfully deployed in the European Digital Mathematics Library
(EuDML). However, its deployment is difficult to automate due to the complexity
of this task. Moreover, the solutions developed so far to tackle this challenge
are imperfect in terms of speed, maintenance, and robustness. In this paper, we
will describe the virtualization of WebMIaS using Docker that solves all three
problems and allows anyone to deploy containerized WebMIaS in a single line of
code. The publicly available Docker image will also help the community push the
development of math-aware search engines in the ARQMath workshop series.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luptak_D/0/1/0/all/0/1"&gt;D&amp;#xe1;vid Lupt&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1"&gt;V&amp;#xed;t Novotn&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1"&gt;Michal &amp;#x160;tef&amp;#xe1;nik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1"&gt;Petr Sojka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review on Edge Analytics: Issues, Challenges, Opportunities, Promises, Future Directions, and Applications. (arXiv:2107.06835v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.06835</id>
        <link href="http://arxiv.org/abs/2107.06835"/>
        <updated>2021-07-15T01:59:02.242Z</updated>
        <summary type="html"><![CDATA[Edge technology aims to bring Cloud resources (specifically, the compute,
storage, and network) to the closed proximity of the Edge devices, i.e., smart
devices where the data are produced and consumed. Embedding computing and
application in Edge devices lead to emerging of two new concepts in Edge
technology, namely, Edge computing and Edge analytics. Edge analytics uses some
techniques or algorithms to analyze the data generated by the Edge devices.
With the emerging of Edge analytics, the Edge devices have become a complete
set. Currently, Edge analytics is unable to provide full support for the
execution of the analytic techniques. The Edge devices cannot execute advanced
and sophisticated analytic algorithms following various constraints such as
limited power supply, small memory size, limited resources, etc. This article
aims to provide a detailed discussion on Edge analytics. A clear explanation to
distinguish between the three concepts of Edge technology, namely, Edge
devices, Edge computing, and Edge analytics, along with their issues.
Furthermore, the article discusses the implementation of Edge analytics to
solve many problems in various areas such as retail, agriculture, industry, and
healthcare. In addition, the research papers of the state-of-the-art edge
analytics are rigorously reviewed in this article to explore the existing
issues, emerging challenges, research opportunities and their directions, and
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1"&gt;Sabuzima Nayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patgiri_R/0/1/0/all/0/1"&gt;Ripon Patgiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waikhom_L/0/1/0/all/0/1"&gt;Lilapati Waikhom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1"&gt;Arif Ahmed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Much Can CLIP Benefit Vision-and-Language Tasks?. (arXiv:2107.06383v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06383</id>
        <link href="http://arxiv.org/abs/2107.06383"/>
        <updated>2021-07-15T01:59:02.234Z</updated>
        <summary type="html"><![CDATA[Most existing Vision-and-Language (V&L) models rely on pre-trained visual
encoders, using a relatively small set of manually-annotated data (as compared
to web-crawled data), to perceive the visual world. However, it has been
observed that large-scale pretraining usually can result in better
generalization performance, e.g., CLIP (Contrastive Language-Image
Pre-training), trained on a massive amount of image-caption pairs, has shown a
strong zero-shot capability on various vision tasks. To further study the
advantage brought by CLIP, we propose to use CLIP as the visual encoder in
various V&L models in two typical scenarios: 1) plugging CLIP into
task-specific fine-tuning; 2) combining CLIP with V&L pre-training and
transferring to downstream tasks. We show that CLIP significantly outperforms
widely-used visual encoders trained with in-domain annotated data, such as
BottomUp-TopDown. We achieve competitive or better results on diverse V&L
tasks, while establishing new state-of-the-art results on Visual Question
Answering, Visual Entailment, and V&L Navigation tasks. We release our code at
https://github.com/clip-vil/CLIP-ViL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Sheng Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liunian Harold Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1"&gt;Anna Rohrbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Speed and High-Quality Text-to-Lip Generation. (arXiv:2107.06831v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.06831</id>
        <link href="http://arxiv.org/abs/2107.06831"/>
        <updated>2021-07-15T01:59:02.205Z</updated>
        <summary type="html"><![CDATA[As a key component of talking face generation, lip movements generation
determines the naturalness and coherence of the generated talking face video.
Prior literature mainly focuses on speech-to-lip generation while there is a
paucity in text-to-lip (T2L) generation. T2L is a challenging task and existing
end-to-end works depend on the attention mechanism and autoregressive (AR)
decoding manner. However, the AR decoding manner generates current lip frame
conditioned on frames generated previously, which inherently hinders the
inference speed, and also has a detrimental effect on the quality of generated
lip frames due to error propagation. This encourages the research of parallel
T2L generation. In this work, we propose a novel parallel decoding model for
high-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we
predict the duration of the encoded linguistic features and model the target
lip frames conditioned on the encoded linguistic features with their duration
in a non-autoregressive manner. Furthermore, we incorporate the structural
similarity index loss and adversarial learning to improve perceptual quality of
generated lip frames and alleviate the blurry prediction problem. Extensive
experiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L
generates lip movements with competitive quality compared with the
state-of-the-art AR T2L model DualLip and exceeds the baseline AR model
TransformerT2L by a notable margin benefiting from the mitigation of the error
propagation problem; and 2) exhibits distinct superiority in inference speed
(an average speedup of 19$\times$ than DualLip on TCD-TIMIT).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhiying Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhou Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Sorting Networks for Scalable Sorting and Ranking Supervision. (arXiv:2105.04019v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04019</id>
        <link href="http://arxiv.org/abs/2105.04019"/>
        <updated>2021-07-15T01:59:02.195Z</updated>
        <summary type="html"><![CDATA[Sorting and ranking supervision is a method for training neural networks
end-to-end based on ordering constraints. That is, the ground truth order of
sets of samples is known, while their absolute values remain unsupervised. For
that, we propose differentiable sorting networks by relaxing their pairwise
conditional swap operations. To address the problems of vanishing gradients and
extensive blurring that arise with larger numbers of layers, we propose mapping
activations to regions with moderate gradients. We consider odd-even as well as
bitonic sorting networks, which outperform existing relaxations of the sorting
operation. We show that bitonic sorting networks can achieve stable training on
large input sets of up to 1024 elements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1"&gt;Felix Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgelt_C/0/1/0/all/0/1"&gt;Christian Borgelt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1"&gt;Hilde Kuehne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deussen_O/0/1/0/all/0/1"&gt;Oliver Deussen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linking Health News to Research Literature. (arXiv:2107.06472v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.06472</id>
        <link href="http://arxiv.org/abs/2107.06472"/>
        <updated>2021-07-15T01:59:02.187Z</updated>
        <summary type="html"><![CDATA[Accurately linking news articles to scientific research works is a critical
component in a number of applications, such as measuring the social impact of a
research work and detecting inaccuracies or distortions in science news.
Although the lack of links between news and literature has been a challenge in
these applications, it is a relatively unexplored research problem. In this
paper we designed and evaluated a new approach that consists of (1) augmenting
latest named-entity recognition techniques to extract various metadata, and (2)
designing a new elastic search engine that can facilitate the use of enriched
metadata queries. To evaluate our approach, we constructed two datasets of
paired news articles and research papers: one is used for training models to
extract metadata, and the other for evaluation. Our experiments showed that the
new approach performed significantly better than a baseline approach used by
altmetric.com (0.89 vs 0.32 in terms of top-1 accuracy). To further demonstrate
the effectiveness of the approach, we also conducted a study on 37,600
health-related press releases published on EurekAlert!, which showed that our
approach was able to identify the corresponding research papers with a top-1
accuracy of at least 0.97.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bei Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Recommendation for Cold-start Users with Meta Transitional Learning. (arXiv:2107.06427v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.06427</id>
        <link href="http://arxiv.org/abs/2107.06427"/>
        <updated>2021-07-15T01:59:02.178Z</updated>
        <summary type="html"><![CDATA[A fundamental challenge for sequential recommenders is to capture the
sequential patterns of users toward modeling how users transit among items. In
many practical scenarios, however, there are a great number of cold-start users
with only minimal logged interactions. As a result, existing sequential
recommendation models will lose their predictive power due to the difficulties
in learning sequential patterns over users with only limited interactions. In
this work, we aim to improve sequential recommendation for cold-start users
with a novel framework named MetaTL, which learns to model the transition
patterns of users through meta-learning. Specifically, the proposed MetaTL: (i)
formulates sequential recommendation for cold-start users as a few-shot
learning problem; (ii) extracts the dynamic transition patterns among users
with a translation-based architecture; and (iii) adopts meta transitional
learning to enable fast learning for cold-start users with only limited
interactions, leading to accurate inference of sequential interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianling Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1"&gt;Kaize Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1"&gt;James Caverlee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (arXiv:2107.06400v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06400</id>
        <link href="http://arxiv.org/abs/2107.06400"/>
        <updated>2021-07-15T01:59:02.170Z</updated>
        <summary type="html"><![CDATA[One of the stratagems used to deceive spam filters is to substitute vocables
with synonyms or similar words that turn the message unrecognisable by the
detection algorithms. In this paper we investigate whether the recent
development of language models sensitive to the semantics and context of words,
such as Google's BERT, may be useful to overcome this adversarial attack
(called "Mad-lib" as per the word substitution game). Using a dataset of 5572
SMS spam messages, we first established a baseline of detection performance
using widely known document representation models (BoW and TFIDF) and the novel
BERT model, coupled with a variety of classification algorithms (Decision Tree,
kNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we
built a thesaurus of the vocabulary contained in these messages, and set up a
Mad-lib attack experiment in which we modified each message of a held out
subset of data (not used in the baseline experiment) with different rates of
substitution of original words with synonyms from the thesaurus. Lastly, we
evaluated the detection performance of the three representation models (BoW,
TFIDF and BERT) coupled with the best classifier from the baseline experiment
(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)
in the original dataset, whereas the BERT model obtained 96%. On the other
hand, the Mad-lib attack experiment showed that BERT encodings manage to
maintain a similar BA performance of 96% with an average substitution rate of
1.82 words per message, and 95% with 3.34 words substituted per message. In
contrast, the BA performance of the BoW and TFIDF encoders dropped to chance.
These results hint at the potential advantage of BERT models to combat these
type of ingenious attacks, offsetting to some extent for the inappropriate use
of semantic relationships in language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rojas_Galeano_S/0/1/0/all/0/1"&gt;Sergio Rojas-Galeano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness in Ranking under Uncertainty. (arXiv:2107.06720v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06720</id>
        <link href="http://arxiv.org/abs/2107.06720"/>
        <updated>2021-07-15T01:59:02.151Z</updated>
        <summary type="html"><![CDATA[Fairness has emerged as an important consideration in algorithmic
decision-making. Unfairness occurs when an agent with higher merit obtains a
worse outcome than an agent with lower merit. Our central point is that a
primary cause of unfairness is uncertainty. A principal or algorithm making
decisions never has access to the agents' true merit, and instead uses proxy
features that only imperfectly predict merit (e.g., GPA, star ratings,
recommendation letters). None of these ever fully capture an agent's merit; yet
existing approaches have mostly been defining fairness notions directly based
on observed features and outcomes.

Our primary point is that it is more principled to acknowledge and model the
uncertainty explicitly. The role of observed features is to give rise to a
posterior distribution of the agents' merits. We use this viewpoint to define a
notion of approximate fairness in ranking. We call an algorithm $\phi$-fair
(for $\phi \in [0,1]$) if it has the following property for all agents $x$ and
all $k$: if agent $x$ is among the top $k$ agents with respect to merit with
probability at least $\rho$ (according to the posterior merit distribution),
then the algorithm places the agent among the top $k$ agents in its ranking
with probability at least $\phi \rho$.

We show how to compute rankings that optimally trade off approximate fairness
against utility to the principal. In addition to the theoretical
characterization, we present an empirical analysis of the potential impact of
the approach in simulation studies. For real-world validation, we applied the
approach in the context of a paper recommendation system that we built and
fielded at a large conference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Ashudeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kempe_D/0/1/0/all/0/1"&gt;David Kempe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1"&gt;Thorsten Joachims&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RCLC: ROI-based joint conventional and learning video compression. (arXiv:2107.06492v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.06492</id>
        <link href="http://arxiv.org/abs/2107.06492"/>
        <updated>2021-07-15T01:59:02.136Z</updated>
        <summary type="html"><![CDATA[COVID-19 leads to the high demand for remote interactive systems ever seen.
One of the key elements of these systems is video streaming, which requires a
very high network bandwidth due to its specific real-time demand, especially
with high-resolution video. Existing video compression methods are struggling
in the trade-off between video quality and the speed requirement. Addressed
that the background information rarely changes in most remote meeting cases, we
introduce a Region-Of-Interests (ROI) based video compression framework (named
RCLC) that leverages the cutting-edge learning-based and conventional
technologies. In RCLC, each coming frame is marked as a background-updating
(BU) or ROI-updating (RU) frame. By applying the conventional video codec, the
BU frame is compressed with low-quality and high-compression, while the ROI
from RU-frame is compressed with high-quality and low-compression. The
learning-based methods are applied to detect the ROI, blend background-ROI, and
enhance video quality. The experimental results show that our RCLC can reduce
up to 32.55\% BD-rate for the ROI region compared to H.265 video codec under a
similar compression time with 1080p resolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1"&gt;Trinh Man Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinjia Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (arXiv:2107.06400v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06400</id>
        <link href="http://arxiv.org/abs/2107.06400"/>
        <updated>2021-07-15T01:59:02.123Z</updated>
        <summary type="html"><![CDATA[One of the stratagems used to deceive spam filters is to substitute vocables
with synonyms or similar words that turn the message unrecognisable by the
detection algorithms. In this paper we investigate whether the recent
development of language models sensitive to the semantics and context of words,
such as Google's BERT, may be useful to overcome this adversarial attack
(called "Mad-lib" as per the word substitution game). Using a dataset of 5572
SMS spam messages, we first established a baseline of detection performance
using widely known document representation models (BoW and TFIDF) and the novel
BERT model, coupled with a variety of classification algorithms (Decision Tree,
kNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we
built a thesaurus of the vocabulary contained in these messages, and set up a
Mad-lib attack experiment in which we modified each message of a held out
subset of data (not used in the baseline experiment) with different rates of
substitution of original words with synonyms from the thesaurus. Lastly, we
evaluated the detection performance of the three representation models (BoW,
TFIDF and BERT) coupled with the best classifier from the baseline experiment
(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)
in the original dataset, whereas the BERT model obtained 96%. On the other
hand, the Mad-lib attack experiment showed that BERT encodings manage to
maintain a similar BA performance of 96% with an average substitution rate of
1.82 words per message, and 95% with 3.34 words substituted per message. In
contrast, the BA performance of the BoW and TFIDF encoders dropped to chance.
These results hint at the potential advantage of BERT models to combat these
type of ingenious attacks, offsetting to some extent for the inappropriate use
of semantic relationships in language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rojas_Galeano_S/0/1/0/all/0/1"&gt;Sergio Rojas-Galeano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSCAN : Dialog Structure discovery using SCAN. (arXiv:2107.06426v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06426</id>
        <link href="http://arxiv.org/abs/2107.06426"/>
        <updated>2021-07-15T01:59:02.099Z</updated>
        <summary type="html"><![CDATA[Can we discover dialog structure by dividing utterances into labelled
clusters. Can these labels be generated from the data. Typically for dialogs we
need an ontology and use that to discover structure, however by using
unsupervised classification and self-labelling we are able to intuit this
structure without any labels or ontology. In this paper we apply SCAN (Semantic
Clustering using Nearest Neighbors) to dialog data. We used BERT for pretext
task and an adaptation of SCAN for clustering and self labeling. These clusters
are used to identify transition probabilities and create the dialog structure.
The self-labelling method used for SCAN makes these structures interpretable as
every cluster has a label. As the approach is unsupervised, evaluation metrics
is a challenge, we use statistical measures as proxies for structure quality]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nath_A/0/1/0/all/0/1"&gt;Apurba Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kubba_A/0/1/0/all/0/1"&gt;Aayush Kubba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What do writing features tell us about AI papers?. (arXiv:2107.06310v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06310</id>
        <link href="http://arxiv.org/abs/2107.06310"/>
        <updated>2021-07-15T01:59:02.084Z</updated>
        <summary type="html"><![CDATA[As the numbers of submissions to conferences grow quickly, the task of
assessing the quality of academic papers automatically, convincingly, and with
high accuracy attracts increasing attention. We argue that studying
interpretable dimensions of these submissions could lead to scalable solutions.
We extract a collection of writing features, and construct a suite of
prediction tasks to assess the usefulness of these features in predicting
citation counts and the publication of AI-related papers. Depending on the
venues, the writing features can predict the conference vs. workshop appearance
with F1 scores up to 60-90, sometimes even outperforming the content-based
tf-idf features and RoBERTa. We show that the features describe writing style
more than content. To further understand the results, we estimate the causal
impact of the most indicative features. Our analysis on writing features
provides a perspective to assessing and refining the writing of academic
articles at scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zining Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1"&gt;Frank Rudzicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Step Critiquing User Interface for Recommender Systems. (arXiv:2107.06416v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.06416</id>
        <link href="http://arxiv.org/abs/2107.06416"/>
        <updated>2021-07-15T01:59:02.063Z</updated>
        <summary type="html"><![CDATA[Recommendations with personalized explanations have been shown to increase
user trust and perceived quality and help users make better decisions.
Moreover, such explanations allow users to provide feedback by critiquing them.
Several algorithms for recommendation systems with multi-step critiquing have
therefore been developed. However, providing a user-friendly interface based on
personalized explanations and critiquing has not been addressed in the last
decade. In this paper, we introduce four different web interfaces (available
under https://lia.epfl.ch/critiquing/) helping users making decisions and
finding their ideal item. We have chosen the hotel recommendation domain as a
use case even though our approach is trivially adaptable for other domains.
Moreover, our system is model-agnostic (for both recommender systems and
critiquing models) allowing a great flexibility and further extensions. Our
interfaces are above all a useful tool to help research in recommendation with
critiquing. They allow to test such systems on a real use case and also to
highlight some limitations of these approaches to find solutions to overcome
them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petrescu_D/0/1/0/all/0/1"&gt;Diana Petrescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition. (arXiv:2107.06538v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.06538</id>
        <link href="http://arxiv.org/abs/2107.06538"/>
        <updated>2021-07-15T01:59:02.048Z</updated>
        <summary type="html"><![CDATA[Fine-grained image recognition is challenging because discriminative clues
are usually fragmented, whether from a single image or multiple images. Despite
their significant improvements, most existing methods still focus on the most
discriminative parts from a single image, ignoring informative details in other
regions and lacking consideration of clues from other associated images. In
this paper, we analyze the difficulties of fine-grained image recognition from
a new perspective and propose a transformer architecture with the peak
suppression module and knowledge guidance module, which respects the
diversification of discriminative features in a single image and the
aggregation of discriminative clues among multiple images. Specifically, the
peak suppression module first utilizes a linear projection to convert the input
image into sequential tokens. It then blocks the token based on the attention
response generated by the transformer encoder. This module penalizes the
attention to the most discriminative parts in the feature learning process,
therefore, enhancing the information exploitation of the neglected regions. The
knowledge guidance module compares the image-based representation generated
from the peak suppression module with the learnable knowledge embedding set to
obtain the knowledge response coefficients. Afterwards, it formalizes the
knowledge learning as a classification problem using response coefficients as
the classification scores. Knowledge embeddings and image-based representations
are updated during training so that the knowledge embedding includes
discriminative clues for different images. Finally, we incorporate the acquired
knowledge embeddings into the image-based representations as comprehensive
representations, leading to significantly higher performance. Extensive
evaluations on the six popular datasets demonstrate the advantage of the
proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinda Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lili Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Recommend Items to Wikidata Editors. (arXiv:2107.06423v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.06423</id>
        <link href="http://arxiv.org/abs/2107.06423"/>
        <updated>2021-07-15T01:59:02.033Z</updated>
        <summary type="html"><![CDATA[Wikidata is an open knowledge graph built by a global community of
volunteers. As it advances in scale, it faces substantial challenges around
editor engagement. These challenges are in terms of both attracting new editors
to keep up with the sheer amount of work and retaining existing editors.
Experience from other online communities and peer-production systems, including
Wikipedia, suggests that personalised recommendations could help, especially
newcomers, who are sometimes unsure about how to contribute best to an ongoing
effort. For this reason, we propose a recommender system WikidataRec for
Wikidata items. The system uses a hybrid of content-based and collaborative
filtering techniques to rank items for editors relying on both item features
and item-editor previous interaction. A neural network, named a neural mixture
of representations, is designed to learn fine weights for the combination of
item-based representations and optimize them with editor-based representation
by item-editor interaction. To facilitate further research in this space, we
also create two benchmark datasets, a general-purpose one with 220,000 editors
responsible for 14 million interactions with 4 million items and a second one
focusing on the contributions of more than 8,000 more active editors. We
perform an offline evaluation of the system on both datasets with promising
results. Our code and datasets are available at
https://github.com/WikidataRec-developer/Wikidata_Recommender.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+AlGhamdi_K/0/1/0/all/0/1"&gt;Kholoud AlGhamdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1"&gt;Miaojing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1"&gt;Elena Simperl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tortured phrases: A dubious writing style emerging in science. Evidence of critical issues affecting established journals. (arXiv:2107.06751v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2107.06751</id>
        <link href="http://arxiv.org/abs/2107.06751"/>
        <updated>2021-07-15T01:59:02.007Z</updated>
        <summary type="html"><![CDATA[Probabilistic text generators have been used to produce fake scientific
papers for more than a decade. Such nonsensical papers are easily detected by
both human and machine. Now more complex AI-powered generation techniques
produce texts indistinguishable from that of humans and the generation of
scientific texts from a few keywords has been documented. Our study introduces
the concept of tortured phrases: unexpected weird phrases in lieu of
established ones, such as 'counterfeit consciousness' instead of 'artificial
intelligence.' We combed the literature for tortured phrases and study one
reputable journal where these concentrated en masse. Hypothesising the use of
advanced language models we ran a detector on the abstracts of recent articles
of this journal and on several control sets. The pairwise comparisons reveal a
concentration of abstracts flagged as 'synthetic' in the journal. We also
highlight irregularities in its operation, such as abrupt changes in editorial
timelines. We substantiate our call for investigation by analysing several
individual dubious articles, stressing questionable features: tortured writing
style, citation of non-existent literature, and unacknowledged image reuse.
Surprisingly, some websites offer to rewrite texts for free, generating
gobbledegook full of tortured phrases. We believe some authors used rewritten
texts to pad their manuscripts. We wish to raise the awareness on publications
containing such questionable AI-generated or rewritten texts that passed (poor)
peer review. Deception with synthetic texts threatens the integrity of the
scientific literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cabanac_G/0/1/0/all/0/1"&gt;Guillaume Cabanac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labbe_C/0/1/0/all/0/1"&gt;Cyril Labb&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magazinov_A/0/1/0/all/0/1"&gt;Alexander Magazinov&lt;/a&gt;</name>
        </author>
    </entry>
</feed>