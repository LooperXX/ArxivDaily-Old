<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-06-22T00:40:31.151Z</updated>
    <generator>osmosfeed 1.10.2</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Probabilistic Sequential Shrinking: A Best Arm Identification Algorithm for Stochastic Bandits with Corruptions. (arXiv:2010.07904v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07904</id>
        <link href="http://arxiv.org/abs/2010.07904"/>
        <updated>2021-06-21T02:07:41.330Z</updated>
        <summary type="html"><![CDATA[We consider a best arm identification (BAI) problem for stochastic bandits
with adversarial corruptions in the fixed-budget setting of T steps. We design
a novel randomized algorithm, Probabilistic Sequential Shrinking($u$)
(PSS($u$)), which is agnostic to the amount of corruptions. When the amount of
corruptions per step (CPS) is below a threshold, PSS($u$) identifies the best
arm or item with probability tending to $1$ as $T\rightarrow \infty$.
Otherwise, the optimality gap of the identified item degrades gracefully with
the CPS.We argue that such a bifurcation is necessary. In PSS($u$), the
parameter $u$ serves to balance between the optimality gap and success
probability. The injection of randomization is shown to be essential to
mitigate the impact of corruptions. To demonstrate this, we design two attack
strategies that are applicable to any algorithm. We apply one of them to a
deterministic analogue of PSS($u$) known as Successive Halving (SH) by Karnin
et al. (2013). The attack strategy results in a high failure probability for
SH, but PSS($u$) remains robust. In the absence of corruptions, PSS($2$)'s
performance guarantee matches SH's. We show that when the CPS is sufficiently
large, no algorithm can achieve a BAI probability tending to $1$ as
$T\rightarrow \infty$. Numerical experiments corroborate our theoretical
findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zixin Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_W/0/1/0/all/0/1"&gt;Wang Chi Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symbolic Parallel Adaptive Importance Sampling for Probabilistic Program Analysis. (arXiv:2010.05050v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05050</id>
        <link href="http://arxiv.org/abs/2010.05050"/>
        <updated>2021-06-21T02:07:41.323Z</updated>
        <summary type="html"><![CDATA[Probabilistic software analysis aims at quantifying the probability of a
target event occurring during the execution of a program processing uncertain
incoming data or written itself using probabilistic programming constructs.
Recent techniques combine symbolic execution with model counting or solution
space quantification methods to obtain accurate estimates of the occurrence
probability of rare target events, such as failures in a mission-critical
system. However, they face several scalability and applicability limitations
when analyzing software processing with high-dimensional and correlated
multivariate input distributions. In this paper, we present SYMbolic Parallel
Adaptive Importance Sampling (SYMPAIS), a new inference method tailored to
analyze path conditions generated from the symbolic execution of programs with
high-dimensional, correlated input distributions. SYMPAIS combines results from
importance sampling and constraint solving to produce accurate estimates of the
satisfaction probability for a broad class of constraints that cannot be
analyzed by current solution space quantification methods. We demonstrate
SYMPAIS's generality and performance compared with state-of-the-art
alternatives on a set of problems from different application domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yicheng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filieri_A/0/1/0/all/0/1"&gt;Antonio Filieri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Materials Representation and Transfer Learning for Multi-Property Prediction. (arXiv:2106.02225v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02225</id>
        <link href="http://arxiv.org/abs/2106.02225"/>
        <updated>2021-06-21T02:07:41.307Z</updated>
        <summary type="html"><![CDATA[The adoption of machine learning in materials science has rapidly transformed
materials property prediction. Hurdles limiting full capitalization of recent
advancements in machine learning include the limited development of methods to
learn the underlying interactions of multiple elements, as well as the
relationships among multiple properties, to facilitate property prediction in
new composition spaces. To address these issues, we introduce the Hierarchical
Correlation Learning for Multi-property Prediction (H-CLMP) framework that
seamlessly integrates (i) prediction using only a material's composition, (ii)
learning and exploitation of correlations among target properties in
multi-target regression, and (iii) leveraging training data from tangential
domains via generative transfer learning. The model is demonstrated for
prediction of spectral optical absorption of complex metal oxides spanning 69
3-cation metal oxide composition spaces. H-CLMP accurately predicts non-linear
composition-property relationships in composition spaces for which no training
data is available, which broadens the purview of machine learning to the
discovery of materials with exceptional properties. This achievement results
from the principled integration of latent embedding learning, property
correlation learning, generative transfer learning, and attention models. The
best performance is obtained using H-CLMP with Transfer learning (H-CLMP(T))
wherein a generative adversarial network is trained on computational density of
states data and deployed in the target domain to augment prediction of optical
absorption from composition. H-CLMP(T) aggregates multiple knowledge sources
with a framework that is well-suited for multi-target regression across the
physical sciences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1"&gt;Shufeng Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guevarra_D/0/1/0/all/0/1"&gt;Dan Guevarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1"&gt;Carla P. Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregoire_J/0/1/0/all/0/1"&gt;John M. Gregoire&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Intervention Networks for Causal Effect Estimation. (arXiv:2106.01939v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01939</id>
        <link href="http://arxiv.org/abs/2106.01939"/>
        <updated>2021-06-21T02:07:41.301Z</updated>
        <summary type="html"><![CDATA[We address the estimation of conditional average treatment effects (CATEs)
when treatments are graph-structured (e.g., molecular graphs of drugs). Given a
weak condition on the effect, we propose a plug-in estimator that decomposes
CATE estimation into separate, simpler optimization problems. Our estimator (a)
isolates the causal estimands (reducing regularization bias), and (b) allows
one to plug in arbitrary models for learning. In experiments with small-world
and molecular graphs, we show that our approach outperforms prior approaches
and is robust to varying selection biases. Our implementation is online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1"&gt;Jean Kaddour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuchen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1"&gt;Matt J. Kusner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1"&gt;Ricardo Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Mathematical Foundation for Robust Machine Learning based on Bias-Variance Trade-off. (arXiv:2106.05522v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05522</id>
        <link href="http://arxiv.org/abs/2106.05522"/>
        <updated>2021-06-21T02:07:41.280Z</updated>
        <summary type="html"><![CDATA[A common assumption in machine learning is that samples are independently and
identically distributed (i.i.d). However, the contributions of different
samples are not identical in training. Some samples are difficult to learn and
some samples are noisy. The unequal contributions of samples has a considerable
effect on training performances. Studies focusing on unequal sample
contributions (e.g., easy, hard, noisy) in learning usually refer to these
contributions as robust machine learning (RML). Weighing and regularization are
two common techniques in RML. Numerous learning algorithms have been proposed
but the strategies for dealing with easy/hard/noisy samples differ or even
contradict with different learning algorithms. For example, some strategies
take the hard samples first, whereas some strategies take easy first.
Conducting a clear comparison for existing RML algorithms in dealing with
different samples is difficult due to lack of a unified theoretical framework
for RML. This study attempts to construct a mathematical foundation for RML
based on the bias-variance trade-off theory. A series of definitions and
properties are presented and proved. Several classical learning algorithms are
also explained and compared. Improvements of existing methods are obtained
based on the comparison. A unified method that combines two classical learning
strategies is proposed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_O/0/1/0/all/0/1"&gt;Ou Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Weiyao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yingjun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haixiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qinghu Hou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields. (arXiv:2106.05187v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05187</id>
        <link href="http://arxiv.org/abs/2106.05187"/>
        <updated>2021-06-21T02:07:41.262Z</updated>
        <summary type="html"><![CDATA[We present implicit displacement fields, a novel representation for detailed
3D geometry. Inspired by a classic surface deformation technique, displacement
mapping, our method represents a complex surface as a smooth base surface plus
a displacement along the base's normal directions, resulting in a
frequency-based shape decomposition, where the high frequency signal is
constrained geometrically by the low frequency signal. Importantly, this
disentanglement is unsupervised thanks to a tailored architectural design that
has an innate frequency hierarchy by construction. We explore implicit
displacement field surface reconstruction and detail transfer and demonstrate
superior representational power, training stability and generalizability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1"&gt;Wang Yifan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmann_L/0/1/0/all/0/1"&gt;Lukas Rahmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1"&gt;Olga Sorkine-Hornung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faster Kernel Matrix Algebra via Density Estimation. (arXiv:2102.08341v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08341</id>
        <link href="http://arxiv.org/abs/2102.08341"/>
        <updated>2021-06-21T02:07:41.256Z</updated>
        <summary type="html"><![CDATA[We study fast algorithms for computing fundamental properties of a positive
semidefinite kernel matrix $K \in \mathbb{R}^{n \times n}$ corresponding to $n$
points $x_1,\ldots,x_n \in \mathbb{R}^d$. In particular, we consider estimating
the sum of kernel matrix entries, along with its top eigenvalue and
eigenvector.

We show that the sum of matrix entries can be estimated to $1+\epsilon$
relative error in time $sublinear$ in $n$ and linear in $d$ for many popular
kernels, including the Gaussian, exponential, and rational quadratic kernels.
For these kernels, we also show that the top eigenvalue (and an approximate
eigenvector) can be approximated to $1+\epsilon$ relative error in time
$subquadratic$ in $n$ and linear in $d$.

Our algorithms represent significant advances in the best known runtimes for
these problems. They leverage the positive definiteness of the kernel matrix,
along with a recent line of work on efficient kernel density estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1"&gt;Arturs Backurs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1"&gt;Piotr Indyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Cameron Musco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1"&gt;Tal Wagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN for time series prediction, data assimilation and uncertainty quantification. (arXiv:2105.13859v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13859</id>
        <link href="http://arxiv.org/abs/2105.13859"/>
        <updated>2021-06-21T02:07:41.248Z</updated>
        <summary type="html"><![CDATA[We propose a new method in which a generative adversarial network (GAN) is
used to quantify the uncertainty of forward simulations in the presence of
observed data. Previously, a method has been developed which enables GANs to
make time series predictions and data assimilation by training a GAN with
unconditional simulations of a high-fidelity numerical model. After training,
the GAN can be used to predict the evolution of the spatial distribution of the
simulation states and observed data is assimilated. In this paper, we describe
the process required in order to quantify uncertainty, during which no
additional simulations of the high-fidelity numerical model are required. These
methods take advantage of the adjoint-like capabilities of generative models
and the ability to simulate forwards and backwards in time. Set within a
reduced-order model framework for efficiency, we apply these methods to a
compartmental model in epidemiology to predict the spread of COVID-19 in an
idealised town. The results show that the proposed method can efficiently
quantify uncertainty in the presence of measurements using only unconditional
simulations of the high-fidelity numerical model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1"&gt;Vinicius L. S. Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heaney_C/0/1/0/all/0/1"&gt;Claire E. Heaney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pain_C/0/1/0/all/0/1"&gt;Christopher C. Pain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Combinatorial Sequential Monte Carlo Methods for Bayesian Phylogenetic Inference. (arXiv:2106.00075v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00075</id>
        <link href="http://arxiv.org/abs/2106.00075"/>
        <updated>2021-06-21T02:07:41.236Z</updated>
        <summary type="html"><![CDATA[Bayesian phylogenetic inference is often conducted via local or sequential
search over topologies and branch lengths using algorithms such as random-walk
Markov chain Monte Carlo (MCMC) or Combinatorial Sequential Monte Carlo (CSMC).
However, when MCMC is used for evolutionary parameter learning, convergence
requires long runs with inefficient exploration of the state space. We
introduce Variational Combinatorial Sequential Monte Carlo (VCSMC), a powerful
framework that establishes variational sequential search to learn distributions
over intricate combinatorial structures. We then develop nested CSMC, an
efficient proposal distribution for CSMC and prove that nested CSMC is an exact
approximation to the (intractable) locally optimal proposal. We use nested CSMC
to define a second objective, VNCSMC which yields tighter lower bounds than
VCSMC. We show that VCSMC and VNCSMC are computationally efficient and explore
higher probability spaces than existing methods on a range of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Moretti_A/0/1/0/all/0/1"&gt;Antonio Khalil Moretti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Naesseth_C/0/1/0/all/0/1"&gt;Christian A. Naesseth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Venner_H/0/1/0/all/0/1"&gt;Hadiah Venner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1"&gt;David Blei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Peer_I/0/1/0/all/0/1"&gt;Itsik Pe&amp;#x27;er&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning. (arXiv:2104.02532v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02532</id>
        <link href="http://arxiv.org/abs/2104.02532"/>
        <updated>2021-06-21T02:07:41.229Z</updated>
        <summary type="html"><![CDATA[Machine Learning models have been deployed across many different aspects of
society, often in situations that affect social welfare. Although these models
offer streamlined solutions to large problems, they may contain biases and
treat groups or individuals unfairly based on protected attributes such as
gender. In this paper, we introduce several examples of machine learning gender
bias in practice followed by formalizations of fairness. We provide a survey of
fairness research by detailing influential pre-processing, in-processing, and
post-processing bias mitigation algorithms. We then propose an
\textup{end-to-end bias mitigation} framework, which employs a fusion of pre-,
in-, and post-processing methods to leverage the strengths of each individual
technique. We test this method, along with the standard techniques we review,
on a deep neural network to analyze bias mitigation in a deep learning setting.
We find that our end-to-end bias mitigation framework outperforms the baselines
with respect to several fairness metrics, suggesting its promise as a method
for improving fairness. As society increasingly relies on artificial
intelligence to help in decision-making, addressing gender biases present in
deep learning models is imperative. To provide readers with the tools to assess
the fairness of machine learning models and mitigate the biases present in
them, we discuss multiple open source packages for fairness in AI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_T/0/1/0/all/0/1"&gt;Tal Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peake_A/0/1/0/all/0/1"&gt;Ashley Peake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10013</id>
        <link href="http://arxiv.org/abs/2105.10013"/>
        <updated>2021-06-21T02:07:41.212Z</updated>
        <summary type="html"><![CDATA[Image classification methods are usually trained to perform predictions
taking into account a predefined group of known classes. Real-world problems,
however, may not allow for a full knowledge of the input and label spaces,
making failures in recognition a hazard to deep visual learning. Open set
recognition methods are characterized by the ability to correctly identifying
inputs of known and unknown classes. In this context, we propose GeMOS: simple
and plug-and-play open set recognition modules that can be attached to
pretrained Deep Neural Networks for visual recognition. The GeMOS framework
pairs pre-trained Convolutional Neural Networks with generative models for open
set recognition to extract open set scores for each sample, allowing for
failure recognition in object recognition tasks. We conduct a thorough
evaluation of the proposed method in comparison with state-of-the-art open set
algorithms, finding that GeMOS either outperforms or is statistically
indistinguishable from more complex and costly models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1"&gt;Marcos Vendramini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1"&gt;Hugo Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1"&gt;Alexei Machado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Clustering-friendly Representations: Subspace Clustering via Graph Filtering. (arXiv:2106.09874v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09874</id>
        <link href="http://arxiv.org/abs/2106.09874"/>
        <updated>2021-06-21T02:07:41.205Z</updated>
        <summary type="html"><![CDATA[Finding a suitable data representation for a specific task has been shown to
be crucial in many applications. The success of subspace clustering depends on
the assumption that the data can be separated into different subspaces.
However, this simple assumption does not always hold since the raw data might
not be separable into subspaces. To recover the ``clustering-friendly''
representation and facilitate the subsequent clustering, we propose a graph
filtering approach by which a smooth representation is achieved. Specifically,
it injects graph similarity into data features by applying a low-pass filter to
extract useful data representations for clustering. Extensive experiments on
image and document clustering datasets demonstrate that our method improves
upon state-of-the-art subspace clustering techniques. Especially, its
comparable performance with deep learning methods emphasizes the effectiveness
of the simple graph filtering scheme for many real-world applications. An
ablation study shows that graph filtering can remove noise, preserve structure
in the image, and increase the separability of classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhengrui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1"&gt;Zhao Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1"&gt;Guangchun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1"&gt;Ling Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection. (arXiv:2106.09989v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09989</id>
        <link href="http://arxiv.org/abs/2106.09989"/>
        <updated>2021-06-21T02:07:41.197Z</updated>
        <summary type="html"><![CDATA[Graph-based Anomaly Detection (GAD) is becoming prevalent due to the powerful
representation abilities of graphs as well as recent advances in graph mining
techniques. These GAD tools, however, expose a new attacking surface,
ironically due to their unique advantage of being able to exploit the relations
among data. That is, attackers now can manipulate those relations (i.e., the
structure of the graph) to allow some target nodes to evade detection. In this
paper, we exploit this vulnerability by designing a new type of targeted
structural poisoning attacks to a representative regression-based GAD system
termed OddBall. Specially, we formulate the attack against OddBall as a
bi-level optimization problem, where the key technical challenge is to
efficiently solve the problem in a discrete domain. We propose a novel attack
method termed BinarizedAttack based on gradient descent. Comparing to prior
arts, BinarizedAttack can better use the gradient information, making it
particularly suitable for solving combinatorial optimization problems.
Furthermore, we investigate the attack transferability of BinarizedAttack by
employing it to attack other representation-learning-based GAD systems. Our
comprehensive experiments demonstrate that BinarizedAttack is very effective in
enabling target nodes to evade graph-based anomaly detection tools with limited
attackers' budget, and in the black-box transfer attack setting,
BinarizedAttack is also tested effective and in particular, can significantly
change the node embeddings learned by the GAD systems. Our research thus opens
the door to studying a new type of attack against security analytic tools that
rely on graph data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yulin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Yuni Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kaifa Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiapu Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Mingquan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jian Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kai Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimising simulations for diphoton production at hadron colliders using amplitude neural networks. (arXiv:2106.09474v1 [hep-ph] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.09474</id>
        <link href="http://arxiv.org/abs/2106.09474"/>
        <updated>2021-06-21T02:07:41.142Z</updated>
        <summary type="html"><![CDATA[Machine learning technology has the potential to dramatically optimise event
generation and simulations. We continue to investigate the use of neural
networks to approximate matrix elements for high-multiplicity scattering
processes. We focus on the case of loop-induced diphoton production through
gluon fusion and develop a realistic simulation method that can be applied to
hadron collider observables. Neural networks are trained using the one-loop
amplitudes implemented in the NJet C++ library and interfaced to the Sherpa
Monte Carlo event generator where we perform a detailed study for $2\to3$ and
$2\to4$ scattering problems. We also consider how the trained networks perform
when varying the kinematic cuts effecting the phase space and the reliability
of the neural network simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Aylett_Bullock_J/0/1/0/all/0/1"&gt;Joseph Aylett-Bullock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Badger_S/0/1/0/all/0/1"&gt;Simon Badger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Moodie_R/0/1/0/all/0/1"&gt;Ryan Moodie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coresets for Classification -- Simplified and Strengthened. (arXiv:2106.04254v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04254</id>
        <link href="http://arxiv.org/abs/2106.04254"/>
        <updated>2021-06-21T02:07:41.136Z</updated>
        <summary type="html"><![CDATA[We give relative error coresets for training linear classifiers with a broad
class of loss functions, including the logistic loss and hinge loss. Our
construction achieves $(1\pm \epsilon)$ relative error with $\tilde O(d \cdot
\mu_y(X)^2/\epsilon^2)$ points, where $\mu_y(X)$ is a natural complexity
measure of the data matrix $X \in \mathbb{R}^{n \times d}$ and label vector $y
\in \{-1,1\}^n$, introduced in by Munteanu et al. 2018. Our result is based on
subsampling data points with probabilities proportional to their $\ell_1$
$Lewis$ $weights$. It significantly improves on existing theoretical bounds and
performs well in practice, outperforming uniform subsampling along with other
importance sampling methods. Our sampling distribution does not depend on the
labels, so can be used for active learning. It also does not depend on the
specific loss function, so a single coreset can be used in multiple training
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1"&gt;Tung Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1"&gt;Anup B. Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Cameron Musco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Doubly Constrained Batch Reinforcement Learning. (arXiv:2102.09225v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09225</id>
        <link href="http://arxiv.org/abs/2102.09225"/>
        <updated>2021-06-21T02:07:41.077Z</updated>
        <summary type="html"><![CDATA[Reliant on too many experiments to learn good actions, current Reinforcement
Learning (RL) algorithms have limited applicability in real-world settings,
which can be too expensive to allow exploration. We propose an algorithm for
batch RL, where effective policies are learned using only a fixed offline
dataset instead of online interactions with the environment. The limited data
in batch RL produces inherent uncertainty in value estimates of states/actions
that were insufficiently represented in the training data. This leads to
particularly severe extrapolation when our candidate policies diverge from one
that generated the data. We propose to mitigate this issue via two
straightforward penalties: a policy-constraint to reduce this divergence and a
value-constraint that discourages overly optimistic estimates. Over a
comprehensive set of 32 continuous-action batch RL benchmarks, our approach
compares favorably to state-of-the-art methods, regardless of how the offline
data were collected.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fakoor_R/0/1/0/all/0/1"&gt;Rasool Fakoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1"&gt;Jonas Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asadi_K/0/1/0/all/0/1"&gt;Kavosh Asadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1"&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1"&gt;Alexander J. Smola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Implicit Networks via Non-Euclidean Contractions. (arXiv:2106.03194v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03194</id>
        <link href="http://arxiv.org/abs/2106.03194"/>
        <updated>2021-06-21T02:07:41.070Z</updated>
        <summary type="html"><![CDATA[Implicit neural networks, a.k.a., deep equilibrium networks, are a class of
implicit-depth learning models where function evaluation is performed by
solving a fixed point equation. They generalize classic feedforward models and
are equivalent to infinite-depth weight-tied feedforward networks. While
implicit models show improved accuracy and significant reduction in memory
consumption, they can suffer from ill-posedness and convergence instability.

This paper provides a new framework to design well-posed and robust implicit
neural networks based upon contraction theory for the non-Euclidean norm
$\ell_\infty$. Our framework includes (i) a novel condition for well-posedness
based on one-sided Lipschitz constants, (ii) an average iteration for computing
fixed-points, and (iii) explicit estimates on input-output Lipschitz constants.
Additionally, we design a training problem with the well-posedness condition
and the average iteration as constraints and, to achieve robust models, with
the input-output Lipschitz constant as a regularizer. Our $\ell_\infty$
well-posedness condition leads to a larger polytopic training search space than
existing conditions and our average iteration enjoys accelerated convergence.
Finally, we perform several numerical experiments for function estimation and
digit classification through the MNIST data set. Our numerical results
demonstrate improved accuracy and robustness of the implicit models with
smaller input-output Lipschitz bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jafarpour_S/0/1/0/all/0/1"&gt;Saber Jafarpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davydov_A/0/1/0/all/0/1"&gt;Alexander Davydov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Proskurnikov_A/0/1/0/all/0/1"&gt;Anton V. Proskurnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bullo_F/0/1/0/all/0/1"&gt;Francesco Bullo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. (arXiv:2106.06054v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06054</id>
        <link href="http://arxiv.org/abs/2106.06054"/>
        <updated>2021-06-21T02:07:41.062Z</updated>
        <summary type="html"><![CDATA[In recent years, many incidents have been reported where machine learning
models exhibited discrimination among people based on race, sex, age, etc.
Research has been conducted to measure and mitigate unfairness in machine
learning models. For a machine learning task, it is a common practice to build
a pipeline that includes an ordered set of data preprocessing stages followed
by a classifier. However, most of the research on fairness has considered a
single classifier based prediction task. What are the fairness impacts of the
preprocessing stages in machine learning pipeline? Furthermore, studies showed
that often the root cause of unfairness is ingrained in the data itself, rather
than the model. But no research has been conducted to measure the unfairness
caused by a specific transformation made in the data preprocessing stage. In
this paper, we introduced the causal method of fairness to reason about the
fairness impact of data preprocessing stages in ML pipeline. We leveraged
existing metrics to define the fairness measures of the stages. Then we
conducted a detailed fairness evaluation of the preprocessing stages in 37
pipelines collected from three different sources. Our results show that certain
data transformers are causing the model to exhibit unfairness. We identified a
number of fairness patterns in several categories of data transformers.
Finally, we showed how the local fairness of a preprocessing stage composes in
the global fairness of the pipeline. We used the fairness composition to choose
appropriate downstream transformer that mitigates unfairness in the machine
learning pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1"&gt;Sumon Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1"&gt;Hridesh Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ray-based framework for state identification in quantum dot devices. (arXiv:2102.11784v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11784</id>
        <link href="http://arxiv.org/abs/2102.11784"/>
        <updated>2021-06-21T02:07:41.054Z</updated>
        <summary type="html"><![CDATA[Quantum dots (QDs) defined with electrostatic gates are a leading platform
for a scalable quantum computing implementation. However, with increasing
numbers of qubits, the complexity of the control parameter space also grows.
Traditional measurement techniques, relying on complete or near-complete
exploration via two-parameter scans (images) of the device response, quickly
become impractical with increasing numbers of gates. Here we propose to
circumvent this challenge by introducing a measurement technique relying on
one-dimensional projections of the device response in the multidimensional
parameter space. Dubbed the ``ray-based classification (RBC) framework,'' we
use this machine learning approach to implement a classifier for QD states,
enabling automated recognition of qubit-relevant parameter regimes. We show
that RBC surpasses the 82 % accuracy benchmark from the experimental
implementation of image-based classification techniques from prior work while
reducing the number of measurement points needed by up to 70 %. The reduction
in measurement cost is a significant gain for time-intensive QD measurements
and is a step forward toward the scalability of these devices. We also discuss
how the RBC-based optimizer, which tunes the device to a multiqubit regime,
performs when tuning in the two-dimensional and three-dimensional parameter
spaces defined by plunger and barrier gates that control the QDs.This work
provides experimental validation of both efficient state identification and
optimization with machine learning techniques for non-traditional measurements
in quantum systems with high-dimensional parameter spaces and time-intensive
measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zwolak_J/0/1/0/all/0/1"&gt;Justyna P. Zwolak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+McJunkin_T/0/1/0/all/0/1"&gt;Thomas McJunkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kalantre_S/0/1/0/all/0/1"&gt;Sandesh S. Kalantre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Neyens_S/0/1/0/all/0/1"&gt;Samuel F. Neyens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+MacQuarrie_E/0/1/0/all/0/1"&gt;E. R. MacQuarrie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Eriksson_M/0/1/0/all/0/1"&gt;Mark A. Eriksson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Taylor_J/0/1/0/all/0/1"&gt;Jacob M. Taylor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Slow Momentum with Fast Reversion: A Trading Strategy Using Deep Learning and Changepoint Detection. (arXiv:2105.13727v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13727</id>
        <link href="http://arxiv.org/abs/2105.13727"/>
        <updated>2021-06-21T02:07:41.037Z</updated>
        <summary type="html"><![CDATA[Momentum strategies are an important part of alternative investments and are
at the heart of commodity trading advisors (CTAs). These strategies have
however been found to have difficulties adjusting to rapid changes in market
conditions, such as during the 2020 market crash. In particular, immediately
after momentum turning points, where a trend reverses from an uptrend
(downtrend) to a downtrend (uptrend), time-series momentum (TSMOM) strategies
are prone to making bad bets. To improve the response to regime change, we
introduce a novel approach, where we insert an online change-point detection
(CPD) module into a Deep Momentum Network (DMN) [1904.04912] pipeline, which
uses an LSTM deep-learning architecture to simultaneously learn both trend
estimation and position sizing. Furthermore, our model is able to optimise the
way in which it balances 1) a slow momentum strategy which exploits persisting
trends, but does not overreact to localised price moves, and 2) a fast
mean-reversion strategy regime by quickly flipping its position, then swapping
it back again to exploit localised price moves. Our CPD module outputs a
changepoint location and severity score, allowing our model to learn to respond
to varying degrees of disequilibrium, or smaller and more localised
changepoints, in a data driven manner. Using a portfolio of 50, liquid,
continuous futures contracts over the period 1990-2020, the addition of the CPD
module leads to an improvement in Sharpe ratio of one-third. Even more notably,
this module is especially beneficial in periods of significant nonstationarity,
and in particular, over the most recent years tested (2015-2020) the
performance boost is approximately two-thirds. This is especially interesting
as traditional momentum strategies have been underperforming in this period.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wood_K/0/1/0/all/0/1"&gt;Kieran Wood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Max-Margin is Dead, Long Live Max-Margin!. (arXiv:2105.15069v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15069</id>
        <link href="http://arxiv.org/abs/2105.15069"/>
        <updated>2021-06-21T02:07:41.028Z</updated>
        <summary type="html"><![CDATA[The foundational concept of Max-Margin in machine learning is ill-posed for
output spaces with more than two labels such as in structured prediction. In
this paper, we show that the Max-Margin loss can only be consistent to the
classification task under highly restrictive assumptions on the discrete loss
measuring the error between outputs. These conditions are satisfied by
distances defined in tree graphs, for which we prove consistency, thus being
the first losses shown to be consistent for Max-Margin beyond the binary
setting. We finally address these limitations by correcting the concept of
Max-Margin and introducing the Restricted-Max-Margin, where the maximization of
the loss-augmented scores is maintained, but performed over a subset of the
original domain. The resulting loss is also a generalization of the binary
support vector machine and it is consistent under milder conditions on the
discrete loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nowak_Vila_A/0/1/0/all/0/1"&gt;Alex Nowak-Vila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Pharmacodynamic State Space Modeling. (arXiv:2102.11218v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11218</id>
        <link href="http://arxiv.org/abs/2102.11218"/>
        <updated>2021-06-21T02:07:41.022Z</updated>
        <summary type="html"><![CDATA[Modeling the time-series of high-dimensional, longitudinal data is important
for predicting patient disease progression. However, existing neural network
based approaches that learn representations of patient state, while very
flexible, are susceptible to overfitting. We propose a deep generative model
that makes use of a novel attention-based neural architecture inspired by the
physics of how treatments affect disease state. The result is a scalable and
accurate model of high-dimensional patient biomarkers as they vary over time.
Our proposed model yields significant improvements in generalization and, on
real-world clinical data, provides interpretable insights into the dynamics of
cancer progression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_Z/0/1/0/all/0/1"&gt;Zeshan Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1"&gt;Rahul G. Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1"&gt;David Sontag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Diverse-Structured Networks for Adversarial Robustness. (arXiv:2102.01886v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01886</id>
        <link href="http://arxiv.org/abs/2102.01886"/>
        <updated>2021-06-21T02:07:41.014Z</updated>
        <summary type="html"><![CDATA[In adversarial training (AT), the main focus has been the objective and
optimizer while the model has been less studied, so that the models being used
are still those classic ones in standard training (ST). Classic network
architectures (NAs) are generally worse than searched NAs in ST, which should
be the same in AT. In this paper, we argue that NA and AT cannot be handled
independently, since given a dataset, the optimal NA in ST would be no longer
optimal in AT. That being said, AT is time-consuming itself; if we directly
search NAs in AT over large search spaces, the computation will be practically
infeasible. Thus, we propose a diverse-structured network (DS-Net), to
significantly reduce the size of the search space: instead of low-level
operations, we only consider predefined atomic blocks, where an atomic block is
a time-tested building block like the residual block. There are only a few
atomic blocks and thus we can weight all atomic blocks rather than find the
best one in a searched block of DS-Net, which is an essential trade-off between
exploring diverse structures and exploiting the best structures. Empirical
results demonstrate the advantages of DS-Net, i.e., weighting the atomic
blocks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1"&gt;Xuefeng Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning. (arXiv:2101.08482v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08482</id>
        <link href="http://arxiv.org/abs/2101.08482"/>
        <updated>2021-06-21T02:07:41.007Z</updated>
        <summary type="html"><![CDATA[We present a plug-in replacement for batch normalization (BN) called
exponential moving average normalization (EMAN), which improves the performance
of existing student-teacher based self- and semi-supervised learning
techniques. Unlike the standard BN, where the statistics are computed within
each batch, EMAN, used in the teacher, updates its statistics by exponential
moving average from the BN statistics of the student. This design reduces the
intrinsic cross-sample dependency of BN and enhances the generalization of the
teacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2
points and semi-supervised learning by about 7/2 points, when 1%/10% supervised
labels are available on ImageNet. These improvements are consistent across
methods, network architectures, training duration, and datasets, demonstrating
the general effectiveness of this technique. The code is available at
https://github.com/amazon-research/exponential-moving-average-normalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhaowei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhransu Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1"&gt;Charless Fowlkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Assimilation Predictive GAN (DA-PredGAN): applied to determine the spread of COVID-19. (arXiv:2105.07729v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07729</id>
        <link href="http://arxiv.org/abs/2105.07729"/>
        <updated>2021-06-21T02:07:41.000Z</updated>
        <summary type="html"><![CDATA[We propose the novel use of a generative adversarial network (GAN) (i) to
make predictions in time (PredGAN) and (ii) to assimilate measurements
(DA-PredGAN). In the latter case, we take advantage of the natural adjoint-like
properties of generative models and the ability to simulate forwards and
backwards in time. GANs have received much attention recently, after achieving
excellent results for their generation of realistic-looking images. We wish to
explore how this property translates to new applications in computational
modelling and to exploit the adjoint-like properties for efficient data
assimilation. To predict the spread of COVID-19 in an idealised town, we apply
these methods to a compartmental model in epidemiology that is able to model
space and time variations. To do this, the GAN is set within a reduced-order
model (ROM), which uses a low-dimensional space for the spatial distribution of
the simulation states. Then the GAN learns the evolution of the low-dimensional
states over time. The results show that the proposed methods can accurately
predict the evolution of the high-fidelity numerical simulation, and can
efficiently assimilate observed data and determine the corresponding model
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1"&gt;Vinicius L. S. Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heaney_C/0/1/0/all/0/1"&gt;Claire E. Heaney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pain_C/0/1/0/all/0/1"&gt;Christopher C. Pain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quasi-Global Momentum: Accelerating Decentralized Deep Learning on Heterogeneous Data. (arXiv:2102.04761v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04761</id>
        <link href="http://arxiv.org/abs/2102.04761"/>
        <updated>2021-06-21T02:07:40.983Z</updated>
        <summary type="html"><![CDATA[Decentralized training of deep learning models is a key element for enabling
data privacy and on-device learning over networks. In realistic learning
scenarios, the presence of heterogeneity across different clients' local
datasets poses an optimization challenge and may severely deteriorate the
generalization performance. In this paper, we investigate and identify the
limitation of several decentralized optimization algorithms for different
degrees of data heterogeneity. We propose a novel momentum-based method to
mitigate this decentralized training difficulty. We show in extensive empirical
experiments on various CV/NLP datasets (CIFAR-10, ImageNet, and AG News) and
several network topologies (Ring and Social Network) that our method is much
more robust to the heterogeneity of clients' data than other existing methods,
by a significant improvement in test performance ($1\% \!-\! 20\%$). Our code
is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1"&gt;Sai Praneeth Karimireddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Dropout Variational Inference for Bayesian Neural Networks. (arXiv:2102.07927v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07927</id>
        <link href="http://arxiv.org/abs/2102.07927"/>
        <updated>2021-06-21T02:07:40.975Z</updated>
        <summary type="html"><![CDATA[Approximate inference in deep Bayesian networks exhibits a dilemma of how to
yield high fidelity posterior approximations while maintaining computational
efficiency and scalability. We tackle this challenge by introducing a novel
variational structured approximation inspired by the Bayesian interpretation of
Dropout regularization. Concretely, we focus on the inflexibility of the
factorized structure in Dropout posterior and then propose an improved method
called Variational Structured Dropout (VSD). VSD employs an orthogonal
transformation to learn a structured representation on the variational noise
and consequently induces statistical dependencies in the approximate posterior.
Theoretically, VSD successfully addresses the pathologies of previous
Variational Dropout methods and thus offers a standard Bayesian justification.
We further show that VSD induces an adaptive regularization term with several
desirable properties which contribute to better generalization. Finally, we
conduct extensive experiments on standard benchmarks to demonstrate the
effectiveness of VSD over state-of-the-art variational methods on predictive
accuracy, uncertainty estimation, and out-of-distribution detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1"&gt;Son Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Duong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khai Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1"&gt;Nhat Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Than_K/0/1/0/all/0/1"&gt;Khoat Than&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_H/0/1/0/all/0/1"&gt;Hung Bui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound. (arXiv:2010.15761v2 [physics.comp-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15761</id>
        <link href="http://arxiv.org/abs/2010.15761"/>
        <updated>2021-06-21T02:07:40.968Z</updated>
        <summary type="html"><![CDATA[Transcranial ultrasound therapy is increasingly used for the non-invasive
treatment of brain disorders. However, conventional numerical wave solvers are
currently too computationally expensive to be used online during treatments to
predict the acoustic field passing through the skull (e.g., to account for
subject-specific dose and targeting variations). As a step towards real-time
predictions, in the current work, a fast iterative solver for the heterogeneous
Helmholtz equation in 2D is developed using a fully-learned optimizer. The
lightweight network architecture is based on a modified UNet that includes a
learned hidden state. The network is trained using a physics-based loss
function and a set of idealized sound speed distributions with fully
unsupervised training (no knowledge of the true solution is required). The
learned optimizer shows excellent performance on the test set, and is capable
of generalization well outside the training examples, including to much larger
computational domains, and more complex source and sound speed distributions,
for example, those derived from x-ray computed tomography images of the skull.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Stanziola_A/0/1/0/all/0/1"&gt;Antonio Stanziola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Arridge_S/0/1/0/all/0/1"&gt;Simon R. Arridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Cox_B/0/1/0/all/0/1"&gt;Ben T. Cox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Treeby_B/0/1/0/all/0/1"&gt;Bradley E. Treeby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Mesh-Based Simulation with Graph Networks. (arXiv:2010.03409v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03409</id>
        <link href="http://arxiv.org/abs/2010.03409"/>
        <updated>2021-06-21T02:07:40.961Z</updated>
        <summary type="html"><![CDATA[Mesh-based simulations are central to modeling complex physical systems in
many disciplines across science and engineering. Mesh representations support
powerful numerical integration methods and their resolution can be adapted to
strike favorable trade-offs between accuracy and efficiency. However,
high-dimensional scientific simulations are very expensive to run, and solvers
and parameters must often be tuned individually to each system studied. Here we
introduce MeshGraphNets, a framework for learning mesh-based simulations using
graph neural networks. Our model can be trained to pass messages on a mesh
graph and to adapt the mesh discretization during forward simulation. Our
results show it can accurately predict the dynamics of a wide range of physical
systems, including aerodynamics, structural mechanics, and cloth. The model's
adaptivity supports learning resolution-independent dynamics and can scale to
more complex state spaces at test time. Our method is also highly efficient,
running 1-2 orders of magnitude faster than the simulation on which it is
trained. Our approach broadens the range of problems on which neural network
simulators can operate and promises to improve the efficiency of complex,
scientific modeling tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pfaff_T/0/1/0/all/0/1"&gt;Tobias Pfaff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortunato_M/0/1/0/all/0/1"&gt;Meire Fortunato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1"&gt;Alvaro Sanchez-Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1"&gt;Peter W. Battaglia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bootstrap an end-to-end ASR system by multilingual training, transfer learning, text-to-text mapping and synthetic audio. (arXiv:2011.12696v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12696</id>
        <link href="http://arxiv.org/abs/2011.12696"/>
        <updated>2021-06-21T02:07:40.944Z</updated>
        <summary type="html"><![CDATA[Bootstrapping speech recognition on limited data resources has been an area
of active research for long. The recent transition to all-neural models and
end-to-end (E2E) training brought along particular challenges as these models
are known to be data hungry, but also came with opportunities around
language-agnostic representations derived from multilingual data as well as
shared word-piece output representations across languages that share script and
roots. We investigate here the effectiveness of different strategies to
bootstrap an RNN-Transducer (RNN-T) based automatic speech recognition (ASR)
system in the low resource regime, while exploiting the abundant resources
available in other languages as well as the synthetic audio from a
text-to-speech (TTS) engine. Our experiments demonstrate that transfer learning
from a multilingual model, using a post-ASR text-to-text mapping and synthetic
audio deliver additive improvements, allowing us to bootstrap a model for a new
language with a fraction of the data that would otherwise be needed. The best
system achieved a 46% relative word error rate (WER) reduction compared to the
monolingual baseline, among which 25% relative WER improvement is attributed to
the post-ASR text-to-text mappings and the TTS synthetic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Giollo_M/0/1/0/all/0/1"&gt;Manuel Giollo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gunceler_D/0/1/0/all/0/1"&gt;Deniz Gunceler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yulan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Willett_D/0/1/0/all/0/1"&gt;Daniel Willett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partition-Guided GANs. (arXiv:2104.00816v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00816</id>
        <link href="http://arxiv.org/abs/2104.00816"/>
        <updated>2021-06-21T02:07:40.937Z</updated>
        <summary type="html"><![CDATA[Despite the success of Generative Adversarial Networks (GANs), their training
suffers from several well-known problems, including mode collapse and
difficulties learning a disconnected set of manifolds. In this paper, we break
down the challenging task of learning complex high dimensional distributions,
supporting diverse data samples, to simpler sub-tasks. Our solution relies on
designing a partitioner that breaks the space into smaller regions, each having
a simpler distribution, and training a different generator for each partition.
This is done in an unsupervised manner without requiring any labels.

We formulate two desired criteria for the space partitioner that aid the
training of our mixture of generators: 1) to produce connected partitions and
2) provide a proxy of distance between partitions and data samples, along with
a direction for reducing that distance. These criteria are developed to avoid
producing samples from places with non-existent data density, and also
facilitate training by providing additional direction to the generators. We
develop theoretical constraints for a space partitioner to satisfy the above
criteria. Guided by our theoretical analysis, we design an effective neural
architecture for the space partitioner that empirically assures these
conditions. Experimental results on various standard benchmarks show that the
proposed unsupervised model outperforms several recent methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Armandpour_M/0/1/0/all/0/1"&gt;Mohammadreza Armandpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadeghian_A/0/1/0/all/0/1"&gt;Ali Sadeghian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoPhy-PGNN: Learning Physics-guided Neural Networks with Competing Loss Functions for Solving Eigenvalue Problems. (arXiv:2007.01420v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.01420</id>
        <link href="http://arxiv.org/abs/2007.01420"/>
        <updated>2021-06-21T02:07:40.929Z</updated>
        <summary type="html"><![CDATA[Physics-guided Neural Networks (PGNNs) represent an emerging class of neural
networks that are trained using physics-guided (PG) loss functions (capturing
violations in network outputs with known physics), along with the supervision
contained in data. Existing work in PGNNs have demonstrated the efficacy of
adding single PG loss functions in the neural network objectives, using
constant trade-off parameters, to ensure better generalizability. However, in
the presence of multiple physics loss functions with competing gradient
directions, there is a need to adaptively tune the contribution of competing PG
loss functions during the course of training to arrive at generalizable
solutions. We demonstrate the presence of competing PG losses in the generic
neural network problem of solving for the lowest (or highest) eigenvector of a
physics-based eigenvalue equation, common to many scientific problems. We
present a novel approach to handle competing PG losses and demonstrate its
efficacy in learning generalizable solutions in two motivating applications of
quantum mechanics and electromagnetic propagation. All the code and data used
in this work is available at https://github.com/jayroxis/Cophy-PGNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elhamod_M/0/1/0/all/0/1"&gt;Mohannad Elhamod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1"&gt;Jie Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1"&gt;Christopher Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Redell_M/0/1/0/all/0/1"&gt;Matthew Redell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Abantika Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Podolskiy_V/0/1/0/all/0/1"&gt;Viktor Podolskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1"&gt;Wei-Cheng Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpatne_A/0/1/0/all/0/1"&gt;Anuj Karpatne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise2Sim -- Similarity-based Self-Learning for Image Denoising. (arXiv:2011.03384v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03384</id>
        <link href="http://arxiv.org/abs/2011.03384"/>
        <updated>2021-06-21T02:07:40.920Z</updated>
        <summary type="html"><![CDATA[Despite its best performance in image denoising, the supervised deep
denoising methods require paired noise-clean data, which are often unavailable.
To address this challenge, Noise2Noise was designed based on the fact that
paired noise-clean images can be replaced by paired noise-noise images that are
easier to collect. However, in many scenarios the collection of paired
noise-noise images is still impractical. To bypass labeled images, Noise2Void
methods predict masked pixels from their surroundings with single noisy images
only and give improved denoising results that still need improvements. An
observation on classic denoising methods is that non-local mean (NLM) outcomes
are typically superior to locally denoised results. In contrast, Noise2Void and
its variants do not utilize self-similarities in an image as the NLM-based
methods do. Here we propose Noise2Sim, an NLM-inspired self-learning method for
image denoising. Specifically, Noise2Sim leverages the self-similarity of image
pixels to train the denoising network, requiring single noisy images only. Our
theoretical analysis shows that Noise2Sim tends to be equivalent to Noise2Noise
under mild conditions. To efficiently manage the computational burden for
globally searching similar pixels, we design a two-step procedure to provide
data for Noise2Sim training. Extensive experiments demonstrate the superiority
of Noise2Sim on common benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1"&gt;Chuang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1"&gt;Fenglei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1"&gt;Qing Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Graph Learning for Recommendation. (arXiv:2010.10783v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10783</id>
        <link href="http://arxiv.org/abs/2010.10783"/>
        <updated>2021-06-21T02:07:40.911Z</updated>
        <summary type="html"><![CDATA[Representation learning on user-item graph for recommendation has evolved
from using single ID or interaction history to exploiting higher-order
neighbors. This leads to the success of graph convolution networks (GCNs) for
recommendation such as PinSage and LightGCN. Despite effectiveness, we argue
that they suffer from two limitations: (1) high-degree nodes exert larger
impact on the representation learning, deteriorating the recommendations of
low-degree (long-tail) items; and (2) representations are vulnerable to noisy
interactions, as the neighborhood aggregation scheme further enlarges the
impact of observed edges.

In this work, we explore self-supervised learning on user-item graph, so as
to improve the accuracy and robustness of GCNs for recommendation. The idea is
to supplement the classical supervised task of recommendation with an auxiliary
self-supervised task, which reinforces node representation learning via
self-discrimination. Specifically, we generate multiple views of a node,
maximizing the agreement between different views of the same node compared to
that of other nodes. We devise three operators to generate the views -- node
dropout, edge dropout, and random walk -- that change the graph structure in
different manners. We term this new learning paradigm as
\textit{Self-supervised Graph Learning} (SGL), implementing it on the
state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL
has the ability of automatically mining hard negatives. Empirical studies on
three benchmark datasets demonstrate the effectiveness of SGL, which improves
the recommendation accuracy, especially on long-tail items, and the robustness
against interaction noises. Our implementations are available at
\url{https://github.com/wujcan/SGL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiancan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1"&gt;Jianxun Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RobustSleepNet: Transfer learning for automated sleep staging at scale. (arXiv:2101.02452v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02452</id>
        <link href="http://arxiv.org/abs/2101.02452"/>
        <updated>2021-06-21T02:07:40.904Z</updated>
        <summary type="html"><![CDATA[Sleep disorder diagnosis relies on the analysis of polysomnography (PSG)
records. As a preliminary step of this examination, sleep stages are
systematically determined. In practice, sleep stage classification relies on
the visual inspection of 30-second epochs of polysomnography signals. Numerous
automatic approaches have been developed to replace this tedious and expensive
task. Although these methods demonstrated better performance than human sleep
experts on specific datasets, they remain largely unused in sleep clinics. The
main reason is that each sleep clinic uses a specific PSG montage that most
automatic approaches cannot handle out-of-the-box. Moreover, even when the PSG
montage is compatible, publications have shown that automatic approaches
perform poorly on unseen data with different demographics. To address these
issues, we introduce RobustSleepNet, a deep learning model for automatic sleep
stage classification able to handle arbitrary PSG montages. We trained and
evaluated this model in a leave-one-out-dataset fashion on a large corpus of 8
heterogeneous sleep staging datasets to make it robust to demographic changes.
When evaluated on an unseen dataset, RobustSleepNet reaches 97% of the F1 of a
model explicitly trained on this dataset. Hence, RobustSleepNet unlocks the
possibility to perform high-quality out-of-the-box automatic sleep staging with
any clinical setup. We further show that finetuning RobustSleepNet, using a
part of the unseen dataset, increases the F1 by 2% when compared to a model
trained specifically for this dataset. Therefore, finetuning might be used to
reach a state-of-the-art level of performance on a specific population.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Guillot_A/0/1/0/all/0/1"&gt;Antoine Guillot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thorey_V/0/1/0/all/0/1"&gt;Valentin Thorey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overparameterization of deep ResNet: zero loss and mean-field analysis. (arXiv:2105.14417v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14417</id>
        <link href="http://arxiv.org/abs/2105.14417"/>
        <updated>2021-06-21T02:07:40.897Z</updated>
        <summary type="html"><![CDATA[Finding parameters in a deep neural network (NN) that fit training data is a
nonconvex optimization problem, but a basic first-order optimization method
(gradient descent) finds a global solution with perfect fit in many practical
situations. We examine this phenomenon for the case of Residual Neural Networks
(ResNet) with smooth activation functions in a limiting regime in which both
the number of layers (depth) and the number of neurons in each layer (width) go
to infinity. First, we use a mean-field-limit argument to prove that the
gradient descent for parameter training becomes a partial differential equation
(PDE) that characterizes gradient flow for a probability distribution in the
large-NN limit. Next, we show that the solution to the PDE converges in the
training time to a zero-loss solution. Together, these results imply that
training of the ResNet also gives a near-zero loss if the Resnet is large
enough. We give estimates of the depth and width needed to reduce the loss
below a given threshold, with high probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zhiyan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wright_S/0/1/0/all/0/1"&gt;Stephen Wright&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Learning Vector Quantization for Classification in Randomized Neural Networks and Hyperdimensional Computing. (arXiv:2106.09821v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09821</id>
        <link href="http://arxiv.org/abs/2106.09821"/>
        <updated>2021-06-21T02:07:40.879Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms deployed on edge devices must meet certain
resource constraints and efficiency requirements. Random Vector Functional Link
(RVFL) networks are favored for such applications due to their simple design
and training efficiency. We propose a modified RVFL network that avoids
computationally expensive matrix operations during training, thus expanding the
network's range of potential applications. Our modification replaces the
least-squares classifier with the Generalized Learning Vector Quantization
(GLVQ) classifier, which only employs simple vector and distance calculations.
The GLVQ classifier can also be considered an improvement upon certain
classification algorithms popularly used in the area of Hyperdimensional
Computing. The proposed approach achieved state-of-the-art accuracy on a
collection of datasets from the UCI Machine Learning Repository - higher than
previously proposed RVFL networks. We further demonstrate that our approach
still achieves high accuracy while severely limited in training iterations
(using on average only 21% of the least-squares classifier computational
costs).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diao_C/0/1/0/all/0/1"&gt;Cameron Diao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1"&gt;Denis Kleyko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabaey_J/0/1/0/all/0/1"&gt;Jan M. Rabaey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olshausen_B/0/1/0/all/0/1"&gt;Bruno A. Olshausen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedADC: Accelerated Federated Learning with Drift Control. (arXiv:2012.09102v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09102</id>
        <link href="http://arxiv.org/abs/2012.09102"/>
        <updated>2021-06-21T02:07:40.850Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has become de facto framework for collaborative
learning among edge devices with privacy concern. The core of the FL strategy
is the use of stochastic gradient descent (SGD) in a distributed manner. Large
scale implementation of FL brings new challenges, such as the incorporation of
acceleration techniques designed for SGD into the distributed setting, and
mitigation of the drift problem due to non-homogeneous distribution of local
datasets. These two problems have been separately studied in the literature;
whereas, in this paper, we show that it is possible to address both problems
using a single strategy without any major alteration to the FL framework, or
introducing additional computation and communication load. To achieve this
goal, we propose FedADC, which is an accelerated FL algorithm with drift
control. We empirically illustrate the advantages of FedADC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozfatura_E/0/1/0/all/0/1"&gt;Emre Ozfatura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozfatura_K/0/1/0/all/0/1"&gt;Kerem Ozfatura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay. (arXiv:2106.09835v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09835</id>
        <link href="http://arxiv.org/abs/2106.09835"/>
        <updated>2021-06-21T02:07:40.839Z</updated>
        <summary type="html"><![CDATA[This paper proposes two novel knowledge transfer techniques for
class-incremental learning (CIL). First, we propose data-free generative replay
(DF-GR) to mitigate catastrophic forgetting in CIL by using synthetic samples
from a generative model. In the conventional generative replay, the generative
model is pre-trained for old data and shared in extra memory for later
incremental learning. In our proposed DF-GR, we train a generative model from
scratch without using any training data, based on the pre-trained
classification model from the past, so we curtail the cost of sharing
pre-trained generative models. Second, we introduce dual-teacher information
distillation (DT-ID) for knowledge distillation from two teachers to one
student. In CIL, we use DT-ID to learn new classes incrementally based on the
pre-trained model for old classes and another model (pre-)trained on the new
data for new classes. We implemented the proposed schemes on top of one of the
state-of-the-art CIL methods and showed the performance improvement on
CIFAR-100 and ImageNet datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yoojin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Khamy_M/0/1/0/all/0/1"&gt;Mostafa El-Khamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jungwon Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Architectural Patterns for the Design of Federated Learning Systems. (arXiv:2101.02373v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02373</id>
        <link href="http://arxiv.org/abs/2101.02373"/>
        <updated>2021-06-21T02:07:40.830Z</updated>
        <summary type="html"><![CDATA[Federated learning has received fast-growing interests from academia and
industry to tackle the challenges of data hungriness and privacy in machine
learning. A federated learning system can be viewed as a large-scale
distributed system with different components and stakeholders as numerous
client devices participate in federated learning. Designing a federated
learning system requires software system design thinking apart from machine
learning knowledge. Although much effort has been put into federated learning
from the machine learning technique aspects, the software architecture design
concerns in building federated learning systems have been largely ignored.
Therefore, in this paper, we present a collection of architectural patterns to
deal with the design challenges of federated learning systems. Architectural
patterns present reusable solutions to a commonly occurring problem within a
given context during software architecture design. The presented patterns are
based on the results of a systematic literature review and include three client
management patterns, four model management patterns, three model training
patterns, and four model aggregation patterns. The patterns are associated to
the particular state transitions in a federated learning model lifecycle,
serving as a guidance for effective use of the patterns in the design of
federated learning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Sin Kit Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1"&gt;Qinghua Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Liming Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1"&gt;Hye-young Paik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Water Temperature Dynamics of Unmonitored Lakes with Meta Transfer Learning. (arXiv:2011.05369v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05369</id>
        <link href="http://arxiv.org/abs/2011.05369"/>
        <updated>2021-06-21T02:07:40.818Z</updated>
        <summary type="html"><![CDATA[Most environmental data come from a minority of well-monitored sites. An
ongoing challenge in the environmental sciences is transferring knowledge from
monitored sites to unmonitored sites. Here, we demonstrate a novel transfer
learning framework that accurately predicts depth-specific temperature in
unmonitored lakes (targets) by borrowing models from well-monitored lakes
(sources). This method, Meta Transfer Learning (MTL), builds a meta-learning
model to predict transfer performance from candidate source models to targets
using lake attributes and candidates' past performance. We constructed source
models at 145 well-monitored lakes using calibrated process-based modeling (PB)
and a recently developed approach called process-guided deep learning (PGDL).
We applied MTL to either PB or PGDL source models (PB-MTL or PGDL-MTL,
respectively) to predict temperatures in 305 target lakes treated as
unmonitored in the Upper Midwestern United States. We show significantly
improved performance relative to the uncalibrated process-based General Lake
Model, where the median RMSE for the target lakes is $2.52^{\circ}C$. PB-MTL
yielded a median RMSE of $2.43^{\circ}C$; PGDL-MTL yielded $2.16^{\circ}C$; and
a PGDL-MTL ensemble of nine sources per target yielded $1.88^{\circ}C$. For
sparsely monitored target lakes, PGDL-MTL often outperformed PGDL models
trained on the target lakes themselves. Differences in maximum depth between
the source and target were consistently the most important predictors. Our
approach readily scales to thousands of lakes in the Midwestern United States,
demonstrating that MTL with meaningful predictor variables and high-quality
source models is a promising approach for many kinds of unmonitored systems and
environmental variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Willard_J/0/1/0/all/0/1"&gt;Jared D. Willard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1"&gt;Jordan S. Read&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Appling_A/0/1/0/all/0/1"&gt;Alison P. Appling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliver_S/0/1/0/all/0/1"&gt;Samantha K. Oliver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaowei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vipin Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Mixup Improves the Model Performance. (arXiv:2006.06231v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06231</id>
        <link href="http://arxiv.org/abs/2006.06231"/>
        <updated>2021-06-21T02:07:40.801Z</updated>
        <summary type="html"><![CDATA[Machine learning techniques are used in a wide range of domains. However,
machine learning models often suffer from the problem of over-fitting. Many
data augmentation methods have been proposed to tackle such a problem, and one
of them is called mixup. Mixup is a recently proposed regularization procedure,
which linearly interpolates a random pair of training examples. This
regularization method works very well experimentally, but its theoretical
guarantee is not adequately discussed. In this study, we aim to discover why
mixup works well from the aspect of the statistical learning theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kimura_M/0/1/0/all/0/1"&gt;Masanari Kimura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep State Space Models for Nonlinear System Identification. (arXiv:2003.14162v3 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.14162</id>
        <link href="http://arxiv.org/abs/2003.14162"/>
        <updated>2021-06-21T02:07:40.774Z</updated>
        <summary type="html"><![CDATA[Deep state space models (SSMs) are an actively researched model class for
temporal models developed in the deep learning community which have a close
connection to classic SSMs. The use of deep SSMs as a black-box identification
model can describe a wide range of dynamics due to the flexibility of deep
neural networks. Additionally, the probabilistic nature of the model class
allows the uncertainty of the system to be modelled. In this work a deep SSM
class and its parameter learning algorithm are explained in an effort to extend
the toolbox of nonlinear identification methods with a deep learning based
method. Six recent deep SSMs are evaluated in a first unified implementation on
nonlinear system identification benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gedon_D/0/1/0/all/0/1"&gt;Daniel Gedon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wahlstrom_N/0/1/0/all/0/1"&gt;Niklas Wahlstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schon_T/0/1/0/all/0/1"&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ljung_L/0/1/0/all/0/1"&gt;Lennart Ljung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Finite Reward Automaton Inference and Reinforcement Learning Using Queries and Counterexamples. (arXiv:2006.15714v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15714</id>
        <link href="http://arxiv.org/abs/2006.15714"/>
        <updated>2021-06-21T02:07:40.766Z</updated>
        <summary type="html"><![CDATA[Despite the fact that deep reinforcement learning (RL) has surpassed
human-level performances in various tasks, it still has several fundamental
challenges. First, most RL methods require intensive data from the exploration
of the environment to achieve satisfactory performance. Second, the use of
neural networks in RL renders it hard to interpret the internals of the system
in a way that humans can understand. To address these two challenges, we
propose a framework that enables an RL agent to reason over its exploration
process and distill high-level knowledge for effectively guiding its future
explorations. Specifically, we propose a novel RL algorithm that learns
high-level knowledge in the form of a finite reward automaton by using the L*
learning algorithm. We prove that in episodic RL, a finite reward automaton can
express any non-Markovian bounded reward functions with finitely many reward
values and approximate any non-Markovian bounded reward function (with
infinitely many reward values) with arbitrary precision. We also provide a
lower bound for the episode length such that the proposed RL approach almost
surely converges to an optimal policy in the limit. We test this approach on
two RL environments with non-Markovian reward functions, choosing a variety of
tasks with increasing complexity for each environment. We compare our algorithm
with the state-of-the-art RL algorithms for non-Markovian reward functions,
such as Joint Inference of Reward machines and Policies for RL (JIRP), Learning
Reward Machine (LRM), and Proximal Policy Optimization (PPO2). Our results show
that our algorithm converges to an optimal policy faster than other baseline
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bo Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1"&gt;Aditya Ojha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1"&gt;Daniel Neider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Semantic Segmentation Augmented with Image-Level Weak Annotations. (arXiv:2007.01496v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.01496</id>
        <link href="http://arxiv.org/abs/2007.01496"/>
        <updated>2021-06-21T02:07:40.759Z</updated>
        <summary type="html"><![CDATA[Despite the great progress made by deep neural networks in the semantic
segmentation task, traditional neural-networkbased methods typically suffer
from a shortage of large amounts of pixel-level annotations. Recent progress in
fewshot semantic segmentation tackles the issue by only a few pixel-level
annotated examples. However, these few-shot approaches cannot easily be applied
to multi-way or weak annotation settings. In this paper, we advance the
few-shot segmentation paradigm towards a scenario where image-level annotations
are available to help the training process of a few pixel-level annotations.
Our key idea is to learn a better prototype representation of the class by
fusing the knowledge from the image-level labeled data. Specifically, we
propose a new framework, called PAIA, to learn the class prototype
representation in a metric space by integrating image-level annotations.
Furthermore, by considering the uncertainty of pseudo-masks, a distilled soft
masked average pooling strategy is designed to handle distractions in
image-level annotations. Extensive empirical results on two datasets show
superior performance of PAIA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1"&gt;Shuo Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianfeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fanglan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chang-Tien Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concurrent Neural Network : A model of competition between times series. (arXiv:2009.14610v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.14610</id>
        <link href="http://arxiv.org/abs/2009.14610"/>
        <updated>2021-06-21T02:07:40.753Z</updated>
        <summary type="html"><![CDATA[Competition between times series often arises in sales prediction, when
similar products are on sale on a marketplace. This article provides a model of
the presence of cannibalization between times series. This model creates a
"competitiveness" function that depends on external features such as price and
margin. It also provides a theoretical guaranty on the error of the model under
some reasonable conditions, and implement this model using a neural network to
compute this competitiveness function. This implementation outperforms other
traditional time series methods and classical neural networks for market share
prediction on a real-world data set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Garnier_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Garnier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Information Agent Modelling in Partially-Observable Environments. (arXiv:2006.09447v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09447</id>
        <link href="http://arxiv.org/abs/2006.09447"/>
        <updated>2021-06-21T02:07:40.735Z</updated>
        <summary type="html"><![CDATA[Modelling the behaviours of other agents is essential for understanding how
agents interact and making effective decisions. Existing methods for agent
modelling commonly assume knowledge of the local observations and chosen
actions of the modelled agents during execution. To eliminate this assumption,
we extract representations from the local information of the controlled agent
using encoder-decoder architectures. Using the observations and actions of the
modelled agents during training, our models learn to extract representations
about the modelled agents conditioned only on the local observations of the
controlled agent. The representations are used to augment the controlled
agent's decision policy which is trained via deep reinforcement learning; thus,
during execution, the policy does not require access to other agents'
information. We provide a comprehensive evaluation and ablations studies in
cooperative, competitive and mixed multi-agent environments, showing that our
method achieves significantly higher returns than baseline methods which do not
use the learned representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papoudakis_G/0/1/0/all/0/1"&gt;Georgios Papoudakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1"&gt;Filippos Christianos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1"&gt;Stefano V. Albrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Go with the Flows: Mixtures of Normalizing Flows for Point Cloud Generation and Reconstruction. (arXiv:2106.03135v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03135</id>
        <link href="http://arxiv.org/abs/2106.03135"/>
        <updated>2021-06-21T02:07:40.727Z</updated>
        <summary type="html"><![CDATA[Recently normalizing flows (NFs) have demonstrated state-of-the-art
performance on modeling 3D point clouds while allowing sampling with arbitrary
resolution at inference time. However, these flow-based models still require
long training times and large models for representing complicated geometries.
This work enhances their representational power by applying mixtures of NFs to
point clouds. We show that in this more general framework each component learns
to specialize in a particular subregion of an object in a completely
unsupervised fashion. By instantiating each mixture component with a
comparatively small NF we generate point clouds with improved details compared
to single-flow-based models while using fewer parameters and considerably
reducing the inference runtime. We further demonstrate that by adding data
augmentation, individual mixture components can learn to specialize in a
semantically meaningful manner. We evaluate mixtures of NFs on generation,
autoencoding and single-view reconstruction based on the ShapeNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Postels_J/0/1/0/all/0/1"&gt;Janis Postels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengya Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spezialetti_R/0/1/0/all/0/1"&gt;Riccardo Spezialetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CIRA Guide to Custom Loss Functions for Neural Networks in Environmental Sciences -- Version 1. (arXiv:2106.09757v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09757</id>
        <link href="http://arxiv.org/abs/2106.09757"/>
        <updated>2021-06-21T02:07:40.721Z</updated>
        <summary type="html"><![CDATA[Neural networks are increasingly used in environmental science applications.
Furthermore, neural network models are trained by minimizing a loss function,
and it is crucial to choose the loss function very carefully for environmental
science applications, as it determines what exactly is being optimized.
Standard loss functions do not cover all the needs of the environmental
sciences, which makes it important for scientists to be able to develop their
own custom loss functions so that they can implement many of the classic
performance measures already developed in environmental science, including
measures developed for spatial model verification. However, there are very few
resources available that cover the basics of custom loss function development
comprehensively, and to the best of our knowledge none that focus on the needs
of environmental scientists. This document seeks to fill this gap by providing
a guide on how to write custom loss functions targeted toward environmental
science applications. Topics include the basics of writing custom loss
functions, common pitfalls, functions to use in loss functions, examples such
as fractions skill score as loss function, how to incorporate physical
constraints, discrete and soft discretization, and concepts such as focal,
robust, and adaptive loss. While examples are currently provided in this guide
for Python with Keras and the TensorFlow backend, the basic concepts also apply
to other environments, such as Python with PyTorch. Similarly, while the sample
loss functions provided here are from meteorology, these are just examples of
how to create custom loss functions. Other fields in the environmental sciences
have very similar needs for custom loss functions, e.g., for evaluating spatial
forecasts effectively, and the concepts discussed here can be applied there as
well. All code samples are provided in a GitHub repository.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ebert_Uphoff_I/0/1/0/all/0/1"&gt;Imme Ebert-Uphoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lagerquist_R/0/1/0/all/0/1"&gt;Ryan Lagerquist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilburn_K/0/1/0/all/0/1"&gt;Kyle Hilburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yoonjin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haynes_K/0/1/0/all/0/1"&gt;Katherine Haynes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stock_J/0/1/0/all/0/1"&gt;Jason Stock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumler_C/0/1/0/all/0/1"&gt;Christina Kumler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stewart_J/0/1/0/all/0/1"&gt;Jebb Q. Stewart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MARS: Masked Automatic Ranks Selection in Tensor Decompositions. (arXiv:2006.10859v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10859</id>
        <link href="http://arxiv.org/abs/2006.10859"/>
        <updated>2021-06-21T02:07:40.714Z</updated>
        <summary type="html"><![CDATA[Tensor decomposition methods are known to be efficient for compressing and
accelerating neural networks. However, the problem of optimal decomposition
structure determination is still not well studied while being quite important.
Specifically, decomposition ranks present the crucial parameter controlling the
compression-accuracy trade-off. In this paper, we introduce MARS -- a new
efficient method for the automatic selection of ranks in general tensor
decompositions. During training, the procedure learns binary masks over
decomposition cores that "select" the optimal tensor structure. The learning is
performed via relaxed maximum a posteriori (MAP) estimation in a specific
Bayesian model. The proposed method achieves better results compared to
previous works in various tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kodryan_M/0/1/0/all/0/1"&gt;Maxim Kodryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kropotov_D/0/1/0/all/0/1"&gt;Dmitry Kropotov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Stochastic Compositional Optimization is Nearly as Easy as Solving Stochastic Optimization. (arXiv:2008.10847v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10847</id>
        <link href="http://arxiv.org/abs/2008.10847"/>
        <updated>2021-06-21T02:07:40.707Z</updated>
        <summary type="html"><![CDATA[Stochastic compositional optimization generalizes classic (non-compositional)
stochastic optimization to the minimization of compositions of functions. Each
composition may introduce an additional expectation. The series of expectations
may be nested. Stochastic compositional optimization is gaining popularity in
applications such as reinforcement learning and meta learning. This paper
presents a new Stochastically Corrected Stochastic Compositional gradient
method (SCSC). SCSC runs in a single-time scale with a single loop, uses a
fixed batch size, and guarantees to converge at the same rate as the stochastic
gradient descent (SGD) method for non-compositional stochastic optimization.
This is achieved by making a careful improvement to a popular stochastic
compositional gradient method. It is easy to apply SGD-improvement techniques
to accelerate SCSC. This helps SCSC achieve state-of-the-art performance for
stochastic compositional optimization. In particular, we apply Adam to SCSC,
and the exhibited rate of convergence matches that of the original Adam on
non-compositional stochastic optimization. We test SCSC using the portfolio
management and model-agnostic meta-learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuejiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wotao Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Change-Point Detection with Training Sequences in the Large and Moderate Deviations Regimes. (arXiv:2003.06511v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.06511</id>
        <link href="http://arxiv.org/abs/2003.06511"/>
        <updated>2021-06-21T02:07:40.688Z</updated>
        <summary type="html"><![CDATA[This paper investigates a novel offline change-point detection problem from
an information-theoretic perspective. In contrast to most related works, we
assume that the knowledge of the underlying pre- and post-change distributions
are not known and can only be learned from the training sequences which are
available. We further require the probability of the \emph{estimation error} to
decay either exponentially or sub-exponentially fast (corresponding
respectively to the large and moderate deviations regimes in information theory
parlance). Based on the training sequences as well as the test sequence
consisting of a single change-point, we design a change-point estimator and
further show that this estimator is optimal by establishing matching (strong)
converses. This leads to a full characterization of the optimal confidence
width (i.e., half the width of the confidence interval within which the true
change-point is located at with high probability) as a function of the
undetected error, under both the large and moderate deviations regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Haiyun He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiaosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Trimmed Lasso: Sparse Recovery Guarantees and Practical Optimization by the Generalized Soft-Min Penalty. (arXiv:2005.09021v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.09021</id>
        <link href="http://arxiv.org/abs/2005.09021"/>
        <updated>2021-06-21T02:07:40.681Z</updated>
        <summary type="html"><![CDATA[We present a new approach to solve the sparse approximation or best subset
selection problem, namely find a $k$-sparse vector ${\bf x}\in\mathbb{R}^d$
that minimizes the $\ell_2$ residual $\lVert A{\bf x}-{\bf y} \rVert_2$. We
consider a regularized approach, whereby this residual is penalized by the
non-convex $\textit{trimmed lasso}$, defined as the $\ell_1$-norm of ${\bf x}$
excluding its $k$ largest-magnitude entries. We prove that the trimmed lasso
has several appealing theoretical properties, and in particular derive sparse
recovery guarantees assuming successful optimization of the penalized
objective. Next, we show empirically that directly optimizing this objective
can be quite challenging. Instead, we propose a surrogate for the trimmed
lasso, called the $\textit{generalized soft-min}$. This penalty smoothly
interpolates between the classical lasso and the trimmed lasso, while taking
into account all possible $k$-sparse patterns. The generalized soft-min penalty
involves summation over $\binom{d}{k}$ terms, yet we derive a polynomial-time
algorithm to compute it. This, in turn, yields a practical method for the
original sparse approximation problem. Via simulations, we demonstrate its
competitive performance compared to current state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amir_T/0/1/0/all/0/1"&gt;Tal Amir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basri_R/0/1/0/all/0/1"&gt;Ronen Basri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadler_B/0/1/0/all/0/1"&gt;Boaz Nadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leakage of Dataset Properties in Multi-Party Machine Learning. (arXiv:2006.07267v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07267</id>
        <link href="http://arxiv.org/abs/2006.07267"/>
        <updated>2021-06-21T02:07:40.662Z</updated>
        <summary type="html"><![CDATA[Secure multi-party machine learning allows several parties to build a model
on their pooled data to increase utility while not explicitly sharing data with
each other. We show that such multi-party computation can cause leakage of
global dataset properties between the parties even when parties obtain only
black-box access to the final model. In particular, a ``curious'' party can
infer the distribution of sensitive attributes in other parties' data with high
accuracy. This raises concerns regarding the confidentiality of properties
pertaining to the whole dataset as opposed to individual data records. We show
that our attack can leak population-level properties in datasets of different
types, including tabular, text, and graph data. To understand and measure the
source of leakage, we consider several models of correlation between a
sensitive attribute and the rest of the data. Using multiple machine learning
models, we show that leakage occurs even if the sensitive attribute is not
included in the training data and has a low correlation with other attributes
or the target variable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wanrong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1"&gt;Shruti Tople&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1"&gt;Olga Ohrimenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NoiseGrad: enhancing explanations by introducing stochasticity to model weights. (arXiv:2106.10185v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10185</id>
        <link href="http://arxiv.org/abs/2106.10185"/>
        <updated>2021-06-21T02:07:40.252Z</updated>
        <summary type="html"><![CDATA[Attribution methods remain a practical instrument that is used in real-world
applications to explain the decision-making process of complex learning
machines. It has been shown that a simple method called SmoothGrad can
effectively reduce the visual diffusion of gradient-based attribution methods
and has established itself among both researchers and practitioners. What
remains unexplored in research, however, is how explanations can be improved by
introducing stochasticity to the model weights. In the light of this, we
introduce - NoiseGrad - a stochastic, method-agnostic explanation-enhancing
method that adds noise to the weights instead of the input data. We investigate
our proposed method through various experiments including different datasets,
explanation methods and network architectures and conclude that NoiseGrad (and
its extension NoiseGrad++) with multiplicative Gaussian noise offers a clear
advantage compared to SmoothGrad on several evaluation criteria. We connect our
proposed method to Bayesian Learning and provide the user with a heuristic for
choosing hyperparameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1"&gt;Kirill Bykov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hedstrom_A/0/1/0/all/0/1"&gt;Anna Hedstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1"&gt;Shinichi Nakajima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1"&gt;Marina M.-C. H&amp;#xf6;hne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FinGAT: Financial Graph Attention Networks for Recommending Top-K Profitable Stocks. (arXiv:2106.10159v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10159</id>
        <link href="http://arxiv.org/abs/2106.10159"/>
        <updated>2021-06-21T02:07:40.210Z</updated>
        <summary type="html"><![CDATA[Financial technology (FinTech) has drawn much attention among investors and
companies. While conventional stock analysis in FinTech targets at predicting
stock prices, less effort is made for profitable stock recommendation. Besides,
in existing approaches on modeling time series of stock prices, the
relationships among stocks and sectors (i.e., categories of stocks) are either
neglected or pre-defined. Ignoring stock relationships will miss the
information shared between stocks while using pre-defined relationships cannot
depict the latent interactions or influence of stock prices between stocks. In
this work, we aim at recommending the top-K profitable stocks in terms of
return ratio using time series of stock prices and sector information. We
propose a novel deep learning-based model, Financial Graph Attention Networks
(FinGAT), to tackle the task under the setting that no pre-defined
relationships between stocks are given. The idea of FinGAT is three-fold.
First, we devise a hierarchical learning component to learn short-term and
long-term sequential patterns from stock time series. Second, a fully-connected
graph between stocks and a fully-connected graph between sectors are
constructed, along with graph attention networks, to learn the latent
interactions among stocks and sectors. Third, a multi-task objective is devised
to jointly recommend the profitable stocks and predict the stock movement.
Experiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit
remarkable recommendation performance of our FinGAT, comparing to
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1"&gt;Yi-Ling Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yu-Che Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Cheng-Te Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low Resource German ASR with Untranscribed Data Spoken by Non-native Children -- INTERSPEECH 2021 Shared Task SPAPL System. (arXiv:2106.09963v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.09963</id>
        <link href="http://arxiv.org/abs/2106.09963"/>
        <updated>2021-06-21T02:07:40.192Z</updated>
        <summary type="html"><![CDATA[This paper describes the SPAPL system for the INTERSPEECH 2021 Challenge:
Shared Task on Automatic Speech Recognition for Non-Native Children's Speech in
German. ~ 5 hours of transcribed data and ~ 60 hours of untranscribed data are
provided to develop a German ASR system for children. For the training of the
transcribed data, we propose a non-speech state discriminative loss (NSDL) to
mitigate the influence of long-duration non-speech segments within speech
utterances. In order to explore the use of the untranscribed data, various
approaches are implemented and combined together to incrementally improve the
system performance. First, bidirectional autoregressive predictive coding
(Bi-APC) is used to learn initial parameters for acoustic modelling using the
provided untranscribed data. Second, incremental semi-supervised learning is
further used to iteratively generate pseudo-transcribed data. Third, different
data augmentation schemes are used at different training stages to increase the
variability and size of the training data. Finally, a recurrent neural network
language model (RNNLM) is used for rescoring. Our system achieves a word error
rate (WER) of 39.68% on the evaluation data, an approximately 12% relative
improvement over the official baseline (45.21%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jinhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yunzheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_R/0/1/0/all/0/1"&gt;Ruchao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chu_W/0/1/0/all/0/1"&gt;Wei Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alwan_A/0/1/0/all/0/1"&gt;Abeer Alwan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Generate Code Sketches. (arXiv:2106.10158v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10158</id>
        <link href="http://arxiv.org/abs/2106.10158"/>
        <updated>2021-06-21T02:07:40.185Z</updated>
        <summary type="html"><![CDATA[Traditional generative models are limited to predicting sequences of terminal
tokens. However, ambiguities in the generation task may lead to incorrect
outputs. Towards addressing this, we introduce Grammformers, transformer-based
grammar-guided models that learn (without explicit supervision) to generate
sketches -- sequences of tokens with holes. Through reinforcement learning,
Grammformers learn to introduce holes avoiding the generation of incorrect
tokens where there is ambiguity in the target task.

We train Grammformers for statement-level source code completion, i.e., the
generation of code snippets given an ambiguous user intent, such as a partial
code context. We evaluate Grammformers on code completion for C# and Python and
show that it generates 10-50% more accurate sketches compared to traditional
generative models and 37-50% longer sketches compared to sketch-generating
baselines trained with similar techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1"&gt;Daya Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1"&gt;Alexey Svyatkovskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jian Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1"&gt;Marc Brockschmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allamanis_M/0/1/0/all/0/1"&gt;Miltiadis Allamanis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating the Role of Negatives in Contrastive Representation Learning. (arXiv:2106.09943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09943</id>
        <link href="http://arxiv.org/abs/2106.09943"/>
        <updated>2021-06-21T02:07:40.158Z</updated>
        <summary type="html"><![CDATA[Noise contrastive learning is a popular technique for unsupervised
representation learning. In this approach, a representation is obtained via
reduction to supervised learning, where given a notion of semantic similarity,
the learner tries to distinguish a similar (positive) example from a collection
of random (negative) examples. The success of modern contrastive learning
pipelines relies on many parameters such as the choice of data augmentation,
the number of negative examples, and the batch size; however, there is limited
understanding as to how these parameters interact and affect downstream
performance. We focus on disambiguating the role of one of these parameters:
the number of negative examples. Theoretically, we show the existence of a
collision-coverage trade-off suggesting that the optimal number of negative
examples should scale with the number of underlying concepts in the data.
Empirically, we scrutinize the role of the number of negatives in both NLP and
vision tasks. In the NLP task, we find that the results broadly agree with our
theory, while our vision experiments are murkier with performance sometimes
even being insensitive to the number of negatives. We discuss plausible
explanations for this behavior and suggest future directions to better align
theory and practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1"&gt;Jordan T. Ash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1"&gt;Surbhi Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1"&gt;Akshay Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[World-GAN: a Generative Model for Minecraft Worlds. (arXiv:2106.10155v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10155</id>
        <link href="http://arxiv.org/abs/2106.10155"/>
        <updated>2021-06-21T02:07:40.152Z</updated>
        <summary type="html"><![CDATA[This work introduces World-GAN, the first method to perform data-driven
Procedural Content Generation via Machine Learning in Minecraft from a single
example. Based on a 3D Generative Adversarial Network (GAN) architecture, we
are able to create arbitrarily sized world snippets from a given sample. We
evaluate our approach on creations from the community as well as structures
generated with the Minecraft World Generator. Our method is motivated by the
dense representations used in Natural Language Processing (NLP) introduced with
word2vec [1]. The proposed block2vec representations make World-GAN independent
from the number of different blocks, which can vary a lot in Minecraft, and
enable the generation of larger levels. Finally, we demonstrate that changing
this new representation space allows us to change the generated style of an
already trained generator. World-GAN enables its users to generate Minecraft
worlds based on parts of their creations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awiszus_M/0/1/0/all/0/1"&gt;Maren Awiszus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_F/0/1/0/all/0/1"&gt;Frederik Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1"&gt;Bodo Rosenhahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Contrastive Representations of Stochastic Processes. (arXiv:2106.10052v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10052</id>
        <link href="http://arxiv.org/abs/2106.10052"/>
        <updated>2021-06-21T02:07:40.145Z</updated>
        <summary type="html"><![CDATA[Learning representations of stochastic processes is an emerging problem in
machine learning with applications from meta-learning to physical object models
to time series. Typical methods rely on exact reconstruction of observations,
but this approach breaks down as observations become high-dimensional or noise
distributions become complex. To address this, we propose a unifying framework
for learning contrastive representations of stochastic processes (CRESP) that
does away with exact reconstruction. We dissect potential use cases for
stochastic process representations, and propose methods that accommodate each.
Empirically, we show that our methods are effective for learning
representations of periodic functions, 3D objects and dynamical processes. Our
methods tolerate noisy high-dimensional observations better than traditional
approaches, and the learned representations transfer to a range of downstream
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mathieu_E/0/1/0/all/0/1"&gt;Emile Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1"&gt;Adam Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wide stochastic networks: Gaussian limit and PAC-Bayesian training. (arXiv:2106.09798v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.09798</id>
        <link href="http://arxiv.org/abs/2106.09798"/>
        <updated>2021-06-21T02:07:40.031Z</updated>
        <summary type="html"><![CDATA[The limit of infinite width allows for substantial simplifications in the
analytical study of overparameterized neural networks. With a suitable random
initialization, an extremely large network is well approximated by a Gaussian
process, both before and during training. In the present work, we establish a
similar result for a simple stochastic architecture whose parameters are random
variables. The explicit evaluation of the output distribution allows for a
PAC-Bayesian training procedure that directly optimizes the generalization
bound. For a large but finite-width network, we show empirically on MNIST that
this training approach can outperform standard PAC-Bayesian methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Clerico_E/0/1/0/all/0/1"&gt;Eugenio Clerico&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1"&gt;George Deligiannidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Analysis of the Deployment of Models Trained on Private Tabular Synthetic Data: Unexpected Surprises. (arXiv:2106.10241v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10241</id>
        <link href="http://arxiv.org/abs/2106.10241"/>
        <updated>2021-06-21T02:07:40.024Z</updated>
        <summary type="html"><![CDATA[Diferentially private (DP) synthetic datasets are a powerful approach for
training machine learning models while respecting the privacy of individual
data providers. The effect of DP on the fairness of the resulting trained
models is not yet well understood. In this contribution, we systematically
study the effects of differentially private synthetic data generation on
classification. We analyze disparities in model utility and bias caused by the
synthetic dataset, measured through algorithmic fairness metrics. Our first set
of results show that although there seems to be a clear negative correlation
between privacy and utility (the more private, the less accurate) across all
data synthesizers we evaluated, more privacy does not necessarily imply more
bias. Additionally, we assess the effects of utilizing synthetic datasets for
model training and model evaluation. We show that results obtained on synthetic
data can misestimate the actual model performance when it is deployed on real
data. We hence advocate on the need for defining proper testing protocols in
scenarios where differentially private synthetic datasets are utilized for
model training and evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pereira_M/0/1/0/all/0/1"&gt;Mayana Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kshirsagar_M/0/1/0/all/0/1"&gt;Meghana Kshirsagar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Sumit Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan Lavista Ferres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistency of Extreme Learning Machines and Regression under Non-Stationarity and Dependence for ML-Enhanced Moving Objects. (arXiv:2005.11115v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.11115</id>
        <link href="http://arxiv.org/abs/2005.11115"/>
        <updated>2021-06-21T02:07:40.017Z</updated>
        <summary type="html"><![CDATA[Supervised learning by extreme learning machines resp. neural networks with
random weights is studied under a non-stationary spatial-temporal sampling
design which especially addresses settings where an autonomous object moving in
a non-stationary spatial environment collects and analyzes data. The stochastic
model especially allows for spatial heterogeneity and weak dependence. As
efficient and computationally cheap learning methods (unconstrained) least
squares, ridge regression and $\ell_s$-penalized least squares (including the
LASSO) are studied. Consistency and asymptotic normality of the least squares
and ridge regression estimates as well as corresponding consistency results for
the $\ell_s$-penalty are shown under weak conditions. The resuts also cover
bounds for the sample squared predicition error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Steland_A/0/1/0/all/0/1"&gt;Ansgar Steland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Reduction and Neural Networks for Parametric PDEs. (arXiv:2005.03180v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.03180</id>
        <link href="http://arxiv.org/abs/2005.03180"/>
        <updated>2021-06-21T02:07:40.010Z</updated>
        <summary type="html"><![CDATA[We develop a general framework for data-driven approximation of input-output
maps between infinite-dimensional spaces. The proposed approach is motivated by
the recent successes of neural networks and deep learning, in combination with
ideas from model reduction. This combination results in a neural network
approximation which, in principle, is defined on infinite-dimensional spaces
and, in practice, is robust to the dimension of finite-dimensional
approximations of these spaces required for computation. For a class of
input-output maps, and suitably chosen probability measures on the inputs, we
prove convergence of the proposed approximation methodology. We also include
numerical experiments which demonstrate the effectiveness of the method,
showing convergence and robustness of the approximation scheme with respect to
the size of the discretization, and compare it with existing algorithms from
the literature; our examples include the mapping from coefficient to solution
in a divergence form elliptic partial differential equation (PDE) problem, and
the solution operator for viscous Burgers' equation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Bhattacharya_K/0/1/0/all/0/1"&gt;Kaushik Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hosseini_B/0/1/0/all/0/1"&gt;Bamdad Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kovachki_N/0/1/0/all/0/1"&gt;Nikola B. Kovachki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1"&gt;Andrew M. Stuart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Error: a New Performance Measure for Adversarial Robustness. (arXiv:2106.10212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10212</id>
        <link href="http://arxiv.org/abs/2106.10212"/>
        <updated>2021-06-21T02:07:40.004Z</updated>
        <summary type="html"><![CDATA[Despite the significant advances in deep learning over the past decade, a
major challenge that limits the wide-spread adoption of deep learning has been
their fragility to adversarial attacks. This sensitivity to making erroneous
predictions in the presence of adversarially perturbed data makes deep neural
networks difficult to adopt for certain real-world, mission-critical
applications. While much of the research focus has revolved around adversarial
example creation and adversarial hardening, the area of performance measures
for assessing adversarial robustness is not well explored. Motivated by this,
this study presents the concept of residual error, a new performance measure
for not only assessing the adversarial robustness of a deep neural network at
the individual sample level, but also can be used to differentiate between
adversarial and non-adversarial examples to facilitate for adversarial example
detection. Furthermore, we introduce a hybrid model for approximating the
residual error in a tractable manner. Experimental results using the case of
image classification demonstrates the effectiveness and efficacy of the
proposed residual error metric for assessing several well-known deep neural
network architectures. These results thus illustrate that the proposed measure
could be a useful tool for not only assessing the robustness of deep neural
networks used in mission-critical scenarios, but also in the design of
adversarially robust models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aboutalebi_H/0/1/0/all/0/1"&gt;Hossein Aboutalebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1"&gt;Mohammad Javad Shafiee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karg_M/0/1/0/all/0/1"&gt;Michelle Karg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scharfenberger_C/0/1/0/all/0/1"&gt;Christian Scharfenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No Routing Needed Between Capsules. (arXiv:2001.09136v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09136</id>
        <link href="http://arxiv.org/abs/2001.09136"/>
        <updated>2021-06-21T02:07:39.984Z</updated>
        <summary type="html"><![CDATA[Most capsule network designs rely on traditional matrix multiplication
between capsule layers and computationally expensive routing mechanisms to deal
with the capsule dimensional entanglement that the matrix multiplication
introduces. By using Homogeneous Vector Capsules (HVCs), which use element-wise
multiplication rather than matrix multiplication, the dimensions of the
capsules remain unentangled. In this work, we study HVCs as applied to the
highly structured MNIST dataset in order to produce a direct comparison to the
capsule research direction of Geoffrey Hinton, et al. In our study, we show
that a simple convolutional neural network using HVCs performs as well as the
prior best performing capsule network on MNIST using 5.5x fewer parameters, 4x
fewer training epochs, no reconstruction sub-network, and requiring no routing
mechanism. The addition of multiple classification branches to the network
establishes a new state of the art for the MNIST dataset with an accuracy of
99.87% for an ensemble of these models, as well as establishing a new state of
the art for a single model (99.83% accurate).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Byerly_A/0/1/0/all/0/1"&gt;Adam Byerly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalganova_T/0/1/0/all/0/1"&gt;Tatiana Kalganova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dear_I/0/1/0/all/0/1"&gt;Ian Dear&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Private Graph Neural Networks. (arXiv:2006.05535v8 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05535</id>
        <link href="http://arxiv.org/abs/2006.05535"/>
        <updated>2021-06-21T02:07:39.978Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have demonstrated superior performance in
learning node representations for various graph inference tasks. However,
learning over graph data can raise privacy concerns when nodes represent people
or human-related variables that involve sensitive or personal information.
While numerous techniques have been proposed for privacy-preserving deep
learning over non-relational data, there is less work addressing the privacy
issues pertained to applying deep learning algorithms on graphs. In this paper,
we study the problem of node data privacy, where graph nodes have potentially
sensitive data that is kept private, but they could be beneficial for a central
server for training a GNN over the graph. To address this problem, we develop a
privacy-preserving, architecture-agnostic GNN learning algorithm with formal
privacy guarantees based on Local Differential Privacy (LDP). Specifically, we
propose an LDP encoder and an unbiased rectifier, by which the server can
communicate with the graph nodes to privately collect their data and
approximate the GNN's first layer. To further reduce the effect of the injected
noise, we propose to prepend a simple graph convolution layer, called KProp,
which is based on the multi-hop aggregation of the nodes' features acting as a
denoising mechanism. Finally, we propose a robust training framework, in which
we benefit from KProp's denoising capability to increase the accuracy of
inference in the presence of noisy labels. Extensive experiments conducted over
real-world datasets demonstrate that our method can maintain a satisfying level
of accuracy with low privacy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sajadmanesh_S/0/1/0/all/0/1"&gt;Sina Sajadmanesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatica_Perez_D/0/1/0/all/0/1"&gt;Daniel Gatica-Perez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Pseudo-Point and State Space Approximations for Sum-Separable Gaussian Processes. (arXiv:2106.10210v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10210</id>
        <link href="http://arxiv.org/abs/2106.10210"/>
        <updated>2021-06-21T02:07:39.970Z</updated>
        <summary type="html"><![CDATA[Gaussian processes (GPs) are important probabilistic tools for inference and
learning in spatio-temporal modelling problems such as those in climate science
and epidemiology. However, existing GP approximations do not simultaneously
support large numbers of off-the-grid spatial data-points and long time-series
which is a hallmark of many applications.

Pseudo-point approximations, one of the gold-standard methods for scaling GPs
to large data sets, are well suited for handling off-the-grid spatial data.
However, they cannot handle long temporal observation horizons effectively
reverting to cubic computational scaling in the time dimension. State space GP
approximations are well suited to handling temporal data, if the temporal GP
prior admits a Markov form, leading to linear complexity in the number of
temporal observations, but have a cubic spatial cost and cannot handle
off-the-grid spatial data.

In this work we show that there is a simple and elegant way to combine
pseudo-point methods with the state space GP approximation framework to get the
best of both worlds. The approach hinges on a surprising conditional
independence property which applies to space--time separable GPs. We
demonstrate empirically that the combined approach is more scalable and
applicable to a greater range of spatio-temporal problems than either method on
its own.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tebbutt_W/0/1/0/all/0/1"&gt;Will Tebbutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1"&gt;Arno Solin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1"&gt;Richard E. Turner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonparametric Hamiltonian Monte Carlo. (arXiv:2106.10238v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10238</id>
        <link href="http://arxiv.org/abs/2106.10238"/>
        <updated>2021-06-21T02:07:39.963Z</updated>
        <summary type="html"><![CDATA[Probabilistic programming uses programs to express generative models whose
posterior probability is then computed by built-in inference engines. A
challenging goal is to develop general purpose inference algorithms that work
out-of-the-box for arbitrary programs in a universal probabilistic programming
language (PPL). The densities defined by such programs, which may use
stochastic branching and recursion, are (in general) nonparametric, in the
sense that they correspond to models on an infinite-dimensional parameter
space. However standard inference algorithms, such as the Hamiltonian Monte
Carlo (HMC) algorithm, target distributions with a fixed number of parameters.
This paper introduces the Nonparametric Hamiltonian Monte Carlo (NP-HMC)
algorithm which generalises HMC to nonparametric models. Inputs to NP-HMC are a
new class of measurable functions called "tree representable", which serve as a
language-independent representation of the density functions of probabilistic
programs in a universal PPL. We provide a correctness proof of NP-HMC, and
empirically demonstrate significant performance improvements over existing
approaches on several nonparametric examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mak_C/0/1/0/all/0/1"&gt;Carol Mak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiser_F/0/1/0/all/0/1"&gt;Fabian Zaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_L/0/1/0/all/0/1"&gt;Luke Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some Theoretical Insights into Wasserstein GANs. (arXiv:2006.02682v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02682</id>
        <link href="http://arxiv.org/abs/2006.02682"/>
        <updated>2021-06-21T02:07:39.956Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have been successful in producing
outstanding results in areas as diverse as image, video, and text generation.
Building on these successes, a large number of empirical studies have validated
the benefits of the cousin approach called Wasserstein GANs (WGANs), which
brings stabilization in the training process. In the present paper, we add a
new stone to the edifice by proposing some theoretical advances in the
properties of WGANs. First, we properly define the architecture of WGANs in the
context of integral probability metrics parameterized by neural networks and
highlight some of their basic mathematical features. We stress in particular
interesting optimization properties arising from the use of a parametric
1-Lipschitz discriminator. Then, in a statistically-driven approach, we study
the convergence of empirical WGANs as the sample size tends to infinity, and
clarify the adversarial effects of the generator and the discriminator by
underlining some trade-off properties. These features are finally illustrated
with experiments using both synthetic and real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biau_G/0/1/0/all/0/1"&gt;G&amp;#xe9;rard Biau&lt;/a&gt; (LPSM (UMR\_8001)), &lt;a href="http://arxiv.org/find/cs/1/au:+Sangnier_M/0/1/0/all/0/1"&gt;Maxime Sangnier&lt;/a&gt; (LPSM (UMR\_8001)), &lt;a href="http://arxiv.org/find/cs/1/au:+Tanielian_U/0/1/0/all/0/1"&gt;Ugo Tanielian&lt;/a&gt; (LPSM (UMR\_8001))</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boltzmann machine learning and regularization methods for inferring evolutionary fields and couplings from a multiple sequence alignment. (arXiv:1909.05006v3 [q-bio.PE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.05006</id>
        <link href="http://arxiv.org/abs/1909.05006"/>
        <updated>2021-06-21T02:07:39.936Z</updated>
        <summary type="html"><![CDATA[The inverse Potts problem to infer a Boltzmann distribution for homologous
protein sequences from their single-site and pairwise amino acid frequencies
recently attracts a great deal of attention in the studies of protein structure
and evolution. We study regularization and learning methods and how to tune
regularization parameters to correctly infer interactions in Boltzmann machine
learning. Using $L_2$ regularization for fields, group $L_1$ for couplings is
shown to be very effective for sparse couplings in comparison with $L_2$ and
$L_1$. Two regularization parameters are tuned to yield equal values for both
the sample and ensemble averages of evolutionary energy. Both averages smoothly
change and converge, but their learning profiles are very different between
learning methods. The Adam method is modified to make stepsize proportional to
the gradient for sparse couplings and to use a soft-thresholding function for
group $L_1$. It is shown by first inferring interactions from protein sequences
and then from Monte Carlo samples that the fields and couplings can be well
recovered, but that recovering the pairwise correlations in the resolution of a
total energy is harder for the natural proteins than for the protein-like
sequences. Selective temperature for folding/structural constrains in protein
evolution is also estimated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Miyazawa_S/0/1/0/all/0/1"&gt;Sanzo Miyazawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On-Device Personalization of Automatic Speech Recognition Models for Disordered Speech. (arXiv:2106.10259v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.10259</id>
        <link href="http://arxiv.org/abs/2106.10259"/>
        <updated>2021-06-21T02:07:39.929Z</updated>
        <summary type="html"><![CDATA[While current state-of-the-art Automatic Speech Recognition (ASR) systems
achieve high accuracy on typical speech, they suffer from significant
performance degradation on disordered speech and other atypical speech
patterns. Personalization of ASR models, a commonly applied solution to this
problem, is usually performed in a server-based training environment posing
problems around data privacy, delayed model-update times, and communication
cost for copying data and models between mobile device and server
infrastructure. In this paper, we present an approach to on-device based ASR
personalization with very small amounts of speaker-specific data. We test our
approach on a diverse set of 100 speakers with disordered speech and find
median relative word error rate improvement of 71% with only 50 short
utterances required per speaker. When tested on a voice-controlled home
automation platform, on-device personalized models show a median task success
rate of 81%, compared to only 40% of the unadapted models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1"&gt;Katrin Tomanek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;oise Beaufays&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cattiau_J/0/1/0/all/0/1"&gt;Julie Cattiau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chandorkar_A/0/1/0/all/0/1"&gt;Angad Chandorkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1"&gt;Khe Chai Sim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embodied Language Grounding with 3D Visual Feature Representations. (arXiv:1910.01210v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.01210</id>
        <link href="http://arxiv.org/abs/1910.01210"/>
        <updated>2021-06-21T02:07:39.922Z</updated>
        <summary type="html"><![CDATA[We propose associating language utterances to 3D visual abstractions of the
scene they describe. The 3D visual abstractions are encoded as 3-dimensional
visual feature maps. We infer these 3D visual scene feature maps from RGB
images of the scene via view prediction: when the generated 3D scene feature
map is neurally projected from a camera viewpoint, it should match the
corresponding RGB image. We present generative models that condition on the
dependency tree of an utterance and generate a corresponding visual 3D feature
map as well as reason about its plausibility, and detector models that
condition on both the dependency tree of an utterance and a related image and
localize the object referents in the 3D feature map inferred from the image.
Our model outperforms models of language and vision that associate language
with 2D CNN activations or 2D images by a large margin in a variety of tasks,
such as, classifying plausibility of utterances, detecting referential
expressions, and supplying rewards for trajectory optimization of object
placement policies from language instructions. We perform numerous ablations
and show the improved performance of our detectors is due to its better
generalization across camera viewpoints and lack of object interferences in the
inferred 3D feature space, and the improved performance of our generators is
due to their ability to spatially reason about objects and their configurations
in 3D when mapping from language to scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prabhudesai_M/0/1/0/all/0/1"&gt;Mihir Prabhudesai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1"&gt;Hsiao-Yu Fish Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1"&gt;Syed Ashar Javed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sieb_M/0/1/0/all/0/1"&gt;Maximilian Sieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1"&gt;Adam W. Harley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1"&gt;Katerina Fragkiadaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Probabilistic Representation of DNNs: Bridging Mutual Information and Generalization. (arXiv:2106.10262v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10262</id>
        <link href="http://arxiv.org/abs/2106.10262"/>
        <updated>2021-06-21T02:07:39.915Z</updated>
        <summary type="html"><![CDATA[Recently, Mutual Information (MI) has attracted attention in bounding the
generalization error of Deep Neural Networks (DNNs). However, it is intractable
to accurately estimate the MI in DNNs, thus most previous works have to relax
the MI bound, which in turn weakens the information theoretic explanation for
generalization. To address the limitation, this paper introduces a
probabilistic representation of DNNs for accurately estimating the MI.
Leveraging the proposed MI estimator, we validate the information theoretic
explanation for generalization, and derive a tighter generalization bound than
the state-of-the-art relaxations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1"&gt;Xinjie Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barner_K/0/1/0/all/0/1"&gt;Kenneth Barner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LoRMIkA: Local rule-based model interpretability with k-optimal associations. (arXiv:1908.03840v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.03840</id>
        <link href="http://arxiv.org/abs/1908.03840"/>
        <updated>2021-06-21T02:07:39.909Z</updated>
        <summary type="html"><![CDATA[As we rely more and more on machine learning models for real-life
decision-making, being able to understand and trust the predictions becomes
ever more important. Local explainer models have recently been introduced to
explain the predictions of complex machine learning models at the instance
level. In this paper, we propose Local Rule-based Model Interpretability with
k-optimal Associations (LoRMIkA), a novel model-agnostic approach that obtains
k-optimal association rules from a neighbourhood of the instance to be
explained. Compared with other rule-based approaches in the literature, we
argue that the most predictive rules are not necessarily the rules that provide
the best explanations. Consequently, the LoRMIkA framework provides a flexible
way to obtain predictive and interesting rules. It uses an efficient search
algorithm guaranteed to find the k-optimal rules with respect to objectives
such as confidence, lift, leverage, coverage, and support. It also provides
multiple rules which explain the decision and counterfactual rules, which give
indications for potential changes to obtain different outputs for given
instances. We compare our approach to other state-of-the-art approaches in
local model interpretability on three different datasets and achieve
competitive results in terms of local accuracy and interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajapaksha_D/0/1/0/all/0/1"&gt;Dilini Rajapaksha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1"&gt;Christoph Bergmeir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1"&gt;Wray Buntine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Investigation into Deep and Shallow Rule Learning. (arXiv:2106.10254v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10254</id>
        <link href="http://arxiv.org/abs/2106.10254"/>
        <updated>2021-06-21T02:07:39.890Z</updated>
        <summary type="html"><![CDATA[Inductive rule learning is arguably among the most traditional paradigms in
machine learning. Although we have seen considerable progress over the years in
learning rule-based theories, all state-of-the-art learners still learn
descriptions that directly relate the input features to the target concept. In
the simplest case, concept learning, this is a disjunctive normal form (DNF)
description of the positive class. While it is clear that this is sufficient
from a logical point of view because every logical expression can be reduced to
an equivalent DNF expression, it could nevertheless be the case that more
structured representations, which form deep theories by forming intermediate
concepts, could be easier to learn, in very much the same way as deep neural
networks are able to outperform shallow networks, even though the latter are
also universal function approximators. In this paper, we empirically compare
deep and shallow rule learning with a uniform general algorithm, which relies
on greedy mini-batch based optimization. Our experiments on both artificial and
real-world benchmark data indicate that deep rule networks outperform shallow
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beck_F/0/1/0/all/0/1"&gt;Florian Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1"&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Riemannian Convex Potential Maps. (arXiv:2106.10272v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10272</id>
        <link href="http://arxiv.org/abs/2106.10272"/>
        <updated>2021-06-21T02:07:39.884Z</updated>
        <summary type="html"><![CDATA[Modeling distributions on Riemannian manifolds is a crucial component in
understanding non-Euclidean data that arises, e.g., in physics and geology. The
budding approaches in this space are limited by representational and
computational tradeoffs. We propose and study a class of flows that uses convex
potentials from Riemannian optimal transport. These are universal and can model
distributions on any compact Riemannian manifold without requiring domain
knowledge of the manifold to be integrated into the architecture. We
demonstrate that these flows can model standard distributions on spheres, and
tori, on synthetic and geological data. Our source code is freely available
online at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1"&gt;Samuel Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amos_B/0/1/0/all/0/1"&gt;Brandon Amos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1"&gt;Yaron Lipman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-view Molecule Pre-training. (arXiv:2106.10234v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2106.10234</id>
        <link href="http://arxiv.org/abs/2106.10234"/>
        <updated>2021-06-21T02:07:39.878Z</updated>
        <summary type="html"><![CDATA[Inspired by its success in natural language processing and computer vision,
pre-training has attracted substantial attention in cheminformatics and
bioinformatics, especially for molecule based tasks. A molecule can be
represented by either a graph (where atoms are connected by bonds) or a SMILES
sequence (where depth-first-search is applied to the molecular graph with
specific rules). Existing works on molecule pre-training use either graph
representations only or SMILES representations only. In this work, we propose
to leverage both the representations and design a new pre-training algorithm,
dual-view molecule pre-training (briefly, DMP), that can effectively combine
the strengths of both types of molecule representations. The model of DMP
consists of two branches: a Transformer branch that takes the SMILES sequence
of a molecule as input, and a GNN branch that takes a molecular graph as input.
The training of DMP contains three tasks: (1) predicting masked tokens in a
SMILES sequence by the Transformer branch, (2) predicting masked atoms in a
molecular graph by the GNN branch, and (3) maximizing the consistency between
the two high-level representations output by the Transformer and GNN branches
separately. After pre-training, we can use either the Transformer branch (this
one is recommended according to empirical results), the GNN branch, or both for
downstream tasks. DMP is tested on nine molecular property prediction tasks and
achieves state-of-the-art performances on seven of them. Furthermore, we test
DMP on three retrosynthesis tasks and achieve state-of-the-result on the
USPTO-full dataset. Our code will be released soon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jinhua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yingce Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wengang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deterministic Gibbs Sampling via Ordinary Differential Equations. (arXiv:2106.10188v1 [stat.CO])]]></title>
        <id>http://arxiv.org/abs/2106.10188</id>
        <link href="http://arxiv.org/abs/2106.10188"/>
        <updated>2021-06-21T02:07:39.870Z</updated>
        <summary type="html"><![CDATA[Deterministic dynamics is an essential part of many MCMC algorithms, e.g.
Hybrid Monte Carlo or samplers utilizing normalizing flows. This paper presents
a general construction of deterministic measure-preserving dynamics using
autonomous ODEs and tools from differential geometry. We show how Hybrid Monte
Carlo and other deterministic samplers follow as special cases of our theory.
We then demonstrate the utility of our approach by constructing a continuous
non-sequential version of Gibbs sampling in terms of an ODE flow and extending
it to discrete state spaces. We find that our deterministic samplers are more
sample efficient than stochastic counterparts, even if the latter generate
independent samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Neklyudov_K/0/1/0/all/0/1"&gt;Kirill Neklyudov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bondesan_R/0/1/0/all/0/1"&gt;Roberto Bondesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Deep Learning in Open Collaborations. (arXiv:2106.10207v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10207</id>
        <link href="http://arxiv.org/abs/2106.10207"/>
        <updated>2021-06-21T02:07:39.863Z</updated>
        <summary type="html"><![CDATA[Modern deep learning applications require increasingly more compute to train
state-of-the-art models. To address this demand, large corporations and
institutions use dedicated High-Performance Computing clusters, whose
construction and maintenance are both environmentally costly and well beyond
the budget of most organizations. As a result, some research directions become
the exclusive domain of a few large industrial and even fewer academic actors.
To alleviate this disparity, smaller groups may pool their computational
resources and run collaborative experiments that benefit all participants. This
paradigm, known as grid- or volunteer computing, has seen successful
applications in numerous scientific areas. However, using this approach for
machine learning is difficult due to high latency, asymmetric bandwidth, and
several challenges unique to volunteer computing. In this work, we carefully
analyze these constraints and propose a novel algorithmic framework designed
specifically for collaborative training. We demonstrate the effectiveness of
our approach for SwAV and ALBERT pretraining in realistic conditions and
achieve performance comparable to traditional setups at a fraction of the cost.
Finally, we provide a detailed report of successful collaborative language
model pretraining with 40 participants.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diskin_M/0/1/0/all/0/1"&gt;Michael Diskin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bukhtiyarov_A/0/1/0/all/0/1"&gt;Alexey Bukhtiyarov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1"&gt;Max Ryabinin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saulnier_L/0/1/0/all/0/1"&gt;Lucile Saulnier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lhoest_Q/0/1/0/all/0/1"&gt;Quentin Lhoest&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinitsin_A/0/1/0/all/0/1"&gt;Anton Sinitsin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popov_D/0/1/0/all/0/1"&gt;Dmitry Popov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pyrkin_D/0/1/0/all/0/1"&gt;Dmitry Pyrkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashirin_M/0/1/0/all/0/1"&gt;Maxim Kashirin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borzunov_A/0/1/0/all/0/1"&gt;Alexander Borzunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moral_A/0/1/0/all/0/1"&gt;Albert Villanova del Moral&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazur_D/0/1/0/all/0/1"&gt;Denis Mazur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobelev_I/0/1/0/all/0/1"&gt;Ilia Kobelev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1"&gt;Yacine Jernite&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1"&gt;Thomas Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pekhimenko_G/0/1/0/all/0/1"&gt;Gennady Pekhimenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees. (arXiv:2002.05551v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05551</id>
        <link href="http://arxiv.org/abs/2002.05551"/>
        <updated>2021-06-21T02:07:39.845Z</updated>
        <summary type="html"><![CDATA[Meta-learning can successfully acquire useful inductive biases from data.
Yet, its generalization properties to unseen learning tasks are poorly
understood. Particularly if the number of meta-training tasks is small, this
raises concerns about overfitting. We provide a theoretical analysis using the
PAC-Bayesian framework and derive novel generalization bounds for
meta-learning. Using these bounds, we develop a class of PAC-optimal
meta-learning algorithms with performance guarantees and a principled
meta-level regularization. Unlike previous PAC-Bayesian meta-learners, our
method results in a standard stochastic optimization problem which can be
solved efficiently and scales well. When instantiating our PAC-optimal
hyper-posterior (PACOH) with Gaussian processes and Bayesian Neural Networks as
base learners, the resulting methods yield state-of-the-art performance, both
in terms of predictive accuracy and the quality of uncertainty estimates.
Thanks to their principled treatment of uncertainty, our meta-learners can also
be successfully employed for sequential decision problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rothfuss_J/0/1/0/all/0/1"&gt;Jonas Rothfuss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Josifoski_M/0/1/0/all/0/1"&gt;Martin Josifoski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A learned conditional prior for the VAE acoustic space of a TTS system. (arXiv:2106.10229v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.10229</id>
        <link href="http://arxiv.org/abs/2106.10229"/>
        <updated>2021-06-21T02:07:39.838Z</updated>
        <summary type="html"><![CDATA[Many factors influence speech yielding different renditions of a given
sentence. Generative models, such as variational autoencoders (VAEs), capture
this variability and allow multiple renditions of the same sentence via
sampling. The degree of prosodic variability depends heavily on the prior that
is used when sampling. In this paper, we propose a novel method to compute an
informative prior for the VAE latent space of a neural text-to-speech (TTS)
system. By doing so, we aim to sample with more prosodic variability, while
gaining controllability over the latent space's structure.

By using as prior the posterior distribution of a secondary VAE, which we
condition on a speaker vector, we can sample from the primary VAE taking
explicitly the conditioning into account and resulting in samples from a
specific region of the latent space for each condition (i.e. speaker). A formal
preference test demonstrates significant preference of the proposed approach
over standard Conditional VAE. We also provide visualisations of the latent
space where well-separated condition-specific clusters appear, as well as
ablation studies to better understand the behaviour of the system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Karanasou_P/0/1/0/all/0/1"&gt;Penny Karanasou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karlapati_S/0/1/0/all/0/1"&gt;Sri Karlapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moinet_A/0/1/0/all/0/1"&gt;Alexis Moinet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Joly_A/0/1/0/all/0/1"&gt;Arnaud Joly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abbas_A/0/1/0/all/0/1"&gt;Ammar Abbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Slangen_S/0/1/0/all/0/1"&gt;Simon Slangen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Trueba_J/0/1/0/all/0/1"&gt;Jaime Lorenzo Trueba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Drugman_T/0/1/0/all/0/1"&gt;Thomas Drugman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Training Helps Transfer Learning via Better Representations. (arXiv:2106.10189v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10189</id>
        <link href="http://arxiv.org/abs/2106.10189"/>
        <updated>2021-06-21T02:07:39.830Z</updated>
        <summary type="html"><![CDATA[Transfer learning aims to leverage models pre-trained on source data to
efficiently adapt to target setting, where only limited data are available for
model fine-tuning. Recent works empirically demonstrate that adversarial
training in the source data can improve the ability of models to transfer to
new domains. However, why this happens is not known. In this paper, we provide
a theoretical model to rigorously analyze how adversarial training helps
transfer learning. We show that adversarial training in the source data
generates provably better representations, so fine-tuning on top of this
representation leads to a more accurate predictor of the target data. We
further demonstrate both theoretically and empirically that semi-supervised
learning in the source data can also improve transfer learning by similarly
improving the representation. Moreover, performing adversarial training on top
of semi-supervised learning can further improve transferability, suggesting
that the two approaches have complementary benefits on representations. We
support our theories with experiments on popular data sets and deep learning
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1"&gt;Zhun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Linjun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vodrahalli_K/0/1/0/all/0/1"&gt;Kailas Vodrahalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Black-Box Importance Sampling for VaR and CVaR Estimation. (arXiv:2106.10236v1 [q-fin.RM])]]></title>
        <id>http://arxiv.org/abs/2106.10236</id>
        <link href="http://arxiv.org/abs/2106.10236"/>
        <updated>2021-06-21T02:07:39.822Z</updated>
        <summary type="html"><![CDATA[This paper considers Importance Sampling (IS) for the estimation of tail
risks of a loss defined in terms of a sophisticated object such as a machine
learning feature map or a mixed integer linear optimisation formulation.
Assuming only black-box access to the loss and the distribution of the
underlying random vector, the paper presents an efficient IS algorithm for
estimating the Value at Risk and Conditional Value at Risk. The key challenge
in any IS procedure, namely, identifying an appropriate change-of-measure, is
automated with a self-structuring IS transformation that learns and replicates
the concentration properties of the conditional excess from less rare samples.
The resulting estimators enjoy asymptotically optimal variance reduction when
viewed in the logarithmic scale. Simulation experiments highlight the efficacy
and practicality of the proposed scheme]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Deo_A/0/1/0/all/0/1"&gt;Anand Deo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Murthy_K/0/1/0/all/0/1"&gt;Karthyek Murthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10199</id>
        <link href="http://arxiv.org/abs/2106.10199"/>
        <updated>2021-06-21T02:07:39.815Z</updated>
        <summary type="html"><![CDATA[We show that with small-to-medium training data, fine-tuning only the bias
terms (or a subset of the bias terms) of pre-trained BERT models is competitive
with (and sometimes better than) fine-tuning the entire model. For larger data,
bias-only fine-tuning is competitive with other sparse fine-tuning methods.
Besides their practical utility, these findings are relevant for the question
of understanding the commonly-used process of finetuning: they support the
hypothesis that finetuning is mainly about exposing knowledge induced by
language-modeling training, rather than learning new task-specific linguistic
knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zaken_E/0/1/0/all/0/1"&gt;Elad Ben Zaken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1"&gt;Shauli Ravfogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MADE: Exploration via Maximizing Deviation from Explored Regions. (arXiv:2106.10268v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10268</id>
        <link href="http://arxiv.org/abs/2106.10268"/>
        <updated>2021-06-21T02:07:39.809Z</updated>
        <summary type="html"><![CDATA[In online reinforcement learning (RL), efficient exploration remains
particularly challenging in high-dimensional environments with sparse rewards.
In low-dimensional environments, where tabular parameterization is possible,
count-based upper confidence bound (UCB) exploration methods achieve minimax
near-optimal rates. However, it remains unclear how to efficiently implement
UCB in realistic RL tasks that involve non-linear function approximation. To
address this, we propose a new exploration approach via \textit{maximizing} the
deviation of the occupancy of the next policy from the explored regions. We add
this term as an adaptive regularizer to the standard RL objective to balance
exploration vs. exploitation. We pair the new objective with a provably
convergent algorithm, giving rise to a new intrinsic reward that adjusts
existing bonuses. The proposed intrinsic reward is easy to implement and
combine with other existing RL algorithms to conduct exploration. As a proof of
concept, we evaluate the new intrinsic reward on tabular examples across a
variety of model-based and model-free algorithms, showing improvements over
count-only exploration strategies. When tested on navigation and locomotion
tasks from MiniGrid and DeepMind Control Suite benchmarks, our approach
significantly improves sample efficiency over state-of-the-art methods. Our
code is available at https://github.com/tianjunz/MADE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianjun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashidinejad_P/0/1/0/all/0/1"&gt;Paria Rashidinejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1"&gt;Jiantao Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuandong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1"&gt;Stuart Russell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition. (arXiv:2106.10169v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10169</id>
        <link href="http://arxiv.org/abs/2106.10169"/>
        <updated>2021-06-21T02:07:39.790Z</updated>
        <summary type="html"><![CDATA[By implicitly recognizing a user based on his/her speech input, speaker
identification enables many downstream applications, such as personalized
system behavior and expedited shopping checkouts. Based on whether the speech
content is constrained or not, both text-dependent (TD) and text-independent
(TI) speaker recognition models may be used. We wish to combine the advantages
of both types of models through an ensemble system to make more reliable
predictions. However, any such combined approach has to be robust to incomplete
inputs, i.e., when either TD or TI input is missing. As a solution we propose a
fusion of embeddings network foenet architecture, combining joint learning with
neural attention. We compare foenet with four competitive baseline methods on a
dataset of voice assistant inputs, and show that it achieves higher accuracy
than the baseline and score fusion methods, especially in the presence of
incomplete inputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruirui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1"&gt;Chelsea J.-T. Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zeya Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1"&gt;Hongda Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elibol_O/0/1/0/all/0/1"&gt;Oguz Elibol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1"&gt;Andreas Stolcke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Incremental Deep Graph Learning for Ethereum Phishing Scam Detection. (arXiv:2106.10176v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10176</id>
        <link href="http://arxiv.org/abs/2106.10176"/>
        <updated>2021-06-21T02:07:39.783Z</updated>
        <summary type="html"><![CDATA[In recent years, phishing scams have become the crime type with the largest
money involved on Ethereum, the second-largest blockchain platform. Meanwhile,
graph neural network (GNN) has shown promising performance in various node
classification tasks. However, for Ethereum transaction data, which could be
naturally abstracted to a real-world complex graph, the scarcity of labels and
the huge volume of transaction data make it difficult to take advantage of GNN
methods. Here in this paper, to address the two challenges, we propose a
Self-supervised Incremental deep Graph learning model (SIEGE), for the phishing
scam detection problem on Ethereum. In our model, two pretext tasks designed
from spatial and temporal perspectives help us effectively learn useful node
embedding from the huge amount of unlabelled transaction data. And the
incremental paradigm allows us to efficiently handle large-scale transaction
data and help the model maintain good performance when the data distribution is
drastically changing. We collect transaction records about half a year from
Ethereum and our extensive experiments show that our model consistently
outperforms strong baselines in both transductive and inductive settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shucheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1"&gt;Fengyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Runchuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1"&gt;Sheng Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Less is More: Feature Selection for Adversarial Robustness with Compressive Counter-Adversarial Attacks. (arXiv:2106.10252v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10252</id>
        <link href="http://arxiv.org/abs/2106.10252"/>
        <updated>2021-06-21T02:07:39.776Z</updated>
        <summary type="html"><![CDATA[A common observation regarding adversarial attacks is that they mostly give
rise to false activation at the penultimate layer to fool the classifier.
Assuming that these activation values correspond to certain features of the
input, the objective becomes choosing the features that are most useful for
classification. Hence, we propose a novel approach to identify the important
features by employing counter-adversarial attacks, which highlights the
consistency at the penultimate layer with respect to perturbations on input
samples. First, we empirically show that there exist a subset of features,
classification based in which bridge the gap between the clean and robust
accuracy. Second, we propose a simple yet efficient mechanism to identify those
features by searching the neighborhood of input sample. We then select features
by observing the consistency of the activation values at the penultimate layer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozfatura_E/0/1/0/all/0/1"&gt;Emre Ozfatura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hameed_M/0/1/0/all/0/1"&gt;Muhammad Zaid Hameed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozfatura_K/0/1/0/all/0/1"&gt;Kerem Ozfatura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Model Sparsification by Scheduled Grow-and-Prune Methods. (arXiv:2106.09857v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09857</id>
        <link href="http://arxiv.org/abs/2106.09857"/>
        <updated>2021-06-21T02:07:39.768Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are effective in solving many real-world
problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but
their excessive computation results in long training and inference time. Model
sparsification can reduce the computation and memory cost while maintaining
model quality. Most existing sparsification algorithms unidirectionally remove
weights, while others randomly or greedily explore a small subset of weights in
each layer. The inefficiency of the algorithms reduces the achievable sparsity
level. In addition, many algorithms still require pre-trained dense models and
thus suffer from large memory footprint and long training time. In this paper,
we propose a novel scheduled grow-and-prune (GaP) methodology without
pre-training the dense models. It addresses the shortcomings of the previous
works by repeatedly growing a subset of layers to dense and then pruning back
to sparse after some training. Experiments have shown that such models can
match or beat the quality of highly optimized dense models at 80% sparsity on a
variety of tasks, such as image classification, objective detection, 3D object
part segmentation, and translation. They also outperform other state-of-the-art
(SOTA) pruning methods, including pruning from pre-trained dense models. As an
example, a 90% sparse ResNet-50 obtained via GaP achieves 77.9% top-1 accuracy
on ImageNet, improving the SOTA results by 1.5%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1"&gt;Minghai Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1"&gt;Fei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1"&gt;Zejiang Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Kun Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Kuang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Rong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuan Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHARP: Shape-Aware Reconstruction of People In Loose Clothing. (arXiv:2106.04778v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04778</id>
        <link href="http://arxiv.org/abs/2106.04778"/>
        <updated>2021-06-21T02:07:39.749Z</updated>
        <summary type="html"><![CDATA[3D human body reconstruction from monocular images is an interesting and
ill-posed problem in computer vision with wider applications in multiple
domains. In this paper, we propose SHARP, a novel end-to-end trainable network
that accurately recovers the detailed geometry and appearance of 3D people in
loose clothing from a monocular image. We propose a sparse and efficient fusion
of a parametric body prior with a non-parametric peeled depth map
representation of clothed models. The parametric body prior constraints our
model in two ways: first, the network retains geometrically consistent body
parts that are not occluded by clothing, and second, it provides a body shape
context that improves prediction of the peeled depth maps. This enables SHARP
to recover fine-grained 3D geometrical details with just L1 losses on the 2D
maps, given an input image. We evaluate SHARP on publicly available Cloth3D and
THuman datasets and report superior performance to state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jinka_S/0/1/0/all/0/1"&gt;Sai Sagar Jinka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chacko_R/0/1/0/all/0/1"&gt;Rohan Chacko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Astitva Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Avinash Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1"&gt;P.J. Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Robustness Propagation: Sharing Adversarial Robustness in Federated Learning. (arXiv:2106.10196v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10196</id>
        <link href="http://arxiv.org/abs/2106.10196"/>
        <updated>2021-06-21T02:07:39.730Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) emerges as a popular distributed learning schema that
learns a model from a set of participating users without requiring raw data to
be shared. One major challenge of FL comes from heterogeneity in users, which
may have distributionally different (or non-iid) data and varying computation
resources. Just like in centralized learning, FL users also desire model
robustness against malicious attackers at test time. Whereas adversarial
training (AT) provides a sound solution for centralized learning, extending its
usage for FL users has imposed significant challenges, as many users may have
very limited training data as well as tight computational budgets, to afford
the data-hungry and costly AT. In this paper, we study a novel learning setting
that propagates adversarial robustness from high-resource users that can afford
AT, to those low-resource users that cannot afford it, during the FL process.
We show that existing FL techniques cannot effectively propagate adversarial
robustness among non-iid users, and propose a simple yet effective propagation
approach that transfers robustness through carefully designed
batch-normalization statistics. We demonstrate the rationality and
effectiveness of our method through extensive experiments. Especially, the
proposed method is shown to grant FL remarkable robustness even when only a
small portion of users afford AT during learning. Codes will be published upon
acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1"&gt;Junyuan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiayu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Investigation into Mini-Batch Rule Learning. (arXiv:2106.10202v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10202</id>
        <link href="http://arxiv.org/abs/2106.10202"/>
        <updated>2021-06-21T02:07:39.723Z</updated>
        <summary type="html"><![CDATA[We investigate whether it is possible to learn rule sets efficiently in a
network structure with a single hidden layer using iterative refinements over
mini-batches of examples. A first rudimentary version shows an acceptable
performance on all but one dataset, even though it does not yet reach the
performance levels of Ripper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beck_F/0/1/0/all/0/1"&gt;Florian Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1"&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[pyWATTS: Python Workflow Automation Tool for Time Series. (arXiv:2106.10157v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10157</id>
        <link href="http://arxiv.org/abs/2106.10157"/>
        <updated>2021-06-21T02:07:39.716Z</updated>
        <summary type="html"><![CDATA[Time series data are fundamental for a variety of applications, ranging from
financial markets to energy systems. Due to their importance, the number and
complexity of tools and methods used for time series analysis is constantly
increasing. However, due to unclear APIs and a lack of documentation,
researchers struggle to integrate them into their research projects and
replicate results. Additionally, in time series analysis there exist many
repetitive tasks, which are often re-implemented for each project,
unnecessarily costing time. To solve these problems we present
\texttt{pyWATTS}, an open-source Python-based package that is a non-sequential
workflow automation tool for the analysis of time series data. pyWATTS includes
modules with clearly defined interfaces to enable seamless integration of new
or existing methods, subpipelining to easily reproduce repetitive tasks, load
and save functionality to simply replicate results, and native support for key
Python machine learning libraries such as scikit-learn, PyTorch, and Keras.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidrich_B/0/1/0/all/0/1"&gt;Benedikt Heidrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartschat_A/0/1/0/all/0/1"&gt;Andreas Bartschat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turowski_M/0/1/0/all/0/1"&gt;Marian Turowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_O/0/1/0/all/0/1"&gt;Oliver Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phipps_K/0/1/0/all/0/1"&gt;Kaleb Phipps&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meisenbacher_S/0/1/0/all/0/1"&gt;Stefan Meisenbacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmieder_K/0/1/0/all/0/1"&gt;Kai Schmieder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludwig_N/0/1/0/all/0/1"&gt;Nicole Ludwig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikut_R/0/1/0/all/0/1"&gt;Ralf Mikut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagenmeyer_V/0/1/0/all/0/1"&gt;Veit Hagenmeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in Deep Neural Networks. (arXiv:2106.10147v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.10147</id>
        <link href="http://arxiv.org/abs/2106.10147"/>
        <updated>2021-06-21T02:07:39.710Z</updated>
        <summary type="html"><![CDATA[Trigger set-based watermarking schemes have gained emerging attention as they
provide a means to prove ownership for deep neural network model owners. In
this paper, we argue that state-of-the-art trigger set-based watermarking
algorithms do not achieve their designed goal of proving ownership. We posit
that this impaired capability stems from two common experimental flaws that the
existing research practice has committed when evaluating the robustness of
watermarking algorithms: (1) incomplete adversarial evaluation and (2)
overlooked adaptive attacks.

We conduct a comprehensive adversarial evaluation of 10 representative
watermarking schemes against six of the existing attacks and demonstrate that
each of these watermarking schemes lacks robustness against at least two
attacks. We also propose novel adaptive attacks that harness the adversary's
knowledge of the underlying watermarking algorithm of a target model. We
demonstrate that the proposed attacks effectively break all of the 10
watermarking schemes, consequently allowing adversaries to obscure the
ownership of any watermarked model. We encourage follow-up studies to consider
our guidelines when evaluating the robustness of their watermarking schemes via
conducting comprehensive adversarial evaluation that include our adaptive
attacks to demonstrate a meaningful upper bound of watermark robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Suyoung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1"&gt;Wonho Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1"&gt;Suman Jana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1"&gt;Meeyoung Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1"&gt;Sooel Son&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistent Posterior Distributions under Vessel-Mixing: A Regularization for Cross-Domain Retinal Artery/Vein Classification. (arXiv:2103.09097v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09097</id>
        <link href="http://arxiv.org/abs/2103.09097"/>
        <updated>2021-06-21T02:07:39.678Z</updated>
        <summary type="html"><![CDATA[Retinal artery/vein (A/V) classification is a critical technique for
diagnosing diabetes and cardiovascular diseases. Although deep learning based
methods achieve impressive results in A/V classification, their performances
usually degrade severely when being directly applied to another database, due
to the domain shift, e.g., caused by the variations in imaging protocols. In
this paper, we propose a novel vessel-mixing based consistency regularization
framework, for cross-domain learning in retinal A/V classification. Specially,
to alleviate the severe bias to source domain, based on the label smooth prior,
the model is regularized to give consistent predictions for unlabeled
target-domain inputs that are under perturbation. This consistency
regularization implicitly introduces a mechanism where the model and the
perturbation is opponent to each other, where the model is pushed to be robust
enough to cope with the perturbation. Thus, we investigate a more difficult
opponent to further inspire the robustness of model, in the scenario of retinal
A/V, called vessel-mixing perturbation. Specially, it effectively disturbs the
fundus images especially the vessel structures by mixing two images regionally.
We conduct extensive experiments on cross-domain A/V classification using four
public datasets, which are collected by diverse institutions and imaging
devices. The results demonstrate that our method achieves the state-of-the-art
cross-domain performance, which is also close to the upper bound obtained by
fully supervised learning on target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunlong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zhehan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wenao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xinghao Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Offline Policy Selection. (arXiv:2106.10251v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10251</id>
        <link href="http://arxiv.org/abs/2106.10251"/>
        <updated>2021-06-21T02:07:39.660Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of policy selection in domains with abundant
logged data, but with a very restricted interaction budget. Solving this
problem would enable safe evaluation and deployment of offline reinforcement
learning policies in industry, robotics, and healthcare domain among others.
Several off-policy evaluation (OPE) techniques have been proposed to assess the
value of policies using only logged data. However, there is still a big gap
between the evaluation by OPE and the full online evaluation in the real
environment. To reduce this gap, we introduce a novel \emph{active offline
policy selection} problem formulation, which combined logged data and limited
online interactions to identify the best policy. We rely on the advances in OPE
to warm start the evaluation. We build upon Bayesian optimization to
iteratively decide which policies to evaluate in order to utilize the limited
environment interactions wisely. Many candidate policies could be proposed,
thus, we focus on making our approach scalable and introduce a kernel function
to model similarity between policies. We use several benchmark environments to
show that the proposed approach improves upon state-of-the-art OPE estimates
and fully online policy evaluation with limited budget. Additionally, we show
that each component of the proposed method is important, it works well with
various number and quality of OPE estimates and even with a large number of
candidate policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Konyushkova_K/0/1/0/all/0/1"&gt;Ksenia Konyushkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yutian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paine_T/0/1/0/all/0/1"&gt;Thomas Paine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1"&gt;Caglar Gulcehre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paduraru_C/0/1/0/all/0/1"&gt;Cosmin Paduraru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mankowitz_D/0/1/0/all/0/1"&gt;Daniel J Mankowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denil_M/0/1/0/all/0/1"&gt;Misha Denil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitas_N/0/1/0/all/0/1"&gt;Nando de Freitas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CompositeTasking: Understanding Images by Spatial Composition of Tasks. (arXiv:2012.09030v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09030</id>
        <link href="http://arxiv.org/abs/2012.09030"/>
        <updated>2021-06-21T02:07:39.653Z</updated>
        <summary type="html"><![CDATA[We define the concept of CompositeTasking as the fusion of multiple,
spatially distributed tasks, for various aspects of image understanding.
Learning to perform spatially distributed tasks is motivated by the frequent
availability of only sparse labels across tasks, and the desire for a compact
multi-tasking network. To facilitate CompositeTasking, we introduce a novel
task conditioning model -- a single encoder-decoder network that performs
multiple, spatially varying tasks at once. The proposed network takes an image
and a set of pixel-wise dense task requests as inputs, and performs the
requested prediction task for each pixel. Moreover, we also learn the
composition of tasks that needs to be performed according to some
CompositeTasking rules, which includes the decision of where to apply which
task. It not only offers us a compact network for multi-tasking, but also
allows for task-editing. Another strength of the proposed method is
demonstrated by only having to supply sparse supervision per task. The obtained
results are on par with our baselines that use dense supervision and a
multi-headed multi-tasking design. The source code will be made publicly
available at www.github.com/nikola3794/composite-tasking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1"&gt;Nikola Popovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1"&gt;Danda Pani Paudel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1"&gt;Thomas Probst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1"&gt;Guolei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid graph convolutional neural networks for landmark-based anatomical segmentation. (arXiv:2106.09832v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09832</id>
        <link href="http://arxiv.org/abs/2106.09832"/>
        <updated>2021-06-21T02:07:39.646Z</updated>
        <summary type="html"><![CDATA[In this work we address the problem of landmark-based segmentation for
anatomical structures. We propose HybridGNet, an encoder-decoder neural
architecture which combines standard convolutions for image feature encoding,
with graph convolutional neural networks to decode plausible representations of
anatomical structures. We benchmark the proposed architecture considering other
standard landmark and pixel-based models for anatomical segmentation in chest
x-ray images, and found that HybridGNet is more robust to image occlusions. We
also show that it can be used to construct landmark-based segmentations from
pixel level annotations. Our experimental results suggest that HybridGNet
produces accurate and anatomically plausible landmark-based segmentations, by
naturally incorporating shape constraints within the decoding process via
spectral convolutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gaggion_N/0/1/0/all/0/1"&gt;Nicol&amp;#xe1;s Gaggion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mansilla_L/0/1/0/all/0/1"&gt;Lucas Mansilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Milone_D/0/1/0/all/0/1"&gt;Diego Milone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1"&gt;Enzo Ferrante&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Being a Bit Frequentist Improves Bayesian Neural Networks. (arXiv:2106.10065v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10065</id>
        <link href="http://arxiv.org/abs/2106.10065"/>
        <updated>2021-06-21T02:07:39.628Z</updated>
        <summary type="html"><![CDATA[Despite their compelling theoretical properties, Bayesian neural networks
(BNNs) tend to perform worse than frequentist methods in classification-based
uncertainty quantification (UQ) tasks such as out-of-distribution (OOD)
detection and dataset-shift robustness. In this work, based on empirical
findings in prior works, we hypothesize that this issue is due to the avoidance
of Bayesian methods in the so-called "OOD training" -- a family of techniques
for incorporating OOD data during training process, which has since been an
integral part of state-of-the-art frequentist UQ methods. To validate this, we
treat OOD data as a first-class citizen in BNN training by exploring four
different ways of incorporating OOD data in Bayesian inference. We show in
extensive experiments that OOD-trained BNNs are competitive to, if not better
than recent frequentist baselines. This work thus provides strong baselines for
future work in both Bayesian and frequentist UQ.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kristiadi_A/0/1/0/all/0/1"&gt;Agustinus Kristiadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1"&gt;Matthias Hein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1"&gt;Philipp Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Problem Dependent View on Structured Thresholding Bandit Problems. (arXiv:2106.10166v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10166</id>
        <link href="http://arxiv.org/abs/2106.10166"/>
        <updated>2021-06-21T02:07:39.620Z</updated>
        <summary type="html"><![CDATA[We investigate the problem dependent regime in the stochastic Thresholding
Bandit problem (TBP) under several shape constraints. In the TBP, the objective
of the learner is to output, at the end of a sequential game, the set of arms
whose means are above a given threshold. The vanilla, unstructured, case is
already well studied in the literature. Taking $K$ as the number of arms, we
consider the case where (i) the sequence of arm's means $(\mu_k)_{k=1}^K$ is
monotonically increasing (MTBP) and (ii) the case where $(\mu_k)_{k=1}^K$ is
concave (CTBP). We consider both cases in the problem dependent regime and
study the probability of error - i.e. the probability to mis-classify at least
one arm. In the fixed budget setting, we provide upper and lower bounds for the
probability of error in both the concave and monotone settings, as well as
associated algorithms. In both settings the bounds match in the problem
dependent regime up to universal constants in the exponential.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cheshire_J/0/1/0/all/0/1"&gt;James Cheshire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Menard_P/0/1/0/all/0/1"&gt;Pierre M&amp;#xe9;nard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Carpentier_A/0/1/0/all/0/1"&gt;Alexandra Carpentier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rational Shapley Values. (arXiv:2106.10191v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10191</id>
        <link href="http://arxiv.org/abs/2106.10191"/>
        <updated>2021-06-21T02:07:39.612Z</updated>
        <summary type="html"><![CDATA[Explaining the predictions of opaque machine learning algorithms is an
important and challenging task, especially as complex models are increasingly
used to assist in high-stakes decisions such as those arising in healthcare and
finance. Most popular tools for post-hoc explainable artificial intelligence
(XAI) are either insensitive to context (e.g., feature attributions) or
difficult to summarize (e.g., counterfactuals). In this paper, I introduce
\emph{rational Shapley values}, a novel XAI method that synthesizes and extends
these seemingly incompatible approaches in a rigorous, flexible manner. I
leverage tools from decision theory and causal modeling to formalize and
implement a pragmatic approach that resolves a number of known challenges in
XAI. By pairing the distribution of random variables with the appropriate
reference class for a given explanation task, I illustrate through theory and
experiments how user goals and knowledge can inform and constrain the solution
set in an iterative fashion. The method compares favorably to state of the art
XAI tools in a range of quantitative and qualitative comparisons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watson_D/0/1/0/all/0/1"&gt;David S. Watson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Principles of Deep Learning Theory. (arXiv:2106.10165v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10165</id>
        <link href="http://arxiv.org/abs/2106.10165"/>
        <updated>2021-06-21T02:07:39.603Z</updated>
        <summary type="html"><![CDATA[This book develops an effective theory approach to understanding deep neural
networks of practical relevance. Beginning from a first-principles
component-level picture of networks, we explain how to determine an accurate
description of the output of trained networks by solving layer-to-layer
iteration equations and nonlinear learning dynamics. A main result is that the
predictions of networks are described by nearly-Gaussian distributions, with
the depth-to-width aspect ratio of the network controlling the deviations from
the infinite-width Gaussian description. We explain how these effectively-deep
networks learn nontrivial representations from training and more broadly
analyze the mechanism of representation learning for nonlinear models. From a
nearly-kernel-methods perspective, we find that the dependence of such models'
predictions on the underlying learning algorithm can be expressed in a simple
and universal way. To obtain these results, we develop the notion of
representation group flow (RG flow) to characterize the propagation of signals
through the network. By tuning networks to criticality, we give a practical
solution to the exploding and vanishing gradient problem. We further explain
how RG flow leads to near-universal behavior and lets us categorize networks
built from different activation functions into universality classes.
Altogether, we show that the depth-to-width ratio governs the effective model
complexity of the ensemble of trained networks. By using information-theoretic
techniques, we estimate the optimal aspect ratio at which we expect the network
to be practically most useful and show how residual connections can be used to
push this scale to arbitrary depths. With these tools, we can learn in detail
about the inductive bias of architectures, hyperparameters, and optimizers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1"&gt;Daniel A. Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yaida_S/0/1/0/all/0/1"&gt;Sho Yaida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanin_B/0/1/0/all/0/1"&gt;Boris Hanin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Sample Complexity of Batch Reinforcement Learning with Policy-Induced Data. (arXiv:2106.09973v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09973</id>
        <link href="http://arxiv.org/abs/2106.09973"/>
        <updated>2021-06-21T02:07:39.596Z</updated>
        <summary type="html"><![CDATA[We study the fundamental question of the sample complexity of learning a good
policy in finite Markov decision processes (MDPs) when the data available for
learning is obtained by following a logging policy that must be chosen without
knowledge of the underlying MDP. Our main results show that the sample
complexity, the minimum number of transitions necessary and sufficient to
obtain a good policy, is an exponential function of the relevant quantities
when the planning horizon $H$ is finite. In particular, we prove that the
sample complexity of obtaining $\epsilon$-optimal policies is at least
$\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H+1)})$ for $\gamma$-discounted
problems, where $\mathrm{S}$ is the number of states, $\mathrm{A}$ is the
number of actions, and $H$ is the effective horizon defined as $H=\lfloor
\tfrac{\ln(1/\epsilon)}{\ln(1/\gamma)} \rfloor$; and it is at least
$\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H)}/\varepsilon^2)$ for finite horizon
problems, where $H$ is the planning horizon of the problem. This lower bound is
essentially matched by an upper bound. For the average-reward setting we show
that there is no algorithm finding $\epsilon$-optimal policies with a finite
amount of data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chenjun Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1"&gt;Ilbin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Bo Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1"&gt;Dale Schuurmans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesvari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Labelling Drifts in a Fault Detection System for Wind Turbine Maintenance. (arXiv:2106.09951v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09951</id>
        <link href="http://arxiv.org/abs/2106.09951"/>
        <updated>2021-06-21T02:07:39.589Z</updated>
        <summary type="html"><![CDATA[A failure detection system is the first step towards predictive maintenance
strategies. A popular data-driven method to detect incipient failures and
anomalies is the training of normal behaviour models by applying a machine
learning technique like feed-forward neural networks (FFNN) or extreme learning
machines (ELM). However, the performance of any of these modelling techniques
can be deteriorated by the unexpected rise of non-stationarities in the dynamic
environment in which industrial assets operate. This unpredictable statistical
change in the measured variable is known as concept drift. In this article a
wind turbine maintenance case is presented, where non-stationarities of various
kinds can happen unexpectedly. Such concept drift events are desired to be
detected by means of statistical detectors and window-based approaches.
However, in real complex systems, concept drifts are not as clear and evident
as in artificially generated datasets. In order to evaluate the effectiveness
of current drift detectors and also to design an appropriate novel technique
for this specific industrial application, it is essential to dispose beforehand
of a characterization of the existent drifts. Under the lack of information in
this regard, a methodology for labelling concept drift events in the lifetime
of wind turbines is proposed. This methodology will facilitate the creation of
a drift database that will serve both as a training ground for concept drift
detectors and as a valuable information to enhance the knowledge about
maintenance of complex systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_I/0/1/0/all/0/1"&gt;I&amp;#xf1;igo Martinez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viles_E/0/1/0/all/0/1"&gt;Elisabeth Viles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cabrejas_I/0/1/0/all/0/1"&gt;I&amp;#xf1;aki Cabrejas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Semantic Segmentation Augmented with Image-Level Weak Annotations. (arXiv:2007.01496v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.01496</id>
        <link href="http://arxiv.org/abs/2007.01496"/>
        <updated>2021-06-21T02:07:39.582Z</updated>
        <summary type="html"><![CDATA[Despite the great progress made by deep neural networks in the semantic
segmentation task, traditional neural-networkbased methods typically suffer
from a shortage of large amounts of pixel-level annotations. Recent progress in
fewshot semantic segmentation tackles the issue by only a few pixel-level
annotated examples. However, these few-shot approaches cannot easily be applied
to multi-way or weak annotation settings. In this paper, we advance the
few-shot segmentation paradigm towards a scenario where image-level annotations
are available to help the training process of a few pixel-level annotations.
Our key idea is to learn a better prototype representation of the class by
fusing the knowledge from the image-level labeled data. Specifically, we
propose a new framework, called PAIA, to learn the class prototype
representation in a metric space by integrating image-level annotations.
Furthermore, by considering the uncertainty of pseudo-masks, a distilled soft
masked average pooling strategy is designed to handle distractions in
image-level annotations. Extensive empirical results on two datasets show
superior performance of PAIA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1"&gt;Shuo Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianfeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fanglan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chang-Tien Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Context Encoder: Graph Feature Inpainting for Graph Generation and Self-supervised Pretraining. (arXiv:2106.10124v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10124</id>
        <link href="http://arxiv.org/abs/2106.10124"/>
        <updated>2021-06-21T02:07:39.575Z</updated>
        <summary type="html"><![CDATA[We propose the Graph Context Encoder (GCE), a simple but efficient approach
for graph representation learning based on graph feature masking and
reconstruction.

GCE models are trained to efficiently reconstruct input graphs similarly to a
graph autoencoder where node and edge labels are masked. In particular, our
model is also allowed to change graph structures by masking and reconstructing
graphs augmented by random pseudo-edges.

We show that GCE can be used for novel graph generation, with applications
for molecule generation. Used as a pretraining method, we also show that GCE
improves baseline performances in supervised classification tasks tested on
multiple standard benchmark graph datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frigo_O/0/1/0/all/0/1"&gt;Oriel Frigo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Brossard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1"&gt;David Dehaene&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Plan via a Multi-Step Policy Regression Method. (arXiv:2106.10075v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10075</id>
        <link href="http://arxiv.org/abs/2106.10075"/>
        <updated>2021-06-21T02:07:39.547Z</updated>
        <summary type="html"><![CDATA[We propose a new approach to increase inference performance in environments
that require a specific sequence of actions in order to be solved. This is for
example the case for maze environments where ideally an optimal path is
determined. Instead of learning a policy for a single step, we want to learn a
policy that can predict n actions in advance. Our proposed method called policy
horizon regression (PHR) uses knowledge of the environment sampled by A2C to
learn an n dimensional policy vector in a policy distillation setup which
yields n sequential actions per observation. We test our method on the MiniGrid
and Pong environments and show drastic speedup during inference time by
successfully predicting sequences of actions on a single observation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1"&gt;Stefan Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janschek_M/0/1/0/all/0/1"&gt;Michael Janschek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uelwer_T/0/1/0/all/0/1"&gt;Tobias Uelwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harmeling_S/0/1/0/all/0/1"&gt;Stefan Harmeling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples. (arXiv:2106.09947v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09947</id>
        <link href="http://arxiv.org/abs/2106.09947"/>
        <updated>2021-06-21T02:07:39.539Z</updated>
        <summary type="html"><![CDATA[Evaluating robustness of machine-learning models to adversarial examples is a
challenging problem. Many defenses have been shown to provide a false sense of
security by causing gradient-based attacks to fail, and they have been broken
under more rigorous evaluations. Although guidelines and best practices have
been suggested to improve current adversarial robustness evaluations, the lack
of automatic testing and debugging tools makes it difficult to apply these
recommendations in a systematic manner. In this work, we overcome these
limitations by (i) defining a set of quantitative indicators which unveil
common failures in the optimization of gradient-based attacks, and (ii)
proposing specific mitigation strategies within a systematic evaluation
protocol. Our extensive experimental analysis shows that the proposed
indicators of failure can be used to visualize, debug and improve current
adversarial robustness evaluations, providing a first concrete step towards
automatizing and systematizing current adversarial robustness evaluations. Our
open-source code is available at:
https://github.com/pralab/IndicatorsOfAttackFailure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1"&gt;Maura Pintor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demetrio_L/0/1/0/all/0/1"&gt;Luca Demetrio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotgiu_A/0/1/0/all/0/1"&gt;Angelo Sotgiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manca_G/0/1/0/all/0/1"&gt;Giovanni Manca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1"&gt;Ambra Demontis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1"&gt;Battista Biggio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1"&gt;Fabio Roli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Dimpled Manifold Model of Adversarial Examples in Machine Learning. (arXiv:2106.10151v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10151</id>
        <link href="http://arxiv.org/abs/2106.10151"/>
        <updated>2021-06-21T02:07:39.525Z</updated>
        <summary type="html"><![CDATA[The extreme fragility of deep neural networks when presented with tiny
perturbations in their inputs was independently discovered by several research
groups in 2013, but in spite of enormous effort these adversarial examples
remained a baffling phenomenon with no clear explanation. In this paper we
introduce a new conceptual framework (which we call the Dimpled Manifold Model)
which provides a simple explanation for why adversarial examples exist, why
their perturbations have such tiny norms, why these perturbations look like
random noise, and why a network which was adversarially trained with
incorrectly labeled images can still correctly classify test images. In the
last part of the paper we describe the results of numerous experiments which
strongly support this new model, and in particular our assertion that
adversarial perturbations are roughly perpendicular to the low dimensional
manifold which contains all the training examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1"&gt;Adi Shamir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melamed_O/0/1/0/all/0/1"&gt;Odelia Melamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+BenShmuel_O/0/1/0/all/0/1"&gt;Oriel BenShmuel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Vertical Federated Learning Framework for Horizontally Partitioned Labels. (arXiv:2106.10056v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10056</id>
        <link href="http://arxiv.org/abs/2106.10056"/>
        <updated>2021-06-21T02:07:39.510Z</updated>
        <summary type="html"><![CDATA[Vertical federated learning is a collaborative machine learning framework to
train deep leaning models on vertically partitioned data with
privacy-preservation. It attracts much attention both from academia and
industry. Unfortunately, applying most existing vertical federated learning
methods in real-world applications still faces two daunting challenges. First,
most existing vertical federated learning methods have a strong assumption that
at least one party holds the complete set of labels of all data samples, while
this assumption is not satisfied in many practical scenarios, where labels are
horizontally partitioned and the parties only hold partial labels. Existing
vertical federated learning methods can only utilize partial labels, which may
lead to inadequate model update in end-to-end backpropagation. Second,
computational and communication resources vary in parties. Some parties with
limited computational and communication resources will become the stragglers
and slow down the convergence of training. Such straggler problem will be
exaggerated in the scenarios of horizontally partitioned labels in vertical
federated learning. To address these challenges, we propose a novel vertical
federated learning framework named Cascade Vertical Federated Learning (CVFL)
to fully utilize all horizontally partitioned labels to train neural networks
with privacy-preservation. To mitigate the straggler problem, we design a novel
optimization objective which can increase straggler's contribution to the
trained models. We conduct a series of qualitative experiments to rigorously
verify the effectiveness of CVFL. It is demonstrated that CVFL can achieve
comparable performance (e.g., accuracy for classification tasks) with
centralized training. The new optimization objective can further mitigate the
straggler problem comparing with only using the asynchronous aggregation
mechanism during training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1"&gt;Wensheng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Ying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhonghai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiaoyong Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Differentially Private Federated Learning: Efficient Algorithms with Tight Risk Bounds. (arXiv:2106.09779v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09779</id>
        <link href="http://arxiv.org/abs/2106.09779"/>
        <updated>2021-06-21T02:07:39.480Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a distributed learning paradigm in which many
clients with heterogeneous, unbalanced, and often sensitive local data,
collaborate to learn a model. Local Differential Privacy (LDP) provides a
strong guarantee that each client's data cannot be leaked during and after
training, without relying on a trusted third party. While LDP is often believed
to be too stringent to allow for satisfactory utility, our paper challenges
this belief. We consider a general setup with unbalanced, heterogeneous data,
disparate privacy needs across clients, and unreliable communication, where a
random number/subset of clients is available each round. We propose three LDP
algorithms for smooth (strongly) convex FL; each are noisy variations of
distributed minibatch SGD. One is accelerated and one involves novel
time-varying noise, which we use to obtain the first non-trivial LDP excess
risk bound for the fully general non-i.i.d. FL problem. Specializing to i.i.d.
clients, our risk bounds interpolate between the best known and/or optimal
bounds in the centralized setting and the cross-device setting, where each
client represents just one person's data. Furthermore, we show that in certain
regimes, our convergence rate (nearly) matches the corresponding non-private
lower bound or outperforms state of the art non-private algorithms (``privacy
for free''). Finally, we validate our theoretical results and illustrate the
practical utility of our algorithm with numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lowy_A/0/1/0/all/0/1"&gt;Andrew Lowy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1"&gt;Meisam Razaviyayn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Steerable Partial Differential Operators for Equivariant Neural Networks. (arXiv:2106.10163v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10163</id>
        <link href="http://arxiv.org/abs/2106.10163"/>
        <updated>2021-06-21T02:07:39.473Z</updated>
        <summary type="html"><![CDATA[Recent work in equivariant deep learning bears strong similarities to
physics. Fields over a base space are fundamental entities in both subjects, as
are equivariant maps between these fields. In deep learning, however, these
maps are usually defined by convolutions with a kernel, whereas they are
partial differential operators (PDOs) in physics. Developing the theory of
equivariant PDOs in the context of deep learning could bring these subjects
even closer together and lead to a stronger flow of ideas. In this work, we
derive a $G$-steerability constraint that completely characterizes when a PDO
between feature vector fields is equivariant, for arbitrary symmetry groups
$G$. We then fully solve this constraint for several important groups. We use
our solutions as equivariant drop-in replacements for convolutional layers and
benchmark them in that role. Finally, we develop a framework for equivariant
maps based on Schwartz distributions that unifies classical convolutions and
differential operators and gives insight about the relation between the two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jenner_E/0/1/0/all/0/1"&gt;Erik Jenner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1"&gt;Maurice Weiler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning Models Predict Visual Responses in the Brain: A Preliminary Result. (arXiv:2106.10112v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10112</id>
        <link href="http://arxiv.org/abs/2106.10112"/>
        <updated>2021-06-21T02:07:39.466Z</updated>
        <summary type="html"><![CDATA[Supervised deep convolutional neural networks (DCNNs) are currently one of
the best computational models that can explain how the primate ventral visual
stream solves object recognition. However, embodied cognition has not been
considered in the existing visual processing models. From the ecological
standpoint, humans learn to recognize objects by interacting with them,
allowing better classification, specialization, and generalization. Here, we
ask if computational models under the embodied learning framework can explain
mechanisms underlying object recognition in the primate visual system better
than the existing supervised models? To address this question, we use
reinforcement learning to train neural network models to play a 3D computer
game and we find that these reinforcement learning models achieve neural
response prediction accuracy scores in the early visual areas (e.g., V1 and V2)
in the levels that are comparable to those accomplished by the supervised
neural network model. In contrast, the supervised neural network models yield
better neural response predictions in the higher visual areas, compared to the
reinforcement learning models. Our preliminary results suggest the future
direction of visual neuroscience in which deep reinforcement learning should be
included to fill the missing embodiment concept.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Piriyajitakonkij_M/0/1/0/all/0/1"&gt;Maytus Piriyajitakonkij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Itthipuripat_S/0/1/0/all/0/1"&gt;Sirawaj Itthipuripat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1"&gt;Theerawit Wilaiprasitporn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilokthanakul_N/0/1/0/all/0/1"&gt;Nat Dilokthanakul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothed Multi-View Subspace Clustering. (arXiv:2106.09875v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09875</id>
        <link href="http://arxiv.org/abs/2106.09875"/>
        <updated>2021-06-21T02:07:39.460Z</updated>
        <summary type="html"><![CDATA[In recent years, multi-view subspace clustering has achieved impressive
performance due to the exploitation of complementary imformation across
multiple views. However, multi-view data can be very complicated and are not
easy to cluster in real-world applications. Most existing methods operate on
raw data and may not obtain the optimal solution. In this work, we propose a
novel multi-view clustering method named smoothed multi-view subspace
clustering (SMVSC) by employing a novel technique, i.e., graph filtering, to
obtain a smooth representation for each view, in which similar data points have
similar feature values. Specifically, it retains the graph geometric features
through applying a low-pass filter. Consequently, it produces a
``clustering-friendly" representation and greatly facilitates the downstream
clustering task. Extensive experiments on benchmark datasets validate the
superiority of our approach. Analysis shows that graph filtering increases the
separability of classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Peng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhengrui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1"&gt;Zhao Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ScoreGrad: Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models. (arXiv:2106.10121v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10121</id>
        <link href="http://arxiv.org/abs/2106.10121"/>
        <updated>2021-06-21T02:07:39.445Z</updated>
        <summary type="html"><![CDATA[Multivariate time series prediction has attracted a lot of attention because
of its wide applications such as intelligence transportation, AIOps. Generative
models have achieved impressive results in time series modeling because they
can model data distribution and take noise into consideration. However, many
existing works can not be widely used because of the constraints of functional
form of generative models or the sensitivity to hyperparameters. In this paper,
we propose ScoreGrad, a multivariate probabilistic time series forecasting
framework based on continuous energy-based generative models. ScoreGrad is
composed of time series feature extraction module and conditional stochastic
differential equation based score matching module. The prediction can be
achieved by iteratively solving reverse-time SDE. To the best of our knowledge,
ScoreGrad is the first continuous energy based generative model used for time
series forecasting. Furthermore, ScoreGrad achieves state-of-the-art results on
six real-world datasets. The impact of hyperparameters and sampler types on the
performance are also explored. Code is available at
https://github.com/yantijin/ScoreGradPred.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1"&gt;Tijin Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1"&gt;Yufeng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yuanqing Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Effects of Compression with Hyperdimensional Computing in Distributed Randomized Neural Networks. (arXiv:2106.09831v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09831</id>
        <link href="http://arxiv.org/abs/2106.09831"/>
        <updated>2021-06-21T02:07:39.332Z</updated>
        <summary type="html"><![CDATA[A change of the prevalent supervised learning techniques is foreseeable in
the near future: from the complex, computational expensive algorithms to more
flexible and elementary training ones. The strong revitalization of randomized
algorithms can be framed in this prospect steering. We recently proposed a
model for distributed classification based on randomized neural networks and
hyperdimensional computing, which takes into account cost of information
exchange between agents using compression. The use of compression is important
as it addresses the issues related to the communication bottleneck, however,
the original approach is rigid in the way the compression is used. Therefore,
in this work, we propose a more flexible approach to compression and compare it
to conventional compression algorithms, dimensionality reduction, and
quantization techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosato_A/0/1/0/all/0/1"&gt;Antonello Rosato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panella_M/0/1/0/all/0/1"&gt;Massimo Panella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osipov_E/0/1/0/all/0/1"&gt;Evgeny Osipov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1"&gt;Denis Kleyko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented KRnet for density estimation and approximation. (arXiv:2105.12866v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12866</id>
        <link href="http://arxiv.org/abs/2105.12866"/>
        <updated>2021-06-21T02:07:39.325Z</updated>
        <summary type="html"><![CDATA[In this work, we have proposed augmented KRnets including both discrete and
continuous models. One difficulty in flow-based generative modeling is to
maintain the invertibility of the transport map, which is often a trade-off
between effectiveness and robustness. The exact invertibility has been achieved
in the real NVP using a specific pattern to exchange information between two
separated groups of dimensions. KRnet has been developed to enhance the
information exchange among data dimensions by incorporating the
Knothe-Rosenblatt rearrangement into the structure of the transport map. Due to
the maintenance of exact invertibility, a full nonlinear update of all data
dimensions needs three iterations in KRnet. To alleviate this issue, we will
add augmented dimensions that act as a channel for communications among the
data dimensions. In the augmented KRnet, a fully nonlinear update is achieved
in two iterations. We also show that the augmented KRnet can be reformulated as
the discretization of a neural ODE, where the exact invertibility is kept such
that the adjoint method can be formulated with respect to the discretized ODE
to obtain the exact gradient. Numerical experiments have been implemented to
demonstrate the effectiveness of our models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xiaoliang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tang_K/0/1/0/all/0/1"&gt;Kejun Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DG-LMC: A Turn-key and Scalable Synchronous Distributed MCMC Algorithm via Langevin Monte Carlo within Gibbs. (arXiv:2106.06300v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06300</id>
        <link href="http://arxiv.org/abs/2106.06300"/>
        <updated>2021-06-21T02:07:39.319Z</updated>
        <summary type="html"><![CDATA[Performing reliable Bayesian inference on a big data scale is becoming a
keystone in the modern era of machine learning. A workhorse class of methods to
achieve this task are Markov chain Monte Carlo (MCMC) algorithms and their
design to handle distributed datasets has been the subject of many works.
However, existing methods are not completely either reliable or computationally
efficient. In this paper, we propose to fill this gap in the case where the
dataset is partitioned and stored on computing nodes within a cluster under a
master/slaves architecture. We derive a user-friendly centralised distributed
MCMC algorithm with provable scaling in high-dimensional settings. We
illustrate the relevance of the proposed methodology on both synthetic and real
data experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Plassier_V/0/1/0/all/0/1"&gt;Vincent Plassier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vono_M/0/1/0/all/0/1"&gt;Maxime Vono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1"&gt;Alain Durmus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Moulines_E/0/1/0/all/0/1"&gt;Eric Moulines&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Connections between Counterfactual Explanations and Adversarial Examples. (arXiv:2106.09992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09992</id>
        <link href="http://arxiv.org/abs/2106.09992"/>
        <updated>2021-06-21T02:07:39.312Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations and adversarial examples have emerged as critical
research areas for addressing the explainability and robustness goals of
machine learning (ML). While counterfactual explanations were developed with
the goal of providing recourse to individuals adversely impacted by algorithmic
decisions, adversarial examples were designed to expose the vulnerabilities of
ML models. While prior research has hinted at the commonalities between these
frameworks, there has been little to no work on systematically exploring the
connections between the literature on counterfactual explanations and
adversarial examples. In this work, we make one of the first attempts at
formalizing the connections between counterfactual explanations and adversarial
examples. More specifically, we theoretically analyze salient counterfactual
explanation and adversarial example generation methods, and highlight the
conditions under which they behave similarly. Our analysis demonstrates that
several popular counterfactual explanation and adversarial example generation
methods such as the ones proposed by Wachter et. al. and Carlini and Wagner
(with mean squared error loss), and C-CHVAE and natural adversarial examples by
Zhao et. al. are equivalent. We also bound the distance between counterfactual
explanations and adversarial examples generated by Wachter et. al. and DeepFool
methods for linear models. Finally, we empirically validate our theoretical
findings using extensive experimentation with synthetic and real world
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pawelczyk_M/0/1/0/all/0/1"&gt;Martin Pawelczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1"&gt;Shalmali Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1"&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Sohini Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoK: Privacy-Preserving Collaborative Tree-based Model Learning. (arXiv:2103.08987v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08987</id>
        <link href="http://arxiv.org/abs/2103.08987"/>
        <updated>2021-06-21T02:07:39.305Z</updated>
        <summary type="html"><![CDATA[Tree-based models are among the most efficient machine learning techniques
for data mining nowadays due to their accuracy, interpretability, and
simplicity. The recent orthogonal needs for more data and privacy protection
call for collaborative privacy-preserving solutions. In this work, we survey
the literature on distributed and privacy-preserving training of tree-based
models and we systematize its knowledge based on four axes: the learning
algorithm, the collaborative model, the protection mechanism, and the threat
model. We use this to identify the strengths and limitations of these works and
provide for the first time a framework analyzing the information leakage
occurring in distributed tree-based model learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chatel_S/0/1/0/all/0/1"&gt;Sylvain Chatel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pyrgelis_A/0/1/0/all/0/1"&gt;Apostolos Pyrgelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Troncoso_Pastoriza_J/0/1/0/all/0/1"&gt;Juan Ramon Troncoso-Pastoriza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubaux_J/0/1/0/all/0/1"&gt;Jean-Pierre Hubaux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fitting summary statistics of neural data with a differentiable spiking network simulator. (arXiv:2106.10064v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10064</id>
        <link href="http://arxiv.org/abs/2106.10064"/>
        <updated>2021-06-21T02:07:39.287Z</updated>
        <summary type="html"><![CDATA[Fitting network models to neural activity is becoming an important tool in
neuroscience. A popular approach is to model a brain area with a probabilistic
recurrent spiking network whose parameters maximize the likelihood of the
recorded activity. Although this is widely used, we show that the resulting
model does not produce realistic neural activity and wrongly estimates the
connectivity matrix when neurons that are not recorded have a substantial
impact on the recorded network. To correct for this, we suggest to augment the
log-likelihood with terms that measure the dissimilarity between simulated and
recorded activity. This dissimilarity is defined via summary statistics
commonly used in neuroscience, and the optimization is efficient because it
relies on back-propagation through the stochastically simulated spike trains.
We analyze this method theoretically and show empirically that it generates
more realistic activity statistics and recovers the connectivity matrix better
than other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bellec_G/0/1/0/all/0/1"&gt;Guillaume Bellec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Modirshanechi_A/0/1/0/all/0/1"&gt;Alireza Modirshanechi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Brea_J/0/1/0/all/0/1"&gt;Johanni Brea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gerstner_W/0/1/0/all/0/1"&gt;Wulfram Gerstner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Play in Multi-Agent Markov Stochastic Games: Stationary Points and Convergence. (arXiv:2106.00198v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00198</id>
        <link href="http://arxiv.org/abs/2106.00198"/>
        <updated>2021-06-21T02:07:39.281Z</updated>
        <summary type="html"><![CDATA[We study the performance of the gradient play algorithm for multi-agent
tabular Markov decision processes (MDPs), which are also known as stochastic
games (SGs), where each agent tries to maximize its own total discounted reward
by making decisions independently based on current state information which is
shared between agents. Policies are directly parameterized by the probability
of choosing a certain action at a given state. We show that Nash equilibria
(NEs) and first order stationary policies are equivalent in this setting, and
give a non-asymptotic global convergence rate analysis to an $\epsilon$-NE for
a subclass of multi-agent MDPs called Markov potential games, which includes
the cooperative setting with identical rewards among agents as an important
special case. Our result shows that the number of iterations to reach an
$\epsilon$-NE scales linearly, instead of exponentially, with the number of
agents. Local geometry and local stability are also considered. For Markov
potential games, we prove that strict NEs are local maxima of the total
potential function and fully-mixed NEs are saddle points. We also give a local
convergence rate around strict NEs for more general settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Runyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhaolin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1"&gt;Na Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information criteria for non-normalized models. (arXiv:1905.05976v4 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.05976</id>
        <link href="http://arxiv.org/abs/1905.05976"/>
        <updated>2021-06-21T02:07:39.272Z</updated>
        <summary type="html"><![CDATA[Many statistical models are given in the form of non-normalized densities
with an intractable normalization constant. Since maximum likelihood estimation
is computationally intensive for these models, several estimation methods have
been developed which do not require explicit computation of the normalization
constant, such as noise contrastive estimation (NCE) and score matching.
However, model selection methods for general non-normalized models have not
been proposed so far. In this study, we develop information criteria for
non-normalized models estimated by NCE or score matching. They are
approximately unbiased estimators of discrepancy measures for non-normalized
models. Simulation results and applications to real data demonstrate that the
proposed criteria enable selection of the appropriate non-normalized model in a
data-driven manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Matsuda_T/0/1/0/all/0/1"&gt;Takeru Matsuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Uehara_M/0/1/0/all/0/1"&gt;Masatoshi Uehara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hyvarinen_A/0/1/0/all/0/1"&gt;Aapo Hyvarinen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Contrastive Learning for Joint Demosaicking and Denoising. (arXiv:2106.10070v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10070</id>
        <link href="http://arxiv.org/abs/2106.10070"/>
        <updated>2021-06-21T02:07:39.266Z</updated>
        <summary type="html"><![CDATA[The breakthrough of contrastive learning (CL) has fueled the recent success
of self-supervised learning (SSL) in high-level vision tasks on RGB images.
However, CL is still ill-defined for low-level vision tasks, such as joint
demosaicking and denoising (JDD), in the RAW domain. To bridge this
methodological gap, we present a novel CL approach on RAW images, residual
contrastive learning (RCL), which aims to learn meaningful representations for
JDD. Our work is built on the assumption that noise contained in each RAW image
is signal-dependent, thus two crops from the same RAW image should have more
similar noise distribution than two crops from different RAW images. We use
residuals as a discriminative feature and the earth mover's distance to measure
the distribution divergence for the contrastive loss. To evaluate the proposed
CL strategy, we simulate a series of unsupervised JDD experiments with
large-scale data corrupted by synthetic signal-dependent noise, where we set a
new benchmark for unsupervised JDD tasks with unknown (random) noise variance.
Our empirical study not only validates that CL can be applied on distributions
(c.f. features), but also exposes the lack of robustness of previous non-ML and
SSL JDD methods when the statistics of the noise are unknown, thus providing
some further insight into signal-dependent noise problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1"&gt;Nanqing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maggioni_M/0/1/0/all/0/1"&gt;Matteo Maggioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1"&gt;Eduardo P&amp;#xe9;rez-Pellitero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ales Leonardis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1"&gt;Steven McDonagh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consensus Control for Decentralized Deep Learning. (arXiv:2102.04828v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04828</id>
        <link href="http://arxiv.org/abs/2102.04828"/>
        <updated>2021-06-21T02:07:39.259Z</updated>
        <summary type="html"><![CDATA[Decentralized training of deep learning models enables on-device learning
over networks, as well as efficient scaling to large compute clusters.
Experiments in earlier works reveal that, even in a data-center setup,
decentralized training often suffers from the degradation in the quality of the
model: the training and test performance of models trained in a decentralized
fashion is in general worse than that of models trained in a centralized
fashion, and this performance drop is impacted by parameters such as network
size, communication topology and data partitioning. We identify the changing
consensus distance between devices as a key parameter to explain the gap
between centralized and decentralized training.

We show in theory that when the training consensus distance is lower than a
critical quantity, decentralized training converges as fast as the centralized
counterpart. We empirically validate that the relation between generalization
performance and consensus distance is consistent with this theoretical
observation. Our empirical insights allow the principled design of better
decentralized training schemes that mitigate the performance drop. To this end,
we provide practical training guidelines and exemplify its effectiveness on the
data-center setup as the important first step.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1"&gt;Lingjing Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koloskova_A/0/1/0/all/0/1"&gt;Anastasia Koloskova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting the Function Approximation Architecture in Online Reinforcement Learning. (arXiv:2106.09776v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09776</id>
        <link href="http://arxiv.org/abs/2106.09776"/>
        <updated>2021-06-21T02:07:39.240Z</updated>
        <summary type="html"><![CDATA[The performance of a reinforcement learning (RL) system depends on the
computational architecture used to approximate a value function. Deep learning
methods provide both optimization techniques and architectures for
approximating nonlinear functions from noisy, high-dimensional observations.
However, prevailing optimization techniques are not designed for
strictly-incremental online updates. Nor are standard architectures designed
for observations with an a priori unknown structure: for example, light sensors
randomly dispersed in space. This paper proposes an online RL prediction
algorithm with an adaptive architecture that efficiently finds useful nonlinear
features. The algorithm is evaluated in a spatial domain with high-dimensional,
stochastic observations. The algorithm outperforms non-adaptive baseline
architectures and approaches the performance of an architecture given
side-channel information. These results are a step towards scalable RL
algorithms for more general problems, where the observation structure is not
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1"&gt;John D. Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modayil_J/0/1/0/all/0/1"&gt;Joseph Modayil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Generalization in Deep Learning Applications for Land Cover Mapping. (arXiv:2008.10351v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10351</id>
        <link href="http://arxiv.org/abs/2008.10351"/>
        <updated>2021-06-21T02:07:39.234Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that deep learning models can be used to classify
land-use data from geospatial satellite imagery. We show that when these deep
learning models are trained on data from specific continents/seasons, there is
a high degree of variability in model performance on out-of-sample
continents/seasons. This suggests that just because a model accurately predicts
land-use classes in one continent or season does not mean that the model will
accurately predict land-use classes in a different continent or season. We then
use clustering techniques on satellite imagery from different continents to
visualize the differences in landscapes that make geospatial generalization
particularly difficult, and summarize our takeaways for future satellite
imagery-related applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Lucas Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1"&gt;Caleb Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1"&gt;Bistra Dilkina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Approximations for a Class of Sequential Testing Problems. (arXiv:2102.07030v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07030</id>
        <link href="http://arxiv.org/abs/2102.07030"/>
        <updated>2021-06-21T02:07:39.226Z</updated>
        <summary type="html"><![CDATA[We consider a decision maker who must choose an action in order to maximize a
reward function that depends also on an unknown parameter {\Theta}. The
decision maker can delay taking the action in order to experiment and gather
additional information on {\Theta}. We model the decision maker's problem using
a Bayesian sequential experimentation framework and use dynamic programming and
diffusion-asymptotic analysis to solve it. For that, we scale our problem in a
way that both the average number of experiments that is conducted per unit of
time is large and the informativeness of each individual experiment is low.
Under such regime, we derive a diffusion approximation for the sequential
experimentation problem, which provides a number of important insights about
the nature of the problem and its solution. Our solution method also shows that
the complexity of the problem grows only quadratically with the cardinality of
the set of actions from which the decision maker can choose. We illustrate our
methodology and results using a concrete application in the context of
assortment selection and new product introduction. Specifically, we study the
problem of a seller who wants to select an optimal assortment of products to
launch into the marketplace and is uncertain about consumers' preferences.
Motivated by emerging practices in e-commerce, we assume that the seller is
able to use a crowdvoting system to learn these preferences before a final
assortment decision is made. In this context, we undertake an extensive
numerical analysis to assess the value of learning and demonstrate the
effectiveness and robustness of the heuristics derived from the diffusion
approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Araman_V/0/1/0/all/0/1"&gt;Victor F. Araman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Caldentey_R/0/1/0/all/0/1"&gt;Rene Caldentey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-healthy synthesis with pathology disentanglement and adversarial learning. (arXiv:2005.01607v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01607</id>
        <link href="http://arxiv.org/abs/2005.01607"/>
        <updated>2021-06-21T02:07:39.216Z</updated>
        <summary type="html"><![CDATA[Pseudo-healthy synthesis is the task of creating a subject-specific `healthy'
image from a pathological one. Such images can be helpful in tasks such as
anomaly detection and understanding changes induced by pathology and disease.
In this paper, we present a model that is encouraged to disentangle the
information of pathology from what seems to be healthy. We disentangle what
appears to be healthy and where disease is as a segmentation map, which are
then recombined by a network to reconstruct the input disease image. We train
our models adversarially using either paired or unpaired settings, where we
pair disease images and maps when available. We quantitatively and
subjectively, with a human study, evaluate the quality of pseudo-healthy images
using several criteria. We show in a series of experiments, performed on ISLES,
BraTS and Cam-CAN datasets, that our method is better than several baselines
and methods from the literature. We also show that due to better training
processes we could recover deformations, on surrounding tissue, caused by
disease. Our implementation is publicly available at
https://github.com/xiat0616/pseudo-healthy-synthesis. This paper has been
accepted by Medical Image Analysis:
https://doi.org/10.1016/j.media.2020.101719.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1"&gt;Tian Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1"&gt;Agisilaos Chartsias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios A. Tsaftaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolving GANs: When Contradictions Turn into Compliance. (arXiv:2106.09946v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09946</id>
        <link href="http://arxiv.org/abs/2106.09946"/>
        <updated>2021-06-21T02:07:39.208Z</updated>
        <summary type="html"><![CDATA[Limited availability of labeled-data makes any supervised learning problem
challenging. Alternative learning settings like semi-supervised and universum
learning alleviate the dependency on labeled data, but still require a large
amount of unlabeled data, which may be unavailable or expensive to acquire.
GAN-based synthetic data generation methods have recently shown promise by
generating synthetic samples to improve task at hand. However, these samples
cannot be used for other purposes. In this paper, we propose a GAN game which
provides improved discriminator accuracy under limited data settings, while
generating realistic synthetic data. This provides the added advantage that now
the generated data can be used for other similar tasks. We provide the
theoretical guarantees and empirical results in support of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhar_S/0/1/0/all/0/1"&gt;Sauptik Dhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heydari_J/0/1/0/all/0/1"&gt;Javad Heydari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Samarth Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurup_U/0/1/0/all/0/1"&gt;Unmesh Kurup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mohak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Message Passing in Graph Convolution Networks via Adaptive Filter Banks. (arXiv:2106.09910v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09910</id>
        <link href="http://arxiv.org/abs/2106.09910"/>
        <updated>2021-06-21T02:07:39.189Z</updated>
        <summary type="html"><![CDATA[Graph convolution networks, like message passing graph convolution networks
(MPGCNs), have been a powerful tool in representation learning of networked
data. However, when data is heterogeneous, most architectures are limited as
they employ a single strategy to handle multi-channel graph signals and they
typically focus on low-frequency information. In this paper, we present a novel
graph convolution operator, termed BankGCN, which keeps benefits of message
passing models, but extends their capabilities beyond `low-pass' features. It
decomposes multi-channel signals on graphs into subspaces and handles
particular information in each subspace with an adapted filter. The filters of
all subspaces have different frequency responses and together form a filter
bank. Furthermore, each filter in the spectral domain corresponds to a message
passing scheme, and diverse schemes are implemented via the filter bank.
Importantly, the filter bank and the signal decomposition are jointly learned
to adapt to the spectral characteristics of data and to target applications.
Furthermore, this is implemented almost without extra parameters in comparison
with most existing MPGCNs. Experimental results show that the proposed
convolution operator permits to achieve excellent performance in graph
classification on a collection of benchmark graph datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xing Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wenrui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenglin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;Junni Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hongkai Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1"&gt;Pascal Frossard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Armed Bandits for Minesweeper: Profiting from Exploration-Exploitation Synergy. (arXiv:2007.12824v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.12824</id>
        <link href="http://arxiv.org/abs/2007.12824"/>
        <updated>2021-06-21T02:07:39.182Z</updated>
        <summary type="html"><![CDATA[A popular computer puzzle, the game of Minesweeper requires its human players
to have a mix of both luck and strategy to succeed. Analyzing these aspects
more formally, in our research we assessed the feasibility of a novel
methodology based on Reinforcement Learning as an adequate approach to tackle
the problem presented by this game. For this purpose we employed Multi-Armed
Bandit algorithms which were carefully adapted in order to enable their use to
define autonomous computational players, targeting to make the best use of some
game peculiarities. After experimental evaluation, results showed that this
approach was indeed successful, especially in smaller game boards, such as the
standard beginner level. Despite this fact the main contribution of this work
is a detailed examination of Minesweeper from a learning perspective, which led
to various original insights which are thoroughly discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lordeiro_I/0/1/0/all/0/1"&gt;Igor Q. Lordeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haddad_D/0/1/0/all/0/1"&gt;Diego B. Haddad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cardoso_D/0/1/0/all/0/1"&gt;Douglas O. Cardoso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap Between Object Detection and User Intent via Query-Modulation. (arXiv:2106.10258v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10258</id>
        <link href="http://arxiv.org/abs/2106.10258"/>
        <updated>2021-06-21T02:07:39.175Z</updated>
        <summary type="html"><![CDATA[When interacting with objects through cameras, or pictures, users often have
a specific intent. For example, they may want to perform a visual search.
However, most object detection models ignore the user intent, relying on image
pixels as their only input. This often leads to incorrect results, such as lack
of a high-confidence detection on the object of interest, or detection with a
wrong class label. In this paper we investigate techniques to modulate standard
object detectors to explicitly account for the user intent, expressed as an
embedding of a simple query. Compared to standard object detectors,
query-modulated detectors show superior performance at detecting objects for a
given label of interest. Thanks to large-scale training data synthesized from
standard object detection annotations, query-modulated detectors can also
outperform specialized referring expression recognition systems. Furthermore,
they can be simultaneously trained to solve for both query-modulated detection
and standard object detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fornoni_M/0/1/0/all/0/1"&gt;Marco Fornoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chaochao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1"&gt;Liangchen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1"&gt;Kimberly Wilber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stark_A/0/1/0/all/0/1"&gt;Alex Stark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yin Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1"&gt;Andrew Howard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local AdaGrad-Type Algorithm for Stochastic Convex-Concave Minimax Problems. (arXiv:2106.10022v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10022</id>
        <link href="http://arxiv.org/abs/2106.10022"/>
        <updated>2021-06-21T02:07:39.168Z</updated>
        <summary type="html"><![CDATA[Large scale convex-concave minimax problems arise in numerous applications,
including game theory, robust training, and training of generative adversarial
networks. Despite their wide applicability, solving such problems efficiently
and effectively is challenging in the presence of large amounts of data using
existing stochastic minimax methods. We study a class of stochastic minimax
methods and develop a communication-efficient distributed stochastic
extragradient algorithm, LocalAdaSEG, with an adaptive learning rate suitable
for solving convex-concave minimax problem in the Parameter-Server model.
LocalAdaSEG has three main features: (i) periodic communication strategy
reduces the communication cost between workers and the server; (ii) an adaptive
learning rate that is computed locally and allows for tuning-free
implementation; and (iii) theoretically, a nearly linear speed-up with respect
to the dominant variance term, arising from estimation of the stochastic
gradient, is proven in both the smooth and nonsmooth convex-concave settings.
LocalAdaSEG is used to solve a stochastic bilinear game, and train generative
adversarial network. We compare LocalAdaSEG against several existing optimizers
for minimax problems and demonstrate its efficacy through several experiments
in both the homogeneous and heterogeneous settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1"&gt;Luofeng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jia Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolar_M/0/1/0/all/0/1"&gt;Mladen Kolar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It's FLAN time! Summing feature-wise latent representations for interpretability. (arXiv:2106.10086v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10086</id>
        <link href="http://arxiv.org/abs/2106.10086"/>
        <updated>2021-06-21T02:07:39.162Z</updated>
        <summary type="html"><![CDATA[Interpretability has become a necessary feature for machine learning models
deployed in critical scenarios, e.g. legal systems, healthcare. In these
situations, algorithmic decisions may have (potentially negative) long-lasting
effects on the end-user affected by the decision. In many cases, the
representational power of deep learning models is not needed, therefore simple
and interpretable models (e.g. linear models) should be preferred. However, in
high-dimensional and/or complex domains (e.g. computer vision), the universal
approximation capabilities of neural networks is required. Inspired by linear
models and the Kolmogorov-Arnol representation theorem, we propose a novel
class of structurally-constrained neural networks, which we call FLANs
(Feature-wise Latent Additive Networks). Crucially, FLANs process each input
feature separately, computing for each of them a representation in a common
latent space. These feature-wise latent representations are then simply summed,
and the aggregated representation is used for prediction. These constraints
(which are at the core of the interpretability of linear models) allow an user
to estimate the effect of each individual feature independently from the
others, enhancing interpretability. In a set of experiments across different
domains, we show how without compromising excessively the test performance, the
structural constraints proposed in FLANs indeed increase the interpretability
of deep learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;An-phi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_M/0/1/0/all/0/1"&gt;Maria Rodriguez Martinez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-healthy synthesis with pathology disentanglement and adversarial learning. (arXiv:2005.01607v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01607</id>
        <link href="http://arxiv.org/abs/2005.01607"/>
        <updated>2021-06-21T02:07:39.155Z</updated>
        <summary type="html"><![CDATA[Pseudo-healthy synthesis is the task of creating a subject-specific `healthy'
image from a pathological one. Such images can be helpful in tasks such as
anomaly detection and understanding changes induced by pathology and disease.
In this paper, we present a model that is encouraged to disentangle the
information of pathology from what seems to be healthy. We disentangle what
appears to be healthy and where disease is as a segmentation map, which are
then recombined by a network to reconstruct the input disease image. We train
our models adversarially using either paired or unpaired settings, where we
pair disease images and maps when available. We quantitatively and
subjectively, with a human study, evaluate the quality of pseudo-healthy images
using several criteria. We show in a series of experiments, performed on ISLES,
BraTS and Cam-CAN datasets, that our method is better than several baselines
and methods from the literature. We also show that due to better training
processes we could recover deformations, on surrounding tissue, caused by
disease. Our implementation is publicly available at
https://github.com/xiat0616/pseudo-healthy-synthesis. This paper has been
accepted by Medical Image Analysis:
https://doi.org/10.1016/j.media.2020.101719.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1"&gt;Tian Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1"&gt;Agisilaos Chartsias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios A. Tsaftaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. (arXiv:2106.10270v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10270</id>
        <link href="http://arxiv.org/abs/2106.10270"/>
        <updated>2021-06-21T02:07:39.136Z</updated>
        <summary type="html"><![CDATA[Vision Transformers (ViT) have been shown to attain highly competitive
performance for a wide range of vision applications, such as image
classification, object detection and semantic image segmentation. In comparison
to convolutional neural networks, the Vision Transformer's weaker inductive
bias is generally found to cause an increased reliance on model regularization
or data augmentation (``AugReg'' for short) when training on smaller training
datasets. We conduct a systematic empirical study in order to better understand
the interplay between the amount of training data, AugReg, model size and
compute budget. As one result of this study we find that the combination of
increased compute and AugReg can yield models with the same performance as
models trained on an order of magnitude more training data: we train ViT models
of various sizes on the public ImageNet-21k dataset which either match or
outperform their counterparts trained on the larger, but not publicly available
JFT-300M dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1"&gt;Andreas Steiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wightman_R/0/1/0/all/0/1"&gt;Ross Wightman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1"&gt;Jakob Uszkoreit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting gender of Brazilian names using deep learning. (arXiv:2106.10156v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10156</id>
        <link href="http://arxiv.org/abs/2106.10156"/>
        <updated>2021-06-21T02:07:39.129Z</updated>
        <summary type="html"><![CDATA[Predicting gender by the name is not a simple task. In many applications,
especially in the natural language processing (NLP) field, this task may be
necessary, mainly when considering foreign names. Some machine learning
algorithms can satisfactorily perform the prediction. In this paper, we
examined and implemented feedforward and recurrent deep neural network models,
such as MLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first
name. A dataset of Brazilian names is used to train and evaluate the models. We
analyzed the accuracy, recall, precision, and confusion matrix to measure the
models' performances. The results indicate that the gender prediction can be
performed from the feature extraction strategy looking at the names as a set of
strings. Some models accurately predict the gender in more than 90% of the
cases. The recurrent models overcome the feedforward models in this binary
classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rego_R/0/1/0/all/0/1"&gt;Rosana C. B. Rego&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1"&gt;Ver&amp;#xf4;nica M. L. Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python. (arXiv:2106.09756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09756</id>
        <link href="http://arxiv.org/abs/2106.09756"/>
        <updated>2021-06-21T02:07:39.121Z</updated>
        <summary type="html"><![CDATA[Machine learning is a general-purpose technology holding promises for many
interdisciplinary research problems. However, significant barriers exist in
crossing disciplinary boundaries when most machine learning tools are developed
in different areas separately. We present Pykale - a Python library for
knowledge-aware machine learning on graphs, images, texts, and videos to enable
and accelerate interdisciplinary research. We formulate new green machine
learning guidelines based on standard software engineering practices and
propose a novel pipeline-based application programming interface (API). PyKale
focuses on leveraging knowledge from multiple sources for accurate and
interpretable prediction, thus supporting multimodal learning and transfer
learning (particularly domain adaptation) with latest deep learning and
dimensionality reduction models. We build PyKale on PyTorch and leverage the
rich PyTorch ecosystem. Our pipeline-based API design enforces standardization
and minimalism, embracing green machine learning concepts via reducing
repetitions and redundancy, reusing existing resources, and recycling learning
models across areas. We demonstrate its interdisciplinary nature via examples
in bioinformatics, knowledge graph, image/video recognition, and medical
imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haiping Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1"&gt;Robert Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_P/0/1/0/all/0/1"&gt;Peizhen Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1"&gt;Raivo E Koot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chasmai_M/0/1/0/all/0/1"&gt;Mustafa Chasmai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schobs_L/0/1/0/all/0/1"&gt;Lawrence Schobs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bad Characters: Imperceptible NLP Attacks. (arXiv:2106.09898v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09898</id>
        <link href="http://arxiv.org/abs/2106.09898"/>
        <updated>2021-06-21T02:07:39.113Z</updated>
        <summary type="html"><![CDATA[Several years of research have shown that machine-learning systems are
vulnerable to adversarial examples, both in theory and in practice. Until now,
such attacks have primarily targeted visual models, exploiting the gap between
human and machine perception. Although text-based models have also been
attacked with adversarial examples, such attacks struggled to preserve semantic
meaning and indistinguishability. In this paper, we explore a large class of
adversarial examples that can be used to attack text-based models in a
black-box setting without making any human-perceptible visual modification to
inputs. We use encoding-specific perturbations that are imperceptible to the
human eye to manipulate the outputs of a wide range of Natural Language
Processing (NLP) systems from neural machine-translation pipelines to web
search engines. We find that with a single imperceptible encoding injection --
representing one invisible character, homoglyph, reordering, or deletion -- an
attacker can significantly reduce the performance of vulnerable models, and
with three injections most models can be functionally broken. Our attacks work
against currently-deployed commercial systems, including those produced by
Microsoft and Google, in addition to open source models published by Facebook
and IBM. This novel series of attacks presents a significant threat to many
language processing systems: an attacker can affect systems in a targeted
manner without any assumptions about the underlying model. We conclude that
text-based NLP systems require careful input sanitization, just like
conventional applications, and that given such systems are now being deployed
rapidly at scale, the urgent attention of architects and operators is required.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boucher_N/0/1/0/all/0/1"&gt;Nicholas Boucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1"&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1"&gt;Ross Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Federated Learning with New Classes for Audio Classification. (arXiv:2106.10019v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10019</id>
        <link href="http://arxiv.org/abs/2106.10019"/>
        <updated>2021-06-21T02:07:39.095Z</updated>
        <summary type="html"><![CDATA[Federated learning is an effective way of extracting insights from different
user devices while preserving the privacy of users. However, new classes with
completely unseen data distributions can stream across any device in a
federated learning setting, whose data cannot be accessed by the global server
or other users. To this end, we propose a unified zero-shot framework to handle
these aforementioned challenges during federated learning. We simulate two
scenarios here -- 1) when the new class labels are not reported by the user,
the traditional FL setting is used; 2) when new class labels are reported by
the user, we synthesize Anonymized Data Impressions by calculating class
similarity matrices corresponding to each device's new classes followed by
unsupervised clustering to distinguish between new classes across different
users. Moreover, our proposed framework can also handle statistical
heterogeneities in both labels and models across the participating users. We
empirically evaluate our framework on-device across different communication
rounds (FL iterations) with new classes in both local and global updates, along
with heterogeneous labels and models, on two widely used audio classification
applications -- keyword spotting and urban sound classification, and observe an
average deterministic accuracy increase of ~4.041% and ~4.258% respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gudur_G/0/1/0/all/0/1"&gt;Gautham Krishna Gudur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perepu_S/0/1/0/all/0/1"&gt;Satheesh K. Perepu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Goal-Directed Planning by Reinforcement Learning and Active Inference. (arXiv:2106.09938v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09938</id>
        <link href="http://arxiv.org/abs/2106.09938"/>
        <updated>2021-06-21T02:07:39.088Z</updated>
        <summary type="html"><![CDATA[What is the difference between goal-directed and habitual behavior? We
propose a novel computational framework of decision making with Bayesian
inference, in which everything is integrated as an entire neural network model.
The model learns to predict environmental state transitions by self-exploration
and generating motor actions by sampling stochastic internal states $z$.
Habitual behavior, which is obtained from the prior distribution of $z$, is
acquired by reinforcement learning. Goal-directed behavior is determined from
the posterior distribution of $z$ by planning, using active inference, to
minimize the free energy for goal observation. We demonstrate the effectiveness
of the proposed framework by experiments in a sensorimotor navigation task with
camera observations and continuous motor actions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dongqi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doya_K/0/1/0/all/0/1"&gt;Kenji Doya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tani_J/0/1/0/all/0/1"&gt;Jun Tani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Bias Quantification for Continuous Treatment. (arXiv:2106.09762v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.09762</id>
        <link href="http://arxiv.org/abs/2106.09762"/>
        <updated>2021-06-21T02:07:39.077Z</updated>
        <summary type="html"><![CDATA[In this work we develop a novel characterization of marginal causal effect
and causal bias in the continuous treatment setting. We show they can be
expressed as an expectation with respect to a conditional probability
distribution, which can be estimated via standard statistical and probabilistic
methods. All terms in the expectations can be computed via automatic
differentiation, also for highly non-linear models. We further develop a new
complete criterion for identifiability of causal effects via covariate
adjustment, showing the bias equals zero if the criterion is met. We study the
effectiveness of our framework in three different scenarios: linear models
under confounding, overcontrol and endogenous selection bias; a non-linear
model where full identifiability cannot be achieved because of missing data; a
simulated medical study of statins and atherosclerotic cardiovascular disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Detommaso_G/0/1/0/all/0/1"&gt;Gianluca Detommaso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bruckner_M/0/1/0/all/0/1"&gt;Michael Br&amp;#xfc;ckner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schulz_P/0/1/0/all/0/1"&gt;Philip Schulz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chernozhukov_V/0/1/0/all/0/1"&gt;Victor Chernozhukov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Escaping strict saddle points of the Moreau envelope in nonsmooth optimization. (arXiv:2106.09815v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.09815</id>
        <link href="http://arxiv.org/abs/2106.09815"/>
        <updated>2021-06-21T02:07:39.068Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that stochastically perturbed gradient methods can
efficiently escape strict saddle points of smooth functions. We extend this
body of work to nonsmooth optimization, by analyzing an inexact analogue of a
stochastically perturbed gradient method applied to the Moreau envelope. The
main conclusion is that a variety of algorithms for nonsmooth optimization can
escape strict saddle points of the Moreau envelope at a controlled rate. The
main technical insight is that typical algorithms applied to the proximal
subproblem yield directions that approximate the gradient of the Moreau
envelope in relative terms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Davis_D/0/1/0/all/0/1"&gt;Damek Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Diaz_M/0/1/0/all/0/1"&gt;Mateo D&amp;#xed;az&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Drusvyatskiy_D/0/1/0/all/0/1"&gt;Dmitriy Drusvyatskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boolean Matrix Factorization with SAT and MaxSAT. (arXiv:2106.10105v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10105</id>
        <link href="http://arxiv.org/abs/2106.10105"/>
        <updated>2021-06-21T02:07:39.056Z</updated>
        <summary type="html"><![CDATA[The Boolean matrix factorization problem consists in approximating a matrix
by the Boolean product of two smaller Boolean matrices. To obtain optimal
solutions when the matrices to be factorized are small, we propose SAT and
MaxSAT encoding; however, when the matrices to be factorized are large, we
propose a heuristic based on the search for maximal biclique edge cover. We
experimentally demonstrate that our approaches allow a better factorization
than existing approaches while keeping reasonable computation times. Our
methods also allow the handling of incomplete matrices with missing entries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avellaneda_F/0/1/0/all/0/1"&gt;Florent Avellaneda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villemaire_R/0/1/0/all/0/1"&gt;Roger Villemaire&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accumulative Poisoning Attacks on Real-time Data. (arXiv:2106.09993v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09993</id>
        <link href="http://arxiv.org/abs/2106.09993"/>
        <updated>2021-06-21T02:07:39.049Z</updated>
        <summary type="html"><![CDATA[Collecting training data from untrusted sources exposes machine learning
services to poisoning adversaries, who maliciously manipulate training data to
degrade the model accuracy. When trained on offline datasets, poisoning
adversaries have to inject the poisoned data in advance before training, and
the order of feeding these poisoned batches into the model is stochastic. In
contrast, practical systems are more usually trained/fine-tuned on sequentially
captured real-time data, in which case poisoning adversaries could dynamically
poison each data batch according to the current model state. In this paper, we
focus on the real-time settings and propose a new attacking strategy, which
affiliates an accumulative phase with poisoning attacks to secretly (i.e.,
without affecting accuracy) magnify the destructive effect of a (poisoned)
trigger batch. By mimicking online learning and federated learning on CIFAR-10,
we show that the model accuracy will significantly drop by a single update step
on the trigger batch after the accumulative phase. Our work validates that a
well-designed but straightforward attacking strategy can dramatically amplify
the poisoning effects, with no need to explore complex techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1"&gt;Tianyu Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection in Dynamic Graphs via Transformer. (arXiv:2106.09876v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09876</id>
        <link href="http://arxiv.org/abs/2106.09876"/>
        <updated>2021-06-21T02:07:39.031Z</updated>
        <summary type="html"><![CDATA[Detecting anomalies for dynamic graphs has drawn increasing attention due to
their wide applications in social networks, e-commerce, and cybersecurity. The
recent deep learning-based approaches have shown promising results over shallow
methods. However, they fail to address two core challenges of anomaly detection
in dynamic graphs: the lack of informative encoding for unattributed nodes and
the difficulty of learning discriminate knowledge from coupled spatial-temporal
dynamic graphs. To overcome these challenges, in this paper, we present a novel
Transformer-based Anomaly Detection framework for DYnamic graph (TADDY). Our
framework constructs a comprehensive node encoding strategy to better represent
each node's structural and temporal roles in an evolving graphs stream.
Meanwhile, TADDY captures informative representation from dynamic graphs with
coupled spatial-temporal patterns via a dynamic graph transformer model. The
extensive experimental results demonstrate that our proposed TADDY framework
outperforms the state-of-the-art methods by a large margin on four real-world
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yixin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Guang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1"&gt;Fei Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_V/0/1/0/all/0/1"&gt;Vincent CS Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Being Properly Improper. (arXiv:2106.09920v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09920</id>
        <link href="http://arxiv.org/abs/2106.09920"/>
        <updated>2021-06-21T02:07:39.025Z</updated>
        <summary type="html"><![CDATA[In today's ML, data can be twisted (changed) in various ways, either for bad
or good intent. Such twisted data challenges the founding theory of properness
for supervised losses which form the basis for many popular losses for class
probability estimation. Unfortunately, at its core, properness ensures that the
optimal models also learn the twist. In this paper, we analyse such class
probability-based losses when they are stripped off the mandatory properness;
we define twist-proper losses as losses formally able to retrieve the optimum
(untwisted) estimate off the twists, and show that a natural extension of a
half-century old loss introduced by S. Arimoto is twist proper. We then turn to
a theory that has provided some of the best off-the-shelf algorithms for proper
losses, boosting. Boosting can require access to the derivative of the convex
conjugate of a loss to compute examples weights. Such a function can be hard to
get, for computational or mathematical reasons; this turns out to be the case
for Arimoto's loss. We bypass this difficulty by inverting the problem as
follows: suppose a blueprint boosting algorithm is implemented with a general
weight update function. What are the losses for which boosting-compliant
minimisation happens? Our answer comes as a general boosting algorithm which
meets the optimal boosting dependence on the number of calls to the weak
learner; when applied to Arimoto's loss, it leads to a simple optimisation
algorithm whose performances are showcased on several domains and twists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nock_R/0/1/0/all/0/1"&gt;Richard Nock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sypherd_T/0/1/0/all/0/1"&gt;Tyler Sypherd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankar_L/0/1/0/all/0/1"&gt;Lalitha Sankar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Distance-based Separability Measure for Internal Cluster Validation. (arXiv:2106.09794v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09794</id>
        <link href="http://arxiv.org/abs/2106.09794"/>
        <updated>2021-06-21T02:07:39.009Z</updated>
        <summary type="html"><![CDATA[To evaluate clustering results is a significant part of cluster analysis.
Since there are no true class labels for clustering in typical unsupervised
learning, many internal cluster validity indices (CVIs), which use predicted
labels and data, have been created. Without true labels, to design an effective
CVI is as difficult as to create a clustering method. And it is crucial to have
more CVIs because there are no universal CVIs that can be used to measure all
datasets and no specific methods of selecting a proper CVI for clusters without
true labels. Therefore, to apply a variety of CVIs to evaluate clustering
results is necessary. In this paper, we propose a novel internal CVI -- the
Distance-based Separability Index (DSI), based on a data separability measure.
We compared the DSI with eight internal CVIs including studies from early Dunn
(1974) to most recent CVDD (2019) and an external CVI as ground truth, by using
clustering results of five clustering algorithms on 12 real and 97 synthetic
datasets. Results show DSI is an effective, unique, and competitive CVI to
other compared CVIs. We also summarized the general process to evaluate CVIs
and created the rank-difference metric for comparison of CVIs' results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1"&gt;Shuyue Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1"&gt;Murray Loew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Self-supervised Vision Transformers for Representation Learning. (arXiv:2106.09785v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09785</id>
        <link href="http://arxiv.org/abs/2106.09785"/>
        <updated>2021-06-21T02:07:39.002Z</updated>
        <summary type="html"><![CDATA[This paper investigates two techniques for developing efficient
self-supervised vision transformers (EsViT) for visual representation learning.
First, we show through a comprehensive empirical study that multi-stage
architectures with sparse self-attentions can significantly reduce modeling
complexity but with a cost of losing the ability to capture fine-grained
correspondences between image regions. Second, we propose a new pre-training
task of region matching which allows the model to capture fine-grained region
dependencies and as a result significantly improves the quality of the learned
vision representations. Our results show that combining the two techniques,
EsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation,
outperforming prior arts with around an order magnitude of higher throughput.
When transferring to downstream linear classification tasks, EsViT outperforms
its supervised counterpart on 17 out of 18 datasets. The code and models will
be publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengchuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1"&gt;Bin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiyang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shuffle Private Stochastic Convex Optimization. (arXiv:2106.09805v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09805</id>
        <link href="http://arxiv.org/abs/2106.09805"/>
        <updated>2021-06-21T02:07:38.982Z</updated>
        <summary type="html"><![CDATA[In shuffle privacy, each user sends a collection of randomized messages to a
trusted shuffler, the shuffler randomly permutes these messages, and the
resulting shuffled collection of messages must satisfy differential privacy.
Prior work in this model has largely focused on protocols that use a single
round of communication to compute algorithmic primitives like means,
histograms, and counts. In this work, we present interactive shuffle protocols
for stochastic convex optimization. Our optimization protocols rely on a new
noninteractive protocol for summing vectors of bounded $\ell_2$ norm. By
combining this sum subroutine with techniques including mini-batch stochastic
gradient descent, accelerated gradient descent, and Nesterov's smoothing
method, we obtain loss guarantees for a variety of convex loss functions that
significantly improve on those of the local model and sometimes match those of
the central model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheu_A/0/1/0/all/0/1"&gt;Albert Cheu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_M/0/1/0/all/0/1"&gt;Matthew Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jieming Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1"&gt;Binghui Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heuristic Stopping Rules For Technology-Assisted Review. (arXiv:2106.09871v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09871</id>
        <link href="http://arxiv.org/abs/2106.09871"/>
        <updated>2021-06-21T02:07:38.976Z</updated>
        <summary type="html"><![CDATA[Technology-assisted review (TAR) refers to human-in-the-loop active learning
workflows for finding relevant documents in large collections. These workflows
often must meet a target for the proportion of relevant documents found (i.e.
recall) while also holding down costs. A variety of heuristic stopping rules
have been suggested for striking this tradeoff in particular settings, but none
have been tested against a range of recall targets and tasks. We propose two
new heuristic stopping rules, Quant and QuantCI based on model-based estimation
techniques from survey research. We compare them against a range of proposed
heuristics and find they are accurate at hitting a range of recall targets
while substantially reducing review costs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eugene Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_D/0/1/0/all/0/1"&gt;David D. Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frieder_O/0/1/0/all/0/1"&gt;Ophir Frieder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-Enabled Ultra-Low-Dose CT Reconstruction. (arXiv:2106.09834v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09834</id>
        <link href="http://arxiv.org/abs/2106.09834"/>
        <updated>2021-06-21T02:07:38.970Z</updated>
        <summary type="html"><![CDATA[By the ALARA (As Low As Reasonably Achievable) principle, ultra-low-dose CT
reconstruction is a holy grail to minimize cancer risks and genetic damages,
especially for children. With the development of medical CT technologies, the
iterative algorithms are widely used to reconstruct decent CT images from a
low-dose scan. Recently, artificial intelligence (AI) techniques have shown a
great promise in further reducing CT radiation dose to the next level. In this
paper, we demonstrate that AI-powered CT reconstruction offers diagnostic image
quality at an ultra-low-dose level comparable to that of radiography.
Specifically, here we develop a Split Unrolled Grid-like Alternative
Reconstruction (SUGAR) network, in which deep learning, physical modeling and
image prior are integrated. The reconstruction results from clinical datasets
show that excellent images can be reconstructed using SUGAR from 36
projections. This approach has a potential to change future healthcare.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1"&gt;Weiwen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1"&gt;Chuang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebrahimian_S/0/1/0/all/0/1"&gt;Shadi Ebrahimian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hengyong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kalra_M/0/1/0/all/0/1"&gt;Mannu Kalra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Invariance Penalties for Risk Minimization. (arXiv:2106.09777v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09777</id>
        <link href="http://arxiv.org/abs/2106.09777"/>
        <updated>2021-06-21T02:07:38.963Z</updated>
        <summary type="html"><![CDATA[The Invariant Risk Minimization (IRM) principle was first proposed by
Arjovsky et al. [2019] to address the domain generalization problem by
leveraging data heterogeneity from differing experimental conditions.
Specifically, IRM seeks to find a data representation under which an optimal
classifier remains invariant across all domains. Despite the conceptual appeal
of IRM, the effectiveness of the originally proposed invariance penalty has
recently been brought into question. In particular, there exists
counterexamples for which that invariance penalty can be arbitrarily small for
non-invariant data representations. We propose an alternative invariance
penalty by revisiting the Gramian matrix of the data representation. We discuss
the role of its eigenvalues in the relationship between the risk and the
invariance penalty, and demonstrate that it is ill-conditioned for said
counterexamples. The proposed approach is guaranteed to recover an invariant
representation for linear settings under mild non-degeneracy conditions. Its
effectiveness is substantiated by experiments on DomainBed and
InvarianceUnitTest, two extensive test beds for domain generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khezeli_K/0/1/0/all/0/1"&gt;Kia Khezeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blaas_A/0/1/0/all/0/1"&gt;Arno Blaas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soboczenski_F/0/1/0/all/0/1"&gt;Frank Soboczenski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chia_N/0/1/0/all/0/1"&gt;Nicholas Chia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalantari_J/0/1/0/all/0/1"&gt;John Kalantari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Indoor Mapping through WiFi Signals. (arXiv:2106.09789v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.09789</id>
        <link href="http://arxiv.org/abs/2106.09789"/>
        <updated>2021-06-21T02:07:38.953Z</updated>
        <summary type="html"><![CDATA[The ubiquitous presence of WiFi access points and mobile devices capable of
measuring WiFi signal strengths allow for real-world applications in indoor
localization and mapping. In particular, no additional infrastructure is
required. Previous approaches in this field were, however, often hindered by
problems such as effortful map-building processes, changing environments and
hardware differences. We tackle these problems focussing on topological maps.
These represent discrete locations, such as rooms, and their relations, e.g.,
distances and transition frequencies. In our unsupervised method, we employ
WiFi signal strength distributions, dimension reduction and clustering. It can
be used in settings where users carry mobile devices and follow their normal
routine. We aim for applications in short-lived indoor events such as
conferences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schaefermeier_B/0/1/0/all/0/1"&gt;Bastian Schaefermeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stumme_G/0/1/0/all/0/1"&gt;Gerd Stumme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanika_T/0/1/0/all/0/1"&gt;Tom Hanika&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Note on Optimizing Distributions using Kernel Mean Embeddings. (arXiv:2106.09994v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09994</id>
        <link href="http://arxiv.org/abs/2106.09994"/>
        <updated>2021-06-21T02:07:38.946Z</updated>
        <summary type="html"><![CDATA[Kernel mean embeddings are a popular tool that consists in representing
probability measures by their infinite-dimensional mean embeddings in a
reproducing kernel Hilbert space. When the kernel is characteristic, mean
embeddings can be used to define a distance between probability measures, known
as the maximum mean discrepancy (MMD). A well-known advantage of mean
embeddings and MMD is their low computational cost and low sample complexity.
However, kernel mean embeddings have had limited applications to problems that
consist in optimizing distributions, due to the difficulty of characterizing
which Hilbert space vectors correspond to a probability distribution. In this
note, we propose to leverage the kernel sums-of-squares parameterization of
positive functions of Marteau-Ferey et al. [2020] to fit distributions in the
MMD geometry. First, we show that when the kernel is characteristic,
distributions with a kernel sum-of-squares density are dense. Then, we provide
algorithms to optimize such distributions in the finite-sample setting, which
we illustrate in a density fitting numerical experiment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muzellec_B/0/1/0/all/0/1"&gt;Boris Muzellec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Generative Adversarial Network Training via Self-Labeling and Self-Attention. (arXiv:2106.09914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09914</id>
        <link href="http://arxiv.org/abs/2106.09914"/>
        <updated>2021-06-21T02:07:38.927Z</updated>
        <summary type="html"><![CDATA[We propose a novel GAN training scheme that can handle any level of labeling
in a unified manner. Our scheme introduces a form of artificial labeling that
can incorporate manually defined labels, when available, and induce an
alignment between them. To define the artificial labels, we exploit the
assumption that neural network generators can be trained more easily to map
nearby latent vectors to data with semantic similarities, than across separate
categories. We use generated data samples and their corresponding artificial
conditioning labels to train a classifier. The classifier is then used to
self-label real data. To boost the accuracy of the self-labeling, we also use
the exponential moving average of the classifier. However, because the
classifier might still make mistakes, especially at the beginning of the
training, we also refine the labels through self-attention, by using the
labeling of real data samples only when the classifier outputs a high
classification probability score. We evaluate our approach on CIFAR-10, STL-10
and SVHN, and show that both self-labeling and self-attention consistently
improve the quality of generated data. More surprisingly, we find that the
proposed scheme can even outperform class-conditional GANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1"&gt;Tomoki Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1"&gt;Paolo Favaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradual Domain Adaptation via Self-Training of Auxiliary Models. (arXiv:2106.09890v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09890</id>
        <link href="http://arxiv.org/abs/2106.09890"/>
        <updated>2021-06-21T02:07:38.921Z</updated>
        <summary type="html"><![CDATA[Domain adaptation becomes more challenging with increasing gaps between
source and target domains. Motivated from an empirical analysis on the
reliability of labeled source data for the use of distancing target domains, we
propose self-training of auxiliary models (AuxSelfTrain) that learns models for
intermediate domains and gradually combats the distancing shifts across
domains. We introduce evolving intermediate domains as combinations of
decreasing proportion of source data and increasing proportion of target data,
which are sampled to minimize the domain distance between consecutive domains.
Then the source model could be gradually adapted for the use in the target
domain by self-training of auxiliary models on evolving intermediate domains.
We also introduce an enhanced indicator for sample selection via implicit
ensemble and extend the proposed method to semi-supervised domain adaptation.
Experiments on benchmark datasets of unsupervised and semi-supervised domain
adaptation verify its efficacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yabin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1"&gt;Bin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Many Agent Reinforcement Learning Under Partial Observability. (arXiv:2106.09825v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09825</id>
        <link href="http://arxiv.org/abs/2106.09825"/>
        <updated>2021-06-21T02:07:38.914Z</updated>
        <summary type="html"><![CDATA[Recent renewed interest in multi-agent reinforcement learning (MARL) has
generated an impressive array of techniques that leverage deep reinforcement
learning, primarily actor-critic architectures, and can be applied to a limited
range of settings in terms of observability and communication. However, a
continuing limitation of much of this work is the curse of dimensionality when
it comes to representations based on joint actions, which grow exponentially
with the number of agents. In this paper, we squarely focus on this challenge
of scalability. We apply the key insight of action anonymity, which leads to
permutation invariance of joint actions, to two recently presented deep MARL
algorithms, MADDPG and IA2C, and compare these instantiations to another recent
technique that leverages action anonymity, viz., mean-field MARL. We show that
our instantiations can learn the optimal behavior in a broader class of agent
networks than the mean-field method, using a recently introduced pragmatic
domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1"&gt;Keyang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1"&gt;Prashant Doshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1"&gt;Bikramjit Banerjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[All You Can Embed: Natural Language based Vehicle Retrieval with Spatio-Temporal Transformers. (arXiv:2106.10153v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10153</id>
        <link href="http://arxiv.org/abs/2106.10153"/>
        <updated>2021-06-21T02:07:38.907Z</updated>
        <summary type="html"><![CDATA[Combining Natural Language with Vision represents a unique and interesting
challenge in the domain of Artificial Intelligence. The AI City Challenge Track
5 for Natural Language-Based Vehicle Retrieval focuses on the problem of
combining visual and textual information, applied to a smart-city use case. In
this paper, we present All You Can Embed (AYCE), a modular solution to
correlate single-vehicle tracking sequences with natural language. The main
building blocks of the proposed architecture are (i) BERT to provide an
embedding of the textual descriptions, (ii) a convolutional backbone along with
a Transformer model to embed the visual information. For the training of the
retrieval model, a variation of the Triplet Margin Loss is proposed to learn a
distance measure between the visual and language embeddings. The code is
publicly available at https://github.com/cscribano/AYCE_2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scribano_C/0/1/0/all/0/1"&gt;Carmelo Scribano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sapienza_D/0/1/0/all/0/1"&gt;Davide Sapienza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franchini_G/0/1/0/all/0/1"&gt;Giorgia Franchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verucchi_M/0/1/0/all/0/1"&gt;Micaela Verucchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertogna_M/0/1/0/all/0/1"&gt;Marko Bertogna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic COVID-19 Chest X-ray Dataset for Computer-Aided Diagnosis. (arXiv:2106.09759v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09759</id>
        <link href="http://arxiv.org/abs/2106.09759"/>
        <updated>2021-06-21T02:07:38.900Z</updated>
        <summary type="html"><![CDATA[We introduce a new dataset called Synthetic COVID-19 Chest X-ray Dataset for
training machine learning models. The dataset consists of 21,295 synthetic
COVID-19 chest X-ray images to be used for computer-aided diagnosis. These
images, generated via an unsupervised domain adaptation approach, are of high
quality. We find that the synthetic images not only improve performance of
various deep learning architectures when used as additional training data under
heavy imbalance conditions, but also detect the target class with high
confidence. We also find that comparable performance can also be achieved when
trained only on synthetic images. Further, salient features of the synthetic
COVID-19 images indicate that the distribution is significantly different from
Non-COVID-19 classes, enabling a proper decision boundary. We hope the
availability of such high fidelity chest X-ray images of COVID-19 will
encourage advances in the development of diagnostic and/or management tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zunair_H/0/1/0/all/0/1"&gt;Hasib Zunair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hamza_A/0/1/0/all/0/1"&gt;A. Ben Hamza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. (arXiv:2106.10270v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10270</id>
        <link href="http://arxiv.org/abs/2106.10270"/>
        <updated>2021-06-21T02:07:38.882Z</updated>
        <summary type="html"><![CDATA[Vision Transformers (ViT) have been shown to attain highly competitive
performance for a wide range of vision applications, such as image
classification, object detection and semantic image segmentation. In comparison
to convolutional neural networks, the Vision Transformer's weaker inductive
bias is generally found to cause an increased reliance on model regularization
or data augmentation (``AugReg'' for short) when training on smaller training
datasets. We conduct a systematic empirical study in order to better understand
the interplay between the amount of training data, AugReg, model size and
compute budget. As one result of this study we find that the combination of
increased compute and AugReg can yield models with the same performance as
models trained on an order of magnitude more training data: we train ViT models
of various sizes on the public ImageNet-21k dataset which either match or
outperform their counterparts trained on the larger, but not publicly available
JFT-300M dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1"&gt;Andreas Steiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wightman_R/0/1/0/all/0/1"&gt;Ross Wightman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1"&gt;Jakob Uszkoreit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batch Multi-Fidelity Bayesian Optimization with Deep Auto-Regressive Networks. (arXiv:2106.09884v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09884</id>
        <link href="http://arxiv.org/abs/2106.09884"/>
        <updated>2021-06-21T02:07:38.875Z</updated>
        <summary type="html"><![CDATA[Bayesian optimization (BO) is a powerful approach for optimizing black-box,
expensive-to-evaluate functions. To enable a flexible trade-off between the
cost and accuracy, many applications allow the function to be evaluated at
different fidelities. In order to reduce the optimization cost while maximizing
the benefit-cost ratio, in this paper, we propose Batch Multi-fidelity Bayesian
Optimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of
Bayesian neural networks to construct a fully auto-regressive model, which is
expressive enough to capture strong yet complex relationships across all the
fidelities, so as to improve the surrogate learning and optimization
performance. Furthermore, to enhance the quality and diversity of queries, we
develop a simple yet efficient batch querying method, without any combinatorial
search over the fidelities. We propose a batch acquisition function based on
Max-value Entropy Search (MES) principle, which penalizes highly correlated
queries and encourages diversity. We use posterior samples and moment matching
to fulfill efficient computation of the acquisition function and conduct
alternating optimization over every fidelity-input pair, which guarantees an
improvement at each step. We demonstrate the advantage of our approach on four
real-world hyperparameter optimization applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shibo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirby_R/0/1/0/all/0/1"&gt;Robert M. Kirby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhe_S/0/1/0/all/0/1"&gt;Shandian Zhe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LSEC: Large-scale spectral ensemble clustering. (arXiv:2106.09852v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09852</id>
        <link href="http://arxiv.org/abs/2106.09852"/>
        <updated>2021-06-21T02:07:38.868Z</updated>
        <summary type="html"><![CDATA[Ensemble clustering is a fundamental problem in the machine learning field,
combining multiple base clusterings into a better clustering result. However,
most of the existing methods are unsuitable for large-scale ensemble clustering
tasks due to the efficiency bottleneck. In this paper, we propose a large-scale
spectral ensemble clustering (LSEC) method to strike a good balance between
efficiency and effectiveness. In LSEC, a large-scale spectral clustering based
efficient ensemble generation framework is designed to generate various base
clusterings within a low computational complexity. Then all based clustering
are combined through a bipartite graph partition based consensus function into
a better consensus clustering result. The LSEC method achieves a lower
computational complexity than most existing ensemble clustering methods.
Experiments conducted on ten large-scale datasets show the efficiency and
effectiveness of the LSEC method. The MATLAB code of the proposed method and
experimental datasets are available at https://github.com/Li-
Hongmin/MyPaperWithCode.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongmin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiucai Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imakura_A/0/1/0/all/0/1"&gt;Akira Imakura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakurai_T/0/1/0/all/0/1"&gt;Tetsuya Sakurai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machining Cycle Time Prediction: Data-driven Modelling of Machine Tool Feedrate Behavior with Neural Networks. (arXiv:2106.09719v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09719</id>
        <link href="http://arxiv.org/abs/2106.09719"/>
        <updated>2021-06-21T02:07:38.862Z</updated>
        <summary type="html"><![CDATA[Accurate prediction of machining cycle times is important in the
manufacturing industry. Usually, Computer Aided Manufacturing (CAM) software
estimates the machining times using the commanded feedrate from the toolpath
file using basic kinematic settings. Typically, the methods do not account for
toolpath geometry or toolpath tolerance and therefore under estimate the
machining cycle times considerably. Removing the need for machine specific
knowledge, this paper presents a data-driven feedrate and machining cycle time
prediction method by building a neural network model for each machine tool
axis. In this study, datasets composed of the commanded feedrate, nominal
acceleration, toolpath geometry and the measured feedrate were used to train a
neural network model. Validation trials using a representative industrial thin
wall structure component on a commercial machining centre showed that this
method estimated the machining time with more than 90% accuracy. This method
showed that neural network models have the capability to learn the behavior of
a complex machine tool system and predict cycle times. Further integration of
the methods will be critical in the implantation of digital twins in Industry
4.0.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dominguez_Caballero_J/0/1/0/all/0/1"&gt;Javier Dominguez-Caballero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ward_R/0/1/0/all/0/1"&gt;Rob Ward&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayvar_Soberanis_S/0/1/0/all/0/1"&gt;Sabino Ayvar-Soberanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Curtis_D/0/1/0/all/0/1"&gt;David Curtis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC Prediction Sets Under Covariate Shift. (arXiv:2106.09848v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09848</id>
        <link href="http://arxiv.org/abs/2106.09848"/>
        <updated>2021-06-21T02:07:38.855Z</updated>
        <summary type="html"><![CDATA[An important challenge facing modern machine learning is how to rigorously
quantify the uncertainty of model predictions. Conveying uncertainty is
especially important when there are changes to the underlying data distribution
that might invalidate the predictive model. Yet, most existing uncertainty
quantification algorithms break down in the presence of such shifts. We propose
a novel approach that addresses this challenge by constructing \emph{probably
approximately correct (PAC)} prediction sets in the presence of covariate
shift. Our approach focuses on the setting where there is a covariate shift
from the source distribution (where we have labeled training examples) to the
target distribution (for which we want to quantify uncertainty). Our algorithm
assumes given importance weights that encode how the probabilities of the
training examples change under the covariate shift. In practice, importance
weights typically need to be estimated; thus, we extend our algorithm to the
setting where we are given confidence intervals for the importance weights
rather than their true value. We demonstrate the effectiveness of our approach
on various covariate shifts designed based on the DomainNet and ImageNet
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sangdon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1"&gt;Edgar Dobriban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1"&gt;Insup Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1"&gt;Osbert Bastani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-free optimization of chaotic acoustics with reservoir computing. (arXiv:2106.09780v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2106.09780</id>
        <link href="http://arxiv.org/abs/2106.09780"/>
        <updated>2021-06-21T02:07:38.849Z</updated>
        <summary type="html"><![CDATA[We develop a versatile optimization method, which finds the design parameters
that minimize time-averaged acoustic cost functionals. The method is
gradient-free, model-informed, and data-driven with reservoir computing based
on echo state networks. First, we analyse the predictive capabilities of echo
state networks both in the short- and long-time prediction of the dynamics. We
find that both fully data-driven and model-informed architectures learn the
chaotic acoustic dynamics, both time-accurately and statistically. Informing
the training with a physical reduced-order model with one acoustic mode
markedly improves the accuracy and robustness of the echo state networks,
whilst keeping the computational cost low. Echo state networks offer accurate
predictions of the long-time dynamics, which would be otherwise expensive by
integrating the governing equations to evaluate the time-averaged quantity to
optimize. Second, we couple echo state networks with a Bayesian technique to
explore the design thermoacoustic parameter space. The computational method is
minimally intrusive. Third, we find the set of flame parameters that minimize
the time-averaged acoustic energy of chaotic oscillations, which are caused by
the positive feedback with a heat source, such as a flame in gas turbines or
rocket motors. These oscillations are known as thermoacoustic oscillations. The
optimal set of flame parameters is found with the same accuracy as brute-force
grid search, but with a convergence rate that is more than one order of
magnitude faster. This work opens up new possibilities for non-intrusive
(``hands-off'') optimization of chaotic systems, in which the cost of
generating data, for example from high-fidelity simulations and experiments, is
high.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Huhn_F/0/1/0/all/0/1"&gt;Francisco Huhn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Magri_L/0/1/0/all/0/1"&gt;Luca Magri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Resource Allocation with Graph Neural Networks. (arXiv:2106.09761v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09761</id>
        <link href="http://arxiv.org/abs/2106.09761"/>
        <updated>2021-06-21T02:07:38.831Z</updated>
        <summary type="html"><![CDATA[We present an approach for maximizing a global utility function by learning
how to allocate resources in an unsupervised way. We expect interactions
between allocation targets to be important and therefore propose to learn the
reward structure for near-optimal allocation policies with a GNN. By relaxing
the resource constraint, we can employ gradient-based optimization in contrast
to more standard evolutionary algorithms. Our algorithm is motivated by a
problem in modern astronomy, where one needs to select-based on limited initial
information-among $10^9$ galaxies those whose detailed measurement will lead to
optimal inference of the composition of the universe. Our technique presents a
way of flexibly learning an allocation strategy by only requiring forward
simulators for the physics of interest and the measurement process. We
anticipate that our technique will also find applications in a range of
resource allocation problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cranmer_M/0/1/0/all/0/1"&gt;Miles Cranmer&lt;/a&gt; (Princeton), &lt;a href="http://arxiv.org/find/cs/1/au:+Melchior_P/0/1/0/all/0/1"&gt;Peter Melchior&lt;/a&gt; (Princeton), &lt;a href="http://arxiv.org/find/cs/1/au:+Nord_B/0/1/0/all/0/1"&gt;Brian Nord&lt;/a&gt; (Fermilab)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Iterative Feature Matching: Toward Provable Domain Generalization with Logarithmic Environments. (arXiv:2106.09913v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09913</id>
        <link href="http://arxiv.org/abs/2106.09913"/>
        <updated>2021-06-21T02:07:38.824Z</updated>
        <summary type="html"><![CDATA[Domain generalization aims at performing well on unseen test environments
with data from a limited number of training environments. Despite a
proliferation of proposal algorithms for this task, assessing their
performance, both theoretically and empirically is still very challenging.
Moreover, recent approaches such as Invariant Risk Minimization (IRM) require a
prohibitively large number of training environments - linear in the dimension
of the spurious feature space $d_s$ - even on simple data models like the one
proposed by [Rosenfeld et al., 2021]. Under a variant of this model, we show
that both ERM and IRM cannot generalize with $o(d_s)$ environments. We then
present a new algorithm based on performing iterative feature matching that is
guaranteed with high probability to yield a predictor that generalizes after
seeing only $O(\log{d_s})$ environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yining Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenfeld_E/0/1/0/all/0/1"&gt;Elan Rosenfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1"&gt;Mark Sellke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1"&gt;Andrej Risteski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided Integrated Gradients: An Adaptive Path Method for Removing Noise. (arXiv:2106.09788v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09788</id>
        <link href="http://arxiv.org/abs/2106.09788"/>
        <updated>2021-06-21T02:07:38.816Z</updated>
        <summary type="html"><![CDATA[Integrated Gradients (IG) is a commonly used feature attribution method for
deep neural networks. While IG has many desirable properties, the method often
produces spurious/noisy pixel attributions in regions that are not related to
the predicted class when applied to visual models. While this has been
previously noted, most existing solutions are aimed at addressing the symptoms
by explicitly reducing the noise in the resulting attributions. In this work,
we show that one of the causes of the problem is the accumulation of noise
along the IG path. To minimize the effect of this source of noise, we propose
adapting the attribution path itself -- conditioning the path not just on the
image but also on the model being explained. We introduce Adaptive Path Methods
(APMs) as a generalization of path methods, and Guided IG as a specific
instance of an APM. Empirically, Guided IG creates saliency maps better aligned
with the model's prediction and the input image that is being explained. We
show through qualitative and quantitative experiments that Guided IG
outperforms other, related methods in nearly every experiment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kapishnikov_A/0/1/0/all/0/1"&gt;Andrei Kapishnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1"&gt;Subhashini Venugopalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avci_B/0/1/0/all/0/1"&gt;Besim Avci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wedin_B/0/1/0/all/0/1"&gt;Ben Wedin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1"&gt;Michael Terry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1"&gt;Tolga Bolukbasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Delving Deep into the Generalization of Vision Transformers under Distribution Shifts. (arXiv:2106.07617v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07617</id>
        <link href="http://arxiv.org/abs/2106.07617"/>
        <updated>2021-06-21T02:07:38.810Z</updated>
        <summary type="html"><![CDATA[Recently, Vision Transformers (ViTs) have achieved impressive results on
various vision tasks. Yet, their generalization ability under different
distribution shifts is rarely understood. In this work, we provide a
comprehensive study on the out-of-distribution generalization of ViTs. To
support a systematic investigation, we first present a taxonomy of distribution
shifts by categorizing them into five conceptual groups: corruption shift,
background shift, texture shift, destruction shift, and style shift. Then we
perform extensive evaluations of ViT variants under different groups of
distribution shifts and compare their generalization ability with CNNs. Several
important observations are obtained: 1) ViTs generalize better than CNNs under
multiple distribution shifts. With the same or fewer parameters, ViTs are ahead
of corresponding CNNs by more than 5% in top-1 accuracy under most distribution
shifts. 2) Larger ViTs gradually narrow the in-distribution and
out-of-distribution performance gap. To further improve the generalization of
ViTs, we design the Generalization-Enhanced ViTs by integrating adversarial
learning, information theory, and self-supervised learning. By investigating
three types of generalization-enhanced ViTs, we observe their
gradient-sensitivity and design a smoother learning strategy to achieve a
stable training process. With modified training schemes, we achieve
improvements on performance towards out-of-distribution data by 4% from vanilla
ViTs. We comprehensively compare three generalization-enhanced ViTs with their
corresponding CNNs, and observe that: 1) For the enhanced model, larger ViTs
still benefit more for the out-of-distribution generalization. 2)
generalization-enhanced ViTs are more sensitive to the hyper-parameters than
corresponding CNNs. We hope our comprehensive study could shed light on the
design of more generalizable learning architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongzhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mingyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shanghang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1"&gt;Daisheng Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qiang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhongang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Haiyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1"&gt;Shuai Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianglong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Deep Hashing Methods. (arXiv:2003.03369v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.03369</id>
        <link href="http://arxiv.org/abs/2003.03369"/>
        <updated>2021-06-21T02:07:38.792Z</updated>
        <summary type="html"><![CDATA[Nearest neighbor search is to find the data points in the database such that
the distances from them to the query are the smallest, which is a fundamental
problem in various domains, such as computer vision, recommendation systems and
machine learning. Hashing is one of the most widely used methods for its
computational and storage efficiency. With the development of deep learning,
deep hashing methods show more advantages than traditional methods. In this
paper, we present a comprehensive survey of the deep hashing algorithms.
Specifically, we categorize deep supervised hashing methods into pairwise
similarity preserving, multiwise similarity preserving, implicit similarity
preserving, classification-oriented preserving as well as quantization
according to the manners of preserving the similarities. In addition, we also
introduce some other topics such as deep unsupervised hashing and multi-modal
deep hashing methods. Meanwhile, we also present some commonly used public
datasets and the scheme to measure the performance of deep hashing algorithms.
Finally, we discussed some potential research directions in conclusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Daqing Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1"&gt;Minghua Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xian-Sheng Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10013</id>
        <link href="http://arxiv.org/abs/2105.10013"/>
        <updated>2021-06-21T02:07:38.785Z</updated>
        <summary type="html"><![CDATA[Image classification methods are usually trained to perform predictions
taking into account a predefined group of known classes. Real-world problems,
however, may not allow for a full knowledge of the input and label spaces,
making failures in recognition a hazard to deep visual learning. Open set
recognition methods are characterized by the ability to correctly identifying
inputs of known and unknown classes. In this context, we propose GeMOS: simple
and plug-and-play open set recognition modules that can be attached to
pretrained Deep Neural Networks for visual recognition. The GeMOS framework
pairs pre-trained Convolutional Neural Networks with generative models for open
set recognition to extract open set scores for each sample, allowing for
failure recognition in object recognition tasks. We conduct a thorough
evaluation of the proposed method in comparison with state-of-the-art open set
algorithms, finding that GeMOS either outperforms or is statistically
indistinguishable from more complex and costly models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1"&gt;Marcos Vendramini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1"&gt;Hugo Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1"&gt;Alexei Machado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Consensual Collaborative Learning Method for Remote Sensing Image Classification Under Noisy Multi-Labels. (arXiv:2105.05496v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05496</id>
        <link href="http://arxiv.org/abs/2105.05496"/>
        <updated>2021-06-21T02:07:38.778Z</updated>
        <summary type="html"><![CDATA[Collecting a large number of reliable training images annotated by multiple
land-cover class labels in the framework of multi-label classification is
time-consuming and costly in remote sensing (RS). To address this problem,
publicly available thematic products are often used for annotating RS images
with zero-labeling-cost. However, such an approach may result in constructing a
training set with noisy multi-labels, distorting the learning process. To
address this problem, we propose a Consensual Collaborative Multi-Label
Learning (CCML) method. The proposed CCML identifies, ranks and corrects
training images with noisy multi-labels through four main modules: 1)
discrepancy module; 2) group lasso module; 3) flipping module; and 4) swap
module. The discrepancy module ensures that the two networks learn diverse
features, while obtaining the same predictions. The group lasso module detects
the potentially noisy labels by estimating the label uncertainty based on the
aggregation of two collaborative networks. The flipping module corrects the
identified noisy labels, whereas the swap module exchanges the ranking
information between the two networks. The experimental results confirm the
success of the proposed CCML under high (synthetically added) multi-label noise
rates. The code of the proposed method is publicly available at
https://noisy-labels-in-rs.org]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aksoy_A/0/1/0/all/0/1"&gt;Ahmet Kerem Aksoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravanbakhsh_M/0/1/0/all/0/1"&gt;Mahdyar Ravanbakhsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreuziger_T/0/1/0/all/0/1"&gt;Tristan Kreuziger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1"&gt;Begum Demir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoencoder-based cleaning in probabilistic databases. (arXiv:2106.09764v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2106.09764</id>
        <link href="http://arxiv.org/abs/2106.09764"/>
        <updated>2021-06-21T02:07:38.763Z</updated>
        <summary type="html"><![CDATA[In the field of data integration, data quality problems are often encountered
when extracting, combining, and merging data. The probabilistic data
integration approach represents information about such problems as
uncertainties in a probabilistic database. In this paper, we propose a
data-cleaning autoencoder capable of near-automatic data quality improvement.
It learns the structure and dependencies in the data to identify and correct
doubtful values. A theoretical framework is provided, and experiments show that
it can remove significant amounts of noise from categorical and numeric
probabilistic data. Our method does not require clean data. We do, however,
show that manually cleaning a small fraction of the data significantly improves
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mauritz_R/0/1/0/all/0/1"&gt;R.R. Mauritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nijweide_F/0/1/0/all/0/1"&gt;F.P.J. Nijweide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goseling_J/0/1/0/all/0/1"&gt;J. Goseling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keulen_M/0/1/0/all/0/1"&gt;M. van Keulen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields. (arXiv:2106.05187v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05187</id>
        <link href="http://arxiv.org/abs/2106.05187"/>
        <updated>2021-06-21T02:07:38.756Z</updated>
        <summary type="html"><![CDATA[We present implicit displacement fields, a novel representation for detailed
3D geometry. Inspired by a classic surface deformation technique, displacement
mapping, our method represents a complex surface as a smooth base surface plus
a displacement along the base's normal directions, resulting in a
frequency-based shape decomposition, where the high frequency signal is
constrained geometrically by the low frequency signal. Importantly, this
disentanglement is unsupervised thanks to a tailored architectural design that
has an innate frequency hierarchy by construction. We explore implicit
displacement field surface reconstruction and detail transfer and demonstrate
superior representational power, training stability and generalizability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1"&gt;Wang Yifan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmann_L/0/1/0/all/0/1"&gt;Lukas Rahmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1"&gt;Olga Sorkine-Hornung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Temporal Action Detection with Transformer. (arXiv:2106.10271v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10271</id>
        <link href="http://arxiv.org/abs/2106.10271"/>
        <updated>2021-06-21T02:07:38.741Z</updated>
        <summary type="html"><![CDATA[Temporal action detection (TAD) aims to determine the semantic label and the
boundaries of every action instance in an untrimmed video. It is a fundamental
task in video understanding and significant progress has been made in TAD.
Previous methods involve multiple stages or networks and hand-designed rules or
operations, which fall short in efficiency and flexibility. Here, we construct
an end-to-end framework for TAD upon Transformer, termed \textit{TadTR}, which
simultaneously predicts all action instances as a set of labels and temporal
locations in parallel. TadTR is able to adaptively extract temporal context
information needed for making action predictions, by selectively attending to a
number of snippets in a video. It greatly simplifies the pipeline of TAD and
runs much faster than previous detectors. Our method achieves state-of-the-art
performance on HACS Segments and THUMOS14 and competitive performance on
ActivityNet-1.3. Our code will be made available at
\url{https://github.com/xlliu7/TadTR}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaolong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qimeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1"&gt;Song Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiang Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning of Generalized Game Representations. (arXiv:2106.10060v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10060</id>
        <link href="http://arxiv.org/abs/2106.10060"/>
        <updated>2021-06-21T02:07:38.710Z</updated>
        <summary type="html"><![CDATA[Representing games through their pixels offers a promising approach for
building general-purpose and versatile game models. While games are not merely
images, neural network models trained on game pixels often capture differences
of the visual style of the image rather than the content of the game. As a
result, such models cannot generalize well even within similar games of the
same genre. In this paper we build on recent advances in contrastive learning
and showcase its benefits for representation learning in games. Learning to
contrast images of games not only classifies games in a more efficient manner;
it also yields models that separate games in a more meaningful fashion by
ignoring the visual style and focusing, instead, on their content. Our results
in a large dataset of sports video games containing 100k images across 175
games and 10 game genres suggest that contrastive learning is better suited for
learning generalized game representations compared to conventional supervised
learning. The findings of this study bring us closer to universal visual
encoders for games that can be reused across previously unseen games without
requiring retraining or fine-tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_C/0/1/0/all/0/1"&gt;Chintan Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1"&gt;Antonios Liapis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1"&gt;Georgios N. Yannakakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Relationships between Object Categories via Universal Canonical Maps. (arXiv:2106.09758v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09758</id>
        <link href="http://arxiv.org/abs/2106.09758"/>
        <updated>2021-06-21T02:07:38.680Z</updated>
        <summary type="html"><![CDATA[We tackle the problem of learning the geometry of multiple categories of
deformable objects jointly. Recent work has shown that it is possible to learn
a unified dense pose predictor for several categories of related objects.
However, training such models requires to initialize inter-category
correspondences by hand. This is suboptimal and the resulting models fail to
maintain correct correspondences as individual categories are learned. In this
paper, we show that improved correspondences can be learned automatically as a
natural byproduct of learning category-specific dense pose predictors. To do
this, we express correspondences between different categories and between
images and categories using a unified embedding. Then, we use the latter to
enforce two constraints: symmetric inter-category cycle consistency and a new
asymmetric image-to-category cycle consistency. Without any manual annotations
for the inter-category correspondences, we obtain state-of-the-art alignment
results, outperforming dedicated methods for matching 3D shapes. Moreover, the
new model is also better at the task of dense pose prediction than prior work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1"&gt;Natalia Neverova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanakoyeu_A/0/1/0/all/0/1"&gt;Artsiom Sanakoyeu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labatut_P/0/1/0/all/0/1"&gt;Patrick Labatut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1"&gt;David Novotny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1"&gt;Andrea Vedaldi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Meshing from Deep Implicit Surface Networks Using an Efficient Implementation of Analytic Marching. (arXiv:2106.10031v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10031</id>
        <link href="http://arxiv.org/abs/2106.10031"/>
        <updated>2021-06-21T02:07:38.672Z</updated>
        <summary type="html"><![CDATA[Reconstruction of object or scene surfaces has tremendous applications in
computer vision, computer graphics, and robotics. In this paper, we study a
fundamental problem in this context about recovering a surface mesh from an
implicit field function whose zero-level set captures the underlying surface.
To achieve the goal, existing methods rely on traditional meshing algorithms;
while promising, they suffer from loss of precision learned in the implicit
surface networks, due to the use of discrete space sampling in marching cubes.
Given that an MLP with activations of Rectified Linear Unit (ReLU) partitions
its input space into a number of linear regions, we are motivated to connect
this local linearity with a same property owned by the desired result of
polygon mesh. More specifically, we identify from the linear regions,
partitioned by an MLP based implicit function, the analytic cells and analytic
faces that are associated with the function's zero-level isosurface. We prove
that under mild conditions, the identified analytic faces are guaranteed to
connect and form a closed, piecewise planar surface. Based on the theorem, we
propose an algorithm of analytic marching, which marches among analytic cells
to exactly recover the mesh captured by an implicit surface network. We also
show that our theory and algorithm are equally applicable to advanced MLPs with
shortcut connections and max pooling. Given the parallel nature of analytic
marching, we contribute AnalyticMesh, a software package that supports
efficient meshing of implicit surface networks via CUDA parallel computing, and
mesh simplification for efficient downstream processing. We apply our method to
different settings of generative shape modeling using implicit surface
networks. Extensive experiments demonstrate our advantages over existing
methods in terms of both meshing accuracy and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jiabao Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Virtual Temporal Samples for Recurrent Neural Networks: applied to semantic segmentation in agriculture. (arXiv:2106.10118v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10118</id>
        <link href="http://arxiv.org/abs/2106.10118"/>
        <updated>2021-06-21T02:07:38.655Z</updated>
        <summary type="html"><![CDATA[This paper explores the potential for performing temporal semantic
segmentation in the context of agricultural robotics without temporally
labelled data. We achieve this by proposing to generate virtual temporal
samples from labelled still images. This allows us, with no extra annotation
effort, to generate virtually labelled temporal sequences. Normally, to train a
recurrent neural network (RNN), labelled samples from a video (temporal)
sequence are required which is laborious and has stymied work in this
direction. By generating virtual temporal samples, we demonstrate that it is
possible to train a lightweight RNN to perform semantic segmentation on two
challenging agricultural datasets. Our results show that by training a temporal
semantic segmenter using virtual samples we can increase the performance by an
absolute amount of 4.6 and 4.9 on sweet pepper and sugar beet datasets,
respectively. This indicates that our virtual data augmentation technique is
able to accurately classify agricultural images temporally without the use of
complicated synthetic data generation techniques nor with the overhead of
labelling large amounts of temporal sequences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmadi_A/0/1/0/all/0/1"&gt;Alireza Ahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halstead_M/0/1/0/all/0/1"&gt;Michael Halstead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCool_C/0/1/0/all/0/1"&gt;Chris McCool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Novelty Detection via Contrastive Learning with Negative Data Augmentation. (arXiv:2106.09958v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09958</id>
        <link href="http://arxiv.org/abs/2106.09958"/>
        <updated>2021-06-21T02:07:38.648Z</updated>
        <summary type="html"><![CDATA[Novelty detection is the process of determining whether a query example
differs from the learned training distribution. Previous methods attempt to
learn the representation of the normal samples via generative adversarial
networks (GANs). However, they will suffer from instability training, mode
dropping, and low discriminative ability. Recently, various pretext tasks (e.g.
rotation prediction and clustering) have been proposed for self-supervised
learning in novelty detection. However, the learned latent features are still
low discriminative. We overcome such problems by introducing a novel
decoder-encoder framework. Firstly, a generative network (a.k.a. decoder)
learns the representation by mapping the initialized latent vector to an image.
In particular, this vector is initialized by considering the entire
distribution of training data to avoid the problem of mode-dropping. Secondly,
a contrastive network (a.k.a. encoder) aims to ``learn to compare'' through
mutual information estimation, which directly helps the generative network to
obtain a more discriminative representation by using a negative data
augmentation strategy. Extensive experiments show that our model has
significant superiority over cutting-edge novelty detectors and achieves new
state-of-the-art results on some novelty detection benchmarks, e.g. CIFAR10 and
DCASE. Moreover, our model is more stable for training in a non-adversarial
manner, compared to other adversarial based novelty detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chengwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shaohui Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1"&gt;Ruizhi Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jian Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xin Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lizhuang Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Contrastive Learning for Joint Demosaicking and Denoising. (arXiv:2106.10070v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10070</id>
        <link href="http://arxiv.org/abs/2106.10070"/>
        <updated>2021-06-21T02:07:38.640Z</updated>
        <summary type="html"><![CDATA[The breakthrough of contrastive learning (CL) has fueled the recent success
of self-supervised learning (SSL) in high-level vision tasks on RGB images.
However, CL is still ill-defined for low-level vision tasks, such as joint
demosaicking and denoising (JDD), in the RAW domain. To bridge this
methodological gap, we present a novel CL approach on RAW images, residual
contrastive learning (RCL), which aims to learn meaningful representations for
JDD. Our work is built on the assumption that noise contained in each RAW image
is signal-dependent, thus two crops from the same RAW image should have more
similar noise distribution than two crops from different RAW images. We use
residuals as a discriminative feature and the earth mover's distance to measure
the distribution divergence for the contrastive loss. To evaluate the proposed
CL strategy, we simulate a series of unsupervised JDD experiments with
large-scale data corrupted by synthetic signal-dependent noise, where we set a
new benchmark for unsupervised JDD tasks with unknown (random) noise variance.
Our empirical study not only validates that CL can be applied on distributions
(c.f. features), but also exposes the lack of robustness of previous non-ML and
SSL JDD methods when the statistics of the noise are unknown, thus providing
some further insight into signal-dependent noise problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1"&gt;Nanqing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maggioni_M/0/1/0/all/0/1"&gt;Matteo Maggioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1"&gt;Eduardo P&amp;#xe9;rez-Pellitero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ales Leonardis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1"&gt;Steven McDonagh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Longitudinal Neighbourhood Embedding. (arXiv:2103.03840v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03840</id>
        <link href="http://arxiv.org/abs/2103.03840"/>
        <updated>2021-06-21T02:07:38.633Z</updated>
        <summary type="html"><![CDATA[Longitudinal MRIs are often used to capture the gradual deterioration of
brain structure and function caused by aging or neurological diseases.
Analyzing this data via machine learning generally requires a large number of
ground-truth labels, which are often missing or expensive to obtain. Reducing
the need for labels, we propose a self-supervised strategy for representation
learning named Longitudinal Neighborhood Embedding (LNE). Motivated by concepts
in contrastive learning, LNE explicitly models the similarity between
trajectory vectors across different subjects. We do so by building a graph in
each training iteration defining neighborhoods in the latent space so that the
progression direction of a subject follows the direction of its neighbors. This
results in a smooth trajectory field that captures the global morphological
change of the brain while maintaining the local continuity. We apply LNE to
longitudinal T1w MRIs of two neuroimaging studies: a dataset composed of 274
healthy subjects, and Alzheimer's Disease Neuroimaging Initiative (ADNI,
N=632). The visualization of the smooth trajectory vector field and superior
performance on downstream tasks demonstrate the strength of the proposed method
over existing self-supervised methods in extracting information associated with
normal aging and in revealing the impact of neurodegenerative disorders. The
code is available at
\url{https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding.git}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1"&gt;Jiahong Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1"&gt;Edith V Sullivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1"&gt;Adolf Pfefferbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaharchuk_G/0/1/0/all/0/1"&gt;Greg Zaharchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M Pohl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation Studies: A Review. (arXiv:2106.09862v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09862</id>
        <link href="http://arxiv.org/abs/2106.09862"/>
        <updated>2021-06-21T02:07:38.626Z</updated>
        <summary type="html"><![CDATA[Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is commonly
used to visualize and quantify left atrial (LA) scars. The position and extent
of scars provide important information of the pathophysiology and progression
of atrial fibrillation (AF). Hence, LA scar segmentation and quantification
from LGE MRI can be useful in computer-assisted diagnosis and treatment
stratification of AF patients. Since manual delineation can be time-consuming
and subject to intra- and inter-expert variability, automating this computing
is highly desired, which nevertheless is still challenging and
under-researched.

This paper aims to provide a systematic review on computing methods for LA
cavity, wall, scar and ablation gap segmentation and quantification from LGE
MRI, and the related literature for AF studies. Specifically, we first
summarize AF-related imaging techniques, particularly LGE MRI. Then, we review
the methodologies of the four computing tasks in detail, and summarize the
validation strategies applied in each task. Finally, the possible future
developments are outlined, with a brief survey on the potential clinical
applications of the aforementioned methods. The review shows that the research
into this topic is still in early stages. Although several methods have been
proposed, especially for LA segmentation, there is still large scope for
further algorithmic developments due to performance issues related to the high
variability of enhancement appearance and differences in image acquisition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_V/0/1/0/all/0/1"&gt;Veronika A. Zimmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1"&gt;Julia A. Schnabel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiahai Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay. (arXiv:2106.09835v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09835</id>
        <link href="http://arxiv.org/abs/2106.09835"/>
        <updated>2021-06-21T02:07:38.617Z</updated>
        <summary type="html"><![CDATA[This paper proposes two novel knowledge transfer techniques for
class-incremental learning (CIL). First, we propose data-free generative replay
(DF-GR) to mitigate catastrophic forgetting in CIL by using synthetic samples
from a generative model. In the conventional generative replay, the generative
model is pre-trained for old data and shared in extra memory for later
incremental learning. In our proposed DF-GR, we train a generative model from
scratch without using any training data, based on the pre-trained
classification model from the past, so we curtail the cost of sharing
pre-trained generative models. Second, we introduce dual-teacher information
distillation (DT-ID) for knowledge distillation from two teachers to one
student. In CIL, we use DT-ID to learn new classes incrementally based on the
pre-trained model for old classes and another model (pre-)trained on the new
data for new classes. We implemented the proposed schemes on top of one of the
state-of-the-art CIL methods and showed the performance improvement on
CIFAR-100 and ImageNet datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yoojin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Khamy_M/0/1/0/all/0/1"&gt;Mostafa El-Khamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jungwon Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Video Representation Learning with Cross-Stream Prototypical Contrasting. (arXiv:2106.10137v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10137</id>
        <link href="http://arxiv.org/abs/2106.10137"/>
        <updated>2021-06-21T02:07:38.609Z</updated>
        <summary type="html"><![CDATA[Instance-level contrastive learning techniques, which rely on data
augmentation and a contrastive loss function, have found great success in the
domain of visual representation learning. They are not suitable for exploiting
the rich dynamical structure of video however, as operations are done on many
augmented instances. In this paper we propose "Video Cross-Stream Prototypical
Contrasting", a novel method which predicts consistent prototype assignments
from both RGB and optical flow views, operating on sets of samples.
Specifically, we alternate the optimization process; while optimizing one of
the streams, all views are mapped to one set of stream prototype vectors. Each
of the assignments is predicted with all views except the one matching the
prediction, pushing representations closer to their assigned prototypes. As a
result, more efficient video embeddings with ingrained motion information are
learned, without the explicit need for optical flow computation during
inference. We obtain state-of-the-art results on nearest neighbour video
retrieval and action recognition, outperforming previous best by +3.2% on
UCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and
+15.1% on HMDB51 using the R(2+1)D backbone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toering_M/0/1/0/all/0/1"&gt;Martine Toering&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatopoulos_I/0/1/0/all/0/1"&gt;Ioannis Gatopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stol_M/0/1/0/all/0/1"&gt;Maarten Stol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1"&gt;Vincent Tao Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ResDepth: Learned Residual Stereo Reconstruction. (arXiv:2001.08026v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.08026</id>
        <link href="http://arxiv.org/abs/2001.08026"/>
        <updated>2021-06-21T02:07:38.601Z</updated>
        <summary type="html"><![CDATA[We propose an embarrassingly simple but very effective scheme for
high-quality dense stereo reconstruction: (i) generate an approximate
reconstruction with your favourite stereo matcher; (ii) rewarp the input images
with that approximate model; (iii) with the initial reconstruction and the
warped images as input, train a deep network to enhance the reconstruction by
regressing a residual correction; and (iv) if desired, iterate the refinement
with the new, improved reconstruction. The strategy to only learn the residual
greatly simplifies the learning problem. A standard Unet without bells and
whistles is enough to reconstruct even small surface details, like dormers and
roof substructures in satellite images. We also investigate residual
reconstruction with less information and find that even a single image is
enough to greatly improve an approximate reconstruction. Our full model reduces
the mean absolute error of state-of-the-art stereo reconstruction systems by
>50%, both in our target domain of satellite stereo and on stereo pairs from
the ETH3D benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stucker_C/0/1/0/all/0/1"&gt;Corinne Stucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Error: a New Performance Measure for Adversarial Robustness. (arXiv:2106.10212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10212</id>
        <link href="http://arxiv.org/abs/2106.10212"/>
        <updated>2021-06-21T02:07:38.577Z</updated>
        <summary type="html"><![CDATA[Despite the significant advances in deep learning over the past decade, a
major challenge that limits the wide-spread adoption of deep learning has been
their fragility to adversarial attacks. This sensitivity to making erroneous
predictions in the presence of adversarially perturbed data makes deep neural
networks difficult to adopt for certain real-world, mission-critical
applications. While much of the research focus has revolved around adversarial
example creation and adversarial hardening, the area of performance measures
for assessing adversarial robustness is not well explored. Motivated by this,
this study presents the concept of residual error, a new performance measure
for not only assessing the adversarial robustness of a deep neural network at
the individual sample level, but also can be used to differentiate between
adversarial and non-adversarial examples to facilitate for adversarial example
detection. Furthermore, we introduce a hybrid model for approximating the
residual error in a tractable manner. Experimental results using the case of
image classification demonstrates the effectiveness and efficacy of the
proposed residual error metric for assessing several well-known deep neural
network architectures. These results thus illustrate that the proposed measure
could be a useful tool for not only assessing the robustness of deep neural
networks used in mission-critical scenarios, but also in the design of
adversarially robust models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aboutalebi_H/0/1/0/all/0/1"&gt;Hossein Aboutalebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1"&gt;Mohammad Javad Shafiee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karg_M/0/1/0/all/0/1"&gt;Michelle Karg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scharfenberger_C/0/1/0/all/0/1"&gt;Christian Scharfenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2021: Team M3EM Technical Report. (arXiv:2106.10026v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10026</id>
        <link href="http://arxiv.org/abs/2106.10026"/>
        <updated>2021-06-21T02:07:38.549Z</updated>
        <summary type="html"><![CDATA[In this report, we describe the technical details of our submission to the
2021 EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action
Recognition. Leveraging multiple modalities has been proved to benefit the
Unsupervised Domain Adaptation (UDA) task. In this work, we present Multi-Modal
Mutual Enhancement Module (M3EM), a deep module for jointly considering
information from multiple modalities to find the most transferable
representations across domains. We achieve this by implementing two sub-modules
for enhancing each modality using the context of other modalities. The first
sub-module exchanges information across modalities through the semantic space,
while the second sub-module finds the most transferable spatial region based on
the consensus of all modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lijin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yifei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugano_Y/0/1/0/all/0/1"&gt;Yusuke Sugano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1"&gt;Yoichi Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partition-Guided GANs. (arXiv:2104.00816v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00816</id>
        <link href="http://arxiv.org/abs/2104.00816"/>
        <updated>2021-06-21T02:07:38.540Z</updated>
        <summary type="html"><![CDATA[Despite the success of Generative Adversarial Networks (GANs), their training
suffers from several well-known problems, including mode collapse and
difficulties learning a disconnected set of manifolds. In this paper, we break
down the challenging task of learning complex high dimensional distributions,
supporting diverse data samples, to simpler sub-tasks. Our solution relies on
designing a partitioner that breaks the space into smaller regions, each having
a simpler distribution, and training a different generator for each partition.
This is done in an unsupervised manner without requiring any labels.

We formulate two desired criteria for the space partitioner that aid the
training of our mixture of generators: 1) to produce connected partitions and
2) provide a proxy of distance between partitions and data samples, along with
a direction for reducing that distance. These criteria are developed to avoid
producing samples from places with non-existent data density, and also
facilitate training by providing additional direction to the generators. We
develop theoretical constraints for a space partitioner to satisfy the above
criteria. Guided by our theoretical analysis, we design an effective neural
architecture for the space partitioner that empirically assures these
conditions. Experimental results on various standard benchmarks show that the
proposed unsupervised model outperforms several recent methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Armandpour_M/0/1/0/all/0/1"&gt;Mohammadreza Armandpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadeghian_A/0/1/0/all/0/1"&gt;Ali Sadeghian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Granularity Network with Modal Attention for Dense Affective Understanding. (arXiv:2106.09964v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09964</id>
        <link href="http://arxiv.org/abs/2106.09964"/>
        <updated>2021-06-21T02:07:38.523Z</updated>
        <summary type="html"><![CDATA[Video affective understanding, which aims to predict the evoked expressions
by the video content, is desired for video creation and recommendation. In the
recent EEV challenge, a dense affective understanding task is proposed and
requires frame-level affective prediction. In this paper, we propose a
multi-granularity network with modal attention (MGN-MA), which employs
multi-granularity features for better description of the target frame.
Specifically, the multi-granularity features could be divided into frame-level,
clips-level and video-level features, which corresponds to visual-salient
content, semantic-context and video theme information. Then the modal attention
fusion module is designed to fuse the multi-granularity features and emphasize
more affection-relevant modals. Finally, the fused feature is fed into a
Mixtures Of Experts (MOE) classifier to predict the expressions. Further
employing model-ensemble post-processing, the proposed method achieves the
correlation score of 0.02292 in the EEV challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Baoming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1"&gt;Ke Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1"&gt;Bo Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ban_C/0/1/0/all/0/1"&gt;Chao Ban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaobo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Gastric Histopathology Subsize Image Database (GasHisSDB) for Classification Algorithm Test: from Linear Regression to Visual Transformer. (arXiv:2106.02473v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02473</id>
        <link href="http://arxiv.org/abs/2106.02473"/>
        <updated>2021-06-21T02:07:38.505Z</updated>
        <summary type="html"><![CDATA[GasHisSDB is a New Gastric Histopathology Subsize Image Database with a total
of 245196 images. GasHisSDB is divided into 160*160 pixels sub-database,
120*120 pixels sub-database and 80*80 pixels sub-database. GasHisSDB is made to
realize the function of valuating image classification. In order to prove that
the methods of different periods in the field of image classification have
discrepancies on GasHisSDB, we select a variety of classifiers for evaluation.
Seven classical machine learning classifiers, three CNN classifiers and a novel
transformer-based classifier are selected for testing on image classification
tasks. GasHisSDB is available at the
URL:https://github.com/NEUhwm/GasHisSDB.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1"&gt;Md Mamunur Rahaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiquan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changhao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yudong Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing Adversarial Robustness of Deep Neural Networks in Pixel Space: a Semantic Perspective. (arXiv:2106.09872v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09872</id>
        <link href="http://arxiv.org/abs/2106.09872"/>
        <updated>2021-06-21T02:07:38.492Z</updated>
        <summary type="html"><![CDATA[The vulnerability of deep neural networks to adversarial examples, which are
crafted maliciously by modifying the inputs with imperceptible perturbations to
misled the network produce incorrect outputs, reveals the lack of robustness
and poses security concerns. Previous works study the adversarial robustness of
image classifiers on image level and use all the pixel information in an image
indiscriminately, lacking of exploration of regions with different semantic
meanings in the pixel space of an image. In this work, we fill this gap and
explore the pixel space of the adversarial image by proposing an algorithm to
looking for possible perturbations pixel by pixel in different regions of the
segmented image. The extensive experimental results on CIFAR-10 and ImageNet
verify that searching for the modified pixel in only some pixels of an image
can successfully launch the one-pixel adversarial attacks without requiring all
the pixels of the entire image, and there exist multiple vulnerable points
scattered in different regions of an image. We also demonstrate that the
adversarial robustness of different regions on the image varies with the amount
of semantic information contained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lina Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xingshu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yulong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yawei Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xuemei Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIR: Self-supervised Image Rectification via Seeing the Same Scene from Multiple Different Lenses. (arXiv:2011.14611v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14611</id>
        <link href="http://arxiv.org/abs/2011.14611"/>
        <updated>2021-06-21T02:07:38.474Z</updated>
        <summary type="html"><![CDATA[Deep learning has demonstrated its power in image rectification by leveraging
the representation capacity of deep neural networks via supervised training
based on a large-scale synthetic dataset. However, the model may overfit the
synthetic images and generalize not well on real-world fisheye images due to
the limited universality of a specific distortion model and the lack of
explicitly modeling the distortion and rectification process. In this paper, we
propose a novel self-supervised image rectification (SIR) method based on an
important insight that the rectified results of distorted images of a same
scene from different lens should be the same. Specifically, we devise a new
network architecture with a shared encoder and several prediction heads, each
of which predicts the distortion parameter of a specific distortion model. We
further leverage a differentiable warping module to generate the rectified
images and re-distorted images from the distortion parameters and exploit the
intra- and inter-model consistency between them during training, thereby
leading to a self-supervised learning scheme without the need for ground-truth
distortion parameters or normal images. Experiments on synthetic dataset and
real-world fisheye images demonstrate that our method achieves comparable or
even better performance than the supervised baseline method and representative
state-of-the-art methods. Self-supervised learning also improves the
universality of distortion models while keeping their self-consistency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jinlong Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GEM: A General Evaluation Benchmark for Multimodal Tasks. (arXiv:2106.09889v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09889</id>
        <link href="http://arxiv.org/abs/2106.09889"/>
        <updated>2021-06-21T02:07:38.455Z</updated>
        <summary type="html"><![CDATA[In this paper, we present GEM as a General Evaluation benchmark for
Multimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,
XGLUE and XTREME that mainly focus on natural language tasks, GEM is a
large-scale vision-language benchmark, which consists of GEM-I for
image-language tasks and GEM-V for video-language tasks. Comparing with
existing multimodal datasets such as MSCOCO and Flicker30K for image-language
tasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the
largest vision-language dataset covering image-language tasks and
video-language tasks at the same time, but also labeled in multiple languages.
We also provide two baseline models for this benchmark. We will release the
dataset, code and baseline models, aiming to advance the development of
multilingual multimodal research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Lin Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1"&gt;Edward Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1"&gt;Lei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Huaishao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1"&gt;Ming Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharti_T/0/1/0/all/0/1"&gt;Taroon Bharti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sacheti_A/0/1/0/all/0/1"&gt;Arun Sacheti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facial Expressions as a Vulnerability in Face Recognition. (arXiv:2011.08809v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08809</id>
        <link href="http://arxiv.org/abs/2011.08809"/>
        <updated>2021-06-21T02:07:38.440Z</updated>
        <summary type="html"><![CDATA[This work explores facial expression bias as a security vulnerability of face
recognition systems. Despite the great performance achieved by state-of-the-art
face recognition systems, the algorithms are still sensitive to a large range
of covariates. We present a comprehensive analysis of how facial expression
bias impacts the performance of face recognition technologies. Our study
analyzes: i) facial expression biases in the most popular face recognition
databases; and ii) the impact of facial expression in face recognition
performances. Our experimental framework includes two face detectors, three
face recognition models, and three different databases. Our results demonstrate
a huge facial expression bias in the most widely used databases, as well as a
related impact of face expression in the performance of state-of-the-art
algorithms. This work opens the door to new research lines focused on
mitigating the observed vulnerability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pena_A/0/1/0/all/0/1"&gt;Alejandro Pe&amp;#xf1;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serna_I/0/1/0/all/0/1"&gt;Ignacio Serna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1"&gt;Aythami Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1"&gt;Julian Fierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapedriza_A/0/1/0/all/0/1"&gt;Agata Lapedriza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge Computing for Real-Time Near-Crash Detection for Smart Transportation Applications. (arXiv:2008.00549v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00549</id>
        <link href="http://arxiv.org/abs/2008.00549"/>
        <updated>2021-06-21T02:07:38.432Z</updated>
        <summary type="html"><![CDATA[Traffic near-crash events serve as critical data sources for various smart
transportation applications, such as being surrogate safety measures for
traffic safety research and corner case data for automated vehicle testing.
However, there are several key challenges for near-crash detection. First,
extracting near-crashes from original data sources requires significant
computing, communication, and storage resources. Also, existing methods lack
efficiency and transferability, which bottlenecks prospective large-scale
applications. To this end, this paper leverages the power of edge computing to
address these challenges by processing the video streams from existing dashcams
onboard in a real-time manner. We design a multi-thread system architecture
that operates on edge devices and model the bounding boxes generated by object
detection and tracking in linear complexity. The method is insensitive to
camera parameters and backward compatible with different vehicles. The edge
computing system has been evaluated with recorded videos and real-world tests
on two cars and four buses for over ten thousand hours. It filters out
irrelevant videos in real-time thereby saving labor cost, processing time,
network bandwidth, and data storage. It collects not only event videos but also
other valuable data such as road user type, event location, time to collision,
vehicle trajectory, vehicle speed, brake switch, and throttle. The experiments
demonstrate the promising performance of the system regarding efficiency,
accuracy, reliability, and transferability. It is among the first efforts in
applying edge computing for real-time traffic video analytics and is expected
to benefit multiple sub-fields in smart transportation research and
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ke_R/0/1/0/all/0/1"&gt;Ruimin Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhiyong Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Meixin Zhu&lt;/a&gt;, Hao (Frank) &lt;a href="http://arxiv.org/find/cs/1/au:+Yang/0/1/0/all/0/1"&gt;Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yinhai Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embodied Language Grounding with 3D Visual Feature Representations. (arXiv:1910.01210v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.01210</id>
        <link href="http://arxiv.org/abs/1910.01210"/>
        <updated>2021-06-21T02:07:38.413Z</updated>
        <summary type="html"><![CDATA[We propose associating language utterances to 3D visual abstractions of the
scene they describe. The 3D visual abstractions are encoded as 3-dimensional
visual feature maps. We infer these 3D visual scene feature maps from RGB
images of the scene via view prediction: when the generated 3D scene feature
map is neurally projected from a camera viewpoint, it should match the
corresponding RGB image. We present generative models that condition on the
dependency tree of an utterance and generate a corresponding visual 3D feature
map as well as reason about its plausibility, and detector models that
condition on both the dependency tree of an utterance and a related image and
localize the object referents in the 3D feature map inferred from the image.
Our model outperforms models of language and vision that associate language
with 2D CNN activations or 2D images by a large margin in a variety of tasks,
such as, classifying plausibility of utterances, detecting referential
expressions, and supplying rewards for trajectory optimization of object
placement policies from language instructions. We perform numerous ablations
and show the improved performance of our detectors is due to its better
generalization across camera viewpoints and lack of object interferences in the
inferred 3D feature space, and the improved performance of our generators is
due to their ability to spatially reason about objects and their configurations
in 3D when mapping from language to scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prabhudesai_M/0/1/0/all/0/1"&gt;Mihir Prabhudesai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1"&gt;Hsiao-Yu Fish Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1"&gt;Syed Ashar Javed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sieb_M/0/1/0/all/0/1"&gt;Maximilian Sieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1"&gt;Adam W. Harley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1"&gt;Katerina Fragkiadaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Radar Camera Fusion via Representation Learning in Autonomous Driving. (arXiv:2103.07825v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07825</id>
        <link href="http://arxiv.org/abs/2103.07825"/>
        <updated>2021-06-21T02:07:38.394Z</updated>
        <summary type="html"><![CDATA[Radars and cameras are mature, cost-effective, and robust sensors and have
been widely used in the perception stack of mass-produced autonomous driving
systems. Due to their complementary properties, outputs from radar detection
(radar pins) and camera perception (2D bounding boxes) are usually fused to
generate the best perception results. The key to successful radar-camera fusion
is the accurate data association. The challenges in the radar-camera
association can be attributed to the complexity of driving scenes, the noisy
and sparse nature of radar measurements, and the depth ambiguity from 2D
bounding boxes. Traditional rule-based association methods are susceptible to
performance degradation in challenging scenarios and failure in corner cases.
In this study, we propose to address radar-camera association via deep
representation learning, to explore feature-level interaction and global
reasoning. Additionally, we design a loss sampling mechanism and an innovative
ordinal loss to overcome the difficulty of imperfect labeling and to enforce
critical human-like reasoning. Despite being trained with noisy labels
generated by a rule-based algorithm, our proposed method achieves a performance
of 92.2% F1 score, which is 11.6% higher than the rule-based teacher. Moreover,
this data-driven method also lends itself to continuous improvement via corner
case mining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Binnan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Yunxiang Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Langechuan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End 3D Point Cloud Learning for Registration Task Using Virtual Correspondences. (arXiv:2011.14579v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14579</id>
        <link href="http://arxiv.org/abs/2011.14579"/>
        <updated>2021-06-21T02:07:38.383Z</updated>
        <summary type="html"><![CDATA[3D Point cloud registration is still a very challenging topic due to the
difficulty in finding the rigid transformation between two point clouds with
partial correspondences, and it's even harder in the absence of any initial
estimation information. In this paper, we present an end-to-end deep-learning
based approach to resolve the point cloud registration problem. Firstly, the
revised LPD-Net is introduced to extract features and aggregate them with the
graph network. Secondly, the self-attention mechanism is utilized to enhance
the structure information in the point cloud and the cross-attention mechanism
is designed to enhance the corresponding information between the two input
point clouds. Based on which, the virtual corresponding points can be generated
by a soft pointer based method, and finally, the point cloud registration
problem can be solved by implementing the SVD method. Comparison results in
ModelNet40 dataset validate that the proposed approach reaches the
state-of-the-art in point cloud registration tasks and experiment resutls in
KITTI dataset validate the effectiveness of the proposed approach in real
applications.Our source code is available at
\url{https://github.com/qiaozhijian/VCR-Net.git}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Zhijian Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Huanshu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suo_C/0/1/0/all/0/1"&gt;Chuanzhe Suo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hesheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Steerable Partial Differential Operators for Equivariant Neural Networks. (arXiv:2106.10163v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10163</id>
        <link href="http://arxiv.org/abs/2106.10163"/>
        <updated>2021-06-21T02:07:38.376Z</updated>
        <summary type="html"><![CDATA[Recent work in equivariant deep learning bears strong similarities to
physics. Fields over a base space are fundamental entities in both subjects, as
are equivariant maps between these fields. In deep learning, however, these
maps are usually defined by convolutions with a kernel, whereas they are
partial differential operators (PDOs) in physics. Developing the theory of
equivariant PDOs in the context of deep learning could bring these subjects
even closer together and lead to a stronger flow of ideas. In this work, we
derive a $G$-steerability constraint that completely characterizes when a PDO
between feature vector fields is equivariant, for arbitrary symmetry groups
$G$. We then fully solve this constraint for several important groups. We use
our solutions as equivariant drop-in replacements for convolutional layers and
benchmark them in that role. Finally, we develop a framework for equivariant
maps based on Schwartz distributions that unifies classical convolutions and
differential operators and gives insight about the relation between the two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jenner_E/0/1/0/all/0/1"&gt;Erik Jenner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1"&gt;Maurice Weiler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Clustering-friendly Representations: Subspace Clustering via Graph Filtering. (arXiv:2106.09874v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09874</id>
        <link href="http://arxiv.org/abs/2106.09874"/>
        <updated>2021-06-21T02:07:38.368Z</updated>
        <summary type="html"><![CDATA[Finding a suitable data representation for a specific task has been shown to
be crucial in many applications. The success of subspace clustering depends on
the assumption that the data can be separated into different subspaces.
However, this simple assumption does not always hold since the raw data might
not be separable into subspaces. To recover the ``clustering-friendly''
representation and facilitate the subsequent clustering, we propose a graph
filtering approach by which a smooth representation is achieved. Specifically,
it injects graph similarity into data features by applying a low-pass filter to
extract useful data representations for clustering. Extensive experiments on
image and document clustering datasets demonstrate that our method improves
upon state-of-the-art subspace clustering techniques. Especially, its
comparable performance with deep learning methods emphasizes the effectiveness
of the simple graph filtering scheme for many real-world applications. An
ablation study shows that graph filtering can remove noise, preserve structure
in the image, and increase the separability of classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhengrui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1"&gt;Zhao Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1"&gt;Guangchun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1"&gt;Ling Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Graph based Trajectory Predictor with Pseudo Oracle. (arXiv:2002.00391v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00391</id>
        <link href="http://arxiv.org/abs/2002.00391"/>
        <updated>2021-06-21T02:07:38.351Z</updated>
        <summary type="html"><![CDATA[Pedestrian trajectory prediction in dynamic scenes remains a challenging and
critical problem in numerous applications, such as self-driving cars and
socially aware robots. Challenges concentrate on capturing pedestrians' motion
patterns and social interactions, as well as handling the future uncertainties.
Recent studies focus on modeling pedestrians' motion patterns with recurrent
neural networks, capturing social interactions with pooling-based or
graph-based methods, and handling future uncertainties by using random Gaussian
noise as the latent variable. However, they do not integrate specific obstacle
avoidance experience (OAE) that may improve prediction performance. For
example, pedestrians' future trajectories are always influenced by others in
front. Here we propose GTPPO (Graph-based Trajectory Predictor with Pseudo
Oracle), an encoder-decoder-based method conditioned on pedestrians' future
behaviors. Pedestrians' motion patterns are encoded with a long short-term
memory unit, which introduces the temporal attention to highlight specific time
steps. Their interactions are captured by a graph-based attention mechanism,
which draws OAE into the data-driven learning process of graph attention.
Future uncertainties are handled by generating multi-modal outputs with an
informative latent variable. Such a variable is generated by a novel pseudo
oracle predictor, which minimizes the knowledge gap between historical and
ground-truth trajectories. Finally, the GTPPO is evaluated on ETH, UCY and
Stanford Drone datasets, and the results demonstrate state-of-the-art
performance. Besides, the qualitative evaluations show successful cases of
handling sudden motion changes in the future. Such findings indicate that GTPPO
can peek into the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Biao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1"&gt;Guocheng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1"&gt;Chingyao Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xiang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation. (arXiv:2103.04717v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04717</id>
        <link href="http://arxiv.org/abs/2103.04717"/>
        <updated>2021-06-21T02:07:38.343Z</updated>
        <summary type="html"><![CDATA[Multi-source unsupervised domain adaptation~(MSDA) aims at adapting models
trained on multiple labeled source domains to an unlabeled target domain. In
this paper, we propose a novel multi-source domain adaptation framework based
on collaborative learning for semantic segmentation. Firstly, a simple image
translation method is introduced to align the pixel value distribution to
reduce the gap between source domains and target domain to some extent. Then,
to fully exploit the essential semantic information across source domains, we
propose a collaborative learning method for domain adaptation without seeing
any data from target domain. In addition, similar to the setting of
unsupervised domain adaptation, unlabeled target domain data is leveraged to
further improve the performance of domain adaptation. This is achieved by
additionally constraining the outputs of multiple adaptation models with pseudo
labels online generated by an ensembled model. Extensive experiments and
ablation studies are conducted on the widely-used domain adaptation benchmark
datasets in semantic segmentation. Our proposed method achieves 59.0\% mIoU on
the validation set of Cityscapes by training on the labeled Synscapes and GTA5
datasets and unlabeled training set of Cityscapes. It significantly outperforms
all previous state-of-the-arts single-source and multi-source unsupervised
domain adaptation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianzhong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xu Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuaijun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianzhuang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Diverse-Structured Networks for Adversarial Robustness. (arXiv:2102.01886v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01886</id>
        <link href="http://arxiv.org/abs/2102.01886"/>
        <updated>2021-06-21T02:07:38.336Z</updated>
        <summary type="html"><![CDATA[In adversarial training (AT), the main focus has been the objective and
optimizer while the model has been less studied, so that the models being used
are still those classic ones in standard training (ST). Classic network
architectures (NAs) are generally worse than searched NAs in ST, which should
be the same in AT. In this paper, we argue that NA and AT cannot be handled
independently, since given a dataset, the optimal NA in ST would be no longer
optimal in AT. That being said, AT is time-consuming itself; if we directly
search NAs in AT over large search spaces, the computation will be practically
infeasible. Thus, we propose a diverse-structured network (DS-Net), to
significantly reduce the size of the search space: instead of low-level
operations, we only consider predefined atomic blocks, where an atomic block is
a time-tested building block like the residual block. There are only a few
atomic blocks and thus we can weight all atomic blocks rather than find the
best one in a searched block of DS-Net, which is an essential trade-off between
exploring diverse structures and exploiting the best structures. Empirical
results demonstrate the advantages of DS-Net, i.e., weighting the atomic
blocks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1"&gt;Xuefeng Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CT Image Synthesis Using Weakly Supervised Segmentation and Geometric Inter-Label Relations For COVID Image Analysis. (arXiv:2106.10230v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10230</id>
        <link href="http://arxiv.org/abs/2106.10230"/>
        <updated>2021-06-21T02:07:38.328Z</updated>
        <summary type="html"><![CDATA[While medical image segmentation is an important task for computer aided
diagnosis, the high expertise requirement for pixelwise manual annotations
makes it a challenging and time consuming task. Since conventional data
augmentations do not fully represent the underlying distribution of the
training set, the trained models have varying performance when tested on images
captured from different sources. Most prior work on image synthesis for data
augmentation ignore the interleaved geometric relationship between different
anatomical labels. We propose improvements over previous GAN-based medical
image synthesis methods by learning the relationship between different
anatomical labels. We use a weakly supervised segmentation method to obtain
pixel level semantic label map of images which is used learn the intrinsic
relationship of geometry and shape across semantic labels. Latent space
variable sampling results in diverse generated images from a base image and
improves robustness. We use the synthetic images from our method to train
networks for segmenting COVID-19 infected areas from lung CT images. The
proposed method outperforms state-of-the-art segmentation methods on a public
dataset. Ablation studies also demonstrate benefits of integrating geometry and
diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mahapatra_D/0/1/0/all/0/1"&gt;Dwarikanath Mahapatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Singh_A/0/1/0/all/0/1"&gt;Ankur Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Reuse and Fusion for Real-time Semantic segmentation. (arXiv:2105.12964v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12964</id>
        <link href="http://arxiv.org/abs/2105.12964"/>
        <updated>2021-06-21T02:07:38.308Z</updated>
        <summary type="html"><![CDATA[For real-time semantic segmentation, how to increase the speed while
maintaining high resolution is a problem that has been discussed and solved.
Backbone design and fusion design have always been two essential parts of
real-time semantic segmentation. We hope to design a light-weight network based
on previous design experience and reach the level of state-of-the-art real-time
semantic segmentation without any pre-training. To achieve this goal, a
encoder-decoder architectures are proposed to solve this problem by applying a
decoder network onto a backbone model designed for real-time segmentation tasks
and designed three different ways to fuse semantics and detailed information in
the aggregation phase. We have conducted extensive experiments on two semantic
segmentation benchmarks. Experiments on the Cityscapes and CamVid datasets show
that the proposed FRFNet strikes a balance between speed calculation and
accuracy. It achieves 72% Mean Intersection over Union (mIoU%) on the
Cityscapes test dataset with the speed of 144 on a single RTX 1080Ti card. The
Code is available at https://github.com/favoMJ/FRFNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sixiang_T/0/1/0/all/0/1"&gt;Tan Sixiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic segmentation of multispectral photoacoustic images using deep learning. (arXiv:2105.09624v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09624</id>
        <link href="http://arxiv.org/abs/2105.09624"/>
        <updated>2021-06-21T02:07:38.300Z</updated>
        <summary type="html"><![CDATA[Photoacoustic imaging has the potential to revolutionise healthcare due to
the valuable information on tissue physiology that is contained in
multispectral photoacoustic measurements. Clinical translation of the
technology requires conversion of the high-dimensional acquired data into
clinically relevant and interpretable information. In this work, we present a
deep learning-based approach to semantic segmentation of multispectral
photoacoustic images to facilitate the interpretability of recorded images.
Manually annotated multispectral photoacoustic imaging data are used as gold
standard reference annotations and enable the training of a deep learning-based
segmentation algorithm in a supervised manner. Based on a validation study with
experimentally acquired data of healthy human volunteers, we show that
automatic tissue segmentation can be used to create powerful analyses and
visualisations of multispectral photoacoustic images. Due to the intuitive
representation of high-dimensional information, such a processing algorithm
could be a valuable means to facilitate the clinical translation of
photoacoustic imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Grohl_J/0/1/0/all/0/1"&gt;Janek Gr&amp;#xf6;hl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schellenberg_M/0/1/0/all/0/1"&gt;Melanie Schellenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dreher_K/0/1/0/all/0/1"&gt;Kris Dreher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Holzwarth_N/0/1/0/all/0/1"&gt;Niklas Holzwarth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1"&gt;Minu D. Tizabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Seitel_A/0/1/0/all/0/1"&gt;Alexander Seitel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dynamic Spatial-temporal Attention Network for Early Anticipation of Traffic Accidents. (arXiv:2106.10197v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10197</id>
        <link href="http://arxiv.org/abs/2106.10197"/>
        <updated>2021-06-21T02:07:38.292Z</updated>
        <summary type="html"><![CDATA[Recently, autonomous vehicles and those equipped with an Advanced Driver
Assistance System (ADAS) are emerging. They share the road with regular ones
operated by human drivers entirely. To ensure guaranteed safety for passengers
and other road users, it becomes essential for autonomous vehicles and ADAS to
anticipate traffic accidents from natural driving scenes. The dynamic
spatial-temporal interaction of the traffic agents is complex, and visual cues
for predicting a future accident are embedded deeply in dashcam video data.
Therefore, early anticipation of traffic accidents remains a challenge. To this
end, the paper presents a dynamic spatial-temporal attention (DSTA) network for
early anticipation of traffic accidents from dashcam videos. The proposed
DSTA-network learns to select discriminative temporal segments of a video
sequence with a module named Dynamic Temporal Attention (DTA). It also learns
to focus on the informative spatial regions of frames with another module named
Dynamic Spatial Attention (DSA). The spatial-temporal relational features of
accidents, along with scene appearance features, are learned jointly with a
Gated Recurrent Unit (GRU) network. The experimental evaluation of the
DSTA-network on two benchmark datasets confirms that it has exceeded the
state-of-the-art performance. A thorough ablation study evaluates the
contributions of individual components of the DSTA-network, revealing how the
network achieves such performance. Furthermore, this paper proposes a new
strategy that fuses the prediction scores from two complementary models and
verifies its effectiveness in further boosting the performance of early
accident anticipation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1"&gt;Muhammad Monjurul Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ruwen Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1"&gt;Zhaozheng Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning. (arXiv:2101.08482v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08482</id>
        <link href="http://arxiv.org/abs/2101.08482"/>
        <updated>2021-06-21T02:07:38.285Z</updated>
        <summary type="html"><![CDATA[We present a plug-in replacement for batch normalization (BN) called
exponential moving average normalization (EMAN), which improves the performance
of existing student-teacher based self- and semi-supervised learning
techniques. Unlike the standard BN, where the statistics are computed within
each batch, EMAN, used in the teacher, updates its statistics by exponential
moving average from the BN statistics of the student. This design reduces the
intrinsic cross-sample dependency of BN and enhances the generalization of the
teacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2
points and semi-supervised learning by about 7/2 points, when 1%/10% supervised
labels are available on ImageNet. These improvements are consistent across
methods, network architectures, training duration, and datasets, demonstrating
the general effectiveness of this technique. The code is available at
https://github.com/amazon-research/exponential-moving-average-normalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhaowei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhransu Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1"&gt;Charless Fowlkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Level Set Stereo for Cooperative Grouping with Occlusion. (arXiv:2006.16094v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16094</id>
        <link href="http://arxiv.org/abs/2006.16094"/>
        <updated>2021-06-21T02:07:38.277Z</updated>
        <summary type="html"><![CDATA[Localizing stereo boundaries is difficult because matching cues are absent in
the occluded regions that are adjacent to them. We introduce an energy and
level-set optimizer that improves boundaries by encoding the essential geometry
of occlusions: The spatial extent of an occlusion must equal the amplitude of
the disparity jump that causes it. In a collection of figure-ground scenes from
Middlebury and Falling Things stereo datasets, the model provides more accurate
boundaries than previous occlusion-handling techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jialiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zickler_T/0/1/0/all/0/1"&gt;Todd Zickler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discerning Generic Event Boundaries in Long-Form Wild Videos. (arXiv:2106.10090v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10090</id>
        <link href="http://arxiv.org/abs/2106.10090"/>
        <updated>2021-06-21T02:07:38.258Z</updated>
        <summary type="html"><![CDATA[Detecting generic, taxonomy-free event boundaries invideos represents a major
stride forward towards holisticvideo understanding. In this paper we present a
technique forgeneric event boundary detection based on a two stream in-flated
3D convolutions architecture, which can learn spatio-temporal features from
videos. Our work is inspired from theGeneric Event Boundary Detection Challenge
(part of CVPR2021 Long Form Video Understanding- LOVEU Workshop).Throughout the
paper we provide an in-depth analysis ofthe experiments performed along with an
interpretation ofthe results obtained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1"&gt;Ayush K Rai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1"&gt;Tarun Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dietlmeier_J/0/1/0/all/0/1"&gt;Julia Dietlmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1"&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1"&gt;Alan F Smeaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1"&gt;Noel E O&amp;#x27;Connor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Iterative Phase Retrieval With Cascaded Neural Networks. (arXiv:2106.10195v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10195</id>
        <link href="http://arxiv.org/abs/2106.10195"/>
        <updated>2021-06-21T02:07:38.251Z</updated>
        <summary type="html"><![CDATA[Fourier phase retrieval is the problem of reconstructing a signal given only
the magnitude of its Fourier transformation. Optimization-based approaches,
like the well-established Gerchberg-Saxton or the hybrid input output
algorithm, struggle at reconstructing images from magnitudes that are not
oversampled. This motivates the application of learned methods, which allow
reconstruction from non-oversampled magnitude measurements after a learning
phase. In this paper, we want to push the limits of these learned methods by
means of a deep neural network cascade that reconstructs the image successively
on different resolutions from its non-oversampled Fourier magnitude. We
evaluate our method on four different datasets (MNIST, EMNIST, Fashion-MNIST,
and KMNIST) and demonstrate that it yields improved performance over other
non-iterative methods and optimization-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Uelwer_T/0/1/0/all/0/1"&gt;Tobias Uelwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hoffmann_T/0/1/0/all/0/1"&gt;Tobias Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Harmeling_S/0/1/0/all/0/1"&gt;Stefan Harmeling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SLSNet: Skin lesion segmentation using a lightweight generative adversarial network. (arXiv:1907.00856v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.00856</id>
        <link href="http://arxiv.org/abs/1907.00856"/>
        <updated>2021-06-21T02:07:38.240Z</updated>
        <summary type="html"><![CDATA[The determination of precise skin lesion boundaries in dermoscopic images
using automated methods faces many challenges, most importantly, the presence
of hair, inconspicuous lesion edges and low contrast in dermoscopic images, and
variability in the color, texture and shapes of skin lesions. Existing deep
learning-based skin lesion segmentation algorithms are expensive in terms of
computational time and memory. Consequently, running such segmentation
algorithms requires a powerful GPU and high bandwidth memory, which are not
available in dermoscopy devices. Thus, this article aims to achieve precise
skin lesion segmentation with minimum resources: a lightweight, efficient
generative adversarial network (GAN) model called SLSNet, which combines 1-D
kernel factorized networks, position and channel attention, and multiscale
aggregation mechanisms with a GAN model. The 1-D kernel factorized network
reduces the computational cost of 2D filtering. The position and channel
attention modules enhance the discriminative ability between the lesion and
non-lesion feature representations in spatial and channel dimensions,
respectively. A multiscale block is also used to aggregate the coarse-to-fine
features of input skin images and reduce the effect of the artifacts. SLSNet is
evaluated on two publicly available datasets: ISBI 2017 and the ISIC 2018.
Although SLSNet has only 2.35 million parameters, the experimental results
demonstrate that it achieves segmentation results on a par with the
state-of-the-art skin lesion segmentation methods with an accuracy of 97.61%,
and Dice and Jaccard similarity coefficients of 90.63% and 81.98%,
respectively. SLSNet can run at more than 110 frames per second (FPS) in a
single GTX1080Ti GPU, which is faster than well-known deep learning-based image
segmentation models, such as FCN. Therefore, SLSNet can be used for practical
dermoscopic applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sarker_M/0/1/0/all/0/1"&gt;Md. Mostafa Kamal Sarker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rashwan_H/0/1/0/all/0/1"&gt;Hatem A. Rashwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Akram_F/0/1/0/all/0/1"&gt;Farhan Akram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Singh_V/0/1/0/all/0/1"&gt;Vivek Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Banu_S/0/1/0/all/0/1"&gt;Syeda Furruka Banu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhury_F/0/1/0/all/0/1"&gt;Forhad U H Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Choudhury_K/0/1/0/all/0/1"&gt;Kabir Ahmed Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chambon_S/0/1/0/all/0/1"&gt;Sylvie Chambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Radeva_P/0/1/0/all/0/1"&gt;Petia Radeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Puig_D/0/1/0/all/0/1"&gt;Domenec Puig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abdel_Nasser_M/0/1/0/all/0/1"&gt;Mohamed Abdel-Nasser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Generalization in Deep Learning Applications for Land Cover Mapping. (arXiv:2008.10351v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10351</id>
        <link href="http://arxiv.org/abs/2008.10351"/>
        <updated>2021-06-21T02:07:38.212Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that deep learning models can be used to classify
land-use data from geospatial satellite imagery. We show that when these deep
learning models are trained on data from specific continents/seasons, there is
a high degree of variability in model performance on out-of-sample
continents/seasons. This suggests that just because a model accurately predicts
land-use classes in one continent or season does not mean that the model will
accurately predict land-use classes in a different continent or season. We then
use clustering techniques on satellite imagery from different continents to
visualize the differences in landscapes that make geospatial generalization
particularly difficult, and summarize our takeaways for future satellite
imagery-related applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Lucas Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1"&gt;Caleb Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1"&gt;Bistra Dilkina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Pollution Reduction in Nighttime Photography. (arXiv:2106.10046v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10046</id>
        <link href="http://arxiv.org/abs/2106.10046"/>
        <updated>2021-06-21T02:07:38.197Z</updated>
        <summary type="html"><![CDATA[Nighttime photographers are often troubled by light pollution of unwanted
artificial lights. Artificial lights, after scattered by aerosols in the
atmosphere, can inundate the starlight and degrade the quality of nighttime
images, by reducing contrast and dynamic range and causing hazes. In this paper
we develop a physically-based light pollution reduction (LPR) algorithm that
can substantially alleviate the aforementioned degradations of perceptual
quality and restore the pristine state of night sky. The key to the success of
the proposed LPR algorithm is an inverse method to estimate the spatial
radiance distribution and spectral signature of ground artificial lights.
Extensive experiments are carried out to evaluate the efficacy and limitations
of the LPR algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiaolin Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Radar-to-Lidar: Heterogeneous Place Recognition via Joint Learning. (arXiv:2102.04960v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04960</id>
        <link href="http://arxiv.org/abs/2102.04960"/>
        <updated>2021-06-21T02:07:38.175Z</updated>
        <summary type="html"><![CDATA[Place recognition is critical for both offline mapping and online
localization. However, current single-sensor based place recognition still
remains challenging in adverse conditions. In this paper, a heterogeneous
measurements based framework is proposed for long-term place recognition, which
retrieves the query radar scans from the existing lidar maps. To achieve this,
a deep neural network is built with joint training in the learning stage, and
then in the testing stage, shared embeddings of radar and lidar are extracted
for heterogeneous place recognition. To validate the effectiveness of the
proposed method, we conduct tests and generalization experiments on the
multi-session public datasets compared to other competitive methods. The
experimental results indicate that our model is able to perform multiple place
recognitions: lidar-to-lidar, radar-to-radar and radar-to-lidar, while the
learned model is trained only once. We also release the source code publicly:
https://github.com/ZJUYH/radar-to-lidar-place-recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Huan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xuecheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1"&gt;Rong Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise2Sim -- Similarity-based Self-Learning for Image Denoising. (arXiv:2011.03384v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03384</id>
        <link href="http://arxiv.org/abs/2011.03384"/>
        <updated>2021-06-21T02:07:38.167Z</updated>
        <summary type="html"><![CDATA[Despite its best performance in image denoising, the supervised deep
denoising methods require paired noise-clean data, which are often unavailable.
To address this challenge, Noise2Noise was designed based on the fact that
paired noise-clean images can be replaced by paired noise-noise images that are
easier to collect. However, in many scenarios the collection of paired
noise-noise images is still impractical. To bypass labeled images, Noise2Void
methods predict masked pixels from their surroundings with single noisy images
only and give improved denoising results that still need improvements. An
observation on classic denoising methods is that non-local mean (NLM) outcomes
are typically superior to locally denoised results. In contrast, Noise2Void and
its variants do not utilize self-similarities in an image as the NLM-based
methods do. Here we propose Noise2Sim, an NLM-inspired self-learning method for
image denoising. Specifically, Noise2Sim leverages the self-similarity of image
pixels to train the denoising network, requiring single noisy images only. Our
theoretical analysis shows that Noise2Sim tends to be equivalent to Noise2Noise
under mild conditions. To efficiently manage the computational burden for
globally searching similar pixels, we design a two-step procedure to provide
data for Noise2Sim training. Extensive experiments demonstrate the superiority
of Noise2Sim on common benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1"&gt;Chuang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1"&gt;Fenglei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1"&gt;Qing Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No Routing Needed Between Capsules. (arXiv:2001.09136v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09136</id>
        <link href="http://arxiv.org/abs/2001.09136"/>
        <updated>2021-06-21T02:07:38.160Z</updated>
        <summary type="html"><![CDATA[Most capsule network designs rely on traditional matrix multiplication
between capsule layers and computationally expensive routing mechanisms to deal
with the capsule dimensional entanglement that the matrix multiplication
introduces. By using Homogeneous Vector Capsules (HVCs), which use element-wise
multiplication rather than matrix multiplication, the dimensions of the
capsules remain unentangled. In this work, we study HVCs as applied to the
highly structured MNIST dataset in order to produce a direct comparison to the
capsule research direction of Geoffrey Hinton, et al. In our study, we show
that a simple convolutional neural network using HVCs performs as well as the
prior best performing capsule network on MNIST using 5.5x fewer parameters, 4x
fewer training epochs, no reconstruction sub-network, and requiring no routing
mechanism. The addition of multiple classification branches to the network
establishes a new state of the art for the MNIST dataset with an accuracy of
99.87% for an ensemble of these models, as well as establishing a new state of
the art for a single model (99.83% accurate).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Byerly_A/0/1/0/all/0/1"&gt;Adam Byerly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalganova_T/0/1/0/all/0/1"&gt;Tatiana Kalganova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dear_I/0/1/0/all/0/1"&gt;Ian Dear&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training or Architecture? How to Incorporate Invariance in Neural Networks. (arXiv:2106.10044v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10044</id>
        <link href="http://arxiv.org/abs/2106.10044"/>
        <updated>2021-06-21T02:07:38.153Z</updated>
        <summary type="html"><![CDATA[Many applications require the robustness, or ideally the invariance, of a
neural network to certain transformations of input data. Most commonly, this
requirement is addressed by either augmenting the training data, using
adversarial training, or defining network architectures that include the
desired invariance automatically. Unfortunately, the latter often relies on the
ability to enlist all possible transformations, which make such approaches
largely infeasible for infinite sets of transformations, such as arbitrary
rotations or scaling. In this work, we propose a method for provably invariant
network architectures with respect to group actions by choosing one element
from a (possibly continuous) orbit based on a fixed criterion. In a nutshell,
we intend to 'undo' any possible transformation before feeding the data into
the actual network. We analyze properties of such approaches, extend them to
equivariant networks, and demonstrate their advantages in terms of robustness
as well as computational efficiency in several numerical examples. In
particular, we investigate the robustness with respect to rotations of images
(which can possibly hold up to discretization artifacts only) as well as the
provable rotational and scaling invariance of 3D point cloud classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gandikota_K/0/1/0/all/0/1"&gt;Kanchana Vaishnavi Gandikota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1"&gt;Jonas Geiping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahner_Z/0/1/0/all/0/1"&gt;Zorah L&amp;#xe4;hner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czaplinski_A/0/1/0/all/0/1"&gt;Adam Czapli&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1"&gt;Michael Moeller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap Between Object Detection and User Intent via Query-Modulation. (arXiv:2106.10258v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10258</id>
        <link href="http://arxiv.org/abs/2106.10258"/>
        <updated>2021-06-21T02:07:38.146Z</updated>
        <summary type="html"><![CDATA[When interacting with objects through cameras, or pictures, users often have
a specific intent. For example, they may want to perform a visual search.
However, most object detection models ignore the user intent, relying on image
pixels as their only input. This often leads to incorrect results, such as lack
of a high-confidence detection on the object of interest, or detection with a
wrong class label. In this paper we investigate techniques to modulate standard
object detectors to explicitly account for the user intent, expressed as an
embedding of a simple query. Compared to standard object detectors,
query-modulated detectors show superior performance at detecting objects for a
given label of interest. Thanks to large-scale training data synthesized from
standard object detection annotations, query-modulated detectors can also
outperform specialized referring expression recognition systems. Furthermore,
they can be simultaneously trained to solve for both query-modulated detection
and standard object detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fornoni_M/0/1/0/all/0/1"&gt;Marco Fornoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chaochao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1"&gt;Liangchen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1"&gt;Kimberly Wilber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stark_A/0/1/0/all/0/1"&gt;Alex Stark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yin Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1"&gt;Andrew Howard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VSAC: Efficient and Accurate Estimator for H and F. (arXiv:2106.10240v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10240</id>
        <link href="http://arxiv.org/abs/2106.10240"/>
        <updated>2021-06-21T02:07:38.126Z</updated>
        <summary type="html"><![CDATA[We present VSAC, a RANSAC-type robust estimator with a number of novelties.
It benefits from the introduction of the concept of independent inliers that
improves significantly the efficacy of the dominant plane handling and, also,
allows near error-free rejection of incorrect models, without false positives.
The local optimization process and its application is improved so that it is
run on average only once. Further technical improvements include adaptive
sequential hypothesis verification and efficient model estimation via Gaussian
elimination. Experiments on four standard datasets show that VSAC is
significantly faster than all its predecessors and runs on average in 1-2 ms,
on a CPU. It is two orders of magnitude faster and yet as precise as MAGSAC++,
the currently most accurate estimator of two-view geometry. In the repeated
runs on EVD, HPatches, PhotoTourism, and Kusvod2 datasets, it never failed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivashechkin_M/0/1/0/all/0/1"&gt;Maksym Ivashechkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1"&gt;Daniel Barath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1"&gt;Jiri Matas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[hSMAL: Detailed Horse Shape and Pose Reconstruction for Motion Pattern Recognition. (arXiv:2106.10102v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10102</id>
        <link href="http://arxiv.org/abs/2106.10102"/>
        <updated>2021-06-21T02:07:38.111Z</updated>
        <summary type="html"><![CDATA[In this paper we present our preliminary work on model-based behavioral
analysis of horse motion. Our approach is based on the SMAL model, a 3D
articulated statistical model of animal shape. We define a novel SMAL model for
horses based on a new template, skeleton and shape space learned from $37$
horse toys. We test the accuracy of our hSMAL model in reconstructing a horse
from 3D mocap data and images. We apply the hSMAL model to the problem of
lameness detection from video, where we fit the model to images to recover 3D
pose and train an ST-GCN network on pose data. A comparison with the same
network trained on mocap points illustrates the benefit of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Ci Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghorbani_N/0/1/0/all/0/1"&gt;Nima Ghorbani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1"&gt;Sofia Broom&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1"&gt;Maheen Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1"&gt;Michael J. Black&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernlund_E/0/1/0/all/0/1"&gt;Elin Hernlund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1"&gt;Hedvig Kjellstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuffi_S/0/1/0/all/0/1"&gt;Silvia Zuffi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Coarse-to-Fine Instance Segmentation Network with Learning Boundary Representation. (arXiv:2106.10213v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10213</id>
        <link href="http://arxiv.org/abs/2106.10213"/>
        <updated>2021-06-21T02:07:38.104Z</updated>
        <summary type="html"><![CDATA[Boundary-based instance segmentation has drawn much attention since of its
attractive efficiency. However, existing methods suffer from the difficulty in
long-distance regression. In this paper, we propose a coarse-to-fine module to
address the problem. Approximate boundary points are generated at the coarse
stage and then features of these points are sampled and fed to a refined
regressor for fine prediction. It is end-to-end trainable since differential
sampling operation is well supported in the module. Furthermore, we design a
holistic boundary-aware branch and introduce instance-agnostic supervision to
assist regression. Equipped with ResNet-101, our approach achieves 31.7\% mask
AP on COCO dataset with single-scale training and testing, outperforming the
baseline 1.3\% mask AP with less than 1\% additional parameters and GFLOPs.
Experiments also show that our proposed method achieves competitive performance
compared to existing boundary-based methods with a lightweight design and a
simple pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1"&gt;Feng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1"&gt;Bin-Bin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jiangpeng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiased Subjective Assessment of Real-World Image Enhancement. (arXiv:2106.10080v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10080</id>
        <link href="http://arxiv.org/abs/2106.10080"/>
        <updated>2021-06-21T02:07:38.096Z</updated>
        <summary type="html"><![CDATA[In real-world image enhancement, it is often challenging (if not impossible)
to acquire ground-truth data, preventing the adoption of distance metrics for
objective quality assessment. As a result, one often resorts to subjective
quality assessment, the most straightforward and reliable means of evaluating
image enhancement. Conventional subjective testing requires manually
pre-selecting a small set of visual examples, which may suffer from three
sources of biases: 1) sampling bias due to the extremely sparse distribution of
the selected samples in the image space; 2) algorithmic bias due to potential
overfitting the selected samples; 3) subjective bias due to further potential
cherry-picking test results. This eventually makes the field of real-world
image enhancement more of an art than a science. Here we take steps towards
debiasing conventional subjective assessment by automatically sampling a set of
adaptive and diverse images for subsequent testing. This is achieved by casting
sample selection into a joint maximization of the discrepancy between the
enhancers and the diversity among the selected input images. Careful visual
inspection on the resulting enhanced images provides a debiased ranking of the
enhancement algorithms. We demonstrate our subjective assessment method using
three popular and practically demanding image enhancement tasks: dehazing,
super-resolution, and low-light enhancement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhangyang_C/0/1/0/all/0/1"&gt;Cao Peibei. Wang Zhangyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kede_M/0/1/0/all/0/1"&gt;Ma Kede&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping. (arXiv:2106.09965v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09965</id>
        <link href="http://arxiv.org/abs/2106.09965"/>
        <updated>2021-06-21T02:07:38.088Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a high fidelity face swapping method, called
HifiFace, which can well preserve the face shape of the source face and
generate photo-realistic results. Unlike other existing face swapping works
that only use face recognition model to keep the identity similarity, we
propose 3D shape-aware identity to control the face shape with the geometric
supervision from 3DMM and 3D face reconstruction method. Meanwhile, we
introduce the Semantic Facial Fusion module to optimize the combination of
encoder and decoder features and make adaptive blending, which makes the
results more photo-realistic. Extensive experiments on faces in the wild
demonstrate that our method can preserve better identity, especially on the
face shape, and can generate more photo-realistic results than previous
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Junwei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1"&gt;Wenqing Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongjian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for Real-time Traffic Trajectory Tracking, Speed Estimation, and Driver Behavior Calibration at Urban Intersections Using Virtual Traffic Lanes. (arXiv:2106.09932v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09932</id>
        <link href="http://arxiv.org/abs/2106.09932"/>
        <updated>2021-06-21T02:07:38.068Z</updated>
        <summary type="html"><![CDATA[In a previous study, we presented VT-Lane, a three-step framework for
real-time vehicle detection, tracking, and turn movement classification at
urban intersections. In this study, we present a case study incorporating the
highly accurate trajectories and movement classification obtained via VT-Lane
for the purpose of speed estimation and driver behavior calibration for traffic
at urban intersections. First, we use a highly instrumented vehicle to verify
the estimated speeds obtained from video inference. The results of the speed
validation show that our method can estimate the average travel speed of
detected vehicles in real-time with an error of 0.19 m/sec, which is equivalent
to 2% of the average observed travel speeds in the intersection of the study.
Instantaneous speeds (at the resolution of 30 Hz) were found to be estimated
with an average error of 0.21 m/sec and 0.86 m/sec respectively for
free-flowing and congested traffic conditions. We then use the estimated speeds
to calibrate the parameters of a driver behavior model for the vehicles in the
area of study. The results show that the calibrated model replicates the
driving behavior with an average error of 0.45 m/sec, indicating the high
potential for using this framework for automated, large-scale calibration of
car-following models from roadside traffic video data, which can lead to
substantial improvements in traffic modeling via microscopic simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdelhalim_A/0/1/0/all/0/1"&gt;Awad Abdelhalim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbas_M/0/1/0/all/0/1"&gt;Montasir Abbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotha_B/0/1/0/all/0/1"&gt;Bhavi Bharat Kotha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wicks_A/0/1/0/all/0/1"&gt;Alfred Wicks&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Fault Detection in Industrial Welding Processes with Deep Learning and Data Augmentation. (arXiv:2106.10160v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10160</id>
        <link href="http://arxiv.org/abs/2106.10160"/>
        <updated>2021-06-21T02:07:38.061Z</updated>
        <summary type="html"><![CDATA[With the rise of deep learning models in the field of computer vision, new
possibilities for their application in industrial processes proves to return
great benefits. Nevertheless, the actual fit of machine learning for highly
standardised industrial processes is still under debate. This paper addresses
the challenges on the industrial realization of the AI tools, considering the
use case of Laser Beam Welding quality control as an example. We use object
detection algorithms from the TensorFlow object detection API and adapt them to
our use case using transfer learning. The baseline models we develop are used
as benchmarks and evaluated and compared to models that undergo dataset scaling
and hyperparameter tuning. We find that moderate scaling of the dataset via
image augmentation leads to improvements in intersection over union (IoU) and
recall, whereas high levels of augmentation and scaling may lead to
deterioration of results. Finally, we put our results into perspective of the
underlying use case and evaluate their fit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antony_J/0/1/0/all/0/1"&gt;Jibinraj Antony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schlather_D/0/1/0/all/0/1"&gt;Dr. Florian Schlather&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Safronov_G/0/1/0/all/0/1"&gt;Georgij Safronov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmitz_M/0/1/0/all/0/1"&gt;Markus Schmitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laerhoven_P/0/1/0/all/0/1"&gt;Prof. Dr. Kristof Van Laerhoven&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolving GANs: When Contradictions Turn into Compliance. (arXiv:2106.09946v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09946</id>
        <link href="http://arxiv.org/abs/2106.09946"/>
        <updated>2021-06-21T02:07:38.054Z</updated>
        <summary type="html"><![CDATA[Limited availability of labeled-data makes any supervised learning problem
challenging. Alternative learning settings like semi-supervised and universum
learning alleviate the dependency on labeled data, but still require a large
amount of unlabeled data, which may be unavailable or expensive to acquire.
GAN-based synthetic data generation methods have recently shown promise by
generating synthetic samples to improve task at hand. However, these samples
cannot be used for other purposes. In this paper, we propose a GAN game which
provides improved discriminator accuracy under limited data settings, while
generating realistic synthetic data. This provides the added advantage that now
the generated data can be used for other similar tasks. We provide the
theoretical guarantees and empirical results in support of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhar_S/0/1/0/all/0/1"&gt;Sauptik Dhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heydari_J/0/1/0/all/0/1"&gt;Javad Heydari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Samarth Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurup_U/0/1/0/all/0/1"&gt;Unmesh Kurup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mohak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards interpreting computer vision based on transformation invariant optimization. (arXiv:2106.09982v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09982</id>
        <link href="http://arxiv.org/abs/2106.09982"/>
        <updated>2021-06-21T02:07:38.047Z</updated>
        <summary type="html"><![CDATA[Interpreting how does deep neural networks (DNNs) make predictions is a vital
field in artificial intelligence, which hinders wide applications of DNNs.
Visualization of learned representations helps we humans understand the vision
of DNNs. In this work, visualized images that can activate the neural network
to the target classes are generated by back-propagation method. Here, rotation
and scaling operations are applied to introduce the transformation invariance
in the image generating process, which we find a significant improvement on
visualization effect. Finally, we show some cases that such method can help us
to gain insight into neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jinzhe Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tonghuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yaqian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Dongdong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;RenGang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[World-GAN: a Generative Model for Minecraft Worlds. (arXiv:2106.10155v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10155</id>
        <link href="http://arxiv.org/abs/2106.10155"/>
        <updated>2021-06-21T02:07:38.024Z</updated>
        <summary type="html"><![CDATA[This work introduces World-GAN, the first method to perform data-driven
Procedural Content Generation via Machine Learning in Minecraft from a single
example. Based on a 3D Generative Adversarial Network (GAN) architecture, we
are able to create arbitrarily sized world snippets from a given sample. We
evaluate our approach on creations from the community as well as structures
generated with the Minecraft World Generator. Our method is motivated by the
dense representations used in Natural Language Processing (NLP) introduced with
word2vec [1]. The proposed block2vec representations make World-GAN independent
from the number of different blocks, which can vary a lot in Minecraft, and
enable the generation of larger levels. Finally, we demonstrate that changing
this new representation space allows us to change the generated style of an
already trained generator. World-GAN enables its users to generate Minecraft
worlds based on parts of their creations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awiszus_M/0/1/0/all/0/1"&gt;Maren Awiszus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_F/0/1/0/all/0/1"&gt;Frederik Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1"&gt;Bodo Rosenhahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Distraction-Robust Active Visual Tracking. (arXiv:2106.10110v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10110</id>
        <link href="http://arxiv.org/abs/2106.10110"/>
        <updated>2021-06-21T02:07:38.000Z</updated>
        <summary type="html"><![CDATA[In active visual tracking, it is notoriously difficult when distracting
objects appear, as distractors often mislead the tracker by occluding the
target or bringing a confusing appearance. To address this issue, we propose a
mixed cooperative-competitive multi-agent game, where a target and multiple
distractors form a collaborative team to play against a tracker and make it
fail to follow. Through learning in our game, diverse distracting behaviors of
the distractors naturally emerge, thereby exposing the tracker's weakness,
which helps enhance the distraction-robustness of the tracker. For effective
learning, we then present a bunch of practical methods, including a reward
function for distractors, a cross-modal teacher-student learning strategy, and
a recurrent attention mechanism for the tracker. The experimental results show
that our tracker performs desired distraction-robust active visual tracking and
can be well generalized to unseen environments. We also show that the
multi-agent game can be used to adversarially test the robustness of trackers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1"&gt;Fangwei Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1"&gt;Peng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1"&gt;Wenhan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1"&gt;Tingyun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhou Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape Prior Non-Uniform Sampling Guided Real-time Stereo 3D Object Detection. (arXiv:2106.10013v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10013</id>
        <link href="http://arxiv.org/abs/2106.10013"/>
        <updated>2021-06-21T02:07:37.993Z</updated>
        <summary type="html"><![CDATA[Pseudo-LiDAR based 3D object detectors have gained popularity due to their
high accuracy. However, these methods need dense depth supervision and suffer
from inferior speed. To solve these two issues, a recently introduced RTS3D
builds an efficient 4D Feature-Consistency Embedding (FCE) space for the
intermediate representation of object without depth supervision. FCE space
splits the entire object region into 3D uniform grid latent space for feature
sampling point generation, which ignores the importance of different object
regions. However, we argue that, compared with the inner region, the outer
region plays a more important role for accurate 3D detection. To encode more
information from the outer region, we propose a shape prior non-uniform
sampling strategy that performs dense sampling in outer region and sparse
sampling in inner region. As a result, more points are sampled from the outer
region and more useful features are extracted for 3D detection. Further, to
enhance the feature discrimination of each sampling point, we propose a
high-level semantic enhanced FCE module to exploit more contextual information
and suppress noise better. Experiments on the KITTI dataset are performed to
show the effectiveness of the proposed method. Compared with the baseline
RTS3D, our proposed method has 2.57% improvement on AP3d almost without extra
network parameters. Moreover, our proposed method outperforms the
state-of-the-art methods without extra supervision at a real-time speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1"&gt;A. Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;J. Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1"&gt;Y. Pang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advanced Hough-based method for on-device document localization. (arXiv:2106.09987v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09987</id>
        <link href="http://arxiv.org/abs/2106.09987"/>
        <updated>2021-06-21T02:07:37.973Z</updated>
        <summary type="html"><![CDATA[The demand for on-device document recognition systems increases in
conjunction with the emergence of more strict privacy and security
requirements. In such systems, there is no data transfer from the end device to
a third-party information processing servers. The response time is vital to the
user experience of on-device document recognition. Combined with the
unavailability of discrete GPUs, powerful CPUs, or a large RAM capacity on
consumer-grade end devices such as smartphones, the time limitations put
significant constraints on the computational complexity of the applied
algorithms for on-device execution.

In this work, we consider document location in an image without prior
knowledge of the document content or its internal structure. In accordance with
the published works, at least 5 systems offer solutions for on-device document
location. All these systems use a location method which can be considered
Hough-based. The precision of such systems seems to be lower than that of the
state-of-the-art solutions which were not designed to account for the limited
computational resources.

We propose an advanced Hough-based method. In contrast with other approaches,
it accounts for the geometric invariants of the central projection model and
combines both edge and color features for document boundary detection. The
proposed method allowed for the second best result for SmartDoc dataset in
terms of precision, surpassed by U-net like neural network. When evaluated on a
more challenging MIDV-500 dataset, the proposed algorithm guaranteed the best
precision compared to published methods. Our method retained the applicability
to on-device computations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tropin_D/0/1/0/all/0/1"&gt;D.V. Tropin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ershov_A/0/1/0/all/0/1"&gt;A.M. Ershov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1"&gt;D.P. Nikolaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arlazarov_V/0/1/0/all/0/1"&gt;V.V. Arlazarov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariance-bridged SO(2)-Invariant Representation Learning using Graph Convolutional Network. (arXiv:2106.09996v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09996</id>
        <link href="http://arxiv.org/abs/2106.09996"/>
        <updated>2021-06-21T02:07:37.966Z</updated>
        <summary type="html"><![CDATA[Training a Convolutional Neural Network (CNN) to be robust against rotation
has mostly been done with data augmentation. In this paper, another progressive
vision of research direction is highlighted to encourage less dependence on
data augmentation by achieving structural rotational invariance of a network.
The deep equivariance-bridged SO(2) invariant network is proposed to echo such
vision. First, Self-Weighted Nearest Neighbors Graph Convolutional Network
(SWN-GCN) is proposed to implement Graph Convolutional Network (GCN) on the
graph representation of an image to acquire rotationally equivariant
representation, as GCN is more suitable for constructing deeper network than
spectral graph convolution-based approaches. Then, invariant representation is
eventually obtained with Global Average Pooling (GAP), a permutation-invariant
operation suitable for aggregating high-dimensional representations, over the
equivariant set of vertices retrieved from SWN-GCN. Our method achieves the
state-of-the-art image classification performance on rotated MNIST and CIFAR-10
images, where the models are trained with a non-augmented dataset only.
Quantitative validations over invariance of the representations also
demonstrate strong invariance of deep representations of SWN-GCN over
rotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sungwon Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1"&gt;Hyungtae Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1"&gt;Hyun Myung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Generative Adversarial Network Training via Self-Labeling and Self-Attention. (arXiv:2106.09914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09914</id>
        <link href="http://arxiv.org/abs/2106.09914"/>
        <updated>2021-06-21T02:07:37.958Z</updated>
        <summary type="html"><![CDATA[We propose a novel GAN training scheme that can handle any level of labeling
in a unified manner. Our scheme introduces a form of artificial labeling that
can incorporate manually defined labels, when available, and induce an
alignment between them. To define the artificial labels, we exploit the
assumption that neural network generators can be trained more easily to map
nearby latent vectors to data with semantic similarities, than across separate
categories. We use generated data samples and their corresponding artificial
conditioning labels to train a classifier. The classifier is then used to
self-label real data. To boost the accuracy of the self-labeling, we also use
the exponential moving average of the classifier. However, because the
classifier might still make mistakes, especially at the beginning of the
training, we also refine the labels through self-attention, by using the
labeling of real data samples only when the classifier outputs a high
classification probability score. We evaluate our approach on CIFAR-10, STL-10
and SVHN, and show that both self-labeling and self-attention consistently
improve the quality of generated data. More surprisingly, we find that the
proposed scheme can even outperform class-conditional GANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1"&gt;Tomoki Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1"&gt;Paolo Favaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Radar Localization on Lidar Maps Using Shared Embedding. (arXiv:2106.10000v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.10000</id>
        <link href="http://arxiv.org/abs/2106.10000"/>
        <updated>2021-06-21T02:07:37.939Z</updated>
        <summary type="html"><![CDATA[We present a heterogeneous localization framework for solving radar global
localization and pose tracking on pre-built lidar maps. To bridge the gap of
sensing modalities, deep neural networks are constructed to create shared
embedding space for radar scans and lidar maps. Herein learned feature
embeddings are supportive for similarity measurement, thus improving map
retrieval and data matching respectively. In RobotCar and MulRan datasets, we
demonstrate the effectiveness of the proposed framework with the comparison to
Scan Context and RaLL. In addition, the proposed pose tracking pipeline is with
less neural networks compared to the original RaLL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Huan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1"&gt;Rong Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combined Person Classification with Airborne Optical Sectioning. (arXiv:2106.10077v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10077</id>
        <link href="http://arxiv.org/abs/2106.10077"/>
        <updated>2021-06-21T02:07:37.932Z</updated>
        <summary type="html"><![CDATA[Fully autonomous drones have been demonstrated to find lost or injured
persons under strongly occluding forest canopy. Airborne Optical Sectioning
(AOS), a novel synthetic aperture imaging technique, together with
deep-learning-based classification enables high detection rates under realistic
search-and-rescue conditions. We demonstrate that false detections can be
significantly suppressed and true detections boosted by combining
classifications from multiple AOS rather than single integral images. This
improves classification rates especially in the presence of occlusion. To make
this possible, we modified the AOS imaging process to support large overlaps
between subsequent integrals, enabling real-time and on-board scanning and
processing of groundspeeds up to 10 m/s.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kurmi_I/0/1/0/all/0/1"&gt;Indrajit Kurmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schedl_D/0/1/0/all/0/1"&gt;David C. Schedl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bimber_O/0/1/0/all/0/1"&gt;Oliver Bimber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accumulative Poisoning Attacks on Real-time Data. (arXiv:2106.09993v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09993</id>
        <link href="http://arxiv.org/abs/2106.09993"/>
        <updated>2021-06-21T02:07:37.919Z</updated>
        <summary type="html"><![CDATA[Collecting training data from untrusted sources exposes machine learning
services to poisoning adversaries, who maliciously manipulate training data to
degrade the model accuracy. When trained on offline datasets, poisoning
adversaries have to inject the poisoned data in advance before training, and
the order of feeding these poisoned batches into the model is stochastic. In
contrast, practical systems are more usually trained/fine-tuned on sequentially
captured real-time data, in which case poisoning adversaries could dynamically
poison each data batch according to the current model state. In this paper, we
focus on the real-time settings and propose a new attacking strategy, which
affiliates an accumulative phase with poisoning attacks to secretly (i.e.,
without affecting accuracy) magnify the destructive effect of a (poisoned)
trigger batch. By mimicking online learning and federated learning on CIFAR-10,
we show that the model accuracy will significantly drop by a single update step
on the trigger batch after the accumulative phase. Our work validates that a
well-designed but straightforward attacking strategy can dramatically amplify
the poisoning effects, with no need to explore complex techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1"&gt;Tianyu Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples. (arXiv:2106.09947v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09947</id>
        <link href="http://arxiv.org/abs/2106.09947"/>
        <updated>2021-06-21T02:07:37.888Z</updated>
        <summary type="html"><![CDATA[Evaluating robustness of machine-learning models to adversarial examples is a
challenging problem. Many defenses have been shown to provide a false sense of
security by causing gradient-based attacks to fail, and they have been broken
under more rigorous evaluations. Although guidelines and best practices have
been suggested to improve current adversarial robustness evaluations, the lack
of automatic testing and debugging tools makes it difficult to apply these
recommendations in a systematic manner. In this work, we overcome these
limitations by (i) defining a set of quantitative indicators which unveil
common failures in the optimization of gradient-based attacks, and (ii)
proposing specific mitigation strategies within a systematic evaluation
protocol. Our extensive experimental analysis shows that the proposed
indicators of failure can be used to visualize, debug and improve current
adversarial robustness evaluations, providing a first concrete step towards
automatizing and systematizing current adversarial robustness evaluations. Our
open-source code is available at:
https://github.com/pralab/IndicatorsOfAttackFailure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1"&gt;Maura Pintor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demetrio_L/0/1/0/all/0/1"&gt;Luca Demetrio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotgiu_A/0/1/0/all/0/1"&gt;Angelo Sotgiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manca_G/0/1/0/all/0/1"&gt;Giovanni Manca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1"&gt;Ambra Demontis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1"&gt;Battista Biggio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1"&gt;Fabio Roli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Lies: Optical Adversarial Attack. (arXiv:2106.09908v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09908</id>
        <link href="http://arxiv.org/abs/2106.09908"/>
        <updated>2021-06-21T02:07:37.880Z</updated>
        <summary type="html"><![CDATA[A significant amount of work has been done on adversarial attacks that inject
imperceptible noise to images to deteriorate the image classification
performance of deep models. However, most of the existing studies consider
attacks in the digital (pixel) domain where an image acquired by an image
sensor with sampling and quantization has been recorded. This paper, for the
first time, introduces an optical adversarial attack, which physically alters
the light field information arriving at the image sensor so that the
classification model yields misclassification. More specifically, we modulate
the phase of the light in the Fourier domain using a spatial light modulator
placed in the photographic system. The operative parameters of the modulator
are obtained by gradient-based optimization to maximize cross-entropy and
minimize distortions. We present experiments based on both simulation and a
real hardware optical system, from which the feasibility of the proposed
optical attack is demonstrated. It is also verified that the proposed attack is
completely different from common optical-domain distortions such as spherical
aberration, defocus, and astigmatism in terms of both perturbation patterns and
classification results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyu-Lim Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jeong-Soo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Seung-Ri Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jun-Ho Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_C/0/1/0/all/0/1"&gt;Chul-Min Joo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jong-Seok Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Matting: A New Perspective on Medical Segmentation with Uncertainty. (arXiv:2106.09887v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09887</id>
        <link href="http://arxiv.org/abs/2106.09887"/>
        <updated>2021-06-21T02:07:37.814Z</updated>
        <summary type="html"><![CDATA[In medical image segmentation, it is difficult to mark ambiguous areas
accurately with binary masks, especially when dealing with small lesions.
Therefore, it is a challenge for radiologists to reach a consensus by using
binary masks under the condition of multiple annotations. However, these areas
may contain anatomical structures that are conducive to diagnosis. Uncertainty
is introduced to study these situations. Nevertheless, the uncertainty is
usually measured by the variances between predictions in a multiple trial way.
It is not intuitive, and there is no exact correspondence in the image.
Inspired by image matting, we introduce matting as a soft segmentation method
and a new perspective to deal with and represent uncertain regions into medical
scenes, namely medical matting. More specifically, because there is no
available medical matting dataset, we first labeled two medical datasets with
alpha matte. Secondly, the matting method applied to the natural image is not
suitable for the medical scene, so we propose a new architecture to generate
binary masks and alpha matte in a row. Thirdly, the uncertainty map is
introduced to highlight the ambiguous regions from the binary results and
improve the matting performance. Evaluated on these datasets, the proposed
model outperformed state-of-the-art matting algorithms by a large margin, and
alpha matte is proved to be a more efficient labeling form than a binary mask.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1"&gt;Lie Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Donghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wanji He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yelin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhiwen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xuan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiufen Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1"&gt;Zongyuan Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python. (arXiv:2106.09756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09756</id>
        <link href="http://arxiv.org/abs/2106.09756"/>
        <updated>2021-06-21T02:07:37.799Z</updated>
        <summary type="html"><![CDATA[Machine learning is a general-purpose technology holding promises for many
interdisciplinary research problems. However, significant barriers exist in
crossing disciplinary boundaries when most machine learning tools are developed
in different areas separately. We present Pykale - a Python library for
knowledge-aware machine learning on graphs, images, texts, and videos to enable
and accelerate interdisciplinary research. We formulate new green machine
learning guidelines based on standard software engineering practices and
propose a novel pipeline-based application programming interface (API). PyKale
focuses on leveraging knowledge from multiple sources for accurate and
interpretable prediction, thus supporting multimodal learning and transfer
learning (particularly domain adaptation) with latest deep learning and
dimensionality reduction models. We build PyKale on PyTorch and leverage the
rich PyTorch ecosystem. Our pipeline-based API design enforces standardization
and minimalism, embracing green machine learning concepts via reducing
repetitions and redundancy, reusing existing resources, and recycling learning
models across areas. We demonstrate its interdisciplinary nature via examples
in bioinformatics, knowledge graph, image/video recognition, and medical
imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haiping Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1"&gt;Robert Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_P/0/1/0/all/0/1"&gt;Peizhen Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1"&gt;Raivo E Koot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chasmai_M/0/1/0/all/0/1"&gt;Mustafa Chasmai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schobs_L/0/1/0/all/0/1"&gt;Lawrence Schobs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep reinforcement learning with automated label extraction from clinical reports accurately classifies 3D MRI brain volumes. (arXiv:2106.09812v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09812</id>
        <link href="http://arxiv.org/abs/2106.09812"/>
        <updated>2021-06-21T02:07:37.792Z</updated>
        <summary type="html"><![CDATA[Purpose: Image classification is perhaps the most fundamental task in imaging
AI. However, labeling images is time-consuming and tedious. We have recently
demonstrated that reinforcement learning (RL) can classify 2D slices of MRI
brain images with high accuracy. Here we make two important steps toward
speeding image classification: Firstly, we automatically extract class labels
from the clinical reports. Secondly, we extend our prior 2D classification work
to fully 3D image volumes from our institution. Hence, we proceed as follows:
in Part 1, we extract labels from reports automatically using the SBERT natural
language processing approach. Then, in Part 2, we use these labels with RL to
train a classification Deep-Q Network (DQN) for 3D image volumes.

Methods: For Part 1, we trained SBERT with 90 radiology report impressions.
We then used the trained SBERT to predict class labels for use in Part 2. In
Part 2, we applied multi-step image classification to allow for combined Deep-Q
learning using 3D convolutions and TD(0) Q learning. We trained on a set of 90
images. We tested on a separate set of 61 images, again using the classes
predicted from patient reports by the trained SBERT in Part 1. For comparison,
we also trained and tested a supervised deep learning classification network on
the same set of training and testing images using the same labels.

Results: Part 1: Upon training with the corpus of radiology reports, the
SBERT model had 100% accuracy for both normal and metastasis-containing scans.
Part 2: Then, using these labels, whereas the supervised approach quickly
overfit the training data and as expected performed poorly on the testing set
(66% accuracy, just over random guessing), the reinforcement learning approach
achieved an accuracy of 92%. The results were found to be statistically
significant, with a p-value of 3.1 x 10^-5.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stember_J/0/1/0/all/0/1"&gt;Joseph Stember&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1"&gt;Hrithwik Shalu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantized Neural Networks via {-1, +1} Encoding Decomposition and Acceleration. (arXiv:2106.09886v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09886</id>
        <link href="http://arxiv.org/abs/2106.09886"/>
        <updated>2021-06-21T02:07:37.784Z</updated>
        <summary type="html"><![CDATA[The training of deep neural networks (DNNs) always requires intensive
resources for both computation and data storage. Thus, DNNs cannot be
efficiently applied to mobile phones and embedded devices, which severely
limits their applicability in industrial applications. To address this issue,
we propose a novel encoding scheme using {-1, +1} to decompose quantized neural
networks (QNNs) into multi-branch binary networks, which can be efficiently
implemented by bitwise operations (i.e., xnor and bitcount) to achieve model
compression, computational acceleration, and resource saving. By using our
method, users can achieve different encoding precisions arbitrarily according
to their requirements and hardware resources. The proposed mechanism is highly
suitable for the use of FPGA and ASIC in terms of data storage and computation,
which provides a feasible idea for smart chips. We validate the effectiveness
of our method on large-scale image classification (e.g., ImageNet), object
detection, and semantic segmentation tasks. In particular, our method with
low-bit encoding can still achieve almost the same performance as its high-bit
counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Qigong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiufang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1"&gt;Fanhua Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1"&gt;Licheng Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Model Sparsification by Scheduled Grow-and-Prune Methods. (arXiv:2106.09857v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09857</id>
        <link href="http://arxiv.org/abs/2106.09857"/>
        <updated>2021-06-21T02:07:37.777Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are effective in solving many real-world
problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but
their excessive computation results in long training and inference time. Model
sparsification can reduce the computation and memory cost while maintaining
model quality. Most existing sparsification algorithms unidirectionally remove
weights, while others randomly or greedily explore a small subset of weights in
each layer. The inefficiency of the algorithms reduces the achievable sparsity
level. In addition, many algorithms still require pre-trained dense models and
thus suffer from large memory footprint and long training time. In this paper,
we propose a novel scheduled grow-and-prune (GaP) methodology without
pre-training the dense models. It addresses the shortcomings of the previous
works by repeatedly growing a subset of layers to dense and then pruning back
to sparse after some training. Experiments have shown that such models can
match or beat the quality of highly optimized dense models at 80% sparsity on a
variety of tasks, such as image classification, objective detection, 3D object
part segmentation, and translation. They also outperform other state-of-the-art
(SOTA) pruning methods, including pruning from pre-trained dense models. As an
example, a 90% sparse ResNet-50 obtained via GaP achieves 77.9% top-1 accuracy
on ImageNet, improving the SOTA results by 1.5%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1"&gt;Minghai Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1"&gt;Fei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1"&gt;Zejiang Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Kun Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Kuang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Rong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuan Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Distance-based Separability Measure for Internal Cluster Validation. (arXiv:2106.09794v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09794</id>
        <link href="http://arxiv.org/abs/2106.09794"/>
        <updated>2021-06-21T02:07:37.759Z</updated>
        <summary type="html"><![CDATA[To evaluate clustering results is a significant part of cluster analysis.
Since there are no true class labels for clustering in typical unsupervised
learning, many internal cluster validity indices (CVIs), which use predicted
labels and data, have been created. Without true labels, to design an effective
CVI is as difficult as to create a clustering method. And it is crucial to have
more CVIs because there are no universal CVIs that can be used to measure all
datasets and no specific methods of selecting a proper CVI for clusters without
true labels. Therefore, to apply a variety of CVIs to evaluate clustering
results is necessary. In this paper, we propose a novel internal CVI -- the
Distance-based Separability Index (DSI), based on a data separability measure.
We compared the DSI with eight internal CVIs including studies from early Dunn
(1974) to most recent CVDD (2019) and an external CVI as ground truth, by using
clustering results of five clustering algorithms on 12 real and 97 synthetic
datasets. Results show DSI is an effective, unique, and competitive CVI to
other compared CVIs. We also summarized the general process to evaluate CVIs
and created the rank-difference metric for comparison of CVIs' results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1"&gt;Shuyue Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1"&gt;Murray Loew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepLab2: A TensorFlow Library for Deep Labeling. (arXiv:2106.09748v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09748</id>
        <link href="http://arxiv.org/abs/2106.09748"/>
        <updated>2021-06-21T02:07:37.711Z</updated>
        <summary type="html"><![CDATA[DeepLab2 is a TensorFlow library for deep labeling, aiming to provide a
state-of-the-art and easy-to-use TensorFlow codebase for general dense pixel
prediction problems in computer vision. DeepLab2 includes all our recently
developed DeepLab model variants with pretrained checkpoints as well as model
training and evaluation code, allowing the community to reproduce and further
improve upon the state-of-art systems. To showcase the effectiveness of
DeepLab2, our Panoptic-DeepLab employing Axial-SWideRNet as network backbone
achieves 68.0% PQ or 83.5% mIoU on Cityscaspes validation set, with only
single-scale inference and ImageNet-1K pretrained checkpoints. We hope that
publicly sharing our library could facilitate future research on dense pixel
labeling tasks and envision new applications of this technology. Code is made
publicly available at \url{https://github.com/google-research/deeplab2}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1"&gt;Mark Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huiyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1"&gt;Siyuan Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jun Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1"&gt;Maxwell D. Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yukun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Liangzhe Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dahun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qihang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1"&gt;Laura Leal-Taixe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan L. Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schroff_F/0/1/0/all/0/1"&gt;Florian Schroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1"&gt;Hartwig Adam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang-Chieh Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RSG: A Simple but Effective Module for Learning Imbalanced Datasets. (arXiv:2106.09859v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09859</id>
        <link href="http://arxiv.org/abs/2106.09859"/>
        <updated>2021-06-21T02:07:37.694Z</updated>
        <summary type="html"><![CDATA[Imbalanced datasets widely exist in practice and area great challenge for
training deep neural models with agood generalization on infrequent classes. In
this work, wepropose a new rare-class sample generator (RSG) to solvethis
problem. RSG aims to generate some new samplesfor rare classes during training,
and it has in particularthe following advantages: (1) it is convenient to use
andhighly versatile, because it can be easily integrated intoany kind of
convolutional neural network, and it works wellwhen combined with different
loss functions, and (2) it isonly used during the training phase, and
therefore, no ad-ditional burden is imposed on deep neural networks duringthe
testing phase. In extensive experimental evaluations, weverify the
effectiveness of RSG. Furthermore, by leveragingRSG, we obtain competitive
results on Imbalanced CIFARand new state-of-the-art results on Places-LT,
ImageNet-LT, and iNaturalist 2018. The source code is available at
https://github.com/Jianf-Wang/RSG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaolin Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhenghua Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid graph convolutional neural networks for landmark-based anatomical segmentation. (arXiv:2106.09832v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09832</id>
        <link href="http://arxiv.org/abs/2106.09832"/>
        <updated>2021-06-21T02:07:37.671Z</updated>
        <summary type="html"><![CDATA[In this work we address the problem of landmark-based segmentation for
anatomical structures. We propose HybridGNet, an encoder-decoder neural
architecture which combines standard convolutions for image feature encoding,
with graph convolutional neural networks to decode plausible representations of
anatomical structures. We benchmark the proposed architecture considering other
standard landmark and pixel-based models for anatomical segmentation in chest
x-ray images, and found that HybridGNet is more robust to image occlusions. We
also show that it can be used to construct landmark-based segmentations from
pixel level annotations. Our experimental results suggest that HybridGNet
produces accurate and anatomically plausible landmark-based segmentations, by
naturally incorporating shape constraints within the decoding process via
spectral convolutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gaggion_N/0/1/0/all/0/1"&gt;Nicol&amp;#xe1;s Gaggion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mansilla_L/0/1/0/all/0/1"&gt;Lucas Mansilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Milone_D/0/1/0/all/0/1"&gt;Diego Milone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1"&gt;Enzo Ferrante&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-Enabled Ultra-Low-Dose CT Reconstruction. (arXiv:2106.09834v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09834</id>
        <link href="http://arxiv.org/abs/2106.09834"/>
        <updated>2021-06-21T02:07:37.644Z</updated>
        <summary type="html"><![CDATA[By the ALARA (As Low As Reasonably Achievable) principle, ultra-low-dose CT
reconstruction is a holy grail to minimize cancer risks and genetic damages,
especially for children. With the development of medical CT technologies, the
iterative algorithms are widely used to reconstruct decent CT images from a
low-dose scan. Recently, artificial intelligence (AI) techniques have shown a
great promise in further reducing CT radiation dose to the next level. In this
paper, we demonstrate that AI-powered CT reconstruction offers diagnostic image
quality at an ultra-low-dose level comparable to that of radiography.
Specifically, here we develop a Split Unrolled Grid-like Alternative
Reconstruction (SUGAR) network, in which deep learning, physical modeling and
image prior are integrated. The reconstruction results from clinical datasets
show that excellent images can be reconstructed using SUGAR from 36
projections. This approach has a potential to change future healthcare.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1"&gt;Weiwen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1"&gt;Chuang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebrahimian_S/0/1/0/all/0/1"&gt;Shadi Ebrahimian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hengyong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kalra_M/0/1/0/all/0/1"&gt;Mannu Kalra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Self-supervised Vision Transformers for Representation Learning. (arXiv:2106.09785v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09785</id>
        <link href="http://arxiv.org/abs/2106.09785"/>
        <updated>2021-06-21T02:07:37.637Z</updated>
        <summary type="html"><![CDATA[This paper investigates two techniques for developing efficient
self-supervised vision transformers (EsViT) for visual representation learning.
First, we show through a comprehensive empirical study that multi-stage
architectures with sparse self-attentions can significantly reduce modeling
complexity but with a cost of losing the ability to capture fine-grained
correspondences between image regions. Second, we propose a new pre-training
task of region matching which allows the model to capture fine-grained region
dependencies and as a result significantly improves the quality of the learned
vision representations. Our results show that combining the two techniques,
EsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation,
outperforming prior arts with around an order magnitude of higher throughput.
When transferring to downstream linear classification tasks, EsViT outperforms
its supervised counterpart on 17 out of 18 datasets. The code and models will
be publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengchuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1"&gt;Bin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiyang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided Integrated Gradients: An Adaptive Path Method for Removing Noise. (arXiv:2106.09788v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09788</id>
        <link href="http://arxiv.org/abs/2106.09788"/>
        <updated>2021-06-21T02:07:37.630Z</updated>
        <summary type="html"><![CDATA[Integrated Gradients (IG) is a commonly used feature attribution method for
deep neural networks. While IG has many desirable properties, the method often
produces spurious/noisy pixel attributions in regions that are not related to
the predicted class when applied to visual models. While this has been
previously noted, most existing solutions are aimed at addressing the symptoms
by explicitly reducing the noise in the resulting attributions. In this work,
we show that one of the causes of the problem is the accumulation of noise
along the IG path. To minimize the effect of this source of noise, we propose
adapting the attribution path itself -- conditioning the path not just on the
image but also on the model being explained. We introduce Adaptive Path Methods
(APMs) as a generalization of path methods, and Guided IG as a specific
instance of an APM. Empirically, Guided IG creates saliency maps better aligned
with the model's prediction and the input image that is being explained. We
show through qualitative and quantitative experiments that Guided IG
outperforms other, related methods in nearly every experiment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kapishnikov_A/0/1/0/all/0/1"&gt;Andrei Kapishnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1"&gt;Subhashini Venugopalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avci_B/0/1/0/all/0/1"&gt;Besim Avci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wedin_B/0/1/0/all/0/1"&gt;Ben Wedin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1"&gt;Michael Terry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1"&gt;Tolga Bolukbasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothed Multi-View Subspace Clustering. (arXiv:2106.09875v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09875</id>
        <link href="http://arxiv.org/abs/2106.09875"/>
        <updated>2021-06-21T02:07:37.609Z</updated>
        <summary type="html"><![CDATA[In recent years, multi-view subspace clustering has achieved impressive
performance due to the exploitation of complementary imformation across
multiple views. However, multi-view data can be very complicated and are not
easy to cluster in real-world applications. Most existing methods operate on
raw data and may not obtain the optimal solution. In this work, we propose a
novel multi-view clustering method named smoothed multi-view subspace
clustering (SMVSC) by employing a novel technique, i.e., graph filtering, to
obtain a smooth representation for each view, in which similar data points have
similar feature values. Specifically, it retains the graph geometric features
through applying a low-pass filter. Consequently, it produces a
``clustering-friendly" representation and greatly facilitates the downstream
clustering task. Extensive experiments on benchmark datasets validate the
superiority of our approach. Analysis shows that graph filtering increases the
separability of classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Peng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhengrui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1"&gt;Zhao Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic COVID-19 Chest X-ray Dataset for Computer-Aided Diagnosis. (arXiv:2106.09759v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09759</id>
        <link href="http://arxiv.org/abs/2106.09759"/>
        <updated>2021-06-21T02:07:37.584Z</updated>
        <summary type="html"><![CDATA[We introduce a new dataset called Synthetic COVID-19 Chest X-ray Dataset for
training machine learning models. The dataset consists of 21,295 synthetic
COVID-19 chest X-ray images to be used for computer-aided diagnosis. These
images, generated via an unsupervised domain adaptation approach, are of high
quality. We find that the synthetic images not only improve performance of
various deep learning architectures when used as additional training data under
heavy imbalance conditions, but also detect the target class with high
confidence. We also find that comparable performance can also be achieved when
trained only on synthetic images. Further, salient features of the synthetic
COVID-19 images indicate that the distribution is significantly different from
Non-COVID-19 classes, enabling a proper decision boundary. We hope the
availability of such high fidelity chest X-ray images of COVID-19 will
encourage advances in the development of diagnostic and/or management tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zunair_H/0/1/0/all/0/1"&gt;Hasib Zunair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hamza_A/0/1/0/all/0/1"&gt;A. Ben Hamza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Development of a conversing and body temperature scanning autonomously navigating robot to help screen for COVID-19. (arXiv:2106.09894v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.09894</id>
        <link href="http://arxiv.org/abs/2106.09894"/>
        <updated>2021-06-21T02:07:37.576Z</updated>
        <summary type="html"><![CDATA[Throughout the COVID-19 pandemic, the most common symptom displayed by
patients has been a fever, leading to the use of temperature scanning as a
preemptive measure to detect potential carriers of the virus. Human employees
with handheld thermometers have been used to fulfill this task, however this
puts them at risk as they cannot be physically distanced and the sequential
nature of this method leads to great inconveniences and inefficiency. The
proposed solution is an autonomously navigating robot capable of conversing and
scanning people's temperature to detect fevers and help screen for COVID-19. To
satisfy this objective, the robot must be able to (1) navigate autonomously,
(2) detect and track people, and (3) get individuals' temperature reading and
converse with them if it exceeds 38{\deg}C. An autonomously navigating mobile
robot is used with a manipulator controlled using a face tracking algorithm,
and an end effector consisting of a thermal camera, smartphone, and chatbot.
The goal is to develop a functioning solution that performs the above tasks. In
addition, technical challenges encountered and their engineering solutions will
be presented, and recommendations will be made for enhancements that could be
incorporated when approaching commercialization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_R/0/1/0/all/0/1"&gt;Ryan Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Transformers for Neural Machine Translation. (arXiv:2106.02242v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02242</id>
        <link href="http://arxiv.org/abs/2106.02242"/>
        <updated>2021-06-21T02:07:37.440Z</updated>
        <summary type="html"><![CDATA[Transformer has been widely adopted in Neural Machine Translation (NMT)
because of its large capacity and parallel training of sequence generation.
However, the deployment of Transformer is challenging because different
scenarios require models of different complexities and scales. Naively training
multiple Transformers is redundant in terms of both computation and memory. In
this paper, we propose a novel Scalable Transformers, which naturally contains
sub-Transformers of different scales and have shared parameters. Each
sub-Transformer can be easily obtained by cropping the parameters of the
largest Transformer. A three-stage training scheme is proposed to tackle the
difficulty of training the Scalable Transformers, which introduces additional
supervisions from word-level and sequence-level self-distillation. Extensive
experiments were conducted on WMT EN-De and En-Fr to validate our proposed
Scalable Transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1"&gt;Shijie Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jifeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces. (arXiv:2106.07505v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07505</id>
        <link href="http://arxiv.org/abs/2106.07505"/>
        <updated>2021-06-21T02:07:37.433Z</updated>
        <summary type="html"><![CDATA[Hate speech and profanity detection suffer from data sparsity, especially for
languages other than English, due to the subjective nature of the tasks and the
resulting annotation incompatibility of existing corpora. In this study, we
identify profane subspaces in word and sentence representations and explore
their generalization capability on a variety of similar and distant target
tasks in a zero-shot setting. This is done monolingually (German) and
cross-lingually to closely-related (English), distantly-related (French) and
non-related (Arabic) tasks. We observe that, on both similar and distant target
tasks and across all languages, the subspace-based representations transfer
more effectively than standard BERT representations in the zero-shot setting,
with improvements between F1 +10.9 and F1 +42.9 over the baselines across all
tested monolingual and cross-lingual scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hahn_V/0/1/0/all/0/1"&gt;Vanessa Hahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1"&gt;Dana Ruiter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleinbauer_T/0/1/0/all/0/1"&gt;Thomas Kleinbauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1"&gt;Dietrich Klakow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Speech Translation via Cross-modal Progressive Training. (arXiv:2104.10380v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10380</id>
        <link href="http://arxiv.org/abs/2104.10380"/>
        <updated>2021-06-21T02:07:37.426Z</updated>
        <summary type="html"><![CDATA[End-to-end speech translation models have become a new trend in research due
to their potential of reducing error propagation. However, these models still
suffer from the challenge of data scarcity. How to effectively use unlabeled or
other parallel corpora from machine translation is promising but still an open
problem. In this paper, we propose Cross Speech-Text Network (XSTNet), an
end-to-end model for speech-to-text translation. XSTNet takes both speech and
text as input and outputs both transcription and translation text. The model
benefits from its three key design aspects: a self-supervised pre-trained
sub-network as the audio encoder, a multi-task training objective to exploit
additional parallel bilingual text, and a progressive training procedure. We
evaluate the performance of XSTNet and baselines on the MuST-C En-X and
LibriSpeech En-Fr datasets. In particular, XSTNet achieves state-of-the-art
results on all language directions with an average BLEU of 28.8, outperforming
the previous best method by 3.2 BLEU. Code, models, cases, and more detailed
analysis are available at https://github.com/ReneeYe/XSTNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1"&gt;Rong Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit. (arXiv:2102.01547v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01547</id>
        <link href="http://arxiv.org/abs/2102.01547"/>
        <updated>2021-06-21T02:07:37.376Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an open source, production first, and production
ready speech recognition toolkit called WeNet in which a new two-pass approach
is implemented to unify streaming and non-streaming end-to-end (E2E) speech
recognition in a single model. The main motivation of WeNet is to close the gap
between the research and the production of E2E speechrecognition models. WeNet
provides an efficient way to ship ASR applications in several real-world
scenarios, which is the main difference and advantage to other open source E2E
speech recognition toolkits. In our toolkit, a new two-pass method is
implemented. Our method propose a dynamic chunk-based attention strategy of the
the transformer layers to allow arbitrary right context length modifies in
hybrid CTC/attention architecture. The inference latency could be easily
controlled by only changing the chunk size. The CTC hypotheses are then
rescored by the attention decoder to get the final result. Our experiments on
the AISHELL-1 dataset using WeNet show that, our model achieves 5.03\% relative
character error rate (CER) reduction in non-streaming ASR compared to a
standard non-streaming transformer. After model quantification, our model
perform reasonable RTF and latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhuoyuan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Di Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Binbin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhendong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1"&gt;Xin Lei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSFCube -- A Test Collection of Computer Science Research Articles for Faceted Query by Example. (arXiv:2103.12906v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12906</id>
        <link href="http://arxiv.org/abs/2103.12906"/>
        <updated>2021-06-21T02:07:37.356Z</updated>
        <summary type="html"><![CDATA[Query by Example is a well-known information retrieval task in which a
document is chosen by the user as the search query and the goal is to retrieve
relevant documents from a large collection. However, a document often covers
multiple aspects of a topic. To address this scenario we introduce the task of
faceted Query by Example in which users can also specify a finer grained aspect
in addition to the input query document. We focus on the application of this
task in scientific literature search. We envision models which are able to
retrieve scientific papers analogous to a query scientific paper along
specifically chosen rhetorical structure elements as one solution to this
problem. In this work, the rhetorical structure elements, which we refer to as
facets, indicate backgrounds, methods, or results of a scientific paper. We
introduce and describe an expert annotated test collection to evaluate models
trained to perform this task. Our test collection consists of a diverse set of
50 query documents, drawn from computational linguistics and machine learning
venues. We carefully followed the annotation guideline used by TREC for depth-k
pooling (k = 100 or 250) and the resulting data collection consists of graded
relevance scores with high annotation agreement. The data is freely available
for research purposes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1"&gt;Sheshera Mysore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1"&gt;Tim O&amp;#x27;Gorman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1"&gt;Andrew McCallum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1"&gt;Hamed Zamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On-Device Personalization of Automatic Speech Recognition Models for Disordered Speech. (arXiv:2106.10259v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.10259</id>
        <link href="http://arxiv.org/abs/2106.10259"/>
        <updated>2021-06-21T02:07:37.348Z</updated>
        <summary type="html"><![CDATA[While current state-of-the-art Automatic Speech Recognition (ASR) systems
achieve high accuracy on typical speech, they suffer from significant
performance degradation on disordered speech and other atypical speech
patterns. Personalization of ASR models, a commonly applied solution to this
problem, is usually performed in a server-based training environment posing
problems around data privacy, delayed model-update times, and communication
cost for copying data and models between mobile device and server
infrastructure. In this paper, we present an approach to on-device based ASR
personalization with very small amounts of speaker-specific data. We test our
approach on a diverse set of 100 speakers with disordered speech and find
median relative word error rate improvement of 71% with only 50 short
utterances required per speaker. When tested on a voice-controlled home
automation platform, on-device personalized models show a median task success
rate of 81%, compared to only 40% of the unadapted models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1"&gt;Katrin Tomanek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;oise Beaufays&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cattiau_J/0/1/0/all/0/1"&gt;Julie Cattiau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chandorkar_A/0/1/0/all/0/1"&gt;Angad Chandorkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1"&gt;Khe Chai Sim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative Self-training for Punctuation Prediction. (arXiv:2104.10339v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10339</id>
        <link href="http://arxiv.org/abs/2104.10339"/>
        <updated>2021-06-21T02:07:37.341Z</updated>
        <summary type="html"><![CDATA[Punctuation prediction for automatic speech recognition (ASR) output
transcripts plays a crucial role for improving the readability of the ASR
transcripts and for improving the performance of downstream natural language
processing applications. However, achieving good performance on punctuation
prediction often requires large amounts of labeled speech transcripts, which is
expensive and laborious. In this paper, we propose a Discriminative
Self-Training approach with weighted loss and discriminative label smoothing to
exploit unlabeled speech transcripts. Experimental results on the English
IWSLT2011 benchmark test set and an internal Chinese spoken language dataset
demonstrate that the proposed approach achieves significant improvement on
punctuation prediction accuracy over strong baselines including BERT, RoBERTa,
and ELECTRA models. The proposed Discriminative Self-Training approach
outperforms the vanilla self-training approach. We establish a new
state-of-the-art (SOTA) on the IWSLT2011 test set, outperforming the current
SOTA model by 1.3% absolute gain on F$_1$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mengzhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinglin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning. (arXiv:2104.10357v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10357</id>
        <link href="http://arxiv.org/abs/2104.10357"/>
        <updated>2021-06-21T02:07:37.334Z</updated>
        <summary type="html"><![CDATA[In the traditional cascading architecture for spoken language understanding
(SLU), it has been observed that automatic speech recognition errors could be
detrimental to the performance of natural language understanding. End-to-end
(E2E) SLU models have been proposed to directly map speech input to desired
semantic frame with a single model, hence mitigating ASR error propagation.
Recently, pre-training technologies have been explored for these E2E models. In
this paper, we propose a novel joint textual-phonetic pre-training approach for
learning spoken language representations, aiming at exploring the full
potentials of phonetic information to improve SLU robustness to ASR errors. We
explore phoneme labels as high-level speech features, and design and compare
pre-training tasks based on conditional masked language model objectives and
inter-sentence relation objectives. We also investigate the efficacy of
combining textual and phonetic information during fine-tuning. Experimental
results on spoken language understanding benchmarks, Fluent Speech Commands and
SNIPS, show that the proposed approach significantly outperforms strong
baseline models and improves robustness of spoken language understanding to ASR
errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinglin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Back Attention Knowledge Transfer for Low-Resource Named Entity Recognition. (arXiv:1906.01183v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.01183</id>
        <link href="http://arxiv.org/abs/1906.01183"/>
        <updated>2021-06-21T02:07:37.325Z</updated>
        <summary type="html"><![CDATA[In recent years, great success has been achieved in the field of natural
language processing (NLP), thanks in part to the considerable amount of
annotated resources. For named entity recognition (NER), most languages do not
have such an abundance of labeled data as English, so the performances of those
languages are relatively lower. To improve the performance, we propose a
general approach called Back Attention Network (BAN). BAN uses a translation
system to translate other language sentences into English and then applies a
new mechanism named back attention knowledge transfer to obtain task-specific
information from pre-trained high-resource languages NER model. This strategy
can transfer high-layer features of well-trained model and enrich the semantic
representations of the original language. Experiments on three different
language datasets indicate that the proposed approach outperforms other
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Shengfei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Linghao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1"&gt;Huixiong Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huanhuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DEUS: A Data-driven Approach to Estimate User Satisfaction in Multi-turn Dialogues. (arXiv:2103.01287v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01287</id>
        <link href="http://arxiv.org/abs/2103.01287"/>
        <updated>2021-06-21T02:07:37.307Z</updated>
        <summary type="html"><![CDATA[Digital assistants are experiencing rapid growth due to their ability to
assist users with day-to-day tasks where most dialogues are happening
multi-turn. However, evaluating multi-turn dialogues remains challenging,
especially at scale. We suggest a context-sensitive method to estimate the
turn-level satisfaction for dialogue considering various types of user
preferences. The costs of interactions between users and dialogue systems are
formulated using a budget consumption concept. We assume users have an initial
interaction budget for a dialogue formed based on the task complexity and that
each turn has a cost. When the task is completed, or the budget has been
exhausted, users quit the dialogue. We demonstrate our method's effectiveness
by extensive experimentation with a simulated dialogue platform and real
multi-turn dialogues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Dookun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1"&gt;Julia Kiseleva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young-Bum Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sungjin Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition. (arXiv:2106.10169v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10169</id>
        <link href="http://arxiv.org/abs/2106.10169"/>
        <updated>2021-06-21T02:07:37.247Z</updated>
        <summary type="html"><![CDATA[By implicitly recognizing a user based on his/her speech input, speaker
identification enables many downstream applications, such as personalized
system behavior and expedited shopping checkouts. Based on whether the speech
content is constrained or not, both text-dependent (TD) and text-independent
(TI) speaker recognition models may be used. We wish to combine the advantages
of both types of models through an ensemble system to make more reliable
predictions. However, any such combined approach has to be robust to incomplete
inputs, i.e., when either TD or TI input is missing. As a solution we propose a
fusion of embeddings network foenet architecture, combining joint learning with
neural attention. We compare foenet with four competitive baseline methods on a
dataset of voice assistant inputs, and show that it achieves higher accuracy
than the baseline and score fusion methods, especially in the presence of
incomplete inputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruirui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1"&gt;Chelsea J.-T. Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zeya Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1"&gt;Hongda Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elibol_O/0/1/0/all/0/1"&gt;Oguz Elibol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1"&gt;Andreas Stolcke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10199</id>
        <link href="http://arxiv.org/abs/2106.10199"/>
        <updated>2021-06-21T02:07:37.239Z</updated>
        <summary type="html"><![CDATA[We show that with small-to-medium training data, fine-tuning only the bias
terms (or a subset of the bias terms) of pre-trained BERT models is competitive
with (and sometimes better than) fine-tuning the entire model. For larger data,
bias-only fine-tuning is competitive with other sparse fine-tuning methods.
Besides their practical utility, these findings are relevant for the question
of understanding the commonly-used process of finetuning: they support the
hypothesis that finetuning is mainly about exposing knowledge induced by
language-modeling training, rather than learning new task-specific linguistic
knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zaken_E/0/1/0/all/0/1"&gt;Elad Ben Zaken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1"&gt;Shauli Ravfogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Stacking of Layers in Neural Networks: An Application to Neural Machine Translation. (arXiv:2106.10002v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10002</id>
        <link href="http://arxiv.org/abs/2106.10002"/>
        <updated>2021-06-21T02:07:37.141Z</updated>
        <summary type="html"><![CDATA[In deep neural network modeling, the most common practice is to stack a
number of recurrent, convolutional, or feed-forward layers in order to obtain
high-quality continuous space representations which in turn improves the
quality of the network's prediction. Conventionally, each layer in the stack
has its own parameters which leads to a significant increase in the number of
model parameters. In this paper, we propose to share parameters across all
layers thereby leading to a recurrently stacked neural network model. We report
on an extensive case study on neural machine translation (NMT), where we apply
our proposed method to an encoder-decoder based neural network model, i.e., the
Transformer model, and experiment with three Japanese--English translation
datasets. We empirically demonstrate that the translation quality of a model
that recurrently stacks a single layer 6 times, despite having significantly
fewer parameters, approaches that of a model that stacks 6 layers where each
layer has different parameters. We also explore the limits of recurrent
stacking where we train extremely deep NMT models. This paper also examines the
utility of our recurrently stacked model as a student model through transfer
learning via leveraging pre-trained parameters and knowledge distillation, and
shows that it compensates for the performance drops in translation quality that
the direct training of recurrently stacked model brings. We also show how
transfer learning helps in faster decoding on top of the already reduced number
of parameters due to recurrent stacking. Finally, we analyze the effects of
recurrently stacked layers by visualizing the attentions of models that use
recurrently stacked layers and models that do not.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1"&gt;Raj Dabre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fujita_A/0/1/0/all/0/1"&gt;Atsushi Fujita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subjective Bias in Abstractive Summarization. (arXiv:2106.10084v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10084</id>
        <link href="http://arxiv.org/abs/2106.10084"/>
        <updated>2021-06-21T02:07:37.133Z</updated>
        <summary type="html"><![CDATA[Due to the subjectivity of the summarization, it is a good practice to have
more than one gold summary for each training document. However, many modern
large-scale abstractive summarization datasets have only one-to-one samples
written by different human with different styles. The impact of this phenomenon
is understudied. We formulate the differences among possible multiple
expressions summarizing the same content as subjective bias and examine the
role of this bias in the context of abstractive summarization. In this paper a
lightweight and effective method to extract the feature embeddings of
subjective styles is proposed. Results of summarization models trained on
style-clustered datasets show that there are certain types of styles that lead
to better convergence, abstraction and generalization. The reproducible code
and generated summaries are available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Litvak_M/0/1/0/all/0/1"&gt;Marina Litvak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vanetik_N/0/1/0/all/0/1"&gt;Natalia Vanetik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1"&gt;Jiacheng Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yinan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1"&gt;Siya Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting gender of Brazilian names using deep learning. (arXiv:2106.10156v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10156</id>
        <link href="http://arxiv.org/abs/2106.10156"/>
        <updated>2021-06-21T02:07:37.091Z</updated>
        <summary type="html"><![CDATA[Predicting gender by the name is not a simple task. In many applications,
especially in the natural language processing (NLP) field, this task may be
necessary, mainly when considering foreign names. Some machine learning
algorithms can satisfactorily perform the prediction. In this paper, we
examined and implemented feedforward and recurrent deep neural network models,
such as MLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first
name. A dataset of Brazilian names is used to train and evaluate the models. We
analyzed the accuracy, recall, precision, and confusion matrix to measure the
models' performances. The results indicate that the gender prediction can be
performed from the feature extraction strategy looking at the names as a set of
strings. Some models accurately predict the gender in more than 90% of the
cases. The recurrent models overcome the feedforward models in this binary
classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rego_R/0/1/0/all/0/1"&gt;Rosana C. B. Rego&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1"&gt;Ver&amp;#xf4;nica M. L. Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges and Limitations with the Metrics Measuring the Complexity of Code-Mixed Text. (arXiv:2106.10123v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10123</id>
        <link href="http://arxiv.org/abs/2106.10123"/>
        <updated>2021-06-21T02:07:37.059Z</updated>
        <summary type="html"><![CDATA[Code-mixing is a frequent communication style among multilingual speakers
where they mix words and phrases from two different languages in the same
utterance of text or speech. Identifying and filtering code-mixed text is a
challenging task due to its co-existence with monolingual and noisy text. Over
the years, several code-mixing metrics have been extensively used to identify
and validate code-mixed text quality. This paper demonstrates several inherent
limitations of code-mixing metrics with examples from the already existing
datasets that are popularly used across various experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1"&gt;Vivek Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synchronising speech segments with musical beats in Mandarin and English singing. (arXiv:2106.10045v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.10045</id>
        <link href="http://arxiv.org/abs/2106.10045"/>
        <updated>2021-06-21T02:07:37.051Z</updated>
        <summary type="html"><![CDATA[Generating synthesised singing voice with models trained on speech data has
many advantages due to the models' flexibility and controllability. However,
since the information about the temporal relationship between segments and
beats are lacking in speech training data, the synthesised singing may sound
off-beat at times. Therefore, the availability of the information on the
temporal relationship between speech segments and music beats is crucial. The
current study investigated the segment-beat synchronisation in singing data,
with hypotheses formed based on the linguistics theories of P-centre and
sonority hierarchy. A Mandarin corpus and an English corpus of professional
singing data were manually annotated and analysed. The results showed that the
presence of musical beats was more dependent on segment duration than sonority.
However, the sonority hierarchy and the P-centre theory were highly related to
the location of beats. Mandarin and English demonstrated cross-linguistic
variations despite exhibiting common patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jian Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPBERT: Pre-training BERT on SPARQL Queries for End-to-end Question Answering over Knowledge Graphs. (arXiv:2106.09997v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09997</id>
        <link href="http://arxiv.org/abs/2106.09997"/>
        <updated>2021-06-21T02:07:37.037Z</updated>
        <summary type="html"><![CDATA[We aim to create an unprecedented attempt to build an end-to-end Question
Answering (QA) over Knowledge Graphs (KGs), which can construct SPARQL queries
from natural language questions and generate a verbalized answer to its
queries. Hence, we introduce SPBERT, a Transformer-based language model
pre-trained on massive SPARQL query logs. By incorporating masked language
modelling objective and word structural objective, SPBERT can learn
general-purpose representations in both natural language and SPARQL query
language and make the most of the sequential order of words that are crucial
for structured language like SPARQL. In this paper, we investigate how SPBERT
and encoder-decoder architecture can be adapted for Knowledge-based QA corpora.
We conduct exhaustive experiments on two auxiliary tasks, including SPARQL
Query Construction and Answer Verbalization Generation. Results show that
SPBERT obtains promising performance and achieves state-of-the-art results on
several of these tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1"&gt;Hieu Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1"&gt;Long Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Truong-Son Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Financial Sentiment Analysis in a South African Landscape. (arXiv:2106.10004v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10004</id>
        <link href="http://arxiv.org/abs/2106.10004"/>
        <updated>2021-06-21T02:07:37.028Z</updated>
        <summary type="html"><![CDATA[Sentiment analysis as a sub-field of natural language processing has received
increased attention in the past decade enabling organisations to more
effectively manage their reputation through online media monitoring. Many
drivers impact reputation, however, this thesis focuses only the aspect of
financial performance and explores the gap with regards to financial sentiment
analysis in a South African context. Results showed that pre-trained sentiment
analysers are least effective for this task and that traditional lexicon-based
and machine learning approaches are best suited to predict financial sentiment
of news articles. The evaluated methods produced accuracies of 84\%-94\%. The
predicted sentiments correlated quite well with share price and highlighted the
potential use of sentiment as an indicator of financial performance. A main
contribution of the study was updating an existing sentiment dictionary for
financial sentiment analysis. Model generalisation was less acceptable due to
the limited amount of training data used. Future work includes expanding the
data set to improve general usability and contribute to an open-source
financial sentiment analyser for South African data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Terblanche_M/0/1/0/all/0/1"&gt;Michelle Terblanche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1"&gt;Vukosi Marivate&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing user creativity: Semantic measures for idea generation. (arXiv:2106.10131v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10131</id>
        <link href="http://arxiv.org/abs/2106.10131"/>
        <updated>2021-06-21T02:07:37.005Z</updated>
        <summary type="html"><![CDATA[Human creativity generates novel ideas to solve real-world problems. This
thereby grants us the power to transform the surrounding world and extend our
human attributes beyond what is currently possible. Creative ideas are not just
new and unexpected, but are also successful in providing solutions that are
useful, efficient and valuable. Thus, creativity optimizes the use of available
resources and increases wealth. The origin of human creativity, however, is
poorly understood, and semantic measures that could predict the success of
generated ideas are currently unknown. Here, we analyze a dataset of design
problem-solving conversations in real-world settings by using 49 semantic
measures based on WordNet 3.1 and demonstrate that a divergence of semantic
similarity, an increased information content, and a decreased polysemy predict
the success of generated ideas. The first feedback from clients also enhances
information content and leads to a divergence of successful ideas in creative
problem solving. These results advance cognitive science by identifying
real-world processes in human problem solving that are relevant to the success
of produced solutions and provide tools for real-time monitoring of problem
solving, student training and skill acquisition. A selected subset of
information content (IC S\'anchez-Batet) and semantic similarity
(Lin/S\'anchez-Batet) measures, which are both statistically powerful and
computationally fast, could support the development of technologies for
computer-assisted enhancements of human creativity or for the implementation of
creativity in machines endowed with general artificial intelligence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Georgiev_G/0/1/0/all/0/1"&gt;Georgi V. Georgiev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1"&gt;Danko D. Georgiev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label Mask for Multi-Label Text Classification. (arXiv:2106.10076v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10076</id>
        <link href="http://arxiv.org/abs/2106.10076"/>
        <updated>2021-06-21T02:07:36.996Z</updated>
        <summary type="html"><![CDATA[One of the key problems in multi-label text classification is how to take
advantage of the correlation among labels. However, it is very challenging to
directly model the correlations among labels in a complex and unknown label
space. In this paper, we propose a Label Mask multi-label text classification
model (LM-MTC), which is inspired by the idea of cloze questions of language
model. LM-MTC is able to capture implicit relationships among labels through
the powerful ability of pre-train language models. On the basis, we assign a
different token to each potential label, and randomly mask the token with a
certain probability to build a label based Masked Language Model (MLM). We
train the MTC and MLM together, further improving the generalization ability of
the model. A large number of experiments on multiple datasets demonstrate the
effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Rui Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xingbing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zelong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1"&gt;Haining An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoguang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hao Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Pre-Training for Multi-Hop Retriever. (arXiv:2106.09983v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09983</id>
        <link href="http://arxiv.org/abs/2106.09983"/>
        <updated>2021-06-21T02:07:36.944Z</updated>
        <summary type="html"><![CDATA[In multi-hop QA, answering complex questions entails iterative document
retrieval for finding the missing entity of the question. The main steps of
this process are sub-question detection, document retrieval for the
sub-question, and generation of a new query for the final document retrieval.
However, building a dataset that contains complex questions with sub-questions
and their corresponding documents requires costly human annotation. To address
the issue, we propose a new method for weakly supervised multi-hop retriever
pre-training without human efforts. Our method includes 1) a pre-training task
for generating vector representations of complex questions, 2) a scalable data
generation method that produces the nested structure of question and
sub-question as weak supervision for pre-training, and 3) a pre-training model
structure based on dense encoders. We conduct experiments to compare the
performance of our pre-trained retriever with several state-of-the-art models
on end-to-end multi-hop QA as well as document retrieval. The experimental
results show that our pre-trained retriever is effective and also robust on
limited data and computational resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seonwoo_Y/0/1/0/all/0/1"&gt;Yeon Seonwoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sang-Woo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Ji-Hoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1"&gt;Jung-Woo Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1"&gt;Alice Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Neural Edge-Editing Approach for Document-Level Relation Graph Extraction. (arXiv:2106.09900v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09900</id>
        <link href="http://arxiv.org/abs/2106.09900"/>
        <updated>2021-06-21T02:07:36.926Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel edge-editing approach to extract relation
information from a document. We treat the relations in a document as a relation
graph among entities in this approach. The relation graph is iteratively
constructed by editing edges of an initial graph, which might be a graph
extracted by another system or an empty graph. The way to edit edges is to
classify them in a close-first manner using the document and
temporally-constructed graph information; each edge is represented with a
document context information by a pretrained transformer model and a graph
context information by a graph convolutional neural network model. We evaluate
our approach on the task to extract material synthesis procedures from
materials science texts. The experimental results show the effectiveness of our
approach in editing the graphs initialized by our in-house rule-based system
and empty graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makino_K/0/1/0/all/0/1"&gt;Kohei Makino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miwa_M/0/1/0/all/0/1"&gt;Makoto Miwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_Y/0/1/0/all/0/1"&gt;Yutaka Sasaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating the Role of Negatives in Contrastive Representation Learning. (arXiv:2106.09943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09943</id>
        <link href="http://arxiv.org/abs/2106.09943"/>
        <updated>2021-06-21T02:07:36.915Z</updated>
        <summary type="html"><![CDATA[Noise contrastive learning is a popular technique for unsupervised
representation learning. In this approach, a representation is obtained via
reduction to supervised learning, where given a notion of semantic similarity,
the learner tries to distinguish a similar (positive) example from a collection
of random (negative) examples. The success of modern contrastive learning
pipelines relies on many parameters such as the choice of data augmentation,
the number of negative examples, and the batch size; however, there is limited
understanding as to how these parameters interact and affect downstream
performance. We focus on disambiguating the role of one of these parameters:
the number of negative examples. Theoretically, we show the existence of a
collision-coverage trade-off suggesting that the optimal number of negative
examples should scale with the number of underlying concepts in the data.
Empirically, we scrutinize the role of the number of negatives in both NLP and
vision tasks. In the NLP task, we find that the results broadly agree with our
theory, while our vision experiments are murkier with performance sometimes
even being insensitive to the number of negatives. We discuss plausible
explanations for this behavior and suggest future directions to better align
theory and practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1"&gt;Jordan T. Ash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1"&gt;Surbhi Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1"&gt;Akshay Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking. (arXiv:2106.09795v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09795</id>
        <link href="http://arxiv.org/abs/2106.09795"/>
        <updated>2021-06-21T02:07:36.895Z</updated>
        <summary type="html"><![CDATA[Entity linking (EL), the task of disambiguating mentions in text by linking
them to entities in a knowledge graph, is crucial for text understanding,
question answering or conversational systems. Entity linking on short text
(e.g., single sentence or question) poses particular challenges due to limited
context. While prior approaches use either heuristics or black-box neural
methods, here we propose LNN-EL, a neuro-symbolic approach that combines the
advantages of using interpretable rules based on first-order logic with the
performance of neural learning. Even though constrained to using rules, LNN-EL
performs competitively against SotA black-box neural approaches, with the added
benefits of extensibility and transferability. In particular, we show that we
can easily blend existing rule templates given by a human expert, with multiple
types of features (priors, BERT encodings, box embeddings, etc), and even
scores resulting from previous EL methods, thus improving on such methods. For
instance, on the LC-QuAD-1.0 dataset, we show more than $4$\% increase in F1
score over previous SotA. Finally, we show that the inductive bias offered by
using logic results in learned rules that transfer well across datasets, even
without fine tuning, while maintaining high accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurajada_S/0/1/0/all/0/1"&gt;Sairam Gurajada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1"&gt;Qiuhao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neelam_S/0/1/0/all/0/1"&gt;Sumit Neelam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popa_L/0/1/0/all/0/1"&gt;Lucian Popa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_P/0/1/0/all/0/1"&gt;Prithviraj Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunyao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gray_A/0/1/0/all/0/1"&gt;Alexander Gray&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction. (arXiv:2106.09895v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09895</id>
        <link href="http://arxiv.org/abs/2106.09895"/>
        <updated>2021-06-21T02:07:36.885Z</updated>
        <summary type="html"><![CDATA[Joint extraction of entities and relations from unstructured texts is a
crucial task in information extraction. Recent methods achieve considerable
performance but still suffer from some inherent limitations, such as redundancy
of relation prediction, poor generalization of span-based extraction and
inefficiency. In this paper, we decompose this task into three subtasks,
Relation Judgement, Entity Extraction and Subject-object Alignment from a novel
perspective and then propose a joint relational triple extraction framework
based on Potential Relation and Global Correspondence (PRGC). Specifically, we
design a component to predict potential relations, which constrains the
following entity extraction to the predicted relation subset rather than all
relations; then a relation-specific sequence tagging component is applied to
handle the overlapping problem between subjects and objects; finally, a global
correspondence component is designed to align the subject and object into a
triple with low-complexity. Extensive experiments show that PRGC achieves
state-of-the-art performance on public benchmarks with higher efficiency and
delivers consistent performance gain on complex scenarios of overlapping
triples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hengyi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_R/0/1/0/all/0/1"&gt;Rui Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yifan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1"&gt;Bin Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Ming Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuity of Topic, Interaction, and Query: Learning to Quote in Online Conversations. (arXiv:2106.09896v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09896</id>
        <link href="http://arxiv.org/abs/2106.09896"/>
        <updated>2021-06-21T02:07:36.876Z</updated>
        <summary type="html"><![CDATA[Quotations are crucial for successful explanations and persuasions in
interpersonal communications. However, finding what to quote in a conversation
is challenging for both humans and machines. This work studies automatic
quotation generation in an online conversation and explores how language
consistency affects whether a quotation fits the given context. Here, we
capture the contextual consistency of a quotation in terms of latent topics,
interactions with the dialogue history, and coherence to the query turn's
existing content. Further, an encoder-decoder neural framework is employed to
continue the context with a quotation via language generation. Experiment
results on two large-scale datasets in English and Chinese demonstrate that our
quotation generation model outperforms the state-of-the-art models. Further
analysis shows that topic, interaction, and query consistency are all helpful
to learn how to quote in online conversations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lingzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xingshan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haisong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1"&gt;Kam-Fai Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Joint Pandemic Concern and Relation Extraction on Twitter. (arXiv:2106.09929v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09929</id>
        <link href="http://arxiv.org/abs/2106.09929"/>
        <updated>2021-06-21T02:07:36.859Z</updated>
        <summary type="html"><![CDATA[Public concern detection provides potential guidance to the authorities for
crisis management before or during a pandemic outbreak. Detecting people's
concerns and attention from online social media platforms has been widely
acknowledged as an effective approach to relieve public panic and prevent a
social crisis. However, detecting concerns in time from massive information in
social media turns out to be a big challenge, especially when sufficient
manually labeled data is in the absence of public health emergencies, e.g.,
COVID-19. In this paper, we propose a novel end-to-end deep learning model to
identify people's concerns and the corresponding relations based on Graph
Convolutional Network and Bi-directional Long Short Term Memory integrated with
Concern Graph. Except for the sequential features from BERT embeddings, the
regional features of tweets can be extracted by the Concern Graph module, which
not only benefits the concern detection but also enables our model to be high
noise-tolerant. Thus, our model can address the issue of insufficient manually
labeled data. We conduct extensive experiments to evaluate the proposed model
by using both manually labeled tweets and automatically labeled tweets. The
experimental results show that our model can outperform the state-of-art models
on real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jingli Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weihua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yongchareon_S/0/1/0/all/0/1"&gt;Sira Yongchareon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1"&gt;Quan Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bad Characters: Imperceptible NLP Attacks. (arXiv:2106.09898v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09898</id>
        <link href="http://arxiv.org/abs/2106.09898"/>
        <updated>2021-06-21T02:07:36.850Z</updated>
        <summary type="html"><![CDATA[Several years of research have shown that machine-learning systems are
vulnerable to adversarial examples, both in theory and in practice. Until now,
such attacks have primarily targeted visual models, exploiting the gap between
human and machine perception. Although text-based models have also been
attacked with adversarial examples, such attacks struggled to preserve semantic
meaning and indistinguishability. In this paper, we explore a large class of
adversarial examples that can be used to attack text-based models in a
black-box setting without making any human-perceptible visual modification to
inputs. We use encoding-specific perturbations that are imperceptible to the
human eye to manipulate the outputs of a wide range of Natural Language
Processing (NLP) systems from neural machine-translation pipelines to web
search engines. We find that with a single imperceptible encoding injection --
representing one invisible character, homoglyph, reordering, or deletion -- an
attacker can significantly reduce the performance of vulnerable models, and
with three injections most models can be functionally broken. Our attacks work
against currently-deployed commercial systems, including those produced by
Microsoft and Google, in addition to open source models published by Facebook
and IBM. This novel series of attacks presents a significant threat to many
language processing systems: an attacker can affect systems in a targeted
manner without any assumptions about the underlying model. We conclude that
text-based NLP systems require careful input sanitization, just like
conventional applications, and that given such systems are now being deployed
rapidly at scale, the urgent attention of architects and operators is required.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boucher_N/0/1/0/all/0/1"&gt;Nicholas Boucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1"&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1"&gt;Ross Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GEM: A General Evaluation Benchmark for Multimodal Tasks. (arXiv:2106.09889v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09889</id>
        <link href="http://arxiv.org/abs/2106.09889"/>
        <updated>2021-06-21T02:07:36.841Z</updated>
        <summary type="html"><![CDATA[In this paper, we present GEM as a General Evaluation benchmark for
Multimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,
XGLUE and XTREME that mainly focus on natural language tasks, GEM is a
large-scale vision-language benchmark, which consists of GEM-I for
image-language tasks and GEM-V for video-language tasks. Comparing with
existing multimodal datasets such as MSCOCO and Flicker30K for image-language
tasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the
largest vision-language dataset covering image-language tasks and
video-language tasks at the same time, but also labeled in multiple languages.
We also provide two baseline models for this benchmark. We will release the
dataset, code and baseline models, aiming to advance the development of
multilingual multimodal research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Lin Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1"&gt;Edward Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1"&gt;Lei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Huaishao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1"&gt;Ming Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharti_T/0/1/0/all/0/1"&gt;Taroon Bharti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sacheti_A/0/1/0/all/0/1"&gt;Arun Sacheti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Task Learning and Adapted Knowledge Models for Emotion-Cause Extraction. (arXiv:2106.09790v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09790</id>
        <link href="http://arxiv.org/abs/2106.09790"/>
        <updated>2021-06-21T02:07:36.699Z</updated>
        <summary type="html"><![CDATA[Detecting what emotions are expressed in text is a well-studied problem in
natural language processing. However, research on finer grained emotion
analysis such as what causes an emotion is still in its infancy. We present
solutions that tackle both emotion recognition and emotion cause detection in a
joint fashion. Considering that common-sense knowledge plays an important role
in understanding implicitly expressed emotions and the reasons for those
emotions, we propose novel methods that combine common-sense knowledge via
adapted knowledge models with multi-task learning to perform joint emotion
classification and emotion cause tagging. We show performance improvement on
both tasks when including common-sense reasoning and a multitask framework. We
provide a thorough analysis to gain insights into model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Turcan_E/0/1/0/all/0/1"&gt;Elsbeth Turcan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anubhai_R/0/1/0/all/0/1"&gt;Rishita Anubhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_K/0/1/0/all/0/1"&gt;Kasturi Bhattacharjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Onaizan_Y/0/1/0/all/0/1"&gt;Yaser Al-Onaizan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1"&gt;Smaranda Muresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSFCube -- A Test Collection of Computer Science Research Articles for Faceted Query by Example. (arXiv:2103.12906v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12906</id>
        <link href="http://arxiv.org/abs/2103.12906"/>
        <updated>2021-06-21T02:07:36.688Z</updated>
        <summary type="html"><![CDATA[Query by Example is a well-known information retrieval task in which a
document is chosen by the user as the search query and the goal is to retrieve
relevant documents from a large collection. However, a document often covers
multiple aspects of a topic. To address this scenario we introduce the task of
faceted Query by Example in which users can also specify a finer grained aspect
in addition to the input query document. We focus on the application of this
task in scientific literature search. We envision models which are able to
retrieve scientific papers analogous to a query scientific paper along
specifically chosen rhetorical structure elements as one solution to this
problem. In this work, the rhetorical structure elements, which we refer to as
facets, indicate backgrounds, methods, or results of a scientific paper. We
introduce and describe an expert annotated test collection to evaluate models
trained to perform this task. Our test collection consists of a diverse set of
50 query documents, drawn from computational linguistics and machine learning
venues. We carefully followed the annotation guideline used by TREC for depth-k
pooling (k = 100 or 250) and the resulting data collection consists of graded
relevance scores with high annotation agreement. The data is freely available
for research purposes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1"&gt;Sheshera Mysore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1"&gt;Tim O&amp;#x27;Gorman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1"&gt;Andrew McCallum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1"&gt;Hamed Zamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Graph Learning for Recommendation. (arXiv:2010.10783v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10783</id>
        <link href="http://arxiv.org/abs/2010.10783"/>
        <updated>2021-06-21T02:07:36.677Z</updated>
        <summary type="html"><![CDATA[Representation learning on user-item graph for recommendation has evolved
from using single ID or interaction history to exploiting higher-order
neighbors. This leads to the success of graph convolution networks (GCNs) for
recommendation such as PinSage and LightGCN. Despite effectiveness, we argue
that they suffer from two limitations: (1) high-degree nodes exert larger
impact on the representation learning, deteriorating the recommendations of
low-degree (long-tail) items; and (2) representations are vulnerable to noisy
interactions, as the neighborhood aggregation scheme further enlarges the
impact of observed edges.

In this work, we explore self-supervised learning on user-item graph, so as
to improve the accuracy and robustness of GCNs for recommendation. The idea is
to supplement the classical supervised task of recommendation with an auxiliary
self-supervised task, which reinforces node representation learning via
self-discrimination. Specifically, we generate multiple views of a node,
maximizing the agreement between different views of the same node compared to
that of other nodes. We devise three operators to generate the views -- node
dropout, edge dropout, and random walk -- that change the graph structure in
different manners. We term this new learning paradigm as
\textit{Self-supervised Graph Learning} (SGL), implementing it on the
state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL
has the ability of automatically mining hard negatives. Empirical studies on
three benchmark datasets demonstrate the effectiveness of SGL, which improves
the recommendation accuracy, especially on long-tail items, and the robustness
against interaction noises. Our implementations are available at
\url{https://github.com/wujcan/SGL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiancan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1"&gt;Jianxun Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval. (arXiv:2104.09791v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09791</id>
        <link href="http://arxiv.org/abs/2104.09791"/>
        <updated>2021-06-21T02:07:36.665Z</updated>
        <summary type="html"><![CDATA[Pre-training and fine-tuning have achieved remarkable success in many
downstream natural language processing (NLP) tasks. Recently, pre-training
methods tailored for information retrieval (IR) have also been explored, and
the latest success is the PROP method which has reached new SOTA on a variety
of ad-hoc retrieval benchmarks. The basic idea of PROP is to construct the
\textit{representative words prediction} (ROP) task for pre-training inspired
by the query likelihood model. Despite its exciting performance, the
effectiveness of PROP might be bounded by the classical unigram language model
adopted in the ROP task construction process. To tackle this problem, we
propose a bootstrapped pre-training method (namely B-PROP) based on BERT for
ad-hoc retrieval. The key idea is to use the powerful contextual language model
BERT to replace the classical unigram language model for the ROP task
construction, and re-train BERT itself towards the tailored objective for IR.
Specifically, we introduce a novel contrastive method, inspired by the
divergence-from-randomness idea, to leverage BERT's self-attention mechanism to
sample representative words from the document. By further fine-tuning on
downstream ad-hoc retrieval tasks, our method achieves significant improvements
over baselines without pre-training or with other pre-training methods, and
further pushes forward the SOTA on a variety of ad-hoc retrieval tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xinyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiafeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yixing Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-mode Transformer Transducer with Stochastic Future Context. (arXiv:2106.09760v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.09760</id>
        <link href="http://arxiv.org/abs/2106.09760"/>
        <updated>2021-06-21T02:07:36.655Z</updated>
        <summary type="html"><![CDATA[Automatic speech recognition (ASR) models make fewer errors when more
surrounding speech information is presented as context. Unfortunately,
acquiring a larger future context leads to higher latency. There exists an
inevitable trade-off between speed and accuracy. Naively, to fit different
latency requirements, people have to store multiple models and pick the best
one under the constraints. Instead, a more desirable approach is to have a
single model that can dynamically adjust its latency based on different
constraints, which we refer to as Multi-mode ASR. A Multi-mode ASR model can
fulfill various latency requirements during inference -- when a larger latency
becomes acceptable, the model can process longer future context to achieve
higher accuracy and when a latency budget is not flexible, the model can be
less dependent on future context but still achieve reliable accuracy. In
pursuit of Multi-mode ASR, we propose Stochastic Future Context, a simple
training procedure that samples one streaming configuration in each iteration.
Through extensive experiments on AISHELL-1 and LibriSpeech datasets, we show
that a Multi-mode ASR model rivals, if not surpasses, a set of competitive
streaming baselines trained with different latency budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kwangyoun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_F/0/1/0/all/0/1"&gt;Felix Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sridhar_P/0/1/0/all/0/1"&gt;Prashant Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_K/0/1/0/all/0/1"&gt;Kyu J. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Information Retrieval Approach to Building Datasets for Hate Speech Detection. (arXiv:2106.09775v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09775</id>
        <link href="http://arxiv.org/abs/2106.09775"/>
        <updated>2021-06-21T02:07:36.643Z</updated>
        <summary type="html"><![CDATA[Building a benchmark dataset for hate speech detection presents several
challenges. Firstly, because hate speech is relatively rare -- e.g., less than
3\% of Twitter posts are hateful \citep{founta2018large} -- random sampling of
tweets to annotate is inefficient in capturing hate speech. A common practice
is to only annotate tweets containing known ``hate words'', but this risks
yielding a biased benchmark that only partially captures the real-world
phenomenon of interest. A second challenge is that definitions of hate speech
tend to be highly variable and subjective. Annotators having diverse prior
notions of hate speech may not only disagree with one another but also struggle
to conform to specified labeling guidelines. Our key insight is that the rarity
and subjectivity of hate speech are akin to that of relevance in information
retrieval (IR). This connection suggests that well-established methodologies
for creating IR test collections might also be usefully applied to create
better benchmark datasets for hate speech detection. Firstly, to intelligently
and efficiently select which tweets to annotate, we apply established IR
techniques of {\em pooling} and {\em active learning}. Secondly, to improve
both consistency and value of annotations, we apply {\em task decomposition}
\cite{Zhang-sigir14} and {\em annotator rationale} \cite{mcdonnell16-hcomp}
techniques. Using the above techniques, we create and share a new benchmark
dataset\footnote{We will release the dataset upon publication.} for hate speech
detection with broader coverage than prior datasets. We also show a dramatic
drop in accuracy of existing detection models when tested on these broader
forms of hate. Collected annotator rationales not only provide documented
support for labeling decisions but also create exciting future work
opportunities for dual-supervision and/or explanation generation in modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Md Mustafizur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_D/0/1/0/all/0/1"&gt;Dinesh Balakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murthy_D/0/1/0/all/0/1"&gt;Dhiraj Murthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutlu_M/0/1/0/all/0/1"&gt;Mucahid Kutlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1"&gt;Matthew Lease&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FinGAT: Financial Graph Attention Networks for Recommending Top-K Profitable Stocks. (arXiv:2106.10159v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10159</id>
        <link href="http://arxiv.org/abs/2106.10159"/>
        <updated>2021-06-21T02:07:36.627Z</updated>
        <summary type="html"><![CDATA[Financial technology (FinTech) has drawn much attention among investors and
companies. While conventional stock analysis in FinTech targets at predicting
stock prices, less effort is made for profitable stock recommendation. Besides,
in existing approaches on modeling time series of stock prices, the
relationships among stocks and sectors (i.e., categories of stocks) are either
neglected or pre-defined. Ignoring stock relationships will miss the
information shared between stocks while using pre-defined relationships cannot
depict the latent interactions or influence of stock prices between stocks. In
this work, we aim at recommending the top-K profitable stocks in terms of
return ratio using time series of stock prices and sector information. We
propose a novel deep learning-based model, Financial Graph Attention Networks
(FinGAT), to tackle the task under the setting that no pre-defined
relationships between stocks are given. The idea of FinGAT is three-fold.
First, we devise a hierarchical learning component to learn short-term and
long-term sequential patterns from stock time series. Second, a fully-connected
graph between stocks and a fully-connected graph between sectors are
constructed, along with graph attention networks, to learn the latent
interactions among stocks and sectors. Third, a multi-task objective is devised
to jointly recommend the profitable stocks and predict the stock movement.
Experiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit
remarkable recommendation performance of our FinGAT, comparing to
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1"&gt;Yi-Ling Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yu-Che Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Cheng-Te Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heuristic Stopping Rules For Technology-Assisted Review. (arXiv:2106.09871v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09871</id>
        <link href="http://arxiv.org/abs/2106.09871"/>
        <updated>2021-06-21T02:07:36.436Z</updated>
        <summary type="html"><![CDATA[Technology-assisted review (TAR) refers to human-in-the-loop active learning
workflows for finding relevant documents in large collections. These workflows
often must meet a target for the proportion of relevant documents found (i.e.
recall) while also holding down costs. A variety of heuristic stopping rules
have been suggested for striking this tradeoff in particular settings, but none
have been tested against a range of recall targets and tasks. We propose two
new heuristic stopping rules, Quant and QuantCI based on model-based estimation
techniques from survey research. We compare them against a range of proposed
heuristics and find they are accurate at hitting a range of recall targets
while substantially reducing review costs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eugene Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_D/0/1/0/all/0/1"&gt;David D. Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frieder_O/0/1/0/all/0/1"&gt;Ophir Frieder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GEM: A General Evaluation Benchmark for Multimodal Tasks. (arXiv:2106.09889v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09889</id>
        <link href="http://arxiv.org/abs/2106.09889"/>
        <updated>2021-06-21T02:07:36.399Z</updated>
        <summary type="html"><![CDATA[In this paper, we present GEM as a General Evaluation benchmark for
Multimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,
XGLUE and XTREME that mainly focus on natural language tasks, GEM is a
large-scale vision-language benchmark, which consists of GEM-I for
image-language tasks and GEM-V for video-language tasks. Comparing with
existing multimodal datasets such as MSCOCO and Flicker30K for image-language
tasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the
largest vision-language dataset covering image-language tasks and
video-language tasks at the same time, but also labeled in multiple languages.
We also provide two baseline models for this benchmark. We will release the
dataset, code and baseline models, aiming to advance the development of
multilingual multimodal research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Lin Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1"&gt;Edward Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1"&gt;Lei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Huaishao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1"&gt;Ming Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharti_T/0/1/0/all/0/1"&gt;Taroon Bharti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sacheti_A/0/1/0/all/0/1"&gt;Arun Sacheti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Minimizing Cost in Legal Document Review Workflows. (arXiv:2106.09866v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09866</id>
        <link href="http://arxiv.org/abs/2106.09866"/>
        <updated>2021-06-21T02:07:36.386Z</updated>
        <summary type="html"><![CDATA[Technology-assisted review (TAR) refers to human-in-the-loop machine learning
workflows for document review in legal discovery and other high recall review
tasks. Attorneys and legal technologists have debated whether review should be
a single iterative process (one-phase TAR workflows) or whether model training
and review should be separate (two-phase TAR workflows), with implications for
the choice of active learning algorithm. The relative cost of manual labeling
for different purposes (training vs. review) and of different documents
(positive vs. negative examples) is a key and neglected factor in this debate.
Using a novel cost dynamics analysis, we show analytically and empirically that
these relative costs strongly impact whether a one-phase or two-phase workflow
minimizes cost. We also show how category prevalence, classification task
difficulty, and collection size impact the optimal choice not only of workflow
type, but of active learning method and stopping point.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eugene Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_D/0/1/0/all/0/1"&gt;David D. Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frieder_O/0/1/0/all/0/1"&gt;Ophir Frieder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Audio-Driven System For Real-Time Music Visualisation. (arXiv:2106.10134v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.10134</id>
        <link href="http://arxiv.org/abs/2106.10134"/>
        <updated>2021-06-21T02:07:36.368Z</updated>
        <summary type="html"><![CDATA[Computer-generated visualisations can accompany recorded or live music to
create novel audiovisual experiences for audiences. We present a system to
streamline the creation of audio-driven visualisations based on audio feature
extraction and mapping interfaces. Its architecture is based on three modular
software components: backend (audio plugin), frontend (3D game-like
environment), and middleware (visual mapping interface). We conducted a user
evaluation comprising two stages. Results from the first stage (34
participants) indicate that music visualisations generated with the system were
significantly better at complementing the music than a baseline visualisation.
Nine participants took part in the second stage involving interactive tasks.
Overall, the system yielded a Creativity Support Index above average (68.1) and
a System Usability Scale index (58.6) suggesting that ease of use can be
improved. Thematic analysis revealed that participants enjoyed the system's
synchronicity and expressive capabilities, but found technical problems and
difficulties understanding the audio feature terminology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Graf_M/0/1/0/all/0/1"&gt;Max Graf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Opara_H/0/1/0/all/0/1"&gt;Harold Chijioke Opara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1"&gt;Mathieu Barthet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point-of-Interest Recommender Systems: A Survey from an Experimental Perspective. (arXiv:2106.10069v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.10069</id>
        <link href="http://arxiv.org/abs/2106.10069"/>
        <updated>2021-06-21T02:07:36.339Z</updated>
        <summary type="html"><![CDATA[Point-of-Interest recommendation is an increasing research and developing
area within the widely adopted technologies known as Recommender Systems. Among
them, those that exploit information coming from Location-Based Social Networks
(LBSNs) are very popular nowadays and could work with different information
sources, which pose several challenges and research questions to the community
as a whole. We present a systematic review focused on the research done in the
last 10 years about this topic. We discuss and categorize the algorithms and
evaluation methodologies used in these works and point out the opportunities
and challenges that remain open in the field. More specifically, we report the
leading recommendation techniques and information sources that have been
exploited more often (such as the geographical signal and deep learning
approaches) while we also alert about the lack of reproducibility in the field
that may hinder real performance improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1"&gt;Pablo S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1"&gt;Alejandro Bellog&amp;#xed;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Information Retrieval Approach to Building Datasets for Hate Speech Detection. (arXiv:2106.09775v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09775</id>
        <link href="http://arxiv.org/abs/2106.09775"/>
        <updated>2021-06-21T02:07:36.316Z</updated>
        <summary type="html"><![CDATA[Building a benchmark dataset for hate speech detection presents several
challenges. Firstly, because hate speech is relatively rare -- e.g., less than
3\% of Twitter posts are hateful \citep{founta2018large} -- random sampling of
tweets to annotate is inefficient in capturing hate speech. A common practice
is to only annotate tweets containing known ``hate words'', but this risks
yielding a biased benchmark that only partially captures the real-world
phenomenon of interest. A second challenge is that definitions of hate speech
tend to be highly variable and subjective. Annotators having diverse prior
notions of hate speech may not only disagree with one another but also struggle
to conform to specified labeling guidelines. Our key insight is that the rarity
and subjectivity of hate speech are akin to that of relevance in information
retrieval (IR). This connection suggests that well-established methodologies
for creating IR test collections might also be usefully applied to create
better benchmark datasets for hate speech detection. Firstly, to intelligently
and efficiently select which tweets to annotate, we apply established IR
techniques of {\em pooling} and {\em active learning}. Secondly, to improve
both consistency and value of annotations, we apply {\em task decomposition}
\cite{Zhang-sigir14} and {\em annotator rationale} \cite{mcdonnell16-hcomp}
techniques. Using the above techniques, we create and share a new benchmark
dataset\footnote{We will release the dataset upon publication.} for hate speech
detection with broader coverage than prior datasets. We also show a dramatic
drop in accuracy of existing detection models when tested on these broader
forms of hate. Collected annotator rationales not only provide documented
support for labeling decisions but also create exciting future work
opportunities for dual-supervision and/or explanation generation in modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Md Mustafizur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_D/0/1/0/all/0/1"&gt;Dinesh Balakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murthy_D/0/1/0/all/0/1"&gt;Dhiraj Murthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutlu_M/0/1/0/all/0/1"&gt;Mucahid Kutlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1"&gt;Matthew Lease&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PixInWav: Residual Steganography for Hiding Pixels in Audio. (arXiv:2106.09814v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.09814</id>
        <link href="http://arxiv.org/abs/2106.09814"/>
        <updated>2021-06-21T02:07:36.277Z</updated>
        <summary type="html"><![CDATA[Steganography comprises the mechanics of hiding data in a host media that may
be publicly available. While previous works focused on unimodal setups (e.g.,
hiding images in images, or hiding audio in audio), PixInWav targets the
multimodal case of hiding images in audio. To this end, we propose a novel
residual architecture operating on top of short-time discrete cosine transform
(STDCT) audio spectrograms. Among our results, we find that the residual audio
steganography setup we propose allows independent encoding of the hidden image
from the host audio without compromising quality. Accordingly, while previous
works require both host and hidden signals to hide a signal, PixInWav can
encode images offline -- which can be later hidden, in a residual fashion, into
any audio signal. Finally, we test our scheme in a lab setting to transmit
images over airwaves from a loudspeaker to a microphone verifying our
theoretical insights and obtaining promising results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geleta_M/0/1/0/all/0/1"&gt;Margarita Geleta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punti_C/0/1/0/all/0/1"&gt;Cristina Punti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1"&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pons_J/0/1/0/all/0/1"&gt;Jordi Pons&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canton_C/0/1/0/all/0/1"&gt;Cristian Canton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1"&gt;Xavier Giro-i-Nieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Unified Framework for Fair and Stable Graph Representation Learning. (arXiv:2102.13186v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13186</id>
        <link href="http://arxiv.org/abs/2102.13186"/>
        <updated>2021-06-18T02:06:46.697Z</updated>
        <summary type="html"><![CDATA[As the representations output by Graph Neural Networks (GNNs) are
increasingly employed in real-world applications, it becomes important to
ensure that these representations are fair and stable. In this work, we
establish a key connection between counterfactual fairness and stability and
leverage it to propose a novel framework, NIFTY (uNIfying Fairness and
stabiliTY), which can be used with any GNN to learn fair and stable
representations. We introduce a novel objective function that simultaneously
accounts for fairness and stability and develop a layer-wise weight
normalization using the Lipschitz constant to enhance neural message passing in
GNNs. In doing so, we enforce fairness and stability both in the objective
function as well as in the GNN architecture. Further, we show theoretically
that our layer-wise weight normalization promotes counterfactual fairness and
stability in the resulting representations. We introduce three new graph
datasets comprising of high-stakes decisions in criminal justice and financial
lending domains. Extensive experimentation with the above datasets demonstrates
the efficacy of our framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1"&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1"&gt;Marinka Zitnik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Backward Gradient Normalization in Deep Neural Networks. (arXiv:2106.09475v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09475</id>
        <link href="http://arxiv.org/abs/2106.09475"/>
        <updated>2021-06-18T02:06:46.689Z</updated>
        <summary type="html"><![CDATA[We introduce a new technique for gradient normalization during neural network
training. The gradients are rescaled during the backward pass using
normalization layers introduced at certain points within the network
architecture. These normalization nodes do not affect forward activity
propagation, but modify backpropagation equations to permit a well-scaled
gradient flow that reaches the deepest network layers without experimenting
vanishing or explosion. Results on tests with very deep neural networks show
that the new technique can do an effective control of the gradient norm,
allowing the update of weights in the deepest layers and improving network
accuracy on several experimental conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cabana_A/0/1/0/all/0/1"&gt;Alejandro Cabana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lago_Fernandez_L/0/1/0/all/0/1"&gt;Luis F. Lago-Fern&amp;#xe1;ndez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BF++: a language for general-purpose program synthesis. (arXiv:2101.09571v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09571</id>
        <link href="http://arxiv.org/abs/2101.09571"/>
        <updated>2021-06-18T02:06:46.682Z</updated>
        <summary type="html"><![CDATA[Most state of the art decision systems based on Reinforcement Learning (RL)
are data-driven black-box neural models, where it is often difficult to
incorporate expert knowledge into the models or let experts review and validate
the learned decision mechanisms. Knowledge-insertion and model review are
important requirements in many applications involving human health and safety.
One way to bridge the gap between data and knowledge driven systems is program
synthesis: replacing a neural network that outputs decisions with a symbolic
program generated by a neural network or by means of genetic programming. We
propose a new programming language, BF++, designed specifically for automatic
programming of agents in a Partially Observable Markov Decision Process (POMDP)
setting and apply neural program synthesis to solve standard OpenAI Gym
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liventsev_V/0/1/0/all/0/1"&gt;Vadim Liventsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harma_A/0/1/0/all/0/1"&gt;Aki H&amp;#xe4;rm&amp;#xe4;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petkovic_M/0/1/0/all/0/1"&gt;Milan Petkovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exponential Error Convergence in Data Classification with Optimized Random Features: Acceleration by Quantum Machine Learning. (arXiv:2106.09028v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.09028</id>
        <link href="http://arxiv.org/abs/2106.09028"/>
        <updated>2021-06-18T02:06:46.675Z</updated>
        <summary type="html"><![CDATA[Random features are a central technique for scalable learning algorithms
based on kernel methods. A recent work has shown that an algorithm for machine
learning by quantum computer, quantum machine learning (QML), can exponentially
speed up sampling of optimized random features, even without imposing
restrictive assumptions on sparsity and low-rankness of matrices that had
limited applicability of conventional QML algorithms; this QML algorithm makes
it possible to significantly reduce and provably minimize the required number
of features for regression tasks. However, a major interest in the field of QML
is how widely the advantages of quantum computation can be exploited, not only
in the regression tasks. We here construct a QML algorithm for a classification
task accelerated by the optimized random features. We prove that the QML
algorithm for sampling optimized random features, combined with stochastic
gradient descent (SGD), can achieve state-of-the-art exponential convergence
speed of reducing classification error in a classification task under a
low-noise condition; at the same time, our algorithm with optimized random
features can take advantage of the significant reduction of the required number
of features so as to accelerate each iteration in the SGD and evaluation of the
classifier obtained from our algorithm. These results discover a promising
application of QML to significant acceleration of the leading classification
algorithm based on kernel methods, without ruining its applicability to a
practical class of data sets and the exponential error-convergence speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Yamasaki_H/0/1/0/all/0/1"&gt;Hayata Yamasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sonoda_S/0/1/0/all/0/1"&gt;Sho Sonoda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autobots: Latent Variable Sequential Set Transformers. (arXiv:2104.00563v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00563</id>
        <link href="http://arxiv.org/abs/2104.00563"/>
        <updated>2021-06-18T02:06:46.265Z</updated>
        <summary type="html"><![CDATA[Robust multi-agent trajectory prediction is essential for the safe control of
robots and vehicles that interact with humans. Many existing methods treat
social and temporal information separately and therefore fall short of
modelling the joint future trajectories of all agents in a socially consistent
way. To address this, we propose a new class of Latent Variable Sequential Set
Transformers which autoregressively model multi-agent trajectories. We refer to
these architectures as "AutoBots". AutoBots model the contents of sets (e.g.
representing the properties of agents in a scene) over time and employ
multi-head self-attention blocks over these sequences of sets to encode the
sociotemporal relationships between the different actors of a scene. This
produces either the trajectory of one ego-agent or a distribution over the
future trajectories for all agents under consideration. Our approach works for
general sequences of sets and we provide illustrative experiments modelling the
sequential structure of the multiple strokes that make up symbols in the
Omniglot data. For the single-agent prediction case, we validate our model on
the NuScenes motion prediction task and achieve competitive results on the
global leaderboard. In the multi-agent forecasting setting, we validate our
model on TrajNet. We find that our method outperforms physical extrapolation
and recurrent network baselines and generates scene-consistent trajectories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girgis_R/0/1/0/all/0/1"&gt;Roger Girgis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1"&gt;Florian Golemo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Codevilla_F/0/1/0/all/0/1"&gt;Felipe Codevilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1"&gt;Jim Aldon D&amp;#x27;Souza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_M/0/1/0/all/0/1"&gt;Martin Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1"&gt;Samira Ebrahimi Kahou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1"&gt;Felix Heide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIFT Matching by Context Exposed. (arXiv:2106.09584v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09584</id>
        <link href="http://arxiv.org/abs/2106.09584"/>
        <updated>2021-06-18T02:06:46.258Z</updated>
        <summary type="html"><![CDATA[This paper investigates how to step up local image descriptor matching by
exploiting matching context information. Two main contexts are identified,
originated respectively from the descriptor space and from the keypoint space.
The former is generally used to design the actual matching strategy while the
latter to filter matches according to the local spatial consistency. On this
basis, a new matching strategy and a novel local spatial filter, named
respectively blob matching and Delaunay Triangulation Matching (DTM) are
devised. Blob matching provides a general matching framework by merging
together several strategies, including pre-filtering as well as many-to-many
and symmetric matching, enabling to achieve a global improvement upon each
individual strategy. DTM alternates between Delaunay triangulation contractions
and expansions to figure out and adjust keypoint neighborhood consistency.
Experimental evaluation shows that DTM is comparable or better than the
state-of-the-art in terms of matching accuracy and robustness, especially for
non-planar scenes. Evaluation is carried out according to a new benchmark
devised for analyzing the matching pipeline in terms of correct correspondences
on both planar and non-planar scenes, including state-of-the-art methods as
well as the common SIFT matching approach for reference. This evaluation can be
of assistance for future research in this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1"&gt;Fabio Bellavia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modelling resource allocation in uncertain system environment through deep reinforcement learning. (arXiv:2106.09461v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09461</id>
        <link href="http://arxiv.org/abs/2106.09461"/>
        <updated>2021-06-18T02:06:38.123Z</updated>
        <summary type="html"><![CDATA[Reinforcement Learning has applications in field of mechatronics, robotics,
and other resource-constrained control system. Problem of resource allocation
is primarily solved using traditional predefined techniques and modern deep
learning methods. The drawback of predefined and most deep learning methods for
resource allocation is failing to meet the requirements in cases of uncertain
system environment. We can approach problem of resource allocation in uncertain
system environment alongside following certain criteria using deep
reinforcement learning. Also, reinforcement learning has ability for adapting
to new uncertain environment for prolonged period of time. The paper provides a
detailed comparative analysis on various deep reinforcement learning methods by
applying different components to modify architecture of reinforcement learning
with use of noisy layers, prioritized replay, bagging, duelling networks, and
other related combination to obtain improvement in terms of performance and
reduction of computational cost. The paper identifies problem of resource
allocation in uncertain environment could be effectively solved using Noisy
Bagging duelling double deep Q network achieving efficiency of 97.7% by
maximizing reward with significant exploration in given simulated environment
for resource allocation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gandhi_N/0/1/0/all/0/1"&gt;Neel Gandhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Shakti Mishra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-VFL: A Vertical Federated Learning System for Multiple Data and Label Owners. (arXiv:2106.05468v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05468</id>
        <link href="http://arxiv.org/abs/2106.05468"/>
        <updated>2021-06-18T02:06:38.112Z</updated>
        <summary type="html"><![CDATA[Vertical Federated Learning (VFL) refers to the collaborative training of a
model on a dataset where the features of the dataset are split among multiple
data owners, while label information is owned by a single data owner. In this
paper, we propose a novel method, Multi Vertical Federated Learning
(Multi-VFL), to train VFL models when there are multiple data and label owners.
Our approach is the first to consider the setting where $D$-data owners (across
which features are distributed) and $K$-label owners (across which labels are
distributed) exist. This proposed configuration allows different entities to
train and learn optimal models without having to share their data. Our
framework makes use of split learning and adaptive federated optimizers to
solve this problem. For empirical evaluation, we run experiments on the MNIST
and FashionMNIST datasets. Our results show that using adaptive optimizers for
model aggregation fastens convergence and improves accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mugunthan_V/0/1/0/all/0/1"&gt;Vaikkunth Mugunthan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Pawan Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kagal_L/0/1/0/all/0/1"&gt;Lalana Kagal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Role of Entropy-based Loss for Learning Causal Structures with Continuous Optimization. (arXiv:2106.02835v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02835</id>
        <link href="http://arxiv.org/abs/2106.02835"/>
        <updated>2021-06-18T02:06:38.019Z</updated>
        <summary type="html"><![CDATA[Causal discovery from observational data is an important but challenging task
in many scientific fields. Recently, NOTEARS [Zheng et al., 2018] formulates
the causal structure learning problem as a continuous optimization problem
using least-square loss with an acyclicity constraint. Though the least-square
loss function is well justified under the standard Gaussian noise assumption,
it is limited if the assumption does not hold. In this work, we theoretically
show that the violation of the Gaussian noise assumption will hinder the causal
direction identification, making the causal orientation fully determined by the
causal strength as well as the variances of noises in the linear case and the
noises of strong non-Gaussianity in the nonlinear case. Consequently, we
propose a more general entropy-based loss that is theoretically consistent with
the likelihood score under any noise distribution. We run extensive empirical
evaluations on both synthetic data and real-world data to validate the
effectiveness of the proposed method and show that our method achieves the best
in Structure Hamming Distance, False Discovery Rate, and True Positive Rate
matrices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1"&gt;Ruichu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weilin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_J/0/1/0/all/0/1"&gt;Jie Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1"&gt;Zhifeng Hao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Secure Multi-Function Computation with Private Remote Sources. (arXiv:2106.09485v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.09485</id>
        <link href="http://arxiv.org/abs/2106.09485"/>
        <updated>2021-06-18T02:06:38.013Z</updated>
        <summary type="html"><![CDATA[We consider a distributed function computation problem in which parties
observing noisy versions of a remote source facilitate the computation of a
function of their observations at a fusion center through public communication.
The distributed function computation is subject to constraints, including not
only reliability and storage but also privacy and secrecy. Specifically, 1) the
remote source should remain private from an eavesdropper and the fusion center,
measured in terms of the information leaked about the remote source; 2) the
function computed should remain secret from the eavesdropper, measured in terms
of the information leaked about the arguments of the function, to ensure
secrecy regardless of the exact function used. We derive the exact rate regions
for lossless and lossy single-function computation and illustrate the lossy
single-function computation rate region for an information bottleneck example,
in which the optimal auxiliary random variables are characterized for
binary-input symmetric-output channels. We extend the approach to lossless and
lossy asynchronous multiple-function computations with joint secrecy and
privacy constraints, in which case inner and outer bounds for the rate regions
differing only in the Markov chain conditions imposed are characterized.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gunlu_O/0/1/0/all/0/1"&gt;Onur G&amp;#xfc;nl&amp;#xfc;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bloch_M/0/1/0/all/0/1"&gt;Matthieu Bloch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaefer_R/0/1/0/all/0/1"&gt;Rafael F. Schaefer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite-Sample Analysis of Stochastic Approximation Using Smooth Convex Envelopes. (arXiv:2002.00874v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00874</id>
        <link href="http://arxiv.org/abs/2002.00874"/>
        <updated>2021-06-18T02:06:38.004Z</updated>
        <summary type="html"><![CDATA[Stochastic Approximation (SA) is a popular approach for solving fixed-point
equations where the information is corrupted by noise. In this paper, we
consider an SA involving a contraction mapping with respect to an arbitrary
norm, and show its finite-sample error bounds while using different stepsizes.
The idea is to construct a smooth Lyapunov function using the generalized
Moreau envelope, and show that the iterates of SA have negative drift with
respect to that Lyapunov function. Our result is applicable in Reinforcement
Learning (RL). In particular, we use it to establish the first-known
convergence rate of the V-trace algorithm for off-policy TD-learning. Moreover,
we also use it to study TD-learning in the on-policy setting, and recover the
existing state-of-the-art results for $Q$-learning. Importantly, our
construction results in only a logarithmic dependence of the convergence bound
on the size of the state-space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zaiwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1"&gt;Siva Theja Maguluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1"&gt;Karthikeyan Shanmugam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discond-VAE: Disentangling Continuous Factors from the Discrete. (arXiv:2009.08039v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08039</id>
        <link href="http://arxiv.org/abs/2009.08039"/>
        <updated>2021-06-18T02:06:37.997Z</updated>
        <summary type="html"><![CDATA[In the real-world data, there are common variations shared by all classes
(e.g. category label) and exclusive variations of each class. We propose a
variant of VAE capable of disentangling both of these variations. To represent
these generative factors of data, we introduce two sets of continuous latent
variables, private variable and public variable. Our proposed framework models
the private variable as a Mixture of Gaussian and the public variable as a
Gaussian, respectively. Each mode of the private variable is responsible for a
class of the discrete variable.

Most of the previous attempts to integrate the discrete generative factors to
disentanglement assume statistical independence between the continuous and
discrete variables. Our proposed model, which we call Discond-VAE, DISentangles
the class-dependent CONtinuous factors from the Discrete factors by introducing
the private variables. The experiments show that Discond-VAE can discover the
private and public factors from data. Moreover, even under the dataset with
only public factors, Discond-VAE does not fail and adapts the private variables
to represent the public factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jaewoong Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1"&gt;Geonho Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Myungjoo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-exponentially weighted aggregation: regret bounds for unbounded loss functions. (arXiv:2009.03017v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.03017</id>
        <link href="http://arxiv.org/abs/2009.03017"/>
        <updated>2021-06-18T02:06:37.989Z</updated>
        <summary type="html"><![CDATA[We tackle the problem of online optimization with a general, possibly
unbounded, loss function. It is well known that when the loss is bounded, the
exponentially weighted aggregation strategy (EWA) leads to a regret in
$\sqrt{T}$ after $T$ steps. In this paper, we study a generalized aggregation
strategy, where the weights no longer depend exponentially on the losses. Our
strategy is based on Follow The Regularized Leader (FTRL): we minimize the
expected losses plus a regularizer, that is here a $\phi$-divergence. When the
regularizer is the Kullback-Leibler divergence, we obtain EWA as a special
case. Using alternative divergences enables unbounded losses, at the cost of a
worst regret bound in some cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Alquier_P/0/1/0/all/0/1"&gt;Pierre Alquier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Graph Neural Networks with 1000 Layers. (arXiv:2106.07476v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07476</id>
        <link href="http://arxiv.org/abs/2106.07476"/>
        <updated>2021-06-18T02:06:37.968Z</updated>
        <summary type="html"><![CDATA[Deep graph neural networks (GNNs) have achieved excellent results on various
tasks on increasingly large graph datasets with millions of nodes and edges.
However, memory complexity has become a major obstacle when training deep GNNs
for practical applications due to the immense number of nodes, edges, and
intermediate activations. To improve the scalability of GNNs, prior works
propose smart graph sampling or partitioning strategies to train GNNs with a
smaller set of nodes or sub-graphs. In this work, we study reversible
connections, group convolutions, weight tying, and equilibrium models to
advance the memory and parameter efficiency of GNNs. We find that reversible
connections in combination with deep network architectures enable the training
of overparameterized GNNs that significantly outperform existing methods on
multiple datasets. Our models RevGNN-Deep (1001 layers with 80 channels each)
and RevGNN-Wide (448 layers with 224 channels each) were both trained on a
single commodity GPU and achieve an ROC-AUC of $87.74 \pm 0.13$ and $88.24 \pm
0.15$ on the ogbn-proteins dataset. To the best of our knowledge, RevGNN-Deep
is the deepest GNN in the literature by one order of magnitude. Please visit
our project website https://www.deepgcns.org/arch/gnn1000 for more information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guohao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1"&gt;Matthias M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1"&gt;Vladlen Koltun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Variance Reduction in Online Experiments. (arXiv:2106.07263v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07263</id>
        <link href="http://arxiv.org/abs/2106.07263"/>
        <updated>2021-06-18T02:06:37.961Z</updated>
        <summary type="html"><![CDATA[We consider the problem of variance reduction in randomized controlled
trials, through the use of covariates correlated with the outcome but
independent of the treatment. We propose a machine learning regression-adjusted
treatment effect estimator, which we call MLRATE. MLRATE uses machine learning
predictors of the outcome to reduce estimator variance. It employs
cross-fitting to avoid overfitting biases, and we prove consistency and
asymptotic normality under general conditions. MLRATE is robust to poor
predictions from the machine learning step: if the predictions are uncorrelated
with the outcomes, the estimator performs asymptotically no worse than the
standard difference-in-means estimator, while if predictions are highly
correlated with outcomes, the efficiency gains are large. In A/A tests, for a
set of 48 outcome metrics commonly monitored in Facebook experiments the
estimator has over 70% lower variance than the simple difference-in-means
estimator, and about 19% lower variance than the common univariate procedure
which adjusts only for pre-experiment values of the outcome.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yongyi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Coey_D/0/1/0/all/0/1"&gt;Dominic Coey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Konutgan_M/0/1/0/all/0/1"&gt;Mikael Konutgan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenting Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schoener_C/0/1/0/all/0/1"&gt;Chris Schoener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Goldman_M/0/1/0/all/0/1"&gt;Matt Goldman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Correlation-Based Multiview Learning and Self-Supervision: A Unifying Perspective. (arXiv:2106.07115v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07115</id>
        <link href="http://arxiv.org/abs/2106.07115"/>
        <updated>2021-06-18T02:06:37.952Z</updated>
        <summary type="html"><![CDATA[Multiple views of data, both naturally acquired (e.g., image and audio) and
artificially produced (e.g., via adding different noise to data samples), have
proven useful in enhancing representation learning. Natural views are often
handled by multiview analysis tools, e.g., (deep) canonical correlation
analysis [(D)CCA], while the artificial ones are frequently used in
self-supervised learning (SSL) paradigms, e.g., SimCLR and Barlow Twins. Both
types of approaches often involve learning neural feature extractors such that
the embeddings of data exhibit high cross-view correlations. Although
intuitive, the effectiveness of correlation-based neural embedding is only
empirically validated. This work puts forth a theory-backed framework for
unsupervised multiview learning. Our development starts with proposing a
multiview model, where each view is a nonlinear mixture of shared and private
components. Consequently, the learning problem boils down to shared/private
component identification and disentanglement. Under this model, latent
correlation maximization is shown to guarantee the extraction of the shared
components across views (up to certain ambiguities). In addition, the private
information in each view can be provably disentangled from the shared using
proper regularization design. The method is tested on a series of tasks, e.g.,
downstream clustering, which all show promising performance. Our development
also provides a unifying perspective for understanding various DCCA and SSL
schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1"&gt;Qi Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiao Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Songtao Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Expert ODEs into Neural ODEs: Pharmacology and Disease Progression. (arXiv:2106.02875v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02875</id>
        <link href="http://arxiv.org/abs/2106.02875"/>
        <updated>2021-06-18T02:06:37.946Z</updated>
        <summary type="html"><![CDATA[Modeling a system's temporal behaviour in reaction to external stimuli is a
fundamental problem in many areas. Pure Machine Learning (ML) approaches often
fail in the small sample regime and cannot provide actionable insights beyond
predictions. A promising modification has been to incorporate expert domain
knowledge into ML models. The application we consider is predicting the
progression of disease under medications, where a plethora of domain knowledge
is available from pharmacology. Pharmacological models describe the dynamics of
carefully-chosen medically meaningful variables in terms of systems of Ordinary
Differential Equations (ODEs). However, these models only describe a limited
collection of variables, and these variables are often not observable in
clinical environments. To close this gap, we propose the latent hybridisation
model (LHM) that integrates a system of expert-designed ODEs with
machine-learned Neural ODEs to fully describe the dynamics of the system and to
link the expert and latent variables to observable quantities. We evaluated LHM
on synthetic data as well as real-world intensive care data of COVID-19
patients. LHM consistently outperforms previous works, especially when few
training samples are available such as at the beginning of the pandemic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1"&gt;Zhaozhi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zame_W/0/1/0/all/0/1"&gt;William R. Zame&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleuren_L/0/1/0/all/0/1"&gt;Lucas M. Fleuren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elbers_P/0/1/0/all/0/1"&gt;Paul Elbers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1"&gt;Mihaela van der Schaar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metrizing Weak Convergence with Maximum Mean Discrepancies. (arXiv:2006.09268v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09268</id>
        <link href="http://arxiv.org/abs/2006.09268"/>
        <updated>2021-06-18T02:06:37.939Z</updated>
        <summary type="html"><![CDATA[This paper characterizes the maximum mean discrepancies (MMD) that metrize
the weak convergence of probability measures for a wide class of kernels. More
precisely, we prove that, on a locally compact, non-compact, Hausdorff space,
the MMD of a bounded continuous Borel measurable kernel k, whose reproducing
kernel Hilbert space (RKHS) functions vanish at infinity, metrizes the weak
convergence of probability measures if and only if k is continuous and
integrally strictly positive definite (i.s.p.d.) over all signed, finite,
regular Borel measures. We also correct a prior result of Simon-Gabriel &
Sch\"olkopf (JMLR, 2018, Thm.12) by showing that there exist both bounded
continuous i.s.p.d. kernels that do not metrize weak convergence and bounded
continuous non-i.s.p.d. kernels that do metrize it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Simon_Gabriel_C/0/1/0/all/0/1"&gt;Carl-Johann Simon-Gabriel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barp_A/0/1/0/all/0/1"&gt;Alessandro Barp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1"&gt;Lester Mackey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Protecting gender and identity with disentangled speech representations. (arXiv:2104.11051v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11051</id>
        <link href="http://arxiv.org/abs/2104.11051"/>
        <updated>2021-06-18T02:06:37.932Z</updated>
        <summary type="html"><![CDATA[Besides its linguistic content, our speech is rich in biometric information
that can be inferred by classifiers. Learning privacy-preserving
representations for speech signals enables downstream tasks without sharing
unnecessary, private information about an individual. In this paper, we show
that protecting gender information in speech is more effective than modelling
speaker-identity information only when generating a non-sensitive
representation of speech. Our method relies on reconstructing speech by
decoding linguistic content along with gender information using a variational
autoencoder. Specifically, we exploit disentangled representation learning to
encode information about different attributes into separate subspaces that can
be factorised independently. We present a novel way to encode gender
information and disentangle two sensitive biometric identifiers, namely gender
and identity, in a privacy-protecting setting. Experiments on the LibriSpeech
dataset show that gender recognition and speaker verification can be reduced to
a random guess, protecting against classification-based attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stoidis_D/0/1/0/all/0/1"&gt;Dimitrios Stoidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1"&gt;Andrea Cavallaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empowerment-based Solution to Robotic Manipulation Tasks with Sparse Rewards. (arXiv:2010.07986v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07986</id>
        <link href="http://arxiv.org/abs/2010.07986"/>
        <updated>2021-06-18T02:06:37.916Z</updated>
        <summary type="html"><![CDATA[In order to provide adaptive and user-friendly solutions to robotic
manipulation, it is important that the agent can learn to accomplish tasks even
if they are only provided with very sparse instruction signals. To address the
issues reinforcement learning algorithms face when task rewards are sparse,
this paper proposes an intrinsic motivation approach that can be easily
integrated into any standard reinforcement learning algorithm and can allow
robotic manipulators to learn useful manipulation skills with only sparse
extrinsic rewards. Through integrating and balancing empowerment and curiosity,
this approach shows superior performance compared to other state-of-the-art
intrinsic exploration approaches during extensive empirical testing.
Qualitative analysis also shows that when combined with diversity-driven
intrinsic motivations, this approach can help manipulators learn a set of
diverse skills which could potentially be applied to other more complicated
manipulation tasks and accelerate their learning process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1"&gt;Siyu Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_A/0/1/0/all/0/1"&gt;Andreas Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1"&gt;Brian Williams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Query Lower Bounds for List-Decodable Linear Regression. (arXiv:2106.09689v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.09689</id>
        <link href="http://arxiv.org/abs/2106.09689"/>
        <updated>2021-06-18T02:06:37.910Z</updated>
        <summary type="html"><![CDATA[We study the problem of list-decodable linear regression, where an adversary
can corrupt a majority of the examples. Specifically, we are given a set $T$ of
labeled examples $(x, y) \in \mathbb{R}^d \times \mathbb{R}$ and a parameter
$0< \alpha <1/2$ such that an $\alpha$-fraction of the points in $T$ are i.i.d.
samples from a linear regression model with Gaussian covariates, and the
remaining $(1-\alpha)$-fraction of the points are drawn from an arbitrary noise
distribution. The goal is to output a small list of hypothesis vectors such
that at least one of them is close to the target regression vector. Our main
result is a Statistical Query (SQ) lower bound of $d^{\mathrm{poly}(1/\alpha)}$
for this problem. Our SQ lower bound qualitatively matches the performance of
previously developed algorithms, providing evidence that current upper bounds
for this task are nearly best possible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1"&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1"&gt;Daniel M. Kane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pensia_A/0/1/0/all/0/1"&gt;Ankit Pensia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pittas_T/0/1/0/all/0/1"&gt;Thanasis Pittas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stewart_A/0/1/0/all/0/1"&gt;Alistair Stewart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualising Deep Network's Time-Series Representations. (arXiv:2103.07176v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07176</id>
        <link href="http://arxiv.org/abs/2103.07176"/>
        <updated>2021-06-18T02:06:37.905Z</updated>
        <summary type="html"><![CDATA[Despite the popularisation of machine learning models, more often than not,
they still operate as black boxes with no insight into what is happening inside
the model. There exist a few methods that allow to visualise and explain why a
model has made a certain prediction. Those methods, however, allow
visualisation of the link between the input and output of the model without
presenting how the model learns to represent the data used to train the model
as whole. In this paper, a method that addresses that issue is proposed, with a
focus on visualising multi-dimensional time-series data. Experiments on a
high-frequency stock market dataset show that the method provides fast and
discernible visualisations. Large datasets can be visualised quickly and on one
plot, which makes it easy for a user to compare the learned representations of
the data. The developed method successfully combines known techniques to
provide an insight into the inner workings of time-series classification
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leporowski_B/0/1/0/all/0/1"&gt;B&amp;#x142;a&amp;#x17c;ej Leporowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evidence-based Factual Error Correction. (arXiv:2106.01072v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01072</id>
        <link href="http://arxiv.org/abs/2106.01072"/>
        <updated>2021-06-18T02:06:37.898Z</updated>
        <summary type="html"><![CDATA[This paper introduces the task of factual error correction: performing edits
to a claim so that the generated rewrite is better supported by evidence. This
extends the well-studied task of fact verification by providing a mechanism to
correct written texts that are refuted or only partially supported by evidence.
We demonstrate that it is feasible to train factual error correction systems
from existing fact checking datasets which only contain labeled claims
accompanied by evidence, but not the correction. We achieve this by employing a
two-stage distant supervision approach that incorporates evidence into masked
claims when generating corrections. Our approach, based on the T5 transformer
and using retrieved evidence, achieved better results than existing work which
used a pointer copy network and gold evidence, producing accurate factual error
corrections for 5x more instances in human evaluation and a .125 increase in
SARI score. The evaluation is conducted on a dataset of 65,000 instances based
on a recent fact verification shared task and we release it to enable further
work on the task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1"&gt;James Thorne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1"&gt;Andreas Vlachos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Randomized Value Functions via Posterior State-Abstraction Sampling. (arXiv:2010.02383v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02383</id>
        <link href="http://arxiv.org/abs/2010.02383"/>
        <updated>2021-06-18T02:06:37.881Z</updated>
        <summary type="html"><![CDATA[State abstraction has been an essential tool for dramatically improving the
sample efficiency of reinforcement-learning algorithms. Indeed, by exposing and
accentuating various types of latent structure within the environment,
different classes of state abstraction have enabled improved theoretical
guarantees and empirical performance. When dealing with state abstractions that
capture structure in the value function, however, a standard assumption is that
the true abstraction has been supplied or unrealistically computed a priori,
leaving open the question of how to efficiently uncover such latent structure
while jointly seeking out optimal behavior. Taking inspiration from the bandit
literature, we propose that an agent seeking out latent task structure must
explicitly represent and maintain its uncertainty over that structure as part
of its overall uncertainty about the environment. We introduce a practical
algorithm for doing this using two posterior distributions over state
abstractions and abstract-state values. In empirically validating our approach,
we find that substantial performance gains lie in the multi-task setting where
tasks share a common, low-dimensional representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arumugam_D/0/1/0/all/0/1"&gt;Dilip Arumugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperparameter Optimization via Sequential Uniform Designs. (arXiv:2009.03586v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.03586</id>
        <link href="http://arxiv.org/abs/2009.03586"/>
        <updated>2021-06-18T02:06:37.874Z</updated>
        <summary type="html"><![CDATA[Hyperparameter optimization (HPO) plays a central role in the automated
machine learning (AutoML). It is a challenging task as the response surfaces of
hyperparameters are generally unknown, hence essentially a global optimization
problem. This paper reformulates HPO as a computer experiment and proposes a
novel sequential uniform design (SeqUD) strategy with three-fold advantages: a)
the hyperparameter space is adaptively explored with evenly spread design
points, without the need of expensive meta-modeling and acquisition
optimization; b) the batch-by-batch design points are sequentially generated
with parallel processing support; c) a new augmented uniform design algorithm
is developed for the efficient real-time generation of follow-up design points.
Extensive experiments are conducted on both global optimization tasks and HPO
applications. The numerical results show that the proposed SeqUD strategy
outperforms benchmark HPO methods, and it can be therefore a promising and
competitive alternative to existing AutoML tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zebin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling. (arXiv:2104.02321v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02321</id>
        <link href="http://arxiv.org/abs/2104.02321"/>
        <updated>2021-06-18T02:06:37.868Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce NU-Wave, the first neural audio upsampling model
to produce waveforms of sampling rate 48kHz from coarse 16kHz or 24kHz inputs,
while prior works could generate only up to 16kHz. NU-Wave is the first
diffusion probabilistic model for audio super-resolution which is engineered
based on neural vocoders. NU-Wave generates high-quality audio that achieves
high performance in terms of signal-to-noise ratio (SNR), log-spectral distance
(LSD), and accuracy of the ABX test. In all cases, NU-Wave outperforms the
baseline models despite the substantially smaller model capacity (3.0M
parameters) than baselines (5.4-21%). The audio samples of our model are
available at https://mindslab-ai.github.io/nuwave, and the code will be made
available soon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junhyeok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1"&gt;Seungu Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Merging versus Ensembling in Multi-Study Prediction: Theoretical Insight from Random Effects. (arXiv:1905.07382v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.07382</id>
        <link href="http://arxiv.org/abs/1905.07382"/>
        <updated>2021-06-18T02:06:37.860Z</updated>
        <summary type="html"><![CDATA[A critical decision point when training predictors using multiple studies is
whether these studies should be combined or treated separately. We compare two
multi-study learning approaches in the presence of potential heterogeneity in
predictor-outcome relationships across datasets. We consider 1) merging all of
the datasets and training a single learner, and 2) multi-study ensembling,
which involves training a separate learner on each dataset and combining the
predictions resulting from each learner. In a linear regression setting, we
show analytically and confirm via simulation that merging yields lower
prediction error than ensembling when the predictor-outcome relationships are
relatively homogeneous across studies. However, as cross-study heterogeneity
increases, there exists a transition point beyond which ensembling outperforms
merging. We provide analytic expressions for the transition point in various
scenarios, study asymptotic properties, and illustrate how transition point
theory can be used for deciding when studies should be combined with an
application from metabolomics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Guan_Z/0/1/0/all/0/1"&gt;Zoe Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Parmigiani_G/0/1/0/all/0/1"&gt;Giovanni Parmigiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Patil_P/0/1/0/all/0/1"&gt;Prasad Patil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On a Sparse Shortcut Topology of Artificial Neural Networks. (arXiv:1811.09003v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1811.09003</id>
        <link href="http://arxiv.org/abs/1811.09003"/>
        <updated>2021-06-18T02:06:37.854Z</updated>
        <summary type="html"><![CDATA[Over recent years, deep learning has become the mainstream data-driven
approach to solve many important real-world problems. In the successful network
architectures, shortcut connections are well established to take the outputs of
earlier layers as additional inputs to later layers, which have produced
excellent results. Despite the extraordinary effectiveness of shortcuts, there
remain important questions on the underlying mechanism and associated
functionalities. For example, why are shortcuts powerful? Why shortcuts
generalize well? To address these questions, we investigate the representation
and generalization ability of a sparse shortcut topology. Specifically, we
first demonstrate that this topology can empower a one-neuron-wide deep network
to approximate any univariate continuous function. Then, we present a novel
width-bounded universal approximator in contrast to depth-bounded universal
approximators, and also extend the approximation result to a family of networks
such that in the view of approximation ability, these networks are equally
competent. Furthermore, we use the generalization bound theory to show that the
investigated shortcut topology enjoys an excellent generalizability. Finally,
we corroborate our theoretical analyses with experiments on some well-known
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1"&gt;Fenglei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dayang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hengtao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qikui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1"&gt;Pingkun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hengyong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental Without Replacement Sampling in Nonconvex Optimization. (arXiv:2007.07557v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.07557</id>
        <link href="http://arxiv.org/abs/2007.07557"/>
        <updated>2021-06-18T02:06:37.849Z</updated>
        <summary type="html"><![CDATA[Minibatch decomposition methods for empirical risk minimization are commonly
analysed in a stochastic approximation setting, also known as sampling with
replacement. On the other hands modern implementations of such techniques are
incremental: they rely on sampling without replacement, for which available
analysis are much scarcer. We provide convergence guaranties for the latter
variant by analysing a versatile incremental gradient scheme. For this scheme,
we consider constant, decreasing or adaptive step sizes. In the smooth setting
we obtain explicit complexity estimates in terms of epoch counter. In the
nonsmooth setting we prove that the sequence is attracted by solutions of
optimality conditions of the problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1"&gt;Edouard Pauwels&lt;/a&gt; (IRIT-ADRIA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedV: Privacy-Preserving Federated Learning over Vertically Partitioned Data. (arXiv:2103.03918v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03918</id>
        <link href="http://arxiv.org/abs/2103.03918"/>
        <updated>2021-06-18T02:06:37.840Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has been proposed to allow collaborative training of
machine learning (ML) models among multiple parties where each party can keep
its data private. In this paradigm, only model updates, such as model weights
or gradients, are shared. Many existing approaches have focused on horizontal
FL, where each party has the entire feature set and labels in the training data
set. However, many real scenarios follow a vertically-partitioned FL setup,
where a complete feature set is formed only when all the datasets from the
parties are combined, and the labels are only available to a single party.
Privacy-preserving vertical FL is challenging because complete sets of labels
and features are not owned by one entity. Existing approaches for vertical FL
require multiple peer-to-peer communications among parties, leading to lengthy
training times, and are restricted to (approximated) linear models and just two
parties. To close this gap, we propose FedV, a framework for secure gradient
computation in vertical settings for several widely used ML models such as
linear models, logistic regression, and support vector machines. FedV removes
the need for peer-to-peer communication among parties by using functional
encryption schemes; this allows FedV to achieve faster training times. It also
works for larger and changing sets of parties. We empirically demonstrate the
applicability for multiple types of ML models and show a reduction of 10%-70%
of training time and 80% to 90% in data transfer with respect to the
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Runhua Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1"&gt;Nathalie Baracaldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1"&gt;Ali Anwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_J/0/1/0/all/0/1"&gt;James Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludwig_H/0/1/0/all/0/1"&gt;Heiko Ludwig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NPAS: A Compiler-aware Framework of Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration. (arXiv:2012.00596v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00596</id>
        <link href="http://arxiv.org/abs/2012.00596"/>
        <updated>2021-06-18T02:06:37.823Z</updated>
        <summary type="html"><![CDATA[With the increasing demand to efficiently deploy DNNs on mobile edge devices,
it becomes much more important to reduce unnecessary computation and increase
the execution speed. Prior methods towards this goal, including model
compression and network architecture search (NAS), are largely performed
independently and do not fully consider compiler-level optimizations which is a
must-do for mobile acceleration. In this work, we first propose (i) a general
category of fine-grained structured pruning applicable to various DNN layers,
and (ii) a comprehensive, compiler automatic code generation framework
supporting different DNNs and different pruning schemes, which bridge the gap
of model compression and NAS. We further propose NPAS, a compiler-aware unified
network pruning, and architecture search. To deal with large search space, we
propose a meta-modeling procedure based on reinforcement learning with fast
evaluation and Bayesian optimization, ensuring the total number of training
epochs comparable with representative NAS frameworks. Our framework achieves
6.7ms, 5.9ms, 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3
level), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an
off-the-shelf mobile phone, consistently outperforming prior work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhengang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1"&gt;Wei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yuxuan Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xuan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1"&gt;Zheng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1"&gt;Zhenglun Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kaiyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1"&gt;Bin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A General Framework For Detecting Anomalous Inputs to DNN Classifiers. (arXiv:2007.15147v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15147</id>
        <link href="http://arxiv.org/abs/2007.15147"/>
        <updated>2021-06-18T02:06:37.815Z</updated>
        <summary type="html"><![CDATA[Detecting anomalous inputs, such as adversarial and out-of-distribution (OOD)
inputs, is critical for classifiers (including deep neural networks or DNNs)
deployed in real-world applications. While prior works have proposed various
methods to detect such anomalous samples using information from the internal
layer representations of a DNN, there is a lack of consensus on a principled
approach for the different components of such a detection method. As a result,
often heuristic and one-off methods are applied for different aspects of this
problem. We propose an unsupervised anomaly detection framework based on the
internal DNN layer representations in the form of a meta-algorithm with
configurable components. We proceed to propose specific instantiations for each
component of the meta-algorithm based on ideas grounded in statistical testing
and anomaly detection. We evaluate the proposed methods on well-known image
classification datasets with strong adversarial attacks and OOD inputs,
including an adaptive attack that uses the internal layer representations of
the DNN (often not considered in prior work). Comparisons with five
recently-proposed competing detection methods demonstrates the effectiveness of
our method in detecting adversarial and OOD inputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raghuram_J/0/1/0/all/0/1"&gt;Jayaram Raghuram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1"&gt;Varun Chandrasekaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1"&gt;Suman Banerjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invertible Concept-based Explanations for CNN Models with Non-negative Concept Activation Vectors. (arXiv:2006.15417v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15417</id>
        <link href="http://arxiv.org/abs/2006.15417"/>
        <updated>2021-06-18T02:06:37.807Z</updated>
        <summary type="html"><![CDATA[Convolutional neural network (CNN) models for computer vision are powerful
but lack explainability in their most basic form. This deficiency remains a key
challenge when applying CNNs in important domains. Recent work on explanations
through feature importance of approximate linear models has moved from
input-level features (pixels or segments) to features from mid-layer feature
maps in the form of concept activation vectors (CAVs). CAVs contain
concept-level information and could be learned via clustering. In this work, we
rethink the ACE algorithm of Ghorbani et~al., proposing an alternative
invertible concept-based explanation (ICE) framework to overcome its
shortcomings. Based on the requirements of fidelity (approximate models to
target models) and interpretability (being meaningful to people), we design
measurements and evaluate a range of matrix factorization methods with our
framework. We find that non-negative concept activation vectors (NCAVs) from
non-negative matrix factorization provide superior performance in
interpretability and fidelity based on computational and human subject
experiments. Our framework provides both local and global concept-level
explanations for pre-trained CNN models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruihan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madumal_P/0/1/0/all/0/1"&gt;Prashan Madumal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1"&gt;Tim Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ehinger_K/0/1/0/all/0/1"&gt;Krista A. Ehinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1"&gt;Benjamin I. P. Rubinstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can convolutional ResNets approximately preserve input distances? A frequency analysis perspective. (arXiv:2106.02469v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02469</id>
        <link href="http://arxiv.org/abs/2106.02469"/>
        <updated>2021-06-18T02:06:37.791Z</updated>
        <summary type="html"><![CDATA[ResNets constrained to be bi-Lipschitz, that is, approximately distance
preserving, have been a crucial component of recently proposed techniques for
deterministic uncertainty quantification in neural models. We show that
theoretical justifications for recent regularisation schemes trying to enforce
such a constraint suffer from a crucial flaw -- the theoretical link between
the regularisation scheme used and bi-Lipschitzness is only valid under
conditions which do not hold in practice, rendering existing theory of limited
use, despite the strong empirical performance of these models. We provide a
theoretical explanation for the effectiveness of these regularisation schemes
using a frequency analysis perspective, showing that under mild conditions
these schemes will enforce a lower Lipschitz bound on the low-frequency
projection of images. We then provide empirical evidence supporting our
theoretical claims, and perform further experiments which demonstrate that our
broader conclusions appear to hold when some of the mathematical assumptions of
our proof are relaxed, corresponding to the setup used in prior work. In
addition, we present a simple constructive algorithm to search for counter
examples to the distance preservation condition, and discuss possible
implications of our theory for future model design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1"&gt;Lewis Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amersfoort_J/0/1/0/all/0/1"&gt;Joost van Amersfoort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haiwen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Boolean Function Learnability on Deep Neural Networks. (arXiv:2009.05908v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05908</id>
        <link href="http://arxiv.org/abs/2009.05908"/>
        <updated>2021-06-18T02:06:37.784Z</updated>
        <summary type="html"><![CDATA[Computational learning theory states that many classes of boolean formulas
are learnable in polynomial time. This paper addresses the understudied subject
of how, in practice, such formulas can be learned by deep neural networks.
Specifically, we analyse boolean formulas associated with the decision version
of combinatorial optimisation problems, model sampling benchmarks, and random
3-CNFs with varying degrees of constrainedness. Our extensive experiments
indicate that: (i) regardless of the combinatorial optimisation problem,
relatively small and shallow neural networks are very good approximators of the
associated formulas; (ii) smaller formulas seem harder to learn, possibly due
to the fewer positive (satisfying) examples available; and (iii) interestingly,
underconstrained 3-CNF formulas are more challenging to learn than
overconstrained ones. Source code and relevant datasets are publicly available
(https://github.com/machine-reasoning-ufrgs/mlbf).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tavares_A/0/1/0/all/0/1"&gt;Anderson R. Tavares&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avelar_P/0/1/0/all/0/1"&gt;Pedro Avelar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flach_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o M. Flach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nicolau_M/0/1/0/all/0/1"&gt;Marcio Nicolau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamb_L/0/1/0/all/0/1"&gt;Luis C. Lamb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vardi_M/0/1/0/all/0/1"&gt;Moshe Vardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Transformers Really Perform Bad for Graph Representation?. (arXiv:2106.05234v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05234</id>
        <link href="http://arxiv.org/abs/2106.05234"/>
        <updated>2021-06-18T02:06:37.777Z</updated>
        <summary type="html"><![CDATA[The Transformer architecture has become a dominant choice in many domains,
such as natural language processing and computer vision. Yet, it has not
achieved competitive performance on popular leaderboards of graph-level
prediction compared to mainstream GNN variants. Therefore, it remains a mystery
how Transformers could perform well for graph representation learning. In this
paper, we solve this mystery by presenting Graphormer, which is built upon the
standard Transformer architecture, and could attain excellent results on a
broad range of graph representation learning tasks, especially on the recent
OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the
graph is the necessity of effectively encoding the structural information of a
graph into the model. To this end, we propose several simple yet effective
structural encoding methods to help Graphormer better model graph-structured
data. Besides, we mathematically characterize the expressive power of
Graphormer and exhibit that with our ways of encoding the structural
information of graphs, many popular GNN variants could be covered as the
special cases of Graphormer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1"&gt;Chengxuan Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shengjie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuxin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1"&gt;Guolin Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yanming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autobots: Latent Variable Sequential Set Transformers. (arXiv:2104.00563v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00563</id>
        <link href="http://arxiv.org/abs/2104.00563"/>
        <updated>2021-06-18T02:06:37.771Z</updated>
        <summary type="html"><![CDATA[Robust multi-agent trajectory prediction is essential for the safe control of
robots and vehicles that interact with humans. Many existing methods treat
social and temporal information separately and therefore fall short of
modelling the joint future trajectories of all agents in a socially consistent
way. To address this, we propose a new class of Latent Variable Sequential Set
Transformers which autoregressively model multi-agent trajectories. We refer to
these architectures as "AutoBots". AutoBots model the contents of sets (e.g.
representing the properties of agents in a scene) over time and employ
multi-head self-attention blocks over these sequences of sets to encode the
sociotemporal relationships between the different actors of a scene. This
produces either the trajectory of one ego-agent or a distribution over the
future trajectories for all agents under consideration. Our approach works for
general sequences of sets and we provide illustrative experiments modelling the
sequential structure of the multiple strokes that make up symbols in the
Omniglot data. For the single-agent prediction case, we validate our model on
the NuScenes motion prediction task and achieve competitive results on the
global leaderboard. In the multi-agent forecasting setting, we validate our
model on TrajNet. We find that our method outperforms physical extrapolation
and recurrent network baselines and generates scene-consistent trajectories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girgis_R/0/1/0/all/0/1"&gt;Roger Girgis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1"&gt;Florian Golemo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Codevilla_F/0/1/0/all/0/1"&gt;Felipe Codevilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1"&gt;Jim Aldon D&amp;#x27;Souza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_M/0/1/0/all/0/1"&gt;Martin Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1"&gt;Samira Ebrahimi Kahou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1"&gt;Felix Heide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BELT: Block-wise Missing Embedding Learning Transformer. (arXiv:2105.10360v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10360</id>
        <link href="http://arxiv.org/abs/2105.10360"/>
        <updated>2021-06-18T02:06:37.764Z</updated>
        <summary type="html"><![CDATA[Matrix completion has attracted attention in many fields, including
statistics, applied mathematics, and electrical engineering. Most of the works
focus on the independent sampling models under which the observed entries are
sampled independently. Motivated by applications in the integration of multiple
Electronic Health Record (EHR) datasets, we propose the method {\bf B}lock-wise
missing {\bf E}mbedding {\bf L}earning {\bf T}ransformer (BELT) to treat
row-wise/column-wise missingness. Specifically, BELT can recover block-wise
missing matrices efficiently when every pair of matrices has an overlap. Our
idea is to exploit the orthogonal Procrustes problem to align the eigenspace of
the two sub-matrices using their overlap, then complete the missing blocks by
the inner product of the two low-rank components. Besides, we prove the
statistical rate for the eigenspace of the underlying matrix, which is
comparable to the rate under the independently missing assumption. Simulation
studies show that the method performs well under a variety of configurations.
In the real data analysis, the method is applied to two tasks: (i) the
integrating of several point-wise mutual information matrices built by English
EHR and Chinese medical text data, and (ii) the machine translation between
English and Chinese medical concepts. Our method shows an advantage over
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Doudou Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianxi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lu_J/0/1/0/all/0/1"&gt;Junwei Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Schr\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02081</id>
        <link href="http://arxiv.org/abs/2106.02081"/>
        <updated>2021-06-18T02:06:37.757Z</updated>
        <summary type="html"><![CDATA[The Schr\"odinger bridge problem (SBP) finds the most likely stochastic
evolution between two probability distributions given a prior stochastic
evolution. As well as applications in the natural sciences, problems of this
kind have important applications in machine learning such as dataset alignment
and hypothesis testing. Whilst the theory behind this problem is relatively
mature, scalable numerical recipes to estimate the Schr\"odinger bridge remain
an active area of research. We prove an equivalence between the SBP and maximum
likelihood estimation enabling direct application of successful machine
learning techniques. We propose a numerical procedure to estimate SBPs using
Gaussian process and demonstrate the practical usage of our approach in
numerical simulations and experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1"&gt;Francisco Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1"&gt;Pierre Thodoroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1"&gt;Neil D. Lawrence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1"&gt;Austen Lamacraft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time Series Domain Adaptation via Sparse Associative Structure Alignment. (arXiv:2012.11797v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11797</id>
        <link href="http://arxiv.org/abs/2012.11797"/>
        <updated>2021-06-18T02:06:37.747Z</updated>
        <summary type="html"><![CDATA[Domain adaptation on time series data is an important but challenging task.
Most of the existing works in this area are based on the learning of the
domain-invariant representation of the data with the help of restrictions like
MMD. However, such extraction of the domain-invariant representation is a
non-trivial task for time series data, due to the complex dependence among the
timestamps. In detail, in the fully dependent time series, a small change of
the time lags or the offsets may lead to difficulty in the domain invariant
extraction. Fortunately, the stability of the causality inspired us to explore
the domain invariant structure of the data. To reduce the difficulty in the
discovery of causal structure, we relax it to the sparse associative structure
and propose a novel sparse associative structure alignment model for domain
adaptation. First, we generate the segment set to exclude the obstacle of
offsets. Second, the intra-variables and inter-variables sparse attention
mechanisms are devised to extract associative structure time-series data with
considering time lags. Finally, the associative structure alignment is used to
guide the transfer of knowledge from the source domain to the target one.
Experimental studies not only verify the good performance of our methods on
three real-world datasets but also provide some insightful discoveries on the
transferred knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1"&gt;Ruichu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zijian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Keli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Junjian Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhuozhang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoyan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhenjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-Learning and TD-Learning Variants. (arXiv:2102.01567v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01567</id>
        <link href="http://arxiv.org/abs/2102.01567"/>
        <updated>2021-06-18T02:06:37.739Z</updated>
        <summary type="html"><![CDATA[This paper develops an unified framework to study finite-sample convergence
guarantees of a large class of value-based asynchronous reinforcement learning
(RL) algorithms. We do this by first reformulating the RL algorithms as
\textit{Markovian Stochastic Approximation} (SA) algorithms to solve
fixed-point equations. We then develop a Lyapunov analysis and derive
mean-square error bounds on the convergence of the Markovian SA. Based on this
result, we establish finite-sample mean-square convergence bounds for
asynchronous RL algorithms such as $Q$-learning, $n$-step TD, TD$(\lambda)$,
and off-policy TD algorithms including V-trace. As a by-product, by analyzing
the convergence bounds of $n$-step TD and TD$(\lambda)$, we provide theoretical
insights into the bias-variance trade-off, i.e., efficiency of bootstrapping in
RL. This was first posed as an open problem in (Sutton, 1999).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zaiwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1"&gt;Siva Theja Maguluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1"&gt;Karthikeyan Shanmugam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pushing the Limits of Non-Autoregressive Speech Recognition. (arXiv:2104.03416v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03416</id>
        <link href="http://arxiv.org/abs/2104.03416"/>
        <updated>2021-06-18T02:06:37.719Z</updated>
        <summary type="html"><![CDATA[We combine recent advancements in end-to-end speech recognition to
non-autoregressive automatic speech recognition. We push the limits of
non-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,
Fisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC
on giant Conformer neural network architectures with SpecAugment and wav2vec2
pre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,
5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without
a language model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ng_E/0/1/0/all/0/1"&gt;Edwin G. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chiu_C/0/1/0/all/0/1"&gt;Chung-Cheng Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1"&gt;William Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Identifiable Features from Noisy Data with Structured Nonlinear ICA. (arXiv:2106.09620v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.09620</id>
        <link href="http://arxiv.org/abs/2106.09620"/>
        <updated>2021-06-18T02:06:37.703Z</updated>
        <summary type="html"><![CDATA[We introduce a new general identifiable framework for principled
disentanglement referred to as Structured Nonlinear Independent Component
Analysis (SNICA). Our contribution is to extend the identifiability theory of
deep generative models for a very broad class of structured models. While
previous works have shown identifiability for specific classes of time-series
models, our theorems extend this to more general temporal structures as well as
to models with more complex structures such as spatial dependencies. In
particular, we establish the major result that identifiability for this
framework holds even in the presence of noise of unknown distribution. The
SNICA setting therefore subsumes all the existing nonlinear ICA models for
time-series and also allows for new much richer identifiable models. Finally,
as an example of our framework's flexibility, we introduce the first nonlinear
ICA model for time-series that combines the following very useful properties:
it accounts for both nonstationarity and autocorrelation in a fully
unsupervised setting; performs dimensionality reduction; models hidden states;
and enables principled estimation and inference by variational
maximum-likelihood.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Halva_H/0/1/0/all/0/1"&gt;Hermanni H&amp;#xe4;lv&amp;#xe4;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Corff_S/0/1/0/all/0/1"&gt;Sylvain Le Corff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lehericy_L/0/1/0/all/0/1"&gt;Luc Leh&amp;#xe9;ricy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+So_J/0/1/0/all/0/1"&gt;Jonathan So&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yongjie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gassiat_E/0/1/0/all/0/1"&gt;Elisabeth Gassiat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hyvarinen_A/0/1/0/all/0/1"&gt;Aapo Hyvarinen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks. (arXiv:2006.12557v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12557</id>
        <link href="http://arxiv.org/abs/2006.12557"/>
        <updated>2021-06-18T02:06:37.685Z</updated>
        <summary type="html"><![CDATA[Data poisoning and backdoor attacks manipulate training data in order to
cause models to fail during inference. A recent survey of industry
practitioners found that data poisoning is the number one concern among threats
ranging from model stealing to adversarial attacks. However, it remains unclear
exactly how dangerous poisoning methods are and which ones are more effective
considering that these methods, even ones with identical objectives, have not
been tested in consistent or realistic settings. We observe that data poisoning
and backdoor attacks are highly sensitive to variations in the testing setup.
Moreover, we find that existing methods may not generalize to realistic
settings. While these existing works serve as valuable prototypes for data
poisoning, we apply rigorous tests to determine the extent to which we should
fear them. In order to promote fair comparison in future work, we develop
standardized benchmarks for data poisoning and backdoor attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1"&gt;Avi Schwarzschild&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Arjun Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1"&gt;John P Dickerson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[There is no data like more data -- current status of machine learning datasets in remote sensing. (arXiv:2105.11726v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11726</id>
        <link href="http://arxiv.org/abs/2105.11726"/>
        <updated>2021-06-18T02:06:37.679Z</updated>
        <summary type="html"><![CDATA[Annotated datasets have become one of the most crucial preconditions for the
development and evaluation of machine learning-based methods designed for the
automated interpretation of remote sensing data. In this paper, we review the
historic development of such datasets, discuss their features based on a few
selected examples, and address open issues for future developments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1"&gt;Michael Schmitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1"&gt;Seyed Ali Ahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansch_R/0/1/0/all/0/1"&gt;Ronny H&amp;#xe4;nsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batch Value-function Approximation with Only Realizability. (arXiv:2008.04990v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.04990</id>
        <link href="http://arxiv.org/abs/2008.04990"/>
        <updated>2021-06-18T02:06:37.659Z</updated>
        <summary type="html"><![CDATA[We make progress in a long-standing problem of batch reinforcement learning
(RL): learning $Q^\star$ from an exploratory and polynomial-sized dataset,
using a realizable and otherwise arbitrary function class. In fact, all
existing algorithms demand function-approximation assumptions stronger than
realizability, and the mounting negative evidence has led to a conjecture that
sample-efficient learning is impossible in this setting (Chen and Jiang, 2019).
Our algorithm, BVFT, breaks the hardness conjecture (albeit under a stronger
notion of exploratory data) via a tournament procedure that reduces the
learning problem to pairwise comparison, and solves the latter with the help of
a state-action partition constructed from the compared functions. We also
discuss how BVFT can be applied to model selection among other extensions and
open problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tengyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1"&gt;Nan Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Proposals for Probabilistic Programs with Inference Combinators. (arXiv:2103.00668v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00668</id>
        <link href="http://arxiv.org/abs/2103.00668"/>
        <updated>2021-06-18T02:06:37.652Z</updated>
        <summary type="html"><![CDATA[We develop operators for construction of proposals in probabilistic programs,
which we refer to as inference combinators. Inference combinators define a
grammar over importance samplers that compose primitive operations such as
application of a transition kernel and importance resampling. Proposals in
these samplers can be parameterized using neural networks, which in turn can be
trained by optimizing variational objectives. The result is a framework for
user-programmable variational methods that are correct by construction and can
be tailored to specific models. We demonstrate the flexibility of this
framework by implementing advanced variational methods based on amortized Gibbs
sampling and annealing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Stites_S/0/1/0/all/0/1"&gt;Sam Stites&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zimmermann_H/0/1/0/all/0/1"&gt;Heiko Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sennesh_E/0/1/0/all/0/1"&gt;Eli Sennesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meent_J/0/1/0/all/0/1"&gt;Jan-Willem van de Meent&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BinaryCoP: Binary Neural Network-based COVID-19 Face-Mask Wear and Positioning Predictor on Edge Devices. (arXiv:2102.03456v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03456</id>
        <link href="http://arxiv.org/abs/2102.03456"/>
        <updated>2021-06-18T02:06:37.646Z</updated>
        <summary type="html"><![CDATA[Face masks have long been used in many areas of everyday life to protect
against the inhalation of hazardous fumes and particles. They also offer an
effective solution in healthcare for bi-directional protection against
air-borne diseases. Wearing and positioning the mask correctly is essential for
its function. Convolutional neural networks (CNNs) offer an excellent solution
for face recognition and classification of correct mask wearing and
positioning. In the context of the ongoing COVID-19 pandemic, such algorithms
can be used at entrances to corporate buildings, airports, shopping areas, and
other indoor locations, to mitigate the spread of the virus. These application
scenarios impose major challenges to the underlying compute platform. The
inference hardware must be cheap, small and energy efficient, while providing
sufficient memory and compute power to execute accurate CNNs at a reasonably
low latency. To maintain data privacy of the public, all processing must remain
on the edge-device, without any communication with cloud servers. To address
these challenges, we present a low-power binary neural network classifier for
correct facial-mask wear and positioning. The classification task is
implemented on an embedded FPGA, performing high-throughput binary operations.
Classification can take place at up to ~6400 frames-per-second, easily enabling
multi-camera, speed-gate settings or statistics collection in crowd settings.
When deployed on a single entrance or gate, the idle power consumption is
reduced to 1.6W, improving the battery-life of the device. We achieve an
accuracy of up to 98% for four wearing positions of the MaskedFace-Net dataset.
To maintain equivalent classification accuracy for all face structures,
skin-tones, hair types, and mask types, the algorithms are tested for their
ability to generalize the relevant features over all subjects using the
Grad-CAM approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fasfous_N/0/1/0/all/0/1"&gt;Nael Fasfous&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vemparala_M/0/1/0/all/0/1"&gt;Manoj-Rohit Vemparala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frickenstein_A/0/1/0/all/0/1"&gt;Alexander Frickenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frickenstein_L/0/1/0/all/0/1"&gt;Lukas Frickenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stechele_W/0/1/0/all/0/1"&gt;Walter Stechele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Learning Guarantees for Compressive Clustering and Compressive Mixture Modeling. (arXiv:2004.08085v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.08085</id>
        <link href="http://arxiv.org/abs/2004.08085"/>
        <updated>2021-06-18T02:06:37.639Z</updated>
        <summary type="html"><![CDATA[We provide statistical learning guarantees for two unsupervised learning
tasks in the context of compressive statistical learning, a general framework
for resource-efficient large-scale learning that we introduced in a companion
paper. The principle of compressive statistical learning is to compress a
training collection, in one pass, into a low-dimensional sketch (a vector of
random empirical generalized moments) that captures the information relevant to
the considered learning task. We explicit random feature functions which
empirical averages preserve the needed information for compressive clustering
and compressive Gaussian mixture modeling with fixed known variance, and
establish sufficient sketch sizes given the problem dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gribonval_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Gribonval&lt;/a&gt; (PANAMA, DANTE), &lt;a href="http://arxiv.org/find/cs/1/au:+Blanchard_G/0/1/0/all/0/1"&gt;Gilles Blanchard&lt;/a&gt; (LMO), &lt;a href="http://arxiv.org/find/cs/1/au:+Keriven_N/0/1/0/all/0/1"&gt;Nicolas Keriven&lt;/a&gt; (GIPSA-GAIA), &lt;a href="http://arxiv.org/find/cs/1/au:+Traonmilin_Y/0/1/0/all/0/1"&gt;Yann Traonmilin&lt;/a&gt; (IMB)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Label Learning from Single Positive Labels. (arXiv:2106.09708v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09708</id>
        <link href="http://arxiv.org/abs/2106.09708"/>
        <updated>2021-06-18T02:06:37.631Z</updated>
        <summary type="html"><![CDATA[Predicting all applicable labels for a given image is known as multi-label
classification. Compared to the standard multi-class case (where each image has
only one label), it is considerably more challenging to annotate training data
for multi-label classification. When the number of potential labels is large,
human annotators find it difficult to mention all applicable labels for each
training image. Furthermore, in some settings detection is intrinsically
difficult e.g. finding small object instances in high resolution images. As a
result, multi-label training data is often plagued by false negatives. We
consider the hardest version of this problem, where annotators provide only one
relevant label for each image. As a result, training sets will have only one
positive label per image and no confirmed negatives. We explore this special
case of learning from missing labels across four different multi-label image
classification datasets for both linear classifiers and end-to-end fine-tuned
deep networks. We extend existing multi-label losses to this setting and
propose novel variants that constrain the number of expected positive labels
during training. Surprisingly, we show that in some cases it is possible to
approach the performance of fully labeled classifiers despite training with
significantly fewer confirmed labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cole_E/0/1/0/all/0/1"&gt;Elijah Cole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1"&gt;Oisin Mac Aodha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorieul_T/0/1/0/all/0/1"&gt;Titouan Lorieul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1"&gt;Pietro Perona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1"&gt;Dan Morris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1"&gt;Nebojsa Jojic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AN-GCN: An Anonymous Graph Convolutional Network Defense Against Edge-Perturbing Attack. (arXiv:2005.03482v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.03482</id>
        <link href="http://arxiv.org/abs/2005.03482"/>
        <updated>2021-06-18T02:06:37.613Z</updated>
        <summary type="html"><![CDATA[Recent studies have revealed the vulnerability of graph convolutional
networks (GCNs) to edge-perturbing attacks, such as maliciously inserting or
deleting graph edges. However, a theoretical proof of such vulnerability
remains a big challenge, and effective defense schemes are still open issues.
In this paper, we first generalize the formulation of edge-perturbing attacks
and strictly prove the vulnerability of GCNs to such attacks in node
classification tasks. Following this, an anonymous graph convolutional network,
named AN-GCN, is proposed to counter against edge-perturbing attacks.
Specifically, we present a node localization theorem to demonstrate how the GCN
locates nodes during its training phase. In addition, we design a staggered
Gaussian noise based node position generator, and devise a spectral graph
convolution based discriminator in detecting the generated node positions.
Further, we give the optimization of the above generator and discriminator.
AN-GCN can classify nodes without taking their position as input. It is
demonstrated that the AN-GCN is secure against edge-perturbing attacks in node
classification tasks, as AN-GCN classifies nodes without the edge information
and thus makes it impossible for attackers to perturb edges anymore. Extensive
evaluations demonstrated the effectiveness of the general edge-perturbing
attack model in manipulating the classification results of the target nodes.
More importantly, the proposed AN-GCN can achieve 82.7% in node classification
accuracy without the edge-reading permission, which outperforms the
state-of-the-art GCN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Ao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Beibei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+wang_R/0/1/0/all/0/1"&gt;Rui wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TacticZero: Learning to Prove Theorems from Scratch with Deep Reinforcement Learning. (arXiv:2102.09756v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09756</id>
        <link href="http://arxiv.org/abs/2102.09756"/>
        <updated>2021-06-18T02:06:37.606Z</updated>
        <summary type="html"><![CDATA[We propose a novel approach to interactive theorem-proving (ITP) using deep
reinforcement learning. The proposed framework is able to learn proof search
strategies as well as tactic and arguments prediction in an end-to-end manner.
We formulate the process of ITP as a Markov decision process (MDP) in which
each state represents a set of potential derivation paths. This structure
allows us to introduce a novel backtracking mechanism which enables the agent
to efficiently discard (predicted) dead-end derivations and restart from
promising alternatives. We implement the framework in the HOL4 theorem prover.
Experimental results show that the framework outperforms existing automated
theorem provers (i.e., hammers) available in HOL4 when evaluated on unseen
problems. We further elaborate the role of key components of the framework
using ablation studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Minchao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norrish_M/0/1/0/all/0/1"&gt;Michael Norrish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1"&gt;Christian Walder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dezfouli_A/0/1/0/all/0/1"&gt;Amir Dezfouli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Short Note of PAGE: Optimal Convergence Rates for Nonconvex Optimization. (arXiv:2106.09663v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.09663</id>
        <link href="http://arxiv.org/abs/2106.09663"/>
        <updated>2021-06-18T02:06:37.600Z</updated>
        <summary type="html"><![CDATA[In this note, we first recall the nonconvex problem setting and introduce the
optimal PAGE algorithm (Li et al., ICML'21). Then we provide a simple and clean
convergence analysis of PAGE for achieving optimal convergence rates. Moreover,
PAGE and its analysis can be easily adopted and generalized to other works. We
hope that this note provides the insights and is helpful for future works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhize Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributional Robust Batch Contextual Bandits. (arXiv:2006.05630v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05630</id>
        <link href="http://arxiv.org/abs/2006.05630"/>
        <updated>2021-06-18T02:06:37.594Z</updated>
        <summary type="html"><![CDATA[Policy learning using historical observational data is an important problem
that has found widespread applications. Examples include selecting offers,
prices, advertisements to send to customers, as well as selecting which
medication to prescribe to a patient. However, existing literature rests on the
crucial assumption that the future environment where the learned policy will be
deployed is the same as the past environment that has generated the data--an
assumption that is often false or too coarse an approximation. In this paper,
we lift this assumption and aim to learn a distributional robust policy with
incomplete (bandit) observational data. We propose a novel learning algorithm
that is able to learn a robust policy to adversarial perturbations and unknown
covariate shifts. We first present a policy evaluation procedure in the
ambiguous environment and then give a performance guarantee based on the theory
of uniform convergence. Additionally, we also give a heuristic algorithm to
solve the distributional robust policy learning problems efficiently. Finally,
we demonstrate the robustness of our methods in the synthetic and real-world
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1"&gt;Nian Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhengyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanchet_J/0/1/0/all/0/1"&gt;Jose Blanchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RAR-U-Net: a Residual Encoder to Attention Decoder by Residual Connections Framework for Spine Segmentation under Noisy Labels. (arXiv:2009.12873v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12873</id>
        <link href="http://arxiv.org/abs/2009.12873"/>
        <updated>2021-06-18T02:06:37.586Z</updated>
        <summary type="html"><![CDATA[Segmentation algorithms for medical images are widely studied for various
clinical and research purposes. In this paper, we propose a new and efficient
method for medical image segmentation under noisy labels. The method operates
under a deep learning paradigm, incorporating four novel contributions.
Firstly, a residual interconnection is explored in different scale encoders to
transfer gradient information efficiently. Secondly, four copy-and-crop
connections are replaced by residual-block-based concatenation to alleviate the
disparity between encoders and decoders. Thirdly, convolutional attention
modules for feature refinement are studied on all scale decoders. Finally, an
adaptive denoising learning strategy (ADL) is introduced into the training
process to avoid too much influence from the noisy labels. Experimental results
are illustrated on a publicly available benchmark database of spine CTs. Our
proposed method achieves competitive performance against other state-of-the-art
methods over a variety of different evaluation measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Voiculescu_I/0/1/0/all/0/1"&gt;Irina Voiculescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks. (arXiv:2004.05937v7 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.05937</id>
        <link href="http://arxiv.org/abs/2004.05937"/>
        <updated>2021-06-18T02:06:37.569Z</updated>
        <summary type="html"><![CDATA[Deep neural models in recent years have been successful in almost every
field, including extremely complex problem statements. However, these models
are huge in size, with millions (and even billions) of parameters, thus
demanding more heavy computation power and failing to be deployed on edge
devices. Besides, the performance boost is highly dependent on redundant
labeled data. To achieve faster speeds and to handle the problems caused by the
lack of data, knowledge distillation (KD) has been proposed to transfer
information learned from one model to another. KD is often characterized by the
so-called `Student-Teacher' (S-T) learning framework and has been broadly
applied in model compression and knowledge transfer. This paper is about KD and
S-T learning, which are being actively studied in recent years. First, we aim
to provide explanations of what KD is and how/why it works. Then, we provide a
comprehensive survey on the recent progress of KD methods together with S-T
frameworks typically for vision tasks. In general, we consider some fundamental
questions that have been driving this research area and thoroughly generalize
the research progress and technical details. Additionally, we systematically
analyze the research status of KD in vision applications. Finally, we discuss
the potentials and open challenges of existing methods and prospect the future
directions of KD and S-T learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1"&gt;Kuk-Jin Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving adversarial robustness of deep neural networks by using semantic information. (arXiv:2008.07838v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07838</id>
        <link href="http://arxiv.org/abs/2008.07838"/>
        <updated>2021-06-18T02:06:37.562Z</updated>
        <summary type="html"><![CDATA[The vulnerability of deep neural networks (DNNs) to adversarial attack, which
is an attack that can mislead state-of-the-art classifiers into making an
incorrect classification with high confidence by deliberately perturbing the
original inputs, raises concerns about the robustness of DNNs to such attacks.
Adversarial training, which is the main heuristic method for improving
adversarial robustness and the first line of defense against adversarial
attacks, requires many sample-by-sample calculations to increase training size
and is usually insufficiently strong for an entire network. This paper provides
a new perspective on the issue of adversarial robustness, one that shifts the
focus from the network as a whole to the critical part of the region close to
the decision boundary corresponding to a given class. From this perspective, we
propose a method to generate a single but image-agnostic adversarial
perturbation that carries the semantic information implying the directions to
the fragile parts on the decision boundary and causes inputs to be
misclassified as a specified target. We call the adversarial training based on
such perturbations "region adversarial training" (RAT), which resembles
classical adversarial training but is distinguished in that it reinforces the
semantic information missing in the relevant regions. Experimental results on
the MNIST and CIFAR-10 datasets show that this approach greatly improves
adversarial robustness even using a very small dataset from the training data;
moreover, it can defend against FGSM adversarial attacks that have a completely
different pattern from the model seen during retraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lina Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Rui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yawei Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xingshu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xuemei Zeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine-learning enhanced dark soliton detection in Bose-Einstein condensates. (arXiv:2101.05404v2 [cond-mat.quant-gas] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05404</id>
        <link href="http://arxiv.org/abs/2101.05404"/>
        <updated>2021-06-18T02:06:37.555Z</updated>
        <summary type="html"><![CDATA[Most data in cold-atom experiments comes from images, the analysis of which
is limited by our preconceptions of the patterns that could be present in the
data. We focus on the well-defined case of detecting dark solitons -- appearing
as local density depletions in a Bose-Einstein condensate (BEC) -- using a
methodology that is extensible to the general task of pattern recognition in
images of cold atoms. Studying soliton dynamics over a wide range of parameters
requires the analysis of large datasets, making the existing
human-inspection-based methodology a significant bottleneck. Here we describe
an automated classification and positioning system for identifying localized
excitations in atomic BECs utilizing deep convolutional neural networks to
eliminate the need for human image examination. Furthermore, we openly publish
our labeled dataset of dark solitons, the first of its kind, for further
machine learning research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shangjie Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Fritsch_A/0/1/0/all/0/1"&gt;Amilson R. Fritsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Greenberg_C/0/1/0/all/0/1"&gt;Craig Greenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Spielman_I/0/1/0/all/0/1"&gt;I. B. Spielman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zwolak_J/0/1/0/all/0/1"&gt;Justyna P. Zwolak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse bottleneck neural networks for exploratory non-linear visualization of Patch-seq data. (arXiv:2006.10411v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10411</id>
        <link href="http://arxiv.org/abs/2006.10411"/>
        <updated>2021-06-18T02:06:37.548Z</updated>
        <summary type="html"><![CDATA[Patch-seq, a recently developed experimental technique, allows
neuroscientists to obtain transcriptomic and electrophysiological information
from the same neurons. Efficiently analyzing and visualizing such paired
multivariate data in order to extract biologically meaningful interpretations
has, however, remained a challenge. Here, we use sparse deep neural networks
with a two-dimensional bottleneck and group lasso penalty to predict
electrophysiological features from the transcriptomic ones, yielding concise
and biologically interpretable two-dimensional visualizations. In two large
example data sets, this visualization reveals known neural classes and their
marker genes without biological prior knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bernaerts_Y/0/1/0/all/0/1"&gt;Yves Bernaerts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berens_P/0/1/0/all/0/1"&gt;Philipp Berens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobak_D/0/1/0/all/0/1"&gt;Dmitry Kobak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient reconstruction of depth three circuits with top fan-in two. (arXiv:2103.07445v2 [cs.CC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07445</id>
        <link href="http://arxiv.org/abs/2103.07445"/>
        <updated>2021-06-18T02:06:37.531Z</updated>
        <summary type="html"><![CDATA[We develop efficient randomized algorithms to solve the black-box
reconstruction problem for polynomials over finite fields, computable by depth
three arithmetic circuits with alternating addition/multiplication gates, such
that output gate is an addition gate with in-degree two. These circuits compute
polynomials of form $G\times(T_1 + T_2)$, where $G,T_1,T_2$ are product of
affine forms, and polynomials $T_1,T_2$ have no common factors. Rank of such a
circuit is defined as dimension of vector space spanned by all affine factors
of $T_1$ and $T_2$. For any polynomial $f$ computable by such a circuit,
$rank(f)$ is defined to be the minimum rank of any such circuit computing it.
Our work develops randomized reconstruction algorithms which take as input
black-box access to a polynomial $f$ (over finite field $\mathbb{F}$),
computable by such a circuit. Here are the results.

1 [Low rank]: When $5\leq rank(f) = O(\log^3 d)$, it runs in time
$(nd^{\log^3d}\log |\mathbb{F}|)^{O(1)}$, and, with high probability, outputs a
depth three circuit computing $f$, with top addition gate having in-degree
$\leq d^{rank(f)}$.

2 [High rank]: When $rank(f) = \Omega(\log^3 d)$, it runs in time $(nd\log
|\mathbb{F}|)^{O(1)}$, and, with high probability, outputs a depth three
circuit computing $f$, with top addition gate having in-degree two.

Ours is the first blackbox reconstruction algorithm for this circuit class,
that runs in time polynomial in $\log |\mathbb{F}|$. This problem has been
mentioned as an open problem in [GKL12] (STOC 2012)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_G/0/1/0/all/0/1"&gt;Gaurav Sinha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution Free Uncertainty for the Minimum Norm Solution of Over-parameterized Linear Regression. (arXiv:2102.07181v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07181</id>
        <link href="http://arxiv.org/abs/2102.07181"/>
        <updated>2021-06-18T02:06:37.215Z</updated>
        <summary type="html"><![CDATA[A fundamental principle of learning theory is that there is a trade-off
between the complexity of a prediction rule and its ability to generalize.
Modern machine learning models do not obey this paradigm: They produce an
accurate prediction even with a perfect fit to the training set. We investigate
over-parameterized linear regression models focusing on the minimum norm
solution: This is the solution with the minimal norm that attains a perfect fit
to the training set. We utilize the recently proposed predictive normalized
maximum likelihood (pNML) learner which is the min-max regret solution for the
distribution-free setting. We derive an upper bound of this min-max regret
which is associated with the prediction uncertainty. We show that if the test
sample lies mostly in a subspace spanned by the eigenvectors associated with
the large eigenvalues of the empirical correlation matrix of the training data,
the model generalizes despite its over-parameterized nature. We demonstrate the
use of the pNML regret as a point-wise learnability measure on synthetic data
and successfully observe the double-decent phenomenon of the over-parameterized
models on UCI datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bibas_K/0/1/0/all/0/1"&gt;Koby Bibas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feder_M/0/1/0/all/0/1"&gt;Meir Feder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. (arXiv:2106.04156v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04156</id>
        <link href="http://arxiv.org/abs/2106.04156"/>
        <updated>2021-06-18T02:06:37.208Z</updated>
        <summary type="html"><![CDATA[Recent works in self-supervised learning have advanced the state-of-the-art
by relying on the contrastive learning paradigm, which learns representations
by pushing positive pairs, or similar examples from the same class, closer
together while keeping negative pairs far apart. Despite the empirical
successes, theoretical foundations are limited -- prior analyses assume
conditional independence of the positive pairs given the same class label, but
recent empirical applications use heavily correlated positive pairs (i.e., data
augmentations of the same image). Our work analyzes contrastive learning
without assuming conditional independence of positive pairs using a novel
concept of the augmentation graph on data. Edges in this graph connect
augmentations of the same data, and ground-truth classes naturally form
connected sub-graphs. We propose a loss that performs spectral decomposition on
the population augmentation graph and can be succinctly written as a
contrastive learning objective on neural net representations. Minimizing this
objective leads to features with provable accuracy guarantees under linear
probe evaluation. By standard generalization bounds, these accuracy guarantees
also hold when minimizing the training contrastive loss. Empirically, the
features learned by our objective can match or outperform several strong
baselines on benchmark vision datasets. In all, this work provides the first
provable analysis for contrastive learning where guarantees for linear probe
evaluation can apply to realistic empirical settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1"&gt;Jeff Z. HaoChen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Colin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1"&gt;Adrien Gaidon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor Neural Network. (arXiv:2103.08160v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08160</id>
        <link href="http://arxiv.org/abs/2103.08160"/>
        <updated>2021-06-18T02:06:37.194Z</updated>
        <summary type="html"><![CDATA[Few-shot learning (FSL) aims to classify images under low-data regimes, where
the conventional pooled global representation is likely to lose useful local
characteristics. Recent work has achieved promising performances by using deep
descriptors. They generally take all deep descriptors from neural networks into
consideration while ignoring that some of them are useless in classification
due to their limited receptive field, e.g., task-irrelevant descriptors could
be misleading and multiple aggregative descriptors from background clutter
could even overwhelm the object's presence. In this paper, we argue that a
Mutual Nearest Neighbor (MNN) relation should be established to explicitly
select the query descriptors that are most relevant to each task and discard
less relevant ones from aggregative clutters in FSL. Specifically, we propose
Discriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive
experiments demonstrate that our method not only qualitatively selects
task-relevant descriptors but also quantitatively outperforms the existing
state-of-the-arts by a large margin of 1.8~4.9% on fine-grained CUB, a
considerable margin of 1.4~2.2% on both supervised and semi-supervised
miniImagenet, and ~1.4% on challenging tieredimagenet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1"&gt;Tu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jie Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofei He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Source Identification on Networks with Statistical Confidence. (arXiv:2106.04800v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04800</id>
        <link href="http://arxiv.org/abs/2106.04800"/>
        <updated>2021-06-18T02:06:37.188Z</updated>
        <summary type="html"><![CDATA[Diffusion source identification on networks is a problem of fundamental
importance in a broad class of applications, including rumor controlling and
virus identification. Though this problem has received significant recent
attention, most studies have focused only on very restrictive settings and lack
theoretical guarantees for more realistic networks. We introduce a statistical
framework for the study of diffusion source identification and develop a
confidence set inference approach inspired by hypothesis testing. Our method
efficiently produces a small subset of nodes, which provably covers the source
node with any pre-specified confidence level without restrictive assumptions on
network structures. Moreover, we propose multiple Monte Carlo strategies for
the inference procedure based on network topology and the probabilistic
properties that significantly improve the scalability. To our knowledge, this
is the first diffusion source identification method with a practically useful
theoretical guarantee on general networks. We demonstrate our approach via
extensive synthetic experiments on well-known random network models and a
mobility network between cities concerning the COVID-19 spreading.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dawkins_Q/0/1/0/all/0/1"&gt;Quinlan Dawkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haifeng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Data Usage via Differentiable Rewards. (arXiv:1911.10088v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.10088</id>
        <link href="http://arxiv.org/abs/1911.10088"/>
        <updated>2021-06-18T02:06:37.159Z</updated>
        <summary type="html"><![CDATA[To acquire a new skill, humans learn better and faster if a tutor, based on
their current knowledge level, informs them of how much attention they should
pay to particular content or practice problems. Similarly, a machine learning
model could potentially be trained better with a scorer that "adapts" to its
current learning state and estimates the importance of each training data
instance. Training such an adaptive scorer efficiently is a challenging
problem; in order to precisely quantify the effect of a data instance at a
given time during the training, it is typically necessary to first complete the
entire training process. To efficiently optimize data usage, we propose a
reinforcement learning approach called Differentiable Data Selection (DDS). In
DDS, we formulate a scorer network as a learnable function of the training
data, which can be efficiently updated along with the main model being trained.
Specifically, DDS updates the scorer with an intuitive reward signal: it should
up-weigh the data that has a similar gradient with a dev set upon which we
would finally like to perform well. Without significant computing overhead, DDS
delivers strong and consistent improvements over several strong baselines on
two very different tasks of machine translation and image classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michel_P/0/1/0/all/0/1"&gt;Paul Michel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1"&gt;Antonios Anastasopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carbonell_J/0/1/0/all/0/1"&gt;Jaime Carbonell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Independent Asymmetric Embedding for Cascade Prediction on Social Networks. (arXiv:2105.08291v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08291</id>
        <link href="http://arxiv.org/abs/2105.08291"/>
        <updated>2021-06-18T02:06:37.142Z</updated>
        <summary type="html"><![CDATA[The prediction for information diffusion on social networks has great
practical significance in marketing and public opinion control. Cascade
prediction aims to predict the individuals who will potentially repost the
message on the social network. One kind of methods either exploit
demographical, structural, and temporal features for prediction, or explicitly
rely on particular information diffusion models. The other kind of models are
fully data-driven and do not require a global network structure. Thus massive
diffusion prediction models based on network embedding are proposed. These
models embed the users into the latent space using their cascade information,
but are lack of consideration for the intervene among users when embedding. In
this paper, we propose an independent asymmetric embedding method to learn
social embedding for cascade prediction. Different from existing methods, our
method embeds each individual into one latent influence space and multiple
latent susceptibility spaces. Furthermore, our method captures the
co-occurrence regulation of user combination in cascades to improve the
calculating effectiveness. The results of extensive experiments conducted on
real-world datasets verify both the predictive accuracy and cost-effectiveness
of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wenjin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaomeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_T/0/1/0/all/0/1"&gt;Tao Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Detection of Alzheimer's Disease from Speech and Text. (arXiv:2012.00096v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00096</id>
        <link href="http://arxiv.org/abs/2012.00096"/>
        <updated>2021-06-18T02:06:37.135Z</updated>
        <summary type="html"><![CDATA[Reliable detection of the prodromal stages of Alzheimer's disease (AD)
remains difficult even today because, unlike other neurocognitive impairments,
there is no definitive diagnosis of AD in vivo. In this context, existing
research has shown that patients often develop language impairment even in mild
AD conditions. We propose a multimodal deep learning method that utilizes
speech and the corresponding transcript simultaneously to detect AD. For audio
signals, the proposed audio-based network, a convolutional neural network (CNN)
based model, predicts the diagnosis for multiple speech segments, which are
combined for the final prediction. Similarly, we use contextual embedding
extracted from BERT concatenated with a CNN-generated embedding for classifying
the transcript. The individual predictions of the two models are then combined
to make the final classification. We also perform experiments to analyze the
model performance when Automated Speech Recognition (ASR) system generated
transcripts are used instead of manual transcription in the text-based model.
The proposed method achieves 85.3% 10-fold cross-validation accuracy when
trained and evaluated on the Dementiabank Pitt corpus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Amish Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1"&gt;Sourav Sahoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datar_A/0/1/0/all/0/1"&gt;Arnhav Datar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadiwala_J/0/1/0/all/0/1"&gt;Juned Kadiwala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1"&gt;Hrithwik Shalu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1"&gt;Jimson Mathew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Low-Rank Regularization with Damping Sequences to Restrict Lazy Weights in Deep Networks. (arXiv:2106.09677v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09677</id>
        <link href="http://arxiv.org/abs/2106.09677"/>
        <updated>2021-06-18T02:06:37.127Z</updated>
        <summary type="html"><![CDATA[Overfitting is one of the critical problems in deep neural networks. Many
regularization schemes try to prevent overfitting blindly. However, they
decrease the convergence speed of training algorithms. Adaptive regularization
schemes can solve overfitting more intelligently. They usually do not affect
the entire network weights. This paper detects a subset of the weighting layers
that cause overfitting. The overfitting recognizes by matrix and tensor
condition numbers. An adaptive regularization scheme entitled Adaptive Low-Rank
(ALR) is proposed that converges a subset of the weighting layers to their
Low-Rank Factorization (LRF). It happens by minimizing a new Tikhonov-based
loss function. ALR also encourages lazy weights to contribute to the
regularization when epochs grow up. It uses a damping sequence to increment
layer selection likelihood in the last generations. Thus before falling the
training accuracy, ALR reduces the lazy weights and regularizes the network
substantially. The experimental results show that ALR regularizes the deep
networks well with high training speed and low resource usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bejani_M/0/1/0/all/0/1"&gt;Mohammad Mahdi Bejani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghatee_M/0/1/0/all/0/1"&gt;Mehdi Ghatee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Dimension Reduction for Supervised Representation Learning. (arXiv:2006.05865v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05865</id>
        <link href="http://arxiv.org/abs/2006.05865"/>
        <updated>2021-06-18T02:06:37.121Z</updated>
        <summary type="html"><![CDATA[The goal of supervised representation learning is to construct effective data
representations for prediction. Among all the characteristics of an ideal
nonparametric representation of high-dimensional complex data, sufficiency, low
dimensionality and disentanglement are some of the most essential ones. We
propose a deep dimension reduction approach to learning representations with
these characteristics. The proposed approach is a nonparametric generalization
of the sufficient dimension reduction method. We formulate the ideal
representation learning task as that of finding a nonparametric representation
that minimizes an objective function characterizing conditional independence
and promoting disentanglement at the population level. We then estimate the
target representation at the sample level nonparametrically using deep neural
networks. We show that the estimated deep nonparametric representation is
consistent in the sense that its excess risk converges to zero. Our extensive
numerical experiments using simulated and real benchmark data demonstrate that
the proposed methods have better performance than several existing dimension
reduction methods and the standard deep learning models in the context of
classification and regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jian Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yuling Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1"&gt;Xu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scrambled Translation Problem: A Problem of Denoising UNMT. (arXiv:1911.01212v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.01212</id>
        <link href="http://arxiv.org/abs/1911.01212"/>
        <updated>2021-06-18T02:06:37.114Z</updated>
        <summary type="html"><![CDATA[In this paper, we identify an interesting kind of error in the output of
Unsupervised Neural Machine Translation (UNMT) systems like
\textit{Undreamt}(footnote). We refer to this error type as \textit{Scrambled
Translation problem}. We observe that UNMT models which use \textit{word
shuffle} noise (as in case of Undreamt) can generate correct words, but fail to
stitch them together to form phrases. As a result, words of the translated
sentence look \textit{scrambled}, resulting in decreased BLEU. We hypothesise
that the reason behind \textit{scrambled translation problem} is 'shuffling
noise' which is introduced in every input sentence as a denoising strategy. To
test our hypothesis, we experiment by retraining UNMT models with a simple
\textit{retraining} strategy. We stop the training of the Denoising UNMT model
after a pre-decided number of iterations and resume the training for the
remaining iterations -- which number is also pre-decided -- using original
sentence as input without adding any noise. Our proposed solution achieves
significant performance improvement UNMT models that train conventionally. We
demonstrate these performance gains on four language pairs, \textit{viz.},
English-French, English-German, English-Spanish, Hindi-Punjabi. Our qualitative
and quantitative analysis shows that the retraining strategy helps achieve
better alignment as observed by attention heatmap and better phrasal
translation, leading to statistically significant improvement in BLEU scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1"&gt;Tamali Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1"&gt;Rudra Murthy V&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1"&gt;Pushpak Bhattacharyya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BABEL: Bodies, Action and Behavior with English Labels. (arXiv:2106.09696v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09696</id>
        <link href="http://arxiv.org/abs/2106.09696"/>
        <updated>2021-06-18T02:06:37.094Z</updated>
        <summary type="html"><![CDATA[Understanding the semantics of human movement -- the what, how and why of the
movement -- is an important problem that requires datasets of human actions
with semantic labels. Existing datasets take one of two approaches. Large-scale
video datasets contain many action labels but do not contain ground-truth 3D
human motion. Alternatively, motion-capture (mocap) datasets have precise body
motions but are limited to a small number of actions. To address this, we
present BABEL, a large dataset with language labels describing the actions
being performed in mocap sequences. BABEL consists of action labels for about
43 hours of mocap sequences from AMASS. Action labels are at two levels of
abstraction -- sequence labels describe the overall action in the sequence, and
frame labels describe all actions in every frame of the sequence. Each frame
label is precisely aligned with the duration of the corresponding action in the
mocap sequence, and multiple actions can overlap. There are over 28k sequence
labels, and 63k frame labels in BABEL, which belong to over 250 unique action
categories. Labels from BABEL can be leveraged for tasks like action
recognition, temporal action localization, motion synthesis, etc. To
demonstrate the value of BABEL as a benchmark, we evaluate the performance of
models on 3D action recognition. We demonstrate that BABEL poses interesting
learning challenges that are applicable to real-world scenarios, and can serve
as a useful benchmark of progress in 3D action recognition. The dataset,
baseline method, and evaluation code is made available, and supported for
academic research purposes at https://babel.is.tue.mpg.de/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Punnakkal_A/0/1/0/all/0/1"&gt;Abhinanda R. Punnakkal&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1"&gt;Arjun Chandrasekaran&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1"&gt;Nikos Athanasiou&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Quiros_Ramirez_A/0/1/0/all/0/1"&gt;Alejandra Quiros-Ramirez&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1"&gt;Michael J. Black&lt;/a&gt; (1) ((1) Max Planck Institute for Intelligent Systems, (2) Universitat Konstanz)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral goodness-of-fit tests for complete and partial network data. (arXiv:2106.09702v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.09702</id>
        <link href="http://arxiv.org/abs/2106.09702"/>
        <updated>2021-06-18T02:06:37.086Z</updated>
        <summary type="html"><![CDATA[Networks describe the, often complex, relationships between individual
actors. In this work, we address the question of how to determine whether a
parametric model, such as a stochastic block model or latent space model, fits
a dataset well and will extrapolate to similar data. We use recent results in
random matrix theory to derive a general goodness-of-fit test for dyadic data.
We show that our method, when applied to a specific model of interest, provides
an straightforward, computationally fast way of selecting parameters in a
number of commonly used network models. For example, we show how to select the
dimension of the latent space in latent space models. Unlike other network
goodness-of-fit methods, our general approach does not require simulating from
a candidate parametric model, which can be cumbersome with large graphs, and
eliminates the need to choose a particular set of statistics on the graph for
comparison. It also allows us to perform goodness-of-fit tests on partial
network data, such as Aggregated Relational Data. We show with simulations that
our method performs well in many situations of interest. We analyze several
empirically relevant networks and show that our method leads to improved
community detection algorithms. R code to implement our method is available on
Github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lubold_S/0/1/0/all/0/1"&gt;Shane Lubold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bolun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+McCormick_T/0/1/0/all/0/1"&gt;Tyler H. McCormick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Multimodal Domino: in Search of Biomarkers for Alzheimer's Disease. (arXiv:2012.13623v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13623</id>
        <link href="http://arxiv.org/abs/2012.13623"/>
        <updated>2021-06-18T02:06:37.079Z</updated>
        <summary type="html"><![CDATA[Sensory input from multiple sources is crucial for robust and coherent human
perception. Different sources contribute complementary explanatory factors.
Similarly, research studies often collect multimodal imaging data, each of
which can provide shared and unique information. This observation motivated the
design of powerful multimodal self-supervised representation-learning
algorithms. In this paper, we unify recent work on multimodal self-supervised
learning under a single framework. Observing that most self-supervised methods
optimize similarity metrics between a set of model components, we propose a
taxonomy of all reasonable ways to organize this process. We first evaluate
models on toy multimodal MNIST datasets and then apply them to a multimodal
neuroimaging dataset with Alzheimer's disease patients. We find that (1)
multimodal contrastive learning has significant benefits over its unimodal
counterpart, (2) the specific composition of multiple contrastive objectives is
critical to performance on a downstream task, (3) maximization of the
similarity between representations has a regularizing effect on a neural
network, which can sometimes lead to reduced downstream performance but still
reveal multimodal relations. Results show that the proposed approach
outperforms previous self-supervised encoder-decoder methods based on canonical
correlation analysis (CCA) or the mixture-of-experts multimodal variational
autoEncoder (MMVAE) on various datasets with a linear evaluation protocol.
Importantly, we find a promising solution to uncover connections between
modalities through a jointly shared subspace that can help advance work in our
search for neuroimaging biomarkers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fedorov_A/0/1/0/all/0/1"&gt;Alex Fedorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sylvain_T/0/1/0/all/0/1"&gt;Tristan Sylvain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geenjaar_E/0/1/0/all/0/1"&gt;Eloy Geenjaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luck_M/0/1/0/all/0/1"&gt;Margaux Luck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DeRamus_T/0/1/0/all/0/1"&gt;Thomas P. DeRamus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirilin_A/0/1/0/all/0/1"&gt;Alex Kirilin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bleklov_D/0/1/0/all/0/1"&gt;Dmitry Bleklov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1"&gt;Vince D. Calhoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1"&gt;Sergey M. Plis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Class2Simi: A Noise Reduction Perspective on Learning with Noisy Labels. (arXiv:2006.07831v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07831</id>
        <link href="http://arxiv.org/abs/2006.07831"/>
        <updated>2021-06-18T02:06:37.072Z</updated>
        <summary type="html"><![CDATA[Learning with noisy labels has attracted a lot of attention in recent years,
where the mainstream approaches are in pointwise manners. Meanwhile, pairwise
manners have shown great potential in supervised metric learning and
unsupervised contrastive learning. Thus, a natural question is raised: does
learning in a pairwise manner mitigate label noise? To give an affirmative
answer, in this paper, we propose a framework called Class2Simi: it transforms
data points with noisy class labels to data pairs with noisy similarity labels,
where a similarity label denotes whether a pair shares the class label or not.
Through this transformation, the reduction of the noise rate is theoretically
guaranteed, and hence it is in principle easier to handle noisy similarity
labels. Amazingly, DNNs that predict the clean class labels can be trained from
noisy data pairs if they are first pretrained from noisy data points.
Class2Simi is computationally efficient because not only this transformation is
on-the-fly in mini-batches, but also it just changes loss computation on top of
model prediction into a pairwise manner. Its effectiveness is verified by
extensive experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Songhua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1"&gt;Xiaobo Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Mingming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Nannan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haifeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Neural Networks for Stochastic Control Problems with Delay. (arXiv:2101.01385v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01385</id>
        <link href="http://arxiv.org/abs/2101.01385"/>
        <updated>2021-06-18T02:06:37.066Z</updated>
        <summary type="html"><![CDATA[Stochastic control problems with delay are challenging due to the
path-dependent feature of the system and thus its intrinsic high dimensions. In
this paper, we propose and systematically study deep neural networks-based
algorithms to solve stochastic control problems with delay features.
Specifically, we employ neural networks for sequence modeling (\emph{e.g.},
recurrent neural networks such as long short-term memory) to parameterize the
policy and optimize the objective function. The proposed algorithms are tested
on three benchmark examples: a linear-quadratic problem, optimal consumption
with fixed finite delay, and portfolio optimization with complete memory.
Particularly, we notice that the architecture of recurrent neural networks
naturally captures the path-dependent feature with much flexibility and yields
better performance with more efficient and stable training of the network
compared to feedforward networks. The superiority is even evident in the case
of portfolio optimization with complete memory, which features infinite delay.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiequn Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hu_R/0/1/0/all/0/1"&gt;Ruimeng Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Postprocessing Ensemble Streamflow Forecasts. (arXiv:2106.09547v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09547</id>
        <link href="http://arxiv.org/abs/2106.09547"/>
        <updated>2021-06-18T02:06:37.047Z</updated>
        <summary type="html"><![CDATA[Skillful streamflow forecasting informs decisions in various areas of water
policy and management. We integrate dynamical modeling with machine learning to
demonstrate the enhanced quality of streamflow forecasts at short-to
medium-range timescales (1 - 7 days). Dynamical modeling generates ensemble
streamflow forecasts by forcing a hydrological model with numerical weather
prediction model outputs. We employ a Long Short-Term Memory (LSTM) neural
network to correct forecast biases in raw ensemble streamflow forecasts
obtained from dynamical modeling. For forecast verification, we use different
metrics such as skill score and reliability diagram conditioned upon the lead
time, flow threshold, and season. The verification results show that the LSTM
can improve streamflow forecasts relative to climatological, temporal
persistence, deterministic, and raw ensemble forecasts. The LSTM demonstrates
improvement across all lead times, flow thresholds, and seasons. As compared to
the raw ensembles, relative gain in forecast skill from LSTM is generally
higher at medium-range timescales compared to initial lead time; high flows
compared to low-moderate flows; and warm-season compared to the cool ones.
Overall, our results highlight the benefits of LSTM for improving both the
skill and reliability of streamflow forecasts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Sanjib Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghimire_G/0/1/0/all/0/1"&gt;Ganesh Raj Ghimire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddique_R/0/1/0/all/0/1"&gt;Ridwan Siddique&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Analysis of the Emotional Content of Speech in Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit. (arXiv:2106.09539v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.09539</id>
        <link href="http://arxiv.org/abs/2106.09539"/>
        <updated>2021-06-18T02:06:37.040Z</updated>
        <summary type="html"><![CDATA[Researchers have recently started to study how the emotional speech heard by
young infants can affect their developmental outcomes. As a part of this
research, hundreds of hours of daylong recordings from preterm infants' audio
environments were collected from two hospitals in Finland and Estonia in the
context of so-called APPLE study. In order to analyze the emotional content of
speech in such a massive dataset, an automatic speech emotion recognition (SER)
system is required. However, there are no emotion labels or existing indomain
SER systems to be used for this purpose. In this paper, we introduce this
initially unannotated large-scale real-world audio dataset and describe the
development of a functional SER system for the Finnish subset of the data. We
explore the effectiveness of alternative state-of-the-art techniques to deploy
a SER system to a new domain, comparing cross-corpus generalization, WGAN-based
domain adaptation, and active learning in the task. As a result, we show that
the best-performing models are able to achieve a classification performance of
73.4% unweighted average recall (UAR) and 73.2% UAR for a binary classification
for valence and arousal, respectively. The results also show that active
learning achieves the most consistent performance compared to the two
alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Vaaras_E/0/1/0/all/0/1"&gt;Einari Vaaras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ahlqvist_Bjorkroth_S/0/1/0/all/0/1"&gt;Sari Ahlqvist-Bj&amp;#xf6;rkroth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Drossos_K/0/1/0/all/0/1"&gt;Konstantinos Drossos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rasanen_O/0/1/0/all/0/1"&gt;Okko R&amp;#xe4;s&amp;#xe4;nen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Author Clustering and Topic Estimation for Short Texts. (arXiv:2106.09533v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09533</id>
        <link href="http://arxiv.org/abs/2106.09533"/>
        <updated>2021-06-18T02:06:37.033Z</updated>
        <summary type="html"><![CDATA[Analysis of short text, such as social media posts, is extremely difficult
because it relies on observing many document-level word co-occurrence pairs.
Beyond topic distributions, a common downstream task of the modeling is
grouping the authors of these documents for subsequent analyses. Traditional
models estimate the document groupings and identify user clusters with an
independent procedure. We propose a novel model that expands on the Latent
Dirichlet Allocation by modeling strong dependence among the words in the same
document, with user-level topic distributions. We also simultaneously cluster
users, removing the need for post-hoc cluster estimation and improving topic
estimation by shrinking noisy user-level topic distributions towards typical
values. Our method performs as well as -- or better -- than traditional
approaches to problems arising in short text, and we demonstrate its usefulness
on a dataset of tweets from United States Senators, recovering both meaningful
topics and clusters that reflect partisan ideology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tierney_G/0/1/0/all/0/1"&gt;Graham Tierney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bail_C/0/1/0/all/0/1"&gt;Christopher Bail&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Volfovsky_A/0/1/0/all/0/1"&gt;Alexander Volfovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Agent Training beyond Zero-Sum with Correlated Equilibrium Meta-Solvers. (arXiv:2106.09435v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2106.09435</id>
        <link href="http://arxiv.org/abs/2106.09435"/>
        <updated>2021-06-18T02:06:37.027Z</updated>
        <summary type="html"><![CDATA[Two-player, constant-sum games are well studied in the literature, but there
has been limited progress outside of this setting. We propose Joint
Policy-Space Response Oracles (JPSRO), an algorithm for training agents in
n-player, general-sum extensive form games, which provably converges to an
equilibrium. We further suggest correlated equilibria (CE) as promising
meta-solvers, and propose a novel solution concept Maximum Gini Correlated
Equilibrium (MGCE), a principled and computationally efficient family of
solutions for solving the correlated equilibrium selection problem. We conduct
several experiments using CE meta-solvers for JPSRO and demonstrate convergence
on n-player, general-sum games.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marris_L/0/1/0/all/0/1"&gt;Luke Marris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_P/0/1/0/all/0/1"&gt;Paul Muller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lanctot_M/0/1/0/all/0/1"&gt;Marc Lanctot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuyls_K/0/1/0/all/0/1"&gt;Karl Tuyls&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grapael_T/0/1/0/all/0/1"&gt;Thore Grapael&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-head or Single-head? An Empirical Comparison for Transformer Training. (arXiv:2106.09650v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09650</id>
        <link href="http://arxiv.org/abs/2106.09650"/>
        <updated>2021-06-18T02:06:37.020Z</updated>
        <summary type="html"><![CDATA[Multi-head attention plays a crucial role in the recent success of
Transformer models, which leads to consistent performance improvements over
conventional attention in various applications. The popular belief is that this
effectiveness stems from the ability of jointly attending multiple positions.
In this paper, we first demonstrate that jointly attending multiple positions
is not a unique feature of multi-head attention, as multi-layer single-head
attention also attends multiple positions and is more effective. Then, we
suggest the main advantage of the multi-head attention is the training
stability, since it has less number of layers than the single-head attention,
when attending the same number of positions. For example, 24-layer 16-head
Transformer (BERT-large) and 384-layer single-head Transformer has the same
total attention head number and roughly the same model size, while the
multi-head one is significantly shallower. Meanwhile, we show that, with recent
advances in deep learning, we can successfully stabilize the training of the
384-layer Transformer. As the training difficulty is no longer a bottleneck,
substantially deeper single-head Transformer achieves consistent performance
improvements without tuning hyper-parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Normalization of breast MRIs using Cycle-Consistent Generative Adversarial Networks. (arXiv:1912.08061v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.08061</id>
        <link href="http://arxiv.org/abs/1912.08061"/>
        <updated>2021-06-18T02:06:37.000Z</updated>
        <summary type="html"><![CDATA[Dynamic Contrast Enhanced-Magnetic Resonance Imaging (DCE-MRI) is widely used
to complement ultrasound examinations and x-ray mammography during the early
detection and diagnosis of breast cancer. However, images generated by various
MRI scanners (e.g. GE Healthcare vs Siemens) differ both in intensity and noise
distribution, preventing algorithms trained on MRIs from one scanner to
generalize to data from other scanners successfully. We propose a method for
image normalization to solve this problem. MRI normalization is challenging
because it requires both normalizing intensity values and mapping between the
noise distributions of different scanners. We utilize a cycle-consistent
generative adversarial network to learn a bidirectional mapping between MRIs
produced by GE Healthcare and Siemens scanners. This allows us learning the
mapping between two different scanner types without matched data, which is not
commonly available. To ensure the preservation of breast shape and structures
within the breast, we propose two technical innovations. First, we incorporate
a mutual information loss with the CycleGAN architecture to ensure that the
structure of the breast is maintained. Second, we propose a modified
discriminator architecture which utilizes a smaller field-of-view to ensure the
preservation of finer details in the breast tissue. Quantitative and
qualitative evaluations show that the second proposed method was able to
consistently preserve a high level of detail in the breast structure while also
performing the proper intensity normalization and noise mapping. Our results
demonstrate that the proposed model can successfully learn a bidirectional
mapping between MRIs produced by different vendors, potentially enabling
improved accuracy of downstream computational algorithms for diagnosis and
detection of breast cancer. All the data used in this study are publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Modanwal_G/0/1/0/all/0/1"&gt;Gourav Modanwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vellal_A/0/1/0/all/0/1"&gt;Adithya Vellal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mazurowski_M/0/1/0/all/0/1"&gt;Maciej A. Mazurowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. (arXiv:2106.09700v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09700</id>
        <link href="http://arxiv.org/abs/2106.09700"/>
        <updated>2021-06-18T02:06:36.993Z</updated>
        <summary type="html"><![CDATA[Biomedical knowledge graphs (KGs) hold rich information on entities such as
diseases, drugs, and genes. Predicting missing links in these graphs can boost
many important applications, such as drug design and repurposing. Recent work
has shown that general-domain language models (LMs) can serve as "soft" KGs,
and that they can be fine-tuned for the task of KG completion. In this work, we
study scientific LMs for KG completion, exploring whether we can tap into their
latent knowledge to enhance biomedical link prediction. We evaluate several
domain-specific LMs, fine-tuning them on datasets centered on drugs and
diseases that we represent as KGs and enrich with textual entity descriptions.
We integrate the LM-based models with KG embedding models, using a router
method that learns to assign each input example to either type of model and
provides a substantial boost in performance. Finally, we demonstrate the
advantage of LM models in the inductive setting with novel scientific entities.
Our datasets and code are made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nadkarni_R/0/1/0/all/0/1"&gt;Rahul Nadkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1"&gt;David Wadden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1"&gt;Iz Beltagy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1"&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1"&gt;Tom Hope&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XCiT: Cross-Covariance Image Transformers. (arXiv:2106.09681v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09681</id>
        <link href="http://arxiv.org/abs/2106.09681"/>
        <updated>2021-06-18T02:06:36.987Z</updated>
        <summary type="html"><![CDATA[Following their success in natural language processing, transformers have
recently shown much promise for computer vision. The self-attention operation
underlying transformers yields global interactions between all tokens ,i.e.
words or image patches, and enables flexible modelling of image data beyond the
local interactions of convolutions. This flexibility, however, comes with a
quadratic complexity in time and memory, hindering application to long
sequences and high-resolution images. We propose a "transposed" version of
self-attention that operates across feature channels rather than tokens, where
the interactions are based on the cross-covariance matrix between keys and
queries. The resulting cross-covariance attention (XCA) has linear complexity
in the number of tokens, and allows efficient processing of high-resolution
images. Our cross-covariance image transformer (XCiT) is built upon XCA. It
combines the accuracy of conventional transformers with the scalability of
convolutional architectures. We validate the effectiveness and generality of
XCiT by reporting excellent results on multiple vision benchmarks, including
image classification and self-supervised feature learning on ImageNet-1k,
object detection and instance segmentation on COCO, and semantic segmentation
on ADE20k.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+El_Nouby_A/0/1/0/all/0/1"&gt;Alaaeldin El-Nouby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1"&gt;Hugo Touvron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1"&gt;Mathilde Caron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1"&gt;Piotr Bojanowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1"&gt;Matthijs Douze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1"&gt;Armand Joulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1"&gt;Ivan Laptev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1"&gt;Natalia Neverova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1"&gt;Gabriel Synnaeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1"&gt;Jakob Verbeek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1"&gt;Herv&amp;#xe9; Jegou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Through the Lens of Example Difficulty. (arXiv:2106.09647v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09647</id>
        <link href="http://arxiv.org/abs/2106.09647"/>
        <updated>2021-06-18T02:06:36.977Z</updated>
        <summary type="html"><![CDATA[Existing work on understanding deep learning often employs measures that
compress all data-dependent information into a few numbers. In this work, we
adopt a perspective based on the role of individual examples. We introduce a
measure of the computational difficulty of making a prediction for a given
input: the (effective) prediction depth. Our extensive investigation reveals
surprising yet simple relationships between the prediction depth of a given
input and the model's uncertainty, confidence, accuracy and speed of learning
for that data point. We further categorize difficult examples into three
interpretable groups, demonstrate how these groups are processed differently
inside deep models and showcase how this understanding allows us to improve
prediction accuracy. Insights from our study lead to a coherent view of a
number of separately reported phenomena in the literature: early layers
generalize while later layers memorize; early layers converge faster and
networks learn easy data and simple functions first.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baldock_R/0/1/0/all/0/1"&gt;Robert J. N. Baldock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maennel_H/0/1/0/all/0/1"&gt;Hartmut Maennel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1"&gt;Behnam Neyshabur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accuracy, Interpretability, and Differential Privacy via Explainable Boosting. (arXiv:2106.09680v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09680</id>
        <link href="http://arxiv.org/abs/2106.09680"/>
        <updated>2021-06-18T02:06:36.970Z</updated>
        <summary type="html"><![CDATA[We show that adding differential privacy to Explainable Boosting Machines
(EBMs), a recent method for training interpretable ML models, yields
state-of-the-art accuracy while protecting privacy. Our experiments on multiple
classification and regression datasets show that DP-EBM models suffer
surprisingly little accuracy loss even with strong differential privacy
guarantees. In addition to high accuracy, two other benefits of applying DP to
EBMs are: a) trained models provide exact global and local interpretability,
which is often important in settings where differential privacy is needed; and
b) the models can be edited after training without loss of privacy to correct
errors which DP noise may have introduced.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nori_H/0/1/0/all/0/1"&gt;Harsha Nori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caruana_R/0/1/0/all/0/1"&gt;Rich Caruana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1"&gt;Zhiqi Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Judy Hanwen Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1"&gt;Janardhan Kulkarni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Low Can We Go: Trading Memory for Error in Low-Precision Training. (arXiv:2106.09686v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09686</id>
        <link href="http://arxiv.org/abs/2106.09686"/>
        <updated>2021-06-18T02:06:36.945Z</updated>
        <summary type="html"><![CDATA[Low-precision arithmetic trains deep learning models using less energy, less
memory and less time. However, we pay a price for the savings: lower precision
may yield larger round-off error and hence larger prediction error. As
applications proliferate, users must choose which precision to use to train a
new model, and chip manufacturers must decide which precisions to manufacture.
We view these precision choices as a hyperparameter tuning problem, and borrow
ideas from meta-learning to learn the tradeoff between memory and error. In
this paper, we introduce Pareto Estimation to Pick the Perfect Precision
(PEPPP). We use matrix factorization to find non-dominated configurations (the
Pareto frontier) with a limited number of network evaluations. For any given
memory budget, the precision that minimizes error is a point on this frontier.
Practitioners can use the frontier to trade memory for error and choose the
best precision for their goals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chengrun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziyang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chee_J/0/1/0/all/0/1"&gt;Jerry Chee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1"&gt;Madeleine Udell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Reinforcement Learning Approach towards Pendulum Swing-up Problem based on TF-Agents. (arXiv:2106.09556v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.09556</id>
        <link href="http://arxiv.org/abs/2106.09556"/>
        <updated>2021-06-18T02:06:36.939Z</updated>
        <summary type="html"><![CDATA[Adapting the idea of training CartPole with Deep Q-learning agent, we are
able to find a promising result that prevent the pole from falling down. The
capacity of reinforcement learning (RL) to learn from the interaction between
the environment and agent provides an optimal control strategy. In this paper,
we aim to solve the classic pendulum swing-up problem that making the learned
pendulum to be in upright position and balanced. Deep Deterministic Policy
Gradient algorithm is introduced to operate over continuous action domain in
this problem. Salient results of optimal pendulum are proved with increasing
average return, decreasing loss, and live video in the code part.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bi_Y/0/1/0/all/0/1"&gt;Yifei Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Caihui Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving On-Screen Sound Separation for Open Domain Videos with Audio-Visual Self-attention. (arXiv:2106.09669v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.09669</id>
        <link href="http://arxiv.org/abs/2106.09669"/>
        <updated>2021-06-18T02:06:36.931Z</updated>
        <summary type="html"><![CDATA[We introduce a state-of-the-art audio-visual on-screen sound separation
system which is capable of learning to separate sounds and associate them with
on-screen objects by looking at in-the-wild videos. We identify limitations of
previous work on audiovisual on-screen sound separation, including the
simplicity and coarse resolution of spatio-temporal attention, and poor
convergence of the audio separation model. Our proposed model addresses these
issues using cross-modal and self-attention modules that capture audio-visual
dependencies at a finer resolution over time, and by unsupervised pre-training
of audio separation model. These improvements allow the model to generalize to
a much wider set of unseen videos. For evaluation and semi-supervised training,
we collected human annotations of on-screen audio from a large database of
in-the-wild videos (YFCC100M). Our results show marked improvements in
on-screen separation performance, in more general conditions than previous
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1"&gt;Efthymios Tzinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wisdom_S/0/1/0/all/0/1"&gt;Scott Wisdom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1"&gt;Tal Remez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hershey_J/0/1/0/all/0/1"&gt;John R. Hershey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LoRA: Low-Rank Adaptation of Large Language Models. (arXiv:2106.09685v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09685</id>
        <link href="http://arxiv.org/abs/2106.09685"/>
        <updated>2021-06-18T02:06:36.924Z</updated>
        <summary type="html"><![CDATA[The dominant paradigm of natural language processing consists of large-scale
pre-training on general domain data and adaptation to particular tasks or
domains. As we pre-train larger models, conventional fine-tuning, which
retrains all model parameters, becomes less feasible. Using GPT-3 175B as an
example, deploying many independent instances of fine-tuned models, each with
175B parameters, is extremely expensive. We propose Low-Rank Adaptation, or
LoRA, which freezes the pre-trained model weights and injects trainable rank
decomposition matrices into each layer of the Transformer architecture, greatly
reducing the number of trainable parameters for downstream tasks. For GPT-3,
LoRA can reduce the number of trainable parameters by 10,000 times and the
computation hardware requirement by 3 times compared to full fine-tuning. LoRA
performs on-par or better than fine-tuning in model quality on both GPT-3 and
GPT-2, despite having fewer trainable parameters, a higher training throughput,
and no additional inference latency. We also provide an empirical investigation
into rank-deficiency in language model adaptations, which sheds light on the
efficacy of LoRA. We release our implementation in GPT-2 at
https://github.com/microsoft/LoRA .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1"&gt;Edward J. Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yelong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1"&gt;Phillip Wallis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1"&gt;Zeyuan Allen-Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shean Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning. (arXiv:2106.09701v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09701</id>
        <link href="http://arxiv.org/abs/2106.09701"/>
        <updated>2021-06-18T02:06:36.917Z</updated>
        <summary type="html"><![CDATA[Modern computer vision applications suffer from catastrophic forgetting when
incrementally learning new concepts over time. The most successful approaches
to alleviate this forgetting require extensive replay of previously seen data,
which is problematic when memory constraints or data legality concerns exist.
In this work, we consider the high-impact problem of Data-Free
Class-Incremental Learning (DFCIL), where an incremental learning agent must
learn new concepts over time without storing generators or training data from
past tasks. One approach for DFCIL is to replay synthetic images produced by
inverting a frozen copy of the learner's classification model, but we show this
approach fails for common class-incremental benchmarks when using standard
distillation strategies. We diagnose the cause of this failure and propose a
novel incremental distillation strategy for DFCIL, contributing a modified
cross-entropy training and importance-weighted feature distillation, and show
that our method results in up to a 25.1% increase in final task accuracy
(absolute difference) compared to SOTA DFCIL methods for common
class-incremental benchmarks. Our method even outperforms several standard
replay based methods which store a coreset of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1"&gt;James Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1"&gt;Yen-Chang Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balloch_J/0/1/0/all/0/1"&gt;Jonathan Balloch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yilin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hongxia Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1"&gt;Zsolt Kira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Visual Robustness by Causal Intervention. (arXiv:2106.09534v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09534</id>
        <link href="http://arxiv.org/abs/2106.09534"/>
        <updated>2021-06-18T02:06:36.901Z</updated>
        <summary type="html"><![CDATA[Adversarial training is the de facto most promising defense against
adversarial examples. Yet, its passive nature inevitably prevents it from being
immune to unknown attackers. To achieve a proactive defense, we need a more
fundamental understanding of adversarial examples, beyond the popular bounded
threat model. In this paper, we provide a causal viewpoint of adversarial
vulnerability: the cause is the confounder ubiquitously existing in learning,
where attackers are precisely exploiting the confounding effect. Therefore, a
fundamental solution for adversarial robustness is causal intervention. As the
confounder is unobserved in general, we propose to use the instrumental
variable that achieves intervention without the need for confounder
observation. We term our robust training method as Causal intervention by
instrumental Variable (CiiV). It has a differentiable retinotopic sampling
layer and a consistency loss, which is stable and guaranteed not to suffer from
gradient obfuscation. Extensive experiments on a wide spectrum of attackers and
settings applied in MNIST, CIFAR-10, and mini-ImageNet datasets empirically
demonstrate that CiiV is robust to adaptive attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Kaihua Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1"&gt;Mingyuan Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanwang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Calibration: Meta-Learning of Model Calibration Using Differentiable Expected Calibration Error. (arXiv:2106.09613v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09613</id>
        <link href="http://arxiv.org/abs/2106.09613"/>
        <updated>2021-06-18T02:06:36.895Z</updated>
        <summary type="html"><![CDATA[Calibration of neural networks is a topical problem that is becoming
increasingly important for real-world use of neural networks. The problem is
especially noticeable when using modern neural networks, for which there is
significant difference between the model confidence and the confidence it
should have. Various strategies have been successfully proposed, yet there is
more space for improvements. We propose a novel approach that introduces a
differentiable metric for expected calibration error and successfully uses it
as an objective for meta-learning, achieving competitive results with
state-of-the-art approaches. Our approach presents a new direction of using
meta-learning to directly optimize model calibration, which we believe will
inspire further work in this promising and new direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1"&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1"&gt;Timothy Hospedales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design and Analysis of Robust Deep Learning Models for Stock Price Prediction. (arXiv:2106.09664v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2106.09664</id>
        <link href="http://arxiv.org/abs/2106.09664"/>
        <updated>2021-06-18T02:06:36.889Z</updated>
        <summary type="html"><![CDATA[Building predictive models for robust and accurate prediction of stock prices
and stock price movement is a challenging research problem to solve. The
well-known efficient market hypothesis believes in the impossibility of
accurate prediction of future stock prices in an efficient stock market as the
stock prices are assumed to be purely stochastic. However, numerous works
proposed by researchers have demonstrated that it is possible to predict future
stock prices with a high level of precision using sophisticated algorithms,
model architectures, and the selection of appropriate variables in the models.
This chapter proposes a collection of predictive regression models built on
deep learning architecture for robust and precise prediction of the future
prices of a stock listed in the diversified sectors in the National Stock
Exchange (NSE) of India. The Metastock tool is used to download the historical
stock prices over a period of two years (2013- 2014) at 5 minutes intervals.
While the records for the first year are used to train the models, the testing
is carried out using the remaining records. The design approaches of all the
models and their performance results are presented in detail. The models are
also compared based on their execution time and accuracy of prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Sen_J/0/1/0/all/0/1"&gt;Jaydip Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Mehtab_S/0/1/0/all/0/1"&gt;Sidra Mehtab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hi-Phy: A Benchmark for Hierarchical Physical Reasoning. (arXiv:2106.09692v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.09692</id>
        <link href="http://arxiv.org/abs/2106.09692"/>
        <updated>2021-06-18T02:06:36.882Z</updated>
        <summary type="html"><![CDATA[Reasoning about the behaviour of physical objects is a key capability of
agents operating in physical worlds. Humans are very experienced in physical
reasoning while it remains a major challenge for AI. To facilitate research
addressing this problem, several benchmarks have been proposed recently.
However, these benchmarks do not enable us to measure an agent's granular
physical reasoning capabilities when solving a complex reasoning task. In this
paper, we propose a new benchmark for physical reasoning that allows us to test
individual physical reasoning capabilities. Inspired by how humans acquire
these capabilities, we propose a general hierarchy of physical reasoning
capabilities with increasing complexity. Our benchmark tests capabilities
according to this hierarchy through generated physical reasoning tasks in the
video game Angry Birds. This benchmark enables us to conduct a comprehensive
agent evaluation by measuring the agent's granular physical reasoning
capabilities. We conduct an evaluation with human players, learning agents, and
heuristic agents and determine their capabilities. Our evaluation shows that
learning agents, with good local generalization ability, still struggle to
learn the underlying physical reasoning capabilities and perform worse than
current state-of-the-art heuristic agents and humans. We believe that this
benchmark will encourage researchers to develop intelligent agents with
advanced, human-like physical reasoning capabilities. URL:
https://github.com/Cheng-Xue/Hi-Phy]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1"&gt;Cheng Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_V/0/1/0/all/0/1"&gt;Vimukthini Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gamage_C/0/1/0/all/0/1"&gt;Chathura Gamage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Peng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renz_J/0/1/0/all/0/1"&gt;Jochen Renz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Knowledge Graph-based World Models of Textual Environments. (arXiv:2106.09608v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09608</id>
        <link href="http://arxiv.org/abs/2106.09608"/>
        <updated>2021-06-18T02:06:36.874Z</updated>
        <summary type="html"><![CDATA[World models improve a learning agent's ability to efficiently operate in
interactive and situated environments. This work focuses on the task of
building world models of text-based game environments. Text-based games, or
interactive narratives, are reinforcement learning environments in which agents
perceive and interact with the world using textual natural language. These
environments contain long, multi-step puzzles or quests woven through a world
that is filled with hundreds of characters, locations, and objects. Our world
model learns to simultaneously: (1) predict changes in the world caused by an
agent's actions when representing the world as a knowledge graph; and (2)
generate the set of contextually relevant natural language actions required to
operate in the world. We frame this task as a Set of Sequences generation
problem by exploiting the inherent structure of knowledge graphs and actions
and introduce both a transformer-based multi-task architecture and a loss
function to train it. A zero-shot ablation study on never-before-seen textual
worlds shows that our methodology significantly outperforms existing textual
world modeling techniques as well as the importance of each of our
contributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1"&gt;Prithviraj Ammanabrolu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1"&gt;Mark O. Riedl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Anytime Learning at Macroscale. (arXiv:2106.09563v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09563</id>
        <link href="http://arxiv.org/abs/2106.09563"/>
        <updated>2021-06-18T02:06:36.856Z</updated>
        <summary type="html"><![CDATA[Classical machine learning frameworks assume access to a possibly large
dataset in order to train a predictive model. In many practical applications
however, data does not arrive all at once, but in batches over time. This
creates a natural trade-off between accuracy of a model and time to obtain such
a model. A greedy predictor could produce non-trivial predictions by
immediately training on batches as soon as these become available but, it may
also make sub-optimal use of future data. On the other hand, a tardy predictor
could wait for a long time to aggregate several batches into a larger dataset,
but ultimately deliver a much better performance. In this work, we consider
such a streaming learning setting, which we dub {\em anytime learning at
macroscale} (ALMA). It is an instance of anytime learning applied not at the
level of a single chunk of data, but at the level of the entire sequence of
large batches. We first formalize this learning setting, we then introduce
metrics to assess how well learners perform on the given task for a given
memory and compute budget, and finally we test several baseline approaches on
standard benchmarks repurposed for anytime learning at macroscale. The general
finding is that bigger models always generalize better. In particular, it is
important to grow model capacity over time if the initial model is relatively
small. Moreover, updating the model at an intermediate rate strikes the best
trade off between accuracy and time to obtain a useful predictor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1"&gt;Lucas Caccia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1"&gt;Myle Ott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1"&gt;Marc&amp;#x27;Aurelio Ranzato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denoyer_L/0/1/0/all/0/1"&gt;Ludovic Denoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Square Root Principal Component Pursuit: Tuning-Free Noisy Robust Matrix Recovery. (arXiv:2106.09211v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09211</id>
        <link href="http://arxiv.org/abs/2106.09211"/>
        <updated>2021-06-18T02:06:36.849Z</updated>
        <summary type="html"><![CDATA[We propose a new framework -- Square Root Principal Component Pursuit -- for
low-rank matrix recovery from observations corrupted with noise and outliers.
Inspired by the square root Lasso, this new formulation does not require prior
knowledge of the noise level. We show that a single, universal choice of the
regularization parameter suffices to achieve reconstruction error proportional
to the (a priori unknown) noise level. In comparison, previous formulations
such as stable PCP rely on noise-dependent parameters to achieve similar
performance, and are therefore challenging to deploy in applications where the
noise level is unknown. We validate the effectiveness of our new method through
experiments on simulated and real datasets. Our simulations corroborate the
claim that a universal choice of the regularization parameter yields near
optimal performance across a range of noise levels, indicating that the
proposed method outperforms the (somewhat loose) bound proved here.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junhui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jingkai Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1"&gt;John Wright&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimality and Stability in Federated Learning: A Game-theoretic Approach. (arXiv:2106.09580v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.09580</id>
        <link href="http://arxiv.org/abs/2106.09580"/>
        <updated>2021-06-18T02:06:36.842Z</updated>
        <summary type="html"><![CDATA[Federated learning is a distributed learning paradigm where multiple agents,
each only with access to local data, jointly learn a global model. There has
recently been an explosion of research aiming not only to improve the accuracy
rates of federated learning, but also provide certain guarantees around social
good properties such as total error. One branch of this research has taken a
game-theoretic approach, and in particular, prior work has viewed federated
learning as a hedonic game, where error-minimizing players arrange themselves
into federating coalitions. This past work proves the existence of stable
coalition partitions, but leaves open a wide range of questions, including how
far from optimal these stable solutions are. In this work, we motivate and
define a notion of optimality given by the average error rates among federating
agents (players). First, we provide and prove the correctness of an efficient
algorithm to calculate an optimal (error minimizing) arrangement of players.
Next, we analyze the relationship between the stability and optimality of an
arrangement. First, we show that for some regions of parameter space, all
stable arrangements are optimal (Price of Anarchy equal to 1). However, we show
this is not true for all settings: there exist examples of stable arrangements
with higher cost than optimal (Price of Anarchy greater than 1). Finally, we
give the first constant-factor bound on the performance gap between stability
and optimality, proving that the total error of the worst stable solution can
be no higher than 9 times the total error of an optimal solution (Price of
Anarchy bound of 9).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Donahue_K/0/1/0/all/0/1"&gt;Kate Donahue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1"&gt;Jon Kleinberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Work in Progress: Mobile or FPGA? A Comprehensive Evaluation on Energy Efficiency and a Unified Optimization Framework. (arXiv:2106.09166v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09166</id>
        <link href="http://arxiv.org/abs/2106.09166"/>
        <updated>2021-06-18T02:06:36.836Z</updated>
        <summary type="html"><![CDATA[Efficient deployment of Deep Neural Networks (DNNs) on edge devices (i.e.,
FPGAs and mobile platforms) is very challenging, especially under a recent
witness of the increasing DNN model size and complexity. Although various
optimization approaches have been proven to be effective in many DNNs on edge
devices, most state-of-the-art work focuses on ad-hoc optimizations, and there
lacks a thorough study to comprehensively reveal the potentials and constraints
of different edge devices when considering different optimizations. In this
paper, we qualitatively and quantitatively compare the energy-efficiency of
FPGA-based and mobile-based DNN executions, and provide detailed analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1"&gt;Peiyan Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Mengshu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1"&gt;Wei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhengang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yuxuan Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Weiwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1"&gt;Bin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xulong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mungojerrie: Reinforcement Learning of Linear-Time Objectives. (arXiv:2106.09161v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09161</id>
        <link href="http://arxiv.org/abs/2106.09161"/>
        <updated>2021-06-18T02:06:36.830Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning synthesizes controllers without prior knowledge of the
system. At each timestep, a reward is given. The controllers optimize the
discounted sum of these rewards. Applying this class of algorithms requires
designing a reward scheme, which is typically done manually. The designer must
ensure that their intent is accurately captured. This may not be trivial, and
is prone to error. An alternative to this manual programming, akin to
programming directly in assembly, is to specify the objective in a formal
language and have it "compiled" to a reward scheme. Mungojerrie
($\href{https://plv.colorado.edu/mungojerrie/}{plv.colorado.edu/mungojerrie}$)
is a tool for testing reward schemes for $\omega$-regular objectives on finite
models. The tool contains reinforcement learning algorithms and a probabilistic
model checker. Mungojerrie supports models specified in PRISM and
$\omega$-automata specified in HOA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hahn_E/0/1/0/all/0/1"&gt;Ernst Moritz Hahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1"&gt;Mateo Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schewe_S/0/1/0/all/0/1"&gt;Sven Schewe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Somenzi_F/0/1/0/all/0/1"&gt;Fabio Somenzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Ashutosh Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wojtczak_D/0/1/0/all/0/1"&gt;Dominik Wojtczak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Power of Preconditioning in Sparse Linear Regression. (arXiv:2106.09207v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09207</id>
        <link href="http://arxiv.org/abs/2106.09207"/>
        <updated>2021-06-18T02:06:36.813Z</updated>
        <summary type="html"><![CDATA[Sparse linear regression is a fundamental problem in high-dimensional
statistics, but strikingly little is known about how to efficiently solve it
without restrictive conditions on the design matrix. We consider the
(correlated) random design setting, where the covariates are independently
drawn from a multivariate Gaussian $N(0,\Sigma)$ with $\Sigma : n \times n$,
and seek estimators $\hat{w}$ minimizing $(\hat{w}-w^*)^T\Sigma(\hat{w}-w^*)$,
where $w^*$ is the $k$-sparse ground truth. Information theoretically, one can
achieve strong error bounds with $O(k \log n)$ samples for arbitrary $\Sigma$
and $w^*$; however, no efficient algorithms are known to match these guarantees
even with $o(n)$ samples, without further assumptions on $\Sigma$ or $w^*$. As
far as hardness, computational lower bounds are only known with worst-case
design matrices. Random-design instances are known which are hard for the
Lasso, but these instances can generally be solved by Lasso after a simple
change-of-basis (i.e. preconditioning).

In this work, we give upper and lower bounds clarifying the power of
preconditioning in sparse linear regression. First, we show that the
preconditioned Lasso can solve a large class of sparse linear regression
problems nearly optimally: it succeeds whenever the dependency structure of the
covariates, in the sense of the Markov property, has low treewidth -- even if
$\Sigma$ is highly ill-conditioned. Second, we construct (for the first time)
random-design instances which are provably hard for an optimally preconditioned
Lasso. In fact, we complete our treewidth classification by proving that for
any treewidth-$t$ graph, there exists a Gaussian Markov Random Field on this
graph such that the preconditioned Lasso, with any choice of preconditioner,
requires $\Omega(t^{1/20})$ samples to recover $O(\log n)$-sparse signals when
covariates are drawn from this model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kelner_J/0/1/0/all/0/1"&gt;Jonathan Kelner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1"&gt;Frederic Koehler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meka_R/0/1/0/all/0/1"&gt;Raghu Meka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohatgi_D/0/1/0/all/0/1"&gt;Dhruv Rohatgi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Self-supervised Method for Entity Alignment. (arXiv:2106.09395v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09395</id>
        <link href="http://arxiv.org/abs/2106.09395"/>
        <updated>2021-06-18T02:06:36.806Z</updated>
        <summary type="html"><![CDATA[Entity alignment, aiming to identify equivalent entities across different
knowledge graphs (KGs), is a fundamental problem for constructing large-scale
KGs. Over the course of its development, supervision has been considered
necessary for accurate alignments. Inspired by the recent progress of
self-supervised learning, we explore the extent to which we can get rid of
supervision for entity alignment. Existing supervised methods for this task
focus on pulling each pair of positive (labeled) entities close to each other.
However, our analysis suggests that the learning of entity alignment can
actually benefit more from pushing sampled (unlabeled) negatives far away than
pulling positive aligned pairs close. We present SelfKG by leveraging this
discovery to design a contrastive learning strategy across two KGs. Extensive
experiments on benchmark datasets demonstrate that SelfKG without supervision
can match or achieve comparable results with state-of-the-art supervised
baselines. The performance of SelfKG demonstrates self-supervised learning
offers great potential for entity alignment in KGs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1"&gt;Haoyun Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinghao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zeyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1"&gt;Evgeny Kharlamov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yuxiao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gone Fishing: Neural Active Learning with Fisher Embeddings. (arXiv:2106.09675v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09675</id>
        <link href="http://arxiv.org/abs/2106.09675"/>
        <updated>2021-06-18T02:06:36.799Z</updated>
        <summary type="html"><![CDATA[There is an increasing need for effective active learning algorithms that are
compatible with deep neural networks. While there are many classic,
well-studied sample selection methods, the non-convexity and varying internal
representation of neural models make it unclear how to extend these approaches.
This article introduces BAIT, a practical, tractable, and high-performing
active learning algorithm for neural networks that addresses these concerns.
BAIT draws inspiration from the theoretical analysis of maximum likelihood
estimators (MLE) for parametric models. It selects batches of samples by
optimizing a bound on the MLE error in terms of the Fisher information, which
we show can be implemented efficiently at scale by exploiting linear-algebraic
structure especially amenable to execution on modern hardware. Our experiments
show that BAIT outperforms the previous state of the art on both classification
and regression problems, and is flexible enough to be used with a variety of
model architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1"&gt;Jordan T. Ash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1"&gt;Surbhi Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1"&gt;Akshay Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham Kakade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-intrusive Nonlinear Model Reduction via Machine Learning Approximations to Low-dimensional Operators. (arXiv:2106.09658v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09658</id>
        <link href="http://arxiv.org/abs/2106.09658"/>
        <updated>2021-06-18T02:06:36.791Z</updated>
        <summary type="html"><![CDATA[Although projection-based reduced-order models (ROMs) for parameterized
nonlinear dynamical systems have demonstrated exciting results across a range
of applications, their broad adoption has been limited by their intrusivity:
implementing such a reduced-order model typically requires significant
modifications to the underlying simulation code. To address this, we propose a
method that enables traditionally intrusive reduced-order models to be
accurately approximated in a non-intrusive manner. Specifically, the approach
approximates the low-dimensional operators associated with projection-based
reduced-order models (ROMs) using modern machine-learning regression
techniques. The only requirement of the simulation code is the ability to
export the velocity given the state and parameters as this functionality is
used to train the approximated low-dimensional operators. In addition to
enabling nonintrusivity, we demonstrate that the approach also leads to very
low computational complexity, achieving up to $1000\times$ reduction in run
time. We demonstrate the effectiveness of the proposed technique on two types
of PDEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1"&gt;Zhe Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1"&gt;Liqian Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotation Invariant Graph Neural Networks using Spin Convolutions. (arXiv:2106.09575v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09575</id>
        <link href="http://arxiv.org/abs/2106.09575"/>
        <updated>2021-06-18T02:06:36.785Z</updated>
        <summary type="html"><![CDATA[Progress towards the energy breakthroughs needed to combat climate change can
be significantly accelerated through the efficient simulation of atomic
systems. Simulation techniques based on first principles, such as Density
Functional Theory (DFT), are limited in their practical use due to their high
computational expense. Machine learning approaches have the potential to
approximate DFT in a computationally efficient manner, which could dramatically
increase the impact of computational simulations on real-world problems.
Approximating DFT poses several challenges. These include accurately modeling
the subtle changes in the relative positions and angles between atoms, and
enforcing constraints such as rotation invariance or energy conservation. We
introduce a novel approach to modeling angular information between sets of
neighboring atoms in a graph neural network. Rotation invariance is achieved
for the network's edge messages through the use of a per-edge local coordinate
frame and a novel spin convolution over the remaining degree of freedom. Two
model variants are proposed for the applications of structure relaxation and
molecular dynamics. State-of-the-art results are demonstrated on the
large-scale Open Catalyst 2020 dataset. Comparisons are also performed on the
MD17 and QM9 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shuaibi_M/0/1/0/all/0/1"&gt;Muhammed Shuaibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolluru_A/0/1/0/all/0/1"&gt;Adeesh Kolluru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1"&gt;Abhishek Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1"&gt;Aditya Grover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sriram_A/0/1/0/all/0/1"&gt;Anuroop Sriram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulissi_Z/0/1/0/all/0/1"&gt;Zachary Ulissi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zitnick_C/0/1/0/all/0/1"&gt;C. Lawrence Zitnick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving Eye-tracking Using Deep Learning. (arXiv:2106.09621v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09621</id>
        <link href="http://arxiv.org/abs/2106.09621"/>
        <updated>2021-06-18T02:06:36.768Z</updated>
        <summary type="html"><![CDATA[The expanding usage of complex machine learning methods like deep learning
has led to an explosion in human activity recognition, particularly applied to
health. In particular, as part of a larger body sensor network system, face and
full-body analysis is becoming increasingly common for evaluating health
status. However, complex models which handle private and sometimes protected
data, raise concerns about the potential leak of identifiable data. In this
work, we focus on the case of a deep network model trained on images of
individual faces. Full-face video recordings taken from 493 individuals
undergoing an eye-tracking based evaluation of neurological function were used.
Outputs, gradients, intermediate layer outputs, loss, and labels were used as
inputs for a deep network with an added support vector machine emission layer
to recognize membership in the training data. The inference attack method and
associated mathematical analysis indicate that there is a low likelihood of
unintended memorization of facial features in the deep learning model. In this
study, it is showed that the named model preserves the integrity of training
data with reasonable confidence. The same process can be implemented in similar
conditions for different models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seyedi_S/0/1/0/all/0/1"&gt;Salman Seyedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zifan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levey_A/0/1/0/all/0/1"&gt;Allan Levey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifford_G/0/1/0/all/0/1"&gt;Gari D. Clifford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prototypical Graph Contrastive Learning. (arXiv:2106.09645v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09645</id>
        <link href="http://arxiv.org/abs/2106.09645"/>
        <updated>2021-06-18T02:06:36.763Z</updated>
        <summary type="html"><![CDATA[Graph-level representations are critical in various real-world applications,
such as predicting the properties of molecules. But in practice, precise graph
annotations are generally very expensive and time-consuming. To address this
issue, graph contrastive learning constructs instance discrimination task which
pulls together positive pairs (augmentation pairs of the same graph) and pushes
away negative pairs (augmentation pairs of different graphs) for unsupervised
representation learning. However, since for a query, its negatives are
uniformly sampled from all graphs, existing methods suffer from the critical
sampling bias issue, i.e., the negatives likely having the same semantic
structure with the query, leading to performance degradation. To mitigate this
sampling bias issue, in this paper, we propose a Prototypical Graph Contrastive
Learning (PGCL) approach. Specifically, PGCL models the underlying semantic
structure of the graph data via clustering semantically similar graphs into the
same group, and simultaneously encourages the clustering consistency for
different augmentations of the same graph. Then given a query, it performs
negative sampling via drawing the graphs from those clusters that differ from
the cluster of query, which ensures the semantic difference between query and
its negative samples. Moreover, for a query, PGCL further reweights its
negative samples based on the distance between their prototypes (cluster
centroids) and the query prototype such that those negatives having moderate
prototype distance enjoy relatively large weights. This reweighting strategy is
proved to be more effective than uniform sampling. Experimental results on
various graph benchmarks testify the advantages of our PGCL over
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shuai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zi-Yuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuojia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1"&gt;Ruihui Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"&gt;Eric Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Explainable Student Group Collaboration Assessment Models Using Temporal Representations of Individual Student Roles. (arXiv:2106.09623v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09623</id>
        <link href="http://arxiv.org/abs/2106.09623"/>
        <updated>2021-06-18T02:06:36.756Z</updated>
        <summary type="html"><![CDATA[Collaboration is identified as a required and necessary skill for students to
be successful in the fields of Science, Technology, Engineering and Mathematics
(STEM). However, due to growing student population and limited teaching staff
it is difficult for teachers to provide constructive feedback and instill
collaborative skills using instructional methods. Development of simple and
easily explainable machine-learning-based automated systems can help address
this problem. Improving upon our previous work, in this paper we propose using
simple temporal-CNN deep-learning models to assess student group collaboration
that take in temporal representations of individual student roles as input. We
check the applicability of dynamically changing feature representations for
student group collaboration assessment and how they impact the overall
performance. We also use Grad-CAM visualizations to better understand and
interpret the important temporal indices that led to the deep-learning model's
decision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Som_A/0/1/0/all/0/1"&gt;Anirudh Som&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sujeong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Prado_B/0/1/0/all/0/1"&gt;Bladimir Lopez-Prado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhamija_S/0/1/0/all/0/1"&gt;Svati Dhamija&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alozie_N/0/1/0/all/0/1"&gt;Nonye Alozie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamrakar_A/0/1/0/all/0/1"&gt;Amir Tamrakar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Poisoning and Backdooring Contrastive Learning. (arXiv:2106.09667v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09667</id>
        <link href="http://arxiv.org/abs/2106.09667"/>
        <updated>2021-06-18T02:06:36.749Z</updated>
        <summary type="html"><![CDATA[Contrastive learning methods like CLIP train on noisy and uncurated training
datasets. This is cheaper than labeling datasets manually, and even improves
out-of-distribution robustness. We show that this practice makes backdoor and
poisoning attacks a significant threat. By poisoning just 0.005% of a dataset
(e.g., just 150 images of the 3 million-example Conceptual Captions dataset),
we can cause the model to misclassify test images by overlaying a small patch.
Targeted poisoning attacks, whereby the model misclassifies a particular test
input with an adversarially-desired label, are even easier requiring control of
less than 0.0001% of the dataset (e.g., just two out of the 3 million images).
Our attacks call into question whether training on noisy and uncurated Internet
scrapes is desirable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terzis_A/0/1/0/all/0/1"&gt;Andreas Terzis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Large Scale Molecular Language Representations Capture Important Structural Information?. (arXiv:2106.09553v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09553</id>
        <link href="http://arxiv.org/abs/2106.09553"/>
        <updated>2021-06-18T02:06:36.743Z</updated>
        <summary type="html"><![CDATA[Predicting chemical properties from the structure of a molecule is of great
importance in many applications including drug discovery and material design.
Machine learning based molecular property prediction holds the promise of
enabling accurate predictions at much less complexity, when compared to, for
example Density Functional Theory (DFT) calculations. Features extracted from
molecular graphs, using graph neural nets in a supervised manner, have emerged
as strong baselines for such tasks. However, the vast chemical space together
with the limited availability of labels makes supervised learning challenging,
calling for learning a general-purpose molecular representation. Recently,
pre-trained transformer-based language models (PTLMs) on large unlabeled corpus
have produced state-of-the-art results in many downstream natural language
processing tasks. Inspired by this development, here we present molecular
embeddings obtained by training an efficient transformer encoder model,
referred to as MoLFormer. This model was employed with a linear attention
mechanism and highly paralleized training on 1D SMILES sequences of 1.1 billion
unlabeled molecules from the PubChem and ZINC datasets. Experiments show that
the learned molecular representation performs competitively, when compared to
existing graph-based and fingerprint-based supervised learning baselines, on
the challenging tasks of predicting properties of QM8 and QM9 molecules.
Further task-specific fine-tuning of the MoLFormerr representation improves
performance on several of those property prediction benchmarks. These results
provide encouraging evidence that large-scale molecular language models can
capture sufficient structural information to be able to accurately predict
quantum chemical properties and beyond.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1"&gt;Jerret Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belgodere_B/0/1/0/all/0/1"&gt;Brian Belgodere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1"&gt;Vijil Chenthamarakshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1"&gt;Inkit Padhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1"&gt;Youssef Mroueh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1"&gt;Payel Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Semi-Supervised Learning for Delayed Partially Labelled Data Streams. (arXiv:2106.09170v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09170</id>
        <link href="http://arxiv.org/abs/2106.09170"/>
        <updated>2021-06-18T02:06:36.731Z</updated>
        <summary type="html"><![CDATA[Unlabelled data appear in many domains and are particularly relevant to
streaming applications, where even though data is abundant, labelled data is
rare. To address the learning problems associated with such data, one can
ignore the unlabelled data and focus only on the labelled data (supervised
learning); use the labelled data and attempt to leverage the unlabelled data
(semi-supervised learning); or assume some labels will be available on request
(active learning). The first approach is the simplest, yet the amount of
labelled data available will limit the predictive performance. The second
relies on finding and exploiting the underlying characteristics of the data
distribution. The third depends on an external agent to provide the required
labels in a timely fashion. This survey pays special attention to methods that
leverage unlabelled data in a semi-supervised setting. We also discuss the
delayed labelling issue, which impacts both fully supervised and
semi-supervised methods. We propose a unified problem setting, discuss the
learning guarantees and existing methods, explain the differences between
related problem settings. Finally, we review the current benchmarking practices
and propose adaptations to enhance them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_H/0/1/0/all/0/1"&gt;Heitor Murilo Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzenda_M/0/1/0/all/0/1"&gt;Maciej Grzenda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mello_R/0/1/0/all/0/1"&gt;Rodrigo Mello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1"&gt;Jesse Read&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Minh Huong Le Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bifet_A/0/1/0/all/0/1"&gt;Albert Bifet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Localized Uncertainty Attacks. (arXiv:2106.09222v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.09222</id>
        <link href="http://arxiv.org/abs/2106.09222"/>
        <updated>2021-06-18T02:06:36.713Z</updated>
        <summary type="html"><![CDATA[The susceptibility of deep learning models to adversarial perturbations has
stirred renewed attention in adversarial examples resulting in a number of
attacks. However, most of these attacks fail to encompass a large spectrum of
adversarial perturbations that are imperceptible to humans. In this paper, we
present localized uncertainty attacks, a novel class of threat models against
deterministic and stochastic classifiers. Under this threat model, we create
adversarial examples by perturbing only regions in the inputs where a
classifier is uncertain. To find such regions, we utilize the predictive
uncertainty of the classifier when the classifier is stochastic or, we learn a
surrogate model to amortize the uncertainty when it is deterministic. Unlike
$\ell_p$ ball or functional attacks which perturb inputs indiscriminately, our
targeted changes can be less perceptible. When considered under our threat
model, these attacks still produce strong adversarial examples; with the
examples retaining a greater degree of similarity with the inputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dia_O/0/1/0/all/0/1"&gt;Ousmane Amadou Dia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karaletsos_T/0/1/0/all/0/1"&gt;Theofanis Karaletsos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hazirbas_C/0/1/0/all/0/1"&gt;Caner Hazirbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ferrer_C/0/1/0/all/0/1"&gt;Cristian Canton Ferrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kabul_I/0/1/0/all/0/1"&gt;Ilknur Kaynar Kabul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meijer_E/0/1/0/all/0/1"&gt;Erik Meijer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis. (arXiv:2106.09660v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.09660</id>
        <link href="http://arxiv.org/abs/2106.09660"/>
        <updated>2021-06-18T02:06:36.706Z</updated>
        <summary type="html"><![CDATA[This paper introduces WaveGrad 2, a non-autoregressive generative model for
text-to-speech synthesis. WaveGrad 2 is trained to estimate the gradient of the
log conditional density of the waveform given a phoneme sequence. The model
takes an input phoneme sequence, and through an iterative refinement process,
generates an audio waveform. This contrasts to the original WaveGrad vocoder
which conditions on mel-spectrogram features, generated by a separate model.
The iterative refinement process starts from Gaussian noise, and through a
series of refinement steps (e.g., 50 steps), progressively recovers the audio
sequence. WaveGrad 2 offers a natural way to trade-off between inference speed
and sample quality, through adjusting the number of refinement steps.
Experiments show that the model can generate high fidelity audio, approaching
the performance of a state-of-the-art neural TTS system. We also report various
ablation studies over different model configurations. Audio samples are
available at https://wavegrad.github.io/v2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nanxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zen_H/0/1/0/all/0/1"&gt;Heiga Zen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Weiss_R/0/1/0/all/0/1"&gt;Ron J. Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Norouzi_M/0/1/0/all/0/1"&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dehak_N/0/1/0/all/0/1"&gt;Najim Dehak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1"&gt;William Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Importance measures derived from random forests: characterisation and extension. (arXiv:2106.09473v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.09473</id>
        <link href="http://arxiv.org/abs/2106.09473"/>
        <updated>2021-06-18T02:06:36.699Z</updated>
        <summary type="html"><![CDATA[Nowadays new technologies, and especially artificial intelligence, are more
and more established in our society. Big data analysis and machine learning,
two sub-fields of artificial intelligence, are at the core of many recent
breakthroughs in many application fields (e.g., medicine, communication,
finance, ...), including some that are strongly related to our day-to-day life
(e.g., social networks, computers, smartphones, ...). In machine learning,
significant improvements are usually achieved at the price of an increasing
computational complexity and thanks to bigger datasets. Currently, cutting-edge
models built by the most advanced machine learning algorithms typically became
simultaneously very efficient and profitable but also extremely complex. Their
complexity is to such an extent that these models are commonly seen as
black-boxes providing a prediction or a decision which can not be interpreted
or justified. Nevertheless, whether these models are used autonomously or as a
simple decision-making support tool, they are already being used in machine
learning applications where health and human life are at stake. Therefore, it
appears to be an obvious necessity not to blindly believe everything coming out
of those models without a detailed understanding of their predictions or
decisions. Accordingly, this thesis aims at improving the interpretability of
models built by a specific family of machine learning algorithms, the so-called
tree-based methods. Several mechanisms have been proposed to interpret these
models and we aim along this thesis to improve their understanding, study their
properties, and define their limitations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sutera_A/0/1/0/all/0/1"&gt;Antonio Sutera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Class Balancing GAN with a Classifier in the Loop. (arXiv:2106.09402v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09402</id>
        <link href="http://arxiv.org/abs/2106.09402"/>
        <updated>2021-06-18T02:06:36.681Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have swiftly evolved to imitate
increasingly complex image distributions. However, majority of the developments
focus on performance of GANs on balanced datasets. We find that the existing
GANs and their training regimes which work well on balanced datasets fail to be
effective in case of imbalanced (i.e. long-tailed) datasets. In this work we
introduce a novel theoretically motivated Class Balancing regularizer for
training GANs. Our regularizer makes use of the knowledge from a pre-trained
classifier to ensure balanced learning of all the classes in the dataset. This
is achieved via modelling the effective class frequency based on the
exponential forgetting observed in neural networks and encouraging the GAN to
focus on underrepresented classes. We demonstrate the utility of our
regularizer in learning representations for long-tailed distributions via
achieving better performance than existing approaches over multiple datasets.
Specifically, when applied to an unconditional GAN, it improves the FID from
$13.03$ to $9.01$ on the long-tailed iNaturalist-$2019$ dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rangwani_H/0/1/0/all/0/1"&gt;Harsh Rangwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1"&gt;Konda Reddy Mopuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1"&gt;R. Venkatesh Babu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning methods for postprocessing ensemble forecasts of wind gusts: A systematic comparison. (arXiv:2106.09512v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.09512</id>
        <link href="http://arxiv.org/abs/2106.09512"/>
        <updated>2021-06-18T02:06:36.675Z</updated>
        <summary type="html"><![CDATA[Postprocessing ensemble weather predictions to correct systematic errors has
become a standard practice in research and operations. However, only few recent
studies have focused on ensemble postprocessing of wind gust forecasts, despite
its importance for severe weather warnings. Here, we provide a comprehensive
review and systematic comparison of eight statistical and machine learning
methods for probabilistic wind gust forecasting via ensemble postprocessing,
that can be divided in three groups: State of the art postprocessing techniques
from statistics (ensemble model output statistics (EMOS), member-by-member
postprocessing, isotonic distributional regression), established machine
learning methods (gradient-boosting extended EMOS, quantile regression forests)
and neural network-based approaches (distributional regression network,
Bernstein quantile network, histogram estimation network). The methods are
systematically compared using six years of data from a high-resolution,
convection-permitting ensemble prediction system that was run operationally at
the German weather service, and hourly observations at 175 surface weather
stations in Germany. While all postprocessing methods yield calibrated
forecasts and are able to correct the systematic errors of the raw ensemble
predictions, incorporating information from additional meteorological predictor
variables beyond wind gusts leads to significant improvements in forecast
skill. In particular, we propose a flexible framework of locally adaptive
neural networks with different probabilistic forecast types as output, which
not only significantly outperform all benchmark postprocessing methods but also
learn physically consistent relations associated with the diurnal cycle,
especially the evening transition of the planetary boundary layer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Schulz_B/0/1/0/all/0/1"&gt;Benedikt Schulz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lerch_S/0/1/0/all/0/1"&gt;Sebastian Lerch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic Bias and Data Bias: Understanding the Relation between Distributionally Robust Optimization and Data Curation. (arXiv:2106.09467v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09467</id>
        <link href="http://arxiv.org/abs/2106.09467"/>
        <updated>2021-06-18T02:06:36.667Z</updated>
        <summary type="html"><![CDATA[Machine learning systems based on minimizing average error have been shown to
perform inconsistently across notable subsets of the data, which is not exposed
by a low average error for the entire dataset. In consequential social and
economic applications, where data represent people, this can lead to
discrimination of underrepresented gender and ethnic groups. Given the
importance of bias mitigation in machine learning, the topic leads to
contentious debates on how to ensure fairness in practice (data bias versus
algorithmic bias). Distributionally Robust Optimization (DRO) seemingly
addresses this problem by minimizing the worst expected risk across
subpopulations. We establish theoretical results that clarify the relation
between DRO and the optimization of the same loss averaged on an adequately
weighted training dataset. The results cover finite and infinite number of
training distributions, as well as convex and non-convex loss functions. We
show that neither DRO nor curating the training set should be construed as a
complete solution for bias mitigation: in the same way that there is no
universally robust training set, there is no universal way to setup a DRO
problem and ensure a socially acceptable set of results. We then leverage these
insights to provide a mininal set of practical recommendations for addressing
bias with DRO. Finally, we discuss ramifications of our results in other
related applications of DRO, using an example of adversarial robustness. Our
results show that there is merit to both the algorithm-focused and the
data-focused side of the bias debate, as long as arguments in favor of these
positions are precisely qualified and backed by relevant mathematics known
today.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slowik_A/0/1/0/all/0/1"&gt;Agnieszka S&amp;#x142;owik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bottou_L/0/1/0/all/0/1"&gt;L&amp;#xe9;on Bottou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Approach for Normalizing E-commerce Text Attributes (SANTA). (arXiv:2106.09493v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09493</id>
        <link href="http://arxiv.org/abs/2106.09493"/>
        <updated>2021-06-18T02:06:36.659Z</updated>
        <summary type="html"><![CDATA[In this paper, we present SANTA, a scalable framework to automatically
normalize E-commerce attribute values (e.g. "Win 10 Pro") to a fixed set of
pre-defined canonical values (e.g. "Windows 10"). Earlier works on attribute
normalization focused on fuzzy string matching (also referred as syntactic
matching in this paper). In this work, we first perform an extensive study of
nine syntactic matching algorithms and establish that 'cosine' similarity leads
to best results, showing 2.7% improvement over commonly used Jaccard index.
Next, we argue that string similarity alone is not sufficient for attribute
normalization as many surface forms require going beyond syntactic matching
(e.g. "720p" and "HD" are synonyms). While semantic techniques like
unsupervised embeddings (e.g. word2vec/fastText) have shown good results in
word similarity tasks, we observed that they perform poorly to distinguish
between close canonical forms, as these close forms often occur in similar
contexts. We propose to learn token embeddings using a twin network with
triplet loss. We propose an embedding learning task leveraging raw attribute
values and product titles to learn these embeddings in a self-supervised
fashion. We show that providing supervision using our proposed task improves
over both syntactic and unsupervised embeddings based techniques for attribute
normalization. Experiments on a real-world attribute normalization dataset of
50 attributes show that the embeddings trained using our proposed approach
obtain 2.3% improvement over best string matching and 19.3% improvement over
best unsupervised embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1"&gt;Ravi Shankar Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_K/0/1/0/all/0/1"&gt;Kartik Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasiwasia_N/0/1/0/all/0/1"&gt;Nikhil Rasiwasia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASR Adaptation for E-commerce Chatbots using Cross-Utterance Context and Multi-Task Language Modeling. (arXiv:2106.09532v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.09532</id>
        <link href="http://arxiv.org/abs/2106.09532"/>
        <updated>2021-06-18T02:06:36.653Z</updated>
        <summary type="html"><![CDATA[Automatic Speech Recognition (ASR) robustness toward slot entities are
critical in e-commerce voice assistants that involve monetary transactions and
purchases. Along with effective domain adaptation, it is intuitive that cross
utterance contextual cues play an important role in disambiguating domain
specific content words from speech. In this paper, we investigate various
techniques to improve contextualization, content word robustness and domain
adaptation of a Transformer-XL neural language model (NLM) to rescore ASR
N-best hypotheses. To improve contextualization, we utilize turn level dialogue
acts along with cross utterance context carry over. Additionally, to adapt our
domain-general NLM towards e-commerce on-the-fly, we use embeddings derived
from a finetuned masked LM on in-domain data. Finally, to improve robustness
towards in-domain content words, we propose a multi-task model that can jointly
perform content word detection and language modeling tasks. Compared to a
non-contextual LSTM LM baseline, our best performing NLM rescorer results in a
content WER reduction of 19.2% on e-commerce audio test set and a slot labeling
F1 improvement of 6.4%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shenoy_A/0/1/0/all/0/1"&gt;Ashish Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bodapati_S/0/1/0/all/0/1"&gt;Sravan Bodapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kirchhoff_K/0/1/0/all/0/1"&gt;Katrin Kirchhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Laws for Acoustic Models. (arXiv:2106.09488v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.09488</id>
        <link href="http://arxiv.org/abs/2106.09488"/>
        <updated>2021-06-18T02:06:36.643Z</updated>
        <summary type="html"><![CDATA[There is a recent trend in machine learning to increase model quality by
growing models to sizes previously thought to be unreasonable. Recent work has
shown that autoregressive generative models with cross-entropy objective
functions exhibit smooth power-law relationships, or scaling laws, that predict
model quality from model size, training set size, and the available compute
budget. These scaling laws allow one to choose nearly optimal hyper-parameters
given constraints on available training data, model parameter count, or
training computation budget. In this paper, we demonstrate that acoustic models
trained with an auto-predictive coding loss behave as if they are subject to
similar scaling laws. We extend previous work to jointly predict loss due to
model size, to training set size, and to the inherent "irreducible loss" of the
task. We find that the scaling laws accurately match model performance over two
orders of magnitude in both model size and training set size, and make
predictions about the limits of model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Droppo_J/0/1/0/all/0/1"&gt;Jasha Droppo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Elibol_O/0/1/0/all/0/1"&gt;Oguz Elibol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coded Federated Learning Framework for AI-Based Mobile Application Services with Privacy-Awareness. (arXiv:2106.09261v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.09261</id>
        <link href="http://arxiv.org/abs/2106.09261"/>
        <updated>2021-06-18T02:06:36.638Z</updated>
        <summary type="html"><![CDATA[By encoding computing tasks, coded computing can not only mitigate straggling
problems in federated learning (FL), but also preserve privacy of sensitive
data uploaded/contributed by participating mobile users (MUs) to the
centralized server, owned by a mobile application provider (MAP). However,
these advantages come with extra coding cost/complexity and communication
overhead (referred to as \emph{privacy cost}) that must be considered given the
limited computing/communications resources at MUs/MAP, the rationality and
incentive competition among MUs in contributing data to the MAP. This article
proposes a novel coded FL-based framework for a privacy-aware mobile
application service to address these challenges. In particular, the MAP first
determines a set of the best MUs for the FL process based on MUs' provided
information/features. Then, each selected MU can propose a contract to the MAP
according to its expected trainable local data and privacy-protected coded
data. To find the optimal contracts that can maximize utilities of the MAP and
all the participating MUs while maintaining high learning quality of the whole
system, we first develop a multi-principal one-agent contract-based problem
leveraging coded FL-based multiple utility functions under the MUs' privacy
cost, the MAP's limited computing resource, and asymmetric information between
the MAP and MUs. Then, we transform the problem into an equivalent
low-complexity problem and develop an iterative algorithm to solve it.
Experiments with a real-world dataset show that our framework can speed up
training time up to 49% and improve prediction accuracy up to 4.6 times while
enhancing network's social welfare, i.e., total utility of all participating
entities, up to 114% under the privacy cost consideration compared with those
of baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saputra_Y/0/1/0/all/0/1"&gt;Yuris Mulya Saputra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Diep N. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1"&gt;Dinh Thai Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutkiewicz_E/0/1/0/all/0/1"&gt;Eryk Dutkiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoANE: Modeling Context Co-occurrence for Attributed Network Embedding. (arXiv:2106.09241v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.09241</id>
        <link href="http://arxiv.org/abs/2106.09241"/>
        <updated>2021-06-18T02:06:36.631Z</updated>
        <summary type="html"><![CDATA[Attributed network embedding (ANE) is to learn low-dimensional vectors so
that not only the network structure but also node attributes can be preserved
in the embedding space. Existing ANE models do not consider the specific
combination between graph structure and attributes. While each node has its
structural characteristics, such as highly-interconnected neighbors along with
their certain patterns of attribute distribution, each node's neighborhood
should be not only depicted by multi-hop nodes, but consider certain clusters
or social circles. To model such information, in this paper, we propose a novel
ANE model, Context Co-occurrence-aware Attributed Network Embedding (CoANE).
The basic idea of CoANE is to model the context attributes that each node's
involved diverse patterns, and apply the convolutional mechanism to encode
positional information by treating each attribute as a channel. The learning of
context co-occurrence can capture the latent social circles of each node. To
better encode structural and semantic knowledge of nodes, we devise a three-way
objective function, consisting of positive graph likelihood, contextual
negative sampling, and attribute reconstruction. We conduct experiments on five
real datasets in the tasks of link prediction, node label classification, and
node clustering. The results exhibit that CoANE can significantly outperform
state-of-the-art ANE models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_I/0/1/0/all/0/1"&gt;I-Chung Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Cheng-Te Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EEG-GNN: Graph Neural Networks for Classification of Electroencephalogram (EEG) Signals. (arXiv:2106.09135v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09135</id>
        <link href="http://arxiv.org/abs/2106.09135"/>
        <updated>2021-06-18T02:06:36.605Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNN) have been frequently used to extract
subject-invariant features from electroencephalogram (EEG) for classification
tasks. This approach holds the underlying assumption that electrodes are
equidistant analogous to pixels of an image and hence fails to explore/exploit
the complex functional neural connectivity between different electrode sites.
We overcome this limitation by tailoring the concepts of convolution and
pooling applied to 2D grid-like inputs for the functional network of electrode
sites. Furthermore, we develop various graph neural network (GNN) models that
project electrodes onto the nodes of a graph, where the node features are
represented as EEG channel samples collected over a trial, and nodes can be
connected by weighted/unweighted edges according to a flexible policy
formulated by a neuroscientist. The empirical evaluations show that our
proposed GNN-based framework outperforms standard CNN classifiers across ErrP,
and RSVP datasets, as well as allowing neuroscientific interpretability and
explainability to deep learning methods tailored to EEG related classification
problems. Another practical advantage of our GNN-based framework is that it can
be used in EEG channel selection, which is critical for reducing computational
cost, and designing portable EEG headsets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_A/0/1/0/all/0/1"&gt;Andac Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koike_Akino_T/0/1/0/all/0/1"&gt;Toshiaki Koike-Akino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Ye Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haruna_M/0/1/0/all/0/1"&gt;Masaki Haruna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1"&gt;Deniz Erdogmus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Rigorous Theoretical Analysis and Evaluation of GNN Explanations. (arXiv:2106.09078v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09078</id>
        <link href="http://arxiv.org/abs/2106.09078"/>
        <updated>2021-06-18T02:06:36.592Z</updated>
        <summary type="html"><![CDATA[As Graph Neural Networks (GNNs) are increasingly employed in real-world
applications, it becomes critical to ensure that the stakeholders understand
the rationale behind their predictions. While several GNN explanation methods
have been proposed recently, there has been little to no work on theoretically
analyzing the behavior of these methods or systematically evaluating their
effectiveness. Here, we introduce the first axiomatic framework for
theoretically analyzing, evaluating, and comparing state-of-the-art GNN
explanation methods. We outline and formalize the key desirable properties that
all GNN explanation methods should satisfy in order to generate reliable
explanations, namely, faithfulness, stability, and fairness. We leverage these
properties to present the first ever theoretical analysis of the effectiveness
of state-of-the-art GNN explanation methods. Our analysis establishes upper
bounds on all the aforementioned properties for popular GNN explanation
methods. We also leverage our framework to empirically evaluate these methods
on multiple real-world datasets from diverse domains. Our empirical results
demonstrate that some popular GNN explanation methods (e.g., gradient-based
methods) perform no better than a random baseline and that methods which
leverage the graph structure are more effective than those that solely rely on
the node features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1"&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1"&gt;Marinka Zitnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FORMS: Fine-grained Polarized ReRAM-based In-situ Computation for Mixed-signal DNN Accelerator. (arXiv:2106.09144v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2106.09144</id>
        <link href="http://arxiv.org/abs/2106.09144"/>
        <updated>2021-06-18T02:06:36.583Z</updated>
        <summary type="html"><![CDATA[Recent works demonstrated the promise of using resistive random access memory
(ReRAM) as an emerging technology to perform inherently parallel analog domain
in-situ matrix-vector multiplication -- the intensive and key computation in
DNNs. With weights stored in the ReRAM crossbar cells as conductance, when the
input vector is applied to word lines, the matrix-vector multiplication results
can be generated as the current in bit lines. A key problem is that the weight
can be either positive or negative, but the in-situ computation assumes all
cells on each crossbar column with the same sign. The current architectures
either use two ReRAM crossbars for positive and negative weights, or add an
offset to weights so that all values become positive. Neither solution is
ideal: they either double the cost of crossbars, or incur extra offset
circuity. To better solve this problem, this paper proposes FORMS, a
fine-grained ReRAM-based DNN accelerator with polarized weights. Instead of
trying to represent the positive/negative weights, our key design principle is
to enforce exactly what is assumed in the in-situ computation -- ensuring that
all weights in the same column of a crossbar have the same sign. It naturally
avoids the cost of an additional crossbar. Such weights can be nicely generated
using alternating direction method of multipliers (ADMM) regularized
optimization, which can exactly enforce certain patterns in DNN weights. To
achieve high accuracy, we propose to use fine-grained sub-array columns, which
provide a unique opportunity for input zero-skipping, significantly avoiding
unnecessary computations. It also makes the hardware much easier to implement.
Putting all together, with the same optimized models, FORMS achieves
significant throughput improvement and speed up in frame per second over ISAAC
with similar area cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behnam_P/0/1/0/all/0/1"&gt;Payman Behnam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhengang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiee_A/0/1/0/all/0/1"&gt;Ali Shafiee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Sheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xuehai Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojnordi_M/0/1/0/all/0/1"&gt;Mahdi Nazm Bojnordi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Caiwen Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning. (arXiv:2106.09226v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09226</id>
        <link href="http://arxiv.org/abs/2106.09226"/>
        <updated>2021-06-18T02:06:36.564Z</updated>
        <summary type="html"><![CDATA[Pretrained language models have achieved state-of-the-art performance when
adapted to a downstream NLP task. However, theoretical analysis of these models
is scarce and challenging since the pretraining and downstream tasks can be
very different. We propose an analysis framework that links the pretraining and
downstream tasks with an underlying latent variable generative model of text --
the downstream classifier must recover a function of the posterior distribution
over the latent variables. We analyze head tuning (learning a classifier on top
of the frozen pretrained model) and prompt tuning in this setting. The
generative model in our analysis is either a Hidden Markov Model (HMM) or an
HMM augmented with a latent memory component, motivated by long-term
dependencies in natural language. We show that 1) under certain non-degeneracy
conditions on the HMM, simple classification heads can solve the downstream
task, 2) prompt tuning obtains downstream guarantees with weaker non-degeneracy
conditions, and 3) our recovery guarantees for the memory-augmented HMM are
stronger than for the vanilla HMM because task-relevant information is easier
to recover from the long-term memory. Experiments on synthetically generated
data from HMMs back our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Colin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Sang Michael Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LiRA: Learning Visual Speech Representations from Audio through Self-supervision. (arXiv:2106.09171v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09171</id>
        <link href="http://arxiv.org/abs/2106.09171"/>
        <updated>2021-06-18T02:06:36.550Z</updated>
        <summary type="html"><![CDATA[The large amount of audiovisual content being shared online today has drawn
substantial attention to the prospect of audiovisual self-supervised learning.
Recent works have focused on each of these modalities separately, while others
have attempted to model both simultaneously in a cross-modal fashion. However,
comparatively little attention has been given to leveraging one modality as a
training objective to learn from the other. In this work, we propose Learning
visual speech Representations from Audio via self-supervision (LiRA).
Specifically, we train a ResNet+Conformer model to predict acoustic features
from unlabelled visual speech. We find that this pre-trained model can be
leveraged towards word-level and sentence-level lip-reading through feature
extraction and fine-tuning experiments. We show that our approach significantly
outperforms other self-supervised methods on the Lip Reading in the Wild (LRW)
dataset and achieves state-of-the-art performance on Lip Reading Sentences 2
(LRS2) using only a fraction of the total labelled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1"&gt;Pingchuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mira_R/0/1/0/all/0/1"&gt;Rodrigo Mira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1"&gt;Stavros Petridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn W. Schuller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1"&gt;Maja Pantic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Bias-Reduced Gradient Methods. (arXiv:2106.09481v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.09481</id>
        <link href="http://arxiv.org/abs/2106.09481"/>
        <updated>2021-06-18T02:06:36.529Z</updated>
        <summary type="html"><![CDATA[We develop a new primitive for stochastic optimization: a low-bias, low-cost
estimator of the minimizer $x_\star$ of any Lipschitz strongly-convex function.
In particular, we use a multilevel Monte-Carlo approach due to Blanchet and
Glynn to turn any optimal stochastic gradient method into an estimator of
$x_\star$ with bias $\delta$, variance $O(\log(1/\delta))$, and an expected
sampling cost of $O(\log(1/\delta))$ stochastic gradient evaluations. As an
immediate consequence, we obtain cheap and nearly unbiased gradient estimators
for the Moreau-Yoshida envelope of any Lipschitz convex function, allowing us
to perform dimension-free randomized smoothing.

We demonstrate the potential of our estimator through four applications.
First, we develop a method for minimizing the maximum of $N$ functions,
improving on recent results and matching a lower bound up logarithmic factors.
Second and third, we recover state-of-the-art rates for projection-efficient
and gradient-efficient optimization using simple algorithms with a transparent
analysis. Finally, we show that an improved version of our estimator would
yield a nearly linear-time, optimal-utility, differentially-private non-smooth
stochastic optimization method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Asi_H/0/1/0/all/0/1"&gt;Hilal Asi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Carmon_Y/0/1/0/all/0/1"&gt;Yair Carmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jambulapati_A/0/1/0/all/0/1"&gt;Arun Jambulapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yujia Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sidford_A/0/1/0/all/0/1"&gt;Aaron Sidford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voice2Series: Reprogramming Acoustic Models for Time Series Classification. (arXiv:2106.09296v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09296</id>
        <link href="http://arxiv.org/abs/2106.09296"/>
        <updated>2021-06-18T02:06:36.507Z</updated>
        <summary type="html"><![CDATA[Learning to classify time series with limited data is a practical yet
challenging problem. Current methods are primarily based on hand-designed
feature extraction rules or domain-specific data augmentation. Motivated by the
advances in deep speech processing models and the fact that voice data are
univariate temporal signals, in this paper, we propose Voice2Series (V2S), a
novel end-to-end approach that reprograms acoustic models for time series
classification, through input transformation learning and output label mapping.
Leveraging the representation learning power of a large-scale pre-trained
speech processing model, on 30 different time series tasks we show that V2S
either outperforms or is tied with state-of-the-art methods on 20 tasks, and
improves their average accuracy by 1.84%. We further provide a theoretical
justification of V2S by proving its population risk is upper bounded by the
source risk and a Wasserstein distance accounting for feature alignment via
reprogramming. Our results offer new and effective means to time series
classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao-Han Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yun-Yun Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning Randomly Initialized Neural Networks with Iterative Randomization. (arXiv:2106.09269v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09269</id>
        <link href="http://arxiv.org/abs/2106.09269"/>
        <updated>2021-06-18T02:06:36.500Z</updated>
        <summary type="html"><![CDATA[Pruning the weights of randomly initialized neural networks plays an
important role in the context of lottery ticket hypothesis. Ramanujan et al.
(2020) empirically showed that only pruning the weights can achieve remarkable
performance instead of optimizing the weight values. However, to achieve the
same level of performance as the weight optimization, the pruning approach
requires more parameters in the networks before pruning and thus more memory
space. To overcome this parameter inefficiency, we introduce a novel framework
to prune randomly initialized neural networks with iteratively randomizing
weight values (IteRand). Theoretically, we prove an approximation theorem in
our framework, which indicates that the randomizing operations are provably
effective to reduce the required number of the parameters. We also empirically
demonstrate the parameter efficiency in multiple experiments on CIFAR-10 and
ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chijiwa_D/0/1/0/all/0/1"&gt;Daiki Chijiwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ida_Y/0/1/0/all/0/1"&gt;Yasutoshi Ida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Umakoshi_K/0/1/0/all/0/1"&gt;Kenji Umakoshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inoue_T/0/1/0/all/0/1"&gt;Tomohiro Inoue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating the Robustness of Bayesian Neural Networks Against Different Types of Attacks. (arXiv:2106.09223v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09223</id>
        <link href="http://arxiv.org/abs/2106.09223"/>
        <updated>2021-06-18T02:06:36.490Z</updated>
        <summary type="html"><![CDATA[To evaluate the robustness gain of Bayesian neural networks on image
classification tasks, we perform input perturbations, and adversarial attacks
to the state-of-the-art Bayesian neural networks, with a benchmark CNN model as
reference. The attacks are selected to simulate signal interference and
cyberattacks towards CNN-based machine learning systems. The result shows that
a Bayesian neural network achieves significantly higher robustness against
adversarial attacks generated against a deterministic neural network model,
without adversarial training. The Bayesian posterior can act as the safety
precursor of ongoing malicious activities. Furthermore, we show that the
stochastic classifier after the deterministic CNN extractor has sufficient
robustness enhancement rather than a stochastic feature extractor before the
stochastic classifier. This advises on utilizing stochastic layers in building
decision-making pipelines within a safety-critical domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1"&gt;Yutian Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Sheng Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jueming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning for complete intersection Calabi-Yau manifolds: a methodological study. (arXiv:2007.15706v2 [hep-th] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15706</id>
        <link href="http://arxiv.org/abs/2007.15706"/>
        <updated>2021-06-18T02:06:36.481Z</updated>
        <summary type="html"><![CDATA[We revisit the question of predicting both Hodge numbers $h^{1,1}$ and
$h^{2,1}$ of complete intersection Calabi-Yau (CICY) 3-folds using machine
learning (ML), considering both the old and new datasets built respectively by
Candelas-Dale-Lutken-Schimmrigk / Green-H\"ubsch-Lutken and by
Anderson-Gao-Gray-Lee. In real world applications, implementing a ML system
rarely reduces to feed the brute data to the algorithm. Instead, the typical
workflow starts with an exploratory data analysis (EDA) which aims at
understanding better the input data and finding an optimal representation. It
is followed by the design of a validation procedure and a baseline model.
Finally, several ML models are compared and combined, often involving neural
networks with a topology more complicated than the sequential models typically
used in physics. By following this procedure, we improve the accuracy of ML
computations for Hodge numbers with respect to the existing literature. First,
we obtain 97% (resp. 99%) accuracy for $h^{1,1}$ using a neural network
inspired by the Inception model for the old dataset, using only 30% (resp. 70%)
of the data for training. For the new one, a simple linear regression leads to
almost 100% accuracy with 30% of the data for training. The computation of
$h^{2,1}$ is less successful as we manage to reach only 50% accuracy for both
datasets, but this is still better than the 16% obtained with a simple neural
network (SVM with Gaussian kernel and feature engineering and sequential
convolutional network reach at best 36%). This serves as a proof of concept
that neural networks can be valuable to study the properties of geometries
appearing in string theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-th/1/au:+Erbin_H/0/1/0/all/0/1"&gt;Harold Erbin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Finotello_R/0/1/0/all/0/1"&gt;Riccardo Finotello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-modal fusion with gating using audio, lexical and disfluency features for Alzheimer's Dementia recognition from spontaneous speech. (arXiv:2106.09668v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09668</id>
        <link href="http://arxiv.org/abs/2106.09668"/>
        <updated>2021-06-18T02:06:36.473Z</updated>
        <summary type="html"><![CDATA[This paper is a submission to the Alzheimer's Dementia Recognition through
Spontaneous Speech (ADReSS) challenge, which aims to develop methods that can
assist in the automated prediction of severity of Alzheimer's Disease from
speech data. We focus on acoustic and natural language features for cognitive
impairment detection in spontaneous speech in the context of Alzheimer's
Disease Diagnosis and the mini-mental state examination (MMSE) score
prediction. We proposed a model that obtains unimodal decisions from different
LSTMs, one for each modality of text and audio, and then combines them using a
gating mechanism for the final prediction. We focused on sequential modelling
of text and audio and investigated whether the disfluencies present in
individuals' speech relate to the extent of their cognitive impairment. Our
results show that the proposed classification and regression schemes obtain
very promising results on both development and test sets. This suggests
Alzheimer's Disease can be detected successfully with sequence modeling of the
speech data of medical sessions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rohanian_M/0/1/0/all/0/1"&gt;Morteza Rohanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hough_J/0/1/0/all/0/1"&gt;Julian Hough&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1"&gt;Matthew Purver&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Prototype Learning for Interpretable Multivariable Time Series Classification. (arXiv:2106.09636v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09636</id>
        <link href="http://arxiv.org/abs/2106.09636"/>
        <updated>2021-06-18T02:06:36.453Z</updated>
        <summary type="html"><![CDATA[Multivariable time series classification problems are increasing in
prevalence and complexity in a variety of domains, such as biology and finance.
While deep learning methods are an effective tool for these problems, they
often lack interpretability. In this work, we propose a novel modular prototype
learning framework for multivariable time series classification. In the first
stage of our framework, encoders extract features from each variable
independently. Prototype layers identify single-variable prototypes in the
resulting feature spaces. The next stage of our framework represents the
multivariable time series sample points in terms of their similarity to these
single-variable prototypes. This results in an inherently interpretable
representation of multivariable patterns, on which prototype learning is
applied to extract representative examples i.e. multivariable prototypes. Our
framework is thus able to explicitly identify both informative patterns in the
individual variables, as well as the relationships between the variables. We
validate our framework on a simulated dataset with embedded patterns, as well
as a real human activity recognition problem. Our framework attains comparable
or superior classification performance to existing time series classification
methods on these tasks. On the simulated dataset, we find that our model
returns interpretations consistent with the embedded patterns. Moreover, the
interpretations learned on the activity recognition dataset align with domain
knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_G/0/1/0/all/0/1"&gt;Gaurav R. Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbasi_Asl_R/0/1/0/all/0/1"&gt;Reza Abbasi-Asl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distance Metric Learning for Graph Structured Data. (arXiv:2002.00727v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00727</id>
        <link href="http://arxiv.org/abs/2002.00727"/>
        <updated>2021-06-18T02:06:36.447Z</updated>
        <summary type="html"><![CDATA[Graphs are versatile tools for representing structured data. As a result, a
variety of machine learning methods have been studied for graph data analysis.
Although many such learning methods depend on the measurement of differences
between input graphs, defining an appropriate distance metric for graphs
remains a controversial issue. Hence, we propose a supervised distance metric
learning method for the graph classification problem. Our method, named
interpretable graph metric learning (IGML), learns discriminative metrics in a
subgraph-based feature space, which has a strong graph representation
capability. By introducing a sparsity-inducing penalty on the weight of each
subgraph, IGML can identify a small number of important subgraphs that can
provide insight into the given classification task. Because our formulation has
a large number of optimization variables, an efficient algorithm that uses
pruning techniques based on safe screening and working set selection methods is
also proposed. An important property of IGML is that solution optimality is
guaranteed because the problem is formulated as a convex problem and our
pruning strategies only discard unnecessary subgraphs. Furthermore, we show
that IGML is also applicable to other structured data such as itemset and
sequence data, and that it can incorporate vertex-label similarity by using a
transportation-based subgraph feature. We empirically evaluate the
computational efficiency and classification performance of IGML on several
benchmark datasets and provide some illustrative examples of how IGML
identifies important subgraphs from a given graph dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yoshida_T/0/1/0/all/0/1"&gt;Tomoki Yoshida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1"&gt;Ichiro Takeuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karasuyama_M/0/1/0/all/0/1"&gt;Masayuki Karasuyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Reinforcement Learning of Symbolic Reasoning Domains. (arXiv:2106.09146v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.09146</id>
        <link href="http://arxiv.org/abs/2106.09146"/>
        <updated>2021-06-18T02:06:36.440Z</updated>
        <summary type="html"><![CDATA[Abstract symbolic reasoning, as required in domains such as mathematics and
logic, is a key component of human intelligence. Solvers for these domains have
important applications, especially to computer-assisted education. But learning
to solve symbolic problems is challenging for machine learning algorithms.
Existing models either learn from human solutions or use hand-engineered
features, making them expensive to apply in new domains. In this paper, we
instead consider symbolic domains as simple environments where states and
actions are given as unstructured text, and binary rewards indicate whether a
problem is solved. This flexible setup makes it easy to specify new domains,
but search and planning become challenging. We introduce four environments
inspired by the Mathematics Common Core Curriculum, and observe that existing
Reinforcement Learning baselines perform poorly. We then present a novel
learning algorithm, Contrastive Policy Learning (ConPoLe) that explicitly
optimizes the InfoNCE loss, which lower bounds the mutual information between
the current state and next states that continue on a path to the solution.
ConPoLe successfully solves all four domains. Moreover, problem representations
learned by ConPoLe enable accurate prediction of the categories of problems in
a real mathematics curriculum. Our results suggest new directions for
reinforcement learning in symbolic domains, as well as applications to
mathematics education.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poesia_G/0/1/0/all/0/1"&gt;Gabriel Poesia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1"&gt;WenXin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1"&gt;Noah Goodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smart Contract Vulnerability Detection: From Pure Neural Network to Interpretable Graph Feature and Expert Pattern Fusion. (arXiv:2106.09282v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09282</id>
        <link href="http://arxiv.org/abs/2106.09282"/>
        <updated>2021-06-18T02:06:36.434Z</updated>
        <summary type="html"><![CDATA[Smart contracts hold digital coins worth billions of dollars, their security
issues have drawn extensive attention in the past years. Towards smart contract
vulnerability detection, conventional methods heavily rely on fixed expert
rules, leading to low accuracy and poor scalability. Recent deep learning
approaches alleviate this issue but fail to encode useful expert knowledge. In
this paper, we explore combining deep learning with expert patterns in an
explainable fashion. Specifically, we develop automatic tools to extract expert
patterns from the source code. We then cast the code into a semantic graph to
extract deep graph features. Thereafter, the global graph feature and local
expert patterns are fused to cooperate and approach the final prediction, while
yielding their interpretable weights. Experiments are conducted on all
available smart contracts with source code in two platforms, Ethereum and VNT
Chain. Empirically, our system significantly outperforms state-of-the-art
methods. Our code is released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhenguang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1"&gt;Peng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1"&gt;Qinming He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shouling Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Dark Side of Calibration for Modern Neural Networks. (arXiv:2106.09385v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09385</id>
        <link href="http://arxiv.org/abs/2106.09385"/>
        <updated>2021-06-18T02:06:36.426Z</updated>
        <summary type="html"><![CDATA[Modern neural networks are highly uncalibrated. It poses a significant
challenge for safety-critical systems to utilise deep neural networks (DNNs),
reliably. Many recently proposed approaches have demonstrated substantial
progress in improving DNN calibration. However, they hardly touch upon
refinement, which historically has been an essential aspect of calibration.
Refinement indicates separability of a network's correct and incorrect
predictions. This paper presents a theoretically and empirically supported
exposition for reviewing a model's calibration and refinement. Firstly, we show
the breakdown of expected calibration error (ECE), into predicted confidence
and refinement. Connecting with this result, we highlight that regularisation
based calibration only focuses on naively reducing a model's confidence. This
logically has a severe downside to a model's refinement. We support our claims
through rigorous empirical evaluations of many state of the art calibration
approaches on standard datasets. We find that many calibration approaches with
the likes of label smoothing, mixup etc. lower the utility of a DNN by
degrading its refinement. Even under natural data shift, this
calibration-refinement trade-off holds for the majority of calibration methods.
These findings call for an urgent retrospective into some popular pathways
taken for modern DNN calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Aditya Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bay_A/0/1/0/all/0/1"&gt;Alessandro Bay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_B/0/1/0/all/0/1"&gt;Biswa Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirabile_A/0/1/0/all/0/1"&gt;Andrea Mirabile&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Heterogeneous Clients with Elastic Federated Learning. (arXiv:2106.09433v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09433</id>
        <link href="http://arxiv.org/abs/2106.09433"/>
        <updated>2021-06-18T02:06:36.406Z</updated>
        <summary type="html"><![CDATA[Federated learning involves training machine learning models over devices or
data silos, such as edge processors or data warehouses, while keeping the data
local. Training in heterogeneous and potentially massive networks introduces
bias into the system, which is originated from the non-IID data and the low
participation rate in reality. In this paper, we propose Elastic Federated
Learning (EFL), an unbiased algorithm to tackle the heterogeneity in the
system, which makes the most informative parameters less volatile during
training, and utilizes the incomplete local updates. It is an efficient and
effective algorithm that compresses both upstream and downstream
communications. Theoretically, the algorithm has convergence guarantee when
training on the non-IID data at the low participation rate. Empirical
experiments corroborate the competitive performance of EFL framework on the
robustness and the efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zichen Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zihan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenye Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Biomedical Interpretable Entity Representations. (arXiv:2106.09502v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09502</id>
        <link href="http://arxiv.org/abs/2106.09502"/>
        <updated>2021-06-18T02:06:36.399Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models induce dense entity representations that offer
strong performance on entity-centric NLP tasks, but such representations are
not immediately interpretable. This can be a barrier to model uptake in
important domains such as biomedicine. There has been recent work on general
interpretable representation learning (Onoe and Durrett, 2020), but these
domain-agnostic representations do not readily transfer to the important domain
of biomedicine. In this paper, we create a new entity type system and training
set from a large corpus of biomedical texts by mapping entities to concepts in
a medical ontology, and from these to Wikipedia pages whose categories are our
types. From this mapping we derive Biomedical Interpretable Entity
Representations(BIERs), in which dimensions correspond to fine-grained entity
types, and values are predicted probabilities that a given entity is of the
corresponding type. We propose a novel method that exploits BIER's final sparse
and intermediate dense representations to facilitate model and entity type
debugging. We show that BIERs achieve strong performance in biomedical tasks
including named entity disambiguation and entity label classification, and we
provide error analysis to highlight the utility of their interpretability,
particularly in low-supervision settings. Finally, we provide our induced 68K
biomedical type system, the corresponding 37 million triples of derived data
used to train BIER models and our best performing model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Olano_D/0/1/0/all/0/1"&gt;Diego Garcia-Olano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onoe_Y/0/1/0/all/0/1"&gt;Yasumasa Onoe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1"&gt;Ioana Baldini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_J/0/1/0/all/0/1"&gt;Joydeep Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1"&gt;Byron C. Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1"&gt;Kush R. Varshney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Path Representation Learning with Curriculum Negative Sampling. (arXiv:2106.09373v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09373</id>
        <link href="http://arxiv.org/abs/2106.09373"/>
        <updated>2021-06-18T02:06:36.391Z</updated>
        <summary type="html"><![CDATA[Path representations are critical in a variety of transportation
applications, such as estimating path ranking in path recommendation systems
and estimating path travel time in navigation systems. Existing studies often
learn task-specific path representations in a supervised manner, which require
a large amount of labeled training data and generalize poorly to other tasks.
We propose an unsupervised learning framework Path InfoMax (PIM) to learn
generic path representations that work for different downstream tasks. We first
propose a curriculum negative sampling method, for each input path, to generate
a small amount of negative paths, by following the principles of curriculum
learning. Next, \emph{PIM} employs mutual information maximization to learn
path representations from both a global and a local view. In the global view,
PIM distinguishes the representations of the input paths from those of the
negative paths. In the local view, \emph{PIM} distinguishes the input path
representations from the representations of the nodes that appear only in the
negative paths. This enables the learned path representations to encode both
global and local information at different scales. Extensive experiments on two
downstream tasks, ranking score estimation and travel time estimation, using
two road network datasets suggest that PIM significantly outperforms other
unsupervised methods and is also able to be used as a pre-training method to
enhance supervised path representation learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sean Bin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Chenjuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jilin Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC-Bayes, MAC-Bayes and Conditional Mutual Information: Fast rate bounds that handle general VC classes. (arXiv:2106.09683v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09683</id>
        <link href="http://arxiv.org/abs/2106.09683"/>
        <updated>2021-06-18T02:06:36.382Z</updated>
        <summary type="html"><![CDATA[We give a novel, unified derivation of conditional PAC-Bayesian and mutual
information (MI) generalization bounds. We derive conditional MI bounds as an
instance, with special choice of prior, of conditional MAC-Bayesian (Mean
Approximately Correct) bounds, itself derived from conditional PAC-Bayesian
bounds, where `conditional' means that one can use priors conditioned on a
joint training and ghost sample. This allows us to get nontrivial PAC-Bayes and
MI-style bounds for general VC classes, something recently shown to be
impossible with standard PAC-Bayesian/MI bounds. Second, it allows us to get
faster rates of order $O \left(({\text{KL}}/n)^{\gamma}\right)$ for $\gamma >
1/2$ if a Bernstein condition holds and for exp-concave losses (with
$\gamma=1$), which is impossible with both standard PAC-Bayes generalization
and MI bounds. Our work extends the recent work by Steinke and Zakynthinou
[2020] who handle MI with VC but neither PAC-Bayes nor fast rates, the recent
work of Hellstr\"om and Durisi [2020] who extend the latter to the PAC-Bayes
setting via a unifying exponential inequality, and Mhammedi et al. [2019] who
initiated fast rate PAC-Bayes generalization error bounds but handle neither MI
nor general VC classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grunwald_P/0/1/0/all/0/1"&gt;Peter Gr&amp;#xfc;nwald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1"&gt;Thomas Steinke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zakynthinou_L/0/1/0/all/0/1"&gt;Lydia Zakynthinou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[mPyPl: Python Monadic Pipeline Library for Complex Functional Data Processing. (arXiv:2106.09164v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2106.09164</id>
        <link href="http://arxiv.org/abs/2106.09164"/>
        <updated>2021-06-18T02:06:36.371Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a new Python library called mPyPl, which is
intended to simplify complex data processing tasks using functional approach.
This library defines operations on lazy data streams of named dictionaries
represented as generators (so-called multi-field datastreams), and allows
enriching those data streams with more 'fields' in the process of data
preparation and feature extraction. Thus, most data preparation tasks can be
expressed in the form of neat linear 'pipeline', similar in syntax to UNIX
pipes, or |> functional composition operator in F#.

We define basic operations on multi-field data streams, which resemble
classical monadic operations, and show similarity of the proposed approach to
monads in functional programming. We also show how the library was used in
complex deep learning tasks of event detection in video, and discuss different
evaluation strategies that allow for different compromises in terms of memory
and performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soshnikov_D/0/1/0/all/0/1"&gt;Dmitry Soshnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valieva_Y/0/1/0/all/0/1"&gt;Yana Valieva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joining datasets via data augmentation in the label space for neural networks. (arXiv:2106.09260v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09260</id>
        <link href="http://arxiv.org/abs/2106.09260"/>
        <updated>2021-06-18T02:06:36.351Z</updated>
        <summary type="html"><![CDATA[Most, if not all, modern deep learning systems restrict themselves to a
single dataset for neural network training and inference. In this article, we
are interested in systematic ways to join datasets that are made of similar
purposes. Unlike previous published works that ubiquitously conduct the dataset
joining in the uninterpretable latent vectorial space, the core to our method
is an augmentation procedure in the label space. The primary challenge to
address the label space for dataset joining is the discrepancy between labels:
non-overlapping label annotation sets, different labeling granularity or
hierarchy and etc. Notably we propose a new technique leveraging artificially
created knowledge graph, recurrent neural networks and policy gradient that
successfully achieve the dataset joining in the label space. Empirical results
on both image and text classification justify the validity of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jake Zhao&lt;/a&gt; (Junbo), &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_M/0/1/0/all/0/1"&gt;Mingfeng Ou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1"&gt;Linji Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yunkai Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Gang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uniform Convergence of Interpolators: Gaussian Width, Norm Bounds, and Benign Overfitting. (arXiv:2106.09276v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.09276</id>
        <link href="http://arxiv.org/abs/2106.09276"/>
        <updated>2021-06-18T02:06:36.344Z</updated>
        <summary type="html"><![CDATA[We consider interpolation learning in high-dimensional linear regression with
Gaussian data, and prove a generic uniform convergence guarantee on the
generalization error of interpolators in an arbitrary hypothesis class in terms
of the class's Gaussian width. Applying the generic bound to Euclidean norm
balls recovers the consistency result of Bartlett et al. (2020) for
minimum-norm interpolators, and confirms a prediction of Zhou et al. (2020) for
near-minimal-norm interpolators in the special case of Gaussian data. We
demonstrate the generality of the bound by applying it to the simplex,
obtaining a novel consistency result for minimum l1-norm interpolators (basis
pursuit). Our results show how norm-based generalization bounds can explain and
be used to analyze benign overfitting, at least in some settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Koehler_F/0/1/0/all/0/1"&gt;Frederic Koehler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Lijia Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1"&gt;Danica J. Sutherland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Srebro_N/0/1/0/all/0/1"&gt;Nathan Srebro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can I Be of Further Assistance? Using Unstructured Knowledge Access to Improve Task-oriented Conversational Modeling. (arXiv:2106.09174v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09174</id>
        <link href="http://arxiv.org/abs/2106.09174"/>
        <updated>2021-06-18T02:06:36.337Z</updated>
        <summary type="html"><![CDATA[Most prior work on task-oriented dialogue systems are restricted to limited
coverage of domain APIs. However, users oftentimes have requests that are out
of the scope of these APIs. This work focuses on responding to these
beyond-API-coverage user turns by incorporating external, unstructured
knowledge sources. Our approach works in a pipelined manner with
knowledge-seeking turn detection, knowledge selection, and response generation
in sequence. We introduce novel data augmentation methods for the first two
steps and demonstrate that the use of information extracted from dialogue
context improves the knowledge selection and end-to-end performances. Through
experiments, we achieve state-of-the-art performance for both automatic and
human evaluation metrics on the DSTC9 Track 1 benchmark dataset, validating the
effectiveness of our contributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1"&gt;Di Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seokhwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1"&gt;Dilek Hakkani-Tur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple Generative Network. (arXiv:2106.09330v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09330</id>
        <link href="http://arxiv.org/abs/2106.09330"/>
        <updated>2021-06-18T02:06:36.331Z</updated>
        <summary type="html"><![CDATA[Generative neural networks are able to mimic intricate probability
distributions such as those of handwritten text, natural images, etc. Since
their inception several models were proposed. The most successful of these were
based on adversarial (GAN), auto-encoding (VAE) and maximum mean discrepancy
(MMD) relatively complex architectures and schemes. Surprisingly, a very simple
architecture (a single feed-forward neural network) in conjunction with an
obvious optimization goal (Kullback_Leibler divergence) was apparently
overlooked. This paper demonstrates that such a model (denoted SGN for its
simplicity) is able to generate samples visually and quantitatively competitive
as compared with the fore-mentioned state of the art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nissani_D/0/1/0/all/0/1"&gt;Daniel N. Nissani&lt;/a&gt; (Nissensohn)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-informed CoKriging model of a redox flow battery. (arXiv:2106.09188v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.09188</id>
        <link href="http://arxiv.org/abs/2106.09188"/>
        <updated>2021-06-18T02:06:36.324Z</updated>
        <summary type="html"><![CDATA[Redox flow batteries (RFBs) offer the capability to store large amounts of
energy cheaply and efficiently, however, there is a need for fast and accurate
models of the charge-discharge curve of a RFB to potentially improve the
battery capacity and performance. We develop a multifidelity model for
predicting the charge-discharge curve of a RFB. In the multifidelity model, we
use the Physics-informed CoKriging (CoPhIK) machine learning method that is
trained on experimental data and constrained by the so-called
"zero-dimensional" physics-based model. Here we demonstrate that the model
shows good agreement with experimental results and significant improvements
over existing zero-dimensional models. We show that the proposed model is
robust as it is not sensitive to the input parameters in the zero-dimensional
model. We also show that only a small amount of high-fidelity experimental
datasets are needed for accurate predictions for the range of considered input
parameters, which include current density, flow rate, and initial
concentrations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Howard_A/0/1/0/all/0/1"&gt;Amanda A. Howard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tartakovsky_A/0/1/0/all/0/1"&gt;Alexandre M. Tartakovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CROP: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing. (arXiv:2106.09292v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09292</id>
        <link href="http://arxiv.org/abs/2106.09292"/>
        <updated>2021-06-18T02:06:36.304Z</updated>
        <summary type="html"><![CDATA[We present the first framework of Certifying Robust Policies for
reinforcement learning (CROP) against adversarial state perturbations. We
propose two particular types of robustness certification criteria: robustness
of per-state actions and lower bound of cumulative rewards. Specifically, we
develop a local smoothing algorithm which uses a policy derived from
Q-functions smoothed with Gaussian noise over each encountered state to
guarantee the robustness of actions taken along this trajectory. Next, we
develop a global smoothing algorithm for certifying the robustness of a
finite-horizon cumulative reward under adversarial state perturbations.
Finally, we propose a local smoothing approach which makes use of adaptive
search in order to obtain tight certification bounds for reward. We use the
proposed RL robustness certification framework to evaluate six methods that
have previously been shown to yield empirically robust RL, including
adversarial training and several forms of regularization, on two representative
Atari games. We show that RegPGD, RegCVX, and RadialRL achieve high certified
robustness among these. Furthermore, we demonstrate that our certifications are
often tight by evaluating these algorithms against adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zijian Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1"&gt;Yevgeniy Vorobeychik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepSplit: Scalable Verification of Deep Neural Networks via Operator Splitting. (arXiv:2106.09117v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09117</id>
        <link href="http://arxiv.org/abs/2106.09117"/>
        <updated>2021-06-18T02:06:36.297Z</updated>
        <summary type="html"><![CDATA[Analyzing the worst-case performance of deep neural networks against input
perturbations amounts to solving a large-scale non-convex optimization problem,
for which several past works have proposed convex relaxations as a promising
alternative. However, even for reasonably-sized neural networks, these
relaxations are not tractable, and so must be replaced by even weaker
relaxations in practice. In this work, we propose a novel operator splitting
method that can directly solve a convex relaxation of the problem to high
accuracy, by splitting it into smaller sub-problems that often have analytical
solutions. The method is modular and scales to problem instances that were
previously impossible to solve exactly due to their size. Furthermore, the
solver operations are amenable to fast parallelization with GPU acceleration.
We demonstrate our method in obtaining tighter bounds on the worst-case
performance of large convolutional networks in image classification and
reinforcement learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shaoru Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1"&gt;Eric Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1"&gt;J. Zico Kolter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazlyab_M/0/1/0/all/0/1"&gt;Mahyar Fazlyab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Fishnet Open Images Database: A Dataset for Fish Detection and Fine-Grained Categorization in Fisheries. (arXiv:2106.09178v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09178</id>
        <link href="http://arxiv.org/abs/2106.09178"/>
        <updated>2021-06-18T02:06:36.290Z</updated>
        <summary type="html"><![CDATA[Camera-based electronic monitoring (EM) systems are increasingly being
deployed onboard commercial fishing vessels to collect essential data for
fisheries management and regulation. These systems generate large quantities of
video data which must be reviewed on land by human experts. Computer vision can
assist this process by automatically detecting and classifying fish species,
however the lack of existing public data in this domain has hindered progress.
To address this, we present the Fishnet Open Images Database, a large dataset
of EM imagery for fish detection and fine-grained categorization onboard
commercial fishing vessels. The dataset consists of 86,029 images containing 34
object classes, making it the largest and most diverse public dataset of
fisheries EM imagery to-date. It includes many of the characteristic challenges
of EM data: visual similarity between species, skewed class distributions,
harsh weather conditions, and chaotic crew activity. We evaluate the
performance of existing detection and classification algorithms and demonstrate
that the dataset can serve as a challenging benchmark for development of
computer vision algorithms in fisheries. The dataset is available at
https://www.fishnet.ai/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kay_J/0/1/0/all/0/1"&gt;Justin Kay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merrifield_M/0/1/0/all/0/1"&gt;Matt Merrifield&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amortized Auto-Tuning: Cost-Efficient Transfer Optimization for Hyperparameter Recommendation. (arXiv:2106.09179v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09179</id>
        <link href="http://arxiv.org/abs/2106.09179"/>
        <updated>2021-06-18T02:06:36.281Z</updated>
        <summary type="html"><![CDATA[With the surge in the number of hyperparameters and training times of modern
machine learning models, hyperparameter tuning is becoming increasingly
expensive. Although methods have been proposed to speed up tuning via knowledge
transfer, they typically require the final performance of hyperparameters and
do not focus on low-fidelity information. Nevertheless, this common practice is
suboptimal and can incur an unnecessary use of resources. It is more
cost-efficient to instead leverage the low-fidelity tuning observations to
measure inter-task similarity and transfer knowledge from existing to new tasks
accordingly. However, performing multi-fidelity tuning comes with its own
challenges in the transfer setting: the noise in the additional observations
and the need for performance forecasting. Therefore, we conduct a thorough
analysis of the multi-task multi-fidelity Bayesian optimization framework,
which leads to the best instantiation--amortized auto-tuning (AT2). We further
present an offline-computed 27-task hyperparameter recommendation (HyperRec)
database to serve the community. Extensive experiments on HyperRec and other
real-world databases illustrate the effectiveness of our AT2 method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yuxin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"&gt;Eric P. Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1"&gt;Willie Neiswanger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dr-Vectors: Decision Residual Networks and an Improved Loss for Speaker Recognition. (arXiv:2104.01989v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01989</id>
        <link href="http://arxiv.org/abs/2104.01989"/>
        <updated>2021-06-18T02:06:36.274Z</updated>
        <summary type="html"><![CDATA[Many neural network speaker recognition systems model each speaker using a
fixed-dimensional embedding vector. These embeddings are generally compared
using either linear or 2nd-order scoring and, until recently, do not handle
utterance-specific uncertainty. In this work we propose scoring these
representations in a way that can capture uncertainty, enroll/test asymmetry
and additional non-linear information. This is achieved by incorporating a
2nd-stage neural network (known as a decision network) as part of an end-to-end
training regimen. In particular, we propose the concept of decision residual
networks which involves the use of a compact decision network to leverage
cosine scores and to model the residual signal that's needed. Additionally, we
present a modification to the generalized end-to-end softmax loss function to
target the separation of same/different speaker scores. We observed significant
performance gains for the two techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pelecanos_J/0/1/0/all/0/1"&gt;Jason Pelecanos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Quan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreno_I/0/1/0/all/0/1"&gt;Ignacio Lopez Moreno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven control of room temperature and bidirectional EV charging using deep reinforcement learning: simulations and experiments. (arXiv:2103.01886v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01886</id>
        <link href="http://arxiv.org/abs/2103.01886"/>
        <updated>2021-06-18T02:06:36.266Z</updated>
        <summary type="html"><![CDATA[This work presents a fully data-driven, black-box pipeline to obtain an
optimal control policy for a multi-loop building control problem based on
historical building and weather data, thus without the need for complex
physics-based modelling. We demonstrate the method for joint control of room
temperature and bidirectional EV charging to maximize the occupant thermal
comfort and energy savings while leaving enough energy in the EV battery for
the next trip. We modelled the room temperature with a recurrent neural network
and EV charging with a piece-wise linear function. Using these models as a
simulation environment, we applied a deep reinforcement learning (DRL)
algorithm to obtain an optimal control policy. The learnt policy achieves on
average 17% energy savings over the heating season and 19% better comfort
satisfaction than a standard RB room temperature controller. When a
bidirectional EV is additionally connected and a two-tariff electricity pricing
is applied, the MIMO DRL policy successfully leverages the battery and
decreases the overall cost of electricity compared to two standard RB
controllers, one controlling the room temperature and another controlling the
bidirectional EV (dis-)charging. Finally, we demonstrate a successful transfer
of the learnt DRL policy from simulation onto a real building, the DFAB HOUSE
at Empa Duebendorf in Switzerland, achieving up to 30% energy savings while
maintaining similar comfort levels compared to a conventional RB room
temperature controller over three weeks during the heating season.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Svetozarevic_B/0/1/0/all/0/1"&gt;B. Svetozarevic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baumann_C/0/1/0/all/0/1"&gt;C.Baumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muntwiler_S/0/1/0/all/0/1"&gt;S. Muntwiler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natale_L/0/1/0/all/0/1"&gt;L. Di Natale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeilinger_M/0/1/0/all/0/1"&gt;M. Zeilinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heer_P/0/1/0/all/0/1"&gt;P. Heer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantized Federated Learning under Transmission Delay and Outage Constraints. (arXiv:2106.09397v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.09397</id>
        <link href="http://arxiv.org/abs/2106.09397"/>
        <updated>2021-06-18T02:06:36.245Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has been recognized as a viable distributed learning
paradigm which trains a machine learning model collaboratively with massive
mobile devices in the wireless edge while protecting user privacy. Although
various communication schemes have been proposed to expedite the FL process,
most of them have assumed ideal wireless channels which provide reliable and
lossless communication links between the server and mobile clients.
Unfortunately, in practical systems with limited radio resources such as
constraint on the training latency and constraints on the transmission power
and bandwidth, transmission of a large number of model parameters inevitably
suffers from quantization errors (QE) and transmission outage (TO). In this
paper, we consider such non-ideal wireless channels, and carry out the first
analysis showing that the FL convergence can be severely jeopardized by TO and
QE, but intriguingly can be alleviated if the clients have uniform outage
probabilities. These insightful results motivate us to propose a robust FL
scheme, named FedTOE, which performs joint allocation of wireless resources and
quantization bits across the clients to minimize the QE while making the
clients have the same TO probability. Extensive experimental results are
presented to show the superior performance of FedTOE for a deep learning-based
classification task with transmission latency constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanmeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanqing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1"&gt;Qingjiang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1"&gt;Tsung-Hui Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Validity of Modeling SGD with Stochastic Differential Equations (SDEs). (arXiv:2102.12470v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12470</id>
        <link href="http://arxiv.org/abs/2102.12470"/>
        <updated>2021-06-18T02:06:36.238Z</updated>
        <summary type="html"><![CDATA[It is generally recognized that finite learning rate (LR), in contrast to
infinitesimal LR, is important for good generalization in real-life deep nets.
Most attempted explanations propose approximating finite-LR SGD with Ito
Stochastic Differential Equations (SDEs), but formal justification for this
approximation (e.g., (Li et al., 2019)) only applies to SGD with tiny LR.
Experimental verification of the approximation appears computationally
infeasible. The current paper clarifies the picture with the following
contributions: (a) An efficient simulation algorithm SVAG that provably
converges to the conventionally used Ito SDE approximation. (b) A theoretically
motivated testable necessary condition for the SDE approximation and its most
famous implication, the linear scaling rule (Goyal et al., 2017), to hold. (c)
Experiments using this simulation to demonstrate that the previously proposed
SDE approximation can meaningfully capture the training and generalization
properties of common deep nets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malladi_S/0/1/0/all/0/1"&gt;Sadhika Malladi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1"&gt;Sanjeev Arora&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Training Data Generation of Handwritten Formulas using Generative Adversarial Networks with Self-Attention. (arXiv:2106.09432v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09432</id>
        <link href="http://arxiv.org/abs/2106.09432"/>
        <updated>2021-06-18T02:06:36.231Z</updated>
        <summary type="html"><![CDATA[The recognition of handwritten mathematical expressions in images and video
frames is a difficult and unsolved problem yet. Deep convectional neural
networks are basically a promising approach, but typically require a large
amount of labeled training data. However, such a large training dataset does
not exist for the task of handwritten formula recognition. In this paper, we
introduce a system that creates a large set of synthesized training examples of
mathematical expressions which are derived from LaTeX documents. For this
purpose, we propose a novel attention-based generative adversarial network to
translate rendered equations to handwritten formulas. The datasets generated by
this approach contain hundreds of thousands of formulas, making it ideal for
pretraining or the design of more complex models. We evaluate our synthesized
dataset and the recognition approach on the CROHME 2014 benchmark dataset.
Experimental results demonstrate the feasibility of the approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Springstein_M/0/1/0/all/0/1"&gt;Matthias Springstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1"&gt;Eric M&amp;#xfc;ller-Budack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1"&gt;Ralph Ewerth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Machine Learning Classifiers for Brain Tumour Survival Prediction. (arXiv:2106.09424v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09424</id>
        <link href="http://arxiv.org/abs/2106.09424"/>
        <updated>2021-06-18T02:06:36.222Z</updated>
        <summary type="html"><![CDATA[Prediction of survival in patients diagnosed with a brain tumour is
challenging because of heterogeneous tumour behaviours and responses to
treatment. Better estimations of prognosis would support treatment planning and
patient support. Advances in machine learning have informed development of
clinical predictive models, but their integration into clinical practice is
almost non-existent. One reasons for this is the lack of interpretability of
models. In this paper, we use a novel brain tumour dataset to compare two
interpretable rule list models against popular machine learning approaches for
brain tumour survival prediction. All models are quantitatively evaluated using
standard performance metrics. The rule lists are also qualitatively assessed
for their interpretability and clinical utility. The interpretability of the
black box machine learning models is evaluated using two post-hoc explanation
techniques, LIME and SHAP. Our results show that the rule lists were only
slightly outperformed by the black box models. We demonstrate that rule list
algorithms produced simple decision lists that align with clinical expertise.
By comparison, post-hoc interpretability methods applied to black box models
may produce unreliable explanations of local model predictions. Model
interpretability is essential for understanding differences in predictive
performance and for integration into clinical practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Charlton_C/0/1/0/all/0/1"&gt;Colleen E. Charlton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_M/0/1/0/all/0/1"&gt;Michael Tin Chung Poon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brennan_P/0/1/0/all/0/1"&gt;Paul M. Brennan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleuriot_J/0/1/0/all/0/1"&gt;Jacques D. Fleuriot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing Differently, Acting Similarly: Imitation Learning with Heterogeneous Observations. (arXiv:2106.09256v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09256</id>
        <link href="http://arxiv.org/abs/2106.09256"/>
        <updated>2021-06-18T02:06:36.215Z</updated>
        <summary type="html"><![CDATA[In many real-world imitation learning tasks, the demonstrator and the learner
have to act in different but full observation spaces. This situation generates
significant obstacles for existing imitation learning approaches to work, even
when they are combined with traditional space adaptation techniques. The main
challenge lies in bridging expert's occupancy measures to learner's dynamically
changing occupancy measures under the different observation spaces. In this
work, we model the above learning problem as Heterogeneous Observations
Imitation Learning (HOIL). We propose the Importance Weighting with REjection
(IWRE) algorithm based on the techniques of importance-weighting, learning with
rejection, and active querying to solve the key challenge of occupancy measure
matching. Experimental results show that IWRE can successfully solve HOIL
tasks, including the challenging task of transforming the vision-based
demonstrations to random access memory (RAM)-based policies under the Atari
domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1"&gt;Xin-Qiang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yao-Xiang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zi-Xuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhi-Hua Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Imprecise SHAP as a Tool for Explaining the Class Probability Distributions under Limited Training Data. (arXiv:2106.09111v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09111</id>
        <link href="http://arxiv.org/abs/2106.09111"/>
        <updated>2021-06-18T02:06:36.196Z</updated>
        <summary type="html"><![CDATA[One of the most popular methods of the machine learning prediction
explanation is the SHapley Additive exPlanations method (SHAP). An imprecise
SHAP as a modification of the original SHAP is proposed for cases when the
class probability distributions are imprecise and represented by sets of
distributions. The first idea behind the imprecise SHAP is a new approach for
computing the marginal contribution of a feature, which fulfils the important
efficiency property of Shapley values. The second idea is an attempt to
consider a general approach to calculating and reducing interval-valued Shapley
values, which is similar to the idea of reachable probability intervals in the
imprecise probability theory. A simple special implementation of the general
approach in the form of linear optimization problems is proposed, which is
based on using the Kolmogorov-Smirnov distance and imprecise contamination
models. Numerical examples with synthetic and real data illustrate the
imprecise SHAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Utkin_L/0/1/0/all/0/1"&gt;Lev V. Utkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konstantinov_A/0/1/0/all/0/1"&gt;Andrei V. Konstantinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vishniakov_K/0/1/0/all/0/1"&gt;Kirill A. Vishniakov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Insights into Data through Model Behaviour: An Explainability-driven Strategy for Data Auditing for Responsible Computer Vision Applications. (arXiv:2106.09177v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09177</id>
        <link href="http://arxiv.org/abs/2106.09177"/>
        <updated>2021-06-18T02:06:36.189Z</updated>
        <summary type="html"><![CDATA[In this study, we take a departure and explore an explainability-driven
strategy to data auditing, where actionable insights into the data at hand are
discovered through the eyes of quantitative explainability on the behaviour of
a dummy model prototype when exposed to data. We demonstrate this strategy by
auditing two popular medical benchmark datasets, and discover hidden data
quality issues that lead deep learning models to make predictions for the wrong
reasons. The actionable insights gained from this explainability driven data
auditing strategy is then leveraged to address the discovered issues to enable
the creation of high-performing deep learning models with appropriate
prediction behaviour. The hope is that such an explainability-driven strategy
can be complimentary to data-driven strategies to facilitate for more
responsible development of machine learning algorithms for computer vision
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dorfman_A/0/1/0/all/0/1"&gt;Adam Dorfman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McInnis_P/0/1/0/all/0/1"&gt;Paul McInnis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunraj_H/0/1/0/all/0/1"&gt;Hayden Gunraj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Popularity of Reddit Posts with AI. (arXiv:2106.07380v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07380</id>
        <link href="http://arxiv.org/abs/2106.07380"/>
        <updated>2021-06-18T02:06:36.182Z</updated>
        <summary type="html"><![CDATA[Social media creates crucial mass changes, as popular posts and opinions cast
a significant influence on users' decisions and thought processes. For example,
the recent Reddit uprising inspired by r/wallstreetbets which had remarkable
economic impact was started with a series of posts on the thread. The
prediction of posts that may have a notable impact will allow for the
preparation of possible following trends. This study aims to develop a machine
learning model capable of accurately predicting the popularity of a Reddit
post. Specifically, the model is predicting the number of upvotes a post will
receive based on its textual content. I experimented with three different
models: a baseline linear regression model, a random forest regression model,
and a neural network. I collected Reddit post data from an online data set and
analyzed the model's performance when trained on a single subreddit and a
collection of subreddits. The results showed that the neural network model
performed the best when the loss of the models were compared. With the use of a
machine learning model to predict social trends through the reaction users have
to post, a better picture of the near future can be envisioned.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Juno Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Shape Rewards using a Game of Switching Controls. (arXiv:2103.09159v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09159</id>
        <link href="http://arxiv.org/abs/2103.09159"/>
        <updated>2021-06-18T02:06:36.175Z</updated>
        <summary type="html"><![CDATA[Reward shaping (RS) is a powerful method in reinforcement learning (RL) for
overcoming the problem of sparse or uninformative rewards. However, RS
typically relies on manually engineered shaping-reward functions whose
construction is time-consuming and error-prone. It also requires domain
knowledge which runs contrary to the goal of autonomous learning. We introduce
Reinforcement Learning Optimal Shaping Algorithm (ROSA), an automated RS
framework in which the shaping-reward function is constructed in a novel Markov
game between two agents. A reward-shaping agent (Shaper) uses switching
controls to determine which states to add shaping rewards and their optimal
values while the other agent (Controller) learns the optimal policy for the
task using these shaped rewards. We prove that ROSA, which easily adopts
existing RL algorithms, learns to construct a shaping-reward function that is
tailored to the task thus ensuring efficient convergence to high performance
policies. We demonstrate ROSA's congenial properties in three carefully
designed experiments and show its superior performance against state-of-the-art
RS algorithms in challenging sparse reward environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mguni_D/0/1/0/all/0/1"&gt;David Mguni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jafferjee_T/0/1/0/all/0/1"&gt;Taher Jafferjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Nieves_N/0/1/0/all/0/1"&gt;Nicolas Perez-Nieves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1"&gt;Wenbin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yaodong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_F/0/1/0/all/0/1"&gt;Feifei Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiangcheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated CycleGAN for Privacy-Preserving Image-to-Image Translation. (arXiv:2106.09246v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09246</id>
        <link href="http://arxiv.org/abs/2106.09246"/>
        <updated>2021-06-18T02:06:36.169Z</updated>
        <summary type="html"><![CDATA[Unsupervised image-to-image translation methods such as CycleGAN learn to
convert images from one domain to another using unpaired training data sets
from different domains. Unfortunately, these approaches still require centrally
collected unpaired records, potentially violating privacy and security issues.
Although the recent federated learning (FL) allows a neural network to be
trained without data exchange, the basic assumption of the FL is that all
clients have their own training data from a similar domain, which is different
from our image-to-image translation scenario in which each client has images
from its unique domain and the goal is to learn image translation between
different domains without accessing the target domain data. To address this,
here we propose a novel federated CycleGAN architecture that can learn image
translation in an unsupervised manner while maintaining the data privacy.
Specifically, our approach arises from a novel observation that CycleGAN loss
can be decomposed into the sum of client specific local objectives that can be
evaluated using only their data. This local objective decomposition allows
multiple clients to participate in federated CycleGAN training without
sacrificing performance. Furthermore, our method employs novel switchable
generator and discriminator architecture using Adaptive Instance Normalization
(AdaIN) that significantly reduces the band-width requirement of the federated
learning. Our experimental results on various unsupervised image translation
tasks show that our federated CycleGAN provides comparable performance compared
to the non-federated counterpart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Joonyoung Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual Policies. (arXiv:2106.09678v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09678</id>
        <link href="http://arxiv.org/abs/2106.09678"/>
        <updated>2021-06-18T02:06:36.144Z</updated>
        <summary type="html"><![CDATA[Generalization has been a long-standing challenge for reinforcement learning
(RL). Visual RL, in particular, can be easily distracted by irrelevant factors
in high-dimensional observation space. In this work, we consider robust policy
learning which targets zero-shot generalization to unseen visual environments
with large distributional shift. We propose SECANT, a novel self-expert cloning
technique that leverages image augmentation in two stages to decouple robust
representation learning from policy optimization. Specifically, an expert
policy is first trained by RL from scratch with weak augmentations. A student
network then learns to mimic the expert policy by supervised learning with
strong augmentations, making its representation more robust against visual
variations compared to the expert. Extensive experiments demonstrate that
SECANT significantly advances the state of the art in zero-shot generalization
across 4 challenging domains. Our average reward improvements over prior SOTAs
are: DeepMind Control (+26.5%), robotic manipulation (+337.8%), vision-based
autonomous driving (+47.7%), and indoor object navigation (+15.8%). Code
release and video are available at https://linxifan.github.io/secant-site/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Linxi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;De-An Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MHNF: Multi-hop Heterogeneous Neighborhood information Fusion graph representation learning. (arXiv:2106.09289v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09289</id>
        <link href="http://arxiv.org/abs/2106.09289"/>
        <updated>2021-06-18T02:06:36.137Z</updated>
        <summary type="html"><![CDATA[Attention mechanism enables the Graph Neural Networks(GNNs) to learn the
attention weights between the target node and its one-hop neighbors, the
performance is further improved. However, the most existing GNNs are oriented
to homogeneous graphs and each layer can only aggregate the information of
one-hop neighbors. Stacking multi-layer networks will introduce a lot of noise
and easily lead to over smoothing. We propose a Multi-hop Heterogeneous
Neighborhood information Fusion graph representation learning method (MHNF).
Specifically, we first propose a hybrid metapath autonomous extraction model to
efficiently extract multi-hop hybrid neighbors. Then, we propose a hop-level
heterogeneous Information aggregation model, which selectively aggregates
different-hop neighborhood information within the same hybrid metapath.
Finally, a hierarchical semantic attention fusion model (HSAF) is proposed,
which can efficiently integrate different-hop and different-path neighborhood
information respectively. This paper can solve the problem of aggregating the
multi-hop neighborhood information and can learn hybrid metapaths for target
task, reducing the limitation of manually specifying metapaths. In addition,
HSAF can extract the internal node information of the metapaths and better
integrate the semantic information of different levels. Experimental results on
real datasets show that MHNF is superior to state-of-the-art methods in node
classification and clustering tasks (10.94% - 69.09% and 11.58% - 394.93%
relative improvement on average, respectively).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1"&gt;Dongjie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yundong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1"&gt;Haiwen Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zhaoshuo Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Languages in the Limit from Positive Information with Finitely Many Memory Changes. (arXiv:2010.04782v3 [cs.FL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04782</id>
        <link href="http://arxiv.org/abs/2010.04782"/>
        <updated>2021-06-18T02:06:36.130Z</updated>
        <summary type="html"><![CDATA[We investigate learning collections of languages from texts by an inductive
inference machine with access to the current datum and a bounded memory in form
of states. Such a bounded memory states (BMS) learner is considered successful
in case it eventually settles on a correct hypothesis while exploiting only
finitely many different states.

We give the complete map of all pairwise relations for an established
collection of criteria of successfull learning. Most prominently, we show that
non-U-shapedness is not restrictive, while conservativeness and (strong)
monotonicity are. Some results carry over from iterative learning by a general
lemma showing that, for a wealth of restrictions (the semantic restrictions),
iterative and bounded memory states learning are equivalent. We also give an
example of a non-semantic restriction (strongly non-U-shapedness) where the two
settings differ.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kotzing_T/0/1/0/all/0/1"&gt;Timo K&amp;#xf6;tzing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seidel_K/0/1/0/all/0/1"&gt;Karen Seidel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Behavioral Priors and Dynamics Models: Improving Performance and Domain Transfer in Offline RL. (arXiv:2106.09119v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09119</id>
        <link href="http://arxiv.org/abs/2106.09119"/>
        <updated>2021-06-18T02:06:36.122Z</updated>
        <summary type="html"><![CDATA[Offline Reinforcement Learning (RL) aims to extract near-optimal policies
from imperfect offline data without additional environment interactions.
Extracting policies from diverse offline datasets has the potential to expand
the range of applicability of RL by making the training process safer, faster,
and more streamlined. We investigate how to improve the performance of offline
RL algorithms, its robustness to the quality of offline data, as well as its
generalization capabilities. To this end, we introduce Offline Model-based RL
with Adaptive Behavioral Priors (MABE). Our algorithm is based on the finding
that dynamics models, which support within-domain generalization, and
behavioral priors, which support cross-domain generalization, are
complementary. When combined together, they substantially improve the
performance and generalization of offline RL policies. In the widely studied
D4RL offline RL benchmark, we find that MABE achieves higher average
performance compared to prior model-free and model-based algorithms. In
experiments that require cross-domain generalization, we find that MABE
outperforms prior methods. Our website is available at
https://sites.google.com/berkeley.edu/mabe .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cang_C/0/1/0/all/0/1"&gt;Catherine Cang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1"&gt;Aravind Rajeswaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laskin_M/0/1/0/all/0/1"&gt;Michael Laskin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Properties and Evolution of Neural Network Eigenspaces during Training. (arXiv:2106.09526v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09526</id>
        <link href="http://arxiv.org/abs/2106.09526"/>
        <updated>2021-06-18T02:06:36.115Z</updated>
        <summary type="html"><![CDATA[In this work we explore the information processing inside neural networks
using logistic regression probes \cite{probes} and the saturation metric
\cite{featurespace_saturation}. We show that problem difficulty and neural
network capacity affect the predictive performance in an antagonistic manner,
opening the possibility of detecting over- and under-parameterization of neural
networks for a given task. We further show that the observed effects are
independent from previously reported pathological patterns like the ``tail
pattern'' described in \cite{featurespace_saturation}. Finally we are able to
show that saturation patterns converge early during training, allowing for a
quicker cycle time during analysis]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krumnack_M/0/1/0/all/0/1"&gt;Mats L. Richter Leila Malihi Anne-Kathrin Patricia Windler Ulf Krumnack&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Winning Hand: Compressing Deep Networks Can Improve Out-Of-Distribution Robustness. (arXiv:2106.09129v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09129</id>
        <link href="http://arxiv.org/abs/2106.09129"/>
        <updated>2021-06-18T02:06:36.096Z</updated>
        <summary type="html"><![CDATA[Two crucial requirements for a successful adoption of deep learning (DL) in
the wild are: (1) robustness to distributional shifts, and (2) model
compactness for achieving efficiency. Unfortunately, efforts towards
simultaneously achieving Out-of-Distribution (OOD) robustness and extreme model
compactness without sacrificing accuracy have mostly been unsuccessful. This
raises an important question: "Is the inability to create compact, accurate,
and robust deep neural networks (CARDs) fundamental?" To answer this question,
we perform a large-scale analysis for a range of popular model compression
techniques which uncovers several intriguing patterns. Notably, in contrast to
traditional pruning approaches (e.g., fine tuning and gradual magnitude
pruning), we find that "lottery ticket-style" pruning approaches can
surprisingly be used to create high performing CARDs. Specifically, we are able
to create extremely compact CARDs that are dramatically more robust than their
significantly larger and full-precision counterparts while matching (or
beating) their test accuracy, simply by pruning and/or quantizing. To better
understand these differences, we perform sensitivity analysis in the Fourier
domain for CARDs trained using different data augmentation methods. Motivated
by our analysis, we develop a simple domain-adaptive test-time ensembling
approach (CARD-Deck) that uses a gating module to dynamically select an
appropriate CARD from the CARD-Deck based on their spectral-similarity with
test samples. By leveraging complementary frequency biases of different
compressed models, the proposed approach builds a "winning hand" of CARDs that
establishes a new state-of-the-art on CIFAR-10-C accuracies (i.e., 96.8% clean
and 92.75% robust) with dramatically better memory usage than their
non-compressed counterparts. We also present some theoretical evidences
supporting our empirical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diffenderfer_J/0/1/0/all/0/1"&gt;James Diffenderfer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartoldson_B/0/1/0/all/0/1"&gt;Brian R. Bartoldson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaganti_S/0/1/0/all/0/1"&gt;Shreya Chaganti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jize Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1"&gt;Bhavya Kailkhura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trainable Discrete Feature Embeddings for Variational Quantum Classifier. (arXiv:2106.09415v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.09415</id>
        <link href="http://arxiv.org/abs/2106.09415"/>
        <updated>2021-06-18T02:06:36.090Z</updated>
        <summary type="html"><![CDATA[Quantum classifiers provide sophisticated embeddings of input data in Hilbert
space promising quantum advantage. The advantage stems from quantum feature
maps encoding the inputs into quantum states with variational quantum circuits.
A recent work shows how to map discrete features with fewer quantum bits using
Quantum Random Access Coding (QRAC), an important primitive to encode binary
strings into quantum states. We propose a new method to embed discrete features
with trainable quantum circuits by combining QRAC and a recently proposed
strategy for training quantum feature map called quantum metric learning. We
show that the proposed trainable embedding requires not only as few qubits as
QRAC but also overcomes the limitations of QRAC to classify inputs whose
classes are based on hard Boolean functions. We numerically demonstrate its use
in variational quantum classifiers to achieve better performances in
classifying real-world datasets, and thus its possibility to leverage near-term
quantum computers for quantum machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Thumwanit_N/0/1/0/all/0/1"&gt;Napat Thumwanit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lortararprasert_C/0/1/0/all/0/1"&gt;Chayaphol Lortararprasert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Yano_H/0/1/0/all/0/1"&gt;Hiroshi Yano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Raymond_R/0/1/0/all/0/1"&gt;Rudy Raymond&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orthogonal-Pad\'e Activation Functions: Trainable Activation functions for smooth and faster convergence in deep networks. (arXiv:2106.09693v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.09693</id>
        <link href="http://arxiv.org/abs/2106.09693"/>
        <updated>2021-06-18T02:06:36.079Z</updated>
        <summary type="html"><![CDATA[We have proposed orthogonal-Pad\'e activation functions, which are trainable
activation functions and show that they have faster learning capability and
improves the accuracy in standard deep learning datasets and models. Based on
our experiments, we have found two best candidates out of six orthogonal-Pad\'e
activations, which we call safe Hermite-Pade (HP) activation functions, namely
HP-1 and HP-2. When compared to ReLU, HP-1 and HP-2 has an increment in top-1
accuracy by 5.06% and 4.63% respectively in PreActResNet-34, by 3.02% and 2.75%
respectively in MobileNet V2 model on CIFAR100 dataset while on CIFAR10 dataset
top-1 accuracy increases by 2.02% and 1.78% respectively in PreActResNet-34, by
2.24% and 2.06% respectively in LeNet, by 2.15% and 2.03% respectively in
Efficientnet B0.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1"&gt;Koushik Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1"&gt;Shilpak Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1"&gt;Ashish Kumar Pandey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time Series is a Special Sequence: Forecasting with Sample Convolution and Interaction. (arXiv:2106.09305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09305</id>
        <link href="http://arxiv.org/abs/2106.09305"/>
        <updated>2021-06-18T02:06:36.053Z</updated>
        <summary type="html"><![CDATA[Time series is a special type of sequence data, a set of observations
collected at even intervals of time and ordered chronologically. Existing deep
learning techniques use generic sequence models (e.g., recurrent neural
network, Transformer model, or temporal convolutional network) for time series
analysis, which ignore some of its unique properties. For example, the
downsampling of time series data often preserves most of the information in the
data, while this is not true for general sequence data such as text sequence
and DNA sequence. Motivated by the above, in this paper, we propose a novel
neural network architecture and apply it for the time series forecasting
problem, wherein we conduct sample convolution and interaction at multiple
resolutions for temporal modeling. The proposed architecture, namelySCINet,
facilitates extracting features with enhanced predictability. Experimental
results show that SCINet achieves significant prediction accuracy improvement
over existing solutions across various real-world time series forecasting
datasets. In particular, it can achieve high fore-casting accuracy for those
temporal-spatial datasets without using sophisticated spatial modeling
techniques. Our codes and data are presented in the supplemental material.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Minhao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Ailing Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Q/0/1/0/all/0/1"&gt;Qiuxia Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large Scale Private Learning via Low-rank Reparametrization. (arXiv:2106.09352v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09352</id>
        <link href="http://arxiv.org/abs/2106.09352"/>
        <updated>2021-06-18T02:06:36.026Z</updated>
        <summary type="html"><![CDATA[We propose a reparametrization scheme to address the challenges of applying
differentially private SGD on large neural networks, which are 1) the huge
memory cost of storing individual gradients, 2) the added noise suffering
notorious dimensional dependence. Specifically, we reparametrize each weight
matrix with two \emph{gradient-carrier} matrices of small dimension and a
\emph{residual weight} matrix. We argue that such reparametrization keeps the
forward/backward process unchanged while enabling us to compute the projected
gradient without computing the gradient itself. To learn with differential
privacy, we design \emph{reparametrized gradient perturbation (RGP)} that
perturbs the gradients on gradient-carrier matrices and reconstructs an update
for the original weight from the noisy gradients. Importantly, we use
historical updates to find the gradient-carrier matrices, whose optimality is
rigorously justified under linear regression and empirically verified with deep
learning tasks. RGP significantly reduces the memory cost and improves the
utility. For example, we are the first able to apply differential privacy on
the BERT model and achieve an average accuracy of $83.9\%$ on four downstream
tasks with $\epsilon=8$, which is within $5\%$ loss compared to the non-private
baseline but enjoys much lower privacy leakage risk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1"&gt;Da Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jian Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimum-statistical collaboration towards efficient black-box optimization. (arXiv:2106.09215v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.09215</id>
        <link href="http://arxiv.org/abs/2106.09215"/>
        <updated>2021-06-18T02:06:35.999Z</updated>
        <summary type="html"><![CDATA[With increasingly more hyperparameters involved in their training, machine
learning systems demand a better understanding of hyperparameter tuning
automation. This has raised interest in studies of provably black-box
optimization, which is made more practical by better exploration mechanism
implemented in algorithm design, managing the flux of both optimization and
statistical errors. Prior efforts focus on delineating optimization errors, but
this is deficient: black-box optimization algorithms can be inefficient without
considering heterogeneity among reward samples. In this paper, we make the key
delineation on the role of statistical uncertainty in black-box optimization,
guiding a more efficient algorithm design. We introduce
\textit{optimum-statistical collaboration}, a framework of managing the
interaction between optimization error flux and statistical error flux evolving
in the optimization process. Inspired by this framework, we propose the
\texttt{VHCT} algorithms for objective functions with only local-smoothness
assumptions. In theory, we prove our algorithm enjoys rate-optimal regret
bounds; in experiments, we show the algorithm outperforms prior efforts in
extensive settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chihua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guang Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invisible for both Camera and LiDAR: Security of Multi-Sensor Fusion based Perception in Autonomous Driving Under Physical-World Attacks. (arXiv:2106.09249v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.09249</id>
        <link href="http://arxiv.org/abs/2106.09249"/>
        <updated>2021-06-18T02:06:35.985Z</updated>
        <summary type="html"><![CDATA[In Autonomous Driving (AD) systems, perception is both security and safety
critical. Despite various prior studies on its security issues, all of them
only consider attacks on camera- or LiDAR-based AD perception alone. However,
production AD systems today predominantly adopt a Multi-Sensor Fusion (MSF)
based design, which in principle can be more robust against these attacks under
the assumption that not all fusion sources are (or can be) attacked at the same
time. In this paper, we present the first study of security issues of MSF-based
perception in AD systems. We directly challenge the basic MSF design assumption
above by exploring the possibility of attacking all fusion sources
simultaneously. This allows us for the first time to understand how much
security guarantee MSF can fundamentally provide as a general defense strategy
for AD perception.

We formulate the attack as an optimization problem to generate a
physically-realizable, adversarial 3D-printed object that misleads an AD system
to fail in detecting it and thus crash into it. We propose a novel attack
pipeline that addresses two main design challenges: (1) non-differentiable
target camera and LiDAR sensing systems, and (2) non-differentiable cell-level
aggregated features popularly used in LiDAR-based AD perception. We evaluate
our attack on MSF included in representative open-source industry-grade AD
systems in real-world driving scenarios. Our results show that the attack
achieves over 90% success rate across different object types and MSF. Our
attack is also found stealthy, robust to victim positions, transferable across
MSF algorithms, and physical-world realizable after being 3D-printed and
captured by LiDAR and camera devices. To concretely assess the end-to-end
safety impact, we further perform simulation evaluation and show that it can
cause a 100% vehicle collision rate for an industry-grade AD system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao%2A_Y/0/1/0/all/0/1"&gt;Yulong Cao*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang%2A_N/0/1/0/all/0/1"&gt;Ningfei Wang*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao%2A_C/0/1/0/all/0/1"&gt;Chaowei Xiao*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang%2A_D/0/1/0/all/0/1"&gt;Dawei Yang*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Ruigang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qi Alfred Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt; (*co-first authors)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Bias of SGD for Diagonal Linear Networks: a Provable Benefit of Stochasticity. (arXiv:2106.09524v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09524</id>
        <link href="http://arxiv.org/abs/2106.09524"/>
        <updated>2021-06-18T02:06:35.973Z</updated>
        <summary type="html"><![CDATA[Understanding the implicit bias of training algorithms is of crucial
importance in order to explain the success of overparametrised neural networks.
In this paper, we study the dynamics of stochastic gradient descent over
diagonal linear networks through its continuous time version, namely stochastic
gradient flow. We explicitly characterise the solution chosen by the stochastic
flow and prove that it always enjoys better generalisation properties than that
of gradient flow. Quite surprisingly, we show that the convergence speed of the
training loss controls the magnitude of the biasing effect: the slower the
convergence, the better the bias. To fully complete our analysis, we provide
convergence guarantees for the dynamics. We also give experimental results
which support our theoretical claims. Our findings highlight the fact that
structured noise can induce better generalisation and they help explain the
greater performances observed in practice of stochastic gradient descent over
gradient descent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pesme_S/0/1/0/all/0/1"&gt;Scott Pesme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pillaud_Vivien_L/0/1/0/all/0/1"&gt;Loucas Pillaud-Vivien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1"&gt;Nicolas Flammarion&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transductive Few-Shot Learning: Clustering is All You Need?. (arXiv:2106.09516v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09516</id>
        <link href="http://arxiv.org/abs/2106.09516"/>
        <updated>2021-06-18T02:06:35.965Z</updated>
        <summary type="html"><![CDATA[We investigate a general formulation for clustering and transductive few-shot
learning, which integrates prototype-based objectives, Laplacian regularization
and supervision constraints from a few labeled data points. We propose a
concave-convex relaxation of the problem, and derive a computationally
efficient block-coordinate bound optimizer, with convergence guarantee. At each
iteration,our optimizer computes independent (parallel) updates for each
point-to-cluster assignment. Therefore, it could be trivially distributed for
large-scale clustering and few-shot tasks. Furthermore, we provides a thorough
convergence analysis based on point-to-set maps. Were port comprehensive
clustering and few-shot learning experiments over various data sets, showing
that our method yields competitive performances, in term of accuracy and
optimization quality, while scaling up to large problems. Using standard
training on the base classes, without resorting to complex meta-learning and
episodic-training strategies, our approach outperforms state-of-the-art
few-shot methods by significant margins, across various models, settings and
data sets. Surprisingly, we found that even standard clustering procedures
(e.g., K-means), which correspond to particular, non-regularized cases of our
general model, already achieve competitive performances in comparison to the
state-of-the-art in few-shot learning. These surprising results point to the
limitations of the current few-shot benchmarks, and question the viability of a
large body of convoluted few-shot learning techniques in the recent literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziko_I/0/1/0/all/0/1"&gt;Imtiaz Masud Ziko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1"&gt;Malik Boudiaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1"&gt;Jose Dolz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1"&gt;Eric Granger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ismail Ben Ayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frustratingly Easy Transferability Estimation. (arXiv:2106.09362v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09362</id>
        <link href="http://arxiv.org/abs/2106.09362"/>
        <updated>2021-06-18T02:06:35.956Z</updated>
        <summary type="html"><![CDATA[Transferability estimation has been an essential tool in selecting a
pre-trained model and the layers of it to transfer, so as to maximize the
performance on a target task and prevent negative transfer. Existing estimation
algorithms either require intensive training on target tasks or have
difficulties in evaluating the transferability between layers. We propose a
simple, efficient, and effective transferability measure named TransRate. With
single pass through the target data, TransRate measures the transferability as
the mutual information between the features of target examples extracted by a
pre-trained model and labels of them. We overcome the challenge of efficient
mutual information estimation by resorting to coding rate that serves as an
effective alternative to entropy. TransRate is theoretically analyzed to be
closely related to the performance after transfer learning. Despite its
extraordinary simplicity in 10 lines of codes, TransRate performs remarkably
well in extensive evaluations on 22 pre-trained models and 16 downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Long-Kai Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Ying Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Correlation-Based Multiview Learning and Self-Supervision: A Unifying Perspective. (arXiv:2106.07115v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07115</id>
        <link href="http://arxiv.org/abs/2106.07115"/>
        <updated>2021-06-18T02:06:35.937Z</updated>
        <summary type="html"><![CDATA[Multiple views of data, both naturally acquired (e.g., image and audio) and
artificially produced (e.g., via adding different noise to data samples), have
proven useful in enhancing representation learning. Natural views are often
handled by multiview analysis tools, e.g., (deep) canonical correlation
analysis [(D)CCA], while the artificial ones are frequently used in
self-supervised learning (SSL) paradigms, e.g., SimCLR and Barlow Twins. Both
types of approaches often involve learning neural feature extractors such that
the embeddings of data exhibit high cross-view correlations. Although
intuitive, the effectiveness of correlation-based neural embedding is only
empirically validated. This work puts forth a theory-backed framework for
unsupervised multiview learning. Our development starts with proposing a
multiview model, where each view is a nonlinear mixture of shared and private
components. Consequently, the learning problem boils down to shared/private
component identification and disentanglement. Under this model, latent
correlation maximization is shown to guarantee the extraction of the shared
components across views (up to certain ambiguities). In addition, the private
information in each view can be provably disentangled from the shared using
proper regularization design. The method is tested on a series of tasks, e.g.,
downstream clustering, which all show promising performance. Our development
also provides a unifying perspective for understanding various DCCA and SSL
schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1"&gt;Qi Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiao Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Songtao Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Reinforcement Learning Using Advantage-Based Intervention. (arXiv:2106.09110v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09110</id>
        <link href="http://arxiv.org/abs/2106.09110"/>
        <updated>2021-06-18T02:06:35.930Z</updated>
        <summary type="html"><![CDATA[Many sequential decision problems involve finding a policy that maximizes
total reward while obeying safety constraints. Although much recent research
has focused on the development of safe reinforcement learning (RL) algorithms
that produce a safe policy after training, ensuring safety during training as
well remains an open problem. A fundamental challenge is performing exploration
while still satisfying constraints in an unknown Markov decision process (MDP).
In this work, we address this problem for the chance-constrained setting. We
propose a new algorithm, SAILR, that uses an intervention mechanism based on
advantage functions to keep the agent safe throughout training and optimizes
the agent's policy using off-the-shelf RL algorithms designed for unconstrained
MDPs. Our method comes with strong guarantees on safety during both training
and deployment (i.e., after training and without the intervention mechanism)
and policy performance compared to the optimal safety-constrained policy. In
our experiments, we show that SAILR violates constraints far less during
training than standard safe RL and constrained MDP approaches and converges to
a well-performing policy that can be deployed safely without intervention. Our
code is available at https://github.com/nolanwagener/safe_rl.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagener_N/0/1/0/all/0/1"&gt;Nolan Wagener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1"&gt;Byron Boots&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Ching-An Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CReST: A Class-Rebalancing Self-Training Framework for Imbalanced Semi-Supervised Learning. (arXiv:2102.09559v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09559</id>
        <link href="http://arxiv.org/abs/2102.09559"/>
        <updated>2021-06-18T02:06:35.924Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning on class-imbalanced data, although a realistic
problem, has been under studied. While existing semi-supervised learning (SSL)
methods are known to perform poorly on minority classes, we find that they
still generate high precision pseudo-labels on minority classes. By exploiting
this property, in this work, we propose Class-Rebalancing Self-Training
(CReST), a simple yet effective framework to improve existing SSL methods on
class-imbalanced data. CReST iteratively retrains a baseline SSL model with a
labeled set expanded by adding pseudo-labeled samples from an unlabeled set,
where pseudo-labeled samples from minority classes are selected more frequently
according to an estimated class distribution. We also propose a progressive
distribution alignment to adaptively adjust the rebalancing strength dubbed
CReST+. We show that CReST and CReST+ improve state-of-the-art SSL algorithms
on various class-imbalanced datasets and consistently outperform other popular
rebalancing methods. Code has been made available at
https://github.com/google-research/crest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Chen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kihyuk Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mellina_C/0/1/0/all/0/1"&gt;Clayton Mellina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifiability-Guaranteed Simplex-Structured Post-Nonlinear Mixture Learning via Autoencoder. (arXiv:2106.09070v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09070</id>
        <link href="http://arxiv.org/abs/2106.09070"/>
        <updated>2021-06-18T02:06:35.917Z</updated>
        <summary type="html"><![CDATA[This work focuses on the problem of unraveling nonlinearly mixed latent
components in an unsupervised manner. The latent components are assumed to
reside in the probability simplex, and are transformed by an unknown
post-nonlinear mixing system. This problem finds various applications in signal
and data analytics, e.g., nonlinear hyperspectral unmixing, image embedding,
and nonlinear clustering. Linear mixture learning problems are already
ill-posed, as identifiability of the target latent components is hard to
establish in general. With unknown nonlinearity involved, the problem is even
more challenging. Prior work offered a function equation-based formulation for
provable latent component identification. However, the identifiability
conditions are somewhat stringent and unrealistic. In addition, the
identifiability analysis is based on the infinite sample (i.e., population)
case, while the understanding for practical finite sample cases has been
elusive. Moreover, the algorithm in the prior work trades model expressiveness
with computational convenience, which often hinders the learning performance.
Our contribution is threefold. First, new identifiability conditions are
derived under largely relaxed assumptions. Second, comprehensive sample
complexity results are presented -- which are the first of the kind. Third, a
constrained autoencoder-based algorithmic framework is proposed for
implementation, which effectively circumvents the challenges in the existing
algorithm. Synthetic and real experiments corroborate our theoretical analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1"&gt;Qi Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiao Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor Neural Network. (arXiv:2103.08160v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08160</id>
        <link href="http://arxiv.org/abs/2103.08160"/>
        <updated>2021-06-18T02:06:35.909Z</updated>
        <summary type="html"><![CDATA[Few-shot learning (FSL) aims to classify images under low-data regimes, where
the conventional pooled global representation is likely to lose useful local
characteristics. Recent work has achieved promising performances by using deep
descriptors. They generally take all deep descriptors from neural networks into
consideration while ignoring that some of them are useless in classification
due to their limited receptive field, e.g., task-irrelevant descriptors could
be misleading and multiple aggregative descriptors from background clutter
could even overwhelm the object's presence. In this paper, we argue that a
Mutual Nearest Neighbor (MNN) relation should be established to explicitly
select the query descriptors that are most relevant to each task and discard
less relevant ones from aggregative clutters in FSL. Specifically, we propose
Discriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive
experiments demonstrate that our method not only qualitatively selects
task-relevant descriptors but also quantitatively outperforms the existing
state-of-the-arts by a large margin of 1.8~4.9% on fine-grained CUB, a
considerable margin of 1.4~2.2% on both supervised and semi-supervised
miniImagenet, and ~1.4% on challenging tieredimagenet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1"&gt;Tu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jie Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofei He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning for Intrusion Detection System: Concepts, Challenges and Future Directions. (arXiv:2106.09527v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.09527</id>
        <link href="http://arxiv.org/abs/2106.09527"/>
        <updated>2021-06-18T02:06:35.902Z</updated>
        <summary type="html"><![CDATA[The rapid development of the Internet and smart devices trigger surge in
network traffic making its infrastructure more complex and heterogeneous. The
predominated usage of mobile phones, wearable devices and autonomous vehicles
are examples of distributed networks which generate huge amount of data each
and every day. The computational power of these devices have also seen steady
progression which has created the need to transmit information, store data
locally and drive network computations towards edge devices. Intrusion
detection systems play a significant role in ensuring security and privacy of
such devices. Machine Learning and Deep Learning with Intrusion Detection
Systems have gained great momentum due to their achievement of high
classification accuracy. However the privacy and security aspects potentially
gets jeopardised due to the need of storing and communicating data to
centralized server. On the contrary, federated learning (FL) fits in
appropriately as a privacy-preserving decentralized learning technique that
does not transfer data but trains models locally and transfers the parameters
to the centralized server. The present paper aims to present an extensive and
exhaustive review on the use of FL in intrusion detection system. In order to
establish the need for FL, various types of IDS, relevant ML approaches and its
associated issues are discussed. The paper presents detailed overview of the
implementation of FL in various aspects of anomaly detection. The allied
challenges of FL implementations are also identified which provides idea on the
scope of future direction of research. The paper finally presents the plausible
solutions associated with the identified challenges in FL based intrusion
detection system implementation acting as a baseline for prospective research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Shaashwat Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1"&gt;Sagnik Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aouedi_O/0/1/0/all/0/1"&gt;Ons Aouedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yenduri_G/0/1/0/all/0/1"&gt;Gokul Yenduri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piamrat_K/0/1/0/all/0/1"&gt;Kandaraj Piamrat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1"&gt;Sweta Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddikunta_P/0/1/0/all/0/1"&gt;Praveen Kumar Reddy Maddikunta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadekallu_T/0/1/0/all/0/1"&gt;Thippa Reddy Gadekallu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding Deep Learning from Noisy Labels with Small-Loss Criterion. (arXiv:2106.09291v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09291</id>
        <link href="http://arxiv.org/abs/2106.09291"/>
        <updated>2021-06-18T02:06:35.882Z</updated>
        <summary type="html"><![CDATA[Deep neural networks need large amounts of labeled data to achieve good
performance. In real-world applications, labels are usually collected from
non-experts such as crowdsourcing to save cost and thus are noisy. In the past
few years, deep learning methods for dealing with noisy labels have been
developed, many of which are based on the small-loss criterion. However, there
are few theoretical analyses to explain why these methods could learn well from
noisy labels. In this paper, we theoretically explain why the widely-used
small-loss criterion works. Based on the explanation, we reformalize the
vanilla small-loss criterion to better tackle noisy labels. The experimental
results verify our theoretical explanation and also demonstrate the
effectiveness of the reformalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gui_X/0/1/0/all/0/1"&gt;Xian-Jin Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zhang-Hao Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Adversarial Transferability with Gradient Refining. (arXiv:2105.04834v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04834</id>
        <link href="http://arxiv.org/abs/2105.04834"/>
        <updated>2021-06-18T02:06:35.875Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are vulnerable to adversarial examples, which are
crafted by adding human-imperceptible perturbations to original images. Most
existing adversarial attack methods achieve nearly 100% attack success rates
under the white-box setting, but only achieve relatively low attack success
rates under the black-box setting. To improve the transferability of
adversarial examples for the black-box setting, several methods have been
proposed, e.g., input diversity, translation-invariant attack, and
momentum-based attack. In this paper, we propose a method named Gradient
Refining, which can further improve the adversarial transferability by
correcting useless gradients introduced by input diversity through multiple
transformations. Our method is generally applicable to many gradient-based
attack methods combined with input diversity. Extensive experiments are
conducted on the ImageNet dataset and our method can achieve an average
transfer success rate of 82.07% for three different models under single-model
setting, which outperforms the other state-of-the-art methods by a large margin
of 6.0% averagely. And we have applied the proposed method to the competition
CVPR 2021 Unrestricted Adversarial Attacks on ImageNet organized by Alibaba and
won the second place in attack success rates among 1558 teams.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guoqiu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Huanqian Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Ying Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xingxing Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Demonstration without Demonstrations. (arXiv:2106.09203v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09203</id>
        <link href="http://arxiv.org/abs/2106.09203"/>
        <updated>2021-06-18T02:06:35.868Z</updated>
        <summary type="html"><![CDATA[State-of-the-art reinforcement learning (RL) algorithms suffer from high
sample complexity, particularly in the sparse reward case. A popular strategy
for mitigating this problem is to learn control policies by imitating a set of
expert demonstrations. The drawback of such approaches is that an expert needs
to produce demonstrations, which may be costly in practice. To address this
shortcoming, we propose Probabilistic Planning for Demonstration Discovery
(P2D2), a technique for automatically discovering demonstrations without access
to an expert. We formulate discovering demonstrations as a search problem and
leverage widely-used planning algorithms such as Rapidly-exploring Random Tree
to find demonstration trajectories. These demonstrations are used to initialize
a policy, then refined by a generic RL algorithm. We provide theoretical
guarantees of P2D2 finding successful trajectories, as well as bounds for its
sampling complexity. We experimentally demonstrate the method outperforms
classic and intrinsic exploration RL techniques in a range of classic control
and robotics tasks, requiring only a fraction of exploration samples and
achieving better asymptotic performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blau_T/0/1/0/all/0/1"&gt;Tom Blau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Francis_G/0/1/0/all/0/1"&gt;Gilad Francis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morere_P/0/1/0/all/0/1"&gt;Philippe Morere&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Online Chats with DAG-Structured LSTMs. (arXiv:2106.09024v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09024</id>
        <link href="http://arxiv.org/abs/2106.09024"/>
        <updated>2021-06-18T02:06:35.860Z</updated>
        <summary type="html"><![CDATA[Many modern messaging systems allow fast and synchronous textual
communication among many users. The resulting sequence of messages hides a more
complicated structure in which independent sub-conversations are interwoven
with one another. This poses a challenge for any task aiming to understand the
content of the chat logs or gather information from them. The ability to
disentangle these conversations is then tantamount to the success of many
downstream tasks such as summarization and question answering. Structured
information accompanying the text such as user turn, user mentions, timestamps,
is used as a cue by the participants themselves who need to follow the
conversation and has been shown to be important for disentanglement. DAG-LSTMs,
a generalization of Tree-LSTMs that can handle directed acyclic dependencies,
are a natural way to incorporate such information and its non-sequential
nature. In this paper, we apply DAG-LSTMs to the conversation disentanglement
task. We perform our experiments on the Ubuntu IRC dataset. We show that the
novel model we propose achieves state of the art status on the task of
recovering reply-to relations and it is competitive on other disentanglement
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pappadopulo_D/0/1/0/all/0/1"&gt;Duccio Pappadopulo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_L/0/1/0/all/0/1"&gt;Lisa Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farina_M/0/1/0/all/0/1"&gt;Marco Farina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irsoy_O/0/1/0/all/0/1"&gt;Ozan &amp;#x130;rsoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NPAS: A Compiler-aware Framework of Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration. (arXiv:2012.00596v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00596</id>
        <link href="http://arxiv.org/abs/2012.00596"/>
        <updated>2021-06-18T02:06:35.841Z</updated>
        <summary type="html"><![CDATA[With the increasing demand to efficiently deploy DNNs on mobile edge devices,
it becomes much more important to reduce unnecessary computation and increase
the execution speed. Prior methods towards this goal, including model
compression and network architecture search (NAS), are largely performed
independently and do not fully consider compiler-level optimizations which is a
must-do for mobile acceleration. In this work, we first propose (i) a general
category of fine-grained structured pruning applicable to various DNN layers,
and (ii) a comprehensive, compiler automatic code generation framework
supporting different DNNs and different pruning schemes, which bridge the gap
of model compression and NAS. We further propose NPAS, a compiler-aware unified
network pruning, and architecture search. To deal with large search space, we
propose a meta-modeling procedure based on reinforcement learning with fast
evaluation and Bayesian optimization, ensuring the total number of training
epochs comparable with representative NAS frameworks. Our framework achieves
6.7ms, 5.9ms, 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3
level), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an
off-the-shelf mobile phone, consistently outperforming prior work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhengang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1"&gt;Wei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yuxuan Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xuan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1"&gt;Zheng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1"&gt;Zhenglun Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kaiyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1"&gt;Bin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Curricula via Expert Demonstrations. (arXiv:2106.09159v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09159</id>
        <link href="http://arxiv.org/abs/2106.09159"/>
        <updated>2021-06-18T02:06:35.835Z</updated>
        <summary type="html"><![CDATA[We propose Automatic Curricula via Expert Demonstrations (ACED), a
reinforcement learning (RL) approach that combines the ideas of imitation
learning and curriculum learning in order to solve challenging robotic
manipulation tasks with sparse reward functions. Curriculum learning solves
complicated RL tasks by introducing a sequence of auxiliary tasks with
increasing difficulty, yet how to automatically design effective and
generalizable curricula remains a challenging research problem. ACED extracts
curricula from a small amount of expert demonstration trajectories by dividing
demonstrations into sections and initializing training episodes to states
sampled from different sections of demonstrations. Through moving the reset
states from the end to the beginning of demonstrations as the learning agent
improves its performance, ACED not only learns challenging manipulation tasks
with unseen initializations and goals, but also discovers novel solutions that
are distinct from the demonstrations. In addition, ACED can be naturally
combined with other imitation learning methods to utilize expert demonstrations
in a more efficient manner, and we show that a combination of ACED with
behavior cloning allows pick-and-place tasks to be learned with as few as 1
demonstration and block stacking tasks to be learned with 20 demonstrations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1"&gt;Siyu Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_A/0/1/0/all/0/1"&gt;Andreas Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1"&gt;Brian Williams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep generative modeling for probabilistic forecasting in power systems. (arXiv:2106.09370v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09370</id>
        <link href="http://arxiv.org/abs/2106.09370"/>
        <updated>2021-06-18T02:06:35.828Z</updated>
        <summary type="html"><![CDATA[Greater direct electrification of end-use sectors with a higher share of
renewables is one of the pillars to power a carbon-neutral society by 2050.
This study uses a recent deep learning technique, the normalizing flows, to
produce accurate probabilistic forecasts that are crucial for decision-makers
to face the new challenges in power systems applications. Through comprehensive
empirical evaluations using the open data of the Global Energy Forecasting
Competition 2014, we demonstrate that our methodology is competitive with other
state-of-the-art deep learning generative models: generative adversarial
networks and variational autoencoders. The models producing weather-based wind,
solar power, and load scenarios are properly compared both in terms of forecast
value, by considering the case study of an energy retailer, and quality using
several complementary metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dumas_J/0/1/0/all/0/1"&gt;Jonathan Dumas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lanaspeze_A/0/1/0/all/0/1"&gt;Antoine Wehenkel Damien Lanaspeze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornelusse_B/0/1/0/all/0/1"&gt;Bertrand Corn&amp;#xe9;lusse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutera_A/0/1/0/all/0/1"&gt;Antonio Sutera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting cognitive scores with graph neural networks through sample selection learning. (arXiv:2106.09408v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09408</id>
        <link href="http://arxiv.org/abs/2106.09408"/>
        <updated>2021-06-18T02:06:35.822Z</updated>
        <summary type="html"><![CDATA[Analyzing the relation between intelligence and neural activity is of the
utmost importance in understanding the working principles of the human brain in
health and disease. In existing literature, functional brain connectomes have
been used successfully to predict cognitive measures such as intelligence
quotient (IQ) scores in both healthy and disordered cohorts using machine
learning models. However, existing methods resort to flattening the brain
connectome (i.e., graph) through vectorization which overlooks its topological
properties. To address this limitation and inspired from the emerging graph
neural networks (GNNs), we design a novel regression GNN model (namely RegGNN)
for predicting IQ scores from brain connectivity. On top of that, we introduce
a novel, fully modular sample selection method to select the best samples to
learn from for our target prediction task. However, since such deep learning
architectures are computationally expensive to train, we further propose a
\emph{learning-based sample selection} method that learns how to choose the
training samples with the highest expected predictive power on unseen samples.
For this, we capitalize on the fact that connectomes (i.e., their adjacency
matrices) lie in the symmetric positive definite (SPD) matrix cone. Our results
on full-scale and verbal IQ prediction outperforms comparison methods in autism
spectrum disorder cohorts and achieves a competitive performance for
neurotypical subjects using 3-fold cross-validation. Furthermore, we show that
our sample selection approach generalizes to other learning-based methods,
which shows its usefulness beyond our GNN architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hanik_M/0/1/0/all/0/1"&gt;Martin Hanik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demirtas_M/0/1/0/all/0/1"&gt;Mehmet Arif Demirta&amp;#x15f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gharsallaoui_M/0/1/0/all/0/1"&gt;Mohammed Amine Gharsallaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rekik_I/0/1/0/all/0/1"&gt;Islem Rekik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IFCNet: A Benchmark Dataset for IFC Entity Classification. (arXiv:2106.09712v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09712</id>
        <link href="http://arxiv.org/abs/2106.09712"/>
        <updated>2021-06-18T02:06:35.815Z</updated>
        <summary type="html"><![CDATA[Enhancing interoperability and information exchange between domain-specific
software products for BIM is an important aspect in the Architecture,
Engineering, Construction and Operations industry. Recent research started
investigating methods from the areas of machine and deep learning for semantic
enrichment of BIM models. However, training and evaluation of these machine
learning algorithms requires sufficiently large and comprehensive datasets.
This work presents IFCNet, a dataset of single-entity IFC files spanning a
broad range of IFC classes containing both geometric and semantic information.
Using only the geometric information of objects, the experiments show that
three different deep learning models are able to achieve good classification
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Emunds_C/0/1/0/all/0/1"&gt;Christoph Emunds&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauen_N/0/1/0/all/0/1"&gt;Nicolas Pauen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richter_V/0/1/0/all/0/1"&gt;Veronika Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frisch_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xf4;me Frisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Treeck_C/0/1/0/all/0/1"&gt;Christoph van Treeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling-up Diverse Orthogonal Convolutional Networks with a Paraunitary Framework. (arXiv:2106.09121v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09121</id>
        <link href="http://arxiv.org/abs/2106.09121"/>
        <updated>2021-06-18T02:06:35.807Z</updated>
        <summary type="html"><![CDATA[Enforcing orthogonality in neural networks is an antidote for gradient
vanishing/exploding problems, sensitivity by adversarial perturbation, and
bounding generalization errors. However, many previous approaches are
heuristic, and the orthogonality of convolutional layers is not systematically
studied: some of these designs are not exactly orthogonal, while others only
consider standard convolutional layers and propose specific classes of their
realizations. To address this problem, we propose a theoretical framework for
orthogonal convolutional layers, which establishes the equivalence between
various orthogonal convolutional layers in the spatial domain and the
paraunitary systems in the spectral domain. Since there exists a complete
spectral factorization of paraunitary systems, any orthogonal convolution layer
can be parameterized as convolutions of spatial filters. Our framework endows
high expressive power to various convolutional layers while maintaining their
exact orthogonality. Furthermore, our layers are memory and computationally
efficient for deep networks compared to previous designs. Our versatile
framework, for the first time, enables the study of architecture designs for
deep orthogonal networks, such as choices of skip connection, initialization,
stride, and dilation. Consequently, we scale up orthogonal networks to deep
architectures, including ResNet, WideResNet, and ShuffleNet, substantially
increasing the performance over the traditional shallow orthogonal networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jiahao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1"&gt;Wonmin Byeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks. (arXiv:2012.12352v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12352</id>
        <link href="http://arxiv.org/abs/2012.12352"/>
        <updated>2021-06-18T02:06:35.782Z</updated>
        <summary type="html"><![CDATA[We investigate the reasoning ability of pretrained vision and language (V&L)
models in two tasks that require multimodal integration: (1) discriminating a
correct image-sentence pair from an incorrect one, and (2) counting entities in
an image. We evaluate three pretrained V&L models on these tasks: ViLBERT,
ViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results
show that models solve task (1) very well, as expected, since all models are
pretrained on task (1). However, none of the pretrained V&L models is able to
adequately solve task (2), our counting probe, and they cannot generalise to
out-of-distribution quantities. We propose a number of explanations for these
findings: LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of
catastrophic forgetting on task (1). Concerning our results on the counting
probe, we find evidence that all models are impacted by dataset bias, and also
fail to individuate entities in the visual input. While a selling point of
pretrained V&L models is their ability to solve complex tasks, our findings
suggest that understanding their reasoning and grounding capabilities requires
more targeted investigations on specific phenomena.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1"&gt;Letitia Parcalabescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1"&gt;Albert Gatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Anette Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1"&gt;Iacer Calixto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Main Character Recognition for Photographic Studies. (arXiv:2106.09064v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09064</id>
        <link href="http://arxiv.org/abs/2106.09064"/>
        <updated>2021-06-18T02:06:35.776Z</updated>
        <summary type="html"><![CDATA[Main characters in images are the most important humans that catch the
viewer's attention upon first look, and they are emphasized by properties such
as size, position, color saturation, and sharpness of focus. Identifying the
main character in images plays an important role in traditional photographic
studies and media analysis, but the task is performed manually and can be slow
and laborious. Furthermore, selection of main characters can be sometimes
subjective. In this paper, we analyze the feasibility of solving the main
character recognition needed for photographic studies automatically and propose
a method for identifying the main characters. The proposed method uses machine
learning based human pose estimation along with traditional computer vision
approaches for this task. We approach the task as a binary classification
problem where each detected human is classified either as a main character or
not. To evaluate both the subjectivity of the task and the performance of our
method, we collected a dataset of 300 varying images from multiple sources and
asked five people, a photographic researcher and four other persons, to
annotate the main characters. Our analysis showed a relatively high agreement
between different annotators. The proposed method achieved a promising F1 score
of 0.83 on the full image set and 0.96 on a subset evaluated as most clear and
important cases by the photographic researcher.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seker_M/0/1/0/all/0/1"&gt;Mert Seker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannisto_A/0/1/0/all/0/1"&gt;Anssi M&amp;#xe4;nnist&amp;#xf6;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1"&gt;Jenni Raitoharju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Multimodal Domino: in Search of Biomarkers for Alzheimer's Disease. (arXiv:2012.13623v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13623</id>
        <link href="http://arxiv.org/abs/2012.13623"/>
        <updated>2021-06-18T02:06:35.769Z</updated>
        <summary type="html"><![CDATA[Sensory input from multiple sources is crucial for robust and coherent human
perception. Different sources contribute complementary explanatory factors.
Similarly, research studies often collect multimodal imaging data, each of
which can provide shared and unique information. This observation motivated the
design of powerful multimodal self-supervised representation-learning
algorithms. In this paper, we unify recent work on multimodal self-supervised
learning under a single framework. Observing that most self-supervised methods
optimize similarity metrics between a set of model components, we propose a
taxonomy of all reasonable ways to organize this process. We first evaluate
models on toy multimodal MNIST datasets and then apply them to a multimodal
neuroimaging dataset with Alzheimer's disease patients. We find that (1)
multimodal contrastive learning has significant benefits over its unimodal
counterpart, (2) the specific composition of multiple contrastive objectives is
critical to performance on a downstream task, (3) maximization of the
similarity between representations has a regularizing effect on a neural
network, which can sometimes lead to reduced downstream performance but still
reveal multimodal relations. Results show that the proposed approach
outperforms previous self-supervised encoder-decoder methods based on canonical
correlation analysis (CCA) or the mixture-of-experts multimodal variational
autoEncoder (MMVAE) on various datasets with a linear evaluation protocol.
Importantly, we find a promising solution to uncover connections between
modalities through a jointly shared subspace that can help advance work in our
search for neuroimaging biomarkers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fedorov_A/0/1/0/all/0/1"&gt;Alex Fedorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sylvain_T/0/1/0/all/0/1"&gt;Tristan Sylvain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geenjaar_E/0/1/0/all/0/1"&gt;Eloy Geenjaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luck_M/0/1/0/all/0/1"&gt;Margaux Luck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DeRamus_T/0/1/0/all/0/1"&gt;Thomas P. DeRamus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirilin_A/0/1/0/all/0/1"&gt;Alex Kirilin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bleklov_D/0/1/0/all/0/1"&gt;Dmitry Bleklov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1"&gt;Vince D. Calhoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1"&gt;Sergey M. Plis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPeCiaL: Self-Supervised Pretraining for Continual Learning. (arXiv:2106.09065v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09065</id>
        <link href="http://arxiv.org/abs/2106.09065"/>
        <updated>2021-06-18T02:06:35.762Z</updated>
        <summary type="html"><![CDATA[This paper presents SPeCiaL: a method for unsupervised pretraining of
representations tailored for continual learning. Our approach devises a
meta-learning objective that differentiates through a sequential learning
process. Specifically, we train a linear model over the representations to
match different augmented views of the same image together, each view presented
sequentially. The linear model is then evaluated on both its ability to
classify images it just saw, and also on images from previous iterations. This
gives rise to representations that favor quick knowledge retention with minimal
forgetting. We evaluate SPeCiaL in the Continual Few-Shot Learning setting, and
show that it can match or outperform other supervised pretraining approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1"&gt;Lucas Caccia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the training of sparse and dense deep neural networks: less parameters, same performance. (arXiv:2106.09021v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09021</id>
        <link href="http://arxiv.org/abs/2106.09021"/>
        <updated>2021-06-18T02:06:35.746Z</updated>
        <summary type="html"><![CDATA[Deep neural networks can be trained in reciprocal space, by acting on the
eigenvalues and eigenvectors of suitable transfer operators in direct space.
Adjusting the eigenvalues, while freezing the eigenvectors, yields a
substantial compression of the parameter space. This latter scales by
definition with the number of computing neurons. The classification scores, as
measured by the displayed accuracy, are however inferior to those attained when
the learning is carried in direct space, for an identical architecture and by
employing the full set of trainable parameters (with a quadratic dependence on
the size of neighbor layers). In this Letter, we propose a variant of the
spectral learning method as appeared in Giambagli et al {Nat. Comm.} 2021,
which leverages on two sets of eigenvalues, for each mapping between adjacent
layers. The eigenvalues act as veritable knobs which can be freely tuned so as
to (i) enhance, or alternatively silence, the contribution of the input nodes,
(ii) modulate the excitability of the receiving nodes with a mechanism which we
interpret as the artificial analogue of the homeostatic plasticity. The number
of trainable parameters is still a linear function of the network size, but the
performances of the trained device gets much closer to those obtained via
conventional algorithms, these latter requiring however a considerably heavier
computational cost. The residual gap between conventional and spectral
trainings can be eventually filled by employing a suitable decomposition for
the non trivial block of the eigenvectors matrix. Each spectral parameter
reflects back on the whole set of inter-nodes weights, an attribute which we
shall effectively exploit to yield sparse networks with stunning classification
abilities, as compared to their homologues trained with conventional means.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chicchi_L/0/1/0/all/0/1"&gt;Lorenzo Chicchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giambagli_L/0/1/0/all/0/1"&gt;Lorenzo Giambagli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buffoni_L/0/1/0/all/0/1"&gt;Lorenzo Buffoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carletti_T/0/1/0/all/0/1"&gt;Timoteo Carletti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciavarella_M/0/1/0/all/0/1"&gt;Marco Ciavarella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fanelli_D/0/1/0/all/0/1"&gt;Duccio Fanelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Construction of Evaluation Suites for Natural Language Generation Datasets. (arXiv:2106.09069v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09069</id>
        <link href="http://arxiv.org/abs/2106.09069"/>
        <updated>2021-06-18T02:06:35.739Z</updated>
        <summary type="html"><![CDATA[Machine learning approaches applied to NLP are often evaluated by summarizing
their performance in a single number, for example accuracy. Since most test
sets are constructed as an i.i.d. sample from the overall data, this approach
overly simplifies the complexity of language and encourages overfitting to the
head of the data distribution. As such, rare language phenomena or text about
underrepresented groups are not equally included in the evaluation. To
encourage more in-depth model analyses, researchers have proposed the use of
multiple test sets, also called challenge sets, that assess specific
capabilities of a model. In this paper, we develop a framework based on this
idea which is able to generate controlled perturbations and identify subsets in
text-to-scalar, text-to-text, or data-to-text settings. By applying this
framework to the GEM generation benchmark, we propose an evaluation suite made
of 80 challenge sets, demonstrate the kinds of analyses that it enables and
shed light onto the limits of current generation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mille_S/0/1/0/all/0/1"&gt;Simon Mille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1"&gt;Kaustubh D. Dhole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahamood_S/0/1/0/all/0/1"&gt;Saad Mahamood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Beltrachini_L/0/1/0/all/0/1"&gt;Laura Perez-Beltrachini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1"&gt;Varun Gangal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1"&gt;Mihir Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miltenburg_E/0/1/0/all/0/1"&gt;Emiel van Miltenburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1"&gt;Sebastian Gehrmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zeroth-Order Methods for Convex-Concave Minmax Problems: Applications to Decision-Dependent Risk Minimization. (arXiv:2106.09082v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.09082</id>
        <link href="http://arxiv.org/abs/2106.09082"/>
        <updated>2021-06-18T02:06:35.733Z</updated>
        <summary type="html"><![CDATA[Min-max optimization is emerging as a key framework for analyzing problems of
robustness to strategically and adversarially generated data. We propose a
random reshuffling-based gradient free Optimistic Gradient Descent-Ascent
algorithm for solving convex-concave min-max problems with finite sum
structure. We prove that the algorithm enjoys the same convergence rate as that
of zeroth-order algorithms for convex minimization problems. We further
specialize the algorithm to solve distributionally robust, decision-dependent
learning problems, where gradient information is not readily available. Through
illustrative simulations, we observe that our proposed approach learns models
that are simultaneously robust against adversarial distribution shifts and
strategic decisions from the data sources, and outperforms existing methods
from the strategic classification literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Maheshwari_C/0/1/0/all/0/1"&gt;Chinmay Maheshwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chiu_C/0/1/0/all/0/1"&gt;Chih-Yuan Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mazumdar_E/0/1/0/all/0/1"&gt;Eric Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sastry_S/0/1/0/all/0/1"&gt;S. Shankar Sastry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ratliff_L/0/1/0/all/0/1"&gt;Lillian J. Ratliff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RHNAS: Realizable Hardware and Neural Architecture Search. (arXiv:2106.09180v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09180</id>
        <link href="http://arxiv.org/abs/2106.09180"/>
        <updated>2021-06-18T02:06:35.727Z</updated>
        <summary type="html"><![CDATA[The rapidly evolving field of Artificial Intelligence necessitates automated
approaches to co-design neural network architecture and neural accelerators to
maximize system efficiency and address productivity challenges. To enable joint
optimization of this vast space, there has been growing interest in
differentiable NN-HW co-design. Fully differentiable co-design has reduced the
resource requirements for discovering optimized NN-HW configurations, but fail
to adapt to general hardware accelerator search spaces. This is due to the
existence of non-synthesizable (invalid) designs in the search space of many
hardware accelerators. To enable efficient and realizable co-design of
configurable hardware accelerators with arbitrary neural network search spaces,
we introduce RHNAS. RHNAS is a method that combines reinforcement learning for
hardware optimization with differentiable neural architecture search. RHNAS
discovers realizable NN-HW designs with 1.84x lower latency and 1.86x lower
energy-delay product (EDP) on ImageNet and 2.81x lower latency and 3.30x lower
EDP on CIFAR-10 over the default hardware accelerator design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhauri_Y/0/1/0/all/0/1"&gt;Yash Akhauri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niranjan_A/0/1/0/all/0/1"&gt;Adithya Niranjan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munoz_J/0/1/0/all/0/1"&gt;J. Pablo Mu&amp;#xf1;oz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1"&gt;Suvadeep Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davare_A/0/1/0/all/0/1"&gt;Abhijit Davare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cocchini_P/0/1/0/all/0/1"&gt;Pasquale Cocchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorokin_A/0/1/0/all/0/1"&gt;Anton A. Sorokin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Ravi Iyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1"&gt;Nilesh Jain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularization of Mixture Models for Robust Principal Graph Learning. (arXiv:2106.09035v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09035</id>
        <link href="http://arxiv.org/abs/2106.09035"/>
        <updated>2021-06-18T02:06:35.720Z</updated>
        <summary type="html"><![CDATA[A regularized version of Mixture Models is proposed to learn a principal
graph from a distribution of $D$-dimensional data points. In the particular
case of manifold learning for ridge detection, we assume that the underlying
manifold can be modeled as a graph structure acting like a topological prior
for the Gaussian clusters turning the problem into a maximum a posteriori
estimation. Parameters of the model are iteratively estimated through an
Expectation-Maximization procedure making the learning of the structure
computationally efficient with guaranteed convergence for any graph prior in a
polynomial time. We also embed in the formalism a natural way to make the
algorithm robust to outliers of the pattern and heteroscedasticity of the
manifold sampling coherently with the graph structure. The method uses a graph
prior given by the minimum spanning tree that we extend using random
sub-samplings of the dataset to take into account cycles that can be observed
in the spatial distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bonnaire_T/0/1/0/all/0/1"&gt;Tony Bonnaire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Decelle_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Decelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghanim_N/0/1/0/all/0/1"&gt;Nabila Aghanim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple Fix to Mahalanobis Distance for Improving Near-OOD Detection. (arXiv:2106.09022v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09022</id>
        <link href="http://arxiv.org/abs/2106.09022"/>
        <updated>2021-06-18T02:06:35.714Z</updated>
        <summary type="html"><![CDATA[Mahalanobis distance (MD) is a simple and popular post-processing method for
detecting out-of-distribution (OOD) inputs in neural networks. We analyze its
failure modes for near-OOD detection and propose a simple fix called relative
Mahalanobis distance (RMD) which improves performance and is more robust to
hyperparameter choice. On a wide selection of challenging vision, language, and
biology OOD benchmarks (CIFAR-100 vs CIFAR-10, CLINC OOD intent detection,
Genomics OOD), we show that RMD meaningfully improves upon MD performance (by
up to 15% AUROC on genomics OOD).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jie Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1"&gt;Stanislav Fort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jeremiah Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1"&gt;Abhijit Guha Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padhy_S/0/1/0/all/0/1"&gt;Shreyas Padhy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QuantumFed: A Federated Learning Framework for Collaborative Quantum Training. (arXiv:2106.09109v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09109</id>
        <link href="http://arxiv.org/abs/2106.09109"/>
        <updated>2021-06-18T02:06:35.707Z</updated>
        <summary type="html"><![CDATA[With the fast development of quantum computing and deep learning, quantum
neural networks have attracted great attention recently. By leveraging the
power of quantum computing, deep neural networks can potentially overcome
computational power limitations in classic machine learning. However, when
multiple quantum machines wish to train a global model using the local data on
each machine, it may be very difficult to copy the data into one machine and
train the model. Therefore, a collaborative quantum neural network framework is
necessary. In this article, we borrow the core idea of federated learning to
propose QuantumFed, a quantum federated learning framework to have multiple
quantum nodes with local quantum data train a mode together. Our experiments
show the feasibility and robustness of our framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1"&gt;Qun Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Recommend Frame for Interactive Video Object Segmentation in the Wild. (arXiv:2103.10391v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10391</id>
        <link href="http://arxiv.org/abs/2103.10391"/>
        <updated>2021-06-18T02:06:35.700Z</updated>
        <summary type="html"><![CDATA[This paper proposes a framework for the interactive video object segmentation
(VOS) in the wild where users can choose some frames for annotations
iteratively. Then, based on the user annotations, a segmentation algorithm
refines the masks. The previous interactive VOS paradigm selects the frame with
some worst evaluation metric, and the ground truth is required for calculating
the evaluation metric, which is impractical in the testing phase. In contrast,
in this paper, we advocate that the frame with the worst evaluation metric may
not be exactly the most valuable frame that leads to the most performance
improvement across the video. Thus, we formulate the frame selection problem in
the interactive VOS as a Markov Decision Process, where an agent is learned to
recommend the frame under a deep reinforcement learning framework. The learned
agent can automatically determine the most valuable frame, making the
interactive setting more practical in the wild. Experimental results on the
public datasets show the effectiveness of our learned agent without any changes
to the underlying VOS algorithms. Our data, code, and models are available at
https://github.com/svip-lab/IVOS-W.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1"&gt;Zhaoyuan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1"&gt;Jia Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1"&gt;Weixin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1"&gt;Shenhan Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanling Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Shenghua Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure. (arXiv:2106.09051v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09051</id>
        <link href="http://arxiv.org/abs/2106.09051"/>
        <updated>2021-06-18T02:06:35.659Z</updated>
        <summary type="html"><![CDATA[Our goal in this work is to generate realistic videos given just one initial
frame as input. Existing unsupervised approaches to this task do not consider
the fact that a video typically shows a 3D environment, and that this should
remain coherent from frame to frame even as the camera and objects move. We
address this by developing a model that first estimates the latent 3D structure
of the scene, including the segmentation of any moving objects. It then
predicts future frames by simulating the object and camera dynamics, and
rendering the resulting views. Importantly, it is trained end-to-end using only
the unsupervised objective of predicting future frames, without any 3D
information nor segmentation annotations. Experiments on two challenging
datasets of natural videos show that our model can estimate 3D structure and
motion segmentation from a single frame, and hence generate plausible and
varied predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1"&gt;Paul Henderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1"&gt;Christoph H. Lampert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bickel_B/0/1/0/all/0/1"&gt;Bernd Bickel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking and Designing a High-performing Automatic License Plate Recognition Approach. (arXiv:2011.14936v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14936</id>
        <link href="http://arxiv.org/abs/2011.14936"/>
        <updated>2021-06-18T02:06:35.647Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a real-time and accurate automatic license plate
recognition (ALPR) approach. Our study illustrates the outstanding design of
ALPR with four insights: (1) the resampling-based cascaded framework is
beneficial to both speed and accuracy; (2) the highly efficient license plate
recognition should abundant additional character segmentation and recurrent
neural network (RNN), but adopt a plain convolutional neural network (CNN); (3)
in the case of CNN, taking advantage of vertex information on license plates
improves the recognition performance; and (4) the weight-sharing character
classifier addresses the lack of training images in small-scale datasets. Based
on these insights, we propose a novel ALPR approach, termed VSNet.
Specifically, VSNet includes two CNNs, i.e., VertexNet for license plate
detection and SCR-Net for license plate recognition, integrated in a
resampling-based cascaded manner. In VertexNet, we propose an efficient
integration block to extract the spatial features of license plates. With
vertex supervisory information, we propose a vertex-estimation branch in
VertexNet such that license plates can be rectified as the input images of
SCR-Net. In SCR-Net, we introduce a horizontal encoding technique for
left-to-right feature extraction and propose a weight-sharing classifier for
character recognition. Experimental results show that the proposed VSNet
outperforms state-of-the-art methods by more than 50% relative improvement on
error rate, achieving > 99% recognition accuracy on CCPD and AOLP datasets with
149 FPS inference speed. Moreover, our method illustrates an outstanding
generalization capability when evaluated on the unseen PKUData and CLPD
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1"&gt;Zhen-Peng Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yunhao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1"&gt;Lap-Pui Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICON: Learning Regular Maps Through Inverse Consistency. (arXiv:2105.04459v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04459</id>
        <link href="http://arxiv.org/abs/2105.04459"/>
        <updated>2021-06-18T02:06:35.637Z</updated>
        <summary type="html"><![CDATA[Learning maps between data samples is fundamental. Applications range from
representation learning, image translation and generative modeling, to the
estimation of spatial deformations. Such maps relate feature vectors, or map
between feature spaces. Well-behaved maps should be regular, which can be
imposed explicitly or may emanate from the data itself. We explore what induces
regularity for spatial transformations, e.g., when computing image
registrations. Classical optimization-based models compute maps between pairs
of samples and rely on an appropriate regularizer for well-posedness. Recent
deep learning approaches have attempted to avoid using such regularizers
altogether by relying on the sample population instead. We explore if it is
possible to obtain spatial regularity using an inverse consistency loss only
and elucidate what explains map regularity in such a context. We find that deep
networks combined with an inverse consistency loss and randomized off-grid
interpolation yield well behaved, approximately diffeomorphic, spatial
transformations. Despite the simplicity of this approach, our experiments
present compelling evidence, on both synthetic and real data, that regular maps
can be obtained without carefully tuned explicit regularizers, while achieving
competitive registration performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Greer_H/0/1/0/all/0/1"&gt;Hastings Greer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwitt_R/0/1/0/all/0/1"&gt;Roland Kwitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vialard_F/0/1/0/all/0/1"&gt;Francois-Xavier Vialard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1"&gt;Marc Niethammer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BinaryCoP: Binary Neural Network-based COVID-19 Face-Mask Wear and Positioning Predictor on Edge Devices. (arXiv:2102.03456v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03456</id>
        <link href="http://arxiv.org/abs/2102.03456"/>
        <updated>2021-06-18T02:06:35.621Z</updated>
        <summary type="html"><![CDATA[Face masks have long been used in many areas of everyday life to protect
against the inhalation of hazardous fumes and particles. They also offer an
effective solution in healthcare for bi-directional protection against
air-borne diseases. Wearing and positioning the mask correctly is essential for
its function. Convolutional neural networks (CNNs) offer an excellent solution
for face recognition and classification of correct mask wearing and
positioning. In the context of the ongoing COVID-19 pandemic, such algorithms
can be used at entrances to corporate buildings, airports, shopping areas, and
other indoor locations, to mitigate the spread of the virus. These application
scenarios impose major challenges to the underlying compute platform. The
inference hardware must be cheap, small and energy efficient, while providing
sufficient memory and compute power to execute accurate CNNs at a reasonably
low latency. To maintain data privacy of the public, all processing must remain
on the edge-device, without any communication with cloud servers. To address
these challenges, we present a low-power binary neural network classifier for
correct facial-mask wear and positioning. The classification task is
implemented on an embedded FPGA, performing high-throughput binary operations.
Classification can take place at up to ~6400 frames-per-second, easily enabling
multi-camera, speed-gate settings or statistics collection in crowd settings.
When deployed on a single entrance or gate, the idle power consumption is
reduced to 1.6W, improving the battery-life of the device. We achieve an
accuracy of up to 98% for four wearing positions of the MaskedFace-Net dataset.
To maintain equivalent classification accuracy for all face structures,
skin-tones, hair types, and mask types, the algorithms are tested for their
ability to generalize the relevant features over all subjects using the
Grad-CAM approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fasfous_N/0/1/0/all/0/1"&gt;Nael Fasfous&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vemparala_M/0/1/0/all/0/1"&gt;Manoj-Rohit Vemparala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frickenstein_A/0/1/0/all/0/1"&gt;Alexander Frickenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frickenstein_L/0/1/0/all/0/1"&gt;Lukas Frickenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stechele_W/0/1/0/all/0/1"&gt;Walter Stechele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BigEarthNet-MM: A Large Scale Multi-Modal Multi-Label Benchmark Archive for Remote Sensing Image Classification and Retrieval. (arXiv:2105.07921v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07921</id>
        <link href="http://arxiv.org/abs/2105.07921"/>
        <updated>2021-06-18T02:06:35.603Z</updated>
        <summary type="html"><![CDATA[This paper presents the multi-modal BigEarthNet (BigEarthNet-MM) benchmark
archive made up of 590,326 pairs of Sentinel-1 and Sentinel-2 image patches to
support the deep learning (DL) studies in multi-modal multi-label remote
sensing (RS) image retrieval and classification. Each pair of patches in
BigEarthNet-MM is annotated with multi-labels provided by the CORINE Land Cover
(CLC) map of 2018 based on its thematically most detailed Level-3 class
nomenclature. Our initial research demonstrates that some CLC classes are
challenging to be accurately described by only considering (single-date)
BigEarthNet-MM images. In this paper, we also introduce an alternative
class-nomenclature as an evolution of the original CLC labels to address this
problem. This is achieved by interpreting and arranging the CLC Level-3
nomenclature based on the properties of BigEarthNet-MM images in a new
nomenclature of 19 classes. In our experiments, we show the potential of
BigEarthNet-MM for multi-modal multi-label image retrieval and classification
problems by considering several state-of-the-art DL models. We also demonstrate
that the DL models trained from scratch on BigEarthNet-MM outperform those
pre-trained on ImageNet, especially in relation to some complex classes,
including agriculture and other vegetated and natural environments. We make all
the data and the DL models publicly available at https://bigearth.net, offering
an important resource to support studies on multi-modal image scene
classification and retrieval problems in RS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1"&gt;Gencer Sumbul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wall_A/0/1/0/all/0/1"&gt;Arne de Wall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreuziger_T/0/1/0/all/0/1"&gt;Tristan Kreuziger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marcelino_F/0/1/0/all/0/1"&gt;Filipe Marcelino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costa_H/0/1/0/all/0/1"&gt;Hugo Costa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benevides_P/0/1/0/all/0/1"&gt;Pedro Benevides&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caetano_M/0/1/0/all/0/1"&gt;M&amp;#xe1;rio Caetano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1"&gt;Beg&amp;#xfc;m Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markl_V/0/1/0/all/0/1"&gt;Volker Markl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XCiT: Cross-Covariance Image Transformers. (arXiv:2106.09681v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09681</id>
        <link href="http://arxiv.org/abs/2106.09681"/>
        <updated>2021-06-18T02:06:35.571Z</updated>
        <summary type="html"><![CDATA[Following their success in natural language processing, transformers have
recently shown much promise for computer vision. The self-attention operation
underlying transformers yields global interactions between all tokens ,i.e.
words or image patches, and enables flexible modelling of image data beyond the
local interactions of convolutions. This flexibility, however, comes with a
quadratic complexity in time and memory, hindering application to long
sequences and high-resolution images. We propose a "transposed" version of
self-attention that operates across feature channels rather than tokens, where
the interactions are based on the cross-covariance matrix between keys and
queries. The resulting cross-covariance attention (XCA) has linear complexity
in the number of tokens, and allows efficient processing of high-resolution
images. Our cross-covariance image transformer (XCiT) is built upon XCA. It
combines the accuracy of conventional transformers with the scalability of
convolutional architectures. We validate the effectiveness and generality of
XCiT by reporting excellent results on multiple vision benchmarks, including
image classification and self-supervised feature learning on ImageNet-1k,
object detection and instance segmentation on COCO, and semantic segmentation
on ADE20k.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+El_Nouby_A/0/1/0/all/0/1"&gt;Alaaeldin El-Nouby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1"&gt;Hugo Touvron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1"&gt;Mathilde Caron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1"&gt;Piotr Bojanowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1"&gt;Matthijs Douze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1"&gt;Armand Joulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1"&gt;Ivan Laptev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1"&gt;Natalia Neverova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1"&gt;Gabriel Synnaeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1"&gt;Jakob Verbeek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1"&gt;Herv&amp;#xe9; Jegou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Image-to-Video Synthesis using cINNs. (arXiv:2105.04551v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04551</id>
        <link href="http://arxiv.org/abs/2105.04551"/>
        <updated>2021-06-18T02:06:35.564Z</updated>
        <summary type="html"><![CDATA[Video understanding calls for a model to learn the characteristic interplay
between static scene content and its dynamics: Given an image, the model must
be able to predict a future progression of the portrayed scene and, conversely,
a video should be explained in terms of its static image content and all the
remaining characteristics not present in the initial frame. This naturally
suggests a bijective mapping between the video domain and the static content as
well as residual information. In contrast to common stochastic image-to-video
synthesis, such a model does not merely generate arbitrary videos progressing
the initial image. Given this image, it rather provides a one-to-one mapping
between the residual vectors and the video with stochastic outcomes when
sampling. The approach is naturally implemented using a conditional invertible
neural network (cINN) that can explain videos by independently modelling static
and other video characteristics, thus laying the basis for controlled video
synthesis. Experiments on four diverse video datasets demonstrate the
effectiveness of our approach in terms of both the quality and diversity of the
synthesized results. Our project page is available at https://bit.ly/3t66bnU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dorkenwald_M/0/1/0/all/0/1"&gt;Michael Dorkenwald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1"&gt;Timo Milbich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blattmann_A/0/1/0/all/0/1"&gt;Andreas Blattmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1"&gt;Robin Rombach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1"&gt;Konstantinos G. Derpanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FG-Net: Fast Large-Scale LiDAR Point Clouds Understanding Network Leveraging Correlated Feature Mining and Geometric-Aware Modelling. (arXiv:2012.09439v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09439</id>
        <link href="http://arxiv.org/abs/2012.09439"/>
        <updated>2021-06-18T02:06:35.556Z</updated>
        <summary type="html"><![CDATA[This work presents FG-Net, a general deep learning framework for large-scale
point clouds understanding without voxelizations, which achieves accurate and
real-time performance with a single NVIDIA GTX 1080 GPU. First, a novel noise
and outlier filtering method is designed to facilitate subsequent high-level
tasks. For effective understanding purpose, we propose a deep convolutional
neural network leveraging correlated feature mining and deformable convolution
based geometric-aware modelling, in which the local feature relationships and
geometric patterns can be fully exploited. For the efficiency issue, we put
forward an inverse density sampling operation and a feature pyramid based
residual learning strategy to save the computational cost and memory
consumption respectively. Extensive experiments on real-world challenging
datasets demonstrated that our approaches outperform state-of-the-art
approaches in terms of accuracy and efficiency. Moreover, weakly supervised
transfer learning is also conducted to demonstrate the generalization capacity
of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kangcheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Feng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Ben M. Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CMA-Net: A Cascaded Mutual Attention Network for Light Field Salient Object Detection. (arXiv:2105.00949v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00949</id>
        <link href="http://arxiv.org/abs/2105.00949"/>
        <updated>2021-06-18T02:06:35.510Z</updated>
        <summary type="html"><![CDATA[In the past few years, numerous deep learning methods have been proposed to
address the task of segmenting salient objects from RGB images. However, these
approaches depending on single modality fail to achieve the state-of-the-art
performance on widely used light field salient object detection (SOD) datasets,
which collect large-scale natural images and provide multiple modalities such
as multi-view, micro-lens images and depth maps. Most recently proposed light
field SOD methods have acquired improving detecting accuracy, yet still predict
rough objects' structures and perform slow inference speed. To this end, we
propose CMA-Net, which consists of two novel cascaded mutual attention modules
aiming at fusing the high level features from the modalities of all-in-focus
and depth. Our proposed CMA-Net outperforms 30 SOD methods (by a large margin)
on two widely applied light field benchmark datasets. Besides, the proposed
CMA-Net can run at a speed of 53 fps, thus being four times faster than the
state-of-the-art multi-modal SOD methods. Extensive quantitative and
qualitative experiments illustrate both the effectiveness and efficiency of our
CMA-Net, inspiring future development of multi-modal learning for both the
RGB-D and light field SOD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1"&gt;Olivier Deforges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Personal Style from Few Examples. (arXiv:2105.14457v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14457</id>
        <link href="http://arxiv.org/abs/2105.14457"/>
        <updated>2021-06-18T02:06:35.503Z</updated>
        <summary type="html"><![CDATA[A key task in design work is grasping the client's implicit tastes. Designers
often do this based on a set of examples from the client. However, recognizing
a common pattern among many intertwining variables such as color, texture, and
layout and synthesizing them into a composite preference can be challenging. In
this paper, we leverage the pattern recognition capability of computational
models to aid in this task. We offer a set of principles for computationally
learning personal style. The principles are manifested in PseudoClient, a deep
learning framework that learns a computational model for personal graphic
design style from only a handful of examples. In several experiments, we found
that PseudoClient achieves a 79.40% accuracy with only five positive and
negative examples, outperforming several alternative methods. Finally, we
discuss how PseudoClient can be utilized as a building block to support the
development of future design applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;David Chuan-En Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martelaro_N/0/1/0/all/0/1"&gt;Nikolas Martelaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Attack Vulnerability of Medical Image Analysis Systems: Unexplored Factors. (arXiv:2006.06356v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06356</id>
        <link href="http://arxiv.org/abs/2006.06356"/>
        <updated>2021-06-18T02:06:35.461Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks are considered a potentially serious security threat for
machine learning systems. Medical image analysis (MedIA) systems have recently
been argued to be vulnerable to adversarial attacks due to strong financial
incentives and the associated technological infrastructure.

In this paper, we study previously unexplored factors affecting adversarial
attack vulnerability of deep learning MedIA systems in three medical domains:
ophthalmology, radiology, and pathology. We focus on adversarial black-box
settings, in which the attacker does not have full access to the target model
and usually uses another model, commonly referred to as surrogate model, to
craft adversarial examples. We consider this to be the most realistic scenario
for MedIA systems.

Firstly, we study the effect of weight initialization (ImageNet vs. random)
on the transferability of adversarial attacks from the surrogate model to the
target model. Secondly, we study the influence of differences in development
data between target and surrogate models. We further study the interaction of
weight initialization and data differences with differences in model
architecture. All experiments were done with a perturbation degree tuned to
ensure maximal transferability at minimal visual perceptibility of the attacks.

Our experiments show that pre-training may dramatically increase the
transferability of adversarial examples, even when the target and surrogate's
architectures are different: the larger the performance gain using
pre-training, the larger the transferability. Differences in the development
data between target and surrogate models considerably decrease the performance
of the attack; this decrease is further amplified by difference in the model
architecture. We believe these factors should be considered when developing
security-critical MedIA systems planned to be deployed in clinical practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bortsova_G/0/1/0/all/0/1"&gt;Gerda Bortsova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Gonzalo_C/0/1/0/all/0/1"&gt;Cristina Gonz&amp;#xe1;lez-Gonzalo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wetstein_S/0/1/0/all/0/1"&gt;Suzanne C. Wetstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubost_F/0/1/0/all/0/1"&gt;Florian Dubost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katramados_I/0/1/0/all/0/1"&gt;Ioannis Katramados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hogeweg_L/0/1/0/all/0/1"&gt;Laurens Hogeweg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liefers_B/0/1/0/all/0/1"&gt;Bart Liefers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ginneken_B/0/1/0/all/0/1"&gt;Bram van Ginneken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pluim_J/0/1/0/all/0/1"&gt;Josien P.W. Pluim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veta_M/0/1/0/all/0/1"&gt;Mitko Veta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_C/0/1/0/all/0/1"&gt;Clara I. S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruijne_M/0/1/0/all/0/1"&gt;Marleen de Bruijne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine-learning enhanced dark soliton detection in Bose-Einstein condensates. (arXiv:2101.05404v2 [cond-mat.quant-gas] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05404</id>
        <link href="http://arxiv.org/abs/2101.05404"/>
        <updated>2021-06-18T02:06:35.451Z</updated>
        <summary type="html"><![CDATA[Most data in cold-atom experiments comes from images, the analysis of which
is limited by our preconceptions of the patterns that could be present in the
data. We focus on the well-defined case of detecting dark solitons -- appearing
as local density depletions in a Bose-Einstein condensate (BEC) -- using a
methodology that is extensible to the general task of pattern recognition in
images of cold atoms. Studying soliton dynamics over a wide range of parameters
requires the analysis of large datasets, making the existing
human-inspection-based methodology a significant bottleneck. Here we describe
an automated classification and positioning system for identifying localized
excitations in atomic BECs utilizing deep convolutional neural networks to
eliminate the need for human image examination. Furthermore, we openly publish
our labeled dataset of dark solitons, the first of its kind, for further
machine learning research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shangjie Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Fritsch_A/0/1/0/all/0/1"&gt;Amilson R. Fritsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Greenberg_C/0/1/0/all/0/1"&gt;Craig Greenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Spielman_I/0/1/0/all/0/1"&gt;I. B. Spielman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zwolak_J/0/1/0/all/0/1"&gt;Justyna P. Zwolak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Normalization of breast MRIs using Cycle-Consistent Generative Adversarial Networks. (arXiv:1912.08061v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.08061</id>
        <link href="http://arxiv.org/abs/1912.08061"/>
        <updated>2021-06-18T02:06:35.442Z</updated>
        <summary type="html"><![CDATA[Dynamic Contrast Enhanced-Magnetic Resonance Imaging (DCE-MRI) is widely used
to complement ultrasound examinations and x-ray mammography during the early
detection and diagnosis of breast cancer. However, images generated by various
MRI scanners (e.g. GE Healthcare vs Siemens) differ both in intensity and noise
distribution, preventing algorithms trained on MRIs from one scanner to
generalize to data from other scanners successfully. We propose a method for
image normalization to solve this problem. MRI normalization is challenging
because it requires both normalizing intensity values and mapping between the
noise distributions of different scanners. We utilize a cycle-consistent
generative adversarial network to learn a bidirectional mapping between MRIs
produced by GE Healthcare and Siemens scanners. This allows us learning the
mapping between two different scanner types without matched data, which is not
commonly available. To ensure the preservation of breast shape and structures
within the breast, we propose two technical innovations. First, we incorporate
a mutual information loss with the CycleGAN architecture to ensure that the
structure of the breast is maintained. Second, we propose a modified
discriminator architecture which utilizes a smaller field-of-view to ensure the
preservation of finer details in the breast tissue. Quantitative and
qualitative evaluations show that the second proposed method was able to
consistently preserve a high level of detail in the breast structure while also
performing the proper intensity normalization and noise mapping. Our results
demonstrate that the proposed model can successfully learn a bidirectional
mapping between MRIs produced by different vendors, potentially enabling
improved accuracy of downstream computational algorithms for diagnosis and
detection of breast cancer. All the data used in this study are publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Modanwal_G/0/1/0/all/0/1"&gt;Gourav Modanwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vellal_A/0/1/0/all/0/1"&gt;Adithya Vellal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mazurowski_M/0/1/0/all/0/1"&gt;Maciej A. Mazurowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic-guided Automatic Natural Image Matting with Light-weight Non-local Attention. (arXiv:2103.17020v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17020</id>
        <link href="http://arxiv.org/abs/2103.17020"/>
        <updated>2021-06-18T02:06:35.435Z</updated>
        <summary type="html"><![CDATA[Natural image matting aims to precisely separate foreground objects from
background using alpha matte. Fully automatic natural image matting without
external annotation is quite challenging. Well-performed matting methods
usually require accurate labor-intensive handcrafted trimap as extra input,
while the performance of automatic trimap generation method of dilating
foreground segmentation fluctuates with segmentation quality. Therefore, we
argue that how to handle trade-off of additional information input is a major
issue in automatic matting. This paper presents a universal semantic-guided
automatic natural image matting pipeline with light-weight non-local attention
without trimap and background image as input. Specifically, guided by semantic
information of coarse foreground segmentation, Trimap Generation Network
estimates accurate trimap. With estimated trimap and RGB image as input, our
light-weight Non-local Matting Network with Refinement produces final alpha
matte, whose trimap-guided global aggregation attention block is equipped with
stride downsampling convolution, reducing computation complexity and promoting
performance. Experimental results show that our matting algorithm has
competitive performance with current state-of-the-art methods in both
trimap-free and trimap-needed aspects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuhongze Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Liguang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1"&gt;Tin Lun Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yangsheng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Selfie Video Stabilization. (arXiv:2009.02007v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.02007</id>
        <link href="http://arxiv.org/abs/2009.02007"/>
        <updated>2021-06-18T02:06:35.417Z</updated>
        <summary type="html"><![CDATA[We propose a novel real-time selfie video stabilization method. Our method is
completely automatic and runs at 26 fps. We use a 1D linear convolutional
network to directly infer the rigid moving least squares warping which
implicitly balances between the global rigidity and local flexibility. Our
network structure is specifically designed to stabilize the background and
foreground at the same time, while providing optional control of stabilization
focus (relative importance of foreground vs. background) to the users. To train
our network, we collect a selfie video dataset with 1005 videos, which is
significantly larger than previous selfie video datasets. We also propose a
grid approximation method to the rigid moving least squares warping that
enables the real-time frame warping. Our method is fully automatic and produces
visually and quantitatively better results than previous real-time general
video stabilization methods. Compared to previous offline selfie video methods,
our approach produces comparable quality with a speed improvement of orders of
magnitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jiyang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1"&gt;Ravi Ramamoorthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1"&gt;Keli Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkis_M/0/1/0/all/0/1"&gt;Michel Sarkis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_N/0/1/0/all/0/1"&gt;Ning Bi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond VQA: Generating Multi-word Answer and Rationale to Visual Questions. (arXiv:2010.12852v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12852</id>
        <link href="http://arxiv.org/abs/2010.12852"/>
        <updated>2021-06-18T02:06:35.411Z</updated>
        <summary type="html"><![CDATA[Visual Question Answering is a multi-modal task that aims to measure
high-level visual understanding. Contemporary VQA models are restrictive in the
sense that answers are obtained via classification over a limited vocabulary
(in the case of open-ended VQA), or via classification over a set of
multiple-choice-type answers. In this work, we present a completely generative
formulation where a multi-word answer is generated for a visual query. To take
this a step forward, we introduce a new task: ViQAR (Visual Question Answering
and Reasoning), wherein a model must generate the complete answer and a
rationale that seeks to justify the generated answer. We propose an end-to-end
architecture to solve this task and describe how to evaluate it. We show that
our model generates strong answers and rationales through qualitative and
quantitative evaluation, as well as through a human Turing Test.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dua_R/0/1/0/all/0/1"&gt;Radhika Dua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kancheti_S/0/1/0/all/0/1"&gt;Sai Srinivas Kancheti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning. (arXiv:2106.09701v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09701</id>
        <link href="http://arxiv.org/abs/2106.09701"/>
        <updated>2021-06-18T02:06:35.404Z</updated>
        <summary type="html"><![CDATA[Modern computer vision applications suffer from catastrophic forgetting when
incrementally learning new concepts over time. The most successful approaches
to alleviate this forgetting require extensive replay of previously seen data,
which is problematic when memory constraints or data legality concerns exist.
In this work, we consider the high-impact problem of Data-Free
Class-Incremental Learning (DFCIL), where an incremental learning agent must
learn new concepts over time without storing generators or training data from
past tasks. One approach for DFCIL is to replay synthetic images produced by
inverting a frozen copy of the learner's classification model, but we show this
approach fails for common class-incremental benchmarks when using standard
distillation strategies. We diagnose the cause of this failure and propose a
novel incremental distillation strategy for DFCIL, contributing a modified
cross-entropy training and importance-weighted feature distillation, and show
that our method results in up to a 25.1% increase in final task accuracy
(absolute difference) compared to SOTA DFCIL methods for common
class-incremental benchmarks. Our method even outperforms several standard
replay based methods which store a coreset of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1"&gt;James Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1"&gt;Yen-Chang Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balloch_J/0/1/0/all/0/1"&gt;Jonathan Balloch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yilin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hongxia Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1"&gt;Zsolt Kira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Analytics with Zero-streaming Cameras. (arXiv:1904.12342v4 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.12342</id>
        <link href="http://arxiv.org/abs/1904.12342"/>
        <updated>2021-06-18T02:06:35.398Z</updated>
        <summary type="html"><![CDATA[Low-cost cameras enable powerful analytics. An unexploited opportunity is
that most captured videos remain "cold" without being queried. For efficiency,
we advocate for these cameras to be zero streaming: capturing videos to local
storage and communicating with the cloud only when analytics is requested. How
to query zero-streaming cameras efficiently? Our response is a camera/cloud
runtime system called DIVA. It addresses two key challenges: to best use
limited camera resource during video capture; to rapidly explore massive videos
during query execution. DIVA contributes two unconventional techniques. (1)
When capturing videos, a camera builds sparse yet accurate landmark frames,
from which it learns reliable knowledge for accelerating future queries. (2)
When executing a query, a camera processes frames in multiple passes with
increasingly more expensive operators. As such, DIVA presents and keeps
refining inexact query results throughout the query's execution. On diverse
queries over 15 videos lasting 720 hours in total, DIVA runs at more than 100x
video realtime and outperforms competitive alternative designs. To our
knowledge, DIVA is the first system for querying large videos stored on
low-cost remote cameras.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tiantu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yunxin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Felix Xiaozhu Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invertible Concept-based Explanations for CNN Models with Non-negative Concept Activation Vectors. (arXiv:2006.15417v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15417</id>
        <link href="http://arxiv.org/abs/2006.15417"/>
        <updated>2021-06-18T02:06:35.390Z</updated>
        <summary type="html"><![CDATA[Convolutional neural network (CNN) models for computer vision are powerful
but lack explainability in their most basic form. This deficiency remains a key
challenge when applying CNNs in important domains. Recent work on explanations
through feature importance of approximate linear models has moved from
input-level features (pixels or segments) to features from mid-layer feature
maps in the form of concept activation vectors (CAVs). CAVs contain
concept-level information and could be learned via clustering. In this work, we
rethink the ACE algorithm of Ghorbani et~al., proposing an alternative
invertible concept-based explanation (ICE) framework to overcome its
shortcomings. Based on the requirements of fidelity (approximate models to
target models) and interpretability (being meaningful to people), we design
measurements and evaluate a range of matrix factorization methods with our
framework. We find that non-negative concept activation vectors (NCAVs) from
non-negative matrix factorization provide superior performance in
interpretability and fidelity based on computational and human subject
experiments. Our framework provides both local and global concept-level
explanations for pre-trained CNN models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruihan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madumal_P/0/1/0/all/0/1"&gt;Prashan Madumal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1"&gt;Tim Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ehinger_K/0/1/0/all/0/1"&gt;Krista A. Ehinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1"&gt;Benjamin I. P. Rubinstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["What's This?" -- Learning to Segment Unknown Objects from Manipulation Sequences. (arXiv:2011.03279v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03279</id>
        <link href="http://arxiv.org/abs/2011.03279"/>
        <updated>2021-06-18T02:06:35.371Z</updated>
        <summary type="html"><![CDATA[We present a novel framework for self-supervised grasped object segmentation
with a robotic manipulator. Our method successively learns an agnostic
foreground segmentation followed by a distinction between manipulator and
object solely by observing the motion between consecutive RGB frames. In
contrast to previous approaches, we propose a single, end-to-end trainable
architecture which jointly incorporates motion cues and semantic knowledge.
Furthermore, while the motion of the manipulator and the object are substantial
cues for our algorithm, we present means to robustly deal with distraction
objects moving in the background, as well as with completely static scenes. Our
method neither depends on any visual registration of a kinematic robot or 3D
object models, nor on precise hand-eye calibration or any additional sensor
data. By extensive experimental evaluation we demonstrate the superiority of
our framework and provide detailed insights on its capability of dealing with
the aforementioned extreme cases of motion. We also show that training a
semantic segmentation network with the automatically labeled data achieves
results on par with manually annotated training data. Code and pretrained model
are available at https://github.com/DLR-RM/DistinctNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boerdijk_W/0/1/0/all/0/1"&gt;Wout Boerdijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundermeyer_M/0/1/0/all/0/1"&gt;Martin Sundermeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durner_M/0/1/0/all/0/1"&gt;Maximilian Durner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1"&gt;Rudolph Triebel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BABEL: Bodies, Action and Behavior with English Labels. (arXiv:2106.09696v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09696</id>
        <link href="http://arxiv.org/abs/2106.09696"/>
        <updated>2021-06-18T02:06:35.363Z</updated>
        <summary type="html"><![CDATA[Understanding the semantics of human movement -- the what, how and why of the
movement -- is an important problem that requires datasets of human actions
with semantic labels. Existing datasets take one of two approaches. Large-scale
video datasets contain many action labels but do not contain ground-truth 3D
human motion. Alternatively, motion-capture (mocap) datasets have precise body
motions but are limited to a small number of actions. To address this, we
present BABEL, a large dataset with language labels describing the actions
being performed in mocap sequences. BABEL consists of action labels for about
43 hours of mocap sequences from AMASS. Action labels are at two levels of
abstraction -- sequence labels describe the overall action in the sequence, and
frame labels describe all actions in every frame of the sequence. Each frame
label is precisely aligned with the duration of the corresponding action in the
mocap sequence, and multiple actions can overlap. There are over 28k sequence
labels, and 63k frame labels in BABEL, which belong to over 250 unique action
categories. Labels from BABEL can be leveraged for tasks like action
recognition, temporal action localization, motion synthesis, etc. To
demonstrate the value of BABEL as a benchmark, we evaluate the performance of
models on 3D action recognition. We demonstrate that BABEL poses interesting
learning challenges that are applicable to real-world scenarios, and can serve
as a useful benchmark of progress in 3D action recognition. The dataset,
baseline method, and evaluation code is made available, and supported for
academic research purposes at https://babel.is.tue.mpg.de/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Punnakkal_A/0/1/0/all/0/1"&gt;Abhinanda R. Punnakkal&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1"&gt;Arjun Chandrasekaran&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1"&gt;Nikos Athanasiou&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Quiros_Ramirez_A/0/1/0/all/0/1"&gt;Alejandra Quiros-Ramirez&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1"&gt;Michael J. Black&lt;/a&gt; (1) ((1) Max Planck Institute for Intelligent Systems, (2) Universitat Konstanz)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Correspondence Hallucination: Towards Geometric Reasoning. (arXiv:2106.09711v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09711</id>
        <link href="http://arxiv.org/abs/2106.09711"/>
        <updated>2021-06-18T02:06:35.356Z</updated>
        <summary type="html"><![CDATA[Given a pair of partially overlapping source and target images and a keypoint
in the source image, the keypoint's correspondent in the target image can be
either visible, occluded or outside the field of view. Local feature matching
methods are only able to identify the correspondent's location when it is
visible, while humans can also hallucinate its location when it is occluded or
outside the field of view through geometric reasoning. In this paper, we bridge
this gap by training a network to output a peaked probability distribution over
the correspondent's location, regardless of this correspondent being visible,
occluded, or outside the field of view. We experimentally demonstrate that this
network is indeed able to hallucinate correspondences on unseen pairs of
images. We also apply this network to a camera pose estimation problem and find
it is significantly more robust than state-of-the-art local feature
matching-based competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Germain_H/0/1/0/all/0/1"&gt;Hugo Germain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1"&gt;Vincent Lepetit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bourmaud_G/0/1/0/all/0/1"&gt;Guillaume Bourmaud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Label Learning from Single Positive Labels. (arXiv:2106.09708v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09708</id>
        <link href="http://arxiv.org/abs/2106.09708"/>
        <updated>2021-06-18T02:06:35.350Z</updated>
        <summary type="html"><![CDATA[Predicting all applicable labels for a given image is known as multi-label
classification. Compared to the standard multi-class case (where each image has
only one label), it is considerably more challenging to annotate training data
for multi-label classification. When the number of potential labels is large,
human annotators find it difficult to mention all applicable labels for each
training image. Furthermore, in some settings detection is intrinsically
difficult e.g. finding small object instances in high resolution images. As a
result, multi-label training data is often plagued by false negatives. We
consider the hardest version of this problem, where annotators provide only one
relevant label for each image. As a result, training sets will have only one
positive label per image and no confirmed negatives. We explore this special
case of learning from missing labels across four different multi-label image
classification datasets for both linear classifiers and end-to-end fine-tuned
deep networks. We extend existing multi-label losses to this setting and
propose novel variants that constrain the number of expected positive labels
during training. Surprisingly, we show that in some cases it is possible to
approach the performance of fully labeled classifiers despite training with
significantly fewer confirmed labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cole_E/0/1/0/all/0/1"&gt;Elijah Cole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1"&gt;Oisin Mac Aodha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorieul_T/0/1/0/all/0/1"&gt;Titouan Lorieul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1"&gt;Pietro Perona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1"&gt;Dan Morris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1"&gt;Nebojsa Jojic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Contextual Prediction for Learned Image Compression. (arXiv:2011.09704v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09704</id>
        <link href="http://arxiv.org/abs/2011.09704"/>
        <updated>2021-06-18T02:06:35.342Z</updated>
        <summary type="html"><![CDATA[Over the past several years, we have witnessed impressive progress in the
field of learned image compression. Recent learned image codecs are commonly
based on autoencoders, that first encode an image into low-dimensional latent
representations and then decode them for reconstruction purposes. To capture
spatial dependencies in the latent space, prior works exploit hyperprior and
spatial context model to build an entropy model, which estimates the bit-rate
for end-to-end rate-distortion optimization. However, such an entropy model is
suboptimal from two aspects: (1) It fails to capture spatially global
correlations among the latents. (2) Cross-channel relationships of the latents
are still underexplored. In this paper, we propose the concept of separate
entropy coding to leverage a serial decoding process for causal contextual
entropy prediction in the latent space. A causal context model is proposed that
separates the latents across channels and makes use of cross-channel
relationships to generate highly informative contexts. Furthermore, we propose
a causal global prediction model, which is able to find global reference points
for accurate predictions of unknown points. Both these two models facilitate
entropy estimation without the transmission of overhead. In addition, we
further adopt a new separate attention module to build more powerful transform
networks. Experimental results demonstrate that our full image compression
model outperforms standard VVC/H.266 codec on Kodak dataset in terms of both
PSNR and MS-SSIM, yielding the state-of-the-art rate-distortion performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zongyu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhizheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Runsen Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhibo Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks. (arXiv:2006.12557v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12557</id>
        <link href="http://arxiv.org/abs/2006.12557"/>
        <updated>2021-06-18T02:06:35.311Z</updated>
        <summary type="html"><![CDATA[Data poisoning and backdoor attacks manipulate training data in order to
cause models to fail during inference. A recent survey of industry
practitioners found that data poisoning is the number one concern among threats
ranging from model stealing to adversarial attacks. However, it remains unclear
exactly how dangerous poisoning methods are and which ones are more effective
considering that these methods, even ones with identical objectives, have not
been tested in consistent or realistic settings. We observe that data poisoning
and backdoor attacks are highly sensitive to variations in the testing setup.
Moreover, we find that existing methods may not generalize to realistic
settings. While these existing works serve as valuable prototypes for data
poisoning, we apply rigorous tests to determine the extent to which we should
fear them. In order to promote fair comparison in future work, we develop
standardized benchmarks for data poisoning and backdoor attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1"&gt;Avi Schwarzschild&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Arjun Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1"&gt;John P Dickerson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Dexterous Grasping with Object-Centric Visual Affordances. (arXiv:2009.01439v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.01439</id>
        <link href="http://arxiv.org/abs/2009.01439"/>
        <updated>2021-06-18T02:06:35.305Z</updated>
        <summary type="html"><![CDATA[Dexterous robotic hands are appealing for their agility and human-like
morphology, yet their high degree of freedom makes learning to manipulate
challenging. We introduce an approach for learning dexterous grasping. Our key
idea is to embed an object-centric visual affordance model within a deep
reinforcement learning loop to learn grasping policies that favor the same
object regions favored by people. Unlike traditional approaches that learn from
human demonstration trajectories (e.g., hand joint sequences captured with a
glove), the proposed prior is object-centric and image-based, allowing the
agent to anticipate useful affordance regions for objects unseen during policy
learning. We demonstrate our idea with a 30-DoF five-fingered robotic hand
simulator on 40 objects from two datasets, where it successfully and
efficiently learns policies for stable functional grasps. Our affordance-guided
policies are significantly more effective, generalize better to novel objects,
train 3 X faster than the baselines, and are more robust to noisy sensor
readings and actuation. Our work offers a step towards manipulation agents that
learn by watching how people use objects, without requiring state and action
information about the human body. Project website:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mandikal_P/0/1/0/all/0/1"&gt;Priyanka Mandikal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks. (arXiv:2004.05937v7 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.05937</id>
        <link href="http://arxiv.org/abs/2004.05937"/>
        <updated>2021-06-18T02:06:35.299Z</updated>
        <summary type="html"><![CDATA[Deep neural models in recent years have been successful in almost every
field, including extremely complex problem statements. However, these models
are huge in size, with millions (and even billions) of parameters, thus
demanding more heavy computation power and failing to be deployed on edge
devices. Besides, the performance boost is highly dependent on redundant
labeled data. To achieve faster speeds and to handle the problems caused by the
lack of data, knowledge distillation (KD) has been proposed to transfer
information learned from one model to another. KD is often characterized by the
so-called `Student-Teacher' (S-T) learning framework and has been broadly
applied in model compression and knowledge transfer. This paper is about KD and
S-T learning, which are being actively studied in recent years. First, we aim
to provide explanations of what KD is and how/why it works. Then, we provide a
comprehensive survey on the recent progress of KD methods together with S-T
frameworks typically for vision tasks. In general, we consider some fundamental
questions that have been driving this research area and thoroughly generalize
the research progress and technical details. Additionally, we systematically
analyze the research status of KD in vision applications. Finally, we discuss
the potentials and open challenges of existing methods and prospect the future
directions of KD and S-T learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1"&gt;Kuk-Jin Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indian Masked Faces in the Wild Dataset. (arXiv:2106.09670v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09670</id>
        <link href="http://arxiv.org/abs/2106.09670"/>
        <updated>2021-06-18T02:06:35.292Z</updated>
        <summary type="html"><![CDATA[Due to the COVID-19 pandemic, wearing face masks has become a mandate in
public places worldwide. Face masks occlude a significant portion of the facial
region. Additionally, people wear different types of masks, from simple ones to
ones with graphics and prints. These pose new challenges to face recognition
algorithms. Researchers have recently proposed a few masked face datasets for
designing algorithms to overcome the challenges of masked face recognition.
However, existing datasets lack the cultural diversity and collection in the
unrestricted settings. Country like India with attire diversity, people are not
limited to wearing traditional masks but also clothing like a thin cotton
printed towel (locally called as ``gamcha''), ``stoles'', and ``handkerchiefs''
to cover their faces. In this paper, we present a novel \textbf{Indian Masked
Faces in the Wild (IMFW)} dataset which contains images with variations in
pose, illumination, resolution, and the variety of masks worn by the subjects.
We have also benchmarked the performance of existing face recognition models on
the proposed IMFW dataset. Experimental results demonstrate the limitations of
existing algorithms in presence of diverse conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Shiksha Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_P/0/1/0/all/0/1"&gt;Puspita Majumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Richa Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vatsa_M/0/1/0/all/0/1"&gt;Mayank Vatsa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MoDist: Motion Distillation for Self-supervised Video Representation Learning. (arXiv:2106.09703v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09703</id>
        <link href="http://arxiv.org/abs/2106.09703"/>
        <updated>2021-06-18T02:06:35.286Z</updated>
        <summary type="html"><![CDATA[We present MoDist as a novel method to explicitly distill motion information
into self-supervised video representations. Compared to previous video
representation learning methods that mostly focus on learning motion cues
implicitly from RGB inputs, we show that the representation learned with our
MoDist method focus more on foreground motion regions and thus generalizes
better to downstream tasks. To achieve this, MoDist enriches standard
contrastive learning objectives for RGB video clips with a cross-modal learning
objective between a Motion pathway and a Visual pathway. We evaluate MoDist on
several datasets for both action recognition (UCF101/HMDB51/SSv2) as well as
action detection (AVA), and demonstrate state-of-the-art self-supervised
performance on all datasets. Furthermore, we show that MoDist representation
can be as effective as (in some cases even better than) representations learned
with full supervision. Given its simplicity, we hope MoDist could serve as a
strong baseline for future research in self-supervised video representation
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1"&gt;Fanyi Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1"&gt;Joseph Tighe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1"&gt;Davide Modolo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual Policies. (arXiv:2106.09678v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09678</id>
        <link href="http://arxiv.org/abs/2106.09678"/>
        <updated>2021-06-18T02:06:35.265Z</updated>
        <summary type="html"><![CDATA[Generalization has been a long-standing challenge for reinforcement learning
(RL). Visual RL, in particular, can be easily distracted by irrelevant factors
in high-dimensional observation space. In this work, we consider robust policy
learning which targets zero-shot generalization to unseen visual environments
with large distributional shift. We propose SECANT, a novel self-expert cloning
technique that leverages image augmentation in two stages to decouple robust
representation learning from policy optimization. Specifically, an expert
policy is first trained by RL from scratch with weak augmentations. A student
network then learns to mimic the expert policy by supervised learning with
strong augmentations, making its representation more robust against visual
variations compared to the expert. Extensive experiments demonstrate that
SECANT significantly advances the state of the art in zero-shot generalization
across 4 challenging domains. Our average reward improvements over prior SOTAs
are: DeepMind Control (+26.5%), robotic manipulation (+337.8%), vision-based
autonomous driving (+47.7%), and indoor object navigation (+15.8%). Code
release and video are available at https://linxifan.github.io/secant-site/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Linxi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;De-An Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scale-Consistent Fusion: from Heterogeneous Local Sampling to Global Immersive Rendering. (arXiv:2106.09548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09548</id>
        <link href="http://arxiv.org/abs/2106.09548"/>
        <updated>2021-06-18T02:06:35.256Z</updated>
        <summary type="html"><![CDATA[Image-based geometric modeling and novel view synthesis based on sparse,
large-baseline samplings are challenging but important tasks for emerging
multimedia applications such as virtual reality and immersive telepresence.
Existing methods fail to produce satisfactory results due to the limitation on
inferring reliable depth information over such challenging reference
conditions. With the popularization of commercial light field (LF) cameras,
capturing LF images (LFIs) is as convenient as taking regular photos, and
geometry information can be reliably inferred. This inspires us to use a sparse
set of LF captures to render high-quality novel views globally. However, fusion
of LF captures from multiple angles is challenging due to the scale
inconsistency caused by various capture settings. To overcome this challenge,
we propose a novel scale-consistent volume rescaling algorithm that robustly
aligns the disparity probability volumes (DPV) among different captures for
scale-consistent global geometry fusion. Based on the fused DPV projected to
the target camera frustum, novel learning-based modules have been proposed
(i.e., the attention-guided multi-scale residual fusion module, and the
disparity field guided deep re-regularization module) which comprehensively
regularize noisy observations from heterogeneous captures for high-quality
rendering of novel LFIs. Both quantitative and qualitative experiments over the
Stanford Lytro Multi-view LF dataset show that the proposed method outperforms
state-of-the-art methods significantly under different experiment settings for
disparity inference and LF synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_W/0/1/0/all/0/1"&gt;Wenpeng Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zaifeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The 2021 Image Similarity Dataset and Challenge. (arXiv:2106.09672v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09672</id>
        <link href="http://arxiv.org/abs/2106.09672"/>
        <updated>2021-06-18T02:06:35.249Z</updated>
        <summary type="html"><![CDATA[This paper introduces a new benchmark for large-scale image similarity
detection. This benchmark is used for the Image Similarity Challenge at
NeurIPS'21 (ISC2021). The goal is to determine whether a query image is a
modified copy of any image in a reference corpus of size 1~million. The
benchmark features a variety of image transformations such as automated
transformations, hand-crafted image edits and machine-learning based
manipulations. This mimics real-life cases appearing in social media, for
example for integrity-related problems dealing with misinformation and
objectionable content. The strength of the image manipulations, and therefore
the difficulty of the benchmark, is calibrated according to the performance of
a set of baseline approaches. Both the query and reference set contain a
majority of ``distractor'' images that do not match, which corresponds to a
real-life needle-in-haystack setting, and the evaluation metric reflects that.
We expect the DISC21 benchmark to promote image copy detection as an important
and challenging computer vision task and refresh the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1"&gt;Matthijs Douze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1"&gt;Giorgos Tolias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pizzi_E/0/1/0/all/0/1"&gt;Ed Pizzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papakipos_Z/0/1/0/all/0/1"&gt;Zo&amp;#xeb; Papakipos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanussot_L/0/1/0/all/0/1"&gt;Lowik Chanussot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1"&gt;Filip Radenovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenicek_T/0/1/0/all/0/1"&gt;Tomas Jenicek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maximov_M/0/1/0/all/0/1"&gt;Maxim Maximov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1"&gt;Laura Leal-Taix&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1"&gt;Ismail Elezi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chum_O/0/1/0/all/0/1"&gt;Ond&amp;#x159;ej Chum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1"&gt;Cristian Canton Ferrer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transductive Few-Shot Learning: Clustering is All You Need?. (arXiv:2106.09516v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09516</id>
        <link href="http://arxiv.org/abs/2106.09516"/>
        <updated>2021-06-18T02:06:35.242Z</updated>
        <summary type="html"><![CDATA[We investigate a general formulation for clustering and transductive few-shot
learning, which integrates prototype-based objectives, Laplacian regularization
and supervision constraints from a few labeled data points. We propose a
concave-convex relaxation of the problem, and derive a computationally
efficient block-coordinate bound optimizer, with convergence guarantee. At each
iteration,our optimizer computes independent (parallel) updates for each
point-to-cluster assignment. Therefore, it could be trivially distributed for
large-scale clustering and few-shot tasks. Furthermore, we provides a thorough
convergence analysis based on point-to-set maps. Were port comprehensive
clustering and few-shot learning experiments over various data sets, showing
that our method yields competitive performances, in term of accuracy and
optimization quality, while scaling up to large problems. Using standard
training on the base classes, without resorting to complex meta-learning and
episodic-training strategies, our approach outperforms state-of-the-art
few-shot methods by significant margins, across various models, settings and
data sets. Surprisingly, we found that even standard clustering procedures
(e.g., K-means), which correspond to particular, non-regularized cases of our
general model, already achieve competitive performances in comparison to the
state-of-the-art in few-shot learning. These surprising results point to the
limitations of the current few-shot benchmarks, and question the viability of a
large body of convoluted few-shot learning techniques in the recent literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziko_I/0/1/0/all/0/1"&gt;Imtiaz Masud Ziko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1"&gt;Malik Boudiaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1"&gt;Jose Dolz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1"&gt;Eric Granger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ismail Ben Ayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Associate Every Segment for Video Panoptic Segmentation. (arXiv:2106.09453v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09453</id>
        <link href="http://arxiv.org/abs/2106.09453"/>
        <updated>2021-06-18T02:06:35.235Z</updated>
        <summary type="html"><![CDATA[Temporal correspondence - linking pixels or objects across frames - is a
fundamental supervisory signal for the video models. For the panoptic
understanding of dynamic scenes, we further extend this concept to every
segment. Specifically, we aim to learn coarse segment-level matching and fine
pixel-level matching together. We implement this idea by designing two novel
learning objectives. To validate our proposals, we adopt a deep siamese model
and train the model to learn the temporal correspondence on two different
levels (i.e., segment and pixel) along with the target task. At inference time,
the model processes each frame independently without any extra computation and
post-processing. We show that our per-frame inference model can achieve new
state-of-the-art results on Cityscapes-VPS and VIPER datasets. Moreover, due to
its high efficiency, the model runs in a fraction of time (3x) compared to the
previous state-of-the-art approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1"&gt;Sanghyun Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dahun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joon-Young Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1"&gt;In So Kweon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving On-Screen Sound Separation for Open Domain Videos with Audio-Visual Self-attention. (arXiv:2106.09669v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.09669</id>
        <link href="http://arxiv.org/abs/2106.09669"/>
        <updated>2021-06-18T02:06:35.227Z</updated>
        <summary type="html"><![CDATA[We introduce a state-of-the-art audio-visual on-screen sound separation
system which is capable of learning to separate sounds and associate them with
on-screen objects by looking at in-the-wild videos. We identify limitations of
previous work on audiovisual on-screen sound separation, including the
simplicity and coarse resolution of spatio-temporal attention, and poor
convergence of the audio separation model. Our proposed model addresses these
issues using cross-modal and self-attention modules that capture audio-visual
dependencies at a finer resolution over time, and by unsupervised pre-training
of audio separation model. These improvements allow the model to generalize to
a much wider set of unseen videos. For evaluation and semi-supervised training,
we collected human annotations of on-screen audio from a large database of
in-the-wild videos (YFCC100M). Our results show marked improvements in
on-screen separation performance, in more general conditions than previous
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1"&gt;Efthymios Tzinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wisdom_S/0/1/0/all/0/1"&gt;Scott Wisdom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1"&gt;Tal Remez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hershey_J/0/1/0/all/0/1"&gt;John R. Hershey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Segmentation of the Prostate on 3D Trans-rectal Ultrasound Images using Statistical Shape Models and Convolutional Neural Networks. (arXiv:2106.09662v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09662</id>
        <link href="http://arxiv.org/abs/2106.09662"/>
        <updated>2021-06-18T02:06:35.208Z</updated>
        <summary type="html"><![CDATA[In this work we propose to segment the prostate on a challenging dataset of
trans-rectal ultrasound (TRUS) images using convolutional neural networks
(CNNs) and statistical shape models (SSMs). TRUS is commonly used for a number
of image-guided interventions on the prostate. Fast and accurate segmentation
on the organ in these images is crucial to planning and fusion with other
modalities such as magnetic resonance images (MRIs) . However, TRUS has limited
soft tissue contrast and signal to noise ratio which makes the task of
segmenting the prostate challenging and subject to inter-observer and
intra-observer variability. This is especially problematic at the base and apex
where the gland boundary is hard to define. In this paper, we aim to tackle
this problem by taking advantage of shape priors learnt on an MR dataset which
has higher soft tissue contrast allowing the prostate to be contoured more
accurately. We use this shape prior in combination with a prostate tissue
probability map computed by a CNN for segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Samei_G/0/1/0/all/0/1"&gt;Golnoosh Samei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karimi_D/0/1/0/all/0/1"&gt;Davood Karimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kesch_C/0/1/0/all/0/1"&gt;Claudia Kesch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salcudean_S/0/1/0/all/0/1"&gt;Septimiu Salcudean&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seesaw Loss for Long-Tailed Instance Segmentation. (arXiv:2008.10032v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10032</id>
        <link href="http://arxiv.org/abs/2008.10032"/>
        <updated>2021-06-18T02:06:35.200Z</updated>
        <summary type="html"><![CDATA[Instance segmentation has witnessed a remarkable progress on class-balanced
benchmarks. However, they fail to perform as accurately in real-world
scenarios, where the category distribution of objects naturally comes with a
long tail. Instances of head classes dominate a long-tailed dataset and they
serve as negative samples of tail categories. The overwhelming gradients of
negative samples on tail classes lead to a biased learning process for
classifiers. Consequently, objects of tail categories are more likely to be
misclassified as backgrounds or head categories. To tackle this problem, we
propose Seesaw Loss to dynamically re-balance gradients of positive and
negative samples for each category, with two complementary factors, i.e.,
mitigation factor and compensation factor. The mitigation factor reduces
punishments to tail categories w.r.t. the ratio of cumulative training
instances between different categories. Meanwhile, the compensation factor
increases the penalty of misclassified instances to avoid false positives of
tail categories. We conduct extensive experiments on Seesaw Loss with
mainstream frameworks and different data sampling strategies. With a simple
end-to-end training pipeline, Seesaw Loss obtains significant gains over
Cross-Entropy Loss, and achieves state-of-the-art performance on LVIS dataset
without bells and whistles. Code is available at
https://github.com/open-mmlab/mmdetection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1"&gt;Yuhang Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yuhang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1"&gt;Jiangmiao Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1"&gt;Tao Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dahua Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep HDR Hallucination for Inverse Tone Mapping. (arXiv:2106.09486v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09486</id>
        <link href="http://arxiv.org/abs/2106.09486"/>
        <updated>2021-06-18T02:06:35.193Z</updated>
        <summary type="html"><![CDATA[Inverse Tone Mapping (ITM) methods attempt to reconstruct High Dynamic Range
(HDR) information from Low Dynamic Range (LDR) image content. The dynamic range
of well-exposed areas must be expanded and any missing information due to
over/under-exposure must be recovered (hallucinated). The majority of methods
focus on the former and are relatively successful, while most attempts on the
latter are not of sufficient quality, even ones based on Convolutional Neural
Networks (CNNs). A major factor for the reduced inpainting quality in some
works is the choice of loss function. Work based on Generative Adversarial
Networks (GANs) shows promising results for image synthesis and LDR inpainting,
suggesting that GAN losses can improve inverse tone mapping results. This work
presents a GAN-based method that hallucinates missing information from badly
exposed areas in LDR images and compares its efficacy with alternative
variations. The proposed method is quantitatively competitive with
state-of-the-art inverse tone mapping methods, providing good dynamic range
expansion for well-exposed areas and plausible hallucinations for saturated and
under-exposed areas. A density-based normalisation method, targeted for HDR
content, is also proposed, as well as an HDR data augmentation method targeted
for HDR hallucination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marnerides_D/0/1/0/all/0/1"&gt;Demetris Marnerides&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bashford_Rogers_T/0/1/0/all/0/1"&gt;Thomas Bashford-Rogers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Debattista_K/0/1/0/all/0/1"&gt;Kurt Debattista&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Predict Visual Attributes in the Wild. (arXiv:2106.09707v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09707</id>
        <link href="http://arxiv.org/abs/2106.09707"/>
        <updated>2021-06-18T02:06:35.187Z</updated>
        <summary type="html"><![CDATA[Visual attributes constitute a large portion of information contained in a
scene. Objects can be described using a wide variety of attributes which
portray their visual appearance (color, texture), geometry (shape, size,
posture), and other intrinsic properties (state, action). Existing work is
mostly limited to study of attribute prediction in specific domains. In this
paper, we introduce a large-scale in-the-wild visual attribute prediction
dataset consisting of over 927K attribute annotations for over 260K object
instances. Formally, object attribute prediction is a multi-label
classification problem where all attributes that apply to an object must be
predicted. Our dataset poses significant challenges to existing methods due to
large number of attributes, label sparsity, data imbalance, and object
occlusion. To this end, we propose several techniques that systematically
tackle these challenges, including a base model that utilizes both low- and
high-level CNN features with multi-hop attention, reweighting and resampling
techniques, a novel negative label expansion scheme, and a novel supervised
attribute-aware contrastive learning algorithm. Using these techniques, we
achieve near 3.7 mAP and 5.7 overall F1 points improvement over the current
state of the art. Further details about the VAW dataset can be found at
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pham_K/0/1/0/all/0/1"&gt;Khoi Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kafle_K/0/1/0/all/0/1"&gt;Kushal Kafle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhe Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zhihong Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1"&gt;Scott Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1"&gt;Quan Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Abhinav Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Anytime Learning at Macroscale. (arXiv:2106.09563v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09563</id>
        <link href="http://arxiv.org/abs/2106.09563"/>
        <updated>2021-06-18T02:06:35.181Z</updated>
        <summary type="html"><![CDATA[Classical machine learning frameworks assume access to a possibly large
dataset in order to train a predictive model. In many practical applications
however, data does not arrive all at once, but in batches over time. This
creates a natural trade-off between accuracy of a model and time to obtain such
a model. A greedy predictor could produce non-trivial predictions by
immediately training on batches as soon as these become available but, it may
also make sub-optimal use of future data. On the other hand, a tardy predictor
could wait for a long time to aggregate several batches into a larger dataset,
but ultimately deliver a much better performance. In this work, we consider
such a streaming learning setting, which we dub {\em anytime learning at
macroscale} (ALMA). It is an instance of anytime learning applied not at the
level of a single chunk of data, but at the level of the entire sequence of
large batches. We first formalize this learning setting, we then introduce
metrics to assess how well learners perform on the given task for a given
memory and compute budget, and finally we test several baseline approaches on
standard benchmarks repurposed for anytime learning at macroscale. The general
finding is that bigger models always generalize better. In particular, it is
important to grow model capacity over time if the initial model is relatively
small. Moreover, updating the model at an intermediate rate strikes the best
trade off between accuracy and time to obtain a useful predictor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1"&gt;Lucas Caccia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1"&gt;Myle Ott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1"&gt;Marc&amp;#x27;Aurelio Ranzato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denoyer_L/0/1/0/all/0/1"&gt;Ludovic Denoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Visual Robustness by Causal Intervention. (arXiv:2106.09534v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09534</id>
        <link href="http://arxiv.org/abs/2106.09534"/>
        <updated>2021-06-18T02:06:35.160Z</updated>
        <summary type="html"><![CDATA[Adversarial training is the de facto most promising defense against
adversarial examples. Yet, its passive nature inevitably prevents it from being
immune to unknown attackers. To achieve a proactive defense, we need a more
fundamental understanding of adversarial examples, beyond the popular bounded
threat model. In this paper, we provide a causal viewpoint of adversarial
vulnerability: the cause is the confounder ubiquitously existing in learning,
where attackers are precisely exploiting the confounding effect. Therefore, a
fundamental solution for adversarial robustness is causal intervention. As the
confounder is unobserved in general, we propose to use the instrumental
variable that achieves intervention without the need for confounder
observation. We term our robust training method as Causal intervention by
instrumental Variable (CiiV). It has a differentiable retinotopic sampling
layer and a consistency loss, which is stable and guaranteed not to suffer from
gradient obfuscation. Extensive experiments on a wide spectrum of attackers and
settings applied in MNIST, CIFAR-10, and mini-ImageNet datasets empirically
demonstrate that CiiV is robust to adaptive attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Kaihua Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1"&gt;Mingyuan Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanwang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To fit or not to fit: Model-based Face Reconstruction and Occlusion Segmentation from Weak Supervision. (arXiv:2106.09614v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09614</id>
        <link href="http://arxiv.org/abs/2106.09614"/>
        <updated>2021-06-18T02:06:35.153Z</updated>
        <summary type="html"><![CDATA[3D face reconstruction from a single image is challenging due to its
ill-posed nature. Model-based face autoencoders address this issue effectively
by fitting a face model to the target image in a weakly supervised manner.
However, in unconstrained environments occlusions distort the face
reconstruction because the model often erroneously tries to adapt to occluded
face regions. Supervised occlusion segmentation is a viable solution to avoid
the fitting of occluded face regions, but it requires a large amount of
annotated training data. In this work, we enable model-based face autoencoders
to segment occluders accurately without requiring any additional supervision
during training, and this separates regions where the model will be fitted from
those where it will not be fitted. To achieve this, we extend face autoencoders
with a segmentation network. The segmentation network decides which regions the
model should adapt to by reaching balances in a trade-off between including
pixels and adapting the model to them, and excluding pixels so that the model
fitting is not negatively affected and reaches higher overall reconstruction
accuracy on pixels showing the face. This leads to a synergistic effect, in
which the occlusion segmentation guides the training of the face autoencoder to
constrain the fitting in the non-occluded regions, while the improved fitting
enables the segmentation model to better predict the occluded face regions.
Qualitative and quantitative experiments on the CelebA-HQ database and the AR
database verify the effectiveness of our model in improving 3D face
reconstruction under occlusions and in enabling accurate occlusion segmentation
from weak supervision only. Code available at
https://github.com/unibas-gravis/Occlusion-Robust-MoFA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunlu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morel_Forster_A/0/1/0/all/0/1"&gt;Andreas Morel-Forster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetter_T/0/1/0/all/0/1"&gt;Thomas Vetter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1"&gt;Bernhard Egger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1"&gt;Adam Kortylewski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AttDLNet: Attention-based DL Network for 3D LiDAR Place Recognition. (arXiv:2106.09637v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09637</id>
        <link href="http://arxiv.org/abs/2106.09637"/>
        <updated>2021-06-18T02:06:35.147Z</updated>
        <summary type="html"><![CDATA[Deep networks have been progressively adapted to new sensor modalities,
namely to 3D LiDAR, which led to unprecedented achievements in autonomous
vehicle-related applications such as place recognition. One of the main
challenges of deep models in place recognition is to extract efficient and
descriptive feature representations that relate places based on their
similarity. To address the problem of place recognition using LiDAR data, this
paper proposes a novel 3D LiDAR-based deep learning network (named AttDLNet)
that comprises an encoder network and exploits an attention mechanism to
selectively focus on long-range context and interfeature relationships. The
proposed network is trained and validated on the KITTI dataset, using the
cosine loss for training and a retrieval-based place recognition pipeline for
validation. Additionally, an ablation study is presented to assess the best
network configuration. Results show that the encoder network features are
already very descriptive, but adding attention to the network further improves
performance. From the ablation study, results indicate that the middle encoder
layers have the highest mean performance, while deeper layers are more robust
to orientation change. The code is publicly available on the project website:
https://github.com/Cybonic/ AttDLNet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barros_T/0/1/0/all/0/1"&gt;Tiago Barros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrote_L/0/1/0/all/0/1"&gt;Lu&amp;#xed;s Garrote&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_R/0/1/0/all/0/1"&gt;Ricardo Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Premebida_C/0/1/0/all/0/1"&gt;Cristiano Premebida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nunes_U/0/1/0/all/0/1"&gt;Urbano J. Nunes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Knowledge Distillation with A Single Stream Structure for RGB-DSalient Object Detection. (arXiv:2106.09517v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09517</id>
        <link href="http://arxiv.org/abs/2106.09517"/>
        <updated>2021-06-18T02:06:35.139Z</updated>
        <summary type="html"><![CDATA[RGB-D salient object detection(SOD) demonstrates its superiority on detecting
in complex environments due to the additional depth information introduced in
the data. Inevitably, an independent stream is introduced to extract features
from depth images, leading to extra computation and parameters. This
methodology which sacrifices the model size to improve the detection accuracy
may impede the practical application of SOD problems. To tackle this dilemma,
we propose a dynamic distillation method along with a lightweight framework,
which significantly reduces the parameters. This method considers the factors
of both teacher and student performance within the training stage and
dynamically assigns the distillation weight instead of applying a fixed weight
on the student model. Extensive experiments are conducted on five public
datasets to demonstrate that our method can achieve competitive performance
compared to 10 prior methods through a 78.2MB lightweight structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1"&gt;Guangyu Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stathaki_T/0/1/0/all/0/1"&gt;Tania Stathaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orthogonal-Pad\'e Activation Functions: Trainable Activation functions for smooth and faster convergence in deep networks. (arXiv:2106.09693v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.09693</id>
        <link href="http://arxiv.org/abs/2106.09693"/>
        <updated>2021-06-18T02:06:35.131Z</updated>
        <summary type="html"><![CDATA[We have proposed orthogonal-Pad\'e activation functions, which are trainable
activation functions and show that they have faster learning capability and
improves the accuracy in standard deep learning datasets and models. Based on
our experiments, we have found two best candidates out of six orthogonal-Pad\'e
activations, which we call safe Hermite-Pade (HP) activation functions, namely
HP-1 and HP-2. When compared to ReLU, HP-1 and HP-2 has an increment in top-1
accuracy by 5.06% and 4.63% respectively in PreActResNet-34, by 3.02% and 2.75%
respectively in MobileNet V2 model on CIFAR100 dataset while on CIFAR10 dataset
top-1 accuracy increases by 2.02% and 1.78% respectively in PreActResNet-34, by
2.24% and 2.06% respectively in LeNet, by 2.15% and 2.03% respectively in
Efficientnet B0.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1"&gt;Koushik Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1"&gt;Shilpak Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1"&gt;Ashish Kumar Pandey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge distillation from multi-modal to mono-modal segmentation networks. (arXiv:2106.09564v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09564</id>
        <link href="http://arxiv.org/abs/2106.09564"/>
        <updated>2021-06-18T02:06:35.109Z</updated>
        <summary type="html"><![CDATA[The joint use of multiple imaging modalities for medical image segmentation
has been widely studied in recent years. The fusion of information from
different modalities has demonstrated to improve the segmentation accuracy,
with respect to mono-modal segmentations, in several applications. However,
acquiring multiple modalities is usually not possible in a clinical setting due
to a limited number of physicians and scanners, and to limit costs and scan
time. Most of the time, only one modality is acquired. In this paper, we
propose KD-Net, a framework to transfer knowledge from a trained multi-modal
network (teacher) to a mono-modal one (student). The proposed method is an
adaptation of the generalized distillation framework where the student network
is trained on a subset (1 modality) of the teacher's inputs (n modalities). We
illustrate the effectiveness of the proposed framework in brain tumor
segmentation with the BraTS 2018 dataset. Using different architectures, we
show that the student network effectively learns from the teacher and always
outperforms the baseline mono-modal network in terms of segmentation accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Minhao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maillard_M/0/1/0/all/0/1"&gt;Matthis Maillard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciceri_T/0/1/0/all/0/1"&gt;Tommaso Ciceri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbera_G/0/1/0/all/0/1"&gt;Giammarco La Barbera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bloch_I/0/1/0/all/0/1"&gt;Isabelle Bloch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1"&gt;Pietro Gori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JOKR: Joint Keypoint Representation for Unsupervised Cross-Domain Motion Retargeting. (arXiv:2106.09679v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09679</id>
        <link href="http://arxiv.org/abs/2106.09679"/>
        <updated>2021-06-18T02:06:35.089Z</updated>
        <summary type="html"><![CDATA[The task of unsupervised motion retargeting in videos has seen substantial
advancements through the use of deep neural networks. While early works
concentrated on specific object priors such as a human face or body, recent
work considered the unsupervised case. When the source and target videos,
however, are of different shapes, current methods fail. To alleviate this
problem, we introduce JOKR - a JOint Keypoint Representation that captures the
motion common to both the source and target videos, without requiring any
object prior or data collection. By employing a domain confusion term, we
enforce the unsupervised keypoint representations of both videos to be
indistinguishable. This encourages disentanglement between the parts of the
motion that are common to the two domains, and their distinctive appearance and
motion, enabling the generation of videos that capture the motion of the one
while depicting the style of the other. To enable cases where the objects are
of different proportions or orientations, we apply a learned affine
transformation between the JOKRs. This augments the representation to be affine
invariant, and in practice broadens the variety of possible retargeting pairs.
This geometry-driven representation enables further intuitive control, such as
temporal coherence and manual editing. Through comprehensive experimentation,
we demonstrate the applicability of our method to different challenging
cross-domain video pairs. We evaluate our method both qualitatively and
quantitatively, and demonstrate that our method handles various cross-domain
scenarios, such as different animals, different flowers, and humans. We also
demonstrate superior temporal coherency and visual quality compared to
state-of-the-art alternatives, through statistical metrics and a user study.
Source code and videos can be found at https://rmokady.github.io/JOKR/ .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mokady_R/0/1/0/all/0/1"&gt;Ron Mokady&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzaban_R/0/1/0/all/0/1"&gt;Rotem Tzaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1"&gt;Sagie Benaim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1"&gt;Amit H. Bermano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving Eye-tracking Using Deep Learning. (arXiv:2106.09621v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09621</id>
        <link href="http://arxiv.org/abs/2106.09621"/>
        <updated>2021-06-18T02:06:35.080Z</updated>
        <summary type="html"><![CDATA[The expanding usage of complex machine learning methods like deep learning
has led to an explosion in human activity recognition, particularly applied to
health. In particular, as part of a larger body sensor network system, face and
full-body analysis is becoming increasingly common for evaluating health
status. However, complex models which handle private and sometimes protected
data, raise concerns about the potential leak of identifiable data. In this
work, we focus on the case of a deep network model trained on images of
individual faces. Full-face video recordings taken from 493 individuals
undergoing an eye-tracking based evaluation of neurological function were used.
Outputs, gradients, intermediate layer outputs, loss, and labels were used as
inputs for a deep network with an added support vector machine emission layer
to recognize membership in the training data. The inference attack method and
associated mathematical analysis indicate that there is a low likelihood of
unintended memorization of facial features in the deep learning model. In this
study, it is showed that the named model preserves the integrity of training
data with reasonable confidence. The same process can be implemented in similar
conditions for different models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seyedi_S/0/1/0/all/0/1"&gt;Salman Seyedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zifan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levey_A/0/1/0/all/0/1"&gt;Allan Levey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifford_G/0/1/0/all/0/1"&gt;Gari D. Clifford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Autoregressive Transformer for Image Captioning. (arXiv:2106.09436v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09436</id>
        <link href="http://arxiv.org/abs/2106.09436"/>
        <updated>2021-06-18T02:06:35.028Z</updated>
        <summary type="html"><![CDATA[Current state-of-the-art image captioning models adopt autoregressive
decoders, \ie they generate each word by conditioning on previously generated
words, which leads to heavy latency during inference. To tackle this issue,
non-autoregressive image captioning models have recently been proposed to
significantly accelerate the speed of inference by generating all words in
parallel. However, these non-autoregressive models inevitably suffer from large
generation quality degradation since they remove words dependence excessively.
To make a better trade-off between speed and quality, we introduce a
semi-autoregressive model for image captioning~(dubbed as SATIC), which keeps
the autoregressive property in global but generates words parallelly in local.
Based on Transformer, there are only a few modifications needed to implement
SATIC. Extensive experiments on the MSCOCO image captioning benchmark show that
SATIC can achieve a better trade-off without bells and whistles. Code is
available at {\color{magenta}\url{https://github.com/YuanEZhou/satic}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuanen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhenzhen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuroMorph: Unsupervised Shape Interpolation and Correspondence in One Go. (arXiv:2106.09431v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09431</id>
        <link href="http://arxiv.org/abs/2106.09431"/>
        <updated>2021-06-18T02:06:35.022Z</updated>
        <summary type="html"><![CDATA[We present NeuroMorph, a new neural network architecture that takes as input
two 3D shapes and produces in one go, i.e. in a single feed forward pass, a
smooth interpolation and point-to-point correspondences between them. The
interpolation, expressed as a deformation field, changes the pose of the source
shape to resemble the target, but leaves the object identity unchanged.
NeuroMorph uses an elegant architecture combining graph convolutions with
global feature pooling to extract local features. During training, the model is
incentivized to create realistic deformations by approximating geodesics on the
underlying shape space manifold. This strong geometric prior allows to train
our model end-to-end and in a fully unsupervised manner without requiring any
manual correspondence annotations. NeuroMorph works well for a large variety of
input shapes, including non-isometric pairs from different object categories.
It obtains state-of-the-art results for both shape correspondence and
interpolation tasks, matching or surpassing the performance of recent
unsupervised and supervised methods on multiple benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eisenberger_M/0/1/0/all/0/1"&gt;Marvin Eisenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1"&gt;David Novotny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerchenbaum_G/0/1/0/all/0/1"&gt;Gael Kerchenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labatut_P/0/1/0/all/0/1"&gt;Patrick Labatut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1"&gt;Natalia Neverova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1"&gt;Andrea Vedaldi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Class Balancing GAN with a Classifier in the Loop. (arXiv:2106.09402v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09402</id>
        <link href="http://arxiv.org/abs/2106.09402"/>
        <updated>2021-06-18T02:06:35.003Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have swiftly evolved to imitate
increasingly complex image distributions. However, majority of the developments
focus on performance of GANs on balanced datasets. We find that the existing
GANs and their training regimes which work well on balanced datasets fail to be
effective in case of imbalanced (i.e. long-tailed) datasets. In this work we
introduce a novel theoretically motivated Class Balancing regularizer for
training GANs. Our regularizer makes use of the knowledge from a pre-trained
classifier to ensure balanced learning of all the classes in the dataset. This
is achieved via modelling the effective class frequency based on the
exponential forgetting observed in neural networks and encouraging the GAN to
focus on underrepresented classes. We demonstrate the utility of our
regularizer in learning representations for long-tailed distributions via
achieving better performance than existing approaches over multiple datasets.
Specifically, when applied to an unconditional GAN, it improves the FID from
$13.03$ to $9.01$ on the long-tailed iNaturalist-$2019$ dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rangwani_H/0/1/0/all/0/1"&gt;Harsh Rangwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1"&gt;Konda Reddy Mopuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1"&gt;R. Venkatesh Babu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Volta at SemEval-2021 Task 9: Statement Verification and Evidence Finding with Tables using TAPAS and Transfer Learning. (arXiv:2106.00248v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00248</id>
        <link href="http://arxiv.org/abs/2106.00248"/>
        <updated>2021-06-18T02:06:34.994Z</updated>
        <summary type="html"><![CDATA[Tables are widely used in various kinds of documents to present information
concisely. Understanding tables is a challenging problem that requires an
understanding of language and table structure, along with numerical and logical
reasoning. In this paper, we present our systems to solve Task 9 of
SemEval-2021: Statement Verification and Evidence Finding with Tables
(SEM-TAB-FACTS). The task consists of two subtasks: (A) Given a table and a
statement, predicting whether the table supports the statement and (B)
Predicting which cells in the table provide evidence for/against the statement.
We fine-tune TAPAS (a model which extends BERT's architecture to capture
tabular structure) for both the subtasks as it has shown state-of-the-art
performance in various table understanding tasks. In subtask A, we evaluate how
transfer learning and standardizing tables to have a single header row improves
TAPAS' performance. In subtask B, we evaluate how different fine-tuning
strategies can improve TAPAS' performance. Our systems achieve an F1 score of
67.34 in subtask A three-way classification, 72.89 in subtask A two-way
classification, and 62.95 in subtask B.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1"&gt;Devansh Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1"&gt;Manish Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Perceptual Manifold of Fonts. (arXiv:2106.09198v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2106.09198</id>
        <link href="http://arxiv.org/abs/2106.09198"/>
        <updated>2021-06-18T02:06:34.987Z</updated>
        <summary type="html"><![CDATA[Along the rapid development of deep learning techniques in generative models,
it is becoming an urgent issue to combine machine intelligence with human
intelligence to solve the practical applications. Motivated by this
methodology, this work aims to adjust the machine generated character fonts
with the effort of human workers in the perception study. Although numerous
fonts are available online for public usage, it is difficult and challenging to
generate and explore a font to meet the preferences for common users. To solve
the specific issue, we propose the perceptual manifold of fonts to visualize
the perceptual adjustment in the latent space of a generative model of fonts.
In our framework, we adopt the variational autoencoder network for the font
generation. Then, we conduct a perceptual study on the generated fonts from the
multi-dimensional latent space of the generative model. After we obtained the
distribution data of specific preferences, we utilize manifold learning
approach to visualize the font distribution. In contrast to the conventional
user interface in our user study, the proposed font-exploring user interface is
efficient and helpful in the designated user preference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Haoran Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fujita_Y/0/1/0/all/0/1"&gt;Yuki Fujita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miyata_K/0/1/0/all/0/1"&gt;Kazunori Miyata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Main Character Recognition for Photographic Studies. (arXiv:2106.09064v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09064</id>
        <link href="http://arxiv.org/abs/2106.09064"/>
        <updated>2021-06-18T02:06:34.979Z</updated>
        <summary type="html"><![CDATA[Main characters in images are the most important humans that catch the
viewer's attention upon first look, and they are emphasized by properties such
as size, position, color saturation, and sharpness of focus. Identifying the
main character in images plays an important role in traditional photographic
studies and media analysis, but the task is performed manually and can be slow
and laborious. Furthermore, selection of main characters can be sometimes
subjective. In this paper, we analyze the feasibility of solving the main
character recognition needed for photographic studies automatically and propose
a method for identifying the main characters. The proposed method uses machine
learning based human pose estimation along with traditional computer vision
approaches for this task. We approach the task as a binary classification
problem where each detected human is classified either as a main character or
not. To evaluate both the subjectivity of the task and the performance of our
method, we collected a dataset of 300 varying images from multiple sources and
asked five people, a photographic researcher and four other persons, to
annotate the main characters. Our analysis showed a relatively high agreement
between different annotators. The proposed method achieved a promising F1 score
of 0.83 on the full image set and 0.96 on a subset evaluated as most clear and
important cases by the photographic researcher.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seker_M/0/1/0/all/0/1"&gt;Mert Seker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannisto_A/0/1/0/all/0/1"&gt;Anssi M&amp;#xe4;nnist&amp;#xf6;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1"&gt;Jenni Raitoharju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[using multiple losses for accurate facial age estimation. (arXiv:2106.09393v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09393</id>
        <link href="http://arxiv.org/abs/2106.09393"/>
        <updated>2021-06-18T02:06:34.971Z</updated>
        <summary type="html"><![CDATA[Age estimation is an essential challenge in computer vision. With the
advances of convolutional neural networks, the performance of age estimation
has been dramatically improved. Existing approaches usually treat age
estimation as a classification problem. However, the age labels are ambiguous,
thus make the classification task difficult. In this paper, we propose a simple
yet effective approach for age estimation, which improves the performance
compared to classification-based methods. The method combines four
classification losses and one regression loss representing different class
granularities together, and we name it as Age-Granularity-Net. We validate the
Age-Granularity-Net framework on the CVPR Chalearn 2016 dataset, and extensive
experiments show that the proposed approach can reduce the prediction error
compared to any individual loss. The source code link is
https://github.com/yipersevere/age-estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huttunen_H/0/1/0/all/0/1"&gt;Heikki Huttunen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elomaa_T/0/1/0/all/0/1"&gt;Tapio Elomaa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Training Data Generation of Handwritten Formulas using Generative Adversarial Networks with Self-Attention. (arXiv:2106.09432v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09432</id>
        <link href="http://arxiv.org/abs/2106.09432"/>
        <updated>2021-06-18T02:06:34.953Z</updated>
        <summary type="html"><![CDATA[The recognition of handwritten mathematical expressions in images and video
frames is a difficult and unsolved problem yet. Deep convectional neural
networks are basically a promising approach, but typically require a large
amount of labeled training data. However, such a large training dataset does
not exist for the task of handwritten formula recognition. In this paper, we
introduce a system that creates a large set of synthesized training examples of
mathematical expressions which are derived from LaTeX documents. For this
purpose, we propose a novel attention-based generative adversarial network to
translate rendered equations to handwritten formulas. The datasets generated by
this approach contain hundreds of thousands of formulas, making it ideal for
pretraining or the design of more complex models. We evaluate our synthesized
dataset and the recognition approach on the CROHME 2014 benchmark dataset.
Experimental results demonstrate the feasibility of the approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Springstein_M/0/1/0/all/0/1"&gt;Matthias Springstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1"&gt;Eric M&amp;#xfc;ller-Budack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1"&gt;Ralph Ewerth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Dark Side of Calibration for Modern Neural Networks. (arXiv:2106.09385v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09385</id>
        <link href="http://arxiv.org/abs/2106.09385"/>
        <updated>2021-06-18T02:06:34.947Z</updated>
        <summary type="html"><![CDATA[Modern neural networks are highly uncalibrated. It poses a significant
challenge for safety-critical systems to utilise deep neural networks (DNNs),
reliably. Many recently proposed approaches have demonstrated substantial
progress in improving DNN calibration. However, they hardly touch upon
refinement, which historically has been an essential aspect of calibration.
Refinement indicates separability of a network's correct and incorrect
predictions. This paper presents a theoretically and empirically supported
exposition for reviewing a model's calibration and refinement. Firstly, we show
the breakdown of expected calibration error (ECE), into predicted confidence
and refinement. Connecting with this result, we highlight that regularisation
based calibration only focuses on naively reducing a model's confidence. This
logically has a severe downside to a model's refinement. We support our claims
through rigorous empirical evaluations of many state of the art calibration
approaches on standard datasets. We find that many calibration approaches with
the likes of label smoothing, mixup etc. lower the utility of a DNN by
degrading its refinement. Even under natural data shift, this
calibration-refinement trade-off holds for the majority of calibration methods.
These findings call for an urgent retrospective into some popular pathways
taken for modern DNN calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Aditya Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bay_A/0/1/0/all/0/1"&gt;Alessandro Bay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_B/0/1/0/all/0/1"&gt;Biswa Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirabile_A/0/1/0/all/0/1"&gt;Andrea Mirabile&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Subdomain Adaptation Network for Image Classification. (arXiv:2106.09388v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09388</id>
        <link href="http://arxiv.org/abs/2106.09388"/>
        <updated>2021-06-18T02:06:34.937Z</updated>
        <summary type="html"><![CDATA[For a target task where labeled data is unavailable, domain adaptation can
transfer a learner from a different source domain. Previous deep domain
adaptation methods mainly learn a global domain shift, i.e., align the global
source and target distributions without considering the relationships between
two subdomains within the same category of different domains, leading to
unsatisfying transfer learning performance without capturing the fine-grained
information. Recently, more and more researchers pay attention to Subdomain
Adaptation which focuses on accurately aligning the distributions of the
relevant subdomains. However, most of them are adversarial methods which
contain several loss functions and converge slowly. Based on this, we present
Deep Subdomain Adaptation Network (DSAN) which learns a transfer network by
aligning the relevant subdomain distributions of domain-specific layer
activations across different domains based on a local maximum mean discrepancy
(LMMD). Our DSAN is very simple but effective which does not need adversarial
training and converges fast. The adaptation can be achieved easily with most
feed-forward network models by extending them with LMMD loss, which can be
trained efficiently via back-propagation. Experiments demonstrate that DSAN can
achieve remarkable results on both object recognition tasks and digit
classification tasks. Our code will be available at:
https://github.com/easezyc/deep-transfer-learning]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yongchun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1"&gt;Fuzhen Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1"&gt;Guolin Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingwu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1"&gt;Jiang Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hui Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1"&gt;Qing He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Cross-Domain Text-to-SQL Semantic Parsing with Auxiliary Task. (arXiv:2106.09588v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09588</id>
        <link href="http://arxiv.org/abs/2106.09588"/>
        <updated>2021-06-18T02:06:34.929Z</updated>
        <summary type="html"><![CDATA[In this work, we focus on two crucial components in the cross-domain
text-to-SQL semantic parsing task: schema linking and value filling. To
encourage the model to learn better encoding ability, we propose a column
selection auxiliary task to empower the encoder with the relevance matching
capability by using explicit learning targets. Furthermore, we propose two
value filling methods to build the bridge from the existing zero-shot semantic
parsers to real-world applications, considering most of the existing parsers
ignore the values filling in the synthesized SQL. With experiments on Spider,
our proposed framework improves over the baselines on the execution accuracy
and exact set match accuracy when database contents are unavailable, and
detailed analysis sheds light on future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1"&gt;Peng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1"&gt;Patrick Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiguo Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Localized Uncertainty Attacks. (arXiv:2106.09222v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.09222</id>
        <link href="http://arxiv.org/abs/2106.09222"/>
        <updated>2021-06-18T02:06:34.922Z</updated>
        <summary type="html"><![CDATA[The susceptibility of deep learning models to adversarial perturbations has
stirred renewed attention in adversarial examples resulting in a number of
attacks. However, most of these attacks fail to encompass a large spectrum of
adversarial perturbations that are imperceptible to humans. In this paper, we
present localized uncertainty attacks, a novel class of threat models against
deterministic and stochastic classifiers. Under this threat model, we create
adversarial examples by perturbing only regions in the inputs where a
classifier is uncertain. To find such regions, we utilize the predictive
uncertainty of the classifier when the classifier is stochastic or, we learn a
surrogate model to amortize the uncertainty when it is deterministic. Unlike
$\ell_p$ ball or functional attacks which perturb inputs indiscriminately, our
targeted changes can be less perceptible. When considered under our threat
model, these attacks still produce strong adversarial examples; with the
examples retaining a greater degree of similarity with the inputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dia_O/0/1/0/all/0/1"&gt;Ousmane Amadou Dia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karaletsos_T/0/1/0/all/0/1"&gt;Theofanis Karaletsos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hazirbas_C/0/1/0/all/0/1"&gt;Caner Hazirbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ferrer_C/0/1/0/all/0/1"&gt;Cristian Canton Ferrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kabul_I/0/1/0/all/0/1"&gt;Ilknur Kaynar Kabul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meijer_E/0/1/0/all/0/1"&gt;Erik Meijer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Sampling-Based Training Criteria for Neural Language Modeling. (arXiv:2104.10507v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10507</id>
        <link href="http://arxiv.org/abs/2104.10507"/>
        <updated>2021-06-18T02:06:34.905Z</updated>
        <summary type="html"><![CDATA[As the vocabulary size of modern word-based language models becomes ever
larger, many sampling-based training criteria are proposed and investigated.
The essence of these sampling methods is that the softmax-related traversal
over the entire vocabulary can be simplified, giving speedups compared to the
baseline. A problem we notice about the current landscape of such sampling
methods is the lack of a systematic comparison and some myths about preferring
one over another. In this work, we consider Monte Carlo sampling, importance
sampling, a novel method we call compensated partial summation, and noise
contrastive estimation. Linking back to the three traditional criteria, namely
mean squared error, binary cross-entropy, and cross-entropy, we derive the
theoretical solutions to the training problems. Contrary to some common belief,
we show that all these sampling methods can perform equally well, as long as we
correct for the intended class posterior probabilities. Experimental results in
language modeling and automatic speech recognition on Switchboard and
LibriSpeech support our claim, with all sampling-based methods showing similar
perplexities and word error rates while giving the expected speedups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yingbo Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thulke_D/0/1/0/all/0/1"&gt;David Thulke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerstenberger_A/0/1/0/all/0/1"&gt;Alexander Gerstenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1"&gt;Khoa Viet Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scrambled Translation Problem: A Problem of Denoising UNMT. (arXiv:1911.01212v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.01212</id>
        <link href="http://arxiv.org/abs/1911.01212"/>
        <updated>2021-06-18T02:06:34.897Z</updated>
        <summary type="html"><![CDATA[In this paper, we identify an interesting kind of error in the output of
Unsupervised Neural Machine Translation (UNMT) systems like
\textit{Undreamt}(footnote). We refer to this error type as \textit{Scrambled
Translation problem}. We observe that UNMT models which use \textit{word
shuffle} noise (as in case of Undreamt) can generate correct words, but fail to
stitch them together to form phrases. As a result, words of the translated
sentence look \textit{scrambled}, resulting in decreased BLEU. We hypothesise
that the reason behind \textit{scrambled translation problem} is 'shuffling
noise' which is introduced in every input sentence as a denoising strategy. To
test our hypothesis, we experiment by retraining UNMT models with a simple
\textit{retraining} strategy. We stop the training of the Denoising UNMT model
after a pre-decided number of iterations and resume the training for the
remaining iterations -- which number is also pre-decided -- using original
sentence as input without adding any noise. Our proposed solution achieves
significant performance improvement UNMT models that train conventionally. We
demonstrate these performance gains on four language pairs, \textit{viz.},
English-French, English-German, English-Spanish, Hindi-Punjabi. Our qualitative
and quantitative analysis shows that the retraining strategy helps achieve
better alignment as observed by attention heatmap and better phrasal
translation, leading to statistically significant improvement in BLEU scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1"&gt;Tamali Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1"&gt;Rudra Murthy V&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1"&gt;Pushpak Bhattacharyya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positional Contrastive Learning for VolumetricMedical Image Segmentation. (arXiv:2106.09157v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09157</id>
        <link href="http://arxiv.org/abs/2106.09157"/>
        <updated>2021-06-18T02:06:34.888Z</updated>
        <summary type="html"><![CDATA[The success of deep learning heavily depends on the availability of large
labeled training sets. However, it is hard to get large labeled datasets in
medical image domain because of the strict privacy concern and costly labeling
efforts. Contrastive learning, an unsupervised learning technique, has been
proved powerful in learning image-level representations from unlabeled data.
The learned encoder can then be transferred or fine-tuned to improve the
performance of downstream tasks with limited labels. A critical step in
contrastive learning is the generation of contrastive data pairs, which is
relatively simple for natural image classification but quite challenging for
medical image segmentation due to the existence of the same tissue or organ
across the dataset. As a result, when applied to medical image segmentation,
most state-of-the-art contrastive learning frameworks inevitably introduce a
lot of false-negative pairs and result in degraded segmentation quality. To
address this issue, we propose a novel positional contrastive learning (PCL)
framework to generate contrastive data pairs by leveraging the position
information in volumetric medical images. Experimental results on CT and MRI
datasets demonstrate that the proposed PCL method can substantially improve the
segmentation performance compared to existing methods in both semi-supervised
setting and transfer learning setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1"&gt;Dewen Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yawen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xinrong Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaowei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Haiyun Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Meiping Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1"&gt;Jian Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jingtong Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yiyu Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optical Mouse: 3D Mouse Pose From Single-View Video. (arXiv:2106.09251v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09251</id>
        <link href="http://arxiv.org/abs/2106.09251"/>
        <updated>2021-06-18T02:06:34.881Z</updated>
        <summary type="html"><![CDATA[We present a method to infer the 3D pose of mice, including the limbs and
feet, from monocular videos. Many human clinical conditions and their
corresponding animal models result in abnormal motion, and accurately measuring
3D motion at scale offers insights into health. The 3D poses improve
classification of health-related attributes over 2D representations. The
inferred poses are accurate enough to estimate stride length even when the feet
are mostly occluded. This method could be applied as part of a continuous
monitoring system to non-invasively measure animal health.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1"&gt;Bo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seybold_B/0/1/0/all/0/1"&gt;Bryan Seybold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1"&gt;David Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sud_A/0/1/0/all/0/1"&gt;Avneesh Sud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruby_G/0/1/0/all/0/1"&gt;Graham Ruby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yi Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Detection of Alzheimer's Disease from Speech and Text. (arXiv:2012.00096v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00096</id>
        <link href="http://arxiv.org/abs/2012.00096"/>
        <updated>2021-06-18T02:06:34.875Z</updated>
        <summary type="html"><![CDATA[Reliable detection of the prodromal stages of Alzheimer's disease (AD)
remains difficult even today because, unlike other neurocognitive impairments,
there is no definitive diagnosis of AD in vivo. In this context, existing
research has shown that patients often develop language impairment even in mild
AD conditions. We propose a multimodal deep learning method that utilizes
speech and the corresponding transcript simultaneously to detect AD. For audio
signals, the proposed audio-based network, a convolutional neural network (CNN)
based model, predicts the diagnosis for multiple speech segments, which are
combined for the final prediction. Similarly, we use contextual embedding
extracted from BERT concatenated with a CNN-generated embedding for classifying
the transcript. The individual predictions of the two models are then combined
to make the final classification. We also perform experiments to analyze the
model performance when Automated Speech Recognition (ASR) system generated
transcripts are used instead of manual transcription in the text-based model.
The proposed method achieves 85.3% 10-fold cross-validation accuracy when
trained and evaluated on the Dementiabank Pitt corpus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Amish Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1"&gt;Sourav Sahoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datar_A/0/1/0/all/0/1"&gt;Arnhav Datar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadiwala_J/0/1/0/all/0/1"&gt;Juned Kadiwala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1"&gt;Hrithwik Shalu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1"&gt;Jimson Mathew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Emotion Label Space Modelling for Affect Lexica. (arXiv:1911.08782v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.08782</id>
        <link href="http://arxiv.org/abs/1911.08782"/>
        <updated>2021-06-18T02:06:34.869Z</updated>
        <summary type="html"><![CDATA[Emotion lexica are commonly used resources to combat data poverty in
automatic emotion detection. However, vocabulary coverage issues, differences
in construction method and discrepancies in emotion framework and
representation result in a heterogeneous landscape of emotion detection
resources, calling for a unified approach to utilising them. To combat this, we
present an extended emotion lexicon of 30,273 unique entries, which is a result
of merging eight existing emotion lexica by means of a multi-view variational
autoencoder (VAE). We showed that a VAE is a valid approach for combining
lexica with different label spaces into a joint emotion label space with a
chosen number of dimensions, and that these dimensions are still interpretable.
We tested the utility of the unified VAE lexicon by employing the lexicon
values as features in an emotion detection model. We found that the VAE lexicon
outperformed individual lexica, but contrary to our expectations, it did not
outperform a naive concatenation of lexica, although it did contribute to the
naive concatenation when added as an extra lexicon. Furthermore, using lexicon
information as additional features next to state-of-the-art language models
usually resulted in a better performance than when no lexicon information was
used.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bruyne_L/0/1/0/all/0/1"&gt;Luna De Bruyne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanasova_P/0/1/0/all/0/1"&gt;Pepa Atanasova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1"&gt;Isabelle Augenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wavelet-Packet Powered Deepfake Image Detection. (arXiv:2106.09369v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09369</id>
        <link href="http://arxiv.org/abs/2106.09369"/>
        <updated>2021-06-18T02:06:34.849Z</updated>
        <summary type="html"><![CDATA[As neural networks become more able to generate realistic artificial images,
they have the potential to improve movies, music, video games and make the
internet an even more creative and inspiring place. Yet, at the same time, the
latest technology potentially enables new digital ways to lie. In response, the
need for a diverse and reliable toolbox arises to identify artificial images
and other content. Previous work primarily relies on pixel-space CNN or the
Fourier transform. To the best of our knowledge, wavelet-based gan analysis and
detection methods have been absent thus far. This paper aims to fill this gap
and describes a wavelet-based approach to gan-generated image analysis and
detection. We evaluate our method on FFHQ, CelebA, and LSUN source
identification problems and find improved or competitive performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolter_M/0/1/0/all/0/1"&gt;Moritz Wolter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanke_F/0/1/0/all/0/1"&gt;Felix Blanke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoyt_C/0/1/0/all/0/1"&gt;Charles Tapley Hoyt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcke_J/0/1/0/all/0/1"&gt;Jochen Garcke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probing Image-Language Transformers for Verb Understanding. (arXiv:2106.09141v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09141</id>
        <link href="http://arxiv.org/abs/2106.09141"/>
        <updated>2021-06-18T02:06:34.840Z</updated>
        <summary type="html"><![CDATA[Multimodal image-language transformers have achieved impressive results on a
variety of tasks that rely on fine-tuning (e.g., visual question answering and
image retrieval). We are interested in shedding light on the quality of their
pretrained representations -- in particular, if these models can distinguish
different types of verbs or if they rely solely on nouns in a given sentence.
To do so, we collect a dataset of image-sentence pairs (in English) consisting
of 421 verbs that are either visual or commonly found in the pretraining data
(i.e., the Conceptual Captions dataset). We use this dataset to evaluate
pretrained image-language transformers and find that they fail more in
situations that require verb understanding compared to other parts of speech.
We also investigate what category of verbs are particularly challenging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1"&gt;Lisa Anne Hendricks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1"&gt;Aida Nematzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topic Modeling and Progression of American Digital News Media During the Onset of the COVID-19 Pandemic. (arXiv:2106.09572v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09572</id>
        <link href="http://arxiv.org/abs/2106.09572"/>
        <updated>2021-06-18T02:06:34.833Z</updated>
        <summary type="html"><![CDATA[Currently, the world is in the midst of a severe global pandemic, which has
affected all aspects of people's lives. As a result, there is a deluge of
COVID-related digital media articles published in the United States, due to the
disparate effects of the pandemic. This large volume of information is
difficult to consume by the audience in a reasonable amount of time. In this
paper, we develop a Natural Language Processing (NLP) pipeline that is capable
of automatically distilling various digital articles into manageable pieces of
information, while also modelling the progression topics discussed over time in
order to aid readers in rapidly gaining holistic perspectives on pressing
issues (i.e., the COVID-19 pandemic) from a diverse array of sources. We
achieve these goals by first collecting a large corpus of COVID-related
articles during the onset of the pandemic. After, we apply unsupervised and
semi-supervised learning procedures to summarize articles, then cluster them
based on their similarities using the community detection methods. Next, we
identify the topic of each cluster of articles using the BART algorithm.
Finally, we provide a detailed digital media analysis based on the NLP-pipeline
outputs and show how the conversation surrounding COVID-19 evolved over time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xiangpeng Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1"&gt;Michael C. Lucic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghazzai_H/0/1/0/all/0/1"&gt;Hakim Ghazzai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Massoud_Y/0/1/0/all/0/1"&gt;Yehia Massoud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Episode Adaptive Embedding Networks for Few-shot Learning. (arXiv:2106.09398v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09398</id>
        <link href="http://arxiv.org/abs/2106.09398"/>
        <updated>2021-06-18T02:06:34.826Z</updated>
        <summary type="html"><![CDATA[Few-shot learning aims to learn a classifier using a few labelled instances
for each class. Metric-learning approaches for few-shot learning embed
instances into a high-dimensional space and conduct classification based on
distances among instance embeddings. However, such instance embeddings are
usually shared across all episodes and thus lack the discriminative power to
generalize classifiers according to episode-specific features. In this paper,
we propose a novel approach, namely \emph{Episode Adaptive Embedding Network}
(EAEN), to learn episode-specific embeddings of instances. By leveraging the
probability distributions of all instances in an episode at each channel-pixel
embedding dimension, EAEN can not only alleviate the overfitting issue
encountered in few-shot learning tasks, but also capture discriminative
features specific to an episode. To empirically verify the effectiveness and
robustness of EAEN, we have conducted extensive experiments on three widely
used benchmark datasets, under various combinations of different generic
embedding backbones and different classifiers. The results show that EAEN
significantly improves classification accuracy about $10\%$ to $20\%$ in
different settings over the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fangbing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qing Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. (arXiv:2106.09700v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09700</id>
        <link href="http://arxiv.org/abs/2106.09700"/>
        <updated>2021-06-18T02:06:34.807Z</updated>
        <summary type="html"><![CDATA[Biomedical knowledge graphs (KGs) hold rich information on entities such as
diseases, drugs, and genes. Predicting missing links in these graphs can boost
many important applications, such as drug design and repurposing. Recent work
has shown that general-domain language models (LMs) can serve as "soft" KGs,
and that they can be fine-tuned for the task of KG completion. In this work, we
study scientific LMs for KG completion, exploring whether we can tap into their
latent knowledge to enhance biomedical link prediction. We evaluate several
domain-specific LMs, fine-tuning them on datasets centered on drugs and
diseases that we represent as KGs and enrich with textual entity descriptions.
We integrate the LM-based models with KG embedding models, using a router
method that learns to assign each input example to either type of model and
provides a substantial boost in performance. Finally, we demonstrate the
advantage of LM models in the inductive setting with novel scientific entities.
Our datasets and code are made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nadkarni_R/0/1/0/all/0/1"&gt;Rahul Nadkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1"&gt;David Wadden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1"&gt;Iz Beltagy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1"&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1"&gt;Tom Hope&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Contrastive Graph Representation via Adaptive Homotopy Learning. (arXiv:2106.09244v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09244</id>
        <link href="http://arxiv.org/abs/2106.09244"/>
        <updated>2021-06-18T02:06:34.801Z</updated>
        <summary type="html"><![CDATA[Homotopy model is an excellent tool exploited by diverse research works in
the field of machine learning. However, its flexibility is limited due to lack
of adaptiveness, i.e., manual fixing or tuning the appropriate homotopy
coefficients. To address the problem above, we propose a novel adaptive
homotopy framework (AH) in which the Maclaurin duality is employed, such that
the homotopy parameters can be adaptively obtained. Accordingly, the proposed
AH can be widely utilized to enhance the homotopy-based algorithm. In
particular, in this paper, we apply AH to contrastive learning (AHCL) such that
it can be effectively transferred from weak-supervised learning (given label
priori) to unsupervised learning, where soft labels of contrastive learning are
directly and adaptively learned. Accordingly, AHCL has the adaptive ability to
extract deep features without any sort of prior information. Consequently, the
affinity matrix formulated by the related adaptive labels can be constructed as
the deep Laplacian graph that incorporates the topology of deep representations
for the inputs. Eventually, extensive experiments on benchmark datasets
validate the superiority of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chengjun Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Z/0/1/0/all/0/1"&gt;Ziheng Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuelong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating the Robustness of Bayesian Neural Networks Against Different Types of Attacks. (arXiv:2106.09223v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09223</id>
        <link href="http://arxiv.org/abs/2106.09223"/>
        <updated>2021-06-18T02:06:34.794Z</updated>
        <summary type="html"><![CDATA[To evaluate the robustness gain of Bayesian neural networks on image
classification tasks, we perform input perturbations, and adversarial attacks
to the state-of-the-art Bayesian neural networks, with a benchmark CNN model as
reference. The attacks are selected to simulate signal interference and
cyberattacks towards CNN-based machine learning systems. The result shows that
a Bayesian neural network achieves significantly higher robustness against
adversarial attacks generated against a deterministic neural network model,
without adversarial training. The Bayesian posterior can act as the safety
precursor of ongoing malicious activities. Furthermore, we show that the
stochastic classifier after the deterministic CNN extractor has sufficient
robustness enhancement rather than a stochastic feature extractor before the
stochastic classifier. This advises on utilizing stochastic layers in building
decision-making pipelines within a safety-critical domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1"&gt;Yutian Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Sheng Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jueming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[THUNDR: Transformer-based 3D HUmaN Reconstruction with Markers. (arXiv:2106.09336v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09336</id>
        <link href="http://arxiv.org/abs/2106.09336"/>
        <updated>2021-06-18T02:06:34.787Z</updated>
        <summary type="html"><![CDATA[We present THUNDR, a transformer-based deep neural network methodology to
reconstruct the 3d pose and shape of people, given monocular RGB images. Key to
our methodology is an intermediate 3d marker representation, where we aim to
combine the predictive power of model-free-output architectures and the
regularizing, anthropometrically-preserving properties of a statistical human
surface model like GHUM -- a recently introduced, expressive full body
statistical 3d human model, trained end-to-end. Our novel transformer-based
prediction pipeline can focus on image regions relevant to the task, supports
self-supervised regimes, and ensures that solutions are consistent with human
anthropometry. We show state-of-the-art results on Human3.6M and 3DPW, for both
the fully-supervised and the self-supervised models, for the task of inferring
3d human shape, joint positions, and global translation. Moreover, we observe
very solid 3d reconstruction performance for difficult human poses collected in
the wild.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zanfir_M/0/1/0/all/0/1"&gt;Mihai Zanfir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanfir_A/0/1/0/all/0/1"&gt;Andrei Zanfir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bazavan_E/0/1/0/all/0/1"&gt;Eduard Gabriel Bazavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukthankar_R/0/1/0/all/0/1"&gt;Rahul Sukthankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1"&gt;Cristian Sminchisescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ShuffleBlock: Shuffle to Regularize Deep Convolutional Neural Networks. (arXiv:2106.09358v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09358</id>
        <link href="http://arxiv.org/abs/2106.09358"/>
        <updated>2021-06-18T02:06:34.780Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have enormous representational power which leads them to
overfit on most datasets. Thus, regularizing them is important in order to
reduce overfitting and enhance their generalization capabilities. Recently,
channel shuffle operation has been introduced for mixing channels in group
convolutions in resource efficient networks in order to reduce memory and
computations. This paper studies the operation of channel shuffle as a
regularization technique in deep convolutional networks. We show that while
random shuffling of channels during training drastically reduce their
performance, however, randomly shuffling small patches between channels
significantly improves their performance. The patches to be shuffled are picked
from the same spatial locations in the feature maps such that a patch, when
transferred from one channel to another, acts as structured noise for the later
channel. We call this method "ShuffleBlock". The proposed ShuffleBlock module
is easy to implement and improves the performance of several baseline networks
on the task of image classification on CIFAR and ImageNet datasets. It also
achieves comparable and in many cases better performance than many other
regularization methods. We provide several ablation studies on selecting
various hyperparameters of the ShuffleBlock module and propose a new scheduling
method that further enhances its performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumawat_S/0/1/0/all/0/1"&gt;Sudhakar Kumawat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanojia_G/0/1/0/all/0/1"&gt;Gagan Kanojia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1"&gt;Shanmuganathan Raman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Evaluation of Self-Supervised Pre-Training for Skin-Lesion Analysis. (arXiv:2106.09229v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09229</id>
        <link href="http://arxiv.org/abs/2106.09229"/>
        <updated>2021-06-18T02:06:34.774Z</updated>
        <summary type="html"><![CDATA[Self-supervised pre-training appears as an advantageous alternative to
supervised pre-trained for transfer learning. By synthesizing annotations on
pretext tasks, self-supervision allows to pre-train models on large amounts of
pseudo-labels before fine-tuning them on the target task. In this work, we
assess self-supervision for the diagnosis of skin lesions, comparing three
self-supervised pipelines to a challenging supervised baseline, on five test
datasets comprising in- and out-of-distribution samples. Our results show that
self-supervision is competitive both in improving accuracies and in reducing
the variability of outcomes. Self-supervision proves particularly useful for
low training data scenarios ($<1\,500$ and $<150$ samples), where its ability
to stabilize the outcomes is essential to provide sound results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaves_L/0/1/0/all/0/1"&gt;Levy Chaves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bissoto_A/0/1/0/all/0/1"&gt;Alceu Bissoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1"&gt;Eduardo Valle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1"&gt;Sandra Avila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying vaccine sentiment tweets by modelling domain-specific representation and commonsense knowledge into context-aware attentive GRU. (arXiv:2106.09589v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09589</id>
        <link href="http://arxiv.org/abs/2106.09589"/>
        <updated>2021-06-18T02:06:34.767Z</updated>
        <summary type="html"><![CDATA[Vaccines are an important public health measure, but vaccine hesitancy and
refusal can create clusters of low vaccine coverage and reduce the
effectiveness of vaccination programs. Social media provides an opportunity to
estimate emerging risks to vaccine acceptance by including geographical
location and detailing vaccine-related concerns. Methods for classifying social
media posts, such as vaccine-related tweets, use language models (LMs) trained
on general domain text. However, challenges to measuring vaccine sentiment at
scale arise from the absence of tonal stress and gestural cues and may not
always have additional information about the user, e.g., past tweets or social
connections. Another challenge in LMs is the lack of commonsense knowledge that
are apparent in users metadata, i.e., emoticons, positive and negative words
etc. In this study, to classify vaccine sentiment tweets with limited
information, we present a novel end-to-end framework consisting of
interconnected components that use domain-specific LM trained on
vaccine-related tweets and models commonsense knowledge into a bidirectional
gated recurrent network (CK-BiGRU) with context-aware attention. We further
leverage syntactical, user metadata and sentiment information to capture the
sentiment of a tweet. We experimented using two popular vaccine-related Twitter
datasets and demonstrate that our proposed approach outperforms
state-of-the-art models in identifying pro-vaccine, anti-vaccine and neutral
tweets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseem_U/0/1/0/all/0/1"&gt;Usman Naseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khushi_M/0/1/0/all/0/1"&gt;Matloob Khushi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jinman Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dunn_A/0/1/0/all/0/1"&gt;Adam G. Dunn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Insights into Data through Model Behaviour: An Explainability-driven Strategy for Data Auditing for Responsible Computer Vision Applications. (arXiv:2106.09177v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09177</id>
        <link href="http://arxiv.org/abs/2106.09177"/>
        <updated>2021-06-18T02:06:34.745Z</updated>
        <summary type="html"><![CDATA[In this study, we take a departure and explore an explainability-driven
strategy to data auditing, where actionable insights into the data at hand are
discovered through the eyes of quantitative explainability on the behaviour of
a dummy model prototype when exposed to data. We demonstrate this strategy by
auditing two popular medical benchmark datasets, and discover hidden data
quality issues that lead deep learning models to make predictions for the wrong
reasons. The actionable insights gained from this explainability driven data
auditing strategy is then leveraged to address the discovered issues to enable
the creation of high-performing deep learning models with appropriate
prediction behaviour. The hope is that such an explainability-driven strategy
can be complimentary to data-driven strategies to facilitate for more
responsible development of machine learning algorithms for computer vision
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dorfman_A/0/1/0/all/0/1"&gt;Adam Dorfman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McInnis_P/0/1/0/all/0/1"&gt;Paul McInnis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunraj_H/0/1/0/all/0/1"&gt;Hayden Gunraj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-task convolutional neural network for blind stereoscopic image quality assessment using naturalness analysis. (arXiv:2106.09303v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09303</id>
        <link href="http://arxiv.org/abs/2106.09303"/>
        <updated>2021-06-18T02:06:34.734Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of blind stereoscopic image quality
assessment (NR-SIQA) using a new multi-task deep learning based-method. In the
field of stereoscopic vision, the information is fairly distributed between the
left and right views as well as the binocular phenomenon. In this work, we
propose to integrate these characteristics to estimate the quality of
stereoscopic images without reference through a convolutional neural network.
Our method is based on two main tasks: the first task predicts naturalness
analysis based features adapted to stereo images, while the second task
predicts the quality of such images. The former, so-called auxiliary task, aims
to find more robust and relevant features to improve the quality prediction. To
do this, we compute naturalness-based features using a Natural Scene Statistics
(NSS) model in the complex wavelet domain. It allows to capture the statistical
dependency between pairs of the stereoscopic images. Experiments are conducted
on the well known LIVE PHASE I and LIVE PHASE II databases. The results
obtained show the relevance of our method when comparing with those of the
state-of-the-art. Our code is available online on
\url{https://github.com/Bourbia-Salima/multitask-cnn-nrsiqa_2021}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bourbia_S/0/1/0/all/0/1"&gt;Salima Bourbia&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Karine_A/0/1/0/all/0/1"&gt;Ayoub Karine&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/eess/1/au:+Chetouani_A/0/1/0/all/0/1"&gt;Aladine Chetouani&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/eess/1/au:+Hassouni_M/0/1/0/all/0/1"&gt;Mohammed El Hassouni&lt;/a&gt; (1 and 4) ((1) LRIT, Mohammed V University in Rabat, Rabat, Morocco, (2) L@bISEN, ISEN Yncrea Ouest, 33 Quater Chemin du Champ de Manoeuvre, 44470 Carquefou, France, (3) Laboratoire PRISME, Universite d&amp;#x27;Orl&amp;#xe9;ans, France, (4) FLSH, Mohammed V University in Rabat, Rabat, Morocco)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controllable Confidence-Based Image Denoising. (arXiv:2106.09311v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09311</id>
        <link href="http://arxiv.org/abs/2106.09311"/>
        <updated>2021-06-18T02:06:34.727Z</updated>
        <summary type="html"><![CDATA[Image denoising is a classic restoration problem. Yet, current deep learning
methods are subject to the problems of generalization and interpretability. To
mitigate these problems, in this project, we present a framework that is
capable of controllable, confidence-based noise removal. The framework is based
on the fusion between two different denoised images, both derived from the same
noisy input. One of the two is denoised using generic algorithms (e.g.
Gaussian), which make few assumptions on the input images, therefore,
generalize in all scenarios. The other is denoised using deep learning,
performing well on seen datasets. We introduce a set of techniques to fuse the
two components smoothly in the frequency domain. Beyond that, we estimate the
confidence of a deep learning denoiser to allow users to interpret the output,
and provide a fusion strategy that safeguards them against out-of-distribution
inputs. Through experiments, we demonstrate the effectiveness of the proposed
framework in different use cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Owsianko_H/0/1/0/all/0/1"&gt;Haley Owsianko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cassayre_F/0/1/0/all/0/1"&gt;Florian Cassayre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_Q/0/1/0/all/0/1"&gt;Qiyuan Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layer Folding: Neural Network Depth Reduction using Activation Linearization. (arXiv:2106.09309v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09309</id>
        <link href="http://arxiv.org/abs/2106.09309"/>
        <updated>2021-06-18T02:06:34.712Z</updated>
        <summary type="html"><![CDATA[Despite the increasing prevalence of deep neural networks, their
applicability in resource-constrained devices is limited due to their
computational load. While modern devices exhibit a high level of parallelism,
real-time latency is still highly dependent on networks' depth. Although recent
works show that below a certain depth, the width of shallower networks must
grow exponentially, we presume that neural networks typically exceed this
minimal depth to accelerate convergence and incrementally increase accuracy.
This motivates us to transform pre-trained deep networks that already exploit
such advantages into shallower forms. We propose a method that learns whether
non-linear activations can be removed, allowing to fold consecutive linear
layers into one. We apply our method to networks pre-trained on CIFAR-10 and
CIFAR-100 and find that they can all be transformed into shallower forms that
share a similar depth. Finally, we use our method to provide more efficient
alternatives to MobileNetV2 and EfficientNet-Lite architectures on the ImageNet
classification task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dror_A/0/1/0/all/0/1"&gt;Amir Ben Dror&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zehngut_N/0/1/0/all/0/1"&gt;Niv Zehngut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raviv_A/0/1/0/all/0/1"&gt;Avraham Raviv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artyomov_E/0/1/0/all/0/1"&gt;Evgeny Artyomov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vitek_R/0/1/0/all/0/1"&gt;Ran Vitek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jevnisek_R/0/1/0/all/0/1"&gt;Roy Jevnisek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invisible for both Camera and LiDAR: Security of Multi-Sensor Fusion based Perception in Autonomous Driving Under Physical-World Attacks. (arXiv:2106.09249v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.09249</id>
        <link href="http://arxiv.org/abs/2106.09249"/>
        <updated>2021-06-18T02:06:34.705Z</updated>
        <summary type="html"><![CDATA[In Autonomous Driving (AD) systems, perception is both security and safety
critical. Despite various prior studies on its security issues, all of them
only consider attacks on camera- or LiDAR-based AD perception alone. However,
production AD systems today predominantly adopt a Multi-Sensor Fusion (MSF)
based design, which in principle can be more robust against these attacks under
the assumption that not all fusion sources are (or can be) attacked at the same
time. In this paper, we present the first study of security issues of MSF-based
perception in AD systems. We directly challenge the basic MSF design assumption
above by exploring the possibility of attacking all fusion sources
simultaneously. This allows us for the first time to understand how much
security guarantee MSF can fundamentally provide as a general defense strategy
for AD perception.

We formulate the attack as an optimization problem to generate a
physically-realizable, adversarial 3D-printed object that misleads an AD system
to fail in detecting it and thus crash into it. We propose a novel attack
pipeline that addresses two main design challenges: (1) non-differentiable
target camera and LiDAR sensing systems, and (2) non-differentiable cell-level
aggregated features popularly used in LiDAR-based AD perception. We evaluate
our attack on MSF included in representative open-source industry-grade AD
systems in real-world driving scenarios. Our results show that the attack
achieves over 90% success rate across different object types and MSF. Our
attack is also found stealthy, robust to victim positions, transferable across
MSF algorithms, and physical-world realizable after being 3D-printed and
captured by LiDAR and camera devices. To concretely assess the end-to-end
safety impact, we further perform simulation evaluation and show that it can
cause a 100% vehicle collision rate for an industry-grade AD system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao%2A_Y/0/1/0/all/0/1"&gt;Yulong Cao*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang%2A_N/0/1/0/all/0/1"&gt;Ningfei Wang*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao%2A_C/0/1/0/all/0/1"&gt;Chaowei Xiao*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang%2A_D/0/1/0/all/0/1"&gt;Dawei Yang*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Ruigang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qi Alfred Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt; (*co-first authors)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can we learn (more) from challenges? A statistical approach to driving future algorithm development. (arXiv:2106.09302v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09302</id>
        <link href="http://arxiv.org/abs/2106.09302"/>
        <updated>2021-06-18T02:06:34.698Z</updated>
        <summary type="html"><![CDATA[Challenges have become the state-of-the-art approach to benchmark image
analysis algorithms in a comparative manner. While the validation on identical
data sets was a great step forward, results analysis is often restricted to
pure ranking tables, leaving relevant questions unanswered. Specifically,
little effort has been put into the systematic investigation on what
characterizes images in which state-of-the-art algorithms fail. To address this
gap in the literature, we (1) present a statistical framework for learning from
challenges and (2) instantiate it for the specific task of instrument instance
segmentation in laparoscopic videos. Our framework relies on the semantic meta
data annotation of images, which serves as foundation for a General Linear
Mixed Models (GLMM) analysis. Based on 51,542 meta data annotations performed
on 2,728 images, we applied our approach to the results of the Robust Medical
Instrument Segmentation Challenge (ROBUST-MIS) challenge 2019 and revealed
underexposure, motion and occlusion of instruments as well as the presence of
smoke or other objects in the background as major sources of algorithm failure.
Our subsequent method development, tailored to the specific remaining issues,
yielded a deep learning model with state-of-the-art overall performance and
specific strengths in the processing of images in which previous methods tended
to fail. Due to the objectivity and generic applicability of our approach, it
could become a valuable tool for validation in the field of medical image
analysis and beyond. and segmentation of small, crossing, moving and
transparent instrument(s) (parts).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ross_T/0/1/0/all/0/1"&gt;Tobias Ro&amp;#xdf;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruno_P/0/1/0/all/0/1"&gt;Pierangela Bruno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reinke_A/0/1/0/all/0/1"&gt;Annika Reinke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiesenfarth_M/0/1/0/all/0/1"&gt;Manuel Wiesenfarth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koeppel_L/0/1/0/all/0/1"&gt;Lisa Koeppel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Full_P/0/1/0/all/0/1"&gt;Peter M. Full&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pekdemir_B/0/1/0/all/0/1"&gt;B&amp;#xfc;nyamin Pekdemir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Godau_P/0/1/0/all/0/1"&gt;Patrick Godau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trofimova_D/0/1/0/all/0/1"&gt;Darya Trofimova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1"&gt;Fabian Isensee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moccia_S/0/1/0/all/0/1"&gt;Sara Moccia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calimeri_F/0/1/0/all/0/1"&gt;Francesco Calimeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1"&gt;Beat P. M&amp;#xfc;ller-Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopp_Schneider_A/0/1/0/all/0/1"&gt;Annette Kopp-Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deformation Driven Seq2Seq Longitudinal Tumor and Organs-at-Risk Prediction for Radiotherapy. (arXiv:2106.09076v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09076</id>
        <link href="http://arxiv.org/abs/2106.09076"/>
        <updated>2021-06-18T02:06:34.690Z</updated>
        <summary type="html"><![CDATA[Purpose: Radiotherapy presents unique challenges and clinical requirements
for longitudinal tumor and organ-at-risk (OAR) prediction during treatment. The
challenges include tumor inflammation/edema and radiation-induced changes in
organ geometry, whereas the clinical requirements demand flexibility in
input/output sequence timepoints to update the predictions on rolling basis and
the grounding of all predictions in relationship to the pre-treatment imaging
information for response and toxicity assessment in adaptive radiotherapy.
Methods: To deal with the aforementioned challenges and to comply with the
clinical requirements, we present a novel 3D sequence-to-sequence model based
on Convolution Long Short Term Memory (ConvLSTM) that makes use of series of
deformation vector fields (DVF) between individual timepoints and reference
pre-treatment/planning CTs to predict future anatomical deformations and
changes in gross tumor volume as well as critical OARs. High-quality DVF
training data is created by employing hyper-parameter optimization on the
subset of the training data with DICE coefficient and mutual information
metric. We validated our model on two radiotherapy datasets: a publicly
available head-and-neck dataset (28 patients with manually contoured pre-,
mid-, and post-treatment CTs), and an internal non-small cell lung cancer
dataset (63 patients with manually contoured planning CT and 6 weekly CBCTs).
Results: The use of DVF representation and skip connections overcomes the
blurring issue of ConvLSTM prediction with the traditional image
representation. The mean and standard deviation of DICE for predictions of lung
GTV at week 4, 5, and 6 were 0.83$\pm$0.09, 0.82$\pm$0.08, and 0.81$\pm$0.10,
respectively, and for post-treatment ipsilateral and contralateral parotids,
were 0.81$\pm$0.06 and 0.85$\pm$0.02.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Donghoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1"&gt;Sadegh R Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jue Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengpeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1"&gt;Saad Nadeem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yu-Chi Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Worlds in Text. (arXiv:2106.09578v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09578</id>
        <link href="http://arxiv.org/abs/2106.09578"/>
        <updated>2021-06-18T02:06:34.682Z</updated>
        <summary type="html"><![CDATA[We provide a dataset that enables the creation of learning agents that can
build knowledge graph-based world models of interactive narratives. Interactive
narratives -- or text-adventure games -- are partially observable environments
structured as long puzzles or quests in which an agent perceives and interacts
with the world purely through textual natural language. Each individual game
typically contains hundreds of locations, characters, and objects -- each with
their own unique descriptions -- providing an opportunity to study the problem
of giving language-based agents the structured memory necessary to operate in
such worlds. Our dataset provides 24198 mappings between rich natural language
observations and: (1) knowledge graphs that reflect the world state in the form
of a map; (2) natural language actions that are guaranteed to cause a change in
that particular world state. The training data is collected across 27 games in
multiple genres and contains a further 7836 heldout instances over 9 additional
games in the test set. We further provide baseline models using rules-based,
question-answering, and sequence learning approaches in addition to an analysis
of the data and corresponding learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1"&gt;Prithviraj Ammanabrolu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1"&gt;Mark O. Riedl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Element Intervention for Open Relation Extraction. (arXiv:2106.09558v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09558</id>
        <link href="http://arxiv.org/abs/2106.09558"/>
        <updated>2021-06-18T02:06:34.675Z</updated>
        <summary type="html"><![CDATA[Open relation extraction aims to cluster relation instances referring to the
same underlying relation, which is a critical step for general relation
extraction. Current OpenRE models are commonly trained on the datasets
generated from distant supervision, which often results in instability and
makes the model easily collapsed. In this paper, we revisit the procedure of
OpenRE from a causal view. By formulating OpenRE using a structural causal
model, we identify that the above-mentioned problems stem from the spurious
correlations from entities and context to the relation type. To address this
issue, we conduct \emph{Element Intervention}, which intervenes on the context
and entities respectively to obtain the underlying causal effects of them. We
also provide two specific implementations of the interventions based on entity
ranking and context contrasting. Experimental results on unsupervised relation
extraction datasets show that our methods outperform previous state-of-the-art
methods and are robust across different datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fangchao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1"&gt;Lingyong Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hongyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xianpei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Le Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evidence-based Factual Error Correction. (arXiv:2106.01072v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01072</id>
        <link href="http://arxiv.org/abs/2106.01072"/>
        <updated>2021-06-18T02:06:34.668Z</updated>
        <summary type="html"><![CDATA[This paper introduces the task of factual error correction: performing edits
to a claim so that the generated rewrite is better supported by evidence. This
extends the well-studied task of fact verification by providing a mechanism to
correct written texts that are refuted or only partially supported by evidence.
We demonstrate that it is feasible to train factual error correction systems
from existing fact checking datasets which only contain labeled claims
accompanied by evidence, but not the correction. We achieve this by employing a
two-stage distant supervision approach that incorporates evidence into masked
claims when generating corrections. Our approach, based on the T5 transformer
and using retrieved evidence, achieved better results than existing work which
used a pointer copy network and gold evidence, producing accurate factual error
corrections for 5x more instances in human evaluation and a .125 increase in
SARI score. The evaluation is conducted on a dataset of 65,000 instances based
on a recent fact verification shared task and we release it to enable further
work on the task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1"&gt;James Thorne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1"&gt;Andreas Vlachos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-level Motion Attention for Human Motion Prediction. (arXiv:2106.09300v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09300</id>
        <link href="http://arxiv.org/abs/2106.09300"/>
        <updated>2021-06-18T02:06:34.659Z</updated>
        <summary type="html"><![CDATA[Human motion prediction aims to forecast future human poses given a
historical motion. Whether based on recurrent or feed-forward neural networks,
existing learning based methods fail to model the observation that human motion
tends to repeat itself, even for complex sports actions and cooking activities.
Here, we introduce an attention based feed-forward network that explicitly
leverages this observation. In particular, instead of modeling frame-wise
attention via pose similarity, we propose to extract motion attention to
capture the similarity between the current motion context and the historical
motion sub-sequences. In this context, we study the use of different types of
attention, computed at joint, body part, and full pose levels. Aggregating the
relevant past motions and processing the result with a graph convolutional
network allows us to effectively exploit motion patterns from the long-term
history to predict the future poses. Our experiments on Human3.6M, AMASS and
3DPW validate the benefits of our approach for both periodical and
non-periodical actions. Thanks to our attention model, it yields
state-of-the-art results on all three datasets. Our code is available at
https://github.com/wei-mao-2019/HisRepItself.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1"&gt;Wei Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Miaomiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1"&gt;Mathieu Salzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongdong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Two-stage Multi-modal Affect Analysis Framework for Children with Autism Spectrum Disorder. (arXiv:2106.09199v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09199</id>
        <link href="http://arxiv.org/abs/2106.09199"/>
        <updated>2021-06-18T02:06:34.633Z</updated>
        <summary type="html"><![CDATA[Autism spectrum disorder (ASD) is a developmental disorder that influences
the communication and social behavior of a person in a way that those in the
spectrum have difficulty in perceiving other people's facial expressions, as
well as presenting and communicating emotions and affect via their own faces
and bodies. Some efforts have been made to predict and improve children with
ASD's affect states in play therapy, a common method to improve children's
social skills via play and games. However, many previous works only used
pre-trained models on benchmark emotion datasets and failed to consider the
distinction in emotion between typically developing children and children with
autism. In this paper, we present an open-source two-stage multi-modal approach
leveraging acoustic and visual cues to predict three main affect states of
children with ASD's affect states (positive, negative, and neutral) in
real-world play therapy scenarios, and achieved an overall accuracy of 72:40%.
This work presents a novel way to combine human expertise and machine
intelligence for ASD affect recognition by proposing a two-stage schema.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jicheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_A/0/1/0/all/0/1"&gt;Anjana Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barmaki_R/0/1/0/all/0/1"&gt;Roghayeh Barmaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated CycleGAN for Privacy-Preserving Image-to-Image Translation. (arXiv:2106.09246v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09246</id>
        <link href="http://arxiv.org/abs/2106.09246"/>
        <updated>2021-06-18T02:06:34.623Z</updated>
        <summary type="html"><![CDATA[Unsupervised image-to-image translation methods such as CycleGAN learn to
convert images from one domain to another using unpaired training data sets
from different domains. Unfortunately, these approaches still require centrally
collected unpaired records, potentially violating privacy and security issues.
Although the recent federated learning (FL) allows a neural network to be
trained without data exchange, the basic assumption of the FL is that all
clients have their own training data from a similar domain, which is different
from our image-to-image translation scenario in which each client has images
from its unique domain and the goal is to learn image translation between
different domains without accessing the target domain data. To address this,
here we propose a novel federated CycleGAN architecture that can learn image
translation in an unsupervised manner while maintaining the data privacy.
Specifically, our approach arises from a novel observation that CycleGAN loss
can be decomposed into the sum of client specific local objectives that can be
evaluated using only their data. This local objective decomposition allows
multiple clients to participate in federated CycleGAN training without
sacrificing performance. Furthermore, our method employs novel switchable
generator and discriminator architecture using Adaptive Instance Normalization
(AdaIN) that significantly reduces the band-width requirement of the federated
learning. Our experimental results on various unsupervised image translation
tasks show that our federated CycleGAN provides comparable performance compared
to the non-federated counterpart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Joonyoung Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pushing the Limits of Non-Autoregressive Speech Recognition. (arXiv:2104.03416v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03416</id>
        <link href="http://arxiv.org/abs/2104.03416"/>
        <updated>2021-06-18T02:06:34.597Z</updated>
        <summary type="html"><![CDATA[We combine recent advancements in end-to-end speech recognition to
non-autoregressive automatic speech recognition. We push the limits of
non-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,
Fisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC
on giant Conformer neural network architectures with SpecAugment and wav2vec2
pre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,
5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without
a language model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ng_E/0/1/0/all/0/1"&gt;Edwin G. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chiu_C/0/1/0/all/0/1"&gt;Chung-Cheng Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1"&gt;William Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Random CNN Sees Objects: One Inductive Bias of CNN and Its Applications. (arXiv:2106.09259v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09259</id>
        <link href="http://arxiv.org/abs/2106.09259"/>
        <updated>2021-06-18T02:06:34.581Z</updated>
        <summary type="html"><![CDATA[This paper starts by revealing a surprising finding: without any learning, a
randomly initialized CNN can localize objects surprisingly well. That is, a CNN
has an inductive bias to naturally focus on objects, named as Tobias (``The
object is at sight'') in this paper. This empirical inductive bias is further
analyzed and successfully applied to self-supervised learning. A CNN is
encouraged to learn representations that focus on the foreground object, by
transforming every image into various versions with different backgrounds,
where the foreground and background separation is guided by Tobias.
Experimental results show that the proposed Tobias significantly improves
downstream tasks, especially for object detection. This paper also shows that
Tobias has consistent improvements on training sets of different sizes, and is
more resilient to changes in image augmentations. Our codes will be available
at https://github.com/CupidJay/Tobias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yun-Hao Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianxin Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-head or Single-head? An Empirical Comparison for Transformer Training. (arXiv:2106.09650v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09650</id>
        <link href="http://arxiv.org/abs/2106.09650"/>
        <updated>2021-06-18T02:06:34.554Z</updated>
        <summary type="html"><![CDATA[Multi-head attention plays a crucial role in the recent success of
Transformer models, which leads to consistent performance improvements over
conventional attention in various applications. The popular belief is that this
effectiveness stems from the ability of jointly attending multiple positions.
In this paper, we first demonstrate that jointly attending multiple positions
is not a unique feature of multi-head attention, as multi-layer single-head
attention also attends multiple positions and is more effective. Then, we
suggest the main advantage of the multi-head attention is the training
stability, since it has less number of layers than the single-head attention,
when attending the same number of positions. For example, 24-layer 16-head
Transformer (BERT-large) and 384-layer single-head Transformer has the same
total attention head number and roughly the same model size, while the
multi-head one is significantly shallower. Meanwhile, we show that, with recent
advances in deep learning, we can successfully stabilize the training of the
384-layer Transformer. As the training difficulty is no longer a bottleneck,
substantially deeper single-head Transformer achieves consistent performance
improvements without tuning hyper-parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trilateral Attention Network for Real-time Medical Image Segmentation. (arXiv:2106.09201v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09201</id>
        <link href="http://arxiv.org/abs/2106.09201"/>
        <updated>2021-06-18T02:06:34.533Z</updated>
        <summary type="html"><![CDATA[Accurate segmentation of medical images into anatomically meaningful regions
is critical for the extraction of quantitative indices or biomarkers. The
common pipeline for segmentation comprises regions of interest detection stage
and segmentation stage, which are independent of each other and typically
performed using separate deep learning networks. The performance of the
segmentation stage highly relies on the extracted set of spatial features and
the receptive fields. In this work, we propose an end-to-end network, called
Trilateral Attention Network (TaNet), for real-time detection and segmentation
in medical images. TaNet has a module for region localization, and three
segmentation pathways: 1) handcrafted pathway with hand-designed convolutional
kernels, 2) detail pathway with regular convolutional kernels, and 3) a global
pathway to enlarge the receptive field. The first two pathways encode rich
handcrafted and low-level features extracted by hand-designed and regular
kernels while the global pathway encodes high-level context information. By
jointly training the network for localization and segmentation using different
sets of features, TaNet achieved superior performance, in terms of accuracy and
speed, when evaluated on an echocardiography dataset for cardiac segmentation.
The code and models will be made publicly available in TaNet Github page.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1"&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachdev_V/0/1/0/all/0/1"&gt;Vandana Sachdev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antani_S/0/1/0/all/0/1"&gt;Sameer Antani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling-up Diverse Orthogonal Convolutional Networks with a Paraunitary Framework. (arXiv:2106.09121v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09121</id>
        <link href="http://arxiv.org/abs/2106.09121"/>
        <updated>2021-06-18T02:06:34.526Z</updated>
        <summary type="html"><![CDATA[Enforcing orthogonality in neural networks is an antidote for gradient
vanishing/exploding problems, sensitivity by adversarial perturbation, and
bounding generalization errors. However, many previous approaches are
heuristic, and the orthogonality of convolutional layers is not systematically
studied: some of these designs are not exactly orthogonal, while others only
consider standard convolutional layers and propose specific classes of their
realizations. To address this problem, we propose a theoretical framework for
orthogonal convolutional layers, which establishes the equivalence between
various orthogonal convolutional layers in the spatial domain and the
paraunitary systems in the spectral domain. Since there exists a complete
spectral factorization of paraunitary systems, any orthogonal convolution layer
can be parameterized as convolutions of spatial filters. Our framework endows
high expressive power to various convolutional layers while maintaining their
exact orthogonality. Furthermore, our layers are memory and computationally
efficient for deep networks compared to previous designs. Our versatile
framework, for the first time, enables the study of architecture designs for
deep orthogonal networks, such as choices of skip connection, initialization,
stride, and dilation. Consequently, we scale up orthogonal networks to deep
architectures, including ResNet, WideResNet, and ShuffleNet, substantially
increasing the performance over the traditional shallow orthogonal networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jiahao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1"&gt;Wonmin Byeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPeCiaL: Self-Supervised Pretraining for Continual Learning. (arXiv:2106.09065v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09065</id>
        <link href="http://arxiv.org/abs/2106.09065"/>
        <updated>2021-06-18T02:06:34.505Z</updated>
        <summary type="html"><![CDATA[This paper presents SPeCiaL: a method for unsupervised pretraining of
representations tailored for continual learning. Our approach devises a
meta-learning objective that differentiates through a sequential learning
process. Specifically, we train a linear model over the representations to
match different augmented views of the same image together, each view presented
sequentially. The linear model is then evaluated on both its ability to
classify images it just saw, and also on images from previous iterations. This
gives rise to representations that favor quick knowledge retention with minimal
forgetting. We evaluate SPeCiaL in the Continual Few-Shot Learning setting, and
show that it can match or outperform other supervised pretraining approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1"&gt;Lucas Caccia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure. (arXiv:2106.09051v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09051</id>
        <link href="http://arxiv.org/abs/2106.09051"/>
        <updated>2021-06-18T02:06:34.499Z</updated>
        <summary type="html"><![CDATA[Our goal in this work is to generate realistic videos given just one initial
frame as input. Existing unsupervised approaches to this task do not consider
the fact that a video typically shows a 3D environment, and that this should
remain coherent from frame to frame even as the camera and objects move. We
address this by developing a model that first estimates the latent 3D structure
of the scene, including the segmentation of any moving objects. It then
predicts future frames by simulating the object and camera dynamics, and
rendering the resulting views. Importantly, it is trained end-to-end using only
the unsupervised objective of predicting future frames, without any 3D
information nor segmentation annotations. Experiments on two challenging
datasets of natural videos show that our model can estimate 3D structure and
motion segmentation from a single frame, and hence generate plausible and
varied predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1"&gt;Paul Henderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1"&gt;Christoph H. Lampert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bickel_B/0/1/0/all/0/1"&gt;Bernd Bickel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-Short Temporal Contrastive Learning of Video Transformers. (arXiv:2106.09212v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09212</id>
        <link href="http://arxiv.org/abs/2106.09212"/>
        <updated>2021-06-18T02:06:34.491Z</updated>
        <summary type="html"><![CDATA[Video transformers have recently emerged as a competitive alternative to 3D
CNNs for video understanding. However, due to their large number of parameters
and reduced inductive biases, these models require supervised pretraining on
large-scale image datasets to achieve top performance. In this paper, we
empirically demonstrate that self-supervised pretraining of video transformers
on video-only datasets can lead to action recognition results that are on par
or better than those obtained with supervised pretraining on large-scale image
datasets, even massive ones such as ImageNet-21K. Since transformer-based
models are effective at capturing dependencies over extended temporal spans, we
propose a simple learning procedure that forces the model to match a long-term
view to a short-term view of the same video. Our approach, named Long-Short
Temporal Contrastive Learning (LSTCL), enables video transformers to learn an
effective clip-level representation by predicting temporal context captured
from a longer temporal extent. To demonstrate the generality of our findings,
we implement and validate our approach under three different self-supervised
contrastive learning frameworks (MoCo v3, BYOL, SimSiam) using two distinct
video-transformer architectures, including an improved variant of the Swin
Transformer augmented with space-time attention. We conduct a thorough ablation
study and show that LSTCL achieves competitive performance on multiple video
benchmarks and represents a convincing alternative to supervised image-based
pretraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1"&gt;Gedas Bertasius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Du Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1"&gt;Lorenzo Torresani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularization of Mixture Models for Robust Principal Graph Learning. (arXiv:2106.09035v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09035</id>
        <link href="http://arxiv.org/abs/2106.09035"/>
        <updated>2021-06-18T02:06:34.475Z</updated>
        <summary type="html"><![CDATA[A regularized version of Mixture Models is proposed to learn a principal
graph from a distribution of $D$-dimensional data points. In the particular
case of manifold learning for ridge detection, we assume that the underlying
manifold can be modeled as a graph structure acting like a topological prior
for the Gaussian clusters turning the problem into a maximum a posteriori
estimation. Parameters of the model are iteratively estimated through an
Expectation-Maximization procedure making the learning of the structure
computationally efficient with guaranteed convergence for any graph prior in a
polynomial time. We also embed in the formalism a natural way to make the
algorithm robust to outliers of the pattern and heteroscedasticity of the
manifold sampling coherently with the graph structure. The method uses a graph
prior given by the minimum spanning tree that we extend using random
sub-samplings of the dataset to take into account cycles that can be observed
in the spatial distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bonnaire_T/0/1/0/all/0/1"&gt;Tony Bonnaire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Decelle_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Decelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghanim_N/0/1/0/all/0/1"&gt;Nabila Aghanim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LiRA: Learning Visual Speech Representations from Audio through Self-supervision. (arXiv:2106.09171v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09171</id>
        <link href="http://arxiv.org/abs/2106.09171"/>
        <updated>2021-06-18T02:06:34.469Z</updated>
        <summary type="html"><![CDATA[The large amount of audiovisual content being shared online today has drawn
substantial attention to the prospect of audiovisual self-supervised learning.
Recent works have focused on each of these modalities separately, while others
have attempted to model both simultaneously in a cross-modal fashion. However,
comparatively little attention has been given to leveraging one modality as a
training objective to learn from the other. In this work, we propose Learning
visual speech Representations from Audio via self-supervision (LiRA).
Specifically, we train a ResNet+Conformer model to predict acoustic features
from unlabelled visual speech. We find that this pre-trained model can be
leveraged towards word-level and sentence-level lip-reading through feature
extraction and fine-tuning experiments. We show that our approach significantly
outperforms other self-supervised methods on the Lip Reading in the Wild (LRW)
dataset and achieves state-of-the-art performance on Lip Reading Sentences 2
(LRS2) using only a fraction of the total labelled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1"&gt;Pingchuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mira_R/0/1/0/all/0/1"&gt;Rodrigo Mira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1"&gt;Stavros Petridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn W. Schuller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1"&gt;Maja Pantic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Fishnet Open Images Database: A Dataset for Fish Detection and Fine-Grained Categorization in Fisheries. (arXiv:2106.09178v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09178</id>
        <link href="http://arxiv.org/abs/2106.09178"/>
        <updated>2021-06-18T02:06:34.427Z</updated>
        <summary type="html"><![CDATA[Camera-based electronic monitoring (EM) systems are increasingly being
deployed onboard commercial fishing vessels to collect essential data for
fisheries management and regulation. These systems generate large quantities of
video data which must be reviewed on land by human experts. Computer vision can
assist this process by automatically detecting and classifying fish species,
however the lack of existing public data in this domain has hindered progress.
To address this, we present the Fishnet Open Images Database, a large dataset
of EM imagery for fish detection and fine-grained categorization onboard
commercial fishing vessels. The dataset consists of 86,029 images containing 34
object classes, making it the largest and most diverse public dataset of
fisheries EM imagery to-date. It includes many of the characteristic challenges
of EM data: visual similarity between species, skewed class distributions,
harsh weather conditions, and chaotic crew activity. We evaluate the
performance of existing detection and classification algorithms and demonstrate
that the dataset can serve as a challenging benchmark for development of
computer vision algorithms in fisheries. The dataset is available at
https://www.fishnet.ai/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kay_J/0/1/0/all/0/1"&gt;Justin Kay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merrifield_M/0/1/0/all/0/1"&gt;Matt Merrifield&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGGGEN: Ordering and Aggregating while Generating. (arXiv:2106.05580v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05580</id>
        <link href="http://arxiv.org/abs/2106.05580"/>
        <updated>2021-06-18T02:06:34.420Z</updated>
        <summary type="html"><![CDATA[We present AGGGEN (pronounced 'again'), a data-to-text model which
re-introduces two explicit sentence planning stages into neural data-to-text
systems: input ordering and input aggregation. In contrast to previous work
using sentence planning, our model is still end-to-end: AGGGEN performs
sentence planning at the same time as generating text by learning latent
alignments (via semantic facts) between input representation and target text.
Experiments on the WebNLG and E2E challenge data show that by using fact-based
alignments our approach is more interpretable, expressive, robust to noise, and
easier to control, while retaining the advantages of end-to-end systems in
terms of fluency. Our code is available at https://github.com/XinnuoXu/AggGen.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xinnuo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1"&gt;Ond&amp;#x159;ej Du&amp;#x161;ek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1"&gt;Verena Rieser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konstas_I/0/1/0/all/0/1"&gt;Ioannis Konstas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DESCGEN: A Distantly Supervised Dataset for Generating Abstractive Entity Descriptions. (arXiv:2106.05365v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05365</id>
        <link href="http://arxiv.org/abs/2106.05365"/>
        <updated>2021-06-18T02:06:34.412Z</updated>
        <summary type="html"><![CDATA[Short textual descriptions of entities provide summaries of their key
attributes and have been shown to be useful sources of background knowledge for
tasks such as entity linking and question answering. However, generating entity
descriptions, especially for new and long-tail entities, can be challenging
since relevant information is often scattered across multiple sources with
varied content and style. We introduce DESCGEN: given mentions spread over
multiple documents, the goal is to generate an entity summary description.
DESCGEN consists of 37K entity descriptions from Wikipedia and Fandom, each
paired with nine evidence documents on average. The documents were collected
using a combination of entity linking and hyperlinks to the Wikipedia and
Fandom entity pages, which together provide high-quality distant supervision.
The resulting summaries are more abstractive than those found in existing
datasets and provide a better proxy for the challenge of describing new and
emerging entities. We also propose a two-stage extract-then-generate baseline
and show that there exists a large gap (19.9% in ROUGE-L) between
state-of-the-art models and human performance, suggesting that the data will
support significant future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Weijia Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1"&gt;Mandar Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DravidianCodeMix: Sentiment Analysis and Offensive Language Identification Dataset for Dravidian Languages in Code-Mixed Text. (arXiv:2106.09460v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09460</id>
        <link href="http://arxiv.org/abs/2106.09460"/>
        <updated>2021-06-18T02:06:34.385Z</updated>
        <summary type="html"><![CDATA[This paper describes the development of a multilingual, manually annotated
dataset for three under-resourced Dravidian languages generated from social
media comments. The dataset was annotated for sentiment analysis and offensive
language identification for a total of more than 60,000 YouTube comments. The
dataset consists of around 44,000 comments in Tamil-English, around 7,000
comments in Kannada-English, and around 20,000 comments in Malayalam-English.
The data was manually annotated by volunteer annotators and has a high
inter-annotator agreement in Krippendorff's alpha. The dataset contains all
types of code-mixing phenomena since it comprises user-generated content from a
multilingual country. We also present baseline experiments to establish
benchmarks on the dataset using machine learning methods. The dataset is
available on Github
(https://github.com/bharathichezhiyan/DravidianCodeMix-Dataset) and Zenodo
(https://zenodo.org/record/4750858\#.YJtw0SYo\_0M).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1"&gt;Bharathi Raja Chakravarthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1"&gt;Ruba Priyadharshini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muralidaran_V/0/1/0/all/0/1"&gt;Vigneshwaran Muralidaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jose_N/0/1/0/all/0/1"&gt;Navya Jose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suryawanshi_S/0/1/0/all/0/1"&gt;Shardul Suryawanshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sherly_E/0/1/0/all/0/1"&gt;Elizabeth Sherly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCrae_J/0/1/0/all/0/1"&gt;John P. McCrae&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LoRA: Low-Rank Adaptation of Large Language Models. (arXiv:2106.09685v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09685</id>
        <link href="http://arxiv.org/abs/2106.09685"/>
        <updated>2021-06-18T02:06:34.376Z</updated>
        <summary type="html"><![CDATA[The dominant paradigm of natural language processing consists of large-scale
pre-training on general domain data and adaptation to particular tasks or
domains. As we pre-train larger models, conventional fine-tuning, which
retrains all model parameters, becomes less feasible. Using GPT-3 175B as an
example, deploying many independent instances of fine-tuned models, each with
175B parameters, is extremely expensive. We propose Low-Rank Adaptation, or
LoRA, which freezes the pre-trained model weights and injects trainable rank
decomposition matrices into each layer of the Transformer architecture, greatly
reducing the number of trainable parameters for downstream tasks. For GPT-3,
LoRA can reduce the number of trainable parameters by 10,000 times and the
computation hardware requirement by 3 times compared to full fine-tuning. LoRA
performs on-par or better than fine-tuning in model quality on both GPT-3 and
GPT-2, despite having fewer trainable parameters, a higher training throughput,
and no additional inference latency. We also provide an empirical investigation
into rank-deficiency in language model adaptations, which sheds light on the
efficacy of LoRA. We release our implementation in GPT-2 at
https://github.com/microsoft/LoRA .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1"&gt;Edward J. Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yelong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1"&gt;Phillip Wallis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1"&gt;Zeyuan Allen-Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shean Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STAN: A stuttering therapy analysis helper. (arXiv:2106.09545v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.09545</id>
        <link href="http://arxiv.org/abs/2106.09545"/>
        <updated>2021-06-18T02:06:34.356Z</updated>
        <summary type="html"><![CDATA[Stuttering is a complex speech disorder identified by repeti-tions,
prolongations of sounds, syllables or words and blockswhile speaking. Specific
stuttering behaviour differs strongly,thus needing personalized therapy.
Therapy sessions requirea high level of concentration by the therapist. We
introduceSTAN, a system to aid speech therapists in stuttering therapysessions.
Such an automated feedback system can lower thecognitive load on the therapist
and thereby enable a more con-sistent therapy as well as allowing analysis of
stuttering overthe span of multiple therapy sessions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bayerl_S/0/1/0/all/0/1"&gt;Sebastian P. Bayerl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wenninger_M/0/1/0/all/0/1"&gt;Marc Wenninger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schmidt_J/0/1/0/all/0/1"&gt;Jochen Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gudenberg_A/0/1/0/all/0/1"&gt;Alexander Wolff von Gudenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Riedhammer_K/0/1/0/all/0/1"&gt;Korbinian Riedhammer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Self-supervised Method for Entity Alignment. (arXiv:2106.09395v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09395</id>
        <link href="http://arxiv.org/abs/2106.09395"/>
        <updated>2021-06-18T02:06:34.349Z</updated>
        <summary type="html"><![CDATA[Entity alignment, aiming to identify equivalent entities across different
knowledge graphs (KGs), is a fundamental problem for constructing large-scale
KGs. Over the course of its development, supervision has been considered
necessary for accurate alignments. Inspired by the recent progress of
self-supervised learning, we explore the extent to which we can get rid of
supervision for entity alignment. Existing supervised methods for this task
focus on pulling each pair of positive (labeled) entities close to each other.
However, our analysis suggests that the learning of entity alignment can
actually benefit more from pushing sampled (unlabeled) negatives far away than
pulling positive aligned pairs close. We present SelfKG by leveraging this
discovery to design a contrastive learning strategy across two KGs. Extensive
experiments on benchmark datasets demonstrate that SelfKG without supervision
can match or achieve comparable results with state-of-the-art supervised
baselines. The performance of SelfKG demonstrates self-supervised learning
offers great potential for entity alignment in KGs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1"&gt;Haoyun Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinghao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zeyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1"&gt;Evgeny Kharlamov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yuxiao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural semi-Markov CRF for Monolingual Word Alignment. (arXiv:2106.02569v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02569</id>
        <link href="http://arxiv.org/abs/2106.02569"/>
        <updated>2021-06-18T02:06:34.342Z</updated>
        <summary type="html"><![CDATA[Monolingual word alignment is important for studying fine-grained editing
operations (i.e., deletion, addition, and substitution) in text-to-text
generation tasks, such as paraphrase generation, text simplification,
neutralizing biased language, etc. In this paper, we present a novel neural
semi-Markov CRF alignment model, which unifies word and phrase alignments
through variable-length spans. We also create a new benchmark with human
annotations that cover four different text genres to evaluate monolingual word
alignment models in more realistic settings. Experimental results show that our
proposed model outperforms all previous approaches for monolingual word
alignment as well as a competitive QA-based baseline, which was previously only
applied to bilingual data. Our model demonstrates good generalizability to
three out-of-domain datasets and shows great utility in two downstream
applications: automatic text simplification and sentence pair classification
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1"&gt;Wuwei Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Chao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetric Regularization based BERT for Pair-wise Semantic Reasoning. (arXiv:1909.03405v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.03405</id>
        <link href="http://arxiv.org/abs/1909.03405"/>
        <updated>2021-06-18T02:06:34.328Z</updated>
        <summary type="html"><![CDATA[The ability of semantic reasoning over the sentence pair is essential for
many natural language understanding tasks, e.g., natural language inference and
machine reading comprehension. A recent significant improvement in these tasks
comes from BERT. As reported, the next sentence prediction (NSP) in BERT, which
learns the contextual relationship between two sentences, is of great
significance for downstream problems with sentence-pair input. Despite the
effectiveness of NSP, we suggest that NSP still lacks the essential signal to
distinguish between entailment and shallow correlation. To remedy this, we
propose to augment the NSP task to a 3-class categorization task, which
includes a category for previous sentence prediction (PSP). The involvement of
PSP encourages the model to focus on the informative semantics to determine the
sentence order, thereby improves the ability of semantic understanding. This
simple modification yields remarkable improvement against vanilla BERT. To
further incorporate the document-level information, the scope of NSP and PSP is
expanded into a broader range, i.e., NSP and PSP also include close but
nonsuccessive sentences, the noise of which is mitigated by the label-smoothing
technique. Both qualitative and quantitative experimental results demonstrate
the effectiveness of the proposed method. Our method consistently improves the
performance on the NLI and MRC benchmarks, including the challenging HANS
dataset \cite{hans}, suggesting that the document-level task is still promising
for the pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Weidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xingyi Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kunlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1"&gt;Bin Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Luo Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1"&gt;Wei Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Taifeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-FACT: A New Benchmark Dataset for Multilingual Fact Checking. (arXiv:2106.09248v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09248</id>
        <link href="http://arxiv.org/abs/2106.09248"/>
        <updated>2021-06-18T02:06:34.303Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce X-FACT: the largest publicly available
multilingual dataset for factual verification of naturally existing real-world
claims. The dataset contains short statements in 25 languages and is labeled
for veracity by expert fact-checkers. The dataset includes a multilingual
evaluation benchmark that measures both out-of-domain generalization, and
zero-shot capabilities of the multilingual models. Using state-of-the-art
multilingual transformer-based models, we develop several automated
fact-checking models that, along with textual claims, make use of additional
metadata and evidence from news stories retrieved using a search engine.
Empirically, our best model attains an F-score of around 40%, suggesting that
our dataset is a challenging benchmark for evaluation of multilingual
fact-checking models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Ashim Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1"&gt;Vivek Srikumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Knowledge Graph-based World Models of Textual Environments. (arXiv:2106.09608v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09608</id>
        <link href="http://arxiv.org/abs/2106.09608"/>
        <updated>2021-06-18T02:06:34.281Z</updated>
        <summary type="html"><![CDATA[World models improve a learning agent's ability to efficiently operate in
interactive and situated environments. This work focuses on the task of
building world models of text-based game environments. Text-based games, or
interactive narratives, are reinforcement learning environments in which agents
perceive and interact with the world using textual natural language. These
environments contain long, multi-step puzzles or quests woven through a world
that is filled with hundreds of characters, locations, and objects. Our world
model learns to simultaneously: (1) predict changes in the world caused by an
agent's actions when representing the world as a knowledge graph; and (2)
generate the set of contextually relevant natural language actions required to
operate in the world. We frame this task as a Set of Sequences generation
problem by exploiting the inherent structure of knowledge graphs and actions
and introduce both a transformer-based multi-task architecture and a loss
function to train it. A zero-shot ablation study on never-before-seen textual
worlds shows that our methodology significantly outperforms existing textual
world modeling techniques as well as the importance of each of our
contributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1"&gt;Prithviraj Ammanabrolu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1"&gt;Mark O. Riedl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can I Be of Further Assistance? Using Unstructured Knowledge Access to Improve Task-oriented Conversational Modeling. (arXiv:2106.09174v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09174</id>
        <link href="http://arxiv.org/abs/2106.09174"/>
        <updated>2021-06-18T02:06:34.274Z</updated>
        <summary type="html"><![CDATA[Most prior work on task-oriented dialogue systems are restricted to limited
coverage of domain APIs. However, users oftentimes have requests that are out
of the scope of these APIs. This work focuses on responding to these
beyond-API-coverage user turns by incorporating external, unstructured
knowledge sources. Our approach works in a pipelined manner with
knowledge-seeking turn detection, knowledge selection, and response generation
in sequence. We introduce novel data augmentation methods for the first two
steps and demonstrate that the use of information extracted from dialogue
context improves the knowledge selection and end-to-end performances. Through
experiments, we achieve state-of-the-art performance for both automatic and
human evaluation metrics on the DSTC9 Track 1 benchmark dataset, validating the
effectiveness of our contributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1"&gt;Di Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seokhwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1"&gt;Dilek Hakkani-Tur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Data Usage via Differentiable Rewards. (arXiv:1911.10088v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.10088</id>
        <link href="http://arxiv.org/abs/1911.10088"/>
        <updated>2021-06-18T02:06:34.266Z</updated>
        <summary type="html"><![CDATA[To acquire a new skill, humans learn better and faster if a tutor, based on
their current knowledge level, informs them of how much attention they should
pay to particular content or practice problems. Similarly, a machine learning
model could potentially be trained better with a scorer that "adapts" to its
current learning state and estimates the importance of each training data
instance. Training such an adaptive scorer efficiently is a challenging
problem; in order to precisely quantify the effect of a data instance at a
given time during the training, it is typically necessary to first complete the
entire training process. To efficiently optimize data usage, we propose a
reinforcement learning approach called Differentiable Data Selection (DDS). In
DDS, we formulate a scorer network as a learnable function of the training
data, which can be efficiently updated along with the main model being trained.
Specifically, DDS updates the scorer with an intuitive reward signal: it should
up-weigh the data that has a similar gradient with a dev set upon which we
would finally like to perform well. Without significant computing overhead, DDS
delivers strong and consistent improvements over several strong baselines on
two very different tasks of machine translation and image classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michel_P/0/1/0/all/0/1"&gt;Paul Michel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1"&gt;Antonios Anastasopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carbonell_J/0/1/0/all/0/1"&gt;Jaime Carbonell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Approach for Normalizing E-commerce Text Attributes (SANTA). (arXiv:2106.09493v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09493</id>
        <link href="http://arxiv.org/abs/2106.09493"/>
        <updated>2021-06-18T02:06:34.258Z</updated>
        <summary type="html"><![CDATA[In this paper, we present SANTA, a scalable framework to automatically
normalize E-commerce attribute values (e.g. "Win 10 Pro") to a fixed set of
pre-defined canonical values (e.g. "Windows 10"). Earlier works on attribute
normalization focused on fuzzy string matching (also referred as syntactic
matching in this paper). In this work, we first perform an extensive study of
nine syntactic matching algorithms and establish that 'cosine' similarity leads
to best results, showing 2.7% improvement over commonly used Jaccard index.
Next, we argue that string similarity alone is not sufficient for attribute
normalization as many surface forms require going beyond syntactic matching
(e.g. "720p" and "HD" are synonyms). While semantic techniques like
unsupervised embeddings (e.g. word2vec/fastText) have shown good results in
word similarity tasks, we observed that they perform poorly to distinguish
between close canonical forms, as these close forms often occur in similar
contexts. We propose to learn token embeddings using a twin network with
triplet loss. We propose an embedding learning task leveraging raw attribute
values and product titles to learn these embeddings in a self-supervised
fashion. We show that providing supervision using our proposed task improves
over both syntactic and unsupervised embeddings based techniques for attribute
normalization. Experiments on a real-world attribute normalization dataset of
50 attributes show that the embeddings trained using our proposed approach
obtain 2.3% improvement over best string matching and 19.3% improvement over
best unsupervised embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1"&gt;Ravi Shankar Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_K/0/1/0/all/0/1"&gt;Kartik Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasiwasia_N/0/1/0/all/0/1"&gt;Nikhil Rasiwasia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Biomedical Interpretable Entity Representations. (arXiv:2106.09502v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09502</id>
        <link href="http://arxiv.org/abs/2106.09502"/>
        <updated>2021-06-18T02:06:34.249Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models induce dense entity representations that offer
strong performance on entity-centric NLP tasks, but such representations are
not immediately interpretable. This can be a barrier to model uptake in
important domains such as biomedicine. There has been recent work on general
interpretable representation learning (Onoe and Durrett, 2020), but these
domain-agnostic representations do not readily transfer to the important domain
of biomedicine. In this paper, we create a new entity type system and training
set from a large corpus of biomedical texts by mapping entities to concepts in
a medical ontology, and from these to Wikipedia pages whose categories are our
types. From this mapping we derive Biomedical Interpretable Entity
Representations(BIERs), in which dimensions correspond to fine-grained entity
types, and values are predicted probabilities that a given entity is of the
corresponding type. We propose a novel method that exploits BIER's final sparse
and intermediate dense representations to facilitate model and entity type
debugging. We show that BIERs achieve strong performance in biomedical tasks
including named entity disambiguation and entity label classification, and we
provide error analysis to highlight the utility of their interpretability,
particularly in low-supervision settings. Finally, we provide our induced 68K
biomedical type system, the corresponding 37 million triples of derived data
used to train BIER models and our best performing model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Olano_D/0/1/0/all/0/1"&gt;Diego Garcia-Olano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onoe_Y/0/1/0/all/0/1"&gt;Yasumasa Onoe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1"&gt;Ioana Baldini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_J/0/1/0/all/0/1"&gt;Joydeep Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1"&gt;Byron C. Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1"&gt;Kush R. Varshney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DocNLI: A Large-scale Dataset for Document-level Natural Language Inference. (arXiv:2106.09449v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09449</id>
        <link href="http://arxiv.org/abs/2106.09449"/>
        <updated>2021-06-18T02:06:34.229Z</updated>
        <summary type="html"><![CDATA[Natural language inference (NLI) is formulated as a unified framework for
solving various NLP problems such as relation extraction, question answering,
summarization, etc. It has been studied intensively in the past few years
thanks to the availability of large-scale labeled datasets. However, most
existing studies focus on merely sentence-level inference, which limits the
scope of NLI's application in downstream NLP problems. This work presents
DocNLI -- a newly-constructed large-scale dataset for document-level NLI.
DocNLI is transformed from a broad range of NLP problems and covers multiple
genres of text. The premises always stay in the document granularity, whereas
the hypotheses vary in length from single sentences to passages with hundreds
of words. Additionally, DocNLI has pretty limited artifacts which unfortunately
widely exist in some popular sentence-level NLI datasets. Our experiments
demonstrate that, even without fine-tuning, a model pretrained on DocNLI shows
promising performance on popular sentence-level benchmarks, and generalizes
well to out-of-domain NLP tasks that rely on inference at document granularity.
Task-specific fine-tuning can bring further improvements. Data, code, and
pretrained models can be found at https://github.com/salesforce/DocNLI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wenpeng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1"&gt;Dragomir Radev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention. (arXiv:2106.09233v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09233</id>
        <link href="http://arxiv.org/abs/2106.09233"/>
        <updated>2021-06-18T02:06:34.218Z</updated>
        <summary type="html"><![CDATA[Distant supervision tackles the data bottleneck in NER by automatically
generating training instances via dictionary matching. Unfortunately, the
learning of DS-NER is severely dictionary-biased, which suffers from spurious
correlations and therefore undermines the effectiveness and the robustness of
the learned models. In this paper, we fundamentally explain the dictionary bias
via a Structural Causal Model (SCM), categorize the bias into intra-dictionary
and inter-dictionary biases, and identify their causes. Based on the SCM, we
learn de-biased DS-NER via causal interventions. For intra-dictionary bias, we
conduct backdoor adjustment to remove the spurious correlations introduced by
the dictionary confounder. For inter-dictionary bias, we propose a causal
invariance regularizer which will make DS-NER models more robust to the
perturbation of dictionaries. Experiments on four datasets and three DS-NER
models show that our method can significantly improve the performance of
DS-NER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenkai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hongyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xianpei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Le Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Laws for Acoustic Models. (arXiv:2106.09488v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.09488</id>
        <link href="http://arxiv.org/abs/2106.09488"/>
        <updated>2021-06-18T02:06:34.210Z</updated>
        <summary type="html"><![CDATA[There is a recent trend in machine learning to increase model quality by
growing models to sizes previously thought to be unreasonable. Recent work has
shown that autoregressive generative models with cross-entropy objective
functions exhibit smooth power-law relationships, or scaling laws, that predict
model quality from model size, training set size, and the available compute
budget. These scaling laws allow one to choose nearly optimal hyper-parameters
given constraints on available training data, model parameter count, or
training computation budget. In this paper, we demonstrate that acoustic models
trained with an auto-predictive coding loss behave as if they are subject to
similar scaling laws. We extend previous work to jointly predict loss due to
model size, to training set size, and to the inherent "irreducible loss" of the
task. We find that the scaling laws accurately match model performance over two
orders of magnitude in both model size and training set size, and make
predictions about the limits of model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Droppo_J/0/1/0/all/0/1"&gt;Jasha Droppo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Elibol_O/0/1/0/all/0/1"&gt;Oguz Elibol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks. (arXiv:2106.09462v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09462</id>
        <link href="http://arxiv.org/abs/2106.09462"/>
        <updated>2021-06-18T02:06:34.202Z</updated>
        <summary type="html"><![CDATA[Extracting opinions from texts has gathered a lot of interest in the last
years, as we are experiencing an unprecedented volume of user-generated content
in social networks and other places. A problem that social researchers find in
using opinion mining tools is that they are usually behind commercial APIs and
unavailable for other languages than English. To address these issues, we
present pysentimiento, a multilingual Python toolkit for Sentiment Analysis and
other Social NLP tasks. This open-source library brings state-of-the-art models
for Spanish and English in a black-box fashion, allowing researchers to easily
access these techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1"&gt;Juan Manuel P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giudici_J/0/1/0/all/0/1"&gt;Juan Carlos Giudici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luque_F/0/1/0/all/0/1"&gt;Franco Luque&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dr-Vectors: Decision Residual Networks and an Improved Loss for Speaker Recognition. (arXiv:2104.01989v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01989</id>
        <link href="http://arxiv.org/abs/2104.01989"/>
        <updated>2021-06-18T02:06:34.194Z</updated>
        <summary type="html"><![CDATA[Many neural network speaker recognition systems model each speaker using a
fixed-dimensional embedding vector. These embeddings are generally compared
using either linear or 2nd-order scoring and, until recently, do not handle
utterance-specific uncertainty. In this work we propose scoring these
representations in a way that can capture uncertainty, enroll/test asymmetry
and additional non-linear information. This is achieved by incorporating a
2nd-stage neural network (known as a decision network) as part of an end-to-end
training regimen. In particular, we propose the concept of decision residual
networks which involves the use of a compact decision network to leverage
cosine scores and to model the residual signal that's needed. Additionally, we
present a modification to the generalized end-to-end softmax loss function to
target the separation of same/different speaker scores. We observed significant
performance gains for the two techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pelecanos_J/0/1/0/all/0/1"&gt;Jason Pelecanos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Quan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreno_I/0/1/0/all/0/1"&gt;Ignacio Lopez Moreno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Listen with Intent: Improving Speech Recognition with Audio-to-Intent Front-End. (arXiv:2105.07071v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07071</id>
        <link href="http://arxiv.org/abs/2105.07071"/>
        <updated>2021-06-18T02:06:34.173Z</updated>
        <summary type="html"><![CDATA[Comprehending the overall intent of an utterance helps a listener recognize
the individual words spoken. Inspired by this fact, we perform a novel study of
the impact of explicitly incorporating intent representations as additional
information to improve a recurrent neural network-transducer (RNN-T) based
automatic speech recognition (ASR) system. An audio-to-intent (A2I) model
encodes the intent of the utterance in the form of embeddings or posteriors,
and these are used as auxiliary inputs for RNN-T training and inference.
Experimenting with a 50k-hour far-field English speech corpus, this study shows
that when running the system in non-streaming mode, where intent representation
is extracted from the entire utterance and then used to bias streaming RNN-T
search from the start, it provides a 5.56% relative word error rate reduction
(WERR). On the other hand, a streaming system using per-frame intent posteriors
as extra inputs for the RNN-T ASR system yields a 3.33% relative WERR. A
further detailed analysis of the streaming system indicates that our proposed
method brings especially good gain on media-playing related intents (e.g. 9.12%
relative WERR on PlayMusicIntent).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ray_S/0/1/0/all/0/1"&gt;Swayambhu Nath Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1"&gt;Minhua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raju_A/0/1/0/all/0/1"&gt;Anirudh Raju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghahremani_P/0/1/0/all/0/1"&gt;Pegah Ghahremani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilgi_R/0/1/0/all/0/1"&gt;Raghavendra Bilgi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rao_M/0/1/0/all/0/1"&gt;Milind Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arsikere_H/0/1/0/all/0/1"&gt;Harish Arsikere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rastrow_A/0/1/0/all/0/1"&gt;Ariya Rastrow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stolcke_A/0/1/0/all/0/1"&gt;Andreas Stolcke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Droppo_J/0/1/0/all/0/1"&gt;Jasha Droppo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating Methods to Improve Language Model Integration for Attention-based Encoder-Decoder ASR Models. (arXiv:2104.05544v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05544</id>
        <link href="http://arxiv.org/abs/2104.05544"/>
        <updated>2021-06-18T02:06:34.166Z</updated>
        <summary type="html"><![CDATA[Attention-based encoder-decoder (AED) models learn an implicit internal
language model (ILM) from the training transcriptions. The integration with an
external LM trained on much more unpaired text usually leads to better
performance. A Bayesian interpretation as in the hybrid autoregressive
transducer (HAT) suggests dividing by the prior of the discriminative acoustic
model, which corresponds to this implicit LM, similarly as in the hybrid hidden
Markov model approach. The implicit LM cannot be calculated efficiently in
general and it is yet unclear what are the best methods to estimate it. In this
work, we compare different approaches from the literature and propose several
novel methods to estimate the ILM directly from the AED model. Our proposed
methods outperform all previous approaches. We also investigate other methods
to suppress the ILM mainly by decreasing the capacity of the AED model,
limiting the label context, and also by training the AED model together with a
pre-existing LM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1"&gt;Mohammad Zeineldeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glushko_A/0/1/0/all/0/1"&gt;Aleksandr Glushko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michel_W/0/1/0/all/0/1"&gt;Wilfried Michel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1"&gt;Albert Zeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks. (arXiv:2012.12352v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12352</id>
        <link href="http://arxiv.org/abs/2012.12352"/>
        <updated>2021-06-18T02:06:34.158Z</updated>
        <summary type="html"><![CDATA[We investigate the reasoning ability of pretrained vision and language (V&L)
models in two tasks that require multimodal integration: (1) discriminating a
correct image-sentence pair from an incorrect one, and (2) counting entities in
an image. We evaluate three pretrained V&L models on these tasks: ViLBERT,
ViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results
show that models solve task (1) very well, as expected, since all models are
pretrained on task (1). However, none of the pretrained V&L models is able to
adequately solve task (2), our counting probe, and they cannot generalise to
out-of-distribution quantities. We propose a number of explanations for these
findings: LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of
catastrophic forgetting on task (1). Concerning our results on the counting
probe, we find evidence that all models are impacted by dataset bias, and also
fail to individuate entities in the visual input. While a selling point of
pretrained V&L models is their ability to solve complex tasks, our findings
suggest that understanding their reasoning and grounding capabilities requires
more targeted investigations on specific phenomena.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1"&gt;Letitia Parcalabescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1"&gt;Albert Gatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Anette Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1"&gt;Iacer Calixto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Denoising Distantly Supervised Named Entity Recognition via a Hypergeometric Probabilistic Model. (arXiv:2106.09234v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09234</id>
        <link href="http://arxiv.org/abs/2106.09234"/>
        <updated>2021-06-18T02:06:34.151Z</updated>
        <summary type="html"><![CDATA[Denoising is the essential step for distant supervision based named entity
recognition. Previous denoising methods are mostly based on instance-level
confidence statistics, which ignore the variety of the underlying noise
distribution on different datasets and entity types. This makes them difficult
to be adapted to high noise rate settings. In this paper, we propose
Hypergeometric Learning (HGL), a denoising algorithm for distantly supervised
NER that takes both noise distribution and instance-level confidence into
consideration. Specifically, during neural network training, we naturally model
the noise samples in each batch following a hypergeometric distribution
parameterized by the noise-rate. Then each instance in the batch is regarded as
either correct or noisy one according to its label confidence derived from
previous training step, as well as the noise distribution in this sampled
batch. Experiments show that HGL can effectively denoise the weakly-labeled
data retrieved from distant supervision, and therefore results in significant
improvements on the trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenkai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hongyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xianpei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Le Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huidan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhicheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1"&gt;Nicholas Jing Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layer Pruning on Demand with Intermediate CTC. (arXiv:2106.09216v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.09216</id>
        <link href="http://arxiv.org/abs/2106.09216"/>
        <updated>2021-06-18T02:06:34.143Z</updated>
        <summary type="html"><![CDATA[Deploying an end-to-end automatic speech recognition (ASR) model on
mobile/embedded devices is a challenging task, since the device computational
power and energy consumption requirements are dynamically changed in practice.
To overcome the issue, we present a training and pruning method for ASR based
on the connectionist temporal classification (CTC) which allows reduction of
model depth at run-time without any extra fine-tuning. To achieve the goal, we
adopt two regularization methods, intermediate CTC and stochastic depth, to
train a model whose performance does not degrade much after pruning. We present
an in-depth analysis of layer behaviors using singular vector canonical
correlation analysis (SVCCA), and efficient strategies for finding layers which
are safe to prune. Using the proposed method, we show that a Transformer-CTC
model can be pruned in various depth on demand, improving real-time factor from
0.005 to 0.002 on GPU, while each pruned sub-model maintains the accuracy of
individually trained model of the same depth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jaesong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kang_J/0/1/0/all/0/1"&gt;Jingu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STN4DST: A Scalable Dialogue State Tracking based on Slot Tagging Navigation. (arXiv:2010.10811v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10811</id>
        <link href="http://arxiv.org/abs/2010.10811"/>
        <updated>2021-06-18T02:06:34.135Z</updated>
        <summary type="html"><![CDATA[Scalability for handling unknown slot values is a important problem in
dialogue state tracking (DST). As far as we know, previous scalable DST
approaches generally rely on either the candidate generation from slot tagging
output or the span extraction in dialogue context. However, the candidate
generation based DST often suffers from error propagation due to its pipelined
two-stage process; meanwhile span extraction based DST has the risk of
generating invalid spans in the lack of semantic constraints between start and
end position pointers. To tackle the above drawbacks, in this paper, we propose
a novel scalable dialogue state tracking method based on slot tagging
navigation, which implements an end-to-end single-step pointer to locate and
extract slot value quickly and accurately by the joint learning of slot tagging
and slot value position prediction in the dialogue context, especially for
unknown slot values. Extensive experiments over several benchmark datasets show
that the proposed model performs better than state-of-the-art baselines
greatly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Puhai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heyan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1"&gt;Xianling Mao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Large Scale Molecular Language Representations Capture Important Structural Information?. (arXiv:2106.09553v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09553</id>
        <link href="http://arxiv.org/abs/2106.09553"/>
        <updated>2021-06-18T02:06:34.115Z</updated>
        <summary type="html"><![CDATA[Predicting chemical properties from the structure of a molecule is of great
importance in many applications including drug discovery and material design.
Machine learning based molecular property prediction holds the promise of
enabling accurate predictions at much less complexity, when compared to, for
example Density Functional Theory (DFT) calculations. Features extracted from
molecular graphs, using graph neural nets in a supervised manner, have emerged
as strong baselines for such tasks. However, the vast chemical space together
with the limited availability of labels makes supervised learning challenging,
calling for learning a general-purpose molecular representation. Recently,
pre-trained transformer-based language models (PTLMs) on large unlabeled corpus
have produced state-of-the-art results in many downstream natural language
processing tasks. Inspired by this development, here we present molecular
embeddings obtained by training an efficient transformer encoder model,
referred to as MoLFormer. This model was employed with a linear attention
mechanism and highly paralleized training on 1D SMILES sequences of 1.1 billion
unlabeled molecules from the PubChem and ZINC datasets. Experiments show that
the learned molecular representation performs competitively, when compared to
existing graph-based and fingerprint-based supervised learning baselines, on
the challenging tasks of predicting properties of QM8 and QM9 molecules.
Further task-specific fine-tuning of the MoLFormerr representation improves
performance on several of those property prediction benchmarks. These results
provide encouraging evidence that large-scale molecular language models can
capture sufficient structural information to be able to accurately predict
quantum chemical properties and beyond.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1"&gt;Jerret Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belgodere_B/0/1/0/all/0/1"&gt;Brian Belgodere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1"&gt;Vijil Chenthamarakshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1"&gt;Inkit Padhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1"&gt;Youssef Mroueh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1"&gt;Payel Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASR Adaptation for E-commerce Chatbots using Cross-Utterance Context and Multi-Task Language Modeling. (arXiv:2106.09532v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.09532</id>
        <link href="http://arxiv.org/abs/2106.09532"/>
        <updated>2021-06-18T02:06:34.106Z</updated>
        <summary type="html"><![CDATA[Automatic Speech Recognition (ASR) robustness toward slot entities are
critical in e-commerce voice assistants that involve monetary transactions and
purchases. Along with effective domain adaptation, it is intuitive that cross
utterance contextual cues play an important role in disambiguating domain
specific content words from speech. In this paper, we investigate various
techniques to improve contextualization, content word robustness and domain
adaptation of a Transformer-XL neural language model (NLM) to rescore ASR
N-best hypotheses. To improve contextualization, we utilize turn level dialogue
acts along with cross utterance context carry over. Additionally, to adapt our
domain-general NLM towards e-commerce on-the-fly, we use embeddings derived
from a finetuned masked LM on in-domain data. Finally, to improve robustness
towards in-domain content words, we propose a multi-task model that can jointly
perform content word detection and language modeling tasks. Compared to a
non-contextual LSTM LM baseline, our best performing NLM rescorer results in a
content WER reduction of 19.2% on e-commerce audio test set and a slot labeling
F1 improvement of 6.4%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shenoy_A/0/1/0/all/0/1"&gt;Ashish Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bodapati_S/0/1/0/all/0/1"&gt;Sravan Bodapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kirchhoff_K/0/1/0/all/0/1"&gt;Katrin Kirchhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction. (arXiv:2106.09232v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09232</id>
        <link href="http://arxiv.org/abs/2106.09232"/>
        <updated>2021-06-18T02:06:34.093Z</updated>
        <summary type="html"><![CDATA[Event extraction is challenging due to the complex structure of event records
and the semantic gap between text and event. Traditional methods usually
extract event records by decomposing the complex structure prediction task into
multiple subtasks. In this paper, we propose Text2Event, a
sequence-to-structure generation paradigm that can directly extract events from
the text in an end-to-end manner. Specifically, we design a
sequence-to-structure network for unified event extraction, a constrained
decoding algorithm for event knowledge injection during inference, and a
curriculum learning algorithm for efficient model learning. Experimental
results show that, by uniformly modeling all tasks in a single model and
universally predicting different labels, our method can achieve competitive
performance using only record-level annotations in both supervised learning and
transfer learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yaojie Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hongyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xianpei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jialong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Annan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Le Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1"&gt;Meng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shaoyi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models. (arXiv:2106.09204v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09204</id>
        <link href="http://arxiv.org/abs/2106.09204"/>
        <updated>2021-06-18T02:06:33.971Z</updated>
        <summary type="html"><![CDATA[The performance of fine-tuning pre-trained language models largely depends on
the hyperparameter configuration. In this paper, we investigate the performance
of modern hyperparameter optimization methods (HPO) on fine-tuning pre-trained
language models. First, we study and report three HPO algorithms' performances
on fine-tuning two state-of-the-art language models on the GLUE dataset. We
find that using the same time budget, HPO often fails to outperform grid search
due to two reasons: insufficient time budget and overfitting. We propose two
general strategies and an experimental procedure to systematically troubleshoot
HPO's failure cases. By applying the procedure, we observe that HPO can succeed
with more appropriate settings in the search space and time budget; however, in
certain cases overfitting remains. Finally, we make suggestions for future
work. Our implementation can be found in
https://github.com/microsoft/FLAML/tree/main/flaml/nlp/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xueqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence. (arXiv:2004.03974v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03974</id>
        <link href="http://arxiv.org/abs/2004.03974"/>
        <updated>2021-06-18T02:06:33.963Z</updated>
        <summary type="html"><![CDATA[Topic models extract groups of words from documents, whose interpretation as
a topic hopefully allows for a better understanding of the data. However, the
resulting word groups are often not coherent, making them harder to interpret.
Recently, neural topic models have shown improvements in overall coherence.
Concurrently, contextual embeddings have advanced the state of the art of
neural models in general. In this paper, we combine contextualized
representations with neural topic models. We find that our approach produces
more meaningful and coherent topics than traditional bag-of-words topic models
and recent neural models. Our results indicate that future improvements in
language models will translate into better topic models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1"&gt;Federico Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terragni_S/0/1/0/all/0/1"&gt;Silvia Terragni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1"&gt;Dirk Hovy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lost in Interpreting: Speech Translation from Source or Interpreter?. (arXiv:2106.09343v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09343</id>
        <link href="http://arxiv.org/abs/2106.09343"/>
        <updated>2021-06-18T02:06:33.954Z</updated>
        <summary type="html"><![CDATA[Interpreters facilitate multi-lingual meetings but the affordable set of
languages is often smaller than what is needed. Automatic simultaneous speech
translation can extend the set of provided languages. We investigate if such an
automatic system should rather follow the original speaker, or an interpreter
to achieve better translation quality at the cost of increased delay.

To answer the question, we release Europarl Simultaneous Interpreting Corpus
(ESIC), 10 hours of recordings and transcripts of European Parliament speeches
in English, with simultaneous interpreting into Czech and German. We evaluate
quality and latency of speaker-based and interpreter-based spoken translation
systems from English to Czech. We study the differences in implicit
simplification and summarization of the human interpreter compared to a machine
translation system trained to shorten the output to some extent. Finally, we
perform human evaluation to measure information loss of each of these
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Machacek_D/0/1/0/all/0/1"&gt;Dominik Mach&amp;#xe1;&amp;#x10d;ek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zilinec_M/0/1/0/all/0/1"&gt;Mat&amp;#xfa;&amp;#x161; &amp;#x17d;ilinec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1"&gt;Ond&amp;#x159;ej Bojar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases. (arXiv:2106.09231v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09231</id>
        <link href="http://arxiv.org/abs/2106.09231"/>
        <updated>2021-06-18T02:06:33.945Z</updated>
        <summary type="html"><![CDATA[Previous literatures show that pre-trained masked language models (MLMs) such
as BERT can achieve competitive factual knowledge extraction performance on
some datasets, indicating that MLMs can potentially be a reliable knowledge
source. In this paper, we conduct a rigorous study to explore the underlying
predicting mechanisms of MLMs over different extraction paradigms. By
investigating the behaviors of MLMs, we find that previous decent performance
mainly owes to the biased prompts which overfit dataset artifacts. Furthermore,
incorporating illustrative cases and external contexts improve knowledge
prediction mainly due to entity type guidance and golden answer leakage. Our
findings shed light on the underlying predicting mechanisms of MLMs, and
strongly question the previous conclusion that current MLMs can potentially
serve as reliable factual knowledge bases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1"&gt;Boxi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hongyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xianpei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Le Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1"&gt;Lingyong Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1"&gt;Meng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1"&gt;Tong Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probing Image-Language Transformers for Verb Understanding. (arXiv:2106.09141v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09141</id>
        <link href="http://arxiv.org/abs/2106.09141"/>
        <updated>2021-06-18T02:06:33.929Z</updated>
        <summary type="html"><![CDATA[Multimodal image-language transformers have achieved impressive results on a
variety of tasks that rely on fine-tuning (e.g., visual question answering and
image retrieval). We are interested in shedding light on the quality of their
pretrained representations -- in particular, if these models can distinguish
different types of verbs or if they rely solely on nouns in a given sentence.
To do so, we collect a dataset of image-sentence pairs (in English) consisting
of 421 verbs that are either visual or commonly found in the pretraining data
(i.e., the Conceptual Captions dataset). We use this dataset to evaluate
pretrained image-language transformers and find that they fail more in
situations that require verb understanding compared to other parts of speech.
We also investigate what category of verbs are particularly challenging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1"&gt;Lisa Anne Hendricks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1"&gt;Aida Nematzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Position-wise Interaction Network for CTR Prediction. (arXiv:2106.05482v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05482</id>
        <link href="http://arxiv.org/abs/2106.05482"/>
        <updated>2021-06-18T02:06:33.753Z</updated>
        <summary type="html"><![CDATA[Click-through rate (CTR) prediction plays an important role in online
advertising and recommender systems. In practice, the training of CTR models
depends on click data which is intrinsically biased towards higher positions
since higher position has higher CTR by nature. Existing methods such as actual
position training with fixed position inference and inverse propensity weighted
training with no position inference alleviate the bias problem to some extend.
However, the different treatment of position information between training and
inference will inevitably lead to inconsistency and sub-optimal online
performance. Meanwhile, the basic assumption of these methods, i.e., the click
probability is the product of examination probability and relevance
probability, is oversimplified and insufficient to model the rich interaction
between position and other information. In this paper, we propose a Deep
Position-wise Interaction Network (DPIN) to efficiently combine all candidate
items and positions for estimating CTR at each position, achieving consistency
between offline and online as well as modeling the deep non-linear interaction
among position, user, context and item under the limit of serving performance.
Following our new treatment to the position bias in CTR prediction, we propose
a new evaluation metrics named PAUC (position-wise AUC) that is suitable for
measuring the ranking quality at a given position. Through extensive
experiments on a real world dataset, we show empirically that our method is
both effective and efficient in solving position bias problem. We have also
deployed our method in production and observed statistically significant
improvement over a highly optimized baseline in a rigorous A/B test.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1"&gt;Ke Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1"&gt;Qingtao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingjian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yi Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jia Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jun Lei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Popularity of Reddit Posts with AI. (arXiv:2106.07380v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07380</id>
        <link href="http://arxiv.org/abs/2106.07380"/>
        <updated>2021-06-18T02:06:33.744Z</updated>
        <summary type="html"><![CDATA[Social media creates crucial mass changes, as popular posts and opinions cast
a significant influence on users' decisions and thought processes. For example,
the recent Reddit uprising inspired by r/wallstreetbets which had remarkable
economic impact was started with a series of posts on the thread. The
prediction of posts that may have a notable impact will allow for the
preparation of possible following trends. This study aims to develop a machine
learning model capable of accurately predicting the popularity of a Reddit
post. Specifically, the model is predicting the number of upvotes a post will
receive based on its textual content. I experimented with three different
models: a baseline linear regression model, a random forest regression model,
and a neural network. I collected Reddit post data from an online data set and
analyzed the model's performance when trained on a single subreddit and a
collection of subreddits. The results showed that the neural network model
performed the best when the loss of the models were compared. With the use of a
machine learning model to predict social trends through the reaction users have
to post, a better picture of the near future can be envisioned.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Juno Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open Data and the Status Quo -- A Fine-Grained Evaluation Framework for Open Data Quality and an Analysis of Open Data portals in Germany. (arXiv:2106.09590v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09590</id>
        <link href="http://arxiv.org/abs/2106.09590"/>
        <updated>2021-06-18T02:06:33.730Z</updated>
        <summary type="html"><![CDATA[This paper presents a framework for assessing data and metadata quality
within Open Data portals. Although a few benchmark frameworks already exist for
this purpose, they are not yet detailed enough in both breadth and depth to
make valid statements about the actual discoverability and accessibility of
publicly available data collections. To address this research gap, we have
designed a quality framework that is able to evaluate data quality in Open Data
portals on dedicated and fine-grained dimensions, such as interoperability,
findability, uniqueness or completeness. Additionally, we propose quality
measures that allow for valid assessments regarding cross-portal findability
and uniqueness of dataset descriptions. We have validated our novel quality
framework for the German Open Data landscape and found out that metadata often
still lacks meaningful descriptions and is not yet extensively connected to the
Semantic Web.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wenige_L/0/1/0/all/0/1"&gt;Lisa Wenige&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stadler_C/0/1/0/all/0/1"&gt;Claus Stadler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1"&gt;Michael Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Figura_R/0/1/0/all/0/1"&gt;Richard Figura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sauter_R/0/1/0/all/0/1"&gt;Robert Sauter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_C/0/1/0/all/0/1"&gt;Christopher W. Frank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Specializing Multilingual Language Models: An Empirical Study. (arXiv:2106.09063v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09063</id>
        <link href="http://arxiv.org/abs/2106.09063"/>
        <updated>2021-06-18T02:06:33.703Z</updated>
        <summary type="html"><![CDATA[Contextualized word representations from pretrained multilingual language
models have become the de facto standard for addressing natural language tasks
in many different languages, but the success of this approach is far from
universal. For languages rarely or never seen by these models, directly using
such models often results in suboptimal representation or use of data,
motivating additional model adaptations to achieve reasonably strong
performance. In this work, we study the performance, extensibility, and
interaction of two such adaptations for this low-resource setting: vocabulary
augmentation and script transliteration. Our evaluations on a set of three
tasks in nine diverse low-resource languages yield a mixed result, upholding
the viability of these approaches while raising new questions around how to
optimally adapt multilingual models to low-resource settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chau_E/0/1/0/all/0/1"&gt;Ethan C. Chau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Self-supervised Method for Entity Alignment. (arXiv:2106.09395v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09395</id>
        <link href="http://arxiv.org/abs/2106.09395"/>
        <updated>2021-06-18T02:06:33.689Z</updated>
        <summary type="html"><![CDATA[Entity alignment, aiming to identify equivalent entities across different
knowledge graphs (KGs), is a fundamental problem for constructing large-scale
KGs. Over the course of its development, supervision has been considered
necessary for accurate alignments. Inspired by the recent progress of
self-supervised learning, we explore the extent to which we can get rid of
supervision for entity alignment. Existing supervised methods for this task
focus on pulling each pair of positive (labeled) entities close to each other.
However, our analysis suggests that the learning of entity alignment can
actually benefit more from pushing sampled (unlabeled) negatives far away than
pulling positive aligned pairs close. We present SelfKG by leveraging this
discovery to design a contrastive learning strategy across two KGs. Extensive
experiments on benchmark datasets demonstrate that SelfKG without supervision
can match or achieve comparable results with state-of-the-art supervised
baselines. The performance of SelfKG demonstrates self-supervised learning
offers great potential for entity alignment in KGs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1"&gt;Haoyun Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinghao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zeyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1"&gt;Evgeny Kharlamov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yuxiao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Online Chats with DAG-Structured LSTMs. (arXiv:2106.09024v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09024</id>
        <link href="http://arxiv.org/abs/2106.09024"/>
        <updated>2021-06-18T02:06:33.655Z</updated>
        <summary type="html"><![CDATA[Many modern messaging systems allow fast and synchronous textual
communication among many users. The resulting sequence of messages hides a more
complicated structure in which independent sub-conversations are interwoven
with one another. This poses a challenge for any task aiming to understand the
content of the chat logs or gather information from them. The ability to
disentangle these conversations is then tantamount to the success of many
downstream tasks such as summarization and question answering. Structured
information accompanying the text such as user turn, user mentions, timestamps,
is used as a cue by the participants themselves who need to follow the
conversation and has been shown to be important for disentanglement. DAG-LSTMs,
a generalization of Tree-LSTMs that can handle directed acyclic dependencies,
are a natural way to incorporate such information and its non-sequential
nature. In this paper, we apply DAG-LSTMs to the conversation disentanglement
task. We perform our experiments on the Ubuntu IRC dataset. We show that the
novel model we propose achieves state of the art status on the task of
recovering reply-to relations and it is competitive on other disentanglement
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pappadopulo_D/0/1/0/all/0/1"&gt;Duccio Pappadopulo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_L/0/1/0/all/0/1"&gt;Lisa Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farina_M/0/1/0/all/0/1"&gt;Marco Farina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irsoy_O/0/1/0/all/0/1"&gt;Ozan &amp;#x130;rsoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Construction of Evaluation Suites for Natural Language Generation Datasets. (arXiv:2106.09069v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09069</id>
        <link href="http://arxiv.org/abs/2106.09069"/>
        <updated>2021-06-18T02:06:33.642Z</updated>
        <summary type="html"><![CDATA[Machine learning approaches applied to NLP are often evaluated by summarizing
their performance in a single number, for example accuracy. Since most test
sets are constructed as an i.i.d. sample from the overall data, this approach
overly simplifies the complexity of language and encourages overfitting to the
head of the data distribution. As such, rare language phenomena or text about
underrepresented groups are not equally included in the evaluation. To
encourage more in-depth model analyses, researchers have proposed the use of
multiple test sets, also called challenge sets, that assess specific
capabilities of a model. In this paper, we develop a framework based on this
idea which is able to generate controlled perturbations and identify subsets in
text-to-scalar, text-to-text, or data-to-text settings. By applying this
framework to the GEM generation benchmark, we propose an evaluation suite made
of 80 challenge sets, demonstrate the kinds of analyses that it enables and
shed light onto the limits of current generation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mille_S/0/1/0/all/0/1"&gt;Simon Mille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1"&gt;Kaustubh D. Dhole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahamood_S/0/1/0/all/0/1"&gt;Saad Mahamood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Beltrachini_L/0/1/0/all/0/1"&gt;Laura Perez-Beltrachini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1"&gt;Varun Gangal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1"&gt;Mihir Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miltenburg_E/0/1/0/all/0/1"&gt;Emiel van Miltenburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1"&gt;Sebastian Gehrmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PEN4Rec: Preference Evolution Networks for Session-based Recommendation. (arXiv:2106.09306v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09306</id>
        <link href="http://arxiv.org/abs/2106.09306"/>
        <updated>2021-06-18T02:06:33.617Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation aims to predict user the next action based on
historical behaviors in an anonymous session. For better recommendations, it is
vital to capture user preferences as well as their dynamics. Besides, user
preferences evolve over time dynamically and each preference has its own
evolving track. However, most previous works neglect the evolving trend of
preferences and can be easily disturbed by the effect of preference drifting.
In this paper, we propose a novel Preference Evolution Networks for
session-based Recommendation (PEN4Rec) to model preference evolving process by
a two-stage retrieval from historical contexts. Specifically, the first-stage
process integrates relevant behaviors according to recent items. Then, the
second-stage process models the preference evolving trajectory over time
dynamically and infer rich preferences. The process can strengthen the effect
of relevant sequential behaviors during the preference evolution and weaken the
disturbance from preference drifting. Extensive experiments on three public
datasets demonstrate the effectiveness and superiority of the proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dou Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Lingwei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huai_X/0/1/0/all/0/1"&gt;Xiaoyong Huai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zhiqi Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Songlin Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Author Clustering and Topic Estimation for Short Texts. (arXiv:2106.09533v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09533</id>
        <link href="http://arxiv.org/abs/2106.09533"/>
        <updated>2021-06-18T02:06:33.599Z</updated>
        <summary type="html"><![CDATA[Analysis of short text, such as social media posts, is extremely difficult
because it relies on observing many document-level word co-occurrence pairs.
Beyond topic distributions, a common downstream task of the modeling is
grouping the authors of these documents for subsequent analyses. Traditional
models estimate the document groupings and identify user clusters with an
independent procedure. We propose a novel model that expands on the Latent
Dirichlet Allocation by modeling strong dependence among the words in the same
document, with user-level topic distributions. We also simultaneously cluster
users, removing the need for post-hoc cluster estimation and improving topic
estimation by shrinking noisy user-level topic distributions towards typical
values. Our method performs as well as -- or better -- than traditional
approaches to problems arising in short text, and we demonstrate its usefulness
on a dataset of tweets from United States Senators, recovering both meaningful
topics and clusters that reflect partisan ideology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tierney_G/0/1/0/all/0/1"&gt;Graham Tierney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bail_C/0/1/0/all/0/1"&gt;Christopher Bail&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Volfovsky_A/0/1/0/all/0/1"&gt;Alexander Volfovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XDM: Improving Sequential Deep Matching with Unclicked User Behaviors for Recommender System. (arXiv:2010.12837v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12837</id>
        <link href="http://arxiv.org/abs/2010.12837"/>
        <updated>2021-06-18T02:06:33.583Z</updated>
        <summary type="html"><![CDATA[Deep learning-based sequential recommender systems have recently attracted
increasing attention from both academia and industry. Most of industrial
Embedding-Based Retrieval (EBR) system for recommendation share the similar
ideas with sequential recommenders. Among them, how to comprehensively capture
sequential user interest is a fundamental problem. However, most existing
sequential recommendation models take as input clicked or purchased behavior
sequences from user-item interactions. This leads to incomprehensive user
representation and sub-optimal model performance, since they ignore the
complete user behavior exposure data, i.e., items impressed yet unclicked by
users. In this work, we attempt to incorporate and model those unclicked item
sequences using a new learning approach in order to explore better sequential
recommendation technique. An efficient triplet metric learning algorithm is
proposed to appropriately learn the representation of unclicked items. Our
method can be simply integrated with existing sequential recommendation models
by a confidence fusion network and further gain better user representation. The
offline experimental results based on real-world E-commerce data demonstrate
the effectiveness and verify the importance of unclicked items in sequential
recommendation. Moreover we deploy our new model (named XDM) into EBR of
recommender system at Taobao, outperforming the deployed previous generation
SDM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lv_F/0/1/0/all/0/1"&gt;Fuyu Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mengxue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1"&gt;Tonglei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Changlong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1"&gt;Fei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1"&gt;Taiwei Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1"&gt;Hong Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guli Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Keping Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_W/0/1/0/all/0/1"&gt;Wilfred Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Current Challenges and Future Directions in Podcast Information Access. (arXiv:2106.09227v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09227</id>
        <link href="http://arxiv.org/abs/2106.09227"/>
        <updated>2021-06-18T02:06:33.569Z</updated>
        <summary type="html"><![CDATA[Podcasts are spoken documents across a wide-range of genres and styles, with
growing listenership across the world, and a rapidly lowering barrier to entry
for both listeners and creators. The great strides in search and recommendation
in research and industry have yet to see impact in the podcast space, where
recommendations are still largely driven by word of mouth. In this perspective
paper, we highlight the many differences between podcasts and other media, and
discuss our perspective on challenges and future research directions in the
domain of podcast information access.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1"&gt;Rosie Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1"&gt;Hamed Zamani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1"&gt;Markus Schedl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Ching-Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1"&gt;Sravana Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifton_A/0/1/0/all/0/1"&gt;Ann Clifton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlgren_J/0/1/0/all/0/1"&gt;Jussi Karlgren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_H/0/1/0/all/0/1"&gt;Helia Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pappu_A/0/1/0/all/0/1"&gt;Aasish Pappu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nazari_Z/0/1/0/all/0/1"&gt;Zahra Nazari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Longqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Semerci_O/0/1/0/all/0/1"&gt;Oguz Semerci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouchard_H/0/1/0/all/0/1"&gt;Hugues Bouchard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carterette_B/0/1/0/all/0/1"&gt;Ben Carterette&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embedding-based Product Retrieval in Taobao Search. (arXiv:2106.09297v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09297</id>
        <link href="http://arxiv.org/abs/2106.09297"/>
        <updated>2021-06-18T02:06:33.557Z</updated>
        <summary type="html"><![CDATA[Nowadays, the product search service of e-commerce platforms has become a
vital shopping channel in people's life. The retrieval phase of products
determines the search system's quality and gradually attracts researchers'
attention. Retrieving the most relevant products from a large-scale corpus
while preserving personalized user characteristics remains an open question.
Recent approaches in this domain have mainly focused on embedding-based
retrieval (EBR) systems. However, after a long period of practice on Taobao, we
find that the performance of the EBR system is dramatically degraded due to
its: (1) low relevance with a given query and (2) discrepancy between the
training and inference phases. Therefore, we propose a novel and practical
embedding-based product retrieval model, named Multi-Grained Deep Semantic
Product Retrieval (MGDSPR). Specifically, we first identify the inconsistency
between the training and inference stages, and then use the softmax
cross-entropy loss as the training objective, which achieves better performance
and faster convergence. Two efficient methods are further proposed to improve
retrieval relevance, including smoothing noisy training data and generating
relevance-improving hard negative samples without requiring extra knowledge and
training procedures. We evaluate MGDSPR on Taobao Product Search with
significant metrics gains observed in offline experiments and online A/B tests.
MGDSPR has been successfully deployed to the existing multi-channel retrieval
system in Taobao Search. We also introduce the online deployment scheme and
share practical lessons of our retrieval system to contribute to the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_F/0/1/0/all/0/1"&gt;Fuyu Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1"&gt;Taiwei Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guli Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Keping Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xiaoyi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiao-Ming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1"&gt;Qianli Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recovery under Side Constraints. (arXiv:2106.09375v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09375</id>
        <link href="http://arxiv.org/abs/2106.09375"/>
        <updated>2021-06-18T02:06:33.545Z</updated>
        <summary type="html"><![CDATA[This paper addresses sparse signal reconstruction under various types of
structural side constraints with applications in multi-antenna systems. Side
constraints may result from prior information on the measurement system and the
sparse signal structure. They may involve the structure of the sensing matrix,
the structure of the non-zero support values, the temporal structure of the
sparse representationvector, and the nonlinear measurement structure. First, we
demonstrate how a priori information in form of structural side constraints
influence recovery guarantees (null space properties) using L1-minimization.
Furthermore, for constant modulus signals, signals with row-, block- and
rank-sparsity, as well as non-circular signals, we illustrate how structural
prior information can be used to devise efficient algorithms with improved
recovery performance and reduced computational complexity. Finally, we address
the measurement system design for linear and nonlinear measurements of sparse
signals. Moreover, we discuss the linear mixing matrix design based on
coherence minimization. Then we extend our focus to nonlinear measurement
systems where we design parallel optimization algorithms to efficiently compute
stationary points in the sparse phase retrieval problem with and without
dictionary learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ardah_K/0/1/0/all/0/1"&gt;Khaled Ardah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haardt_M/0/1/0/all/0/1"&gt;Martin Haardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matter_F/0/1/0/all/0/1"&gt;Frederic Matter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pesavento_M/0/1/0/all/0/1"&gt;Marius Pesavento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfetsch_M/0/1/0/all/0/1"&gt;Marc E. Pfetsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Effectiveness of Reviews in E-commerce Top-N Recommendation. (arXiv:2106.09665v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09665</id>
        <link href="http://arxiv.org/abs/2106.09665"/>
        <updated>2021-06-18T02:06:33.522Z</updated>
        <summary type="html"><![CDATA[Modern E-commerce websites contain heterogeneous sources of information, such
as numerical ratings, textual reviews and images. These information can be
utilized to assist recommendation. Through textual reviews, a user explicitly
express her affinity towards the item. Previous researchers found that by using
the information extracted from these reviews, we can better profile the users'
explicit preferences as well as the item features, leading to the improvement
of recommendation performance. However, most of the previous algorithms were
only utilizing the review information for explicit-feedback problem i.e. rating
prediction, and when it comes to implicit-feedback ranking problem such as
top-N recommendation, the usage of review information has not been fully
explored. Seeing this gap, in this work, we investigate the effectiveness of
textual review information for top-N recommendation under E-commerce settings.
We adapt several SOTA review-based rating prediction models for top-N
recommendation tasks and compare them to existing top-N recommendation models
from both performance and efficiency. We find that models utilizing only review
information can not achieve better performances than vanilla implicit-feedback
matrix factorization method. When utilizing review information as a regularizer
or auxiliary information, the performance of implicit-feedback matrix
factorization method can be further improved. However, the optimal model
structure to utilize textual reviews for E-commerce top-N recommendation is yet
to be determined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhichao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Hansi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingyao Ai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model. (arXiv:2106.09317v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.09317</id>
        <link href="http://arxiv.org/abs/2106.09317"/>
        <updated>2021-06-18T02:06:33.097Z</updated>
        <summary type="html"><![CDATA[Recently, there has been an increasing interest in neural speech synthesis.
While the deep neural network achieves the state-of-the-art result in
text-to-speech (TTS) tasks, how to generate a more emotional and more
expressive speech is becoming a new challenge to researchers due to the
scarcity of high-quality emotion speech dataset and the lack of advanced
emotional TTS model. In this paper, we first briefly introduce and publicly
release a Mandarin emotion speech dataset including 9,724 samples with audio
files and its emotion human-labeled annotation. After that, we propose a simple
but efficient architecture for emotional speech synthesis called EMSpeech.
Unlike those models which need additional reference audio as input, our model
could predict emotion labels just from the input text and generate more
expressive speech conditioned on the emotion embedding. In the experiment
phase, we first validate the effectiveness of our dataset by an emotion
classification task. Then we train our model on the proposed dataset and
conduct a series of subjective evaluations. Finally, by showing a comparable
performance in the emotional speech synthesis task, we successfully demonstrate
the ability of the proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1"&gt;Chenye Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Feiyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1"&gt;Rongjie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_M/0/1/0/all/0/1"&gt;Ming Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhou Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Two-stage Multi-modal Affect Analysis Framework for Children with Autism Spectrum Disorder. (arXiv:2106.09199v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09199</id>
        <link href="http://arxiv.org/abs/2106.09199"/>
        <updated>2021-06-18T02:06:33.053Z</updated>
        <summary type="html"><![CDATA[Autism spectrum disorder (ASD) is a developmental disorder that influences
the communication and social behavior of a person in a way that those in the
spectrum have difficulty in perceiving other people's facial expressions, as
well as presenting and communicating emotions and affect via their own faces
and bodies. Some efforts have been made to predict and improve children with
ASD's affect states in play therapy, a common method to improve children's
social skills via play and games. However, many previous works only used
pre-trained models on benchmark emotion datasets and failed to consider the
distinction in emotion between typically developing children and children with
autism. In this paper, we present an open-source two-stage multi-modal approach
leveraging acoustic and visual cues to predict three main affect states of
children with ASD's affect states (positive, negative, and neutral) in
real-world play therapy scenarios, and achieved an overall accuracy of 72:40%.
This work presents a novel way to combine human expertise and machine
intelligence for ASD affect recognition by proposing a two-stage schema.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jicheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_A/0/1/0/all/0/1"&gt;Anjana Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barmaki_R/0/1/0/all/0/1"&gt;Roghayeh Barmaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Predictive Coding Account for Chaotic Itinerancy. (arXiv:2106.08937v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.08937</id>
        <link href="http://arxiv.org/abs/2106.08937"/>
        <updated>2021-06-17T16:16:41.906Z</updated>
        <summary type="html"><![CDATA[As a phenomenon in dynamical systems allowing autonomous switching between
stable behaviors, chaotic itinerancy has gained interest in neurorobotics
research. In this study, we draw a connection between this phenomenon and the
predictive coding theory by showing how a recurrent neural network implementing
predictive coding can generate neural trajectories similar to chaotic
itinerancy in the presence of input noise. We propose two scenarios generating
random and past-independent attractor switching trajectories using our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Annabi_L/0/1/0/all/0/1"&gt;Louis Annabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pitti_A/0/1/0/all/0/1"&gt;Alexandre Pitti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quoy_M/0/1/0/all/0/1"&gt;Mathias Quoy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised GANs with Label Augmentation. (arXiv:2106.08601v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08601</id>
        <link href="http://arxiv.org/abs/2106.08601"/>
        <updated>2021-06-17T16:16:41.579Z</updated>
        <summary type="html"><![CDATA[Recently, transformation-based self-supervised learning has been applied to
generative adversarial networks (GANs) to mitigate the catastrophic forgetting
problem of discriminator by learning stable representations. However, the
separate self-supervised tasks in existing self-supervised GANs cause an
inconsistent goal with generative modeling due to the learning of the generator
from their generator distribution-agnostic classifiers. To address this issue,
we propose a novel self-supervised GANs framework with label augmentation,
i.e., augmenting the GAN labels (real or fake) with the self-supervised
pseudo-labels. In particular, the discriminator and the self-supervised
classifier are unified to learn a single task that predicts the augmented label
such that the discriminator/classifier is aware of the generator distribution,
while the generator tries to confuse the discriminator/classifier by optimizing
the discrepancy between the transformed real and generated distributions.
Theoretically, we prove that the generator, at the equilibrium point, converges
to replicate the data distribution. Empirically, we demonstrate that the
proposed method significantly outperforms competitive baselines on both
generative modeling and representation learning across benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1"&gt;Liang Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Huawei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1"&gt;Qi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automating Augmentation Through Random Unidimensional Search. (arXiv:2106.08756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08756</id>
        <link href="http://arxiv.org/abs/2106.08756"/>
        <updated>2021-06-17T16:16:41.547Z</updated>
        <summary type="html"><![CDATA[It is no secret amongst deep learning researchers that finding the right data
augmentation strategy during training can mean the difference between a
state-of-the-art result and a run-of-the-mill ranking. To that end, the
community has seen many efforts to automate the process of finding the perfect
augmentation procedure for any task at hand. Unfortunately, even recent
cutting-edge methods bring massive computational overhead, requiring as many as
100 full model trainings to settle on an ideal configuration. We show how to
achieve even better performance in just 7: with Random Unidimensional
Augmentation. Source code is available at https://github.com/fastestimator/RUA]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaomeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1"&gt;Michael Potter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1"&gt;Gaurav Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yun-Chan Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saripalli_V/0/1/0/all/0/1"&gt;V. Ratna Saripalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Space Search for Pareto-Efficient Spaces. (arXiv:2104.11014v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11014</id>
        <link href="http://arxiv.org/abs/2104.11014"/>
        <updated>2021-06-17T15:44:17.182Z</updated>
        <summary type="html"><![CDATA[Network spaces have been known as a critical factor in both handcrafted
network designs or defining search spaces for Neural Architecture Search (NAS).
However, an effective space involves tremendous prior knowledge and/or manual
effort, and additional constraints are required to discover efficiency-aware
architectures. In this paper, we define a new problem, Network Space Search
(NSS), as searching for favorable network spaces instead of a single
architecture. We propose an NSS method to directly search for efficient-aware
network spaces automatically, reducing the manual effort and immense cost in
discovering satisfactory ones. The resultant network spaces, named Elite
Spaces, are discovered from Expanded Search Space with minimal human expertise
imposed. The Pareto-efficient Elite Spaces are aligned with the Pareto front
under various complexity constraints and can be further served as NAS search
spaces, benefiting differentiable NAS approaches (e.g. In CIFAR-100, an
averagely 2.3% lower error rate and 3.7% closer to target constraint than the
baseline with around 90% fewer samples required to find satisfactory networks).
Moreover, our NSS approach is capable of searching for superior spaces in
future unexplored spaces, revealing great potential in searching for network
spaces automatically. Website:
https://minhungchen.netlify.app/publication/nss/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1"&gt;Min-Fong Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao-Yun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Min-Hung Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yu-Syuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1"&gt;Hsien-Kai Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yi-Min Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hung-Jen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jou_K/0/1/0/all/0/1"&gt;Kevin Jou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FetReg: Placental Vessel Segmentation and Registration in Fetoscopy Challenge Dataset. (arXiv:2106.05923v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05923</id>
        <link href="http://arxiv.org/abs/2106.05923"/>
        <updated>2021-06-17T15:44:17.153Z</updated>
        <summary type="html"><![CDATA[Fetoscopy laser photocoagulation is a widely used procedure for the treatment
of Twin-to-Twin Transfusion Syndrome (TTTS), that occur in mono-chorionic
multiple pregnancies due to placental vascular anastomoses. This procedure is
particularly challenging due to limited field of view, poor manoeuvrability of
the fetoscope, poor visibility due to fluid turbidity, variability in light
source, and unusual position of the placenta. This may lead to increased
procedural time and incomplete ablation, resulting in persistent TTTS.
Computer-assisted intervention may help overcome these challenges by expanding
the fetoscopic field of view through video mosaicking and providing better
visualization of the vessel network. However, the research and development in
this domain remain limited due to unavailability of high-quality data to encode
the intra- and inter-procedure variability. Through the \textit{Fetoscopic
Placental Vessel Segmentation and Registration (FetReg)} challenge, we present
a large-scale multi-centre dataset for the development of generalized and
robust semantic segmentation and video mosaicking algorithms for the fetal
environment with a focus on creating drift-free mosaics from long duration
fetoscopy videos. In this paper, we provide an overview of the FetReg dataset,
challenge tasks, evaluation metrics and baseline methods for both segmentation
and registration. Baseline methods results on the FetReg dataset shows that our
dataset poses interesting challenges, offering large opportunity for the
creation of novel methods and models through a community effort initiative
guided by the FetReg challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1"&gt;Sophia Bano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casella_A/0/1/0/all/0/1"&gt;Alessandro Casella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1"&gt;Francisco Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moccia_S/0/1/0/all/0/1"&gt;Sara Moccia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Attilakos_G/0/1/0/all/0/1"&gt;George Attilakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wimalasundera_R/0/1/0/all/0/1"&gt;Ruwan Wimalasundera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1"&gt;Anna L. David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paladini_D/0/1/0/all/0/1"&gt;Dario Paladini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deprest_J/0/1/0/all/0/1"&gt;Jan Deprest&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momi_E/0/1/0/all/0/1"&gt;Elena De Momi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattos_L/0/1/0/all/0/1"&gt;Leonardo S. Mattos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separation Results between Fixed-Kernel and Feature-Learning Probability Metrics. (arXiv:2106.05739v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05739</id>
        <link href="http://arxiv.org/abs/2106.05739"/>
        <updated>2021-06-17T15:44:17.143Z</updated>
        <summary type="html"><![CDATA[Several works in implicit and explicit generative modeling empirically
observed that feature-learning discriminators outperform fixed-kernel
discriminators in terms of the sample quality of the models. We provide
separation results between probability metrics with fixed-kernel and
feature-learning discriminators using the function classes $\mathcal{F}_2$ and
$\mathcal{F}_1$ respectively, which were developed to study overparametrized
two-layer neural networks. In particular, we construct pairs of distributions
over hyper-spheres that can not be discriminated by fixed kernel
$(\mathcal{F}_2)$ integral probability metric (IPM) and Stein discrepancy (SD)
in high dimensions, but that can be discriminated by their feature learning
($\mathcal{F}_1$) counterparts. To further study the separation we provide
links between the $\mathcal{F}_1$ and $\mathcal{F}_2$ IPMs with sliced
Wasserstein distances. Our work suggests that fixed-kernel discriminators
perform worse than their feature learning counterparts because their
corresponding metrics are weaker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Domingo_Enrich_C/0/1/0/all/0/1"&gt;Carles Domingo-Enrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mroueh_Y/0/1/0/all/0/1"&gt;Youssef Mroueh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses. (arXiv:2106.05426v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05426</id>
        <link href="http://arxiv.org/abs/2106.05426"/>
        <updated>2021-06-17T15:44:17.131Z</updated>
        <summary type="html"><![CDATA[How related are the representations learned by neural language models,
translation models, and language tagging tasks? We answer this question by
adapting an encoder-decoder transfer learning method from computer vision to
investigate the structure among 100 different feature spaces extracted from
hidden representations of various networks trained on language tasks. This
method reveals a low-dimensional structure where language models and
translation models smoothly interpolate between word embeddings, syntactic and
semantic tasks, and future word embeddings. We call this low-dimensional
structure a language representation embedding because it encodes the
relationships between representations needed to process language for a variety
of NLP tasks. We find that this representation embedding can predict how well
each individual feature space maps to human brain responses to natural language
stimuli recorded using fMRI. Additionally, we find that the principal dimension
of this structure can be used to create a metric which highlights the brain's
natural language processing hierarchy. This suggests that the embedding
captures some part of the brain's natural language representation structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1"&gt;Richard Antonello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1"&gt;Javier Turek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1"&gt;Vy Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1"&gt;Alexander Huth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FetReg: Placental Vessel Segmentation and Registration in Fetoscopy Challenge Dataset. (arXiv:2106.05923v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05923</id>
        <link href="http://arxiv.org/abs/2106.05923"/>
        <updated>2021-06-17T15:44:16.763Z</updated>
        <summary type="html"><![CDATA[Fetoscopy laser photocoagulation is a widely used procedure for the treatment
of Twin-to-Twin Transfusion Syndrome (TTTS), that occur in mono-chorionic
multiple pregnancies due to placental vascular anastomoses. This procedure is
particularly challenging due to limited field of view, poor manoeuvrability of
the fetoscope, poor visibility due to fluid turbidity, variability in light
source, and unusual position of the placenta. This may lead to increased
procedural time and incomplete ablation, resulting in persistent TTTS.
Computer-assisted intervention may help overcome these challenges by expanding
the fetoscopic field of view through video mosaicking and providing better
visualization of the vessel network. However, the research and development in
this domain remain limited due to unavailability of high-quality data to encode
the intra- and inter-procedure variability. Through the \textit{Fetoscopic
Placental Vessel Segmentation and Registration (FetReg)} challenge, we present
a large-scale multi-centre dataset for the development of generalized and
robust semantic segmentation and video mosaicking algorithms for the fetal
environment with a focus on creating drift-free mosaics from long duration
fetoscopy videos. In this paper, we provide an overview of the FetReg dataset,
challenge tasks, evaluation metrics and baseline methods for both segmentation
and registration. Baseline methods results on the FetReg dataset shows that our
dataset poses interesting challenges, offering large opportunity for the
creation of novel methods and models through a community effort initiative
guided by the FetReg challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1"&gt;Sophia Bano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casella_A/0/1/0/all/0/1"&gt;Alessandro Casella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1"&gt;Francisco Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moccia_S/0/1/0/all/0/1"&gt;Sara Moccia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Attilakos_G/0/1/0/all/0/1"&gt;George Attilakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wimalasundera_R/0/1/0/all/0/1"&gt;Ruwan Wimalasundera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1"&gt;Anna L. David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paladini_D/0/1/0/all/0/1"&gt;Dario Paladini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deprest_J/0/1/0/all/0/1"&gt;Jan Deprest&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momi_E/0/1/0/all/0/1"&gt;Elena De Momi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattos_L/0/1/0/all/0/1"&gt;Leonardo S. Mattos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Space Search for Pareto-Efficient Spaces. (arXiv:2104.11014v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11014</id>
        <link href="http://arxiv.org/abs/2104.11014"/>
        <updated>2021-06-17T15:44:16.740Z</updated>
        <summary type="html"><![CDATA[Network spaces have been known as a critical factor in both handcrafted
network designs or defining search spaces for Neural Architecture Search (NAS).
However, an effective space involves tremendous prior knowledge and/or manual
effort, and additional constraints are required to discover efficiency-aware
architectures. In this paper, we define a new problem, Network Space Search
(NSS), as searching for favorable network spaces instead of a single
architecture. We propose an NSS method to directly search for efficient-aware
network spaces automatically, reducing the manual effort and immense cost in
discovering satisfactory ones. The resultant network spaces, named Elite
Spaces, are discovered from Expanded Search Space with minimal human expertise
imposed. The Pareto-efficient Elite Spaces are aligned with the Pareto front
under various complexity constraints and can be further served as NAS search
spaces, benefiting differentiable NAS approaches (e.g. In CIFAR-100, an
averagely 2.3% lower error rate and 3.7% closer to target constraint than the
baseline with around 90% fewer samples required to find satisfactory networks).
Moreover, our NSS approach is capable of searching for superior spaces in
future unexplored spaces, revealing great potential in searching for network
spaces automatically. Website:
https://minhungchen.netlify.app/publication/nss/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1"&gt;Min-Fong Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao-Yun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Min-Hung Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yu-Syuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1"&gt;Hsien-Kai Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yi-Min Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hung-Jen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jou_K/0/1/0/all/0/1"&gt;Kevin Jou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SVMA: A GAN-based model for Monocular 3D Human Pose Estimation. (arXiv:2106.05616v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05616</id>
        <link href="http://arxiv.org/abs/2106.05616"/>
        <updated>2021-06-17T15:44:16.717Z</updated>
        <summary type="html"><![CDATA[Recovering 3D human pose from 2D joints is a highly unconstrained problem,
especially without any video or multi-view information. We present an
unsupervised GAN-based model to recover 3D human pose from 2D joint locations
extracted from a single image. Our model uses a GAN to learn the mapping of
distribution from 2D poses to 3D poses, not the simple 2D-3D correspondence.
Considering the reprojection constraint, our model can estimate the camera so
that we can reproject the estimated 3D pose to the original 2D pose. Based on
this reprojection method, we can rotate and reproject the generated pose to get
our "new" 2D pose and then use a weight sharing generator to estimate the "new"
3D pose and a "new" camera. Through the above estimation process, we can define
the single-view-multi-angle consistency loss during training to simulate
multi-view consistency, which means the 3D poses and cameras estimated from two
angles of a single view should be able to be mixed to generate rich 2D
reprojections, and the 2D reprojections reprojected from the same 3D pose
should be consistent. The experimental results on Human3.6M show that our
method outperforms all the state-of-the-art methods, and results on
MPI-INF-3DHP show that our method outperforms state-of-the-art by approximately
15.0%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yicheng Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yongqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiahui Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses. (arXiv:2106.05426v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05426</id>
        <link href="http://arxiv.org/abs/2106.05426"/>
        <updated>2021-06-17T15:44:16.262Z</updated>
        <summary type="html"><![CDATA[How related are the representations learned by neural language models,
translation models, and language tagging tasks? We answer this question by
adapting an encoder-decoder transfer learning method from computer vision to
investigate the structure among 100 different feature spaces extracted from
hidden representations of various networks trained on language tasks. This
method reveals a low-dimensional structure where language models and
translation models smoothly interpolate between word embeddings, syntactic and
semantic tasks, and future word embeddings. We call this low-dimensional
structure a language representation embedding because it encodes the
relationships between representations needed to process language for a variety
of NLP tasks. We find that this representation embedding can predict how well
each individual feature space maps to human brain responses to natural language
stimuli recorded using fMRI. Additionally, we find that the principal dimension
of this structure can be used to create a metric which highlights the brain's
natural language processing hierarchy. This suggests that the embedding
captures some part of the brain's natural language representation structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1"&gt;Richard Antonello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1"&gt;Javier Turek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1"&gt;Vy Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1"&gt;Alexander Huth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Predictive Control with and without Terminal Weight: Stability and Algorithms. (arXiv:2011.14193v2 [eess.SY] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2011.14193</id>
        <link href="http://arxiv.org/abs/2011.14193"/>
        <updated>2021-06-17T01:58:46.948Z</updated>
        <summary type="html"><![CDATA[This paper presents stability analysis tools for model predictive control
(MPC) with and without terminal weight. Stability analysis of MPC with a
limited horizon but without terminal weight is a long-standing open problem. By
using a modified value function as an Lyapunov function candidate and the
principle of optimality, this paper establishes stability conditions for this
type of widely spread MPC algorithms. A new stability guaranteed MPC algorithm
without terminal weight (MPCS) is presented. With the help of designing a new
sublevel set defined by the value function of one-step ahead stage cost,
conditions for checking its recursive feasibility and stability of the proposed
MPC algorithm are presented. The new stability condition and the derived MPCS
overcome the difficulties arising in the existing terminal weight based MPC
framework, including the need of searching a suitable terminal weight and
possible poor performance caused by an inappropriate terminal weight. This work
is further extended to MPC with a terminal weight for the completeness.
Numerical examples are presented to demonstrate the effectiveness of the
proposed tool, whereas the existing stability analysis tools are either not
applicable or lead to quite conservative results. It shows that the proposed
tools offer a number of mechanisms to achieve stability: adjusting state and/or
control weights, extending the length of horizon, and adding a simple extra
constraint on the first or second state in the optimisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wen-Hua Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold-Aware Deep Clustering: Maximizing Angles between Embedding Vectors Based on Regular Simplex. (arXiv:2106.02331v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02331</id>
        <link href="http://arxiv.org/abs/2106.02331"/>
        <updated>2021-06-17T01:58:46.936Z</updated>
        <summary type="html"><![CDATA[This paper presents a new deep clustering (DC) method called manifold-aware
DC (M-DC) that can enhance hyperspace utilization more effectively than the
original DC. The original DC has a limitation in that a pair of two speakers
has to be embedded having an orthogonal relationship due to its use of the
one-hot vector-based loss function, while our method derives a unique loss
function aimed at maximizing the target angle in the hyperspace based on the
nature of a regular simplex. Our proposed loss imposes a higher penalty than
the original DC when the speaker is assigned incorrectly. The change from DC to
M-DC can be easily achieved by rewriting just one term in the loss function of
DC, without any other modifications to the network architecture or model
parameters. As such, our method has high practicability because it does not
affect the original inference part. The experimental results show that the
proposed method improves the performances of the original DC and its expansion
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tanaka_K/0/1/0/all/0/1"&gt;Keitaro Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sawata_R/0/1/0/all/0/1"&gt;Ryosuke Sawata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Takahashi_S/0/1/0/all/0/1"&gt;Shusuke Takahashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Less is More: A privacy-respecting Android malware classifier using Federated Learning. (arXiv:2007.08319v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08319</id>
        <link href="http://arxiv.org/abs/2007.08319"/>
        <updated>2021-06-17T01:58:46.932Z</updated>
        <summary type="html"><![CDATA[In this paper we present LiM ("Less is More"), a malware classification
framework that leverages Federated Learning to detect and classify malicious
apps in a privacy-respecting manner. Information about newly installed apps is
kept locally on users' devices, so that the provider cannot infer which apps
were installed by users. At the same time, input from all users is taken into
account in the federated learning process and they all benefit from better
classification performance. A key challenge of this setting is that users do
not have access to the ground truth (i.e. they cannot correctly identify
whether an app is malicious). To tackle this, LiM uses a safe semi-supervised
ensemble that maximizes classification accuracy with respect to a baseline
classifier trained by the service provider (i.e. the cloud). We implement LiM
and show that the cloud server has F1 score of 95%, while clients have perfect
recall with only 1 false positive in >100 apps, using a dataset of 25K clean
apps and 25K malicious apps, 200 users and 50 rounds of federation.
Furthermore, we conduct a security analysis and demonstrate that LiM is robust
against both poisoning attacks by adversaries who control half of the clients,
and inference attacks performed by an honest-but-curious cloud server. Further
experiments with MaMaDroid's dataset confirm resistance against poisoning
attacks and a performance improvement due to the federation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galvez_R/0/1/0/all/0/1"&gt;Rafa G&amp;#xe1;lvez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moonsamy_V/0/1/0/all/0/1"&gt;Veelasha Moonsamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_C/0/1/0/all/0/1"&gt;Claudia Diaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Topic Coverage Approach to Evaluation of Topic Models. (arXiv:2012.06274v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06274</id>
        <link href="http://arxiv.org/abs/2012.06274"/>
        <updated>2021-06-17T01:58:46.925Z</updated>
        <summary type="html"><![CDATA[Topic models are widely used unsupervised models of text capable of learning
topics - weighted lists of words and documents - from large collections of text
documents. When topic models are used for discovery of topics in text
collections, a question that arises naturally is how well the model-induced
topics correspond to topics of interest to the analyst. In this paper we
revisit and extend a so far neglected approach to topic model evaluation based
on measuring topic coverage - computationally matching model topics with a set
of reference topics that models are expected to uncover. The approach is well
suited for analyzing models' performance in topic discovery and for large-scale
analysis of both topic models and measures of model quality. We propose new
measures of coverage and evaluate, in a series of experiments, different types
of topic models on two distinct text domains for which interest for topic
discovery exists. The experiments include evaluation of model quality, analysis
of coverage of distinct topic categories, and the analysis of the relationship
between coverage and other methods of topic model evaluation. The contributions
of the paper include new measures of coverage, insights into both topic models
and other methods of model evaluation, and the datasets and code for
facilitating future research of both topic coverage and other approaches to
topic model evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korencic_D/0/1/0/all/0/1"&gt;Damir Koren&amp;#x10d;i&amp;#x107;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ristov_S/0/1/0/all/0/1"&gt;Strahil Ristov&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Repar_J/0/1/0/all/0/1"&gt;Jelena Repar&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1"&gt;Jan &amp;#x160;najder&lt;/a&gt; (2) ((1) Rudjer Bo&amp;#x161;kovi&amp;#x107; Institute, Croatia, (2) University of Zagreb, Faculty of Electrical Engineering and Computing, Croatia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deriving Autism Spectrum Disorder Functional Networks from RS-FMRI Data using Group ICA and Dictionary Learning. (arXiv:2106.09000v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.09000</id>
        <link href="http://arxiv.org/abs/2106.09000"/>
        <updated>2021-06-17T01:58:46.920Z</updated>
        <summary type="html"><![CDATA[The objective of this study is to derive functional networks for the autism
spectrum disorder (ASD) population using the group ICA and dictionary learning
model together and to classify ASD and typically developing (TD) participants
using the functional connectivity calculated from the derived functional
networks. In our experiments, the ASD functional networks were derived from
resting-state functional magnetic resonance imaging (rs-fMRI) data. We
downloaded a total of 120 training samples, including 58 ASD and 62 TD
participants, which were obtained from the public repository: Autism Brain
Imaging Data Exchange I (ABIDE I). Our methodology and results have five main
parts. First, we utilize a group ICA model to extract functional networks from
the ASD group and rank the top 20 regions of interest (ROIs). Second, we
utilize a dictionary learning model to extract functional networks from the ASD
group and rank the top 20 ROIs. Third, we merged the 40 selected ROIs from the
two models together as the ASD functional networks. Fourth, we generate three
corresponding masks based on the 20 selected ROIs from group ICA, the 20 ROIs
selected from dictionary learning, and the 40 combined ROIs selected from both.
Finally, we extract ROIs for all training samples using the above three masks,
and the calculated functional connectivity was used as features for ASD and TD
classification. The classification results showed that the functional networks
derived from ICA and dictionary learning together outperform those derived from
a single ICA model or a single dictionary learning model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ning Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wang_D/0/1/0/all/0/1"&gt;Donglin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outside the Echo Chamber: Optimizing the Performative Risk. (arXiv:2102.08570v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08570</id>
        <link href="http://arxiv.org/abs/2102.08570"/>
        <updated>2021-06-17T01:58:46.914Z</updated>
        <summary type="html"><![CDATA[In performative prediction, predictions guide decision-making and hence can
influence the distribution of future data. To date, work on performative
prediction has focused on finding performatively stable models, which are the
fixed points of repeated retraining. However, stable solutions can be far from
optimal when evaluated in terms of the performative risk, the loss experienced
by the decision maker when deploying a model. In this paper, we shift attention
beyond performative stability and focus on optimizing the performative risk
directly. We identify a natural set of properties of the loss function and
model-induced distribution shift under which the performative risk is convex, a
property which does not follow from convexity of the loss alone. Furthermore,
we develop algorithms that leverage our structural assumptions to optimize the
performative risk with better sample efficiency than generic methods for
derivative-free convex optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1"&gt;John Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perdomo_J/0/1/0/all/0/1"&gt;Juan C. Perdomo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zrnic_T/0/1/0/all/0/1"&gt;Tijana Zrnic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data. (arXiv:2010.03622v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03622</id>
        <link href="http://arxiv.org/abs/2010.03622"/>
        <updated>2021-06-17T01:58:46.909Z</updated>
        <summary type="html"><![CDATA[Self-training algorithms, which train a model to fit pseudolabels predicted
by another previously-learned model, have been very successful for learning
with unlabeled data using neural networks. However, the current theoretical
understanding of self-training only applies to linear models. This work
provides a unified theoretical analysis of self-training with deep networks for
semi-supervised learning, unsupervised domain adaptation, and unsupervised
learning. At the core of our analysis is a simple but realistic "expansion"
assumption, which states that a low probability subset of the data must expand
to a neighborhood with large probability relative to the subset. We also assume
that neighborhoods of examples in different classes have minimal overlap. We
prove that under these assumptions, the minimizers of population objectives
based on self-training and input-consistency regularization will achieve high
accuracy with respect to ground-truth labels. By using off-the-shelf
generalization bounds, we immediately convert this result to sample complexity
guarantees for neural nets that are polynomial in the margin and Lipschitzness.
Our results help explain the empirical successes of recently proposed
self-training algorithms which use input consistency regularization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Colin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1"&gt;Kendrick Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yining Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved CNN-based Learning of Interpolation Filters for Low-Complexity Inter Prediction in Video Coding. (arXiv:2106.08936v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08936</id>
        <link href="http://arxiv.org/abs/2106.08936"/>
        <updated>2021-06-17T01:58:46.903Z</updated>
        <summary type="html"><![CDATA[The versatility of recent machine learning approaches makes them ideal for
improvement of next generation video compression solutions. Unfortunately,
these approaches typically bring significant increases in computational
complexity and are difficult to interpret into explainable models, affecting
their potential for implementation within practical video coding applications.
This paper introduces a novel explainable neural network-based inter-prediction
scheme, to improve the interpolation of reference samples needed for fractional
precision motion compensation. The approach requires a single neural network to
be trained from which a full quarter-pixel interpolation filter set is derived,
as the network is easily interpretable due to its linear structure. A novel
training framework enables each network branch to resemble a specific
fractional shift. This practical solution makes it very efficient to use
alongside conventional video coding schemes. When implemented in the context of
the state-of-the-art Versatile Video Coding (VVC) test model, 0.77%, 1.27% and
2.25% BD-rate savings can be achieved on average for lower resolution sequences
under the random access, low-delay B and low-delay P configurations,
respectively, while the complexity of the learned interpolation schemes is
significantly reduced compared to the interpolation with full CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1"&gt;Luka Murn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blasi_S/0/1/0/all/0/1"&gt;Saverio Blasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smeaton_A/0/1/0/all/0/1"&gt;Alan F. Smeaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1"&gt;Marta Mrak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FGLP: A Federated Fine-Grained Location Prediction System for Mobile Users. (arXiv:2106.08946v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08946</id>
        <link href="http://arxiv.org/abs/2106.08946"/>
        <updated>2021-06-17T01:58:46.882Z</updated>
        <summary type="html"><![CDATA[Fine-grained location prediction on smart phones can be used to improve
app/system performance. Application scenarios include video quality adaptation
as a function of the 5G network quality at predicted user locations, and
augmented reality apps that speed up content rendering based on predicted user
locations. Such use cases require prediction error in the same range as the GPS
error, and no existing works on location prediction can achieve this level of
accuracy. We present a system for fine-grained location prediction (FGLP) of
mobile users, based on GPS traces collected on the phones. FGLP has two
components: a federated learning framework and a prediction model. The
framework runs on the phones of the users and also on a server that coordinates
learning from all users in the system. FGLP represents the user location data
as relative points in an abstract 2D space, which enables learning across
different physical spaces. The model merges Bidirectional Long Short-Term
Memory (BiLSTM) and Convolutional Neural Networks (CNN), where BiLSTM learns
the speed and direction of the mobile users, and CNN learns information such as
user movement preferences. FGLP uses federated learning to protect user privacy
and reduce bandwidth consumption. Our experimental results, using a dataset
with over 600,000 users, demonstrate that FGLP outperforms baseline models in
terms of prediction accuracy. We also demonstrate that FGLP works well in
conjunction with transfer learning, which enables model reusability. Finally,
benchmark results on several types of Android phones demonstrate FGLP's
feasibility in real life.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaopeng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobson_G/0/1/0/all/0/1"&gt;Guy Jacobson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jana_R/0/1/0/all/0/1"&gt;Rittwik Jana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wen-Ling Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talasila_M/0/1/0/all/0/1"&gt;Manoop Talasila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aftab_S/0/1/0/all/0/1"&gt;Syed Anwar Aftab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borcea_C/0/1/0/all/0/1"&gt;Cristian Borcea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RefBERT: Compressing BERT by Referencing to Pre-computed Representations. (arXiv:2106.08898v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08898</id>
        <link href="http://arxiv.org/abs/2106.08898"/>
        <updated>2021-06-17T01:58:46.875Z</updated>
        <summary type="html"><![CDATA[Recently developed large pre-trained language models, e.g., BERT, have
achieved remarkable performance in many downstream natural language processing
applications. These pre-trained language models often contain hundreds of
millions of parameters and suffer from high computation and latency in
real-world applications. It is desirable to reduce the computation overhead of
the models for fast training and inference while keeping the model performance
in downstream applications. Several lines of work utilize knowledge
distillation to compress the teacher model to a smaller student model. However,
they usually discard the teacher's knowledge when in inference. Differently, in
this paper, we propose RefBERT to leverage the knowledge learned from the
teacher, i.e., facilitating the pre-computed BERT representation on the
reference sample and compressing BERT into a smaller student model. To
guarantee our proposal, we provide theoretical justification on the loss
function and the usage of reference samples. Significantly, the theoretical
result shows that including the pre-computed teacher's representations on the
reference samples indeed increases the mutual information in learning the
student model. Finally, we conduct the empirical evaluation and show that our
RefBERT can beat the vanilla TinyBERT over 8.1\% and achieves more than 94\% of
the performance of $\BERTBASE$ on the GLUE benchmark. Meanwhile, RefBERT is
7.4x smaller and 9.5x faster on inference than BERT$_{\rm BASE}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haiqin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1"&gt;Yang Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jianping Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explicitly Encouraging Low Fractional Dimensional Trajectories Via Reinforcement Learning. (arXiv:2012.11662v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11662</id>
        <link href="http://arxiv.org/abs/2012.11662"/>
        <updated>2021-06-17T01:58:46.865Z</updated>
        <summary type="html"><![CDATA[A key limitation in using various modern methods of machine learning in
developing feedback control policies is the lack of appropriate methodologies
to analyze their long-term dynamics, in terms of making any sort of guarantees
(even statistically) about robustness. The central reasons for this are largely
due to the so-called curse of dimensionality, combined with the black-box
nature of the resulting control policies themselves. This paper aims at the
first of these issues. Although the full state space of a system may be quite
large in dimensionality, it is a common feature of most model-based control
methods that the resulting closed-loop systems demonstrate dominant dynamics
that are rapidly driven to some lower-dimensional sub-space within. In this
work we argue that the dimensionality of this subspace is captured by tools
from fractal geometry, namely various notions of a fractional dimension. We
then show that the dimensionality of trajectories induced by model free
reinforcement learning agents can be influenced adding a post processing
function to the agents reward signal. We verify that the dimensionality
reduction is robust to noise being added to the system and show that that the
modified agents are more actually more robust to noise and push disturbances in
general for the systems we examined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gillen_S/0/1/0/all/0/1"&gt;Sean Gillen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byl_K/0/1/0/all/0/1"&gt;Katie Byl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably End-to-end Label-Noise Learning without Anchor Points. (arXiv:2102.02400v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02400</id>
        <link href="http://arxiv.org/abs/2102.02400"/>
        <updated>2021-06-17T01:58:46.844Z</updated>
        <summary type="html"><![CDATA[In label-noise learning, the transition matrix plays a key role in building
statistically consistent classifiers. Existing consistent estimators for the
transition matrix have been developed by exploiting anchor points. However, the
anchor-point assumption is not always satisfied in real scenarios. In this
paper, we propose an end-to-end framework for solving label-noise learning
without anchor points, in which we simultaneously optimize two objectives: the
cross entropy loss between the noisy label and the predicted probability by the
neural network, and the volume of the simplex formed by the columns of the
transition matrix. Our proposed framework can identify the transition matrix if
the clean class-posterior probabilities are sufficiently scattered. This is by
far the mildest assumption under which the transition matrix is provably
identifiable and the learned classifier is statistically consistent.
Experimental results on benchmark datasets demonstrate the effectiveness and
robustness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuefeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning effective stochastic differential equations from microscopic simulations: combining stochastic numerics and deep learning. (arXiv:2106.09004v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2106.09004</id>
        <link href="http://arxiv.org/abs/2106.09004"/>
        <updated>2021-06-17T01:58:46.836Z</updated>
        <summary type="html"><![CDATA[We identify effective stochastic differential equations (SDE) for coarse
observables of fine-grained particle- or agent-based simulations; these SDE
then provide coarse surrogate models of the fine scale dynamics. We approximate
the drift and diffusivity functions in these effective SDE through neural
networks, which can be thought of as effective stochastic ResNets. The loss
function is inspired by, and embodies, the structure of established stochastic
numerical integrators (here, Euler-Maruyama and Milstein); our approximations
can thus benefit from error analysis of these underlying numerical schemes.
They also lend themselves naturally to "physics-informed" gray-box
identification when approximate coarse models, such as mean field equations,
are available. Our approach does not require long trajectories, works on
scattered snapshot data, and is designed to naturally handle different time
steps per snapshot. We consider both the case where the coarse collective
observables are known in advance, as well as the case where they must be found
in a data-driven manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Dietrich_F/0/1/0/all/0/1"&gt;Felix Dietrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Makeev_A/0/1/0/all/0/1"&gt;Alexei Makeev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kevrekidis_G/0/1/0/all/0/1"&gt;George Kevrekidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Evangelou_N/0/1/0/all/0/1"&gt;Nikolaos Evangelou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bertalan_T/0/1/0/all/0/1"&gt;Tom Bertalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Reich_S/0/1/0/all/0/1"&gt;Sebastian Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kevrekidis_I/0/1/0/all/0/1"&gt;Ioannis G. Kevrekidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonparametric Empirical Bayes Estimation and Testing for Sparse and Heteroscedastic Signals. (arXiv:2106.08881v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08881</id>
        <link href="http://arxiv.org/abs/2106.08881"/>
        <updated>2021-06-17T01:58:46.823Z</updated>
        <summary type="html"><![CDATA[Large-scale modern data often involves estimation and testing for
high-dimensional unknown parameters. It is desirable to identify the sparse
signals, ``the needles in the haystack'', with accuracy and false discovery
control. However, the unprecedented complexity and heterogeneity in modern data
structure require new machine learning tools to effectively exploit
commonalities and to robustly adjust for both sparsity and heterogeneity. In
addition, estimates for high-dimensional parameters often lack uncertainty
quantification. In this paper, we propose a novel Spike-and-Nonparametric
mixture prior (SNP) -- a spike to promote the sparsity and a nonparametric
structure to capture signals. In contrast to the state-of-the-art methods, the
proposed methods solve the estimation and testing problem at once with several
merits: 1) an accurate sparsity estimation; 2) point estimates with
shrinkage/soft-thresholding property; 3) credible intervals for uncertainty
quantification; 4) an optimal multiple testing procedure that controls false
discovery rate. Our method exhibits promising empirical performance on both
simulated data and a gene expression case study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Junhui Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritov_Y/0/1/0/all/0/1"&gt;Ya&amp;#x27;acov Ritov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Linda Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Attacks Against Deep Reinforcement Learning Policies. (arXiv:2106.08746v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08746</id>
        <link href="http://arxiv.org/abs/2106.08746"/>
        <updated>2021-06-17T01:58:46.812Z</updated>
        <summary type="html"><![CDATA[Recent work has discovered that deep reinforcement learning (DRL) policies
are vulnerable to adversarial examples. These attacks mislead the policy of DRL
agents by perturbing the state of the environment observed by agents. They are
feasible in principle but too slow to fool DRL policies in real time. We
propose a new attack to fool DRL policies that is both effective and efficient
enough to be mounted in real time. We utilize the Universal Adversarial
Perturbation (UAP) method to compute effective perturbations independent of the
individual inputs to which they are applied. Via an extensive evaluation using
Atari 2600 games, we show that our technique is effective, as it fully degrades
the performance of both deterministic and stochastic policies (up to 100%, even
when the $l_\infty$ bound on the perturbation is as small as 0.005). We also
show that our attack is efficient, incurring an online computational cost of
0.027ms on average. It is faster compared to the response time (0.6ms on
average) of agents with different DRL policies, and considerably faster than
prior attacks (2.7ms on average). Furthermore, we demonstrate that known
defenses are ineffective against universal perturbations. We propose an
effective detection technique which can form the basis for robust defenses
against attacks based on universal perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tekgul_B/0/1/0/all/0/1"&gt;Buse G.A. Tekgul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shelly Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marchal_S/0/1/0/all/0/1"&gt;Samuel Marchal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asokan_N/0/1/0/all/0/1"&gt;N. Asokan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Based Counterfactual Synthesizer for Interpretation. (arXiv:2106.08971v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08971</id>
        <link href="http://arxiv.org/abs/2106.08971"/>
        <updated>2021-06-17T01:58:46.798Z</updated>
        <summary type="html"><![CDATA[Counterfactuals, serving as one of the emerging type of model
interpretations, have recently received attention from both researchers and
practitioners. Counterfactual explanations formalize the exploration of
``what-if'' scenarios, and are an instance of example-based reasoning using a
set of hypothetical data samples. Counterfactuals essentially show how the
model decision alters with input perturbations. Existing methods for generating
counterfactuals are mainly algorithm-based, which are time-inefficient and
assume the same counterfactual universe for different queries. To address these
limitations, we propose a Model-based Counterfactual Synthesizer (MCS)
framework for interpreting machine learning models. We first analyze the
model-based counterfactual process and construct a base synthesizer using a
conditional generative adversarial net (CGAN). To better approximate the
counterfactual universe for those rare queries, we novelly employ the umbrella
sampling technique to conduct the MCS framework training. Besides, we also
enhance the MCS framework by incorporating the causal dependence among
attributes with model inductive bias, and validate its design correctness from
the causality identification perspective. Experimental results on several
datasets demonstrate the effectiveness as well as efficiency of our proposed
MCS framework, and verify the advantages compared with other alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alva_S/0/1/0/all/0/1"&gt;Sahan Suresh Alva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiahao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xia Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Off-Belief Learning. (arXiv:2103.04000v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04000</id>
        <link href="http://arxiv.org/abs/2103.04000"/>
        <updated>2021-06-17T01:58:46.782Z</updated>
        <summary type="html"><![CDATA[The standard problem setting in Dec-POMDPs is self-play, where the goal is to
find a set of policies that play optimally together. Policies learned through
self-play may adopt arbitrary conventions and implicitly rely on multi-step
reasoning based on fragile assumptions about other agents' actions and thus
fail when paired with humans or independently trained agents at test time. To
address this, we present off-belief learning (OBL). At each timestep OBL agents
follow a policy $\pi_1$ that is optimized assuming past actions were taken by a
given, fixed policy ($\pi_0$), but assuming that future actions will be taken
by $\pi_1$. When $\pi_0$ is uniform random, OBL converges to an optimal policy
that does not rely on inferences based on other agents' behavior (an optimal
grounded policy). OBL can be iterated in a hierarchy, where the optimal policy
from one level becomes the input to the next, thereby introducing multi-level
cognitive reasoning in a controlled manner. Unlike existing approaches, which
may converge to any equilibrium policy, OBL converges to a unique policy,
making it suitable for zero-shot coordination (ZSC). OBL can be scaled to
high-dimensional settings with a fictitious transition mechanism and shows
strong performance in both a toy-setting and the benchmark human-AI & ZSC
problem Hanabi.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hengyuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1"&gt;Adam Lerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Brandon Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;David Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1"&gt;Luis Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1"&gt;Noam Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1"&gt;Jakob Foerster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimized ensemble deep learning framework for scalable forecasting of dynamics containing extreme events. (arXiv:2106.08968v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08968</id>
        <link href="http://arxiv.org/abs/2106.08968"/>
        <updated>2021-06-17T01:58:46.776Z</updated>
        <summary type="html"><![CDATA[The remarkable flexibility and adaptability of both deep learning models and
ensemble methods have led to the proliferation for their application in
understanding many physical phenomena. Traditionally, these two techniques have
largely been treated as independent methodologies in practical applications.
This study develops an optimized ensemble deep learning (OEDL) framework
wherein these two machine learning techniques are jointly used to achieve
synergistic improvements in model accuracy, stability, scalability, and
reproducibility prompting a new wave of applications in the forecasting of
dynamics. Unpredictability is considered as one of the key features of chaotic
dynamics, so forecasting such dynamics of nonlinear systems is a relevant issue
in the scientific community. It becomes more challenging when the prediction of
extreme events is the focus issue for us. In this circumstance, the proposed
OEDL model based on a best convex combination of feed-forward neural networks,
reservoir computing, and long short-term memory can play a key role in
advancing predictions of dynamics consisting of extreme events. The combined
framework can generate the best out-of-sample performance than the individual
deep learners and standard ensemble framework for both numerically simulated
and real world data sets. We exhibit the outstanding performance of the OEDL
framework for forecasting extreme events generated from Lienard-type system,
prediction of COVID-19 cases in Brazil, dengue cases in San Juan, and sea
surface temperature in Nino 3.4 region.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1"&gt;Arnob Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanujit Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1"&gt;Dibakar Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Fair Policies in Decentralized Cooperative Multi-Agent Reinforcement Learning. (arXiv:2012.09421v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09421</id>
        <link href="http://arxiv.org/abs/2012.09421"/>
        <updated>2021-06-17T01:58:46.767Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning fair policies in (deep) cooperative
multi-agent reinforcement learning (MARL). We formalize it in a principled way
as the problem of optimizing a welfare function that explicitly encodes two
important aspects of fairness: efficiency and equity. As a solution method, we
propose a novel neural network architecture, which is composed of two
sub-networks specifically designed for taking into account the two aspects of
fairness. In experiments, we demonstrate the importance of the two sub-networks
for fair optimization. Our overall approach is general as it can accommodate
any (sub)differentiable welfare function. Therefore, it is compatible with
various notions of fairness that have been proposed in the literature (e.g.,
lexicographic maximin, generalized Gini social welfare function, proportional
fairness). Our solution method is generic and can be implemented in various
MARL settings: centralized training and decentralized execution, or fully
decentralized. Finally, we experimentally validate our approach in various
domains and show that it can perform much better than previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1"&gt;Matthieu Zimmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glanois_C/0/1/0/all/0/1"&gt;Claire Glanois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddique_U/0/1/0/all/0/1"&gt;Umer Siddique&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_P/0/1/0/all/0/1"&gt;Paul Weng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursive Construction of Stable Assemblies of Recurrent Neural Networks. (arXiv:2106.08928v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08928</id>
        <link href="http://arxiv.org/abs/2106.08928"/>
        <updated>2021-06-17T01:58:46.760Z</updated>
        <summary type="html"><![CDATA[Advanced applications of modern machine learning will likely involve
combinations of trained networks, as are already used in spectacular systems
such as DeepMind's AlphaGo. Recursively building such combinations in an
effective and stable fashion while also allowing for continual refinement of
the individual networks - as nature does for biological networks - will require
new analysis tools. This paper takes a step in this direction by establishing
contraction properties of broad classes of nonlinear recurrent networks and
neural ODEs, and showing how these quantified properties allow in turn to
recursively construct stable networks of networks in a systematic fashion. The
results can also be used to stably combine recurrent networks and physical
systems with quantified contraction properties. Similarly, they may be applied
to modular computational models of cognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ennis_M/0/1/0/all/0/1"&gt;Michaela Ennis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kozachkov_L/0/1/0/all/0/1"&gt;Leo Kozachkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slotine_J/0/1/0/all/0/1"&gt;Jean-Jacques Slotine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$C^3$: Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08914</id>
        <link href="http://arxiv.org/abs/2106.08914"/>
        <updated>2021-06-17T01:58:46.725Z</updated>
        <summary type="html"><![CDATA[Video-grounded dialogue systems aim to integrate video understanding and
dialogue understanding to generate responses that are relevant to both the
dialogue and video context. Most existing approaches employ deep learning
models and have achieved remarkable performance, given the relatively small
datasets available. However, the results are partly accomplished by exploiting
biases in the datasets rather than developing multimodal reasoning, resulting
in limited generalization. In this paper, we propose a novel approach of
Compositional Counterfactual Contrastive Learning ($C^3$) to develop
contrastive training between factual and counterfactual samples in
video-grounded dialogues. Specifically, we design factual/counterfactual
sampling based on the temporal steps in videos and tokens in dialogues and
propose contrastive loss functions that exploit object-level or action-level
variance. Different from prior approaches, we focus on contrastive hidden state
representations among compositional output tokens to optimize the
representation space in a generation setting. We achieved promising performance
gains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the
benefits of our approach in grounding video and dialogue context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hung Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nancy F. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1"&gt;Steven C.H. Hoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligent Tire-Based Slip Ratio Estimation Using Different Machine Learning Algorithms. (arXiv:2106.08961v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08961</id>
        <link href="http://arxiv.org/abs/2106.08961"/>
        <updated>2021-06-17T01:58:46.709Z</updated>
        <summary type="html"><![CDATA[Estimation of the longitudinal slip ratio of tires is important in boosting
the control performance of the vehicle under driving and braking conditions. In
this paper, the slip ratio is estimated using four machine learning algorithms
(Neural Network, Gradient Boosting Machine, Random Forest and Support Vector
Machine) based on the acceleration signals from the tri-axial MEMS
accelerometers utilized in the intelligent tire system. The experimental data
are collected through the MTS experimental platform. The corresponding
acceleration signals within the tire contact patch are extracted after
filtering to be used for the training the aforesaid machine learning
algorithms. A comparison is provided between the implemented ML algorithms
using a 10-fold CV. NRMS errors in the CV results indicate that NN has the
highest accuracy in comparison with other techniques. The NRSM errors of NN,
GBM, RF, and SVM are 2.59\%, 3.30\%, 4.21\%, and 5.34\%, respectively. Among
these techniques, GBM has a more stable results as it has the smallest output
variance. The present study with the fusion of intelligent tire system and
machine learning algorithms paves the way for the accurate estimation of tire
slip ratio, which is critical for the development of reliable vehicle control
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1"&gt;Nan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zepeng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianfeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Askari_H/0/1/0/all/0/1"&gt;Hassan Askari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Tikhonov: Faster Learning with Self-Concordant Losses via Iterative Regularization. (arXiv:2106.08855v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08855</id>
        <link href="http://arxiv.org/abs/2106.08855"/>
        <updated>2021-06-17T01:58:46.703Z</updated>
        <summary type="html"><![CDATA[The theory of spectral filtering is a remarkable tool to understand the
statistical properties of learning with kernels. For least squares, it allows
to derive various regularization schemes that yield faster convergence rates of
the excess risk than with Tikhonov regularization. This is typically achieved
by leveraging classical assumptions called source and capacity conditions,
which characterize the difficulty of the learning task. In order to understand
estimators derived from other loss functions, Marteau-Ferey et al. have
extended the theory of Tikhonov regularization to generalized self concordant
loss functions (GSC), which contain, e.g., the logistic loss. In this paper, we
go a step further and show that fast and optimal rates can be achieved for GSC
by using the iterated Tikhonov regularization scheme, which is intrinsically
related to the proximal point method in optimization, and overcomes the
limitation of the classical Tikhonov regularization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beugnot_G/0/1/0/all/0/1"&gt;Gaspard Beugnot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1"&gt;Julien Mairal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Economic Nowcasting with Long Short-Term Memory Artificial Neural Networks (LSTM). (arXiv:2106.08901v1 [econ.EM])]]></title>
        <id>http://arxiv.org/abs/2106.08901</id>
        <link href="http://arxiv.org/abs/2106.08901"/>
        <updated>2021-06-17T01:58:46.697Z</updated>
        <summary type="html"><![CDATA[Artificial neural networks (ANNs) have been the catalyst to numerous advances
in a variety of fields and disciplines in recent years. Their impact on
economics, however, has been comparatively muted. One type of ANN, the long
short-term memory network (LSTM), is particularly wellsuited to deal with
economic time-series. Here, the architecture's performance and characteristics
are evaluated in comparison with the dynamic factor model (DFM), currently a
popular choice in the field of economic nowcasting. LSTMs are found to produce
superior results to DFMs in the nowcasting of three separate variables; global
merchandise export values and volumes, and global services exports. Further
advantages include their ability to handle large numbers of input features in a
variety of time frequencies. A disadvantage is the inability to ascribe
contributions of input features to model outputs, common to all ANNs. In order
to facilitate continued applied research of the methodology by avoiding the
need for any knowledge of deep-learning libraries, an accompanying Python
library was developed using PyTorch, https://pypi.org/project/nowcast-lstm/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Hopp_D/0/1/0/all/0/1"&gt;Daniel Hopp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandit Modeling of Map Selection in Counter-Strike: Global Offensive. (arXiv:2106.08888v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08888</id>
        <link href="http://arxiv.org/abs/2106.08888"/>
        <updated>2021-06-17T01:58:46.673Z</updated>
        <summary type="html"><![CDATA[Many esports use a pick and ban process to define the parameters of a match
before it starts. In Counter-Strike: Global Offensive (CSGO) matches, two teams
first pick and ban maps, or virtual worlds, to play. Teams typically ban and
pick maps based on a variety of factors, such as banning maps which they do not
practice, or choosing maps based on the team's recent performance. We introduce
a contextual bandit framework to tackle the problem of map selection in CSGO
and to investigate teams' pick and ban decision-making. Using a data set of
over 3,500 CSGO matches and over 25,000 map selection decisions, we consider
different framings for the problem, different contexts, and different reward
metrics. We find that teams have suboptimal map choice policies with respect to
both picking and banning. We also define an approach for rewarding bans, which
has not been explored in the bandit setting, and find that incorporating ban
rewards improves model performance. Finally, we determine that usage of our
model could improve teams' predicted map win probability by up to 11% and raise
overall match win probabilities by 19.8% for evenly-matched teams.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petri_G/0/1/0/all/0/1"&gt;Guido Petri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanley_M/0/1/0/all/0/1"&gt;Michael H. Stanley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hon_A/0/1/0/all/0/1"&gt;Alec B. Hon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_A/0/1/0/all/0/1"&gt;Alexander Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xenopoulos_P/0/1/0/all/0/1"&gt;Peter Xenopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1"&gt;Cl&amp;#xe1;udio Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical and Private (Deep) Learning without Sampling or Shuffling. (arXiv:2103.00039v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00039</id>
        <link href="http://arxiv.org/abs/2103.00039"/>
        <updated>2021-06-17T01:58:46.610Z</updated>
        <summary type="html"><![CDATA[We consider training models with differential privacy (DP) using mini-batch
gradients. The existing state-of-the-art, Differentially Private Stochastic
Gradient Descent (DP-SGD), requires privacy amplification by sampling or
shuffling to obtain the best privacy/accuracy/computation trade-offs.
Unfortunately, the precise requirements on exact sampling and shuffling can be
hard to obtain in important practical scenarios, particularly federated
learning (FL). We design and analyze a DP variant of
Follow-The-Regularized-Leader (DP-FTRL) that compares favorably (both
theoretically and empirically) to amplified DP-SGD, while allowing for much
more flexible data access patterns. DP-FTRL does not use any form of privacy
amplification.

The code is available at
https://github.com/google-research/federated/tree/master/dp_ftrl and
https://github.com/google-research/DP-FTRL .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1"&gt;Peter Kairouz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McMahan_B/0/1/0/all/0/1"&gt;Brendan McMahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shuang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakkar_O/0/1/0/all/0/1"&gt;Om Thakkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakurta_A/0/1/0/all/0/1"&gt;Abhradeep Thakurta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zheng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch. (arXiv:2106.08970v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08970</id>
        <link href="http://arxiv.org/abs/2106.08970"/>
        <updated>2021-06-17T01:58:46.568Z</updated>
        <summary type="html"><![CDATA[As the curation of data for machine learning becomes increasingly automated,
dataset tampering is a mounting threat. Backdoor attackers tamper with training
data to embed a vulnerability in models that are trained on that data. This
vulnerability is then activated at inference time by placing a "trigger" into
the model's input. Typical backdoor attacks insert the trigger directly into
the training data, although the presence of such an attack may be visible upon
inspection. In contrast, the Hidden Trigger Backdoor Attack achieves poisoning
without placing a trigger into the training data at all. However, this hidden
trigger attack is ineffective at poisoning neural networks trained from
scratch. We develop a new hidden trigger attack, Sleeper Agent, which employs
gradient matching, data selection, and target model re-training during the
crafting process. Sleeper Agent is the first hidden trigger backdoor attack to
be effective against neural networks trained from scratch. We demonstrate its
effectiveness on ImageNet and in black-box settings. Our implementation code
can be found at https://github.com/hsouri/Sleeper-Agent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Souri_H/0/1/0/all/0/1"&gt;Hossein Souri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1"&gt;Liam Fowl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1"&gt;Rama Chellappa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super-k: A Piecewise Linear Classifier Based on Voronoi Tessellations. (arXiv:2012.15492v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15492</id>
        <link href="http://arxiv.org/abs/2012.15492"/>
        <updated>2021-06-17T01:58:46.497Z</updated>
        <summary type="html"><![CDATA[Voronoi tessellations are used to partition the Euclidean space into
polyhedral regions, which are called Voronoi cells. Labeling the Voronoi cells
with the class information, we can map any classification problem into a
Voronoi tessellation. In this way, the classification problem changes into a
query of just finding the enclosing Voronoi cell. In order to accomplish this
task, we have developed a new algorithm which generates a labeled Voronoi
tessellation that partitions the training data into polyhedral regions and
obtains interclass boundaries as an indirect result. It is called Supervised
k-Voxels or in short Super-k. We are introducing Super-k as a foundational new
algorithm and opening the possibility of a new family of algorithms. In this
paper, it is shown via comparisons on certain datasets that the Super-k
algorithm has the potential of providing comparable performance of the
well-known SVM family of algorithms with less complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zengin_R/0/1/0/all/0/1"&gt;Rahman Salim Zengin&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Sezer_V/0/1/0/all/0/1"&gt;Volkan Sezer&lt;/a&gt; (1) ((1) Istanbul Technical University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Federated Learning with Compensated Overlap-FedAvg. (arXiv:2012.06706v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06706</id>
        <link href="http://arxiv.org/abs/2012.06706"/>
        <updated>2021-06-17T01:58:46.455Z</updated>
        <summary type="html"><![CDATA[Petabytes of data are generated each day by emerging Internet of Things
(IoT), but only few of them can be finally collected and used for Machine
Learning (ML) purposes due to the apprehension of data & privacy leakage, which
seriously retarding ML's growth. To alleviate this problem, Federated learning
is proposed to perform model training by multiple clients' combined data
without the dataset sharing within the cluster. Nevertheless, federated
learning introduces massive communication overhead as the synchronized data in
each epoch is of the same size as the model, and thereby leading to a low
communication efficiency. Consequently, variant methods mainly focusing on the
communication rounds reduction and data compression are proposed to reduce the
communication overhead of federated learning. In this paper, we propose
Overlap-FedAvg, a framework that parallels the model training phase with model
uploading & downloading phase, so that the latter phase can be totally covered
by the former phase. Compared to vanilla FedAvg, Overlap-FedAvg is further
developed with a hierarchical computing strategy, a data compensation mechanism
and a nesterov accelerated gradients~(NAG) algorithm. Besides, Overlap-FedAvg
is orthogonal to many other compression methods so that they can be applied
together to maximize the utilization of the cluster. Furthermore, the
theoretical analysis is provided to prove the convergence of the proposed
Overlap-FedAvg framework. Extensive experiments on both conventional and
recurrent tasks with multiple models and datasets also demonstrate that the
proposed Overlap-FedAvg framework substantially boosts the federated learning
process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qing_Y/0/1/0/all/0/1"&gt;Ye Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1"&gt;Jiancheng Lv&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing. (arXiv:2011.09899v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09899</id>
        <link href="http://arxiv.org/abs/2011.09899"/>
        <updated>2021-06-17T01:58:46.236Z</updated>
        <summary type="html"><![CDATA[User data confidentiality protection is becoming a rising challenge in the
present deep learning research. Without access to data, conventional
data-driven model compression faces a higher risk of performance degradation.
Recently, some works propose to generate images from a specific pretrained
model to serve as training data. However, the inversion process only utilizes
biased feature statistics stored in one model and is from low-dimension to
high-dimension. As a consequence, it inevitably encounters the difficulties of
generalizability and inexact inversion, which leads to unsatisfactory
performance. To address these problems, we propose MixMix based on two simple
yet effective techniques: (1) Feature Mixing: utilizes various models to
construct a universal feature space for generalized inversion; (2) Data Mixing:
mixes the synthesized images and labels to generate exact label information. We
prove the effectiveness of MixMix from both theoretical and empirical
perspectives. Extensive experiments show that MixMix outperforms existing
methods on the mainstream compression tasks, including quantization, knowledge
distillation, and pruning. Specifically, MixMix achieves up to 4% and 20%
accuracy uplift on quantization and pruning, respectively, compared to existing
data-free compression work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Feng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Ruihao Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1"&gt;Mingzhu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fengwei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shaoqing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shi Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Dimensional Bayesian Optimisation with Variational Autoencoders and Deep Metric Learning. (arXiv:2106.03609v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03609</id>
        <link href="http://arxiv.org/abs/2106.03609"/>
        <updated>2021-06-17T01:58:46.235Z</updated>
        <summary type="html"><![CDATA[We introduce a method based on deep metric learning to perform Bayesian
optimisation over high-dimensional, structured input spaces using variational
autoencoders (VAEs). By extending ideas from supervised deep metric learning,
we address a longstanding problem in high-dimensional VAE Bayesian
optimisation, namely how to enforce a discriminative latent space as an
inductive bias. Importantly, we achieve such an inductive bias using just 1% of
the available labelled data relative to previous work, highlighting the sample
efficiency of our approach. As a theoretical contribution, we present a proof
of vanishing regret for our method. As an empirical contribution, we present
state-of-the-art results on real-world high-dimensional black-box optimisation
problems including property-guided molecule generation. It is the hope that the
results presented in this paper can act as a guiding principle for realising
effective high-dimensional Bayesian optimisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grosnit_A/0/1/0/all/0/1"&gt;Antoine Grosnit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tutunov_R/0/1/0/all/0/1"&gt;Rasul Tutunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maraval_A/0/1/0/all/0/1"&gt;Alexandre Max Maraval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1"&gt;Ryan-Rhys Griffiths&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cowen_Rivers_A/0/1/0/all/0/1"&gt;Alexander I. Cowen-Rivers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_W/0/1/0/all/0/1"&gt;Wenlong Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhitang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jan Peters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bou_Ammar_H/0/1/0/all/0/1"&gt;Haitham Bou-Ammar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated scoring of pre-REM sleep in mice with deep learning. (arXiv:2105.01933v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01933</id>
        <link href="http://arxiv.org/abs/2105.01933"/>
        <updated>2021-06-17T01:58:46.222Z</updated>
        <summary type="html"><![CDATA[Reliable automation of the labor-intensive manual task of scoring animal
sleep can facilitate the analysis of long-term sleep studies. In recent years,
deep-learning-based systems, which learn optimal features from the data,
increased scoring accuracies for the classical sleep stages of Wake, REM, and
Non-REM. Meanwhile, it has been recognized that the statistics of transitional
stages such as pre-REM, found between Non-REM and REM, may hold additional
insight into the physiology of sleep and are now under vivid investigation. We
propose a classification system based on a simple neural network architecture
that scores the classical stages as well as pre-REM sleep in mice. When
restricted to the classical stages, the optimized network showed
state-of-the-art classification performance with an out-of-sample F1 score of
0.95 in male C57BL/6J mice. When unrestricted, the network showed lower F1
scores on pre-REM (0.5) compared to the classical stages. The result is
comparable to previous attempts to score transitional stages in other species
such as transition sleep in rats or N1 sleep in humans. Nevertheless, we
observed that the sequence of predictions including pre-REM typically
transitioned from Non-REM to REM reflecting sleep dynamics observed by human
scorers. Our findings provide further evidence for the difficulty of scoring
transitional sleep stages, likely because such stages of sleep are
under-represented in typical data sets or show large inter-scorer variability.
We further provide our source code and an online platform to run predictions
with our trained network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Grieger_N/0/1/0/all/0/1"&gt;Niklas Grieger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Schwabedal_J/0/1/0/all/0/1"&gt;Justus T. C. Schwabedal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wendel_S/0/1/0/all/0/1"&gt;Stefanie Wendel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ritze_Y/0/1/0/all/0/1"&gt;Yvonne Ritze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bialonski_S/0/1/0/all/0/1"&gt;Stephan Bialonski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local plasticity rules can learn deep representations using self-supervised contrastive predictions. (arXiv:2010.08262v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08262</id>
        <link href="http://arxiv.org/abs/2010.08262"/>
        <updated>2021-06-17T01:58:46.215Z</updated>
        <summary type="html"><![CDATA[Learning in the brain is poorly understood and learning rules that respect
biological constraints, yet yield deep hierarchical representations, are still
unknown. Here, we propose a learning rule that takes inspiration from
neuroscience and recent advances in self-supervised deep learning. Learning
minimizes a simple layer-specific loss function and does not need to
back-propagate error signals within or between layers. Instead, weight updates
follow a local, Hebbian, learning rule that only depends on pre- and
post-synaptic neuronal activity, predictive dendritic input and widely
broadcasted modulation factors which are identical for large groups of neurons.
The learning rule applies contrastive predictive learning to a causal,
biological setting using saccades (i.e. rapid shifts in gaze direction). We
find that networks trained with this self-supervised and local rule build deep
hierarchical representations of images, speech and video.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Illing_B/0/1/0/all/0/1"&gt;Bernd Illing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventura_J/0/1/0/all/0/1"&gt;Jean Ventura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellec_G/0/1/0/all/0/1"&gt;Guillaume Bellec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1"&gt;Wulfram Gerstner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-memory stochastic backpropagation with multi-channel randomized trace estimation. (arXiv:2106.06998v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06998</id>
        <link href="http://arxiv.org/abs/2106.06998"/>
        <updated>2021-06-17T01:58:46.215Z</updated>
        <summary type="html"><![CDATA[Thanks to the combination of state-of-the-art accelerators and highly
optimized open software frameworks, there has been tremendous progress in the
performance of deep neural networks. While these developments have been
responsible for many breakthroughs, progress towards solving large-scale
problems, such as video encoding and semantic segmentation in 3D, is hampered
because access to on-premise memory is often limited. Instead of relying on
(optimal) checkpointing or invertibility of the network layers -- to recover
the activations during backpropagation -- we propose to approximate the
gradient of convolutional layers in neural networks with a multi-channel
randomized trace estimation technique. Compared to other methods, this approach
is simple, amenable to analyses, and leads to a greatly reduced memory
footprint. Even though the randomized trace estimation introduces stochasticity
during training, we argue that this is of little consequence as long as the
induced errors are of the same order as errors in the gradient due to the use
of stochastic gradient descent. We discuss the performance of networks trained
with stochastic backpropagation and how the error can be controlled while
maximizing memory usage and minimizing computational overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Louboutin_M/0/1/0/all/0/1"&gt;Mathias Louboutin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siahkoohi_A/0/1/0/all/0/1"&gt;Ali Siahkoohi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rongrong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herrmann_F/0/1/0/all/0/1"&gt;Felix J. Herrmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automated Website Classification by Deep Learning. (arXiv:1910.09991v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.09991</id>
        <link href="http://arxiv.org/abs/1910.09991"/>
        <updated>2021-06-17T01:58:46.203Z</updated>
        <summary type="html"><![CDATA[In recent years, the interest in Big Data sources has been steadily growing
within the Official Statistic community. The Italian National Institute of
Statistics (Istat) is currently carrying out several Big Data pilot studies.
One of these studies, the ICT Big Data pilot, aims at exploiting massive
amounts of textual data automatically scraped from the websites of Italian
enterprises in order to predict a set of target variables (e.g. e-commerce)
that are routinely observed by the traditional ICT Survey. In this paper, we
show that Deep Learning techniques can successfully address this problem.
Essentially, we tackle a text classification task: an algorithm must learn to
infer whether an Italian enterprise performs e-commerce from the textual
content of its website. To reach this goal, we developed a sophisticated
processing pipeline and evaluated its performance through extensive
experiments. Our pipeline uses Convolutional Neural Networks and relies on Word
Embeddings to encode raw texts into grayscale images (i.e. normalized numeric
matrices). Web-scraped texts are huge and have very low signal to noise ratio:
to overcome these issues, we adopted a framework known as False Positive
Reduction, which has seldom (if ever) been applied before to text
classification tasks. Several original contributions enable our processing
pipeline to reach good classification results. Empirical evidence shows that
our proposal outperforms all the alternative Machine Learning solutions already
tested in Istat for the same task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fausti_F/0/1/0/all/0/1"&gt;Fabrizio De Fausti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pugliese_F/0/1/0/all/0/1"&gt;Francesco Pugliese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zardetto_D/0/1/0/all/0/1"&gt;Diego Zardetto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Partial Response Network: a neural network nomogram. (arXiv:1908.05978v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.05978</id>
        <link href="http://arxiv.org/abs/1908.05978"/>
        <updated>2021-06-17T01:58:46.197Z</updated>
        <summary type="html"><![CDATA[Among interpretable machine learning methods, the class of Generalised
Additive Neural Networks (GANNs) is referred to as Self-Explaining Neural
Networks (SENN) because of the linear dependence on explicit functions of the
inputs. In binary classification this shows the precise weight that each input
contributes towards the logit. The nomogram is a graphical representation of
these weights. We show that functions of individual and pairs of variables can
be derived from a functional Analysis of Variance (ANOVA) representation,
enabling an efficient feature selection to be carried by application of the
logistic Lasso. This process infers the structure of GANNs which otherwise
needs to be predefined. As this method is particularly suited for tabular data,
it starts by fitting a generic flexible model, in this case a Multi-layer
Perceptron (MLP) to which the ANOVA decomposition is applied. This has the
further advantage that the resulting GANN can be replicated as a SENN, enabling
further refinement of the univariate and bivariate component functions to take
place. The component functions are partial responses hence the SENN is a
partial response network. The Partial Response Network (PRN) is equally as
transparent as a traditional logistic regression model, but capable of
non-linear classification with comparable or superior performance to the
original MLP. In other words, the PRN is a fully interpretable representation
of the MLP, at the level of univariate and bivariate effects. The performance
of the PRN is shown to be competitive for benchmark data, against
state-of-the-art machine learning methods including GBM, SVM and Random
Forests. It is also compared with spline-based Sparse Additive Models (SAM)
showing that a semi-parametric representation of the GAM as a neural network
can be as effective as the SAM though less constrained by the need to set
spline nodes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lisboa_P/0/1/0/all/0/1"&gt;Paulo J. G. Lisboa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortega_Martorell_S/0/1/0/all/0/1"&gt;Sandra Ortega-Martorell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cashman_S/0/1/0/all/0/1"&gt;Sadie Cashman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olier_I/0/1/0/all/0/1"&gt;Ivan Olier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automatic Actor-Critic Solutions to Continuous Control. (arXiv:2106.08918v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08918</id>
        <link href="http://arxiv.org/abs/2106.08918"/>
        <updated>2021-06-17T01:58:46.195Z</updated>
        <summary type="html"><![CDATA[Model-free off-policy actor-critic methods are an efficient solution to
complex continuous control tasks. However, these algorithms rely on a number of
design tricks and many hyperparameters, making their applications to new
domains difficult and computationally expensive. This paper creates an
evolutionary approach that automatically tunes these design decisions and
eliminates the RL-specific hyperparameters from the Soft Actor-Critic
algorithm. Our design is sample efficient and provides practical advantages
over baseline approaches, including improved exploration, generalization over
multiple control frequencies, and a robust ensemble of high-performance
policies. Empirically, we show that our agent outperforms well-tuned
hyperparameter settings in popular benchmarks from the DeepMind Control Suite.
We then apply it to new control tasks to find high-performance solutions with
minimal compute and research effort.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grigsby_J/0/1/0/all/0/1"&gt;Jake Grigsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1"&gt;Jin Yong Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yanjun Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Social Distance Estimation From Images: Performance Evaluation, Test Benchmark, and Algorithm. (arXiv:2103.06759v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06759</id>
        <link href="http://arxiv.org/abs/2103.06759"/>
        <updated>2021-06-17T01:58:46.187Z</updated>
        <summary type="html"><![CDATA[The COVID-19 virus has caused a global pandemic since March 2020. The World
Health Organization (WHO) has provided guidelines on how to reduce the spread
of the virus and one of the most important measures is social distancing.
Maintaining a minimum of one meter distance from other people is strongly
suggested to reduce the risk of infection. This has created a strong interest
in monitoring the social distances either as a safety measure or to study how
the measures have affected human behavior and country-wise differences in this.
The need for automatic social distance estimation algorithms is evident, but
there is no suitable test benchmark for such algorithms. Collecting images with
measured ground-truth pair-wise distances between all the people using
different camera settings is cumbersome. Furthermore, performance evaluation
for social distance estimation algorithms is not straightforward and there is
no widely accepted evaluation protocol. In this paper, we provide a dataset of
varying images with measured pair-wise social distances under different camera
positionings and focal length values. We suggest a performance evaluation
protocol and provide a benchmark to easily evaluate social distance estimation
algorithms. We also propose a method for automatic social distance estimation.
Our method takes advantage of object detection and human pose estimation. It
can be applied on any single image as long as focal length and sensor size
information are known. The results on our benchmark are encouraging with 92%
human detection rate and only 28.9% average error in distance estimation among
the detected people.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seker_M/0/1/0/all/0/1"&gt;Mert Seker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannisto_A/0/1/0/all/0/1"&gt;Anssi M&amp;#xe4;nnist&amp;#xf6;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1"&gt;Jenni Raitoharju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamically Grown Generative Adversarial Networks. (arXiv:2106.08505v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08505</id>
        <link href="http://arxiv.org/abs/2106.08505"/>
        <updated>2021-06-17T01:58:46.180Z</updated>
        <summary type="html"><![CDATA[Recent work introduced progressive network growing as a promising way to ease
the training for large GANs, but the model design and architecture-growing
strategy still remain under-explored and needs manual design for different
image data. In this paper, we propose a method to dynamically grow a GAN during
training, optimizing the network architecture and its parameters together with
automation. The method embeds architecture search techniques as an interleaving
step with gradient-based training to periodically seek the optimal
architecture-growing strategy for the generator and discriminator. It enjoys
the benefits of both eased training because of progressive growing and improved
performance because of broader architecture design space. Experimental results
demonstrate new state-of-the-art of image generation. Observations in the
search procedure also provide constructive insights into the GAN model design
such as generator-discriminator balance and convolutional layer choices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lanlan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuting Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jia Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random feature neural networks learn Black-Scholes type PDEs without curse of dimensionality. (arXiv:2106.08900v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08900</id>
        <link href="http://arxiv.org/abs/2106.08900"/>
        <updated>2021-06-17T01:58:46.167Z</updated>
        <summary type="html"><![CDATA[This article investigates the use of random feature neural networks for
learning Kolmogorov partial (integro-)differential equations associated to
Black-Scholes and more general exponential L\'evy models. Random feature neural
networks are single-hidden-layer feedforward neural networks in which only the
output weights are trainable. This makes training particularly simple, but (a
priori) reduces expressivity. Interestingly, this is not the case for
Black-Scholes type PDEs, as we show here. We derive bounds for the prediction
error of random neural networks for learning sufficiently non-degenerate
Black-Scholes type models. A full error analysis is provided and it is shown
that the derived bounds do not suffer from the curse of dimensionality. We also
investigate an application of these results to basket options and validate the
bounds numerically.

These results prove that neural networks are able to \textit{learn} solutions
to Black-Scholes type PDEs without the curse of dimensionality. In addition,
this provides an example of a relevant learning problem in which random feature
neural networks are provably efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gonon_L/0/1/0/all/0/1"&gt;Lukas Gonon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised GANs with Label Augmentation. (arXiv:2106.08601v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08601</id>
        <link href="http://arxiv.org/abs/2106.08601"/>
        <updated>2021-06-17T01:58:46.166Z</updated>
        <summary type="html"><![CDATA[Recently, transformation-based self-supervised learning has been applied to
generative adversarial networks (GANs) to mitigate the catastrophic forgetting
problem of discriminator by learning stable representations. However, the
separate self-supervised tasks in existing self-supervised GANs cause an
inconsistent goal with generative modeling due to the learning of the generator
from their generator distribution-agnostic classifiers. To address this issue,
we propose a novel self-supervised GANs framework with label augmentation,
i.e., augmenting the GAN labels (real or fake) with the self-supervised
pseudo-labels. In particular, the discriminator and the self-supervised
classifier are unified to learn a single task that predicts the augmented label
such that the discriminator/classifier is aware of the generator distribution,
while the generator tries to confuse the discriminator/classifier by optimizing
the discrepancy between the transformed real and generated distributions.
Theoretically, we prove that the generator, at the equilibrium point, converges
to replicate the data distribution. Empirically, we demonstrate that the
proposed method significantly outperforms competitive baselines on both
generative modeling and representation learning across benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1"&gt;Liang Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Huawei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1"&gt;Qi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drum-Aware Ensemble Architecture for Improved Joint Musical Beat and Downbeat Tracking. (arXiv:2106.08685v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08685</id>
        <link href="http://arxiv.org/abs/2106.08685"/>
        <updated>2021-06-17T01:58:46.160Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel system architecture that integrates blind source
separation with joint beat and downbeat tracking in musical audio signals. The
source separation module segregates the percussive and non-percussive
components of the input signal, over which beat and downbeat tracking are
performed separately and then the results are aggregated with a learnable
fusion mechanism. This way, the system can adaptively determine how much the
tracking result for an input signal should depend on the input's percussive or
non-percussive components. Evaluation on four testing sets that feature
different levels of presence of drum sounds shows that the new architecture
consistently outperforms the widely-adopted baseline architecture that does not
employ source separation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1"&gt;Ching-Yu Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_A/0/1/0/all/0/1"&gt;Alvin Wen-Yu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Quantum Property Prediction via Deeper 2D and 3D Graph Networks. (arXiv:2106.08551v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08551</id>
        <link href="http://arxiv.org/abs/2106.08551"/>
        <updated>2021-06-17T01:58:46.159Z</updated>
        <summary type="html"><![CDATA[Molecular property prediction is gaining increasing attention due to its
diverse applications. One task of particular interests and importance is to
predict quantum chemical properties without 3D equilibrium structures. This is
practically favorable since obtaining 3D equilibrium structures requires
extremely expensive calculations. In this work, we design a deep graph neural
network to predict quantum properties by directly learning from 2D molecular
graphs. In addition, we propose a 3D graph neural network to learn from
low-cost conformer sets, which can be obtained with open-source tools using an
affordable budget. We employ our methods to participate in the 2021 KDD Cup on
OGB Large-Scale Challenge (OGB-LSC), which aims to predict the HOMO-LUMO energy
gap of molecules. Final evaluation results reveal that we are one of the
winners with a mean absolute error of 0.1235 on the holdout test set. Our
implementation is available as part of the MoleculeX package
(https://github.com/divelab/MoleculeX).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Meng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Cong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Limei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yaochen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Hao Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Youzhi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shenglong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shuiwang Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Agnostic Federated Averaging. (arXiv:2104.02748v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02748</id>
        <link href="http://arxiv.org/abs/2104.02748"/>
        <updated>2021-06-17T01:58:46.159Z</updated>
        <summary type="html"><![CDATA[In distributed learning settings such as federated learning, the training
algorithm can be potentially biased towards different clients. Mohri et al.
(2019) proposed a domain-agnostic learning algorithm, where the model is
optimized for any target distribution formed by a mixture of the client
distributions in order to overcome this bias. They further proposed an
algorithm for the cross-silo federated learning setting, where the number of
clients is small. We consider this problem in the cross-device setting, where
the number of clients is much larger. We propose a communication-efficient
distributed algorithm called Agnostic Federated Averaging (or AgnosticFedAvg)
to minimize the domain-agnostic objective proposed in Mohri et al. (2019),
which is amenable to other private mechanisms such as secure aggregation. We
highlight two types of naturally occurring domains in federated learning and
argue that AgnosticFedAvg performs well on both. To demonstrate the practical
effectiveness of AgnosticFedAvg, we report positive results for large-scale
language modeling tasks in both simulation and live experiments, where the
latter involves training language models for Spanish virtual keyboard for
millions of user devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ro_J/0/1/0/all/0/1"&gt;Jae Ro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathews_R/0/1/0/all/0/1"&gt;Rajiv Mathews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1"&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ananda Theertha Suresh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Sample Complexity and Metastability of Heavy-tailed Policy Search in Continuous Control. (arXiv:2106.08414v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08414</id>
        <link href="http://arxiv.org/abs/2106.08414"/>
        <updated>2021-06-17T01:58:46.152Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning is a framework for interactive decision-making with
incentives sequentially revealed across time without a system dynamics model.
Due to its scaling to continuous spaces, we focus on policy search where one
iteratively improves a parameterized policy with stochastic policy gradient
(PG) updates. In tabular Markov Decision Problems (MDPs), under persistent
exploration and suitable parameterization, global optimality may be obtained.
By contrast, in continuous space, the non-convexity poses a pathological
challenge as evidenced by existing convergence results being mostly limited to
stationarity or arbitrary local extrema. To close this gap, we step towards
persistent exploration in continuous space through policy parameterizations
defined by distributions of heavier tails defined by tail-index parameter
alpha, which increases the likelihood of jumping in state space. Doing so
invalidates smoothness conditions of the score function common to PG. Thus, we
establish how the convergence rate to stationarity depends on the policy's tail
index alpha, a Holder continuity parameter, integrability conditions, and an
exploration tolerance parameter introduced here for the first time. Further, we
characterize the dependence of the set of local maxima on the tail index
through an exit and transition time analysis of a suitably defined Markov
chain, identifying that policies associated with Levy Processes of a heavier
tail converge to wider peaks. This phenomenon yields improved stability to
perturbations in supervised learning, which we corroborate also manifests in
improved performance of policy search, especially when myopic and farsighted
incentives are misaligned.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bedi_A/0/1/0/all/0/1"&gt;Amrit Singh Bedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parayil_A/0/1/0/all/0/1"&gt;Anjaly Parayil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koppel_A/0/1/0/all/0/1"&gt;Alec Koppel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Evaluating and Training Verifiably Robust Neural Networks. (arXiv:2104.00447v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00447</id>
        <link href="http://arxiv.org/abs/2104.00447"/>
        <updated>2021-06-17T01:58:46.146Z</updated>
        <summary type="html"><![CDATA[Recent works have shown that interval bound propagation (IBP) can be used to
train verifiably robust neural networks. Reseachers observe an intriguing
phenomenon on these IBP trained networks: CROWN, a bounding method based on
tight linear relaxation, often gives very loose bounds on these networks. We
also observe that most neurons become dead during the IBP training process,
which could hurt the representation capability of the network. In this paper,
we study the relationship between IBP and CROWN, and prove that CROWN is always
tighter than IBP when choosing appropriate bounding lines. We further propose a
relaxed version of CROWN, linear bound propagation (LBP), that can be used to
verify large networks to obtain lower verified errors than IBP. We also design
a new activation function, parameterized ramp function (ParamRamp), which has
more diversity of neuron status than ReLU. We conduct extensive experiments on
MNIST, CIFAR-10 and Tiny-ImageNet with ParamRamp activation and achieve
state-of-the-art verified robustness. Code and the appendix are available at
https://github.com/ZhaoyangLyu/VerifiablyRobustNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1"&gt;Zhaoyang Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Minghao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guodong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kehuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dahua Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining the Behavior of Black-Box Prediction Algorithms with Causal Learning. (arXiv:2006.02482v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02482</id>
        <link href="http://arxiv.org/abs/2006.02482"/>
        <updated>2021-06-17T01:58:46.133Z</updated>
        <summary type="html"><![CDATA[We propose to explain the behavior of black-box prediction methods (e.g.,
deep neural networks trained on image pixel data) using causal graphical
models. Specifically, we explore learning the structure of a causal graph where
the nodes represent prediction outcomes along with a set of macro-level
"interpretable" features, while allowing for arbitrary unmeasured confounding
among these variables. The resulting graph may indicate which of the
interpretable features, if any, are possible causes of the prediction outcome
and which may be merely associated with prediction outcomes due to confounding.
The approach is motivated by a counterfactual theory of causal explanation
wherein good explanations point to factors that are "difference-makers" in an
interventionist sense. The resulting analysis may be useful in algorithm
auditing and evaluation, by identifying features which make a causal difference
to the algorithm's output.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sani_N/0/1/0/all/0/1"&gt;Numair Sani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malinsky_D/0/1/0/all/0/1"&gt;Daniel Malinsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shpitser_I/0/1/0/all/0/1"&gt;Ilya Shpitser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thompson Sampling with Information Relaxation Penalties. (arXiv:1902.04251v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1902.04251</id>
        <link href="http://arxiv.org/abs/1902.04251"/>
        <updated>2021-06-17T01:58:46.116Z</updated>
        <summary type="html"><![CDATA[We consider a finite-horizon multi-armed bandit (MAB) problem in a Bayesian
setting, for which we propose an information relaxation sampling framework.
With this framework, we define an intuitive family of control policies that
include Thompson sampling (TS) and the Bayesian optimal policy as endpoints.
Analogous to TS, which, at each decision epoch pulls an arm that is best with
respect to the randomly sampled parameters, our algorithms sample entire future
reward realizations and take the corresponding best action. However, this is
done in the presence of "penalties" that seek to compensate for the
availability of future information.

We develop several novel policies and performance bounds for MAB problems
that vary in terms of improving performance and increasing computational
complexity between the two endpoints. Our policies can be viewed as natural
generalizations of TS that simultaneously incorporate knowledge of the time
horizon and explicitly consider the exploration-exploitation trade-off. We
prove associated structural results on performance bounds and suboptimality
gaps. Numerical experiments suggest that this new class of policies perform
well, in particular in settings where the finite time horizon introduces
significant exploration-exploitation tension into the problem. Finally,
inspired by the finite-horizon Gittins index, we propose an index policy that
builds on our framework that particularly outperforms the state-of-the-art
algorithms in our numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1"&gt;Seungki Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maglaras_C/0/1/0/all/0/1"&gt;Costis Maglaras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moallemi_C/0/1/0/all/0/1"&gt;Ciamac C. Moallemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense for the Price of Sparse: Improved Performance of Sparsely Initialized Networks via a Subspace Offset. (arXiv:2102.07655v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07655</id>
        <link href="http://arxiv.org/abs/2102.07655"/>
        <updated>2021-06-17T01:58:46.110Z</updated>
        <summary type="html"><![CDATA[That neural networks may be pruned to high sparsities and retain high
accuracy is well established. Recent research efforts focus on pruning
immediately after initialization so as to allow the computational savings
afforded by sparsity to extend to the training process. In this work, we
introduce a new `DCT plus Sparse' layer architecture, which maintains
information propagation and trainability even with as little as 0.01% trainable
kernel parameters remaining. We show that standard training of networks built
with these layers, and pruned at initialization, achieves state-of-the-art
accuracy for extreme sparsities on a variety of benchmark network architectures
and datasets. Moreover, these results are achieved using only simple heuristics
to determine the locations of the trainable parameters in the network, and thus
without having to initially store or compute with the full, unpruned network,
as is required by competing prune-at-initialization algorithms. Switching from
standard sparse layers to DCT plus Sparse layers does not increase the storage
footprint of a network and incurs only a small additional computational
overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Price_I/0/1/0/all/0/1"&gt;Ilan Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanner_J/0/1/0/all/0/1"&gt;Jared Tanner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-Layered Approach for Measuring the Simulation-to-Reality Gap of Radar Perception for Autonomous Driving. (arXiv:2106.08372v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.08372</id>
        <link href="http://arxiv.org/abs/2106.08372"/>
        <updated>2021-06-17T01:58:46.104Z</updated>
        <summary type="html"><![CDATA[With the increasing safety validation requirements for the release of a
self-driving car, alternative approaches, such as simulation-based testing, are
emerging in addition to conventional real-world testing. In order to rely on
virtual tests the employed sensor models have to be validated. For this reason,
it is necessary to quantify the discrepancy between simulation and reality in
order to determine whether a certain fidelity is sufficient for a desired
intended use. There exists no sound method to measure this
simulation-to-reality gap of radar perception for autonomous driving. We
address this problem by introducing a multi-layered evaluation approach, which
consists of a combination of an explicit and an implicit sensor model
evaluation. The former directly evaluates the realism of the synthetically
generated sensor data, while the latter refers to an evaluation of a downstream
target application. In order to demonstrate the method, we evaluated the
fidelity of three typical radar model types (ideal, data-driven, ray
tracing-based) and their applicability for virtually testing radar-based
multi-object tracking. We have shown the effectiveness of the proposed approach
in terms of providing an in-depth sensor model assessment that renders existing
disparities visible and enables a realistic estimation of the overall model
fidelity across different scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_A/0/1/0/all/0/1"&gt;Anthony Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1"&gt;Max Paul Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Resch_M/0/1/0/all/0/1"&gt;Michael Resch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Visibility Graph Neural Network and It's Application in Modulation Classification. (arXiv:2106.08564v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08564</id>
        <link href="http://arxiv.org/abs/2106.08564"/>
        <updated>2021-06-17T01:58:46.098Z</updated>
        <summary type="html"><![CDATA[Our digital world is full of time series and graphs which capture the various
aspects of many complex systems. Traditionally, there are respective methods in
processing these two different types of data, e.g., Recurrent Neural Network
(RNN) and Graph Neural Network (GNN), while in recent years, time series could
be mapped to graphs by using the techniques such as Visibility Graph (VG), so
that researchers can use graph algorithms to mine the knowledge in time series.
Such mapping methods establish a bridge between time series and graphs, and
have high potential to facilitate the analysis of various real-world time
series. However, the VG method and its variants are just based on fixed rules
and thus lack of flexibility, largely limiting their application in reality. In
this paper, we propose an Adaptive Visibility Graph (AVG) algorithm that can
adaptively map time series into graphs, based on which we further establish an
end-to-end classification framework AVGNet, by utilizing GNN model DiffPool as
the classifier. We then adopt AVGNet for radio signal modulation classification
which is an important task in the field of wireless communication. The
simulations validate that AVGNet outperforms a series of advanced deep learning
methods, achieving the state-of-the-art performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1"&gt;Qi Xuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1"&gt;Kunfeng Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinchao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhuangzhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dongwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shilian Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoniu Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Refining Language Models with Compositional Explanations. (arXiv:2103.10415v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10415</id>
        <link href="http://arxiv.org/abs/2103.10415"/>
        <updated>2021-06-17T01:58:46.079Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models have been successful on text classification
tasks, but are prone to learning spurious correlations from biased datasets,
and are thus vulnerable when making inferences in a new domain. Prior works
reveal such spurious patterns via post-hoc explanation algorithms which compute
the importance of input features. Further, the model is regularized to align
the importance scores with human knowledge, so that the unintended model
behaviors are eliminated. However, such a regularization technique lacks
flexibility and coverage, since only importance scores towards a pre-defined
list of features are adjusted, while more complex human knowledge such as
feature interaction and pattern generalization can hardly be incorporated. In
this work, we propose to refine a learned language model for a target domain by
collecting human-provided compositional explanations regarding observed biases.
By parsing these explanations into executable logic rules, the human-specified
refinement advice from a small set of explanations can be generalized to more
training examples. We additionally introduce a regularization term allowing
adjustments for both importance and interaction of features to better rectify
model behavior. We demonstrate the effectiveness of the proposed approach on
two text classification tasks by showing improved performance in target domain
as well as improved model fairness after refinement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Huihan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qinyuan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xisen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Wasserstein Minimax Framework for Mixed Linear Regression. (arXiv:2106.07537v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07537</id>
        <link href="http://arxiv.org/abs/2106.07537"/>
        <updated>2021-06-17T01:58:46.072Z</updated>
        <summary type="html"><![CDATA[Multi-modal distributions are commonly used to model clustered data in
statistical learning tasks. In this paper, we consider the Mixed Linear
Regression (MLR) problem. We propose an optimal transport-based framework for
MLR problems, Wasserstein Mixed Linear Regression (WMLR), which minimizes the
Wasserstein distance between the learned and target mixture regression models.
Through a model-based duality analysis, WMLR reduces the underlying MLR task to
a nonconvex-concave minimax optimization problem, which can be provably solved
to find a minimax stationary point by the Gradient Descent Ascent (GDA)
algorithm. In the special case of mixtures of two linear regression models, we
show that WMLR enjoys global convergence and generalization guarantees. We
prove that WMLR's sample complexity grows linearly with the dimension of data.
Finally, we discuss the application of WMLR to the federated learning task
where the training samples are collected by multiple agents in a network.
Unlike the Expectation Maximization algorithm, WMLR directly extends to the
distributed, federated learning setting. We support our theoretical results
through several numerical experiments, which highlight our framework's ability
to handle the federated learning setting with mixture models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Diamandis_T/0/1/0/all/0/1"&gt;Theo Diamandis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Eldar_Y/0/1/0/all/0/1"&gt;Yonina C. Eldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fallah_A/0/1/0/all/0/1"&gt;Alireza Fallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Farnia_F/0/1/0/all/0/1"&gt;Farzan Farnia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ozdaglar_A/0/1/0/all/0/1"&gt;Asuman Ozdaglar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models. (arXiv:2106.03969v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03969</id>
        <link href="http://arxiv.org/abs/2106.03969"/>
        <updated>2021-06-17T01:58:46.066Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning a tree-structured Ising model from data,
such that subsequent predictions computed using the model are accurate.
Concretely, we aim to learn a model such that posteriors $P(X_i|X_S)$ for small
sets of variables $S$ are accurate. Since its introduction more than 50 years
ago, the Chow-Liu algorithm, which efficiently computes the maximum likelihood
tree, has been the benchmark algorithm for learning tree-structured graphical
models. A bound on the sample complexity of the Chow-Liu algorithm with respect
to the prediction-centric local total variation loss was shown in [BK19]. While
those results demonstrated that it is possible to learn a useful model even
when recovering the true underlying graph is impossible, their bound depends on
the maximum strength of interactions and thus does not achieve the
information-theoretic optimum. In this paper, we introduce a new algorithm that
carefully combines elements of the Chow-Liu algorithm with tree metric
reconstruction methods to efficiently and optimally learn tree Ising models
under a prediction-centric loss. Our algorithm is robust to model
misspecification and adversarial corruptions. In contrast, we show that the
celebrated Chow-Liu algorithm can be arbitrarily suboptimal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boix_Adsera_E/0/1/0/all/0/1"&gt;Enric Boix-Adsera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bresler_G/0/1/0/all/0/1"&gt;Guy Bresler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1"&gt;Frederic Koehler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Momentum Pseudo-Labeling for Semi-Supervised Speech Recognition. (arXiv:2106.08922v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.08922</id>
        <link href="http://arxiv.org/abs/2106.08922"/>
        <updated>2021-06-17T01:58:46.060Z</updated>
        <summary type="html"><![CDATA[Pseudo-labeling (PL) has been shown to be effective in semi-supervised
automatic speech recognition (ASR), where a base model is self-trained with
pseudo-labels generated from unlabeled data. While PL can be further improved
by iteratively updating pseudo-labels as the model evolves, most of the
previous approaches involve inefficient retraining of the model or intricate
control of the label update. We present momentum pseudo-labeling (MPL), a
simple yet effective strategy for semi-supervised ASR. MPL consists of a pair
of online and offline models that interact and learn from each other, inspired
by the mean teacher method. The online model is trained to predict
pseudo-labels generated on the fly by the offline model. The offline model
maintains a momentum-based moving average of the online model. MPL is performed
in a single training process and the interaction between the two models
effectively helps them reinforce each other to improve the ASR performance. We
apply MPL to an end-to-end ASR model based on the connectionist temporal
classification. The experimental results demonstrate that MPL effectively
improves over the base model and is scalable to different semi-supervised
scenarios with varying amounts of data or domain mismatch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1"&gt;Yosuke Higuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moritz_N/0/1/0/all/0/1"&gt;Niko Moritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Roux_J/0/1/0/all/0/1"&gt;Jonathan Le Roux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hori_T/0/1/0/all/0/1"&gt;Takaaki Hori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imperfect ImaGANation: Implications of GANs Exacerbating Biases on Facial Data Augmentation and Snapchat Selfie Lenses. (arXiv:2001.09528v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09528</id>
        <link href="http://arxiv.org/abs/2001.09528"/>
        <updated>2021-06-17T01:58:46.054Z</updated>
        <summary type="html"><![CDATA[In this paper, we show that popular Generative Adversarial Networks (GANs)
exacerbate biases along the axes of gender and skin tone when given a skewed
distribution of face-shots. While practitioners celebrate synthetic data
generation using GANs as an economical way to augment data for training
data-hungry machine learning models, it is unclear whether they recognize the
perils of such techniques when applied to real world datasets biased along
latent dimensions. Specifically, we show that (1) traditional GANs further skew
the distribution of a dataset consisting of engineering faculty headshots,
generating minority modes less often and of worse quality and (2)
image-to-image translation (conditional) GANs also exacerbate biases by
lightening skin color of non-white faces and transforming female facial
features to be masculine when generating faces of engineering professors. Thus,
our study is meant to serve as a cautionary tale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1"&gt;Niharika Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olmo_A/0/1/0/all/0/1"&gt;Alberto Olmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1"&gt;Sailik Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manikonda_L/0/1/0/all/0/1"&gt;Lydia Manikonda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1"&gt;Subbarao Kambhampati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code to Comment Translation: A Comparative Study on Model Effectiveness & Errors. (arXiv:2106.08415v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.08415</id>
        <link href="http://arxiv.org/abs/2106.08415"/>
        <updated>2021-06-17T01:58:46.039Z</updated>
        <summary type="html"><![CDATA[Automated source code summarization is a popular software engineering
research topic wherein machine translation models are employed to "translate"
code snippets into relevant natural language descriptions. Most evaluations of
such models are conducted using automatic reference-based metrics. However,
given the relatively large semantic gap between programming languages and
natural language, we argue that this line of research would benefit from a
qualitative investigation into the various error modes of current
state-of-the-art models. Therefore, in this work, we perform both a
quantitative and qualitative comparison of three recently proposed source code
summarization models. In our quantitative evaluation, we compare the models
based on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics,
and in our qualitative evaluation, we perform a manual open-coding of the most
common errors committed by the models when compared to ground truth captions.
Our investigation reveals new insights into the relationship between
metric-based performance and model prediction errors grounded in an empirically
derived error taxonomy that can be used to drive future research efforts]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmud_J/0/1/0/all/0/1"&gt;Junayed Mahmud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1"&gt;Fahim Faisal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnob_R/0/1/0/all/0/1"&gt;Raihan Islam Arnob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1"&gt;Antonios Anastasopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moran_K/0/1/0/all/0/1"&gt;Kevin Moran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Contextual Bandits with Overparameterized Models. (arXiv:2006.15368v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15368</id>
        <link href="http://arxiv.org/abs/2006.15368"/>
        <updated>2021-06-17T01:58:46.033Z</updated>
        <summary type="html"><![CDATA[Recent results in supervised learning suggest that while overparameterized
models have the capacity to overfit, they in fact generalize quite well. We ask
whether the same phenomenon occurs for offline contextual bandits. Our results
are mixed. Value-based algorithms benefit from the same generalization behavior
as overparameterized supervised learning, but policy-based algorithms do not.
We show that this discrepancy is due to the \emph{action-stability} of their
objectives. An objective is action-stable if there exists a prediction
(action-value vector or action distribution) which is optimal no matter which
action is observed. While value-based objectives are action-stable,
policy-based objectives are unstable. We formally prove upper bounds on the
regret of overparameterized value-based learning and lower bounds on the regret
for policy-based algorithms. In our experiments with large neural networks,
this gap between action-stable value-based objectives and unstable policy-based
objectives leads to significant performance differences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandfonbrener_D/0/1/0/all/0/1"&gt;David Brandfonbrener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1"&gt;William F. Whitney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1"&gt;Joan Bruna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge Sparse Basis Network: A Deep Learning Framework for EEG Source Localization. (arXiv:2102.09188v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09188</id>
        <link href="http://arxiv.org/abs/2102.09188"/>
        <updated>2021-06-17T01:58:46.028Z</updated>
        <summary type="html"><![CDATA[EEG source localization is an important technical issue in EEG analysis.
Despite many numerical methods existed for EEG source localization, they all
rely on strong priors and the deep sources are intractable. Here we propose a
deep learning framework using spatial basis function decomposition for EEG
source localization. This framework combines the edge sparsity prior and
Gaussian source basis, called Edge Sparse Basis Network (ESBN). The performance
of ESBN is validated by both synthetic data and real EEG data during motor
tasks. The results suggest that the supervised ESBN outperforms the traditional
numerical methods in synthetic data and the unsupervised fine-tuning provides
more focal and accurate localizations in real data. Our proposed deep learning
framework can be extended to account for other source priors, and the real-time
property of ESBN can facilitate the applications of EEG in brain-computer
interfaces and clinics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Chen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_K/0/1/0/all/0/1"&gt;Kexin Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhengyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mingqi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mantini_D/0/1/0/all/0/1"&gt;Dante Mantini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quanying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Averaging on the Bures-Wasserstein manifold: dimension-free convergence of gradient descent. (arXiv:2106.08502v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.08502</id>
        <link href="http://arxiv.org/abs/2106.08502"/>
        <updated>2021-06-17T01:58:46.022Z</updated>
        <summary type="html"><![CDATA[We study first-order optimization algorithms for computing the barycenter of
Gaussian distributions with respect to the optimal transport metric. Although
the objective is geodesically non-convex, Riemannian GD empirically converges
rapidly, in fact faster than off-the-shelf methods such as Euclidean GD and SDP
solvers. This stands in stark contrast to the best-known theoretical results
for Riemannian GD, which depend exponentially on the dimension. In this work,
we prove new geodesic convexity results which provide stronger control of the
iterates, yielding a dimension-free convergence rate. Our techniques also
enable the analysis of two related notions of averaging, the
entropically-regularized barycenter and the geometric median, providing the
first convergence guarantees for Riemannian GD for these problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Altschuler_J/0/1/0/all/0/1"&gt;Jason M. Altschuler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chewi_S/0/1/0/all/0/1"&gt;Sinho Chewi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gerber_P/0/1/0/all/0/1"&gt;Patrik Gerber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Stromme_A/0/1/0/all/0/1"&gt;Austin J. Stromme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dataset-Level Geometric Framework for Ensemble Classifiers. (arXiv:2106.08658v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08658</id>
        <link href="http://arxiv.org/abs/2106.08658"/>
        <updated>2021-06-17T01:58:46.016Z</updated>
        <summary type="html"><![CDATA[Ensemble classifiers have been investigated by many in the artificial
intelligence and machine learning community. Majority voting and weighted
majority voting are two commonly used combination schemes in ensemble learning.
However, understanding of them is incomplete at best, with some properties even
misunderstood. In this paper, we present a group of properties of these two
schemes formally under a dataset-level geometric framework. Two key factors,
every component base classifier's performance and dissimilarity between each
pair of component classifiers are evaluated by the same metric - the Euclidean
distance. Consequently, ensembling becomes a deterministic problem and the
performance of an ensemble can be calculated directly by a formula. We prove
several theorems of interest and explain their implications for ensembles. In
particular, we compare and contrast the effect of the number of component
classifiers on these two types of ensemble schemes. Empirical investigation is
also conducted to verify the theoretical results when other metrics such as
accuracy are used. We believe that the results from this paper are very useful
for us to understand the fundamental properties of these two combination
schemes and the principles of ensemble classifiers in general. The results are
also helpful for us to investigate some issues in ensemble classifiers, such as
ensemble performance prediction, selecting a small number of base classifiers
to obtain efficient and effective ensembles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shengli Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Weimin Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The shape and simplicity biases of adversarially robust ImageNet-trained CNNs. (arXiv:2006.09373v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09373</id>
        <link href="http://arxiv.org/abs/2006.09373"/>
        <updated>2021-06-17T01:58:45.997Z</updated>
        <summary type="html"><![CDATA[Adversarial training has been the topic of dozens of studies and a leading
method for defending against adversarial attacks. Yet, it remains largely
unknown (a) how adversarially-robust ImageNet classifiers (R classifiers)
generalize to out-of-distribution examples; and (b) how their generalization
capability relates to their hidden representations. In this paper, we perform a
thorough, systematic study to answer these two questions across AlexNet,
GoogLeNet, and ResNet-50 architectures. We found that while standard ImageNet
classifiers have a strong texture bias, their R counterparts rely heavily on
shapes. Remarkably, adversarial training induces three simplicity biases into
hidden neurons in the process of 'robustifying' the network. That is, each
convolutional neuron in R networks often changes to detecting (1) pixel-wise
smoother patterns i.e. a mechanism that blocks high-frequency noise from
passing through the network; (2) more lower-level features i.e. textures and
colors (instead of objects); and (3) fewer types of inputs. Our findings reveal
the interesting mechanisms that made networks more adversarially robust and
also explain some recent findings. Our findings reveal the interesting
mechanisms that made networks more adversarially robust and also explain some
recent findings e.g. why R networks benefit from much larger capacity (Xie and
Yuille, 2020) and can act as a strong image prior in image synthesis (Santurkar
et al., 2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Peijie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1"&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hash Layers For Large Sparse Models. (arXiv:2106.04426v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04426</id>
        <link href="http://arxiv.org/abs/2106.04426"/>
        <updated>2021-06-17T01:58:45.991Z</updated>
        <summary type="html"><![CDATA[We investigate the training of sparse layers that use different parameters
for different inputs based on hashing in large Transformer models.
Specifically, we modify the feedforward layer to hash to different sets of
weights depending on the current token, over all tokens in the sequence. We
show that this procedure either outperforms or is competitive with
learning-to-route mixture-of-expert methods such as Switch Transformers and
BASE Layers, while requiring no routing parameters or extra terms in the
objective function such as a load balancing loss, and no sophisticated
assignment algorithm. We study the performance of different hashing techniques,
hash sizes and input features, and show that balanced and random hashes focused
on the most local features work best, compared to either learning clusters or
using longer-range context. We show our approach works well both on large
language modeling and dialogue tasks, and on downstream fine-tuning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Loss Landscape in Neural Architecture Search. (arXiv:2005.02960v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.02960</id>
        <link href="http://arxiv.org/abs/2005.02960"/>
        <updated>2021-06-17T01:58:45.984Z</updated>
        <summary type="html"><![CDATA[Neural architecture search (NAS) has seen a steep rise in interest over the
last few years. Many algorithms for NAS consist of searching through a space of
architectures by iteratively choosing an architecture, evaluating its
performance by training it, and using all prior evaluations to come up with the
next choice. The evaluation step is noisy - the final accuracy varies based on
the random initialization of the weights. Prior work has focused on devising
new search algorithms to handle this noise, rather than quantifying or
understanding the level of noise in architecture evaluations. In this work, we
show that (1) the simplest hill-climbing algorithm is a powerful baseline for
NAS, and (2), when the noise in popular NAS benchmark datasets is reduced to a
minimum, hill-climbing to outperforms many popular state-of-the-art algorithms.
We further back up this observation by showing that the number of local minima
is substantially reduced as the noise decreases, and by giving a theoretical
characterization of the performance of local search in NAS. Based on our
findings, for NAS research we suggest (1) using local search as a baseline, and
(2) denoising the training pipeline when possible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Colin White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nolen_S/0/1/0/all/0/1"&gt;Sam Nolen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savani_Y/0/1/0/all/0/1"&gt;Yash Savani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting crop yields with little ground truth: A simple statistical model for in-season forecasting. (arXiv:2106.08720v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08720</id>
        <link href="http://arxiv.org/abs/2106.08720"/>
        <updated>2021-06-17T01:58:45.976Z</updated>
        <summary type="html"><![CDATA[We present a fully automated model for in-season crop yield prediction,
designed to work where there is a dearth of sub-national "ground truth"
information. Our approach relies primarily on satellite data and is
characterized by careful feature engineering combined with a simple regression
model. As such, it can work almost anywhere in the world. Applying it to 10
different crop-country pairs (5 cereals -- corn, wheat, sorghum, barley and
millet, in 2 countries -- Ethiopia and Kenya), we achieve RMSEs of 5\%-10\% for
predictions 9 months into the year, and 7\%-14\% for predictions 3 months into
the year. The model outputs daily forecasts for the final yield of the current
year. It is trained using approximately 4 million data points for each
crop-country pair. These consist of: historical country-level annual yields,
crop calendars, crop cover, NDVI, temperature, rainfall, and
evapotransporation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Semret_N/0/1/0/all/0/1"&gt;Nemo Semret&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-Based Vulnerability Analysis of Cyber-Physical Systems. (arXiv:2103.06271v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06271</id>
        <link href="http://arxiv.org/abs/2103.06271"/>
        <updated>2021-06-17T01:58:45.969Z</updated>
        <summary type="html"><![CDATA[This work focuses on the use of deep learning for vulnerability analysis of
cyber-physical systems (CPS). Specifically, we consider a control architecture
widely used in CPS (e.g., robotics), where the low-level control is based on
e.g., the extended Kalman filter (EKF) and an anomaly detector. To facilitate
analyzing the impact potential sensing attacks could have, our objective is to
develop learning-enabled attack generators capable of designing stealthy
attacks that maximally degrade system operation. We show how such problem can
be cast within a learning-based grey-box framework where parts of the runtime
information are known to the attacker, and introduce two models based on
feed-forward neural networks (FNN); both models are trained offline, using a
cost function that combines the attack effects on the estimation error and the
residual signal used for anomaly detection, so that the trained models are
capable of recursively generating such effective sensor attacks in real-time.
The effectiveness of the proposed methods is illustrated on several case
studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khazraei_A/0/1/0/all/0/1"&gt;Amir Khazraei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hallyburton_S/0/1/0/all/0/1"&gt;Spencer Hallyburton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qitong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pajic_M/0/1/0/all/0/1"&gt;Miroslav Pajic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interval-censored Hawkes processes. (arXiv:2104.07932v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07932</id>
        <link href="http://arxiv.org/abs/2104.07932"/>
        <updated>2021-06-17T01:58:45.963Z</updated>
        <summary type="html"><![CDATA[This work builds a novel point process and tools to use the Hawkes process
with interval-censored data. Such data records the aggregated counts of events
solely during specific time intervals -- such as the number of patients
admitted to the hospital or the volume of vehicles passing traffic loop
detectors -- and not the exact occurrence time of the events. First, we
establish the Mean Behavior Poisson (MBP) process, a novel Poisson process with
a direct parameter correspondence to the popular self-exciting Hawkes process.
The event intensity function of the MBP is the expected intensity over all
possible Hawkes realizations with the same parameter set. We fit MBP in the
interval-censored setting using an interval-censored Poisson log-likelihood
(IC-LL). We use the parameter equivalence to uncover the parameters of the
associated Hawkes process. Second, we introduce two novel exogenous functions
to distinguish the exogenous from the endogenous events. We propose the
multi-impulse exogenous function when the exogenous events are observed as
event time and the latent homogeneous Poisson process exogenous function when
the exogenous events are presented as interval-censored volumes. Third, we
provide several approximation methods to estimate the intensity and compensator
function of MBP when no analytical solution exists. Fourth and finally, we
connect the interval-censored loss of MBP to a broader class of Bregman
divergence-based functions. Using the connection, we show that the current
state of the art in popularity estimation (Hawkes Intensity Process (HIP)
(Rizoiu et al.,2017b)) is a particular case of the MBP process. We verify our
models through empirical testing on synthetic data and real-world data. We find
that on real-world datasets that ourMBP process outperforms HIP for the task of
popularity prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1"&gt;Marian-Andrei Rizoiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soen_A/0/1/0/all/0/1"&gt;Alexander Soen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shidi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Leanne Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calderon_P/0/1/0/all/0/1"&gt;Pio Calderon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Krishna Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lexing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PettingZoo: Gym for Multi-Agent Reinforcement Learning. (arXiv:2009.14471v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.14471</id>
        <link href="http://arxiv.org/abs/2009.14471"/>
        <updated>2021-06-17T01:58:45.947Z</updated>
        <summary type="html"><![CDATA[This paper introduces the PettingZoo library and the accompanying Agent
Environment Cycle ("AEC") games model. PettingZoo is a library of diverse sets
of multi-agent environments with a universal, elegant Python API. PettingZoo
was developed with the goal of accelerating research in Multi-Agent
Reinforcement Learning ("MARL"), by making work more interchangeable,
accessible and reproducible akin to what OpenAI's Gym library did for
single-agent reinforcement learning. PettingZoo's API, while inheriting many
features of Gym, is unique amongst MARL APIs in that it's based around the
novel AEC games model. We argue, in part through case studies on major problems
in popular MARL environments, that the popular game models are poor conceptual
models of the games commonly used with MARL, that they promote severe bugs that
are hard to detect, and that the AEC games model addresses these problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Terry_J/0/1/0/all/0/1"&gt;J. K. Terry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_B/0/1/0/all/0/1"&gt;Benjamin Black&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grammel_N/0/1/0/all/0/1"&gt;Nathaniel Grammel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayakumar_M/0/1/0/all/0/1"&gt;Mario Jayakumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hari_A/0/1/0/all/0/1"&gt;Ananth Hari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sullivan_R/0/1/0/all/0/1"&gt;Ryan Sullivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1"&gt;Luis Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_R/0/1/0/all/0/1"&gt;Rodrigo Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horsch_C/0/1/0/all/0/1"&gt;Caroline Horsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dieffendahl_C/0/1/0/all/0/1"&gt;Clemens Dieffendahl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_N/0/1/0/all/0/1"&gt;Niall L. Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lokesh_Y/0/1/0/all/0/1"&gt;Yashas Lokesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravi_P/0/1/0/all/0/1"&gt;Praveen Ravi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Source Separation-based Data Augmentation for Improved Joint Beat and Downbeat Tracking. (arXiv:2106.08703v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08703</id>
        <link href="http://arxiv.org/abs/2106.08703"/>
        <updated>2021-06-17T01:58:45.941Z</updated>
        <summary type="html"><![CDATA[Due to advances in deep learning, the performance of automatic beat and
downbeat tracking in musical audio signals has seen great improvement in recent
years. In training such deep learning based models, data augmentation has been
found an important technique. However, existing data augmentation methods for
this task mainly target at balancing the distribution of the training data with
respect to their tempo. In this paper, we investigate another approach for data
augmentation, to account for the composition of the training data in terms of
the percussive and non-percussive sound sources. Specifically, we propose to
employ a blind drum separation model to segregate the drum and non-drum sounds
from each training audio signal, filtering out training signals that are
drumless, and then use the obtained drum and non-drum stems to augment the
training data. We report experiments on four completely unseen test sets,
validating the effectiveness of the proposed method, and accordingly the
importance of drum sound composition in the training data for beat and downbeat
tracking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1"&gt;Ching-Yu Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1"&gt;Joann Ching&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsiao_W/0/1/0/all/0/1"&gt;Wen-Yi Hsiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Hua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_A/0/1/0/all/0/1"&gt;Alvin Wen-Yu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Biomanufacturing Harvesting Decisions under Limited Historical Data. (arXiv:2101.03735v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.03735</id>
        <link href="http://arxiv.org/abs/2101.03735"/>
        <updated>2021-06-17T01:58:45.933Z</updated>
        <summary type="html"><![CDATA[In biopharmaceutical manufacturing, fermentation processes play a critical
role on productivity and profit. A fermentation process uses living cells with
complex biological mechanisms, and this leads to high variability in the
process outputs. By building on the biological mechanisms of protein and
impurity growth, we introduce a stochastic model to characterize the
accumulation of the protein and impurity levels in the fermentation process.
However, a common challenge in industry is the availability of only very
limited amount of data especially in the development and early stage of
production. This adds an additional layer of uncertainty, referred to as model
risk, due to the difficulty of estimating the model parameters with limited
data. In this paper, we study the harvesting decision for a fermentation
process under model risk. In particular, we adopt a Bayesian approach to update
the unknown parameters of the growth-rate distributions, and use the resulting
posterior distributions to characterize the impact of model risk on
fermentation output variability. The harvesting problem is formulated as a
Markov decision process model with knowledge states that summarize the
posterior distributions and hence incorporate the model risk in
decision-making. The resulting model is solved by using a reinforcement
learning algorithm based on Bayesian sparse sampling. We provide analytical
results on the structure of the optimal policy and its objective function, and
explicitly study the impact of model risk on harvesting decisions. Our case
studies at MSD Animal Health demonstrate that the proposed model and solution
approach improve the harvesting decisions in real life by achieving
substantially higher average output from a fermentation batch along with lower
batch-to-batch variability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Martagan_T/0/1/0/all/0/1"&gt;Tugce Martagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Akcay_A/0/1/0/all/0/1"&gt;Alp Akcay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ravenstein_B/0/1/0/all/0/1"&gt;Bram van Ravenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topology Distillation for Recommender System. (arXiv:2106.08700v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08700</id>
        <link href="http://arxiv.org/abs/2106.08700"/>
        <updated>2021-06-17T01:58:45.925Z</updated>
        <summary type="html"><![CDATA[Recommender Systems (RS) have employed knowledge distillation which is a
model compression technique training a compact student model with the knowledge
transferred from a pre-trained large teacher model. Recent work has shown that
transferring knowledge from the teacher's intermediate layer significantly
improves the recommendation quality of the student. However, they transfer the
knowledge of individual representation point-wise and thus have a limitation in
that primary information of RS lies in the relations in the representation
space. This paper proposes a new topology distillation approach that guides the
student by transferring the topological structure built upon the relations in
the teacher space. We first observe that simply making the student learn the
whole topological structure is not always effective and even degrades the
student's performance. We demonstrate that because the capacity of the student
is highly limited compared to that of the teacher, learning the whole
topological structure is daunting for the student. To address this issue, we
propose a novel method named Hierarchical Topology Distillation (HTD) which
distills the topology hierarchically to cope with the large capacity gap. Our
extensive experiments on real-world datasets show that the proposed method
significantly outperforms the state-of-the-art competitors. We also provide
in-depth analyses to ascertain the benefit of distilling the topology for RS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;SeongKu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Junyoung Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kweon_W/0/1/0/all/0/1"&gt;Wonbin Kweon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hwanjo Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling Self-Knowledge From Contrastive Links to Classify Graph Nodes Without Passing Messages. (arXiv:2106.08541v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08541</id>
        <link href="http://arxiv.org/abs/2106.08541"/>
        <updated>2021-06-17T01:58:45.914Z</updated>
        <summary type="html"><![CDATA[Nowadays, Graph Neural Networks (GNNs) following the Message Passing paradigm
become the dominant way to learn on graphic data. Models in this paradigm have
to spend extra space to look up adjacent nodes with adjacency matrices and
extra time to aggregate multiple messages from adjacent nodes. To address this
issue, we develop a method called LinkDist that distils self-knowledge from
connected node pairs into a Multi-Layer Perceptron (MLP) without the need to
aggregate messages. Experiment with 8 real-world datasets shows the MLP derived
from LinkDist can predict the label of a node without knowing its adjacencies
but achieve comparable accuracy against GNNs in the contexts of semi- and
full-supervised node classification. Moreover, LinkDist benefits from its
Non-Message Passing paradigm that we can also distil self-knowledge from
arbitrarily sampled node pairs in a contrastive way to further boost the
performance of LinkDist.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Aiguo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1"&gt;Ke Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1"&gt;Ling Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum-inspired event reconstruction with Tensor Networks: Matrix Product States. (arXiv:2106.08334v1 [hep-ph])]]></title>
        <id>http://arxiv.org/abs/2106.08334</id>
        <link href="http://arxiv.org/abs/2106.08334"/>
        <updated>2021-06-17T01:58:45.884Z</updated>
        <summary type="html"><![CDATA[Tensor Networks are non-trivial representations of high-dimensional tensors,
originally designed to describe quantum many-body systems. We show that Tensor
Networks are ideal vehicles to connect quantum mechanical concepts to machine
learning techniques, thereby facilitating an improved interpretability of
neural networks. This study presents the discrimination of top quark signal
over QCD background processes using a Matrix Product State classifier. We show
that entanglement entropy can be used to interpret what a network learns, which
can be used to reduce the complexity of the network and feature space without
loss of generality or performance. For the optimisation of the network, we
compare the Density Matrix Renormalization Group (DMRG) algorithm to stochastic
gradient descent (SGD) and propose a joined training algorithm to harness the
explainability of DMRG with the efficiency of SGD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Araz_J/0/1/0/all/0/1"&gt;Jack Y. Araz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Spannowsky_M/0/1/0/all/0/1"&gt;Michael Spannowsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polynomial Trajectory Predictions for Improved Learning Performance. (arXiv:2101.12616v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12616</id>
        <link href="http://arxiv.org/abs/2101.12616"/>
        <updated>2021-06-17T01:58:45.875Z</updated>
        <summary type="html"><![CDATA[The rising demand for Active Safety systems in automotive applications
stresses the need for a reliable short to mid-term trajectory prediction.
Anticipating the unfolding path of road users, one can act to increase the
overall safety. In this work, we propose to train artificial neural networks
for movement understanding by predicting trajectories in their natural form, as
a function of time. Predicting polynomial coefficients allows us to increased
accuracy and improve generalisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_I/0/1/0/all/0/1"&gt;Ido Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kummert_A/0/1/0/all/0/1"&gt;Anton Kummert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-sample Test using Projected Wasserstein Distance: Breaking the Curse of Dimensionality. (arXiv:2010.11970v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11970</id>
        <link href="http://arxiv.org/abs/2010.11970"/>
        <updated>2021-06-17T01:58:45.869Z</updated>
        <summary type="html"><![CDATA[We develop a projected Wasserstein distance for the two-sample test, a
fundamental problem in statistics and machine learning: given two sets of
samples, to determine whether they are from the same distribution. In
particular, we aim to circumvent the curse of dimensionality in Wasserstein
distance: when the dimension is high, it has diminishing testing power, which
is inherently due to the slow concentration property of Wasserstein metrics in
the high dimension space. A key contribution is to couple optimal projection to
find the low dimensional linear mapping to maximize the Wasserstein distance
between projected probability distributions. We characterize the theoretical
property of the finite-sample convergence rate on IPMs and present practical
algorithms for computing this metric. Numerical examples validate our
theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gao_R/0/1/0/all/0/1"&gt;Rui Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yao Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An unifying point of view on expressive power of GNNs. (arXiv:2106.08992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08992</id>
        <link href="http://arxiv.org/abs/2106.08992"/>
        <updated>2021-06-17T01:58:45.863Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are a wide class of connectionist models for
graph processing. They perform an iterative message passing operation on each
node and its neighbors, to solve classification/ clustering tasks --- on some
nodes or on the whole graph --- collecting all such messages, regardless of
their order. Despite the differences among the various models belonging to this
class, most of them adopt the same computation scheme, based on a local
aggregation mechanism and, intuitively, the local computation framework is
mainly responsible for the expressive power of GNNs. In this paper, we prove
that the Weisfeiler--Lehman test induces an equivalence relationship on the
graph nodes that exactly corresponds to the unfolding equivalence, defined on
the original GNN model. Therefore, the results on the expressive power of the
original GNNs can be extended to general GNNs which, under mild conditions, can
be proved capable of approximating, in probability and up to any precision, any
function on graphs that respects the unfolding equivalence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DInverno_G/0/1/0/all/0/1"&gt;Giuseppe Alessio D&amp;#x27;Inverno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bianchini_M/0/1/0/all/0/1"&gt;Monica Bianchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sampoli_M/0/1/0/all/0/1"&gt;Maria Lucia Sampoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1"&gt;Franco Scarselli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Schr\"odinger Bridge with Applications to Score-Based Generative Modeling. (arXiv:2106.01357v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01357</id>
        <link href="http://arxiv.org/abs/2106.01357"/>
        <updated>2021-06-17T01:58:45.856Z</updated>
        <summary type="html"><![CDATA[Progressively applying Gaussian noise transforms complex data distributions
to approximately Gaussian. Reversing this dynamic defines a generative model.
When the forward noising process is given by a Stochastic Differential Equation
(SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the
associated reverse-time SDE may be estimated using score-matching. A limitation
of this approach is that the forward-time SDE must be run for a sufficiently
long time for the final distribution to be approximately Gaussian. In contrast,
solving the Schr\"odinger Bridge problem (SB), i.e. an entropy-regularized
optimal transport problem on path spaces, yields diffusions which generate
samples from the data distribution in finite time. We present Diffusion SB
(DSB), an original approximation of the Iterative Proportional Fitting (IPF)
procedure to solve the SB problem, and provide theoretical analysis along with
generative modeling experiments. The first DSB iteration recovers the
methodology proposed by Song et al. (2021), with the flexibility of using
shorter time intervals, as subsequent DSB iterations reduce the discrepancy
between the final-time marginal of the forward (resp. backward) SDE with
respect to the prior (resp. data) distribution. Beyond generative modeling, DSB
offers a widely applicable computational optimal transport tool as the
continuous state-space analogue of the popular Sinkhorn algorithm (Cuturi,
2013).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bortoli_V/0/1/0/all/0/1"&gt;Valentin De Bortoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thornton_J/0/1/0/all/0/1"&gt;James Thornton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heng_J/0/1/0/all/0/1"&gt;Jeremy Heng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonequilibrium thermodynamics of self-supervised learning. (arXiv:2106.08981v1 [cond-mat.stat-mech])]]></title>
        <id>http://arxiv.org/abs/2106.08981</id>
        <link href="http://arxiv.org/abs/2106.08981"/>
        <updated>2021-06-17T01:58:45.847Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (SSL) of energy based models has an intuitive
relation to equilibrium thermodynamics because the softmax layer, mapping
energies to probabilities, is a Gibbs distribution. However, in what way SSL is
a thermodynamic process? We show that some SSL paradigms behave as a
thermodynamic composite system formed by representations and self-labels in
contact with a nonequilibrium reservoir. Moreover, this system is subjected to
usual thermodynamic cycles, such as adiabatic expansion and isochoric heating,
resulting in a generalized Gibbs ensemble (GGE). In this picture, we show that
learning is seen as a demon that operates in cycles using feedback measurements
to extract negative work from the system. As applications, we examine some SSL
algorithms using this idea.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Salazar_D/0/1/0/all/0/1"&gt;Domingos S. P. Salazar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis. (arXiv:2106.08352v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.08352</id>
        <link href="http://arxiv.org/abs/2106.08352"/>
        <updated>2021-06-17T01:58:45.827Z</updated>
        <summary type="html"><![CDATA[Text does not fully specify the spoken form, so text-to-speech models must be
able to learn from speech data that vary in ways not explained by the
corresponding text. One way to reduce the amount of unexplained variation in
training data is to provide acoustic information as an additional learning
signal. When generating speech, modifying this acoustic information enables
multiple distinct renditions of a text to be produced.

Since much of the unexplained variation is in the prosody, we propose a model
that generates speech explicitly conditioned on the three primary acoustic
correlates of prosody: $F_{0}$, energy and duration. The model is flexible
about how the values of these features are specified: they can be externally
provided, or predicted from text, or predicted then subsequently modified.

Compared to a model that employs a variational auto-encoder to learn
unsupervised latent features, our model provides more interpretable,
temporally-precise, and disentangled control. When automatically predicting the
acoustic features from text, it generates speech that is more natural than that
from a Tacotron 2 model with reference encoder. Subsequent human-in-the-loop
modification of the predicted acoustic features can significantly further
increase naturalness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mohan_D/0/1/0/all/0/1"&gt;Devang S Ram Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_V/0/1/0/all/0/1"&gt;Vivian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Teh_T/0/1/0/all/0/1"&gt;Tian Huey Teh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Torresquintero_A/0/1/0/all/0/1"&gt;Alexandra Torresquintero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wallis_C/0/1/0/all/0/1"&gt;Christopher G. R. Wallis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Staib_M/0/1/0/all/0/1"&gt;Marlene Staib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Foglianti_L/0/1/0/all/0/1"&gt;Lorenzo Foglianti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jiameng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+King_S/0/1/0/all/0/1"&gt;Simon King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Reinforcement Learning Under Minimax Regret for Green Security. (arXiv:2106.08413v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08413</id>
        <link href="http://arxiv.org/abs/2106.08413"/>
        <updated>2021-06-17T01:58:45.817Z</updated>
        <summary type="html"><![CDATA[Green security domains feature defenders who plan patrols in the face of
uncertainty about the adversarial behavior of poachers, illegal loggers, and
illegal fishers. Importantly, the deterrence effect of patrols on adversaries'
future behavior makes patrol planning a sequential decision-making problem.
Therefore, we focus on robust sequential patrol planning for green security
following the minimax regret criterion, which has not been considered in the
literature. We formulate the problem as a game between the defender and nature
who controls the parameter values of the adversarial behavior and design an
algorithm MIRROR to find a robust policy. MIRROR uses two reinforcement
learning-based oracles and solves a restricted game considering limited
defender strategies and parameter values. We evaluate MIRROR on real-world
poaching data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lily Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perrault_A/0/1/0/all/0/1"&gt;Andrew Perrault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1"&gt;Fei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haipeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1"&gt;Milind Tambe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Interpretable Spatio-temporal Logic Properties for Spatially Distributed Systems. (arXiv:2106.08548v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08548</id>
        <link href="http://arxiv.org/abs/2106.08548"/>
        <updated>2021-06-17T01:58:45.810Z</updated>
        <summary type="html"><![CDATA[The Internet-of-Things, complex sensor networks, multi-agent cyber-physical
systems are all examples of spatially distributed systems that continuously
evolve in time. Such systems generate huge amounts of spatio-temporal data, and
system designers are often interested in analyzing and discovering structure
within the data. There has been considerable interest in learning causal and
logical properties of temporal data using logics such as Signal Temporal Logic
(STL); however, there is limited work on discovering such relations on
spatio-temporal data. We propose the first set of algorithms for unsupervised
learning for spatio-temporal data. Our method does automatic feature extraction
from the spatio-temporal data by projecting it onto the parameter space of a
parametric spatio-temporal reach and escape logic (PSTREL). We propose an
agglomerative hierarchical clustering technique that guarantees that each
cluster satisfies a distinct STREL formula. We show that our method generates
STREL formulas of bounded description complexity using a novel decision-tree
approach which generalizes previous unsupervised learning techniques for Signal
Temporal Logic. We demonstrate the effectiveness of our approach on case
studies from diverse domains such as urban transportation, epidemiology, green
infrastructure, and air quality monitoring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadinejad_S/0/1/0/all/0/1"&gt;Sara Mohammadinejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1"&gt;Jyotirmy V. Deshmukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nenzi_L/0/1/0/all/0/1"&gt;Laura Nenzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selection of Source Images Heavily Influences the Effectiveness of Adversarial Attacks. (arXiv:2106.07141v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07141</id>
        <link href="http://arxiv.org/abs/2106.07141"/>
        <updated>2021-06-17T01:58:45.797Z</updated>
        <summary type="html"><![CDATA[Although the adoption rate of deep neural networks (DNNs) has tremendously
increased in recent years, a solution for their vulnerability against
adversarial examples has not yet been found. As a result, substantial research
efforts are dedicated to fix this weakness, with many studies typically using a
subset of source images to generate adversarial examples, treating every image
in this subset as equal. We demonstrate that, in fact, not every source image
is equally suited for this kind of assessment. To do so, we devise a
large-scale model-to-model transferability scenario for which we meticulously
analyze the properties of adversarial examples, generated from every suitable
source image in ImageNet by making use of two of the most frequently deployed
attacks. In this transferability scenario, which involves seven distinct DNN
models, including the recently proposed vision transformers, we reveal that it
is possible to have a difference of up to $12.5\%$ in model-to-model
transferability success, $1.01$ in average $L_2$ perturbation, and $0.03$
($8/225$) in average $L_{\infty}$ perturbation when $1,000$ source images are
sampled randomly among all suitable candidates. We then take one of the first
steps in evaluating the robustness of images used to create adversarial
examples, proposing a number of simple but effective methods to identify
unsuitable source images, thus making it possible to mitigate extreme cases in
experimentation and support high-quality benchmarking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozbulak_U/0/1/0/all/0/1"&gt;Utku Ozbulak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anzaku_E/0/1/0/all/0/1"&gt;Esla Timothy Anzaku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neve_W/0/1/0/all/0/1"&gt;Wesley De Neve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Messem_A/0/1/0/all/0/1"&gt;Arnout Van Messem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Better Understanding of Linear Models for Recommendation. (arXiv:2105.12937v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12937</id>
        <link href="http://arxiv.org/abs/2105.12937"/>
        <updated>2021-06-17T01:58:45.779Z</updated>
        <summary type="html"><![CDATA[Recently, linear regression models, such as EASE and SLIM, have shown to
often produce rather competitive results against more sophisticated deep
learning models. On the other side, the (weighted) matrix factorization
approaches have been popular choices for recommendation in the past and widely
adopted in the industry. In this work, we aim to theoretically understand the
relationship between these two approaches, which are the cornerstones of
model-based recommendations. Through the derivation and analysis of the
closed-form solutions for two basic regression and matrix factorization
approaches, we found these two approaches are indeed inherently related but
also diverge in how they "scale-down" the singular values of the original
user-item interaction matrix. This analysis also helps resolve the questions
related to the regularization parameter range and model complexities. We
further introduce a new learning algorithm in searching (hyper)parameters for
the closed-form solution and utilize it to discover the nearby models of the
existing solutions. The experimental results demonstrate that the basic models
and their closed-form solutions are indeed quite competitive against the
state-of-the-art models, thus, confirming the validity of studying the basic
models. The effectiveness of exploring the nearby models are also
experimentally validated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Ruoming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jing Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yang Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear Classifiers in Product Space Forms. (arXiv:2102.10204v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10204</id>
        <link href="http://arxiv.org/abs/2102.10204"/>
        <updated>2021-06-17T01:58:45.767Z</updated>
        <summary type="html"><![CDATA[Embedding methods for product spaces are powerful techniques for
low-distortion and low-dimensional representation of complex data structures.
Nevertheless, little is known regarding downstream learning and optimization
problems in such spaces. Here, we address the problem of linear classification
in a product space form -- a mix of Euclidean, spherical, and hyperbolic
spaces. First, we describe new formulations for linear classifiers on a
Riemannian manifold using geodesics and Riemannian metrics which generalize
straight lines and inner products in vector spaces, respectively. Second, we
prove that linear classifiers in $d$-dimensional space forms of any curvature
have the same expressive power, i.e., they can shatter exactly $d+1$ points.
Third, we formalize linear classifiers in product space forms, describe the
first corresponding perceptron and SVM classification algorithms, and establish
rigorous convergence results for the former. We support our theoretical
findings with simulation results on several datasets, including synthetic data,
CIFAR-100, MNIST, Omniglot, and single-cell RNA sequencing data. The results
show that learning methods applied to small-dimensional embeddings in product
space forms outperform their algorithmic counterparts in each space form.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1"&gt;Puoya Tabaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1"&gt;Eli Chien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1"&gt;Chao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jianhao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1"&gt;Olgica Milenkovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ideal formulations for constrained convex optimization problems with indicator variables. (arXiv:2007.00107v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00107</id>
        <link href="http://arxiv.org/abs/2007.00107"/>
        <updated>2021-06-17T01:58:45.759Z</updated>
        <summary type="html"><![CDATA[Motivated by modern regression applications, in this paper, we study the
convexification of a class of convex optimization problems with indicator
variables and combinatorial constraints on the indicators. Unlike most of the
previous work on convexification of sparse regression problems, we
simultaneously consider the nonlinear non-separable objective, indicator
variables, and combinatorial constraints. Specifically, we give the convex hull
description of the epigraph of the composition of a one-dimensional convex
function and an affine function under arbitrary combinatorial constraints. As
special cases of this result, we derive ideal convexifications for problems
with hierarchy, multi-collinearity, and sparsity constraints. Moreover, we also
give a short proof that for a separable objective function, the perspective
reformulation is ideal independent from the constraints of the problem. Our
computational experiments with regression problems under hierarchy constraints
on real datasets demonstrate the potential of the proposed approach in
improving the relaxation quality without significant computational overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Wei_L/0/1/0/all/0/1"&gt;Linchuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gomez_A/0/1/0/all/0/1"&gt;Andres Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kucukyavuz_S/0/1/0/all/0/1"&gt;Simge Kucukyavuz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based Support Estimation in Sublinear Time. (arXiv:2106.08396v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08396</id>
        <link href="http://arxiv.org/abs/2106.08396"/>
        <updated>2021-06-17T01:58:45.726Z</updated>
        <summary type="html"><![CDATA[We consider the problem of estimating the number of distinct elements in a
large data set (or, equivalently, the support size of the distribution induced
by the data set) from a random sample of its elements. The problem occurs in
many applications, including biology, genomics, computer systems and
linguistics. A line of research spanning the last decade resulted in algorithms
that estimate the support up to $ \pm \varepsilon n$ from a sample of size
$O(\log^2(1/\varepsilon) \cdot n/\log n)$, where $n$ is the data set size.
Unfortunately, this bound is known to be tight, limiting further improvements
to the complexity of this problem. In this paper we consider estimation
algorithms augmented with a machine-learning-based predictor that, given any
element, returns an estimation of its frequency. We show that if the predictor
is correct up to a constant approximation factor, then the sample complexity
can be reduced significantly, to \[ \ \log (1/\varepsilon) \cdot
n^{1-\Theta(1/\log(1/\varepsilon))}. \] We evaluate the proposed algorithms on
a collection of data sets, using the neural-network based estimators from {Hsu
et al, ICLR'19} as predictors. Our experiments demonstrate substantial (up to
3x) improvements in the estimation accuracy compared to the state of the art
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eden_T/0/1/0/all/0/1"&gt;Talya Eden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1"&gt;Piotr Indyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Shyam Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinfeld_R/0/1/0/all/0/1"&gt;Ronitt Rubinfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1"&gt;Sandeep Silwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1"&gt;Tal Wagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Localization, Convexity, and Star Aggregation. (arXiv:2105.08866v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08866</id>
        <link href="http://arxiv.org/abs/2105.08866"/>
        <updated>2021-06-17T01:58:45.720Z</updated>
        <summary type="html"><![CDATA[Offset Rademacher complexities have been shown to imply sharp, data-dependent
upper bounds for the square loss in a broad class of problems including
improper statistical learning and online learning. We show that in the
statistical setting, the offset complexity upper bound can be generalized to
any loss satisfying a certain uniform convexity condition. Amazingly, this
condition is shown to also capture exponential concavity and self-concordance,
uniting several apparently disparate results. By a unified geometric argument,
these bounds translate directly to improper learning in a non-convex class
using Audibert's "star algorithm." As applications, we recover the optimal
rates for proper and improper learning with the $p$-loss, $1 < p < \infty$ and
show that improper variants of empirical risk minimization can attain fast
rates for logistic regression and other generalized linear models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vijaykumar_S/0/1/0/all/0/1"&gt;Suhas Vijaykumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlation Clustering in Constant Many Parallel Rounds. (arXiv:2106.08448v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.08448</id>
        <link href="http://arxiv.org/abs/2106.08448"/>
        <updated>2021-06-17T01:58:45.700Z</updated>
        <summary type="html"><![CDATA[Correlation clustering is a central topic in unsupervised learning, with many
applications in ML and data mining. In correlation clustering, one receives as
input a signed graph and the goal is to partition it to minimize the number of
disagreements. In this work we propose a massively parallel computation (MPC)
algorithm for this problem that is considerably faster than prior work. In
particular, our algorithm uses machines with memory sublinear in the number of
nodes in the graph and returns a constant approximation while running only for
a constant number of rounds. To the best of our knowledge, our algorithm is the
first that can provably approximate a clustering problem on graphs using only a
constant number of MPC rounds in the sublinear memory regime. We complement our
analysis with an experimental analysis of our techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Addad_V/0/1/0/all/0/1"&gt;Vincent Cohen-Addad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lattanzi_S/0/1/0/all/0/1"&gt;Silvio Lattanzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitrovic_S/0/1/0/all/0/1"&gt;Slobodan Mitrovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norouzi_Fard_A/0/1/0/all/0/1"&gt;Ashkan Norouzi-Fard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parotsidis_N/0/1/0/all/0/1"&gt;Nikos Parotsidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarnawski_J/0/1/0/all/0/1"&gt;Jakub Tarnawski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Pixel Attack Deceives Computer-Assisted Diagnosis of Cancer. (arXiv:2012.00517v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00517</id>
        <link href="http://arxiv.org/abs/2012.00517"/>
        <updated>2021-06-17T01:58:45.694Z</updated>
        <summary type="html"><![CDATA[Computer vision and machine learning can be used to automate various tasks in
cancer diagnostic and detection. If an attacker can manipulate the automated
processing, the results can be devastating and in the worst case lead to wrong
diagnosis and treatment. In this research, the goal is to demonstrate the use
of one-pixel attacks in a real-life scenario with a real pathology dataset,
TUPAC16, which consists of digitized whole-slide images. We attack against the
IBM CODAIT's MAX breast cancer detector using adversarial images. These
adversarial examples are found using differential evolution to perform the
one-pixel modification to the images in the dataset. The results indicate that
a minor one-pixel modification of a whole slide image under analysis can affect
the diagnosis by reversing the automatic diagnosis result. The attack poses a
threat from the cyber security perspective: the one-pixel method can be used as
an attack vector by a motivated attacker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korpihalkola_J/0/1/0/all/0/1"&gt;Joni Korpihalkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sipola_T/0/1/0/all/0/1"&gt;Tuomo Sipola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puuska_S/0/1/0/all/0/1"&gt;Samir Puuska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokkonen_T/0/1/0/all/0/1"&gt;Tero Kokkonen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DMSANet: Dual Multi Scale Attention Network. (arXiv:2106.08382v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08382</id>
        <link href="http://arxiv.org/abs/2106.08382"/>
        <updated>2021-06-17T01:58:45.686Z</updated>
        <summary type="html"><![CDATA[Attention mechanism of late has been quite popular in the computer vision
community. A lot of work has been done to improve the performance of the
network, although almost always it results in increased computational
complexity. In this paper, we propose a new attention module that not only
achieves the best performance but also has lesser parameters compared to most
existing models. Our attention module can easily be integrated with other
convolutional neural networks because of its lightweight nature. The proposed
network named Dual Multi Scale Attention Network (DMSANet) is comprised of two
parts: the first part is used to extract features at various scales and
aggregate them, the second part uses spatial and channel attention modules in
parallel to adaptively integrate local features with their global dependencies.
We benchmark our network performance for Image Classification on ImageNet
dataset, Object Detection and Instance Segmentation both on MS COCO dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental Limits of Reinforcement Learning in Environment with Endogeneous and Exogeneous Uncertainty. (arXiv:2106.08477v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08477</id>
        <link href="http://arxiv.org/abs/2106.08477"/>
        <updated>2021-06-17T01:58:45.615Z</updated>
        <summary type="html"><![CDATA[Online reinforcement learning (RL) has been widely applied in information
processing scenarios, which usually exhibit much uncertainty due to the
intrinsic randomness of channels and service demands. In this paper, we
consider an un-discounted RL in general Markov decision processes (MDPs) with
both endogeneous and exogeneous uncertainty, where both the rewards and state
transition probability are unknown to the RL agent and evolve with the time as
long as their respective variations do not exceed certain dynamic budget (i.e.,
upper bound). We first develop a variation-aware Bernstein-based upper
confidence reinforcement learning (VB-UCRL), which we allow to restart
according to a schedule dependent on the variations. We successfully overcome
the challenges due to the exogeneous uncertainty and establish a regret bound
of saving at most $\sqrt{S}$ or $S^{\frac{1}{6}}T^{\frac{1}{12}}$ compared with
the latest results in the literature, where $S$ denotes the state size of the
MDP and $T$ indicates the iteration index of learning steps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rongpeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascading Modular Network (CAM-Net) for Multimodal Image Synthesis. (arXiv:2106.09015v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09015</id>
        <link href="http://arxiv.org/abs/2106.09015"/>
        <updated>2021-06-17T01:58:45.215Z</updated>
        <summary type="html"><![CDATA[Deep generative models such as GANs have driven impressive advances in
conditional image synthesis in recent years. A persistent challenge has been to
generate diverse versions of output images from the same input image, due to
the problem of mode collapse: because only one ground truth output image is
given per input image, only one mode of the conditional distribution is
modelled. In this paper, we focus on this problem of multimodal conditional
image synthesis and build on the recently proposed technique of Implicit
Maximum Likelihood Estimation (IMLE). Prior IMLE-based methods required
different architectures for different tasks, which limit their applicability,
and were lacking in fine details in the generated images. We propose CAM-Net, a
unified architecture that can be applied to a broad range of tasks.
Additionally, it is capable of generating convincing high frequency details,
achieving a reduction of the Frechet Inception Distance (FID) by up to 45.3%
compared to the baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shichong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moazeni_A/0/1/0/all/0/1"&gt;Alireza Moazeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpeakerStew: Scaling to Many Languages with a Triaged Multilingual Text-Dependent and Text-Independent Speaker Verification System. (arXiv:2104.02125v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02125</id>
        <link href="http://arxiv.org/abs/2104.02125"/>
        <updated>2021-06-17T01:58:45.188Z</updated>
        <summary type="html"><![CDATA[In this paper, we describe SpeakerStew - a hybrid system to perform speaker
verification on 46 languages. Two core ideas were explored in this system: (1)
Pooling training data of different languages together for multilingual
generalization and reducing development cycles; (2) A novel triage mechanism
between text-dependent and text-independent models to reduce runtime cost and
expected latency. To the best of our knowledge, this is the first study of
speaker verification systems at the scale of 46 languages. The problem is
framed from the perspective of using a smart speaker device with interactions
consisting of a wake-up keyword (text-dependent) followed by a speech query
(text-independent). Experimental evidence suggests that training on multiple
languages can generalize to unseen varieties while maintaining performance on
seen varieties. We also found that it can reduce computational requirements for
training models by an order of magnitude. Furthermore, during model inference
on English data, we observe that leveraging a triage framework can reduce the
number of calls to the more computationally expensive text-independent system
by 73% (and reduce latency by 59%) while maintaining an EER no worse than the
text-independent setup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chojnacka_R/0/1/0/all/0/1"&gt;Roza Chojnacka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pelecanos_J/0/1/0/all/0/1"&gt;Jason Pelecanos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Quan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1"&gt;Ignacio Lopez Moreno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Objective Evaluation of Post Hoc Explainers. (arXiv:2106.08376v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08376</id>
        <link href="http://arxiv.org/abs/2106.08376"/>
        <updated>2021-06-17T01:58:45.163Z</updated>
        <summary type="html"><![CDATA[Many applications of data-driven models demand transparency of decisions,
especially in health care, criminal justice, and other high-stakes
environments. Modern trends in machine learning research have led to algorithms
that are increasingly intricate to the degree that they are considered to be
black boxes. In an effort to reduce the opacity of decisions, methods have been
proposed to construe the inner workings of such models in a
human-comprehensible manner. These post hoc techniques are described as being
universal explainers - capable of faithfully augmenting decisions with
algorithmic insight. Unfortunately, there is little agreement about what
constitutes a "good" explanation. Moreover, current methods of explanation
evaluation are derived from either subjective or proxy means. In this work, we
propose a framework for the evaluation of post hoc explainers on ground truth
that is directly derived from the additive structure of a model. We demonstrate
the efficacy of the framework in understanding explainers by evaluating popular
explainers on thousands of synthetic and several real-world tasks. The
framework unveils that explanations may be accurate but misattribute the
importance of individual features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carmichael_Z/0/1/0/all/0/1"&gt;Zachariah Carmichael&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1"&gt;Walter J. Scheirer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Resolution Continuous Normalizing Flows. (arXiv:2106.08462v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08462</id>
        <link href="http://arxiv.org/abs/2106.08462"/>
        <updated>2021-06-17T01:58:45.157Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that Neural Ordinary Differential Equations (ODEs) can
serve as generative models of images using the perspective of Continuous
Normalizing Flows (CNFs). Such models offer exact likelihood calculation, and
invertible generation/density estimation. In this work we introduce a
Multi-Resolution variant of such models (MRCNF), by characterizing the
conditional distribution over the additional information required to generate a
fine image that is consistent with the coarse image. We introduce a
transformation between resolutions that allows for no change in the log
likelihood. We show that this approach yields comparable likelihood values for
various image datasets, with improved performance at higher resolutions, with
fewer parameters, using only 1 GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1"&gt;Vikram Voleti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finlay_C/0/1/0/all/0/1"&gt;Chris Finlay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1"&gt;Adam Oberman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Adversarial Robustness via Transductive Learning. (arXiv:2106.08387v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08387</id>
        <link href="http://arxiv.org/abs/2106.08387"/>
        <updated>2021-06-17T01:58:45.149Z</updated>
        <summary type="html"><![CDATA[There has been emerging interest to use transductive learning for adversarial
robustness (Goldwasser et al., NeurIPS 2020; Wu et al., ICML 2020). Compared to
traditional "test-time" defenses, these defense mechanisms "dynamically
retrain" the model based on test time input via transductive learning; and
theoretically, attacking these defenses boils down to bilevel optimization,
which seems to raise the difficulty for adaptive attacks. In this paper, we
first formalize and analyze modeling aspects of transductive robustness. Then,
we propose the principle of attacking model space for solving bilevel attack
objectives, and present an instantiation of the principle which breaks previous
transductive defenses. These attacks thus point to significant difficulties in
the use of transductive learning to improve adversarial robustness. To this
end, we present new theoretical and empirical evidence in support of the
utility of transductive learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiefeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lao_Q/0/1/0/all/0/1"&gt;Qicheng Lao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yingyu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best of both worlds: local and global explanations with human-understandable concepts. (arXiv:2106.08641v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08641</id>
        <link href="http://arxiv.org/abs/2106.08641"/>
        <updated>2021-06-17T01:58:45.134Z</updated>
        <summary type="html"><![CDATA[Interpretability techniques aim to provide the rationale behind a model's
decision, typically by explaining either an individual prediction (local
explanation, e.g. `why is this patient diagnosed with this condition') or a
class of predictions (global explanation, e.g. `why are patients diagnosed with
this condition in general'). While there are many methods focused on either
one, few frameworks can provide both local and global explanations in a
consistent manner. In this work, we combine two powerful existing techniques,
one local (Integrated Gradients, IG) and one global (Testing with Concept
Activation Vectors), to provide local, and global concept-based explanations.
We first validate our idea using two synthetic datasets with a known ground
truth, and further demonstrate with a benchmark natural image dataset. We test
our method with various concepts, target classes, model architectures and IG
baselines. We show that our method improves global explanations over TCAV when
compared to ground truth, and provides useful insights. We hope our work
provides a step towards building bridges between many existing local and global
methods to get the best of both worlds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schrouff_J/0/1/0/all/0/1"&gt;Jessica Schrouff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baur_S/0/1/0/all/0/1"&gt;Sebastien Baur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1"&gt;Shaobo Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mincu_D/0/1/0/all/0/1"&gt;Diana Mincu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loreaux_E/0/1/0/all/0/1"&gt;Eric Loreaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanes_R/0/1/0/all/0/1"&gt;Ralph Blanes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wexler_J/0/1/0/all/0/1"&gt;James Wexler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1"&gt;Alan Karthikesalingam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Been Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Affective XAI: Facial Affect Analysis for Understanding Explainable Human-AI Interactions. (arXiv:2106.08761v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08761</id>
        <link href="http://arxiv.org/abs/2106.08761"/>
        <updated>2021-06-17T01:58:45.117Z</updated>
        <summary type="html"><![CDATA[As machine learning approaches are increasingly used to augment human
decision-making, eXplainable Artificial Intelligence (XAI) research has
explored methods for communicating system behavior to humans. However, these
approaches often fail to account for the emotional responses of humans as they
interact with explanations. Facial affect analysis, which examines human facial
expressions of emotions, is one promising lens for understanding how users
engage with explanations. Therefore, in this work, we aim to (1) identify which
facial affect features are pronounced when people interact with XAI interfaces,
and (2) develop a multitask feature embedding for linking facial affect signals
with participants' use of explanations. Our analyses and results show that the
occurrence and values of facial AU1 and AU4, and Arousal are heightened when
participants fail to use explanations effectively. This suggests that facial
affect analysis should be incorporated into XAI to personalize explanations to
individuals' interaction styles and to adapt explanations based on the
difficulty of the task performed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guerdan_L/0/1/0/all/0/1"&gt;Luke Guerdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raymond_A/0/1/0/all/0/1"&gt;Alex Raymond&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunes_H/0/1/0/all/0/1"&gt;Hatice Gunes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Pixel Attack Deceives Computer-Assisted Diagnosis of Cancer. (arXiv:2012.00517v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00517</id>
        <link href="http://arxiv.org/abs/2012.00517"/>
        <updated>2021-06-17T01:58:45.110Z</updated>
        <summary type="html"><![CDATA[Computer vision and machine learning can be used to automate various tasks in
cancer diagnostic and detection. If an attacker can manipulate the automated
processing, the results can be devastating and in the worst case lead to wrong
diagnosis and treatment. In this research, the goal is to demonstrate the use
of one-pixel attacks in a real-life scenario with a real pathology dataset,
TUPAC16, which consists of digitized whole-slide images. We attack against the
IBM CODAIT's MAX breast cancer detector using adversarial images. These
adversarial examples are found using differential evolution to perform the
one-pixel modification to the images in the dataset. The results indicate that
a minor one-pixel modification of a whole slide image under analysis can affect
the diagnosis by reversing the automatic diagnosis result. The attack poses a
threat from the cyber security perspective: the one-pixel method can be used as
an attack vector by a motivated attacker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korpihalkola_J/0/1/0/all/0/1"&gt;Joni Korpihalkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sipola_T/0/1/0/all/0/1"&gt;Tuomo Sipola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puuska_S/0/1/0/all/0/1"&gt;Samir Puuska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokkonen_T/0/1/0/all/0/1"&gt;Tero Kokkonen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nystr\"om Method, and Use of Kernels in Machine Learning: Tutorial and Survey. (arXiv:2106.08443v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08443</id>
        <link href="http://arxiv.org/abs/2106.08443"/>
        <updated>2021-06-17T01:58:45.097Z</updated>
        <summary type="html"><![CDATA[This is a tutorial and survey paper on kernels, kernel methods, and related
fields. We start with reviewing the history of kernels in functional analysis
and machine learning. Then, Mercer kernel, Hilbert and Banach spaces,
Reproducing Kernel Hilbert Space (RKHS), Mercer's theorem and its proof,
frequently used kernels, kernel construction from distance metric, important
classes of kernels (including bounded, integrally positive definite, universal,
stationary, and characteristic kernels), kernel centering and normalization,
and eigenfunctions are explained in detail. Then, we introduce types of use of
kernels in machine learning including kernel methods (such as kernel support
vector machines), kernel learning by semi-definite programming, Hilbert-Schmidt
independence criterion, maximum mean discrepancy, kernel mean embedding, and
kernel dimensionality reduction. We also cover rank and factorization of kernel
matrix as well as the approximation of eigenfunctions and kernels using the
Nystr{\"o}m method. This paper can be useful for various fields of science
including machine learning, dimensionality reduction, functional analysis in
mathematics, and mathematical physics in quantum mechanics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation. (arXiv:2106.09017v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09017</id>
        <link href="http://arxiv.org/abs/2106.09017"/>
        <updated>2021-06-17T01:58:45.091Z</updated>
        <summary type="html"><![CDATA[Multi-task learning (MTL) aims to improve the generalization of several
related tasks by learning them jointly. As a comparison, in addition to the
joint training scheme, modern meta-learning allows unseen tasks with limited
labels during the test phase, in the hope of fast adaptation over them. Despite
the subtle difference between MTL and meta-learning in the problem formulation,
both learning paradigms share the same insight that the shared structure
between existing training tasks could lead to better generalization and
adaptation. In this paper, we take one important step further to understand the
close connection between these two learning paradigms, through both theoretical
analysis and empirical investigation. Theoretically, we first demonstrate that
MTL shares the same optimization formulation with a class of gradient-based
meta-learning (GBML) algorithms. We then prove that for over-parameterized
neural networks with sufficient depth, the learned predictive functions of MTL
and GBML are close. In particular, this result implies that the predictions
given by these two models are similar over the same unseen task. Empirically,
we corroborate our theoretical findings by showing that, with proper
implementation, MTL is competitive against state-of-the-art GBML algorithms on
a set of few-shot image classification benchmarks. Since existing GBML
algorithms often involve costly second-order bi-level optimization, our
first-order MTL method is an order of magnitude faster on large-scale datasets
such as mini-ImageNet. We believe this work could help bridge the gap between
these two learning paradigms, and provide a computationally efficient
alternative to GBML that also supports fast task adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Finite-Horizon Approximation and Efficient Optimal Algorithms for Stochastic Shortest Path. (arXiv:2106.08377v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08377</id>
        <link href="http://arxiv.org/abs/2106.08377"/>
        <updated>2021-06-17T01:58:45.086Z</updated>
        <summary type="html"><![CDATA[We introduce a generic template for developing regret minimization algorithms
in the Stochastic Shortest Path (SSP) model, which achieves minimax optimal
regret as long as certain properties are ensured. The key of our analysis is a
new technique called implicit finite-horizon approximation, which approximates
the SSP model by a finite-horizon counterpart only in the analysis without
explicit implementation. Using this template, we develop two new algorithms:
the first one is model-free (the first in the literature to our knowledge) and
minimax optimal under strictly positive costs; the second one is model-based
and minimax optimal even with zero-cost state-action pairs, matching the best
existing result from [Tarbouriech et al., 2021b]. Importantly, both algorithms
admit highly sparse updates, making them computationally more efficient than
all existing algorithms. Moreover, both can be made completely parameter-free.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jafarnia_Jahromi_M/0/1/0/all/0/1"&gt;Mehdi Jafarnia-Jahromi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1"&gt;Rahul Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Optimisation of Bellman Residual Errors with Neural Function Approximation. (arXiv:2106.08774v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08774</id>
        <link href="http://arxiv.org/abs/2106.08774"/>
        <updated>2021-06-17T01:58:45.080Z</updated>
        <summary type="html"><![CDATA[Recent development of Deep Reinforcement Learning has demonstrated superior
performance of neural networks in solving challenging problems with large or
even continuous state spaces. One specific approach is to deploy neural
networks to approximate value functions by minimising the Mean Squared Bellman
Error function. Despite great successes of Deep Reinforcement Learning,
development of reliable and efficient numerical algorithms to minimise the
Bellman Error is still of great scientific interest and practical demand. Such
a challenge is partially due to the underlying optimisation problem being
highly non-convex or using incorrect gradient information as done in
Semi-Gradient algorithms. In this work, we analyse the Mean Squared Bellman
Error from a smooth optimisation perspective combined with a Residual Gradient
formulation. Our contribution is two-fold.

First, we analyse critical points of the error function and provide technical
insights on the optimisation procure and design choices for neural networks.
When the existence of global minima is assumed and the objective fulfils
certain conditions we can eliminate suboptimal local minima when using
over-parametrised neural networks. We can construct an efficient Approximate
Newton's algorithm based on our analysis and confirm theoretical properties of
this algorithm such as being locally quadratically convergent to a global
minimum numerically.

Second, we demonstrate feasibility and generalisation capabilities of the
proposed algorithm empirically using continuous control problems and provide a
numerical verification of our critical point analysis. We outline the short
coming of Semi-Gradients. To benefit from an approximate Newton's algorithm
complete derivatives of the Mean Squared Bellman error must be considered
during training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gottwald_M/0/1/0/all/0/1"&gt;Martin Gottwald&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Gronauer_S/0/1/0/all/0/1"&gt;Sven Gronauer&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Hao Shen&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Diepold_K/0/1/0/all/0/1"&gt;Klaus Diepold&lt;/a&gt; (1) ((1) Technical University of Munich, (2) fortiss)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation for Graph Convolutional Network on Semi-Supervised Classification. (arXiv:2106.08848v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08848</id>
        <link href="http://arxiv.org/abs/2106.08848"/>
        <updated>2021-06-17T01:58:45.074Z</updated>
        <summary type="html"><![CDATA[Data augmentation aims to generate new and synthetic features from the
original data, which can identify a better representation of data and improve
the performance and generalizability of downstream tasks. However, data
augmentation for graph-based models remains a challenging problem, as graph
data is more complex than traditional data, which consists of two features with
different properties: graph topology and node attributes. In this paper, we
study the problem of graph data augmentation for Graph Convolutional Network
(GCN) in the context of improving the node embeddings for semi-supervised node
classification. Specifically, we conduct cosine similarity based cross
operation on the original features to create new graph features, including new
node attributes and new graph topologies, and we combine them as new pairwise
inputs for specific GCNs. Then, we propose an attentional integrating model to
weighted sum the hidden node embeddings encoded by these GCNs into the final
node embeddings. We also conduct a disparity constraint on these hidden node
embeddings when training to ensure that non-redundant information is captured
from different features. Experimental results on five real-world datasets show
that our method improves the classification accuracy with a clear margin (+2.5%
- +84.2%) than the original GCN model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhengzheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Ziyue Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1"&gt;Xuehai Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dharejo_F/0/1/0/all/0/1"&gt;Fayaz Ali Dharejo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuanchun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yi Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Anomaly Detection in Edge Streams. (arXiv:2009.08452v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08452</id>
        <link href="http://arxiv.org/abs/2009.08452"/>
        <updated>2021-06-17T01:58:45.068Z</updated>
        <summary type="html"><![CDATA[Given a stream of graph edges from a dynamic graph, how can we assign anomaly
scores to edges in an online manner, for the purpose of detecting unusual
behavior, using constant time and memory? Existing approaches aim to detect
individually surprising edges. In this work, we propose MIDAS, which focuses on
detecting microcluster anomalies, or suddenly arriving groups of suspiciously
similar edges, such as lockstep behavior, including denial of service attacks
in network traffic data. We further propose MIDAS-F, to solve the problem by
which anomalies are incorporated into the algorithm's internal states, creating
a `poisoning' effect that can allow future anomalies to slip through
undetected. MIDAS-F introduces two modifications: 1) We modify the anomaly
scoring function, aiming to reduce the `poisoning' effect of newly arriving
edges; 2) We introduce a conditional merge step, which updates the algorithm's
data structures after each time tick, but only if the anomaly score is below a
threshold value, also to reduce the `poisoning' effect. Experiments show that
MIDAS-F has significantly higher accuracy than MIDAS. MIDAS has the following
properties: (a) it detects microcluster anomalies while providing theoretical
guarantees about its false positive probability; (b) it is online, thus
processing each edge in constant time and constant memory, and also processes
the data orders-of-magnitude faster than state-of-the-art approaches; (c) it
provides up to 62% higher ROC-AUC than state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1"&gt;Siddharth Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Rui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1"&gt;Bryan Hooi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_M/0/1/0/all/0/1"&gt;Minji Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1"&gt;Kijung Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faloutsos_C/0/1/0/all/0/1"&gt;Christos Faloutsos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Morphed Face Images Using Discriminative Wavelet Sub-bands. (arXiv:2106.08565v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08565</id>
        <link href="http://arxiv.org/abs/2106.08565"/>
        <updated>2021-06-17T01:58:45.062Z</updated>
        <summary type="html"><![CDATA[This work investigates the well-known problem of morphing attacks, which has
drawn considerable attention in the biometrics community. Morphed images have
exposed face recognition systems' susceptibility to false acceptance, resulting
in dire consequences, especially for national security applications. To detect
morphing attacks, we propose a method which is based on a discriminative 2D
Discrete Wavelet Transform (2D-DWT). A discriminative wavelet sub-band can
highlight inconsistencies between a real and a morphed image. We observe that
there is a salient discrepancy between the entropy of a given sub-band in a
bona fide image, and the same sub-band's entropy in a morphed sample.
Considering this dissimilarity between these two entropy values, we find the
Kullback-Leibler divergence between the two distributions, namely the entropy
of the bona fide and the corresponding morphed images. The most discriminative
wavelet sub-bands are those with the highest corresponding KL-divergence
values. Accordingly, 22 sub-bands are selected as the most discriminative ones
in terms of morph detection. We show that a Deep Neural Network (DNN) trained
on the 22 discriminative sub-bands can detect morphed samples precisely. Most
importantly, the effectiveness of our algorithm is validated through
experiments on three datasets: VISAPP17, LMA, and MorGAN. We also performed an
ablation study on the sub-band selection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1"&gt;Poorya Aghdaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1"&gt;Baaria Chaudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1"&gt;Sobhan Soleymani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1"&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1"&gt;Nasser M. Nasrabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness of Object Detectors in Degrading Weather Conditions. (arXiv:2106.08795v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08795</id>
        <link href="http://arxiv.org/abs/2106.08795"/>
        <updated>2021-06-17T01:58:45.055Z</updated>
        <summary type="html"><![CDATA[State-of-the-art object detection systems for autonomous driving achieve
promising results in clear weather conditions. However, such autonomous safety
critical systems also need to work in degrading weather conditions, such as
rain, fog and snow. Unfortunately, most approaches evaluate only on the KITTI
dataset, which consists only of clear weather scenes. In this paper we address
this issue and perform one of the most detailed evaluation on single and dual
modality architectures on data captured in real weather conditions. We analyse
the performance degradation of these architectures in degrading weather
conditions. We demonstrate that an object detection architecture performing
good in clear weather might not be able to handle degrading weather conditions.
We also perform ablation studies on the dual modality architectures and show
their limitations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mirza_M/0/1/0/all/0/1"&gt;Muhammad Jehanzeb Mirza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buerkle_C/0/1/0/all/0/1"&gt;Cornelius Buerkle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jarquin_J/0/1/0/all/0/1"&gt;Julio Jarquin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Opitz_M/0/1/0/all/0/1"&gt;Michael Opitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oboril_F/0/1/0/all/0/1"&gt;Fabian Oboril&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholl_K/0/1/0/all/0/1"&gt;Kay-Ulrich Scholl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bischof_H/0/1/0/all/0/1"&gt;Horst Bischof&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure First Detail Next: Image Inpainting with Pyramid Generator. (arXiv:2106.08905v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08905</id>
        <link href="http://arxiv.org/abs/2106.08905"/>
        <updated>2021-06-17T01:58:45.039Z</updated>
        <summary type="html"><![CDATA[Recent deep generative models have achieved promising performance in image
inpainting. However, it is still very challenging for a neural network to
generate realistic image details and textures, due to its inherent spectral
bias. By our understanding of how artists work, we suggest to adopt a
`structure first detail next' workflow for image inpainting. To this end, we
propose to build a Pyramid Generator by stacking several sub-generators, where
lower-layer sub-generators focus on restoring image structures while the
higher-layer sub-generators emphasize image details. Given an input image, it
will be gradually restored by going through the entire pyramid in a bottom-up
fashion. Particularly, our approach has a learning scheme of progressively
increasing hole size, which allows it to restore large-hole images. In
addition, our method could fully exploit the benefits of learning with
high-resolution images, and hence is suitable for high-resolution image
inpainting. Extensive experimental results on benchmark datasets have validated
the effectiveness of our approach compared with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1"&gt;Shuyi Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1"&gt;Zhenxing Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1"&gt;Matan Protter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimerman_G/0/1/0/all/0/1"&gt;Gadi Zimerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yinghui Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LCDNet: Deep Loop Closure Detection and Point Cloud Registration for LiDAR SLAM. (arXiv:2103.05056v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05056</id>
        <link href="http://arxiv.org/abs/2103.05056"/>
        <updated>2021-06-17T01:58:45.014Z</updated>
        <summary type="html"><![CDATA[Loop closure detection is an essential component of Simultaneous Localization
and Mapping (SLAM) systems, which reduces the drift accumulated over time. Over
the years, several deep learning approaches have been proposed to address this
task, however their performance has been subpar compared to handcrafted
techniques, especially while dealing with reverse loops. In this paper, we
introduce the novel LCDNet that effectively detects loop closures in LiDAR
point clouds by simultaneously identifying previously visited places and
estimating the 6-DoF relative transformation between the current scan and the
map. LCDNet is composed of a shared encoder, a place recognition head that
extracts global descriptors, and a relative pose head that estimates the
transformation between two point clouds. We introduce a novel relative pose
head based on the unbalanced optimal transport theory that we implement in a
differentiable manner to allow for end-to-end training. Extensive evaluations
of LCDNet on multiple real-world autonomous driving datasets show that our
approach outperforms state-of-the-art loop closure detection and point cloud
registration techniques by a large margin, especially while dealing with
reverse loops. Moreover, we integrate our proposed loop closure detection
approach into a LiDAR SLAM library to provide a complete mapping system and
demonstrate the generalization ability using different sensor setup in an
unseen city.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1"&gt;Daniele Cattaneo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1"&gt;Matteo Vaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1"&gt;Abhinav Valada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for Discovering Optimal Solutions in Photonic Inverse Design. (arXiv:2106.08419v1 [physics.optics])]]></title>
        <id>http://arxiv.org/abs/2106.08419</id>
        <link href="http://arxiv.org/abs/2106.08419"/>
        <updated>2021-06-17T01:58:45.006Z</updated>
        <summary type="html"><![CDATA[Photonic inverse design has emerged as an indispensable engineering tool for
complex optical systems. In many instances it is important to optimize for both
material and geometry configurations, which results in complex non-smooth
search spaces with multiple local minima. Finding solutions approaching global
optimum may present a computationally intractable task. Here, we develop a
framework that allows expediting the search of solutions close to global
optimum on complex optimization spaces. We study the way representative black
box optimization algorithms work, including genetic algorithm (GA), particle
swarm optimization (PSO), simulated annealing (SA), and mesh adaptive direct
search (NOMAD). We then propose and utilize a two-step approach that identifies
best performance algorithms on arbitrarily complex search spaces. We reveal a
connection between the search space complexity and algorithm performance and
find that PSO and NOMAD consistently deliver better performance for mixed
integer problems encountered in photonic inverse design, particularly with the
account of material combinations. Our results differ from a commonly
anticipated advantage of GA. Our findings will foster more efficient design of
photonic systems with optimal performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Digani_J/0/1/0/all/0/1"&gt;Jagrit Digani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hon_P/0/1/0/all/0/1"&gt;Phillip Hon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Davoyan_A/0/1/0/all/0/1"&gt;Artur R. Davoyan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HELP: Hardware-Adaptive Efficient Latency Predictor for NAS via Meta-Learning. (arXiv:2106.08630v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08630</id>
        <link href="http://arxiv.org/abs/2106.08630"/>
        <updated>2021-06-17T01:58:44.993Z</updated>
        <summary type="html"><![CDATA[For deployment, neural architecture search should be hardware-aware, in order
to satisfy the device-specific constraints (e.g., memory usage, latency and
energy consumption) and enhance the model efficiency. Existing methods on
hardware-aware NAS collect a large number of samples (e.g., accuracy and
latency) from a target device, either builds a lookup table or a latency
estimator. However, such approach is impractical in real-world scenarios as
there exist numerous devices with different hardware specifications, and
collecting samples from such a large number of devices will require prohibitive
computational and monetary cost. To overcome such limitations, we propose
Hardware-adaptive Efficient Latency Predictor (HELP), which formulates the
device-specific latency estimation problem as a meta-learning problem, such
that we can estimate the latency of a model's performance for a given task on
an unseen device with a few samples. To this end, we introduce novel hardware
embeddings to embed any devices considering them as black-box functions that
output latencies, and meta-learn the hardware-adaptive latency predictor in a
device-dependent manner, using the hardware embeddings. We validate the
proposed HELP for its latency estimation performance on unseen platforms, on
which it achieves high estimation performance with as few as 10 measurement
samples, outperforming all relevant baselines. We also validate end-to-end NAS
frameworks using HELP against ones without it, and show that it largely reduces
the total time cost of the base NAS method, in latency-constrained settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hayeon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sewoong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chong_S/0/1/0/all/0/1"&gt;Song Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascading Modular Network (CAM-Net) for Multimodal Image Synthesis. (arXiv:2106.09015v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09015</id>
        <link href="http://arxiv.org/abs/2106.09015"/>
        <updated>2021-06-17T01:58:44.978Z</updated>
        <summary type="html"><![CDATA[Deep generative models such as GANs have driven impressive advances in
conditional image synthesis in recent years. A persistent challenge has been to
generate diverse versions of output images from the same input image, due to
the problem of mode collapse: because only one ground truth output image is
given per input image, only one mode of the conditional distribution is
modelled. In this paper, we focus on this problem of multimodal conditional
image synthesis and build on the recently proposed technique of Implicit
Maximum Likelihood Estimation (IMLE). Prior IMLE-based methods required
different architectures for different tasks, which limit their applicability,
and were lacking in fine details in the generated images. We propose CAM-Net, a
unified architecture that can be applied to a broad range of tasks.
Additionally, it is capable of generating convincing high frequency details,
achieving a reduction of the Frechet Inception Distance (FID) by up to 45.3%
compared to the baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shichong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moazeni_A/0/1/0/all/0/1"&gt;Alireza Moazeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LassoNet: A Neural Network with Feature Sparsity. (arXiv:1907.12207v10 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.12207</id>
        <link href="http://arxiv.org/abs/1907.12207"/>
        <updated>2021-06-17T01:58:44.962Z</updated>
        <summary type="html"><![CDATA[Much work has been done recently to make neural networks more interpretable,
and one obvious approach is to arrange for the network to use only a subset of
the available features. In linear models, Lasso (or $\ell_1$-regularized)
regression assigns zero weights to the most irrelevant or redundant features,
and is widely used in data science. However the Lasso only applies to linear
models. Here we introduce LassoNet, a neural network framework with global
feature selection. Our approach enforces a hierarchy: specifically a feature
can participate in a hidden unit only if its linear representative is active.
Unlike other approaches to feature selection for neural nets, our method uses a
modified objective function with constraints, and so integrates feature
selection with the parameter learning directly. As a result, it delivers an
entire regularization path of solutions with a range of feature sparsity. On
systematic experiments, LassoNet significantly outperforms state-of-the-art
methods for feature selection and regression. The LassoNet method uses
projected proximal gradient descent, and generalizes directly to deep networks.
It can be implemented by adding just a few lines of code to a standard neural
network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lemhadri_I/0/1/0/all/0/1"&gt;Ismael Lemhadri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ruan_F/0/1/0/all/0/1"&gt;Feng Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Abraham_L/0/1/0/all/0/1"&gt;Louis Abraham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tibshirani_R/0/1/0/all/0/1"&gt;Robert Tibshirani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissecting Hessian: Understanding Common Structure of Hessian in Neural Networks. (arXiv:2010.04261v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04261</id>
        <link href="http://arxiv.org/abs/2010.04261"/>
        <updated>2021-06-17T01:58:44.943Z</updated>
        <summary type="html"><![CDATA[Hessian captures important properties of the deep neural network loss
landscape. Previous works have observed low rank structure in the Hessians of
neural networks. We make several new observations about the top eigenspace of
layer-wise Hessian: top eigenspaces for different models have surprisingly high
overlap, and top eigenvectors form low rank matrices when they are reshaped
into the same shape as the corresponding weight matrix. Towards formally
explaining such structures of the Hessian, we show that the new eigenspace
structure can be explained by approximating the Hessian using Kronecker
factorization; we also prove the low rank structure for random data at random
initialization for over-parametrized two-layer neural nets. Our new
understanding can explain why some of these structures become weaker when the
network is trained with batch normalization. The Kronecker factorization also
leads to better explicit generalization bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yikai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xingyu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Annie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1"&gt;Rong Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the exchange-correlation functional from nature with fully differentiable density functional theory. (arXiv:2102.04229v4 [physics.chem-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04229</id>
        <link href="http://arxiv.org/abs/2102.04229"/>
        <updated>2021-06-17T01:58:44.918Z</updated>
        <summary type="html"><![CDATA[Improving the predictive capability of molecular properties in ab initio
simulations is essential for advanced material discovery. Despite recent
progress making use of machine learning, utilizing deep neural networks to
improve quantum chemistry modelling remains severely limited by the scarcity
and heterogeneity of appropriate experimental data. Here we show how training a
neural network to replace the exchange-correlation functional within a
fully-differentiable three-dimensional Kohn-Sham density functional theory
(DFT) framework can greatly improve simulation accuracy. Using only eight
experimental data points on diatomic molecules, our trained
exchange-correlation networks enable improved prediction accuracy of
atomization energies across a collection of 104 molecules containing new bonds
and atoms that are not present in the training dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Kasim_M/0/1/0/all/0/1"&gt;Muhammad F. Kasim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Vinko_S/0/1/0/all/0/1"&gt;Sam M. Vinko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EdgeConv with Attention Module for Monocular Depth Estimation. (arXiv:2106.08615v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08615</id>
        <link href="http://arxiv.org/abs/2106.08615"/>
        <updated>2021-06-17T01:58:44.910Z</updated>
        <summary type="html"><![CDATA[Monocular depth estimation is an especially important task in robotics and
autonomous driving, where 3D structural information is essential. However,
extreme lighting conditions and complex surface objects make it difficult to
predict depth in a single image. Therefore, to generate accurate depth maps, it
is important for the model to learn structural information about the scene. We
propose a novel Patch-Wise EdgeConv Module (PEM) and EdgeConv Attention Module
(EAM) to solve the difficulty of monocular depth estimation. The proposed
modules extract structural information by learning the relationship between
image patches close to each other in space using edge convolution. Our method
is evaluated on two popular datasets, the NYU Depth V2 and the KITTI Eigen
split, achieving state-of-the-art performance. We prove that the proposed model
predicts depth robustly in challenging scenes through various comparative
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Minhyeok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sangwon Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chaewon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangyoun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational System Identification for Nonlinear State-Space Models. (arXiv:2012.05072v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05072</id>
        <link href="http://arxiv.org/abs/2012.05072"/>
        <updated>2021-06-17T01:58:44.902Z</updated>
        <summary type="html"><![CDATA[This paper considers parameter estimation for nonlinear state-space models,
which is an important but challenging problem. We address this challenge by
employing a variational inference (VI) approach, which is a principled method
that has deep connections to maximum likelihood estimation. This VI approach
ultimately provides estimates of the model as solutions to an optimisation
problem, which is deterministic, tractable and can be solved using standard
optimisation tools. A specialisation of this approach for systems with additive
Gaussian noise is also detailed. The proposed method is examined numerically on
a range of simulated and real examples focusing on the robustness to parameter
initialisation; additionally, favourable comparisons are performed against
state-of-the-art alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Courts_J/0/1/0/all/0/1"&gt;Jarrad Courts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wills_A/0/1/0/all/0/1"&gt;Adrian Wills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schon_T/0/1/0/all/0/1"&gt;Thomas Sch&amp;#xf6;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ninness_B/0/1/0/all/0/1"&gt;Brett Ninness&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hash Layers For Large Sparse Models. (arXiv:2106.04426v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04426</id>
        <link href="http://arxiv.org/abs/2106.04426"/>
        <updated>2021-06-17T01:58:44.889Z</updated>
        <summary type="html"><![CDATA[We investigate the training of sparse layers that use different parameters
for different inputs based on hashing in large Transformer models.
Specifically, we modify the feedforward layer to hash to different sets of
weights depending on the current token, over all tokens in the sequence. We
show that this procedure either outperforms or is competitive with
learning-to-route mixture-of-expert methods such as Switch Transformers and
BASE Layers, while requiring no routing parameters or extra terms in the
objective function such as a load balancing loss, and no sophisticated
assignment algorithm. We study the performance of different hashing techniques,
hash sizes and input features, and show that balanced and random hashes focused
on the most local features work best, compared to either learning clusters or
using longer-range context. We show our approach works well both on large
language modeling and dialogue tasks, and on downstream fine-tuning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Disentanglement for Rare Event Modeling. (arXiv:2009.08541v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08541</id>
        <link href="http://arxiv.org/abs/2009.08541"/>
        <updated>2021-06-17T01:58:44.852Z</updated>
        <summary type="html"><![CDATA[Combining the increasing availability and abundance of healthcare data and
the current advances in machine learning methods have created renewed
opportunities to improve clinical decision support systems. However, in
healthcare risk prediction applications, the proportion of cases with the
condition (label) of interest is often very low relative to the available
sample size. Though very prevalent in healthcare, such imbalanced
classification settings are also common and challenging in many other
scenarios. So motivated, we propose a variational disentanglement approach to
semi-parametrically learn from rare events in heavily imbalanced classification
problems. Specifically, we leverage the imposed extreme-distribution behavior
on a latent space to extract information from low-prevalence events, and
develop a robust prediction arm that joins the merits of the generalized
additive model and isotonic neural nets. Results on synthetic studies and
diverse real-world datasets, including mortality prediction on a COVID-19
cohort, demonstrate that the proposed approach outperforms existing
alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Xiu_Z/0/1/0/all/0/1"&gt;Zidi Xiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chenyang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gao_M/0/1/0/all/0/1"&gt;Michael Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Davis_C/0/1/0/all/0/1"&gt;Connor Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Goldstein_B/0/1/0/all/0/1"&gt;Benjamin A. Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Henao_R/0/1/0/all/0/1"&gt;Ricardo Henao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Causal Semantic Representation for Out-of-Distribution Prediction. (arXiv:2011.01681v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.01681</id>
        <link href="http://arxiv.org/abs/2011.01681"/>
        <updated>2021-06-17T01:58:44.840Z</updated>
        <summary type="html"><![CDATA[Conventional supervised learning methods, especially deep ones, are found to
be sensitive to out-of-distribution (OOD) examples, largely because the learned
representation mixes the semantic factor with the variation factor due to their
domain-specific correlation, while only the semantic factor causes the output.
To address the problem, we propose a Causal Semantic Generative model (CSG)
based on a causal reasoning so that the two factors are modeled separately, and
develop methods for OOD prediction from a single training domain, which is
common and challenging. The methods are based on the causal invariance
principle, with a novel design for both efficient learning and easy prediction.
Theoretically, we prove that under certain conditions, CSG can identify the
semantic factor by fitting training data, and this semantic-identification
guarantees the boundedness of OOD generalization error and the success of
adaptation. Empirical study shows improved OOD performance over prevailing
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xinwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haoyue Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_T/0/1/0/all/0/1"&gt;Tao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Complexity aspects of local minima and related notions. (arXiv:2008.06148v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.06148</id>
        <link href="http://arxiv.org/abs/2008.06148"/>
        <updated>2021-06-17T01:58:44.823Z</updated>
        <summary type="html"><![CDATA[We consider the notions of (i) critical points, (ii) second-order points,
(iii) local minima, and (iv) strict local minima for multivariate polynomials.
For each type of point, and as a function of the degree of the polynomial, we
study the complexity of deciding (1) if a given point is of that type, and (2)
if a polynomial has a point of that type. Our results characterize the
complexity of these two questions for all degrees left open by prior
literature. Our main contributions reveal that many of these questions turn out
to be tractable for cubic polynomials. In particular, we present an
efficiently-checkable necessary and sufficient condition for local minimality
of a point for a cubic polynomial. We also show that a local minimum of a cubic
polynomial can be efficiently found by solving semidefinite programs of size
linear in the number of variables. By contrast, we show that it is strongly
NP-hard to decide if a cubic polynomial has a critical point. We also prove
that the set of second-order points of any cubic polynomial is a spectrahedron,
and conversely that any spectrahedron is the projection of the set of
second-order points of a cubic polynomial. In our final section, we briefly
present a potential application of finding local minima of cubic polynomials to
the design of a third-order Newton method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ahmadi_A/0/1/0/all/0/1"&gt;Amir Ali Ahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jeffrey Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Variational Attention Models for Language Generation. (arXiv:2004.09764v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09764</id>
        <link href="http://arxiv.org/abs/2004.09764"/>
        <updated>2021-06-17T01:58:44.816Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders have been widely applied for natural language
generation, however, there are two long-standing problems: information
under-representation and posterior collapse. The former arises from the fact
that only the last hidden state from the encoder is transformed to the latent
space, which is insufficient to summarize data. The latter comes as a result of
the imbalanced scale between the reconstruction loss and the KL divergence in
the objective function. To tackle these issues, in this paper we propose the
discrete variational attention model with categorical distribution over the
attention mechanism owing to the discrete nature in languages. Our approach is
combined with an auto-regressive prior to capture the sequential dependency
from observations, which can enhance the latent space for language generation.
Moreover, thanks to the property of discreteness, the training of our proposed
approach does not suffer from posterior collapse. Furthermore, we carefully
analyze the superiority of discrete latent space over the continuous space with
the common Gaussian distribution. Extensive experiments on language generation
demonstrate superior advantages of our proposed approach in comparison with the
state-of-the-art counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Development of Quantized DNN Library for Exact Hardware Emulation. (arXiv:2106.08892v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08892</id>
        <link href="http://arxiv.org/abs/2106.08892"/>
        <updated>2021-06-17T01:58:44.802Z</updated>
        <summary type="html"><![CDATA[Quantization is used to speed up execution time and save power when runnning
Deep neural networks (DNNs) on edge devices like AI chips. To investigate the
effect of quantization, we need performing inference after quantizing the
weights of DNN with 32-bit floating-point precision by a some bit width, and
then quantizing them back to 32-bit floating-point precision. This is because
the DNN library can only handle floating-point numbers. However, the accuracy
of the emulation does not provide accurate precision. We need accurate
precision to detect overflow in MAC operations or to verify the operation on
edge de vices. We have developed PyParch, a DNN library that executes quantized
DNNs (QNNs) with exactly the same be havior as hardware. In this paper, we
describe a new proposal and implementation of PyParch. As a result of the
evaluation, the accuracy of QNNs with arbitrary bit widths can be estimated for
la rge and complex DNNs such as YOLOv5, and the overflow can be detected. We
evaluated the overhead of the emulation time and found that it was 5.6 times
slower for QNN and 42

times slower for QNN with overflow detection compared to the normal DNN
execution time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kiyama_M/0/1/0/all/0/1"&gt;Masato Kiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amagasaki_M/0/1/0/all/0/1"&gt;Motoki Amagasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iida_M/0/1/0/all/0/1"&gt;Masahiro Iida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reset-Free Lifelong Learning with Skill-Space Planning. (arXiv:2012.03548v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03548</id>
        <link href="http://arxiv.org/abs/2012.03548"/>
        <updated>2021-06-17T01:58:44.778Z</updated>
        <summary type="html"><![CDATA[The objective of lifelong reinforcement learning (RL) is to optimize agents
which can continuously adapt and interact in changing environments. However,
current RL approaches fail drastically when environments are non-stationary and
interactions are non-episodic. We propose Lifelong Skill Planning (LiSP), an
algorithmic framework for non-episodic lifelong RL based on planning in an
abstract space of higher-order skills. We learn the skills in an unsupervised
manner using intrinsic rewards and plan over the learned skills using a learned
dynamics model. Moreover, our framework permits skill discovery even from
offline data, thereby reducing the need for excessive real-world interactions.
We demonstrate empirically that LiSP successfully enables long-horizon planning
and learns agents that can avoid catastrophic failures even in challenging
non-stationary and non-episodic environments derived from gridworld and MuJoCo
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Kevin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1"&gt;Aditya Grover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1"&gt;Igor Mordatch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GemNet: Universal Directional Graph Neural Networks for Molecules. (arXiv:2106.08903v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2106.08903</id>
        <link href="http://arxiv.org/abs/2106.08903"/>
        <updated>2021-06-17T01:58:44.766Z</updated>
        <summary type="html"><![CDATA[Effectively predicting molecular interactions has the potential to accelerate
molecular dynamics by multiple orders of magnitude and thus revolutionize
chemical simulations. Graph neural networks (GNNs) have recently shown great
successes for this task, overtaking classical methods based on fixed molecular
kernels. However, they still appear very limited from a theoretical
perspective, since regular GNNs cannot distinguish certain types of graphs. In
this work we close this gap between theory and practice. We show that GNNs with
directed edge embeddings and two-hop message passing are indeed universal
approximators for predictions that are invariant to global rotation and
translation, and equivariant to permutation. We then leverage these insights
and multiple structural improvements to propose the geometric message passing
neural network (GemNet). We demonstrate the benefits of the proposed changes in
multiple ablation studies. GemNet outperforms previous models on the COLL and
MD17 molecular dynamics datasets by 36%, performing especially well on the most
challenging molecules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Klicpera_J/0/1/0/all/0/1"&gt;Johannes Klicpera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Becker_F/0/1/0/all/0/1"&gt;Florian Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Generative Adversarial Networks in One Stage. (arXiv:2103.00430v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00430</id>
        <link href="http://arxiv.org/abs/2103.00430"/>
        <updated>2021-06-17T01:58:44.759Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have demonstrated unprecedented
success in various image generation tasks. The encouraging results, however,
come at the price of a cumbersome training process, during which the generator
and discriminator are alternately updated in two stages. In this paper, we
investigate a general training scheme that enables training GANs efficiently in
only one stage. Based on the adversarial losses of the generator and
discriminator, we categorize GANs into two classes, Symmetric GANs and
Asymmetric GANs, and introduce a novel gradient decomposition method to unify
the two, allowing us to train both classes in one stage and hence alleviate the
training effort. We also computationally analyze the efficiency of the proposed
method, and empirically demonstrate that, the proposed method yields a solid
$1.5\times$ acceleration across various datasets and network architectures.
Furthermore, we show that the proposed method is readily applicable to other
adversarial-training scenarios, such as data-free knowledge distillation. The
code is available at https://github.com/zju-vipa/OSGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chengchao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Youtan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xubin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jie Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Mingli Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularized Orthogonal Machine Learning for Nonlinear Semiparametric Models. (arXiv:1806.04823v7 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1806.04823</id>
        <link href="http://arxiv.org/abs/1806.04823"/>
        <updated>2021-06-17T01:58:44.719Z</updated>
        <summary type="html"><![CDATA[This paper proposes a Lasso-type estimator for a high-dimensional sparse
parameter identified by a single index conditional moment restriction (CMR). In
addition to this parameter, the moment function can also depend on a nuisance
function, such as the propensity score or the conditional choice probability,
which we estimate by modern machine learning tools. We first adjust the moment
function so that the gradient of the future loss function is insensitive
(formally, Neyman-orthogonal) with respect to the first-stage regularization
bias, preserving the single index property. We then take the loss function to
be an indefinite integral of the adjusted moment function with respect to the
single index. The proposed Lasso estimator converges at the oracle rate, where
the oracle knows the nuisance function and solves only the parametric problem.
We demonstrate our method by estimating the short-term heterogeneous impact of
Connecticut's Jobs First welfare reform experiment on women's welfare
participation decision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Nekipelov_D/0/1/0/all/0/1"&gt;Denis Nekipelov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Semenova_V/0/1/0/all/0/1"&gt;Vira Semenova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Syrgkanis_V/0/1/0/all/0/1"&gt;Vasilis Syrgkanis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Bellman Operators. (arXiv:2106.05012v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05012</id>
        <link href="http://arxiv.org/abs/2106.05012"/>
        <updated>2021-06-17T01:58:44.690Z</updated>
        <summary type="html"><![CDATA[We introduce a novel perspective on Bayesian reinforcement learning (RL);
whereas existing approaches infer a posterior over the transition distribution
or Q-function, we characterise the uncertainty in the Bellman operator. Our
Bayesian Bellman operator (BBO) framework is motivated by the insight that when
bootstrapping is introduced, model-free approaches actually infer a posterior
over Bellman operators, not value functions. In this paper, we use BBO to
provide a rigorous theoretical analysis of model-free Bayesian RL to better
understand its relationshipto established frequentist RL methodologies. We
prove that Bayesian solutions are consistent with frequentist RL solutions,
even when approximate inference isused, and derive conditions for which
convergence properties hold. Empirically, we demonstrate that algorithms
derived from the BBO framework have sophisticated deep exploration properties
that enable them to solve continuous control tasks at which state-of-the-art
regularised actor-critic algorithms fail catastrophically]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fellows_M/0/1/0/all/0/1"&gt;Matthew Fellows&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartikainen_K/0/1/0/all/0/1"&gt;Kristian Hartikainen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1"&gt;Shimon Whiteson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Communication. (arXiv:1909.05350v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.05350</id>
        <link href="http://arxiv.org/abs/1909.05350"/>
        <updated>2021-06-17T01:58:44.681Z</updated>
        <summary type="html"><![CDATA[We analyze (stochastic) gradient descent (SGD) with delayed updates on smooth
quasi-convex and non-convex functions and derive concise, non-asymptotic,
convergence rates. We show that the rate of convergence in all cases consists
of two terms: (i) a stochastic term which is not affected by the delay, and
(ii) a higher order deterministic term which is only linearly slowed down by
the delay. Thus, in the presence of noise, the effects of the delay become
negligible after a few iterations and the algorithm converges at the same
optimal rate as standard SGD. This result extends a line of research that
showed similar results in the asymptotic regime or for strongly-convex
quadratic functions only. We further show similar results for SGD with more
intricate form of delayed gradients---compressed gradients under error
compensation and for local~SGD where multiple workers perform local steps
before communicating with each other. In all of these settings, we improve upon
the best known rates. These results show that SGD is robust to compressed
and/or delayed stochastic gradient updates. This is in particular important for
distributed parallel implementations, where asynchronous and communication
efficient methods are the key to achieve linear speedups for optimization with
multiple devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1"&gt;Sai Praneeth Karimireddy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better. (arXiv:2106.08962v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08962</id>
        <link href="http://arxiv.org/abs/2106.08962"/>
        <updated>2021-06-17T01:58:44.674Z</updated>
        <summary type="html"><![CDATA[Deep Learning has revolutionized the fields of computer vision, natural
language understanding, speech recognition, information retrieval and more.
However, with the progressive improvements in deep learning models, their
number of parameters, latency, resources required to train, etc. have all have
increased significantly. Consequently, it has become important to pay attention
to these footprint metrics of a model as well, not just its quality. We present
and motivate the problem of efficiency in deep learning, followed by a thorough
survey of the five core areas of model efficiency (spanning modeling
techniques, infrastructure, and hardware) and the seminal work there. We also
present an experiment-based guide along with code, for practitioners to
optimize their model training and deployment. We believe this is the first
comprehensive survey in the efficient deep learning space that covers the
landscape of model efficiency from modeling techniques to hardware support. Our
hope is that this survey would provide the reader with the mental model and the
necessary understanding of the field to apply generic efficiency techniques to
immediately get significant improvements, and also equip them with ideas for
further research and experimentation to achieve additional gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Menghani_G/0/1/0/all/0/1"&gt;Gaurav Menghani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving filling level classification with adversarial training. (arXiv:2102.04057v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04057</id>
        <link href="http://arxiv.org/abs/2102.04057"/>
        <updated>2021-06-17T01:58:44.599Z</updated>
        <summary type="html"><![CDATA[We investigate the problem of classifying - from a single image - the level
of content in a cup or a drinking glass. This problem is made challenging by
several ambiguities caused by transparencies, shape variations and partial
occlusions, and by the availability of only small training datasets. In this
paper, we tackle this problem with an appropriate strategy for transfer
learning. Specifically, we use adversarial training in a generic source dataset
and then refine the training with a task-specific dataset. We also discuss and
experimentally evaluate several training strategies and their combination on a
range of container types of the CORSMAL Containers Manipulation dataset. We
show that transfer learning with adversarial training in the source domain
consistently improves the classification accuracy on the test set and limits
the overfitting of the classifier to specific features of the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Modas_A/0/1/0/all/0/1"&gt;Apostolos Modas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1"&gt;Alessio Xompero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Matilla_R/0/1/0/all/0/1"&gt;Ricardo Sanchez-Matilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1"&gt;Pascal Frossard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1"&gt;Andrea Cavallaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Sample Complexities for Deep Networks and Robust Classification via an All-Layer Margin. (arXiv:1910.04284v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.04284</id>
        <link href="http://arxiv.org/abs/1910.04284"/>
        <updated>2021-06-17T01:58:44.517Z</updated>
        <summary type="html"><![CDATA[For linear classifiers, the relationship between (normalized) output margin
and generalization is captured in a clear and simple bound -- a large output
margin implies good generalization. Unfortunately, for deep models, this
relationship is less clear: existing analyses of the output margin give
complicated bounds which sometimes depend exponentially on depth. In this work,
we propose to instead analyze a new notion of margin, which we call the
"all-layer margin." Our analysis reveals that the all-layer margin has a clear
and direct relationship with generalization for deep models. This enables the
following concrete applications of the all-layer margin: 1) by analyzing the
all-layer margin, we obtain tighter generalization bounds for neural nets which
depend on Jacobian and hidden layer norms and remove the exponential dependency
on depth 2) our neural net results easily translate to the adversarially robust
setting, giving the first direct analysis of robust test error for deep
networks, and 3) we present a theoretically inspired training algorithm for
increasing the all-layer margin. Our algorithm improves both clean and
adversarially robust test performance over strong baselines in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Colin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent. (arXiv:2009.04709v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04709</id>
        <link href="http://arxiv.org/abs/2009.04709"/>
        <updated>2021-06-17T01:58:44.507Z</updated>
        <summary type="html"><![CDATA[Adversarial training, especially projected gradient descent (PGD), has been a
successful approach for improving robustness against adversarial attacks. After
adversarial training, gradients of models with respect to their inputs have a
preferential direction. However, the direction of alignment is not
mathematically well established, making it difficult to evaluate
quantitatively. We propose a novel definition of this direction as the
direction of the vector pointing toward the closest point of the support of the
closest inaccurate class in decision space. To evaluate the alignment with this
direction after adversarial training, we apply a metric that uses generative
adversarial networks to produce the smallest residual needed to change the
class present in the image. We show that PGD-trained models have a higher
alignment than the baseline according to our definition, that our metric
presents higher alignment values than a competing metric formulation, and that
enforcing this alignment increases the robustness of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lanfredi_R/0/1/0/all/0/1"&gt;Ricardo Bigolin Lanfredi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schroeder_J/0/1/0/all/0/1"&gt;Joyce D. Schroeder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tasdizen_T/0/1/0/all/0/1"&gt;Tolga Tasdizen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimality of short-term synaptic plasticity in modelling certain dynamic environments. (arXiv:2009.06808v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06808</id>
        <link href="http://arxiv.org/abs/2009.06808"/>
        <updated>2021-06-17T01:58:44.499Z</updated>
        <summary type="html"><![CDATA[Biological neurons and their in-silico emulations for neuromorphic artificial
intelligence (AI) use extraordinarily energy-efficient mechanisms, such as
spike-based communication and local synaptic plasticity. It remains unclear
whether these neuronal mechanisms only offer efficiency or also underlie the
superiority of biological intelligence. Here, we prove rigorously that, indeed,
the Bayes-optimal prediction and inference of randomly but continuously
transforming environments, a common natural setting, relies on short-term
spike-timing-dependent plasticity, a hallmark of biological synapses. Further,
this dynamic Bayesian inference through plasticity enables circuits of the
cerebral cortex in simulations to recognize previously unseen, highly distorted
dynamic stimuli. Strikingly, this also introduces a biologically-modelled AI,
the first to overcome multiple limitations of deep learning and outperform
artificial neural networks in a visual task. The cortical-like network is
spiking and event-based, trained only with unsupervised and local plasticity,
on a small, narrow, and static training dataset, but achieves recognition of
unseen, transformed, and dynamic data better than deep neural networks with
continuous activations, trained with supervised backpropagation on the
transforming data. These results link short-term plasticity to high-level
cortical function, suggest optimality of natural intelligence for natural
environments, and repurpose neuromorphic AI from mere efficiency to
computational supremacy altogether.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1"&gt;Timoleon Moraitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1"&gt;Abu Sebastian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eleftheriou_E/0/1/0/all/0/1"&gt;Evangelos Eleftheriou&lt;/a&gt; (IBM Research - Zurich)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ditto: Fair and Robust Federated Learning Through Personalization. (arXiv:2012.04221v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04221</id>
        <link href="http://arxiv.org/abs/2012.04221"/>
        <updated>2021-06-17T01:58:44.492Z</updated>
        <summary type="html"><![CDATA[Fairness and robustness are two important concerns for federated learning
systems. In this work, we identify that robustness to data and model poisoning
attacks and fairness, measured as the uniformity of performance across devices,
are competing constraints in statistically heterogeneous networks. To address
these constraints, we propose employing a simple, general framework for
personalized federated learning, Ditto, that can inherently provide fairness
and robustness benefits, and develop a scalable solver for it. Theoretically,
we analyze the ability of Ditto to achieve fairness and robustness
simultaneously on a class of linear problems. Empirically, across a suite of
federated datasets, we show that Ditto not only achieves competitive
performance relative to recent personalization methods, but also enables more
accurate, robust, and fair models relative to state-of-the-art fair or robust
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shengyuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1"&gt;Ahmad Beirami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1"&gt;Virginia Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation. (arXiv:2106.09016v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09016</id>
        <link href="http://arxiv.org/abs/2106.09016"/>
        <updated>2021-06-17T01:58:44.486Z</updated>
        <summary type="html"><![CDATA[Image-to-Image (I2I) multi-domain translation models are usually evaluated
also using the quality of their semantic interpolation results. However,
state-of-the-art models frequently show abrupt changes in the image appearance
during interpolation, and usually perform poorly in interpolations across
domains. In this paper, we propose a new training protocol based on three
specific losses which help a translation network to learn a smooth and
disentangled latent style space in which: 1) Both intra- and inter-domain
interpolations correspond to gradual changes in the generated images and 2) The
content of the source image is better preserved during the translation.
Moreover, we propose a novel evaluation metric to properly measure the
smoothness of latent style space of I2I translation models. The proposed method
can be plugged into existing translation approaches, and our extensive
experiments on different datasets show that it can significantly boost the
quality of the generated images and the graduality of the interpolations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yahui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1"&gt;Enver Sangineto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yajing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1"&gt;Linchao Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoxian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1"&gt;Bruno Lepri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadai_M/0/1/0/all/0/1"&gt;Marco De Nadai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KALE Flow: A Relaxed KL Gradient Flow for Probabilities with Disjoint Support. (arXiv:2106.08929v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08929</id>
        <link href="http://arxiv.org/abs/2106.08929"/>
        <updated>2021-06-17T01:58:44.471Z</updated>
        <summary type="html"><![CDATA[We study the gradient flow for a relaxed approximation to the
Kullback-Leibler (KL) divergence between a moving source and a fixed target
distribution. This approximation, termed the KALE (KL approximate lower-bound
estimator), solves a regularized version of the Fenchel dual problem defining
the KL over a restricted class of functions. When using a Reproducing Kernel
Hilbert Space (RKHS) to define the function class, we show that the KALE
continuously interpolates between the KL and the Maximum Mean Discrepancy
(MMD). Like the MMD and other Integral Probability Metrics, the KALE remains
well defined for mutually singular distributions. Nonetheless, the KALE
inherits from the limiting KL a greater sensitivity to mismatch in the support
of the distributions, compared with the MMD. These two properties make the KALE
gradient flow particularly well suited when the target distribution is
supported on a low-dimensional manifold. Under an assumption of sufficient
smoothness of the trajectories, we show the global convergence of the KALE
flow. We propose a particle implementation of the flow given initial samples
from the source and the target distribution, which we use to empirically
confirm the KALE's properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Glaser_P/0/1/0/all/0/1"&gt;Pierre Glaser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Arbel_M/0/1/0/all/0/1"&gt;Michael Arbel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Tertiary Protein Structures via an Interpretative Variational Autoencoder. (arXiv:2004.07119v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07119</id>
        <link href="http://arxiv.org/abs/2004.07119"/>
        <updated>2021-06-17T01:58:44.465Z</updated>
        <summary type="html"><![CDATA[Much scientific enquiry across disciplines is founded upon a mechanistic
treatment of dynamic systems that ties form to function. A highly visible
instance of this is in molecular biology, where an important goal is to
determine functionally-relevant forms/structures that a protein molecule
employs to interact with molecular partners in the living cell. This goal is
typically pursued under the umbrella of stochastic optimization with algorithms
that optimize a scoring function. Research repeatedly shows that current
scoring function, though steadily improving, correlate weakly with molecular
activity. Inspired by recent momentum in generative deep learning, this paper
proposes and evaluates an alternative approach to generating
functionally-relevant three-dimensional structures of a protein. Though
typically deep generative models struggle with highly-structured data, the work
presented here circumvents this challenge via graph-generative models. A
comprehensive evaluation of several deep architectures shows the promise of
generative models in directly revealing the latent space for sampling novel
tertiary structures, as well as in highlighting axes/factors that carry
structural meaning and open the black box often associated with deep models.
The work presented here is a first step towards interpretative, deep generative
models becoming viable and informative complementary approaches to protein
structure prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaojie Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuanqi Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Tadepalli_S/0/1/0/all/0/1"&gt;Sivani Tadepalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Shehu_A/0/1/0/all/0/1"&gt;Amarda Shehu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting chaos in lineage-trees: A deep learning approach. (arXiv:2106.08956v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08956</id>
        <link href="http://arxiv.org/abs/2106.08956"/>
        <updated>2021-06-17T01:58:44.458Z</updated>
        <summary type="html"><![CDATA[Many complex phenomena, from weather systems to heartbeat rhythm patterns,
are effectively modeled as low-dimensional dynamical systems. Such systems may
behave chaotically under certain conditions, and so the ability to detect chaos
based on empirical measurement is an important step in characterizing and
predicting these processes. Classifying a system as chaotic usually requires
estimating its largest Lyapunov exponent, which quantifies the average rate of
convergence or divergence of initially close trajectories in state space, and
for which a positive value is generally accepted as an operational definition
of chaos. Estimating the largest Lyapunov exponent from observations of a
process is especially challenging in systems affected by dynamical noise, which
is the case for many models of real-world processes, in particular models of
biological systems. We describe a novel method for estimating the largest
Lyapunov exponent from data, based on training Deep Learning models on
synthetically generated trajectories, and demonstrate that this method yields
accurate and noise-robust predictions given relatively short inputs and across
a range of different dynamical systems. Our method is unique in that it can
analyze tree-shaped data, a ubiquitous topology in biological settings, and
specifically in dynamics over lineages of cells or organisms. We also
characterize the types of input information extracted by our models for their
predictions, allowing for a deeper understanding into the different ways by
which chaos can be analyzed in different topologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rappeport_H/0/1/0/all/0/1"&gt;Hagai Rappeport&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reisman_I/0/1/0/all/0/1"&gt;Irit Levin Reisman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tishby_N/0/1/0/all/0/1"&gt;Naftali Tishby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balaban_N/0/1/0/all/0/1"&gt;Nathalie Q. Balaban&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization. (arXiv:2102.01670v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01670</id>
        <link href="http://arxiv.org/abs/2102.01670"/>
        <updated>2021-06-17T01:58:44.439Z</updated>
        <summary type="html"><![CDATA[Training sparse networks to converge to the same performance as dense neural
architectures has proven to be elusive. Recent work suggests that
initialization is the key. However, while this direction of research has had
some success, focusing on initialization alone appears to be inadequate. In
this paper, we take a broader view of training sparse networks and consider the
role of regularization, optimization, and architecture choices on sparse
models. We propose a simple experimental framework, Same Capacity Sparse vs
Dense Comparison (SC-SDC), that allows for a fair comparison of sparse and
dense networks. Furthermore, we propose a new measure of gradient flow,
Effective Gradient Flow (EGF), that better correlates to performance in sparse
networks. Using top-line metrics, SC-SDC and EGF, we show that default choices
of optimizers, activation functions and regularizers used for dense networks
can disadvantage sparse networks. Based upon these findings, we show that
gradient flow in sparse networks can be improved by reconsidering aspects of
the architecture design and the training regime. Our work suggests that
initialization is only one piece of the puzzle and taking a wider view of
tailoring optimization to sparse networks yields promising results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1"&gt;Kale-ab Tessera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1"&gt;Sara Hooker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosman_B/0/1/0/all/0/1"&gt;Benjamin Rosman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LieTransformer: Equivariant self-attention for Lie Groups. (arXiv:2012.10885v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10885</id>
        <link href="http://arxiv.org/abs/2012.10885"/>
        <updated>2021-06-17T01:58:44.433Z</updated>
        <summary type="html"><![CDATA[Group equivariant neural networks are used as building blocks of group
invariant neural networks, which have been shown to improve generalisation
performance and data efficiency through principled parameter sharing. Such
works have mostly focused on group equivariant convolutions, building on the
result that group equivariant linear maps are necessarily convolutions. In this
work, we extend the scope of the literature to self-attention, that is emerging
as a prominent building block of deep learning models. We propose the
LieTransformer, an architecture composed of LieSelfAttention layers that are
equivariant to arbitrary Lie groups and their discrete subgroups. We
demonstrate the generality of our approach by showing experimental results that
are competitive to baseline methods on a wide range of tasks: shape counting on
point clouds, molecular property regression and modelling particle trajectories
under Hamiltonian dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hutchinson_M/0/1/0/all/0/1"&gt;Michael Hutchinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Charline Le Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1"&gt;Sheheryar Zaidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dupont_E/0/1/0/all/0/1"&gt;Emilien Dupont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunjik Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Quasi-Bayesian Inference for Instrumental Variable Regression. (arXiv:2106.08750v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08750</id>
        <link href="http://arxiv.org/abs/2106.08750"/>
        <updated>2021-06-17T01:58:44.353Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed an upsurge of interest in employing flexible
machine learning models for instrumental variable (IV) regression, but the
development of uncertainty quantification methodology is still lacking. In this
work we present a scalable quasi-Bayesian procedure for IV regression, building
upon the recently developed kernelized IV models. Contrary to Bayesian modeling
for IV, our approach does not require additional assumptions on the data
generating process, and leads to a scalable approximate inference algorithm
with time cost comparable to the corresponding point estimation methods. Our
algorithm can be further extended to work with neural network models. We
analyze the theoretical properties of the proposed quasi-posterior, and
demonstrate through empirical evaluation the competitive performance of our
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ren_T/0/1/0/all/0/1"&gt;Tongzheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Banker Online Mirror Descent. (arXiv:2106.08943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08943</id>
        <link href="http://arxiv.org/abs/2106.08943"/>
        <updated>2021-06-17T01:58:44.340Z</updated>
        <summary type="html"><![CDATA[We propose Banker-OMD, a novel framework generalizing the classical Online
Mirror Descent (OMD) technique in online learning algorithm design. Banker-OMD
allows algorithms to robustly handle delayed feedback, and offers a general
methodology for achieving $\tilde{O}(\sqrt{T} + \sqrt{D})$-style regret bounds
in various delayed-feedback online learning tasks, where $T$ is the time
horizon length and $D$ is the total feedback delay. We demonstrate the power of
Banker-OMD with applications to three important bandit scenarios with delayed
feedback, including delayed adversarial Multi-armed bandits (MAB), delayed
adversarial linear bandits, and a novel delayed best-of-both-worlds MAB
setting. Banker-OMD achieves nearly-optimal performance in all the three
settings. In particular, it leads to the first delayed adversarial linear
bandit algorithm achieving $\tilde{O}(\text{poly}(n)(\sqrt{T} + \sqrt{D}))$
regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiatai Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longbo Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry of Similarity Comparisons. (arXiv:2006.09858v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09858</id>
        <link href="http://arxiv.org/abs/2006.09858"/>
        <updated>2021-06-17T01:58:44.335Z</updated>
        <summary type="html"><![CDATA[Many data analysis problems can be cast as distance geometry problems in
\emph{space forms} -- Euclidean, spherical, or hyperbolic spaces. Often,
absolute distance measurements are often unreliable or simply unavailable and
only proxies to absolute distances in the form of similarities are available.
Hence we ask the following: Given only \emph{comparisons} of similarities
amongst a set of entities, what can be said about the geometry of the
underlying space form? To study this question, we introduce the notions of the
\textit{ordinal capacity} of a target space form and \emph{ordinal spread} of
the similarity measurements. The latter is an indicator of complex patterns in
the measurements, while the former quantifies the capacity of a space form to
accommodate a set of measurements with a specific ordinal spread profile. We
prove that the ordinal capacity of a space form is related to its dimension and
the sign of its curvature. This leads to a lower bound on the Euclidean and
spherical embedding dimension of what we term similarity graphs. More
importantly, we show that the statistical behavior of the ordinal spread random
variables defined on a similarity graph can be used to identify its underlying
space form. We support our theoretical claims with experiments on weighted
trees, single-cell RNA expression data and spherical cartographic measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1"&gt;Puoya Tabaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jianhao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1"&gt;Olgica Milenkovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1"&gt;Ivan Dokmani&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08977</id>
        <link href="http://arxiv.org/abs/2106.08977"/>
        <updated>2021-06-17T01:58:44.321Z</updated>
        <summary type="html"><![CDATA[Weak supervision has shown promising results in many natural language
processing tasks, such as Named Entity Recognition (NER). Existing work mainly
focuses on learning deep NER models only with weak supervision, i.e., without
any human annotation, and shows that by merely using weakly labeled data, one
can achieve good performance, though still underperforms fully supervised NER
with manually/strongly labeled data. In this paper, we consider a more
practical scenario, where we have both a small amount of strongly labeled data
and a large amount of weakly labeled data. Unfortunately, we observe that
weakly labeled data does not necessarily improve, or even deteriorate the model
performance (due to the extensive noise in the weak labels) when we train deep
NER models over a simple or weighted combination of the strongly labeled and
weakly labeled data. To address this issue, we propose a new multi-stage
computational framework -- NEEDLE with three essential ingredients: (1) weak
label completion, (2) noise-aware loss function, and (3) final fine-tuning over
the strongly labeled data. Through experiments on E-commerce query NER and
Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise
of the weak labels and outperforms existing methods. In particular, we achieve
new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,
BC5CDR-disease 90.69, NCBI-disease 92.28.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Danqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tianyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Bing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Costs and Benefits of Wasserstein Fair Regression. (arXiv:2106.08812v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08812</id>
        <link href="http://arxiv.org/abs/2106.08812"/>
        <updated>2021-06-17T01:58:44.313Z</updated>
        <summary type="html"><![CDATA[Real-world applications of machine learning tools in high-stakes domains are
often regulated to be fair, in the sense that the predicted target should
satisfy some quantitative notion of parity with respect to a protected
attribute. However, the exact tradeoff between fairness and accuracy with a
real-valued target is not clear. In this paper, we characterize the inherent
tradeoff between statistical parity and accuracy in the regression setting by
providing a lower bound on the error of any fair regressor. Our lower bound is
sharp, algorithm-independent, and admits a simple interpretation: when the
moments of the target differ between groups, any fair algorithm has to make a
large error on at least one of the groups. We further extend this result to
give a lower bound on the joint error of any (approximately) fair algorithm,
using the Wasserstein distance to measure the quality of the approximation. On
the upside, we establish the first connection between individual fairness,
accuracy parity, and the Wasserstein distance by showing that if a regressor is
individually fair, it also approximately verifies the accuracy parity, where
the gap is given by the Wasserstein distance between the two groups. Inspired
by our theoretical results, we develop a practical algorithm for fair
regression through the lens of representation learning, and conduct experiments
on a real-world dataset to corroborate our findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Circa: Stochastic ReLUs for Private Deep Learning. (arXiv:2106.08475v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08475</id>
        <link href="http://arxiv.org/abs/2106.08475"/>
        <updated>2021-06-17T01:58:44.271Z</updated>
        <summary type="html"><![CDATA[The simultaneous rise of machine learning as a service and concerns over user
privacy have increasingly motivated the need for private inference (PI). While
recent work demonstrates PI is possible using cryptographic primitives, the
computational overheads render it impractical. The community is largely
unprepared to address these overheads, as the source of slowdown in PI stems
from the ReLU operator whereas optimizations for plaintext inference focus on
optimizing FLOPs. In this paper we re-think the ReLU computation and propose
optimizations for PI tailored to properties of neural networks. Specifically,
we reformulate ReLU as an approximate sign test and introduce a novel
truncation method for the sign test that significantly reduces the cost per
ReLU. These optimizations result in a specific type of stochastic ReLU. The key
observation is that the stochastic fault behavior is well suited for the
fault-tolerant properties of neural network inference. Thus, we provide
significant savings without impacting accuracy. We collectively call the
optimizations Circa and demonstrate improvements of up to 4.7x storage and 3x
runtime over baseline implementations; we further show that Circa can be used
on top of recent PI optimizations to obtain 1.8x additional speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghodsi_Z/0/1/0/all/0/1"&gt;Zahra Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1"&gt;Nandan Kumar Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reagen_B/0/1/0/all/0/1"&gt;Brandon Reagen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Siddharth Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automating Augmentation Through Random Unidimensional Search. (arXiv:2106.08756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08756</id>
        <link href="http://arxiv.org/abs/2106.08756"/>
        <updated>2021-06-17T01:58:44.265Z</updated>
        <summary type="html"><![CDATA[It is no secret amongst deep learning researchers that finding the right data
augmentation strategy during training can mean the difference between a
state-of-the-art result and a run-of-the-mill ranking. To that end, the
community has seen many efforts to automate the process of finding the perfect
augmentation procedure for any task at hand. Unfortunately, even recent
cutting-edge methods bring massive computational overhead, requiring as many as
100 full model trainings to settle on an ideal configuration. We show how to
achieve even better performance in just 7: with Random Unidimensional
Augmentation. Source code is available at https://github.com/fastestimator/RUA]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaomeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1"&gt;Michael Potter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1"&gt;Gaurav Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yun-Chan Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saripalli_V/0/1/0/all/0/1"&gt;V. Ratna Saripalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning-based analysis of hyperspectral images for automated sepsis diagnosis. (arXiv:2106.08445v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08445</id>
        <link href="http://arxiv.org/abs/2106.08445"/>
        <updated>2021-06-17T01:58:44.256Z</updated>
        <summary type="html"><![CDATA[Sepsis is a leading cause of mortality and critical illness worldwide. While
robust biomarkers for early diagnosis are still missing, recent work indicates
that hyperspectral imaging (HSI) has the potential to overcome this bottleneck
by monitoring microcirculatory alterations. Automated machine learning-based
diagnosis of sepsis based on HSI data, however, has not been explored to date.
Given this gap in the literature, we leveraged an existing data set to (1)
investigate whether HSI-based automated diagnosis of sepsis is possible and (2)
put forth a list of possible confounders relevant for HSI-based tissue
classification. While we were able to classify sepsis with an accuracy of over
$98\,\%$ using the existing data, our research also revealed several subject-,
therapy- and imaging-related confounders that may lead to an overestimation of
algorithm performance when not balanced across the patient groups. We conclude
that further prospective studies, carefully designed with respect to these
confounders, are necessary to confirm the preliminary results obtained in this
study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dietrich_M/0/1/0/all/0/1"&gt;Maximilian Dietrich&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Seidlitz_S/0/1/0/all/0/1"&gt;Silvia Seidlitz&lt;/a&gt; (2, 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Schreck_N/0/1/0/all/0/1"&gt;Nicholas Schreck&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Wiesenfarth_M/0/1/0/all/0/1"&gt;Manuel Wiesenfarth&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Godau_P/0/1/0/all/0/1"&gt;Patrick Godau&lt;/a&gt; (2, 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1"&gt;Minu Tizabi&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Sellner_J/0/1/0/all/0/1"&gt;Jan Sellner&lt;/a&gt; (2, 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Marx_S/0/1/0/all/0/1"&gt;Sebastian Marx&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Knodler_S/0/1/0/all/0/1"&gt;Samuel Kn&amp;#xf6;dler&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Allers_M/0/1/0/all/0/1"&gt;Michael M. Allers&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Ayala_L/0/1/0/all/0/1"&gt;Leonardo Ayala&lt;/a&gt; (2, 7), &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_K/0/1/0/all/0/1"&gt;Karsten Schmidt&lt;/a&gt; (8), &lt;a href="http://arxiv.org/find/cs/1/au:+Brenner_T/0/1/0/all/0/1"&gt;Thorsten Brenner&lt;/a&gt; (8), &lt;a href="http://arxiv.org/find/cs/1/au:+Studier_Fischer_A/0/1/0/all/0/1"&gt;Alexander Studier-Fischer&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Nickel_F/0/1/0/all/0/1"&gt;Felix Nickel&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1"&gt;Beat P. M&amp;#xfc;ller-Stich&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Kopp_Schneider_A/0/1/0/all/0/1"&gt;Annette Kopp-Schneider&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Weigand_M/0/1/0/all/0/1"&gt;Markus A. Weigand&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt; (2, 6, 7) ((1) Department of Anesthesiology, Heidelberg University Hospital, Heidelberg, Germany, (2) Division of Computer Assisted Medical Interventions, German Cancer Research Center (DKFZ), Heidelberg, Germany, (3) HIDSS4Health - Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany (4) Division of Biostatistics, German Cancer Research Center (DKFZ), Heidelberg, Germany, (5) Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany, (6) Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany, (7) Medical Faculty, Heidelberg University, Heidelberg, Germany, (8) Department of Anesthesiology and Intensive Care Medicine, University Hospital Essen, University Duisburg-Essen, Essen, Germany)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cardiovascular Disease Prediction using Recursive Feature Elimination and Gradient Boosting Classification Techniques. (arXiv:2106.08889v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08889</id>
        <link href="http://arxiv.org/abs/2106.08889"/>
        <updated>2021-06-17T01:58:44.248Z</updated>
        <summary type="html"><![CDATA[Cardiovascular diseases (CVDs) are one of the most common chronic illnesses
that affect peoples health. Early detection of CVDs can reduce mortality rates
by preventing or reducing the severity of the disease. Machine learning
algorithms are a promising method for identifying risk factors. This paper
proposes a proposed recursive feature elimination-based gradient boosting
(RFE-GB) algorithm in order to obtain accurate heart disease prediction. The
patients health record with important CVD features has been analyzed for the
evaluation of the results. Several other machine learning methods were also
used to build the prediction model, and the results were compared with the
proposed model. The results of this proposed model infer that the combined
recursive feature elimination and gradient boosting algorithm achieves the
highest accuracy (89.7 %). Further, with an area under the curve of 0.84, the
proposed RFE-GB algorithm was found superior and had obtained a substantial
gain over other techniques. Thus, the proposed RFE-GB algorithm will serve as a
prominent model for CVD estimation and treatment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Theerthagiri_P/0/1/0/all/0/1"&gt;Prasannavenkatesan Theerthagiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+J_V/0/1/0/all/0/1"&gt;Vidya J&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Continuous Control with Episodic Memory. (arXiv:2106.08832v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08832</id>
        <link href="http://arxiv.org/abs/2106.08832"/>
        <updated>2021-06-17T01:58:44.243Z</updated>
        <summary type="html"><![CDATA[Episodic memory lets reinforcement learning algorithms remember and exploit
promising experience from the past to improve agent performance. Previous works
on memory mechanisms show benefits of using episodic-based data structures for
discrete action problems in terms of sample-efficiency. The application of
episodic memory for continuous control with a large action space is not
trivial. Our study aims to answer the question: can episodic memory be used to
improve agent's performance in continuous control? Our proposed algorithm
combines episodic memory with Actor-Critic architecture by modifying critic's
objective. We further improve performance by introducing episodic-based replay
buffer prioritization. We evaluate our algorithm on OpenAI gym domains and show
greater sample-efficiency compared with the state-of-the art model-free
off-policy algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1"&gt;Igor Kuznetsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1"&gt;Andrey Filchenkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Training in High Dimensions via Block Coordinate Geometric Median Descent. (arXiv:2106.08882v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08882</id>
        <link href="http://arxiv.org/abs/2106.08882"/>
        <updated>2021-06-17T01:58:44.227Z</updated>
        <summary type="html"><![CDATA[Geometric median (\textsc{Gm}) is a classical method in statistics for
achieving a robust estimation of the uncorrupted data; under gross corruption,
it achieves the optimal breakdown point of 0.5. However, its computational
complexity makes it infeasible for robustifying stochastic gradient descent
(SGD) for high-dimensional optimization problems. In this paper, we show that
by applying \textsc{Gm} to only a judiciously chosen block of coordinates at a
time and using a memory mechanism, one can retain the breakdown point of 0.5
for smooth non-convex problems, with non-asymptotic convergence rates
comparable to the SGD with \textsc{Gm}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1"&gt;Anish Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1"&gt;Abolfazl Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1"&gt;Sujay Sanghavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Input Invex Neural Network. (arXiv:2106.08748v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08748</id>
        <link href="http://arxiv.org/abs/2106.08748"/>
        <updated>2021-06-17T01:58:44.221Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel method to constrain invexity on Neural
Networks (NN). Invex functions ensure every stationary point is global minima.
Hence, gradient descent commenced from any point will lead to the global
minima. Another advantage of invexity on NN is to divide data space locally
into two connected sets with a highly non-linear decision boundary by simply
thresholding the output. To this end, we formulate a universal invex function
approximator and employ it to enforce invexity in NN. We call it Input Invex
Neural Networks (II-NN). We first fit data with a known invex function,
followed by modification with a NN, compare the direction of the gradient and
penalize the direction of gradient on NN if it contradicts with the direction
of reference invex function. In order to penalize the direction of the gradient
we perform Gradient Clipped Gradient Penalty (GC-GP). We applied our method to
the existing NNs for both image classification and regression tasks. From the
extensive empirical and qualitative experiments, we observe that our method
gives the performance similar to ordinary NN yet having invexity. Our method
outperforms linear NN and Input Convex Neural Network (ICNN) with a large
margin. We publish our code and implementation details at github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sapkota_S/0/1/0/all/0/1"&gt;Suman Sapkota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1"&gt;Binod Bhattarai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridge Networks. (arXiv:2106.08446v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08446</id>
        <link href="http://arxiv.org/abs/2106.08446"/>
        <updated>2021-06-17T01:58:44.216Z</updated>
        <summary type="html"><![CDATA[Despite rapid progress, current deep learning methods face a number of
critical challenges. These include high energy consumption, catastrophic
forgetting, dependance on global losses, and an inability to reason
symbolically. By combining concepts from information bottleneck theory and
vector-symbolic architectures, we propose and implement a novel information
processing architecture, the 'Bridge network.' We show this architecture
provides unique advantages which can address the problem of global losses and
catastrophic forgetting. Furthermore, we argue that it provides a further basis
for increasing energy efficiency of execution and the ability to reason
symbolically.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olin_Ammentorp_W/0/1/0/all/0/1"&gt;Wilkie Olin-Ammentorp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bazhenov_M/0/1/0/all/0/1"&gt;Maxim Bazhenov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased Methods for Multi-Goal Reinforcement Learning. (arXiv:2106.08863v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08863</id>
        <link href="http://arxiv.org/abs/2106.08863"/>
        <updated>2021-06-17T01:58:44.210Z</updated>
        <summary type="html"><![CDATA[In multi-goal reinforcement learning (RL) settings, the reward for each goal
is sparse, and located in a small neighborhood of the goal. In large dimension,
the probability of reaching a reward vanishes and the agent receives little
learning signal. Methods such as Hindsight Experience Replay (HER) tackle this
issue by also learning from realized but unplanned-for goals. But HER is known
to introduce bias, and can converge to low-return policies by overestimating
chancy outcomes. First, we vindicate HER by proving that it is actually
unbiased in deterministic environments, such as many optimal control settings.
Next, for stochastic environments in continuous spaces, we tackle sparse
rewards by directly taking the infinitely sparse reward limit. We fully
formalize the problem of multi-goal RL with infinitely sparse Dirac rewards at
each goal. We introduce unbiased deep Q-learning and actor-critic algorithms
that can handle such infinitely sparse rewards, and test them in toy
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blier_L/0/1/0/all/0/1"&gt;L&amp;#xe9;onard Blier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ollivier_Y/0/1/0/all/0/1"&gt;Yann Ollivier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous Training of Partially Masked Neural Networks. (arXiv:2106.08895v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08895</id>
        <link href="http://arxiv.org/abs/2106.08895"/>
        <updated>2021-06-17T01:58:44.205Z</updated>
        <summary type="html"><![CDATA[For deploying deep learning models to lower end devices, it is necessary to
train less resource-demanding variants of state-of-the-art architectures. This
does not eliminate the need for more expensive models as they have a higher
performance. In order to avoid training two separate models, we show that it is
possible to train neural networks in such a way that a predefined 'core'
subnetwork can be split-off from the trained full network with remarkable good
performance. We extend on prior methods that focused only on core networks of
smaller width, while we focus on supporting arbitrary core network
architectures. Our proposed training scheme switches consecutively between
optimizing only the core part of the network and the full one. The accuracy of
the full model remains comparable, while the core network achieves better
performance than when it is trained in isolation. In particular, we show that
training a Transformer with a low-rank core gives a low-rank model with
superior performance than when training the low-rank model alone. We analyze
our training scheme theoretically, and show its convergence under assumptions
that are either standard or practically justified. Moreover, we show that the
developed theoretical framework allows analyzing many other partial training
schemes for neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohtashami_A/0/1/0/all/0/1"&gt;Amirkeivan Mohtashami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Markovian Bandits: Is Posterior Sampling more Scalable than Optimism?. (arXiv:2106.08771v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08771</id>
        <link href="http://arxiv.org/abs/2106.08771"/>
        <updated>2021-06-17T01:58:44.190Z</updated>
        <summary type="html"><![CDATA[We study learning algorithms for the classical Markovian bandit problem with
discount. We explain how to adapt PSRL [24] and UCRL2 [2] to exploit the
problem structure. These variants are called MB-PSRL and MB-UCRL2. While the
regret bound and runtime of vanilla implementations of PSRL and UCRL2 are
exponential in the number of bandits, we show that the episodic regret of
MB-PSRL and MB-UCRL2 is(S $\sqrt$ nK) where K is the number of episodes, n is
the number of bandits and S is the number of states of each bandit (the exact
bound in S, n and K is given in the paper). Up to a factor $\sqrt$ S, this
matches the lower bound of $\Omega$($\sqrt$ SnK) that we also derive in the
paper. MB-PSRL is also computationally efficient: its runtime is linear in the
number of bandits. We further show that this linear runtime cannot be achieved
by adapting classical non-Bayesian algorithms such as UCRL2 or UCBVI to
Markovian bandit problems. Finally, we perform numerical experiments that
confirm that MB-PSRL outperforms other existing algorithms in practice, both in
terms of regret and of computation time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gast_N/0/1/0/all/0/1"&gt;Nicolas Gast&lt;/a&gt; (POLARIS), &lt;a href="http://arxiv.org/find/cs/1/au:+Gaujal_B/0/1/0/all/0/1"&gt;Bruno Gaujal&lt;/a&gt; (POLARIS), &lt;a href="http://arxiv.org/find/cs/1/au:+Khun_K/0/1/0/all/0/1"&gt;Kimang Khun&lt;/a&gt; (POLARIS)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Rhythm Style Transfer Without Text Transcriptions. (arXiv:2106.08519v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.08519</id>
        <link href="http://arxiv.org/abs/2106.08519"/>
        <updated>2021-06-17T01:58:44.183Z</updated>
        <summary type="html"><![CDATA[Prosody plays an important role in characterizing the style of a speaker or
an emotion, but most non-parallel voice or emotion style transfer algorithms do
not convert any prosody information. Two major components of prosody are pitch
and rhythm. Disentangling the prosody information, particularly the rhythm
component, from the speech is challenging because it involves breaking the
synchrony between the input speech and the disentangled speech representation.
As a result, most existing prosody style transfer algorithms would need to rely
on some form of text transcriptions to identify the content information, which
confines their application to high-resource languages only. Recently,
SpeechSplit has made sizeable progress towards unsupervised prosody style
transfer, but it is unable to extract high-level global prosody style in an
unsupervised manner. In this paper, we propose AutoPST, which can disentangle
global prosody style from speech without relying on any text transcriptions.
AutoPST is an Autoencoder-based Prosody Style Transfer framework with a
thorough rhythm removal module guided by the self-expressive representation
learning. Experiments on different style transfer tasks show that AutoPST can
effectively convert prosody that correctly reflects the styles of the target
domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Qian_K/0/1/0/all/0/1"&gt;Kaizhi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jinjun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cox_D/0/1/0/all/0/1"&gt;David Cox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1"&gt;Mark Hasegawa-Johnson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Split and Expand: An inference-time improvement for Weakly Supervised Cell Instance Segmentation. (arXiv:2007.10817v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10817</id>
        <link href="http://arxiv.org/abs/2007.10817"/>
        <updated>2021-06-17T01:58:44.177Z</updated>
        <summary type="html"><![CDATA[We consider the problem of segmenting cell nuclei instances from Hematoxylin
and Eosin (H&E) stains with dot annotations only. While most recent works focus
on improving the segmentation quality, this is usually insufficient for
instance segmentation of cell instances clustered together or with a small
size. In this work, we propose a simple two-step post-processing procedure,
Split and Expand, that directly improves the conversion of segmentation maps to
instances. In the splitting step, we generate fine-grained cell instances from
the segmentation map with the guidance of cell-center predictions. For the
expansion step, we utilize Layer-wise Relevance Propagation (LRP) explanation
results to add small cells that are not captured in the segmentation map.
Although we additionally train an output head to predict cell-centers, the
post-processing procedure itself is not explicitly trained and is executed at
inference-time only. A feature re-weighting loss based on LRP is proposed to
improve our method even further. We test our procedure on the MoNuSeg and TNBC
datasets and show quantitatively and qualitatively that our proposed method
improves object-level metrics substantially.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Foo_L/0/1/0/all/0/1"&gt;Lin Geng Foo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiamei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1"&gt;Alexander Binder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Class Classification from Single-Class Data with Confidences. (arXiv:2106.08864v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08864</id>
        <link href="http://arxiv.org/abs/2106.08864"/>
        <updated>2021-06-17T01:58:44.172Z</updated>
        <summary type="html"><![CDATA[Can we learn a multi-class classifier from only data of a single class? We
show that without any assumptions on the loss functions, models, and
optimizers, we can successfully learn a multi-class classifier from only data
of a single class with a rigorous consistency guarantee when confidences (i.e.,
the class-posterior probabilities for all the classes) are available.
Specifically, we propose an empirical risk minimization framework that is
loss-/model-/optimizer-independent. Instead of constructing a boundary between
the given class and other classes, our method can conduct discriminative
classification between all the classes even if no data from the other classes
are provided. We further theoretically and experimentally show that our method
can be Bayes-consistent with a simple modification even if the provided
confidences are highly noisy. Then, we provide an extension of our method for
the case where data from a subset of all the classes are available.
Experimental results demonstrate the effectiveness of our methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yuzhou Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_S/0/1/0/all/0/1"&gt;Senlin Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yitian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1"&gt;Bo An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the long-term learning ability of LSTM LMs. (arXiv:2106.08927v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08927</id>
        <link href="http://arxiv.org/abs/2106.08927"/>
        <updated>2021-06-17T01:58:44.166Z</updated>
        <summary type="html"><![CDATA[We inspect the long-term learning ability of Long Short-Term Memory language
models (LSTM LMs) by evaluating a contextual extension based on the Continuous
Bag-of-Words (CBOW) model for both sentence- and discourse-level LSTM LMs and
by analyzing its performance. We evaluate on text and speech. Sentence-level
models using the long-term contextual module perform comparably to vanilla
discourse-level LSTM LMs. On the other hand, the extension does not provide
gains for discourse-level models. These findings indicate that discourse-level
LSTM LMs already rely on contextual information to perform long-term learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boes_W/0/1/0/all/0/1"&gt;Wim Boes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rompaey_R/0/1/0/all/0/1"&gt;Robbe Van Rompaey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verwimp_L/0/1/0/all/0/1"&gt;Lyan Verwimp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelemans_J/0/1/0/all/0/1"&gt;Joris Pelemans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+hamme_H/0/1/0/all/0/1"&gt;Hugo Van hamme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wambacq_P/0/1/0/all/0/1"&gt;Patrick Wambacq&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections. (arXiv:2106.08908v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08908</id>
        <link href="http://arxiv.org/abs/2106.08908"/>
        <updated>2021-06-17T01:58:44.150Z</updated>
        <summary type="html"><![CDATA[Question answering (QA) systems for large document collections typically use
pipelines that (i) retrieve possibly relevant documents, (ii) re-rank them,
(iii) rank paragraphs or other snippets of the top-ranked documents, and (iv)
select spans of the top-ranked snippets as exact answers. Pipelines are
conceptually simple, but errors propagate from one component to the next,
without later components being able to revise earlier decisions. We present an
architecture for joint document and snippet ranking, the two middle stages,
which leverages the intuition that relevant documents have good snippets and
good snippets come from relevant documents. The architecture is general and can
be used with any neural text relevance ranker. We experiment with two main
instantiations of the architecture, based on POSIT-DRMM (PDRMM) and a
BERT-based ranker. Experiments on biomedical data from BIOASQ show that our
joint models vastly outperform the pipelines in snippet retrieval, the main
goal for QA, with fewer trainable parameters, also remaining competitive in
document retrieval. Furthermore, our joint PDRMM-based model is competitive
with BERT-based models, despite using orders of magnitude fewer parameters.
These claims are also supported by human evaluation on two test batches of
BIOASQ. To test our key findings on another dataset, we modified the Natural
Questions dataset so that it can also be used for document and snippet
retrieval. Our joint PDRMM-based model again outperforms the corresponding
pipeline in snippet retrieval on the modified Natural Questions dataset, even
though it performs worse than the pipeline in document retrieval. We make our
code and the modified Natural Questions dataset publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pappas_D/0/1/0/all/0/1"&gt;Dimitris Pappas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1"&gt;Ion Androutsopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covariance-based smoothed particle hydrodynamics. A machine-learning application to simulating disc fragmentation. (arXiv:2106.08870v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2106.08870</id>
        <link href="http://arxiv.org/abs/2106.08870"/>
        <updated>2021-06-17T01:58:44.144Z</updated>
        <summary type="html"><![CDATA[A PCA-based, machine learning version of the SPH method is proposed. In the
present scheme, the smoothing tensor is computed to have their eigenvalues
proportional to the covariance's principal components, using a modified octree
data structure, which allows the fast estimation of the anisotropic
self-regulating kNN. Each SPH particle is the center of such an optimal kNN
cluster, i.e., the one whose covariance tensor allows the find of the kNN
cluster itself according to the Mahalanobis metric. Such machine learning
constitutes a fixed point problem. The definitive (self-regulating) kNN cluster
defines the smoothing volume, or properly saying, the smoothing ellipsoid,
required to perform the anisotropic interpolation. Thus, the smoothing kernel
has an ellipsoidal profile, which changes how the kernel gradients are
computed. As an application, it was performed the simulation of collapse and
fragmentation of a non-magnetic, rotating gaseous sphere. An interesting
outcome was the formation of protostars in the disc fragmentation, shown to be
much more persistent and much more abundant in the anisotropic simulation than
in the isotropic case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Marinho_E/0/1/0/all/0/1"&gt;Eraldo Pereira Marinho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding Spatio-Temporal Language with Transformers. (arXiv:2106.08858v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.08858</id>
        <link href="http://arxiv.org/abs/2106.08858"/>
        <updated>2021-06-17T01:58:44.138Z</updated>
        <summary type="html"><![CDATA[Language is an interface to the outside world. In order for embodied agents
to use it, language must be grounded in other, sensorimotor modalities. While
there is an extended literature studying how machines can learn grounded
language, the topic of how to learn spatio-temporal linguistic concepts is
still largely uncharted. To make progress in this direction, we here introduce
a novel spatio-temporal language grounding task where the goal is to learn the
meaning of spatio-temporal descriptions of behavioral traces of an embodied
agent. This is achieved by training a truth function that predicts if a
description matches a given history of observations. The descriptions involve
time-extended predicates in past and present tense as well as spatio-temporal
references to objects in the scene. To study the role of architectural biases
in this task, we train several models including multimodal Transformer
architectures; the latter implement different attention computations between
words and objects across space and time. We test models on two classes of
generalization: 1) generalization to randomly held-out sentences; 2)
generalization to grammar primitives. We observe that maintaining object
identity in the attention computation of our Transformers is instrumental to
achieving good performance on generalization overall, and that summarizing
object traces in a single token has little influence on performance. We then
discuss how this opens new perspectives for language-guided autonomous embodied
agents. We also release our code under open-source license as well as
pretrained models and datasets to encourage the wider community to build upon
and extend our work in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1"&gt;Tristan Karch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1"&gt;Laetitia Teodorescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1"&gt;Katja Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Moulin-Frier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1"&gt;Pierre-Yves Oudeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scene Transformer: A unified multi-task model for behavior prediction and planning. (arXiv:2106.08417v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08417</id>
        <link href="http://arxiv.org/abs/2106.08417"/>
        <updated>2021-06-17T01:58:44.132Z</updated>
        <summary type="html"><![CDATA[Predicting the future motion of multiple agents is necessary for planning in
dynamic environments. This task is challenging for autonomous driving since
agents (e.g., vehicles and pedestrians) and their associated behaviors may be
diverse and influence each other. Most prior work has focused on first
predicting independent futures for each agent based on all past motion, and
then planning against these independent predictions. However, planning against
fixed predictions can suffer from the inability to represent the future
interaction possibilities between different agents, leading to sub-optimal
planning. In this work, we formulate a model for predicting the behavior of all
agents jointly in real-world driving environments in a unified manner. Inspired
by recent language modeling approaches, we use a masking strategy as the query
to our model, enabling one to invoke a single model to predict agent behavior
in many ways, such as potentially conditioned on the goal or full future
trajectory of the autonomous vehicle or the behavior of other agents in the
environment. Our model architecture fuses heterogeneous world state in a
unified Transformer architecture by employing attention across road elements,
agent interactions and time steps. We evaluate our approach on autonomous
driving datasets for behavior prediction, and achieve state-of-the-art
performance. Our work demonstrates that formulating the problem of behavior
prediction in a unified architecture with a masking strategy may allow us to
have a single model that can perform multiple motion prediction and planning
related tasks effectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngiam_J/0/1/0/all/0/1"&gt;Jiquan Ngiam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1"&gt;Benjamin Caine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1"&gt;Vijay Vasudevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1"&gt;Hao-Tien Lewis Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1"&gt;Jeffrey Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1"&gt;Rebecca Roelofs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1"&gt;Alex Bewley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chenxi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venugopal_A/0/1/0/all/0/1"&gt;Ashish Venugopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_D/0/1/0/all/0/1"&gt;David Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1"&gt;Ben Sapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhifeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1"&gt;Jonathon Shlens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithm to Compilation Codesign: An Integrated View of Neural Network Sparsity. (arXiv:2106.08846v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08846</id>
        <link href="http://arxiv.org/abs/2106.08846"/>
        <updated>2021-06-17T01:58:44.126Z</updated>
        <summary type="html"><![CDATA[Reducing computation cost, inference latency, and memory footprint of neural
networks are frequently cited as research motivations for pruning and sparsity.
However, operationalizing those benefits and understanding the end-to-end
effect of algorithm design and regularization on the runtime execution is not
often examined in depth.

Here we apply structured and unstructured pruning to attention weights of
transformer blocks of the BERT language model, while also expanding block
sparse representation (BSR) operations in the TVM compiler. Integration of BSR
operations enables the TVM runtime execution to leverage structured pattern
sparsity induced by model regularization.

This integrated view of pruning algorithms enables us to study relationships
between modeling decisions and their direct impact on sparsity-enhanced
execution. Our main findings are: 1) we validate that performance benefits of
structured sparsity block regularization must be enabled by the BSR
augmentations to TVM, with 4x speedup relative to vanilla PyTorch and 2.2x
speedup relative to standard TVM compilation (without expanded BSR support). 2)
for BERT attention weights, the end-to-end optimal block sparsity shape in this
CPU inference context is not a square block (as in \cite{gray2017gpu}) but
rather a linear 32x1 block 3) the relationship between performance and block
size / shape is is suggestive of how model regularization parameters interact
with task scheduler optimizations resulting in the observed end-to-end
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1"&gt;Fu-Ming Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1"&gt;Austin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering Mixture Models in Almost-Linear Time via List-Decodable Mean Estimation. (arXiv:2106.08537v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.08537</id>
        <link href="http://arxiv.org/abs/2106.08537"/>
        <updated>2021-06-17T01:58:44.118Z</updated>
        <summary type="html"><![CDATA[We study the problem of list-decodable mean estimation, where an adversary
can corrupt a majority of the dataset. Specifically, we are given a set $T$ of
$n$ points in $\mathbb{R}^d$ and a parameter $0< \alpha <\frac 1 2$ such that
an $\alpha$-fraction of the points in $T$ are i.i.d. samples from a
well-behaved distribution $\mathcal{D}$ and the remaining $(1-\alpha)$-fraction
of the points are arbitrary. The goal is to output a small list of vectors at
least one of which is close to the mean of $\mathcal{D}$. As our main
contribution, we develop new algorithms for list-decodable mean estimation,
achieving nearly-optimal statistical guarantees, with running time $n^{1 +
o(1)} d$. All prior algorithms for this problem had additional polynomial
factors in $\frac 1 \alpha$. As a corollary, we obtain the first almost-linear
time algorithms for clustering mixtures of $k$ separated well-behaved
distributions, nearly-matching the statistical guarantees of spectral methods.
Prior clustering algorithms inherently relied on an application of $k$-PCA,
thereby incurring runtimes of $\Omega(n d k)$. This marks the first runtime
improvement for this basic statistical problem in nearly two decades.

The starting point of our approach is a novel and simpler near-linear time
robust mean estimation algorithm in the $\alpha \to 1$ regime, based on a
one-shot matrix multiplicative weights-inspired potential decrease. We
crucially leverage this new algorithmic framework in the context of the
iterative multi-filtering technique of Diakonikolas et. al. '18, '20, providing
a method to simultaneously cluster and downsample points using one-dimensional
projections --- thus, bypassing the $k$-PCA subroutines required by prior
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1"&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1"&gt;Daniel M. Kane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kongsgaard_D/0/1/0/all/0/1"&gt;Daniel Kongsgaard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jerry Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1"&gt;Kevin Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating the Robustness of Public Transport Systems Using Machine Learning. (arXiv:2106.08967v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08967</id>
        <link href="http://arxiv.org/abs/2106.08967"/>
        <updated>2021-06-17T01:58:44.102Z</updated>
        <summary type="html"><![CDATA[The planning of attractive and cost efficient public transport systems is a
highly complex optimization process involving many steps. Integrating
robustness from a passenger's point of view makes the task even more
challenging. With numerous different definitions of robustness in literature, a
real-world acceptable evaluation of the robustness of a public transport system
is to simulate its performance under a large number of possible scenarios.
Unfortunately, this is computationally very expensive. In this paper, we
therefore explore a new way of such a scenario-based robustness approximation
by using methods from machine learning. We achieve a fast approach with a very
high accuracy by gathering a subset of key features of a public transport
system and its passenger demand and training an artificial neural network to
learn the outcome of a given set of robustness tests. The network is then able
to predict the robustness of untrained instances with high accuracy using only
its key features, allowing for a robustness oracle for transport planners that
approximates the robustness in constant time. Such an oracle can be used as
black box to increase the robustness within a local search framework for
integrated public transportation planning. In computational experiments with
different benchmark instances we demonstrate an excellent quality of our
predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Hannemann_M/0/1/0/all/0/1"&gt;Matthias M&amp;#xfc;ller-Hannemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruckert_R/0/1/0/all/0/1"&gt;Ralf R&amp;#xfc;ckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiewe_A/0/1/0/all/0/1"&gt;Alexander Schiewe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schobel_A/0/1/0/all/0/1"&gt;Anita Sch&amp;#xf6;bel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ParticleAugment: Sampling-Based Data Augmentation. (arXiv:2106.08693v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08693</id>
        <link href="http://arxiv.org/abs/2106.08693"/>
        <updated>2021-06-17T01:58:44.096Z</updated>
        <summary type="html"><![CDATA[We present an automated data augmentation approach for image classification.
We formulate the problem as Monte Carlo sampling where our goal is to
approximate the optimal augmentation policies. We propose a particle filtering
formulation to find optimal augmentation policies and their schedules during
model training. Our performance measurement procedure relies on a validation
subset of our training set, while the policy transition model depends on a
Gaussian prior and an optional augmentation velocity parameter. In our
experiments, we show that our formulation for automated augmentation reaches
promising results on CIFAR-10, CIFAR-100, and ImageNet datasets using the
standard network architectures for this problem. By comparing with the related
work, we also show that our method reaches a balance between the computational
cost of policy search and the model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsaregorodtsev_A/0/1/0/all/0/1"&gt;Alexander Tsaregorodtsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1"&gt;Vasileios Belagiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Probabilistic Circuits for Nonparametric Multi-Output Regression. (arXiv:2106.08687v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08687</id>
        <link href="http://arxiv.org/abs/2106.08687"/>
        <updated>2021-06-17T01:58:44.089Z</updated>
        <summary type="html"><![CDATA[Inspired by recent advances in the field of expert-based approximations of
Gaussian processes (GPs), we present an expert-based approach to large-scale
multi-output regression using single-output GP experts. Employing a deeply
structured mixture of single-output GPs encoded via a probabilistic circuit
allows us to capture correlations between multiple output dimensions
accurately. By recursively partitioning the covariate space and the output
space, posterior inference in our model reduces to inference on single-output
GP experts, which only need to be conditioned on a small subset of the
observations. We show that inference can be performed exactly and efficiently
in our model, that it can capture correlations between output dimensions and,
hence, often outperforms approaches that do not incorporate inter-output
correlations, as demonstrated on several data sets in terms of the negative log
predictive density.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhongjie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Mingye Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1"&gt;Martin Trapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skryagin_A/0/1/0/all/0/1"&gt;Arseny Skryagin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maxmin-Fair Ranking: Individual Fairness under Group-Fairness Constraints. (arXiv:2106.08652v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08652</id>
        <link href="http://arxiv.org/abs/2106.08652"/>
        <updated>2021-06-17T01:58:44.072Z</updated>
        <summary type="html"><![CDATA[We study a novel problem of fairness in ranking aimed at minimizing the
amount of individual unfairness introduced when enforcing group-fairness
constraints. Our proposal is rooted in the distributional maxmin fairness
theory, which uses randomization to maximize the expected satisfaction of the
worst-off individuals. We devise an exact polynomial-time algorithm to find
maxmin-fair distributions of general search problems (including, but not
limited to, ranking), and show that our algorithm can produce rankings which,
while satisfying the given group-fairness constraints, ensure that the maximum
possible value is brought to individuals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Soriano_D/0/1/0/all/0/1"&gt;David Garcia-Soriano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1"&gt;Francesco Bonchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Learning to Select High-Quality Measurements. (arXiv:2106.08891v1 [physics.data-an])]]></title>
        <id>http://arxiv.org/abs/2106.08891</id>
        <link href="http://arxiv.org/abs/2106.08891"/>
        <updated>2021-06-17T01:58:44.065Z</updated>
        <summary type="html"><![CDATA[We describe the use of machine learning algorithms to select high-quality
measurements for the Mu2e experiment. This technique is important for
experiments with backgrounds that arise due to measurement errors. The
algorithms use multiple pieces of ancillary information that are sensitive to
measurement quality to separate high-quality and low-quality measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Edmonds_A/0/1/0/all/0/1"&gt;Andrew Edmonds&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Brown_D/0/1/0/all/0/1"&gt;David Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Vinas_L/0/1/0/all/0/1"&gt;Luciano Vinas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pagan_S/0/1/0/all/0/1"&gt;Samantha Pagan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Silent Speech and Emotion Recognition from Vocal Tract Shape Dynamics in Real-Time MRI. (arXiv:2106.08706v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08706</id>
        <link href="http://arxiv.org/abs/2106.08706"/>
        <updated>2021-06-17T01:58:44.057Z</updated>
        <summary type="html"><![CDATA[Speech sounds of spoken language are obtained by varying configuration of the
articulators surrounding the vocal tract. They contain abundant information
that can be utilized to better understand the underlying mechanism of human
speech production. We propose a novel deep neural network-based learning
framework that understands acoustic information in the variable-length sequence
of vocal tract shaping during speech production, captured by real-time magnetic
resonance imaging (rtMRI), and translate it into text. The proposed framework
comprises of spatiotemporal convolutions, a recurrent network, and the
connectionist temporal classification loss, trained entirely end-to-end. On the
USC-TIMIT corpus, the model achieved a 40.6% PER at sentence-level, much better
compared to the existing models. To the best of our knowledge, this is the
first study that demonstrates the recognition of entire spoken sentence based
on an individual's articulatory motions captured by rtMRI video. We also
performed an analysis of variations in the geometry of articulation in each
sub-regions of the vocal tract (i.e., pharyngeal, velar and dorsal, hard
palate, labial constriction region) with respect to different emotions and
genders. Results suggest that each sub-regions distortion is affected by both
emotion and gender.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pandey_L/0/1/0/all/0/1"&gt;Laxmi Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arif_A/0/1/0/all/0/1"&gt;Ahmed Sabbir Arif&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Accounting of Differential Privacy via Characteristic Function. (arXiv:2106.08567v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08567</id>
        <link href="http://arxiv.org/abs/2106.08567"/>
        <updated>2021-06-17T01:58:44.035Z</updated>
        <summary type="html"><![CDATA[Characterizing the privacy degradation over compositions, i.e., privacy
accounting, is a fundamental topic in differential privacy (DP) with many
applications to differentially private machine learning and federated learning.

We propose a unification of recent advances (Renyi DP, privacy profiles,
$f$-DP and the PLD formalism) via the characteristic function ($\phi$-function)
of a certain ``worst-case'' privacy loss random variable.

We show that our approach allows natural adaptive composition like Renyi DP,

provides exactly tight privacy accounting like PLD, and can be (often
losslessly) converted to privacy profile and $f$-DP, thus providing
$(\epsilon,\delta)$-DP guarantees and interpretable tradeoff functions.
Algorithmically, we propose an analytical Fourier accountant that represents
the complex logarithm of $\phi$-functions symbolically and uses Gaussian
quadrature for numerical computation. On several popular DP mechanisms and
their subsampled counterparts, we demonstrate the flexibility and tightness of
our approach in theory and experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuqing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jinshuo Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence. (arXiv:2106.08710v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.08710</id>
        <link href="http://arxiv.org/abs/2106.08710"/>
        <updated>2021-06-17T01:58:44.026Z</updated>
        <summary type="html"><![CDATA[Mobile Augmented Reality (MAR) integrates computer-generated virtual objects
with physical environments for mobile devices. MAR systems enable users to
interact with MAR devices, such as smartphones and head-worn wearables, and
performs seamless transitions from the physical world to a mixed world with
digital entities. These MAR systems support user experiences by using MAR
devices to provide universal accessibility to digital contents. Over the past
20 years, a number of MAR systems have been developed, however, the studies and
design of MAR frameworks have not yet been systematically reviewed from the
perspective of user-centric design. This article presents the first effort of
surveying existing MAR frameworks (count: 37) and further discusses the latest
studies on MAR through a top-down approach: 1) MAR applications; 2) MAR
visualisation techniques adaptive to user mobility and contexts; 3) systematic
evaluation of MAR frameworks including supported platforms and corresponding
features such as tracking, feature extraction plus sensing capabilities; and 4)
underlying machine learning approaches supporting intelligent operations within
MAR systems. Finally, we summarise the development of emerging research fields,
current state-of-the-art, and discuss the important open challenges and
possible theoretical and technical directions. This survey aims to benefit both
researchers and MAR system developers alike.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jacky Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1"&gt;Kit-Yung Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1"&gt;Lik-Hang Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_P/0/1/0/all/0/1"&gt;Pan Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xiang Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation. (arXiv:2106.09017v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09017</id>
        <link href="http://arxiv.org/abs/2106.09017"/>
        <updated>2021-06-17T01:58:44.019Z</updated>
        <summary type="html"><![CDATA[Multi-task learning (MTL) aims to improve the generalization of several
related tasks by learning them jointly. As a comparison, in addition to the
joint training scheme, modern meta-learning allows unseen tasks with limited
labels during the test phase, in the hope of fast adaptation over them. Despite
the subtle difference between MTL and meta-learning in the problem formulation,
both learning paradigms share the same insight that the shared structure
between existing training tasks could lead to better generalization and
adaptation. In this paper, we take one important step further to understand the
close connection between these two learning paradigms, through both theoretical
analysis and empirical investigation. Theoretically, we first demonstrate that
MTL shares the same optimization formulation with a class of gradient-based
meta-learning (GBML) algorithms. We then prove that for over-parameterized
neural networks with sufficient depth, the learned predictive functions of MTL
and GBML are close. In particular, this result implies that the predictions
given by these two models are similar over the same unseen task. Empirically,
we corroborate our theoretical findings by showing that, with proper
implementation, MTL is competitive against state-of-the-art GBML algorithms on
a set of few-shot image classification benchmarks. Since existing GBML
algorithms often involve costly second-order bi-level optimization, our
first-order MTL method is an order of magnitude faster on large-scale datasets
such as mini-ImageNet. We believe this work could help bridge the gap between
these two learning paradigms, and provide a computationally efficient
alternative to GBML that also supports fast task adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How memory architecture affects performance and learning in simple POMDPs. (arXiv:2106.08849v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08849</id>
        <link href="http://arxiv.org/abs/2106.08849"/>
        <updated>2021-06-17T01:58:44.013Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning is made much more complex when the agent's observation
is partial or noisy. This case corresponds to a partially observable Markov
decision process (POMDP). One strategy to seek good performance in POMDPs is to
endow the agent with a finite memory, whose update is governed by the policy.
However, policy optimization is non-convex in that case and can lead to poor
training performance for random initialization. The performance can be
empirically improved by constraining the memory architecture, then sacrificing
optimality to facilitate training. Here we study this trade-off in the two-arm
bandit problem, and compare two extreme cases: (i) the random access memory
where any transitions between $M$ memory states are allowed and (ii) a fixed
memory where the agent can access its last $m$ actions and rewards. For (i),
the probability $q$ to play the worst arm is known to be exponentially small in
$M$ for the optimal policy. Our main result is to show that similar performance
can be reached for (ii) as well, despite the simplicity of the memory
architecture: using a conjecture on Gray-ordered binary necklaces, we find
policies for which $q$ is exponentially small in $2^m$ i.e. $q\sim\alpha^{2^m}$
for some $\alpha < 1$. Interestingly, we observe empirically that training from
random initialization leads to very poor results for (i), and significantly
better results for (ii).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1"&gt;Mario Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eloy_C/0/1/0/all/0/1"&gt;Christophe Eloy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1"&gt;Matthieu Wyart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Auto-regressive Variational Attention Models for Text Modeling. (arXiv:2106.08571v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08571</id>
        <link href="http://arxiv.org/abs/2106.08571"/>
        <updated>2021-06-17T01:58:44.008Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) have been widely applied for text modeling.
In practice, however, they are troubled by two challenges: information
underrepresentation and posterior collapse. The former arises as only the last
hidden state of LSTM encoder is transformed into the latent space, which is
generally insufficient to summarize the data. The latter is a long-standing
problem during the training of VAEs as the optimization is trapped to a
disastrous local optimum. In this paper, we propose Discrete Auto-regressive
Variational Attention Model (DAVAM) to address the challenges. Specifically, we
introduce an auto-regressive variational attention approach to enrich the
latent space by effectively capturing the semantic dependency from the input.
We further design discrete latent space for the variational attention and
mathematically show that our model is free from posterior collapse. Extensive
experiments on language modeling tasks demonstrate the superiority of DAVAM
against several VAE counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locality defeats the curse of dimensionality in convolutional teacher-student scenarios. (arXiv:2106.08619v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08619</id>
        <link href="http://arxiv.org/abs/2106.08619"/>
        <updated>2021-06-17T01:58:43.990Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks perform a local and translationally-invariant
treatment of the data: quantifying which of these two aspects is central to
their success remains a challenge. We study this problem within a
teacher-student framework for kernel regression, using `convolutional' kernels
inspired by the neural tangent kernel of simple convolutional architectures of
given filter size. Using heuristic methods from physics, we find in the
ridgeless case that locality is key in determining the learning curve exponent
$\beta$ (that relates the test error $\epsilon_t\sim P^{-\beta}$ to the size of
the training set $P$), whereas translational invariance is not. In particular,
if the filter size of the teacher $t$ is smaller than that of the student $s$,
$\beta$ is a function of $s$ only and does not depend on the input dimension.
We confirm our predictions on $\beta$ empirically. Theoretically, in some cases
(including when teacher and student are equal) it can be shown that this
prediction is an upper bound on performance. We conclude by proving, using a
natural universality assumption, that performing kernel regression with a ridge
that decreases with the size of the training set leads to similar learning
curve exponents to those we obtain in the ridgeless case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Favero_A/0/1/0/all/0/1"&gt;Alessandro Favero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cagnetta_F/0/1/0/all/0/1"&gt;Francesco Cagnetta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wyart_M/0/1/0/all/0/1"&gt;Matthieu Wyart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameter-free Locally Accelerated Conditional Gradients. (arXiv:2102.06806v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06806</id>
        <link href="http://arxiv.org/abs/2102.06806"/>
        <updated>2021-06-17T01:58:43.962Z</updated>
        <summary type="html"><![CDATA[Projection-free conditional gradient (CG) methods are the algorithms of
choice for constrained optimization setups in which projections are often
computationally prohibitive but linear optimization over the constraint set
remains computationally feasible. Unlike in projection-based methods, globally
accelerated convergence rates are in general unattainable for CG. However, a
very recent work on Locally accelerated CG (LaCG) has demonstrated that local
acceleration for CG is possible for many settings of interest. The main
downside of LaCG is that it requires knowledge of the smoothness and strong
convexity parameters of the objective function. We remove this limitation by
introducing a novel, Parameter-Free Locally accelerated CG (PF-LaCG) algorithm,
for which we provide rigorous convergence guarantees. Our theoretical results
are complemented by numerical experiments, which demonstrate local acceleration
and showcase the practical improvements of PF-LaCG over non-accelerated
algorithms, both in terms of iteration count and wall-clock time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Carderera_A/0/1/0/all/0/1"&gt;Alejandro Carderera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Diakonikolas_J/0/1/0/all/0/1"&gt;Jelena Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lin_C/0/1/0/all/0/1"&gt;Cheuk Yin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Pokutta_S/0/1/0/all/0/1"&gt;Sebastian Pokutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training. (arXiv:2106.08616v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08616</id>
        <link href="http://arxiv.org/abs/2106.08616"/>
        <updated>2021-06-17T01:58:43.945Z</updated>
        <summary type="html"><![CDATA[Out-of-scope intent detection is of practical importance in task-oriented
dialogue systems. Since the distribution of outlier utterances is arbitrary and
unknown in the training stage, existing methods commonly rely on strong
assumptions on data distribution such as mixture of Gaussians to make
inference, resulting in either complex multi-step training procedures or
hand-crafted rules such as confidence threshold selection for outlier
detection. In this paper, we propose a simple yet effective method to train an
out-of-scope intent classifier in a fully end-to-end manner by simulating the
test scenario in training, which requires no assumption on data distribution
and no additional post-processing or threshold setting. Specifically, we
construct a set of pseudo outliers in the training stage, by generating
synthetic outliers using inliner features via self-supervision and sampling
out-of-scope sentences from easily available open-domain datasets. The pseudo
outliers are used to train a discriminative classifier that can be directly
applied to and generalize well on the test task. We evaluate our method
extensively on four benchmark dialogue datasets and observe significant
improvements over state-of-the-art approaches. Our code has been released at
https://github.com/liam0949/DCLOOS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1"&gt;Li-Ming Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Haowen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lu Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiao-Ming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_A/0/1/0/all/0/1"&gt;Albert Y.S. Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Gender Bias in Hindi-English Machine Translation. (arXiv:2106.08680v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08680</id>
        <link href="http://arxiv.org/abs/2106.08680"/>
        <updated>2021-06-17T01:58:43.939Z</updated>
        <summary type="html"><![CDATA[With language models being deployed increasingly in the real world, it is
essential to address the issue of the fairness of their outputs. The word
embedding representations of these language models often implicitly draw
unwanted associations that form a social bias within the model. The nature of
gendered languages like Hindi, poses an additional problem to the
quantification and mitigation of bias, owing to the change in the form of the
words in the sentence, based on the gender of the subject. Additionally, there
is sparse work done in the realm of measuring and debiasing systems for Indic
languages. In our work, we attempt to evaluate and quantify the gender bias
within a Hindi-English machine translation system. We implement a modified
version of the existing TGBI metric based on the grammatical considerations for
Hindi. We also compare and contrast the resulting bias measurements across
multiple metrics for pre-trained embeddings and the ones learned by our machine
translation model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1"&gt;Gauri Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1"&gt;Krithika Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sanjay Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CODA: Constructivism Learning for Instance-Dependent Dropout Architecture Construction. (arXiv:2106.08444v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08444</id>
        <link href="http://arxiv.org/abs/2106.08444"/>
        <updated>2021-06-17T01:58:43.933Z</updated>
        <summary type="html"><![CDATA[Dropout is attracting intensive research interest in deep learning as an
efficient approach to prevent overfitting. Recently incorporating structural
information when deciding which units to drop out produced promising results
comparing to methods that ignore the structural information. However, a major
issue of the existing work is that it failed to differentiate among instances
when constructing the dropout architecture. This can be a significant
deficiency for many applications. To solve this issue, we propose
Constructivism learning for instance-dependent Dropout Architecture (CODA),
which is inspired from a philosophical theory, constructivism learning.
Specially, based on the theory we have designed a better drop out technique,
Uniform Process Mixture Models, using a Bayesian nonparametric method Uniform
process. We have evaluated our proposed method on 5 real-world datasets and
compared the performance with other state-of-the-art dropout techniques. The
experimental results demonstrated the effectiveness of CODA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoli Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Directed Graph Embeddings in Pseudo-Riemannian Manifolds. (arXiv:2106.08678v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08678</id>
        <link href="http://arxiv.org/abs/2106.08678"/>
        <updated>2021-06-17T01:58:43.880Z</updated>
        <summary type="html"><![CDATA[The inductive biases of graph representation learning algorithms are often
encoded in the background geometry of their embedding space. In this paper, we
show that general directed graphs can be effectively represented by an
embedding model that combines three components: a pseudo-Riemannian metric
structure, a non-trivial global topology, and a unique likelihood function that
explicitly incorporates a preferred direction in embedding space. We
demonstrate the representational capabilities of this method by applying it to
the task of link prediction on a series of synthetic and real directed graphs
from natural language applications and biology. In particular, we show that
low-dimensional cylindrical Minkowski and anti-de Sitter spacetimes can produce
equal or better graph representations than curved Riemannian manifolds of
higher dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sim_A/0/1/0/all/0/1"&gt;Aaron Sim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wiatrak_M/0/1/0/all/0/1"&gt;Maciej Wiatrak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Brayne_A/0/1/0/all/0/1"&gt;Angus Brayne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Creed_P/0/1/0/all/0/1"&gt;P&amp;#xe1;id&amp;#xed; Creed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Paliwal_S/0/1/0/all/0/1"&gt;Saee Paliwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spoofing Generalization: When Can't You Trust Proprietary Models?. (arXiv:2106.08393v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08393</id>
        <link href="http://arxiv.org/abs/2106.08393"/>
        <updated>2021-06-17T01:58:43.865Z</updated>
        <summary type="html"><![CDATA[In this work, we study the computational complexity of determining whether a
machine learning model that perfectly fits the training data will generalizes
to unseen data. In particular, we study the power of a malicious agent whose
goal is to construct a model g that fits its training data and nothing else,
but is indistinguishable from an accurate model f. We say that g strongly
spoofs f if no polynomial-time algorithm can tell them apart. If instead we
restrict to algorithms that run in $n^c$ time for some fixed $c$, we say that g
c-weakly spoofs f. Our main results are

1. Under cryptographic assumptions, strong spoofing is possible and 2. For
any c> 0, c-weak spoofing is possible unconditionally

While the assumption of a malicious agent is an extreme scenario (hopefully
companies training large models are not malicious), we believe that it sheds
light on the inherent difficulties of blindly trusting large proprietary models
or data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1"&gt;Ankur Moitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mossel_E/0/1/0/all/0/1"&gt;Elchanan Mossel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandon_C/0/1/0/all/0/1"&gt;Colin Sandon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Graphs for Explainable Classification of Brain Networks. (arXiv:2106.08640v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.08640</id>
        <link href="http://arxiv.org/abs/2106.08640"/>
        <updated>2021-06-17T01:58:43.845Z</updated>
        <summary type="html"><![CDATA[Training graph classifiers able to distinguish between healthy brains and
dysfunctional ones, can help identifying substructures associated to specific
cognitive phenotypes. However, the mere predictive power of the graph
classifier is of limited interest to the neuroscientists, which have plenty of
tools for the diagnosis of specific mental disorders. What matters is the
interpretation of the model, as it can provide novel insights and new
hypotheses.

In this paper we propose \emph{counterfactual graphs} as a way to produce
local post-hoc explanations of any black-box graph classifier. Given a graph
and a black-box, a counterfactual is a graph which, while having high
structural similarity with the original graph, is classified by the black-box
in a different class. We propose and empirically compare several strategies for
counterfactual graph search. Our experiments against a white-box classifier
with known optimal counterfactual, show that our methods, although heuristic,
can produce counterfactuals very close to the optimal one. Finally, we show how
to use counterfactual graphs to build global explanations correctly capturing
the behaviour of different black-box classifiers and providing interesting
insights for the neuroscientists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abrate_C/0/1/0/all/0/1"&gt;Carlo Abrate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1"&gt;Francesco Bonchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predictive Modeling of Hospital Readmission: Challenges and Solutions. (arXiv:2106.08488v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08488</id>
        <link href="http://arxiv.org/abs/2106.08488"/>
        <updated>2021-06-17T01:58:43.743Z</updated>
        <summary type="html"><![CDATA[Hospital readmission prediction is a study to learn models from historical
medical data to predict probability of a patient returning to hospital in a
certain period, 30 or 90 days, after the discharge. The motivation is to help
health providers deliver better treatment and post-discharge strategies, lower
the hospital readmission rate, and eventually reduce the medical costs. Due to
inherent complexity of diseases and healthcare ecosystems, modeling hospital
readmission is facing many challenges. By now, a variety of methods have been
developed, but existing literature fails to deliver a complete picture to
answer some fundamental questions, such as what are the main challenges and
solutions in modeling hospital readmission; what are typical features/models
used for readmission prediction; how to achieve meaningful and transparent
predictions for decision making; and what are possible conflicts when deploying
predictive approaches for real-world usages. In this paper, we systematically
review computational models for hospital readmission prediction, and propose a
taxonomy of challenges featuring four main categories: (1) data variety and
complexity; (2) data imbalance, locality and privacy; (3) model
interpretability; and (4) model implementation. The review summarizes methods
in each category, and highlights technical solutions proposed to address the
challenges. In addition, a review of datasets and resources available for
hospital readmission modeling also provides firsthand materials to support
researchers and practitioners to design new approaches for effective and
efficient hospital readmission prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuwen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xingquan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ada-BKB: Scalable Gaussian Process Optimization on Continuous Domain by Adaptive Discretization. (arXiv:2106.08598v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08598</id>
        <link href="http://arxiv.org/abs/2106.08598"/>
        <updated>2021-06-17T01:58:43.738Z</updated>
        <summary type="html"><![CDATA[Gaussian process optimization is a successful class of algorithms (e.g.
GP-UCB) to optimize a black-box function through sequential evaluations.
However, when the domain of the function is continuous, Gaussian process
optimization has to either rely on a fixed discretization of the space, or
solve a non-convex optimization subproblem at each evaluation. The first
approach can negatively affect performance, while the second one puts a heavy
computational burden on the algorithm. A third option, that only recently has
been theoretically studied, is to adaptively discretize the function domain.
Even though this approach avoids the extra non-convex optimization costs, the
overall computational complexity is still prohibitive. An algorithm such as
GP-UCB has a runtime of $O(T^4)$, where $T$ is the number of iterations. In
this paper, we introduce Ada-BKB (Adaptive Budgeted Kernelized Bandit), a
no-regret Gaussian process optimization algorithm for functions on continuous
domains, that provably runs in $O(T^2 d_\text{eff}^2)$, where $d_\text{eff}$ is
the effective dimension of the explored space, and which is typically much
smaller than $T$. We corroborate our findings with experiments on synthetic
non-convex functions and on the real-world problem of hyper-parameter
optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rando_M/0/1/0/all/0/1"&gt;Marco Rando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carratino_L/0/1/0/all/0/1"&gt;Luigi Carratino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villa_S/0/1/0/all/0/1"&gt;Silvia Villa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosasco_L/0/1/0/all/0/1"&gt;Lorenzo Rosasco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WaveNet-Based Deep Neural Networks for the Characterization of Anomalous Diffusion (WADNet). (arXiv:2106.08887v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08887</id>
        <link href="http://arxiv.org/abs/2106.08887"/>
        <updated>2021-06-17T01:58:43.631Z</updated>
        <summary type="html"><![CDATA[Anomalous diffusion, which shows a deviation of transport dynamics from the
framework of standard Brownian motion, is involved in the evolution of various
physical, chemical, biological, and economic systems. The study of such random
processes is of fundamental importance in unveiling the physical properties of
random walkers and complex systems. However, classical methods to characterize
anomalous diffusion are often disqualified for individual short trajectories,
leading to the launch of the Anomalous Diffusion (AnDi) Challenge. This
challenge aims at objectively assessing and comparing new approaches for single
trajectory characterization, with respect to three different aspects: the
inference of the anomalous diffusion exponent; the classification of the
diffusion model; and the segmentation of trajectories. In this article, to
address the inference and classification tasks in the challenge, we develop a
WaveNet-based deep neural network (WADNet) by combining a modified WaveNet
encoder with long short-term memory networks, without any prior knowledge of
anomalous diffusion. As the performance of our model has surpassed the current
1st places in the challenge leaderboard on both two tasks for all dimensions (6
subtasks), WADNet could be the part of state-of-the-art techniques to decode
the AnDi database. Our method presents a benchmark for future research, and
could accelerate the development of a versatile tool for the characterization
of anomalous diffusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dezhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1"&gt;Qiujin Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zihan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSO: Curriculum Generation using continuous optimization. (arXiv:2106.08569v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08569</id>
        <link href="http://arxiv.org/abs/2106.08569"/>
        <updated>2021-06-17T01:58:43.593Z</updated>
        <summary type="html"><![CDATA[The training of deep learning models poses vast challenges of including
parameter tuning and ordering of training data. Significant research has been
done in Curriculum learning for optimizing the sequence of training data.
Recent works have focused on using complex reinforcement learning techniques to
find the optimal data ordering strategy to maximize learning for a given
network. In this paper, we present a simple and efficient technique based on
continuous optimization. We call this new approach Training Sequence
Optimization (TSO). There are three critical components in our proposed
approach: (a) An encoder network maps/embeds training sequence into continuous
space. (b) A predictor network uses the continuous representation of a strategy
as input and predicts the accuracy for fixed network architecture. (c) A
decoder further maps a continuous representation of a strategy to the ordered
training dataset. The performance predictor and encoder enable us to perform
gradient-based optimization in the continuous space to find the embedding of
optimal training data ordering with potentially better accuracy. Experiments
show that we can gain 2AP with our generated optimal curriculum strategy over
the random strategy using the CIFAR-100 dataset and have better boosts than the
state of the art CL algorithms. We do an ablation study varying the
architecture, dataset and sample sizes showcasing our approach's robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1"&gt;Dipankar Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Mukur Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-PSD Matrix Sketching with Applications to Regression and Optimization. (arXiv:2106.08544v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08544</id>
        <link href="http://arxiv.org/abs/2106.08544"/>
        <updated>2021-06-17T01:58:43.570Z</updated>
        <summary type="html"><![CDATA[A variety of dimensionality reduction techniques have been applied for
computations involving large matrices. The underlying matrix is randomly
compressed into a smaller one, while approximately retaining many of its
original properties. As a result, much of the expensive computation can be
performed on the small matrix. The sketching of positive semidefinite (PSD)
matrices is well understood, but there are many applications where the related
matrices are not PSD, including Hessian matrices in non-convex optimization and
covariance matrices in regression applications involving complex numbers. In
this paper, we present novel dimensionality reduction methods for non-PSD
matrices, as well as their ``square-roots", which involve matrices with complex
entries. We show how these techniques can be used for multiple downstream
tasks. In particular, we show how to use the proposed matrix sketching
techniques for both convex and non-convex optimization, $\ell_p$-regression for
every $1 \leq p \leq \infty$, and vector-matrix-vector queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhili Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roosta_F/0/1/0/all/0/1"&gt;Fred Roosta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dataset Dynamics via Gradient Flows in Probability Space. (arXiv:2010.12760v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12760</id>
        <link href="http://arxiv.org/abs/2010.12760"/>
        <updated>2021-06-17T01:58:43.563Z</updated>
        <summary type="html"><![CDATA[Various machine learning tasks, from generative modeling to domain
adaptation, revolve around the concept of dataset transformation and
manipulation. While various methods exist for transforming unlabeled datasets,
principled methods to do so for labeled (e.g., classification) datasets are
missing. In this work, we propose a novel framework for dataset transformation,
which we cast as optimization over data-generating joint probability
distributions. We approach this class of problems through Wasserstein gradient
flows in probability space, and derive practical and efficient particle-based
methods for a flexible but well-behaved class of objective functions. Through
various experiments, we show that this framework can be used to impose
constraints on classification datasets, adapt them for transfer learning, or to
re-purpose fixed or black-box models to classify ---with high accuracy---
previously unseen datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_Melis_D/0/1/0/all/0/1"&gt;David Alvarez-Melis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fusi_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Fusi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic DAG Search. (arXiv:2106.08717v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08717</id>
        <link href="http://arxiv.org/abs/2106.08717"/>
        <updated>2021-06-17T01:58:43.543Z</updated>
        <summary type="html"><![CDATA[Exciting contemporary machine learning problems have recently been phrased in
the classic formalism of tree search -- most famously, the game of Go.
Interestingly, the state-space underlying these sequential decision-making
problems often posses a more general latent structure than can be captured by a
tree. In this work, we develop a probabilistic framework to exploit a search
space's latent structure and thereby share information across the search tree.
The method is based on a combination of approximate inference in jointly
Gaussian models for the explored part of the problem, and an abstraction for
the unexplored part that imposes a reduction of complexity ad hoc. We
empirically find our algorithm to compare favorably to existing
non-probabilistic alternatives in Tic-Tac-Toe and a feature selection
application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grosse_J/0/1/0/all/0/1"&gt;Julia Grosse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1"&gt;Philipp Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint detection and matching of feature points in multimodal images. (arXiv:1810.12941v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.12941</id>
        <link href="http://arxiv.org/abs/1810.12941"/>
        <updated>2021-06-17T01:58:43.503Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a novel Convolutional Neural Network (CNN)
architecture for the joint detection and matching of feature points in images
acquired by different sensors using a single forward pass. The resulting
feature detector is tightly coupled with the feature descriptor, in contrast to
classical approaches (SIFT, etc.), where the detection phase precedes and
differs from computing the descriptor. Our approach utilizes two CNN
subnetworks, the first being a Siamese CNN and the second, consisting of dual
non-weight-sharing CNNs. This allows simultaneous processing and fusion of the
joint and disjoint cues in the multimodal image patches. The proposed approach
is experimentally shown to outperform contemporary state-of-the-art schemes
when applied to multiple datasets of multimodal images. It is also shown to
provide repeatable feature points detections across multisensor images,
outperforming state-of-the-art detectors. To the best of our knowledge, it is
the first unified approach for the detection and matching of such images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baruch_E/0/1/0/all/0/1"&gt;Elad Ben Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1"&gt;Yosi Keller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To Raise or Not To Raise: The Autonomous Learning Rate Question. (arXiv:2106.08767v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08767</id>
        <link href="http://arxiv.org/abs/2106.08767"/>
        <updated>2021-06-17T01:58:43.471Z</updated>
        <summary type="html"><![CDATA[There is a parameter ubiquitous throughout the deep learning world: learning
rate. There is likewise a ubiquitous question: what should that learning rate
be? The true answer to this question is often tedious and time consuming to
obtain, and a great deal of arcane knowledge has accumulated in recent years
over how to pick and modify learning rates to achieve optimal training
performance. Moreover, the long hours spent carefully crafting the perfect
learning rate can come to nothing the moment your network architecture,
optimizer, dataset, or initial conditions change ever so slightly. But it need
not be this way. We propose a new answer to the great learning rate question:
the Autonomous Learning Rate Controller. Find it at
https://github.com/fastestimator/ARC]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaomeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1"&gt;Michael Potter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yun-Chan Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1"&gt;Gaurav Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saripalli_V/0/1/0/all/0/1"&gt;V. Ratna Saripalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ModelDiff: Testing-Based DNN Similarity Comparison for Model Reuse Detection. (arXiv:2106.08890v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08890</id>
        <link href="http://arxiv.org/abs/2106.08890"/>
        <updated>2021-06-17T01:58:43.444Z</updated>
        <summary type="html"><![CDATA[The knowledge of a deep learning model may be transferred to a student model,
leading to intellectual property infringement or vulnerability propagation.
Detecting such knowledge reuse is nontrivial because the suspect models may not
be white-box accessible and/or may serve different tasks. In this paper, we
propose ModelDiff, a testing-based approach to deep learning model similarity
comparison. Instead of directly comparing the weights, activations, or outputs
of two models, we compare their behavioral patterns on the same set of test
inputs. Specifically, the behavioral pattern of a model is represented as a
decision distance vector (DDV), in which each element is the distance between
the model's reactions to a pair of inputs. The knowledge similarity between two
models is measured with the cosine similarity between their DDVs. To evaluate
ModelDiff, we created a benchmark that contains 144 pairs of models that cover
most popular model reuse methods, including transfer learning, model
compression, and model stealing. Our method achieved 91.7% correctness on the
benchmark, which demonstrates the effectiveness of using ModelDiff for model
reuse detection. A study on mobile deep learning apps has shown the feasibility
of ModelDiff on real-world models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanchun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bingyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ziyue Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yunxin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SEEN: Sharpening Explanations for Graph Neural Networks using Explanations from Neighborhoods. (arXiv:2106.08532v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08532</id>
        <link href="http://arxiv.org/abs/2106.08532"/>
        <updated>2021-06-17T01:58:43.437Z</updated>
        <summary type="html"><![CDATA[Explaining the foundations for predictions obtained from graph neural
networks (GNNs) is critical for credible use of GNN models for real-world
problems. Owing to the rapid growth of GNN applications, recent progress in
explaining predictions from GNNs, such as sensitivity analysis, perturbation
methods, and attribution methods, showed great opportunities and possibilities
for explaining GNN predictions. In this study, we propose a method to improve
the explanation quality of node classification tasks that can be applied in a
post hoc manner through aggregation of auxiliary explanations from important
neighboring nodes, named SEEN. Applying SEEN does not require modification of a
graph and can be used with diverse explainability techniques due to its
independent mechanism. Experiments on matching motif-participating nodes from a
given graph show great improvement in explanation accuracy of up to 12.71% and
demonstrate the correlation between the auxiliary explanations and the enhanced
explanation accuracy through leveraging their contributions. SEEN provides a
simple but effective method to enhance the explanation quality of GNN model
outputs, and this method is applicable in combination with most explainability
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hyeoncheol Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1"&gt;Youngrock Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_E/0/1/0/all/0/1"&gt;Eunjoo Jeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Achieving Domain Robustness in Stereo Matching Networks by Removing Shortcut Learning. (arXiv:2106.08486v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08486</id>
        <link href="http://arxiv.org/abs/2106.08486"/>
        <updated>2021-06-17T01:58:43.414Z</updated>
        <summary type="html"><![CDATA[Learning-based stereo matching and depth estimation networks currently excel
on public benchmarks with impressive results. However, state-of-the-art
networks often fail to generalize from synthetic imagery to more challenging
real data domains. This paper is an attempt to uncover hidden secrets of
achieving domain robustness and in particular, discovering the important
ingredients of generalization success of stereo matching networks by analyzing
the effect of synthetic image learning on real data performance. We provide
evidence that demonstrates that learning of features in the synthetic domain by
a stereo matching network is heavily influenced by two "shortcuts" presented in
the synthetic data: (1) identical local statistics (RGB colour features)
between matching pixels in the synthetic stereo images and (2) lack of realism
in synthetic textures on 3D objects simulated in game engines. We will show
that by removing such shortcuts, we can achieve domain robustness in the
state-of-the-art stereo matching frameworks and produce a remarkable
performance on multiple realistic datasets, despite the fact that the networks
were trained on synthetic data, only. Our experimental results point to the
fact that eliminating shortcuts from the synthetic data is key to achieve
domain-invariant generalization between synthetic and real data domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chuah_W/0/1/0/all/0/1"&gt;WeiQin Chuah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tennakoon_R/0/1/0/all/0/1"&gt;Ruwan Tennakoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bab_Hadiashar_A/0/1/0/all/0/1"&gt;Alireza Bab-Hadiashar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1"&gt;David Suter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamically Grown Generative Adversarial Networks. (arXiv:2106.08505v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08505</id>
        <link href="http://arxiv.org/abs/2106.08505"/>
        <updated>2021-06-17T01:58:43.368Z</updated>
        <summary type="html"><![CDATA[Recent work introduced progressive network growing as a promising way to ease
the training for large GANs, but the model design and architecture-growing
strategy still remain under-explored and needs manual design for different
image data. In this paper, we propose a method to dynamically grow a GAN during
training, optimizing the network architecture and its parameters together with
automation. The method embeds architecture search techniques as an interleaving
step with gradient-based training to periodically seek the optimal
architecture-growing strategy for the generator and discriminator. It enjoys
the benefits of both eased training because of progressive growing and improved
performance because of broader architecture design space. Experimental results
demonstrate new state-of-the-art of image generation. Observations in the
search procedure also provide constructive insights into the GAN model design
such as generator-discriminator balance and convolutional layer choices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lanlan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuting Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jia Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memorization and Generalization in Neural Code Intelligence Models. (arXiv:2106.08704v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.08704</id>
        <link href="http://arxiv.org/abs/2106.08704"/>
        <updated>2021-06-17T01:58:43.361Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNN) are increasingly commonly used in software
engineering and code intelligence tasks. These are powerful tools that are
capable of learning highly generalizable patterns from large datasets through
millions of parameters. At the same time, training DNNs means walking a knife's
edges, because their large capacity also renders them prone to memorizing data
points. While traditionally thought of as an aspect of over-training, recent
work suggests that the memorization risk manifests especially strongly when the
training datasets are noisy and memorization is the only recourse.
Unfortunately, most code intelligence tasks rely on rather noise-prone and
repetitive data sources, such as GitHub, which, due to their sheer size, cannot
be manually inspected and evaluated. We evaluate the memorization and
generalization tendencies in neural code intelligence models through a case
study across several benchmarks and model families by leveraging established
approaches from other fields that use DNNs, such as introducing targeted noise
into the training dataset. In addition to reinforcing prior general findings
about the extent of memorization in DNNs, our results shed light on the impact
of noisy dataset in training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rabin_M/0/1/0/all/0/1"&gt;Md Rafiqul Islam Rabin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1"&gt;Aftab Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1"&gt;Vincent J. Hellendoorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alipour_M/0/1/0/all/0/1"&gt;Mohammad Amin Alipour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Synchronized Reprojection-based Model for 3D Human Pose Estimation. (arXiv:2106.04274v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04274</id>
        <link href="http://arxiv.org/abs/2106.04274"/>
        <updated>2021-06-17T01:58:43.315Z</updated>
        <summary type="html"><![CDATA[3D human pose estimation is still a challenging problem despite the large
amount of work that has been done in this field. Generally, most methods
directly use neural networks and ignore certain constraints (e.g., reprojection
constraints and joint angle and bone length constraints). This paper proposes a
weakly supervised GAN-based model for 3D human pose estimation that considers
3D information along with 2D information simultaneously, in which a
reprojection network is employed to learn the mapping of the distribution from
3D poses to 2D poses. In particular, we train the reprojection network and the
generative adversarial network synchronously. Furthermore, inspired by the
typical kinematic chain space (KCS) matrix, we propose a weighted KCS matrix,
which is added into the discriminator's input to impose joint angle and bone
length constraints. The experimental results on Human3.6M show that our method
outperforms state-of-the-art methods by approximately 5.1\%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yicheng Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Cheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yongqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiahui Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selection of Source Images Heavily Influences the Effectiveness of Adversarial Attacks. (arXiv:2106.07141v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07141</id>
        <link href="http://arxiv.org/abs/2106.07141"/>
        <updated>2021-06-17T01:58:43.309Z</updated>
        <summary type="html"><![CDATA[Although the adoption rate of deep neural networks (DNNs) has tremendously
increased in recent years, a solution for their vulnerability against
adversarial examples has not yet been found. As a result, substantial research
efforts are dedicated to fix this weakness, with many studies typically using a
subset of source images to generate adversarial examples, treating every image
in this subset as equal. We demonstrate that, in fact, not every source image
is equally suited for this kind of assessment. To do so, we devise a
large-scale model-to-model transferability scenario for which we meticulously
analyze the properties of adversarial examples, generated from every suitable
source image in ImageNet by making use of two of the most frequently deployed
attacks. In this transferability scenario, which involves seven distinct DNN
models, including the recently proposed vision transformers, we reveal that it
is possible to have a difference of up to $12.5\%$ in model-to-model
transferability success, $1.01$ in average $L_2$ perturbation, and $0.03$
($8/225$) in average $L_{\infty}$ perturbation when $1,000$ source images are
sampled randomly among all suitable candidates. We then take one of the first
steps in evaluating the robustness of images used to create adversarial
examples, proposing a number of simple but effective methods to identify
unsuitable source images, thus making it possible to mitigate extreme cases in
experimentation and support high-quality benchmarking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozbulak_U/0/1/0/all/0/1"&gt;Utku Ozbulak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anzaku_E/0/1/0/all/0/1"&gt;Esla Timothy Anzaku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neve_W/0/1/0/all/0/1"&gt;Wesley De Neve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Messem_A/0/1/0/all/0/1"&gt;Arnout Van Messem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation. (arXiv:2106.09016v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09016</id>
        <link href="http://arxiv.org/abs/2106.09016"/>
        <updated>2021-06-17T01:58:43.303Z</updated>
        <summary type="html"><![CDATA[Image-to-Image (I2I) multi-domain translation models are usually evaluated
also using the quality of their semantic interpolation results. However,
state-of-the-art models frequently show abrupt changes in the image appearance
during interpolation, and usually perform poorly in interpolations across
domains. In this paper, we propose a new training protocol based on three
specific losses which help a translation network to learn a smooth and
disentangled latent style space in which: 1) Both intra- and inter-domain
interpolations correspond to gradual changes in the generated images and 2) The
content of the source image is better preserved during the translation.
Moreover, we propose a novel evaluation metric to properly measure the
smoothness of latent style space of I2I translation models. The proposed method
can be plugged into existing translation approaches, and our extensive
experiments on different datasets show that it can significantly boost the
quality of the generated images and the graduality of the interpolations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yahui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1"&gt;Enver Sangineto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yajing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1"&gt;Linchao Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoxian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1"&gt;Bruno Lepri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadai_M/0/1/0/all/0/1"&gt;Marco De Nadai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TextStyleBrush: Transfer of Text Aesthetics from a Single Example. (arXiv:2106.08385v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08385</id>
        <link href="http://arxiv.org/abs/2106.08385"/>
        <updated>2021-06-17T01:58:43.297Z</updated>
        <summary type="html"><![CDATA[We present a novel approach for disentangling the content of a text image
from all aspects of its appearance. The appearance representation we derive can
then be applied to new content, for one-shot transfer of the source style to
new content. We learn this disentanglement in a self-supervised manner. Our
method processes entire word boxes, without requiring segmentation of text from
background, per-character processing, or making assumptions on string lengths.
We show results in different text domains which were previously handled by
specialized methods, e.g., scene text, handwritten text. To these ends, we make
a number of technical contributions: (1) We disentangle the style and content
of a textual image into a non-parametric, fixed-dimensional vector. (2) We
propose a novel approach inspired by StyleGAN but conditioned over the example
style at different resolution and content. (3) We present novel self-supervised
training criteria which preserve both source style and target content using a
pre-trained font classifier and text recognizer. Finally, (4) we also introduce
Imgur5K, a new challenging dataset for handwritten word images. We offer
numerous qualitative photo-realistic results of our method. We further show
that our method surpasses previous work in quantitative tests on scene text and
handwriting datasets, as well as in a user study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_P/0/1/0/all/0/1"&gt;Praveen Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovvuri_R/0/1/0/all/0/1"&gt;Rama Kovvuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1"&gt;Guan Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vassilev_B/0/1/0/all/0/1"&gt;Boris Vassilev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1"&gt;Tal Hassner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment Analysis Methods. (arXiv:2106.08829v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.08829</id>
        <link href="http://arxiv.org/abs/2106.08829"/>
        <updated>2021-06-17T01:58:43.289Z</updated>
        <summary type="html"><![CDATA[Opinion and sentiment analysis is a vital task to characterize subjective
information in social media posts. In this paper, we present a comprehensive
experimental evaluation and comparison with six state-of-the-art methods, from
which we have re-implemented one of them. In addition, we investigate different
textual and visual feature embeddings that cover different aspects of the
content, as well as the recently introduced multimodal CLIP embeddings.
Experimental results are presented for two different publicly available
benchmark datasets of tweets and corresponding images. In contrast to the
evaluation methodology of previous work, we introduce a reproducible and fair
evaluation scheme to make results comparable. Finally, we conduct an error
analysis to outline the limitations of the methods and possibilities for the
future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheema_G/0/1/0/all/0/1"&gt;Gullal S. Cheema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1"&gt;Sherzod Hakimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1"&gt;Eric M&amp;#xfc;ller-Budack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1"&gt;Ralph Ewerth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Evaluating and Training Verifiably Robust Neural Networks. (arXiv:2104.00447v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00447</id>
        <link href="http://arxiv.org/abs/2104.00447"/>
        <updated>2021-06-17T01:58:43.273Z</updated>
        <summary type="html"><![CDATA[Recent works have shown that interval bound propagation (IBP) can be used to
train verifiably robust neural networks. Reseachers observe an intriguing
phenomenon on these IBP trained networks: CROWN, a bounding method based on
tight linear relaxation, often gives very loose bounds on these networks. We
also observe that most neurons become dead during the IBP training process,
which could hurt the representation capability of the network. In this paper,
we study the relationship between IBP and CROWN, and prove that CROWN is always
tighter than IBP when choosing appropriate bounding lines. We further propose a
relaxed version of CROWN, linear bound propagation (LBP), that can be used to
verify large networks to obtain lower verified errors than IBP. We also design
a new activation function, parameterized ramp function (ParamRamp), which has
more diversity of neuron status than ReLU. We conduct extensive experiments on
MNIST, CIFAR-10 and Tiny-ImageNet with ParamRamp activation and achieve
state-of-the-art verified robustness. Code and the appendix are available at
https://github.com/ZhaoyangLyu/VerifiablyRobustNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1"&gt;Zhaoyang Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Minghao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guodong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kehuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dahua Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Neural Architecture Search. (arXiv:2006.06863v8 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06863</id>
        <link href="http://arxiv.org/abs/2006.06863"/>
        <updated>2021-06-17T01:58:43.267Z</updated>
        <summary type="html"><![CDATA[Efficient evaluation of a network architecture drawn from a large search
space remains a key challenge in Neural Architecture Search (NAS). Vanilla NAS
evaluates each architecture by training from scratch, which gives the true
performance but is extremely time-consuming. Recently, one-shot NAS
substantially reduces the computation cost by training only one supernetwork,
a.k.a. supernet, to approximate the performance of every architecture in the
search space via weight-sharing. However, the performance estimation can be
very inaccurate due to the co-adaption among operations. In this paper, we
propose few-shot NAS that uses multiple supernetworks, called sub-supernet,
each covering different regions of the search space to alleviate the undesired
co-adaption. Compared to one-shot NAS, few-shot NAS improves the accuracy of
architecture evaluation with a small increase of evaluation cost. With only up
to 7 sub-supernets, few-shot NAS establishes new SoTAs: on ImageNet, it finds
models that reach 80.5% top-1 accuracy at 600 MB FLOPS and 77.5% top-1 accuracy
at 238 MFLOPS; on CIFAR10, it reaches 98.72% top-1 accuracy without using extra
data or transfer learning. In Auto-GAN, few-shot NAS outperforms the previously
published results by up to 20%. Extensive experiments show that few-shot NAS
significantly improves various one-shot methods, including 4 gradient-based and
6 search-based methods on 3 different tasks in NasBench-201 and
NasBench1-shot-1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linnan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuandong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_R/0/1/0/all/0/1"&gt;Rodrigo Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1"&gt;Tian Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Device-Cloud Collaborative Learning for Recommendation. (arXiv:2104.06624v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06624</id>
        <link href="http://arxiv.org/abs/2104.06624"/>
        <updated>2021-06-17T01:58:43.240Z</updated>
        <summary type="html"><![CDATA[With the rapid development of storage and computing power on mobile devices,
it becomes critical and popular to deploy models on devices to save onerous
communication latencies and to capture real-time features. While quite a lot of
works have explored to facilitate on-device learning and inference, most of
them focus on dealing with response delay or privacy protection. Little has
been done to model the collaboration between the device and the cloud modeling
and benefit both sides jointly. To bridge this gap, we are among the first
attempts to study the Device-Cloud Collaborative Learning (DCCL) framework.
Specifically, we propose a novel MetaPatch learning approach on the device side
to efficiently achieve "thousands of people with thousands of models" given a
centralized cloud model. Then, with billions of updated personalized device
models, we propose a "model-over-models" distillation algorithm, namely
MoMoDistill, to update the centralized cloud model. Our extensive experiments
over a range of datasets with different settings demonstrate the effectiveness
of such collaboration on both cloud and devices, especially its superiority to
model long-tailed users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jiangchao Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Feng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;KunYang Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metamorphic image registration using a semi-Lagrangian scheme. (arXiv:2106.08817v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08817</id>
        <link href="http://arxiv.org/abs/2106.08817"/>
        <updated>2021-06-17T01:58:43.234Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an implementation of both Large Deformation
Diffeomorphic Metric Mapping (LDDMM) and Metamorphosis image registration using
a semi-Lagrangian scheme for geodesic shooting. We propose to solve both
problems as an inexact matching providing a single and unifying cost function.
We demonstrate that for image registration the use of a semi-Lagrangian scheme
is more stable than a standard Eulerian scheme. Our GPU implementation is based
on PyTorch, which greatly simplifies and accelerates the computations thanks to
its powerful automatic differentiation engine. It will be freely available at
https://github.com/antonfrancois/Demeter_metamorphosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Francois_A/0/1/0/all/0/1"&gt;Anton Fran&amp;#xe7;ois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1"&gt;Pietro Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaunes_J/0/1/0/all/0/1"&gt;Joan Glaun&amp;#xe8;s&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Inference in medicine and in health policy, a summary. (arXiv:2105.04655v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04655</id>
        <link href="http://arxiv.org/abs/2105.04655"/>
        <updated>2021-06-17T01:58:43.226Z</updated>
        <summary type="html"><![CDATA[A data science task can be deemed as making sense of the data or testing a
hypothesis about it. The conclusions inferred from data can greatly guide us to
make informative decisions. Big data has enabled us to carry out countless
prediction tasks in conjunction with machine learning, such as identifying high
risk patients suffering from a certain disease and taking preventable measures.
However, healthcare practitioners are not content with mere predictions - they
are also interested in the cause-effect relation between input features and
clinical outcomes. Understanding such relations will help doctors treat
patients and reduce the risk effectively. Causality is typically identified by
randomized controlled trials. Often such trials are not feasible when
scientists and researchers turn to observational studies and attempt to draw
inferences. However, observational studies may also be affected by selection
and/or confounding biases that can result in wrong causal conclusions. In this
chapter, we will try to highlight some of the drawbacks that may arise in
traditional machine learning and statistical approaches to analyze the
observational data, particularly in the healthcare data analytics domain. We
will discuss causal inference and ways to discover the cause-effect from
observational studies in healthcare domain. Moreover, we will demonstrate the
applications of causal inference in tackling some common machine learning
issues such as missing data and model transportability. Finally, we will
discuss the possibility of integrating reinforcement learning with causality as
a way to counter confounding bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramezani_R/0/1/0/all/0/1"&gt;Ramin Ramezani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeim_A/0/1/0/all/0/1"&gt;Arash Naeim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Morphset:Augmenting categorical emotion datasets with dimensional affect labels using face morphing. (arXiv:2103.02854v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02854</id>
        <link href="http://arxiv.org/abs/2103.02854"/>
        <updated>2021-06-17T01:58:43.209Z</updated>
        <summary type="html"><![CDATA[Emotion recognition and understanding is a vital component in human-machine
interaction. Dimensional models of affect such as those using valence and
arousal have advantages over traditional categorical ones due to the complexity
of emotional states in humans. However, dimensional emotion annotations are
difficult and expensive to collect, therefore they are not as prevalent in the
affective computing community. To address these issues, we propose a method to
generate synthetic images from existing categorical emotion datasets using face
morphing as well as dimensional labels in the circumplex space with full
control over the resulting sample distribution, while achieving augmentation
factors of at least 20x or more.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vonikakis_V/0/1/0/all/0/1"&gt;Vassilios Vonikakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neo_D/0/1/0/all/0/1"&gt;Dexter Neo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1"&gt;Stefan Winkler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds. (arXiv:2007.07978v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.07978</id>
        <link href="http://arxiv.org/abs/2007.07978"/>
        <updated>2021-06-17T01:58:43.191Z</updated>
        <summary type="html"><![CDATA[Forecasting the formation and development of clouds is a central element of
modern weather forecasting systems. Incorrect clouds forecasts can lead to
major uncertainty in the overall accuracy of weather forecasts due to their
intrinsic role in the Earth's climate system. Few studies have tackled this
challenging problem from a machine learning point-of-view due to a shortage of
high-resolution datasets with many historical observations globally. In this
paper, we present a novel satellite-based dataset called ``CloudCast''. It
consists of 70,080 images with 10 different cloud types for multiple layers of
the atmosphere annotated on a pixel level. The spatial resolution of the
dataset is 928 x 1530 pixels (3x3 km per pixel) with 15-min intervals between
frames for the period 2017-01-01 to 2018-12-31. All frames are centered and
projected over Europe. To supplement the dataset, we conduct an evaluation
study with current state-of-the-art video prediction methods such as
convolutional long short-term memory networks, generative adversarial networks,
and optical flow-based extrapolation methods. As the evaluation of video
prediction is difficult in practice, we aim for a thorough evaluation in the
spatial and temporal domain. Our benchmark models show promising results but
with ample room for improvement. This is the first publicly available
global-scale dataset with high-resolution cloud types on a high temporal
granularity to the authors' best knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_A/0/1/0/all/0/1"&gt;A. H. Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;A. Iosifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karstoft_H/0/1/0/all/0/1"&gt;H. Karstoft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amortized Synthesis of Constrained Configurations Using a Differentiable Surrogate. (arXiv:2106.09019v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09019</id>
        <link href="http://arxiv.org/abs/2106.09019"/>
        <updated>2021-06-17T01:58:43.178Z</updated>
        <summary type="html"><![CDATA[In design, fabrication, and control problems, we are often faced with the
task of synthesis, in which we must generate an object or configuration that
satisfies a set of constraints while maximizing one or more objective
functions. The synthesis problem is typically characterized by a physical
process in which many different realizations may achieve the goal. This
many-to-one map presents challenges to the supervised learning of feed-forward
synthesis, as the set of viable designs may have a complex structure. In
addition, the non-differentiable nature of many physical simulations prevents
direct optimization. We address both of these problems with a two-stage neural
network architecture that we may consider to be an autoencoder. We first learn
the decoder: a differentiable surrogate that approximates the many-to-one
physical realization process. We then learn the encoder, which maps from goal
to design, while using the fixed decoder to evaluate the quality of the
realization. We evaluate the approach on two case studies: extruder path
planning in additive manufacturing and constrained soft robot inverse
kinematics. We compare our approach to direct optimization of design using the
learned surrogate, and to supervised learning of the synthesis problem. We find
that our approach produces higher quality solutions than supervised learning,
while being competitive in quality with direct optimization, at a greatly
reduced computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xingyuan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1"&gt;Tianju Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rusinkiewicz_S/0/1/0/all/0/1"&gt;Szymon M. Rusinkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adams_R/0/1/0/all/0/1"&gt;Ryan P. Adams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data. (arXiv:2106.03096v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03096</id>
        <link href="http://arxiv.org/abs/2106.03096"/>
        <updated>2021-06-17T01:58:43.170Z</updated>
        <summary type="html"><![CDATA[Tabular data are ubiquitous for the widespread applications of tables and
hence have attracted the attention of researchers to extract underlying
information. One of the critical problems in mining tabular data is how to
understand their inherent semantic structures automatically. Existing studies
typically adopt Convolutional Neural Network (CNN) to model the spatial
information of tabular structures yet ignore more diverse relational
information between cells, such as the hierarchical and paratactic
relationships. To simultaneously extract spatial and relational information
from tables, we propose a novel neural network architecture, TabularNet. The
spatial encoder of TabularNet utilizes the row/column-level Pooling and the
Bidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information
and local positional correlation, respectively. For relational information, we
design a new graph construction method based on the WordNet tree and adopt a
Graph Convolutional Network (GCN) based encoder that focuses on the
hierarchical and paratactic relationships between cells. Our neural network
architecture can be a unified neural backbone for different understanding tasks
and utilized in a multitask scenario. We conduct extensive experiments on three
classification tasks with two real-world spreadsheet data sets, and the results
demonstrate the effectiveness of our proposed TabularNet over state-of-the-art
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1"&gt;Lun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1"&gt;Fei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ran Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junshan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Learning and Personalization in Multi-Agent Stochastic Linear Bandits. (arXiv:2106.08902v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08902</id>
        <link href="http://arxiv.org/abs/2106.08902"/>
        <updated>2021-06-17T01:58:43.142Z</updated>
        <summary type="html"><![CDATA[We consider the problem of minimizing regret in an $N$ agent heterogeneous
stochastic linear bandits framework, where the agents (users) are similar but
not all identical. We model user heterogeneity using two popularly used ideas
in practice; (i) A clustering framework where users are partitioned into groups
with users in the same group being identical to each other, but different
across groups, and (ii) a personalization framework where no two users are
necessarily identical, but a user's parameters are close to that of the
population average. In the clustered users' setup, we propose a novel
algorithm, based on successive refinement of cluster identities and regret
minimization. We show that, for any agent, the regret scales as
$\mathcal{O}(\sqrt{T/N})$, if the agent is in a `well separated' cluster, or
scales as $\mathcal{O}(T^{\frac{1}{2} + \varepsilon}/(N)^{\frac{1}{2}
-\varepsilon})$ if its cluster is not well separated, where $\varepsilon$ is
positive and arbitrarily close to $0$. Our algorithm is adaptive to the cluster
separation, and is parameter free -- it does not need to know the number of
clusters, separation and cluster size, yet the regret guarantee adapts to the
inherent complexity. In the personalization framework, we introduce a natural
algorithm where, the personal bandit instances are initialized with the
estimates of the global average model. We show that, an agent $i$ whose
parameter deviates from the population average by $\epsilon_i$, attains a
regret scaling of $\widetilde{O}(\epsilon_i\sqrt{T})$. This demonstrates that
if the user representations are close (small $\epsilon_i)$, the resulting
regret is low, and vice-versa. The results are empirically validated and we
observe superior performance of our adaptive algorithms over non-adaptive
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Avishek Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sankararaman_A/0/1/0/all/0/1"&gt;Abishek Sankararaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1"&gt;Kannan Ramchandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation. (arXiv:2106.08823v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08823</id>
        <link href="http://arxiv.org/abs/2106.08823"/>
        <updated>2021-06-17T01:58:43.128Z</updated>
        <summary type="html"><![CDATA[State-of-the-art transformer models use pairwise dot-product based
self-attention, which comes at a computational cost quadratic in the input
sequence length. In this paper, we investigate the global structure of
attention scores computed using this dot product mechanism on a typical
distribution of inputs, and study the principal components of their variation.
Through eigen analysis of full attention score matrices, as well as of their
individual rows, we find that most of the variation among attention scores lie
in a low-dimensional eigenspace. Moreover, we find significant overlap between
these eigenspaces for different layers and even different transformer models.
Based on this, we propose to compute scores only for a partial subset of token
pairs, and use them to estimate scores for the remaining pairs. Beyond
investigating the accuracy of reconstructing attention scores themselves, we
investigate training transformer models that employ these approximations, and
analyze the effect on overall accuracy. Our analysis and the proposed method
provide insights into how to balance the benefits of exact pair-wise attention
and its significant computational expense.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1"&gt;Srinadh Bhojanapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1"&gt;Ayan Chakrabarti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1"&gt;Himanshu Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1"&gt;Michal Lukasik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1"&gt;Andreas Veit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaze Preserving CycleGANs for Eyeglass Removal & Persistent Gaze Estimation. (arXiv:2002.02077v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02077</id>
        <link href="http://arxiv.org/abs/2002.02077"/>
        <updated>2021-06-17T01:58:43.122Z</updated>
        <summary type="html"><![CDATA[A driver's gaze is critical for determining their attention, state,
situational awareness, and readiness to take over control from partially
automated vehicles. Estimating the gaze direction is the most obvious way to
gauge a driver's state under ideal conditions when limited to using
non-intrusive imaging sensors. Unfortunately, the vehicular environment
introduces a variety of challenges that are usually unaccounted for - harsh
illumination, nighttime conditions, and reflective eyeglasses. Relying on head
pose alone under such conditions can prove to be unreliable and erroneous. In
this study, we offer solutions to address these problems encountered in the
real world. To solve issues with lighting, we demonstrate that using an
infrared camera with suitable equalization and normalization suffices. To
handle eyeglasses and their corresponding artifacts, we adopt image-to-image
translation using generative adversarial networks to pre-process images prior
to gaze estimation. Our proposed Gaze Preserving CycleGAN (GPCycleGAN) is
trained to preserve the driver's gaze while removing potential eyeglasses from
face images. GPCycleGAN is based on the well-known CycleGAN approach - with the
addition of a gaze classifier and a gaze consistency loss for additional
supervision. Our approach exhibits improved performance, interpretability,
robustness and superior qualitative results on challenging real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1"&gt;Akshay Rangesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bowen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1"&gt;Mohan M. Trivedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing the Impact: Does an Improvement to a Revenue Management System Lead to an Improved Revenue?. (arXiv:2101.10249v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10249</id>
        <link href="http://arxiv.org/abs/2101.10249"/>
        <updated>2021-06-17T01:58:43.115Z</updated>
        <summary type="html"><![CDATA[Airlines and other industries have been making use of sophisticated Revenue
Management Systems to maximize revenue for decades. While improving the
different components of these systems has been the focus of numerous studies,
estimating the impact of such improvements on the revenue has been overlooked
in the literature despite its practical importance. Indeed, quantifying the
benefit of a change in a system serves as support for investment decisions.
This is a challenging problem as it corresponds to the difference between the
generated value and the value that would have been generated keeping the system
as before. The latter is not observable. Moreover, the expected impact can be
small in relative value. In this paper, we cast the problem as counterfactual
prediction of unobserved revenue. The impact on revenue is then the difference
between the observed and the estimated revenue. The originality of this work
lies in the innovative application of econometric methods proposed for
macroeconomic applications to a new problem setting. Broadly applicable, the
approach benefits from only requiring revenue data observed for
origin-destination pairs in the network of the airline at each day, before and
after a change in the system is applied. We report results using real
large-scale data from Air Canada. We compare a deep neural network
counterfactual predictions model with econometric models. They achieve
respectively 1% and 1.1% of error on the counterfactual revenue predictions,
and allow to accurately estimate small impacts (in the order of 2%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laage_G/0/1/0/all/0/1"&gt;Greta Laage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frejinger_E/0/1/0/all/0/1"&gt;Emma Frejinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lodi_A/0/1/0/all/0/1"&gt;Andrea Lodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1"&gt;Guillaume Rabusseau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline RL Without Off-Policy Evaluation. (arXiv:2106.08909v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08909</id>
        <link href="http://arxiv.org/abs/2106.08909"/>
        <updated>2021-06-17T01:58:43.108Z</updated>
        <summary type="html"><![CDATA[Most prior approaches to offline reinforcement learning (RL) have taken an
iterative actor-critic approach involving off-policy evaluation. In this paper
we show that simply doing one step of constrained/regularized policy
improvement using an on-policy Q estimate of the behavior policy performs
surprisingly well. This one-step algorithm beats the previously reported
results of iterative algorithms on a large portion of the D4RL benchmark. The
simple one-step baseline achieves this strong performance without many of the
tricks used by previously proposed iterative algorithms and is more robust to
hyperparameters. We argue that the relatively poor performance of iterative
approaches is a result of the high variance inherent in doing off-policy
evaluation and magnified by the repeated optimization of policies against those
high-variance estimates. In addition, we hypothesize that the strong
performance of the one-step algorithm is due to a combination of favorable
structure in the environment and behavior policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandfonbrener_D/0/1/0/all/0/1"&gt;David Brandfonbrener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1"&gt;William F. Whitney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1"&gt;Joan Bruna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep-learning based Tools for Automated Protocol Definition of Advanced Diagnostic Imaging Exams. (arXiv:2106.08963v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08963</id>
        <link href="http://arxiv.org/abs/2106.08963"/>
        <updated>2021-06-17T01:58:43.094Z</updated>
        <summary type="html"><![CDATA[Purpose: This study evaluates the effectiveness and impact of automated
order-based protocol assignment for magnetic resonance imaging (MRI) exams
using natural language processing (NLP) and deep learning (DL).

Methods: NLP tools were applied to retrospectively process orders from over
116,000 MRI exams with 200 unique sub-specialized protocols ("Local" protocol
class). Separate DL models were trained on 70\% of the processed data for
"Local" protocols as well as 93 American College of Radiology ("ACR") protocols
and 48 "General" protocols. The DL Models were assessed in an "auto-protocoling
(AP)" inference mode which returns the top recommendation and in a "clinical
decision support (CDS)" inference mode which returns up to 10 protocols for
radiologist review. The accuracy of each protocol recommendation was computed
and analyzed based on the difference between the normalized output score of the
corresponding neural net for the top two recommendations.

Results: The top predicted protocol in AP mode was correct for 82.8%, 73.8%,
and 69.3% of the test cases for "General", "ACR", and "Local" protocol classes,
respectively. Higher levels of accuracy over 96% were obtained for all protocol
classes in CDS mode. However, at current validation performance levels, the
proposed models offer modest, positive, financial impact on large-scale imaging
networks.

Conclusions: DL-based protocol automation is feasible and can be tuned to
route substantial fractions of exams for auto-protocoling, with higher accuracy
with more general protocols. Economic analyses of the tested algorithms
indicate that improved algorithm performance is required to yield a practical
exam auto-protocoling tool for sub-specialized imaging exams.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nencka_A/0/1/0/all/0/1"&gt;Andrew S. Nencka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sherafati_M/0/1/0/all/0/1"&gt;Mohammad Sherafati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_T/0/1/0/all/0/1"&gt;Timothy Goebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolat_P/0/1/0/all/0/1"&gt;Parag Tolat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_K/0/1/0/all/0/1"&gt;Kevin M. Koch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LemgoRL: An open-source Benchmark Tool to Train Reinforcement Learning Agents for Traffic Signal Control in a real-world simulation scenario. (arXiv:2103.16223v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16223</id>
        <link href="http://arxiv.org/abs/2103.16223"/>
        <updated>2021-06-17T01:58:43.086Z</updated>
        <summary type="html"><![CDATA[Sub-optimal control policies in intersection traffic signal controllers (TSC)
contribute to congestion and lead to negative effects on human health and the
environment. Reinforcement learning (RL) for traffic signal control is a
promising approach to design better control policies and has attracted
considerable research interest in recent years. However, most work done in this
area used simplified simulation environments of traffic scenarios to train
RL-based TSC. To deploy RL in real-world traffic systems, the gap between
simplified simulation environments and real-world applications has to be
closed. Therefore, we propose LemgoRL, a benchmark tool to train RL agents as
TSC in a realistic simulation environment of Lemgo, a medium-sized town in
Germany. In addition to the realistic simulation model, LemgoRL encompasses a
traffic signal logic unit that ensures compliance with all regulatory and
safety requirements. LemgoRL offers the same interface as the well-known OpenAI
gym toolkit to enable easy deployment in existing research work. Our benchmark
tool drives the development of RL algorithms towards real-world applications.
We provide LemgoRL as an open-source tool at https://github.com/rl-ina/lemgorl.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_A/0/1/0/all/0/1"&gt;Arthur M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangras_V/0/1/0/all/0/1"&gt;Vishal Rangras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnittker_G/0/1/0/all/0/1"&gt;Georg Schnittker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waldmann_M/0/1/0/all/0/1"&gt;Michael Waldmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friesen_M/0/1/0/all/0/1"&gt;Maxim Friesen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferfers_T/0/1/0/all/0/1"&gt;Tobias Ferfers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schreckenberg_L/0/1/0/all/0/1"&gt;Lukas Schreckenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hufen_F/0/1/0/all/0/1"&gt;Florian Hufen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jasperneite_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Jasperneite&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiering_M/0/1/0/all/0/1"&gt;Marco Wiering&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of Outlier Detection Techniques for Structured Data. (arXiv:2106.08779v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08779</id>
        <link href="http://arxiv.org/abs/2106.08779"/>
        <updated>2021-06-17T01:58:43.074Z</updated>
        <summary type="html"><![CDATA[An outlier is an observation or a data point that is far from rest of the
data points in a given dataset or we can be said that an outlier is away from
the center of mass of observations. Presence of outliers can skew statistical
measures and data distributions which can lead to misleading representation of
the underlying data and relationships. It is seen that the removal of outliers
from the training dataset before modeling can give better predictions. With the
advancement of machine learning, the outlier detection models are also
advancing at a good pace. The goal of this work is to highlight and compare
some of the existing outlier detection techniques for the data scientists to
use that information for outlier algorithm selection while building a machine
learning model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Amulya Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nitin Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilinear Dirichlet Processes. (arXiv:2106.08852v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08852</id>
        <link href="http://arxiv.org/abs/2106.08852"/>
        <updated>2021-06-17T01:58:43.069Z</updated>
        <summary type="html"><![CDATA[Dependent Dirichlet processes (DDP) have been widely applied to model data
from distributions over collections of measures which are correlated in some
way. On the other hand, in recent years, increasing research efforts in machine
learning and data mining have been dedicated to dealing with data involving
interactions from two or more factors. However, few researchers have addressed
the heterogeneous relationship in data brought by modulation of multiple
factors using techniques of DDP. In this paper, we propose a novel technique,
MultiLinear Dirichlet Processes (MLDP), to constructing DDPs by combining DP
with a state-of-the-art factor analysis technique, multilinear factor analyzers
(MLFA). We have evaluated MLDP on real-word data sets for different
applications and have achieved state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoli Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early fault detection with multi-target neural networks. (arXiv:2106.08957v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08957</id>
        <link href="http://arxiv.org/abs/2106.08957"/>
        <updated>2021-06-17T01:58:43.063Z</updated>
        <summary type="html"><![CDATA[Wind power is seeing a strong growth around the world. At the same time,
shrinking profit margins in the energy markets let wind farm managers explore
options for cost reductions in the turbine operation and maintenance.
Sensor-based condition monitoring facilitates remote diagnostics of turbine
subsystems, enabling faster responses when unforeseen maintenance is required.
Condition monitoring with data from the turbines' supervisory control and data
acquisition (SCADA) systems was proposed and SCADA-based fault detection and
diagnosis approaches introduced based on single-task normal operation models of
turbine state variables. As the number of SCADA channels has grown strongly,
thousands of independent single-target models are in place today for monitoring
a single turbine. Multi-target learning was recently proposed to limit the
number of models. This study applied multi-target neural networks to the task
of early fault detection in drive-train components. The accuracy and delay of
detecting gear bearing faults were compared to state-of-the-art single-target
approaches. We found that multi-target multi-layer perceptrons (MLPs) detected
faults at least as early and in many cases earlier than single-target MLPs. The
multi-target MLPs could detect faults up to several days earlier than the
single-target models. This can deliver a significant advantage in the planning
and performance of maintenance work. At the same time, the multi-target MLPs
achieved the same level of prediction stability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_A/0/1/0/all/0/1"&gt;Angela Meyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[mSHAP: SHAP Values for Two-Part Models. (arXiv:2106.08990v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08990</id>
        <link href="http://arxiv.org/abs/2106.08990"/>
        <updated>2021-06-17T01:58:43.043Z</updated>
        <summary type="html"><![CDATA[Two-part models are important to and used throughout insurance and actuarial
science. Since insurance is required for registering a car, obtaining a
mortgage, and participating in certain businesses, it is especially important
that the models which price insurance policies are fair and non-discriminatory.
Black box models can make it very difficult to know which covariates are
influencing the results. SHAP values enable interpretation of various black box
models, but little progress has been made in two-part models. In this paper, we
propose mSHAP (or multiplicative SHAP), a method for computing SHAP values of
two-part models using the SHAP values of the individual models. This method
will allow for the predictions of two-part models to be explained at an
individual observation level. After developing mSHAP, we perform an in-depth
simulation study. Although the kernelSHAP algorithm is also capable of
computing approximate SHAP values for a two-part model, a comparison with our
method demonstrates that mSHAP is exponentially faster. Ultimately, we apply
mSHAP to a two-part ratemaking model for personal auto property damage
insurance coverage. Additionally, an R package (mshap) is available to easily
implement the method in a wide variety of applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Matthews_S/0/1/0/all/0/1"&gt;Spencer Matthews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hartman_B/0/1/0/all/0/1"&gt;Brian Hartman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolving Image Compositions for Feature Representation Learning. (arXiv:2106.09011v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09011</id>
        <link href="http://arxiv.org/abs/2106.09011"/>
        <updated>2021-06-17T01:58:43.037Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks for visual recognition require large amounts of
training samples and usually benefit from data augmentation. This paper
proposes PatchMix, a data augmentation method that creates new samples by
composing patches from pairs of images in a grid-like pattern. These new
samples' ground truth labels are set as proportional to the number of patches
from each image. We then add a set of additional losses at the patch-level to
regularize and to encourage good representations at both the patch and image
levels. A ResNet-50 model trained on ImageNet using PatchMix exhibits superior
transfer learning capabilities across a wide array of benchmarks. Although
PatchMix can rely on random pairings and random grid-like patterns for mixing,
we explore evolutionary search as a guiding strategy to discover optimal
grid-like patterns and image pairing jointly. For this purpose, we conceive a
fitness function that bypasses the need to re-train a model to evaluate each
choice. In this way, PatchMix outperforms a base model on CIFAR-10 (+1.91),
CIFAR-100 (+5.31), Tiny Imagenet (+3.52), and ImageNet (+1.16) by significant
margins, also outperforming previous state-of-the-art pairwise augmentation
strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cascante_Bonilla_P/0/1/0/all/0/1"&gt;Paola Cascante-Bonilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1"&gt;Arshdeep Sekhon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yanjun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1"&gt;Vicente Ordonez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Voice and Biofeedback to Predict User Engagement during Requirements Interviews. (arXiv:2104.02410v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02410</id>
        <link href="http://arxiv.org/abs/2104.02410"/>
        <updated>2021-06-17T01:58:43.031Z</updated>
        <summary type="html"><![CDATA[Capturing users engagement is crucial for gathering feedback about the
features of a software product. In a market-driven context, current approaches
to collect and analyze users feedback are based on techniques leveraging
information extracted from product reviews and social media. These approaches
are hardly applicable in bespoke software development, or in contexts in which
one needs to gather information from specific users. In such cases, companies
need to resort to face-to-face interviews to get feedback on their products. In
this paper, we propose to utilize biometric data, in terms of physiological and
voice features, to complement interviews with information about the engagement
of the user on the discussed product-relevant topics. We evaluate our approach
by interviewing users while gathering their physiological data (i.e.,
biofeedback) using an Empatica E4 wristband, and capturing their voice through
the default audio-recorder of a common laptop. Our results show that we can
predict users' engagement by training supervised machine learning algorithms on
biometric data, and that voice features alone can be sufficiently effective.
The performance of the prediction algorithms is maximised when pre-processing
the training data with the synthetic minority oversampling technique (SMOTE).
The results of our work suggest that biofeedback and voice analysis can be used
to facilitate prioritization of requirements oriented to product improvement,
and to steer the interview based on users' engagement. Furthermore, the usage
of voice features can be particularly helpful for emotion-aware requirements
elicitation in remote communication, either performed by human analysts or
voice-based chatbots.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferrari_A/0/1/0/all/0/1"&gt;Alessio Ferrari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huichapa_T/0/1/0/all/0/1"&gt;Thaide Huichapa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spoletini_P/0/1/0/all/0/1"&gt;Paola Spoletini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novielli_N/0/1/0/all/0/1"&gt;Nicole Novielli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fucci_D/0/1/0/all/0/1"&gt;Davide Fucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girardi_D/0/1/0/all/0/1"&gt;Daniela Girardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voicy: Zero-Shot Non-Parallel Voice Conversion in Noisy Reverberant Environments. (arXiv:2106.08873v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08873</id>
        <link href="http://arxiv.org/abs/2106.08873"/>
        <updated>2021-06-17T01:58:43.023Z</updated>
        <summary type="html"><![CDATA[Voice Conversion (VC) is a technique that aims to transform the
non-linguistic information of a source utterance to change the perceived
identity of the speaker. While there is a rich literature on VC, most proposed
methods are trained and evaluated on clean speech recordings. However, many
acoustic environments are noisy and reverberant, severely restricting the
applicability of popular VC methods to such scenarios. To address this
limitation, we propose Voicy, a new VC framework particularly tailored for
noisy speech. Our method, which is inspired by the de-noising auto-encoders
framework, is comprised of four encoders (speaker, content, phonetic and
acoustic-ASR) and one decoder. Importantly, Voicy is capable of performing
non-parallel zero-shot VC, an important requirement for any VC system that
needs to work on speakers not seen during training. We have validated our
approach using a noisy reverberant version of the LibriSpeech dataset.
Experimental results show that Voicy outperforms other tested VC techniques in
terms of naturalness and target speaker similarity in noisy reverberant
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mottini_A/0/1/0/all/0/1"&gt;Alejandro Mottini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1"&gt;Jaime Lorenzo-Trueba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlapati_S/0/1/0/all/0/1"&gt;Sri Vishnu Kumar Karlapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drugman_T/0/1/0/all/0/1"&gt;Thomas Drugman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PRASEMap: A Probabilistic Reasoning and Semantic Embedding based Knowledge Graph Alignment System. (arXiv:2106.08801v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08801</id>
        <link href="http://arxiv.org/abs/2106.08801"/>
        <updated>2021-06-17T01:58:43.005Z</updated>
        <summary type="html"><![CDATA[Knowledge Graph (KG) alignment aims at finding equivalent entities and
relations (i.e., mappings) between two KGs. The existing approaches utilize
either reasoning-based or semantic embedding-based techniques, but few studies
explore their combination. In this demonstration, we present PRASEMap, an
unsupervised KG alignment system that iteratively computes the Mappings with
both Probabilistic Reasoning (PR) And Semantic Embedding (SE) techniques.
PRASEMap can support various embedding-based KG alignment approaches as the SE
module, and enables easy human computer interaction that additionally provides
an option for users to feed the mapping annotations back to the system for
better results. The demonstration showcases these features via a stand-alone
Web application with user friendly interfaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhiyuan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Vaccines: Characterizing Misinformation Campaigns and Vaccine Hesitancy on Twitter. (arXiv:2106.08423v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.08423</id>
        <link href="http://arxiv.org/abs/2106.08423"/>
        <updated>2021-06-17T01:58:42.999Z</updated>
        <summary type="html"><![CDATA[Vaccine hesitancy and misinformation on social media has increased concerns
about COVID-19 vaccine uptake required to achieve herd immunity and overcome
the pandemic. However anti-science and political misinformation and
conspiracies have been rampant throughout the pandemic. For COVID-19 vaccines,
we investigate misinformation and conspiracy campaigns and their characteristic
behaviours. We identify whether coordinated efforts are used to promote
misinformation in vaccine related discussions, and find accounts coordinately
promoting a `Great Reset' conspiracy group promoting vaccine related
misinformation and strong anti-vaccine and anti-social messages such as boycott
vaccine passports, no lock-downs and masks. We characterize other
misinformation communities from the information diffusion structure, and study
the large anti-vaccine misinformation community and smaller anti-vaccine
communities, including a far-right anti-vaccine conspiracy group. In comparison
with the mainstream and health news, left-leaning group, which are more
pro-vaccine, the right-leaning group is influenced more by the anti-vaccine and
far-right misinformation/conspiracy communities. The misinformation communities
are more vocal either specific to the vaccine discussion or political
discussion, and we find other differences in the characteristic behaviours of
different communities. Lastly, we investigate misinformation narratives and
tactics of information distortion that can increase vaccine hesitancy, using
topic modeling and comparison with reported vaccine side-effects (VAERS)
finding rarer side-effects are more frequently discussed on social media.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1"&gt;Karishma Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yizhou Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge-Adaptation Priors. (arXiv:2106.08769v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08769</id>
        <link href="http://arxiv.org/abs/2106.08769"/>
        <updated>2021-06-17T01:58:42.993Z</updated>
        <summary type="html"><![CDATA[Humans and animals have a natural ability to quickly adapt to their
surroundings, but machine-learning models, when subjected to changes, often
require a complete retraining from scratch. We present Knowledge-adaptation
priors (K-priors) to reduce the cost of retraining by enabling quick and
accurate adaptation for a wide-variety of tasks and models. This is made
possible by a combination of weight and function-space priors to reconstruct
the gradients of the past, which recovers and generalizes many existing, but
seemingly-unrelated, adaptation strategies. Training with simple first-order
gradient methods can often recover the exact retrained model to an arbitrary
accuracy by choosing a sufficiently large memory of the past data. Empirical
results confirm that the adaptation can be cheap and accurate, and a promising
alternative to retraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swaroop_S/0/1/0/all/0/1"&gt;Siddharth Swaroop&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Breaking The Dimension Dependence in Sparse Distribution Estimation under Communication Constraints. (arXiv:2106.08597v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08597</id>
        <link href="http://arxiv.org/abs/2106.08597"/>
        <updated>2021-06-17T01:58:42.986Z</updated>
        <summary type="html"><![CDATA[We consider the problem of estimating a $d$-dimensional $s$-sparse discrete
distribution from its samples observed under a $b$-bit communication
constraint. The best-known previous result on $\ell_2$ estimation error for
this problem is $O\left( \frac{s\log\left( {d}/{s}\right)}{n2^b}\right)$.
Surprisingly, we show that when sample size $n$ exceeds a minimum threshold
$n^*(s, d, b)$, we can achieve an $\ell_2$ estimation error of $O\left(
\frac{s}{n2^b}\right)$. This implies that when $n>n^*(s, d, b)$ the convergence
rate does not depend on the ambient dimension $d$ and is the same as knowing
the support of the distribution beforehand.

We next ask the question: ``what is the minimum $n^*(s, d, b)$ that allows
dimension-free convergence?''. To upper bound $n^*(s, d, b)$, we develop novel
localization schemes to accurately and efficiently localize the unknown
support. For the non-interactive setting, we show that $n^*(s, d, b) = O\left(
\min \left( {d^2\log^2 d}/{2^b}, {s^4\log^2 d}/{2^b}\right) \right)$. Moreover,
we connect the problem with non-adaptive group testing and obtain a
polynomial-time estimation scheme when $n = \tilde{\Omega}\left({s^4\log^4
d}/{2^b}\right)$. This group testing based scheme is adaptive to the sparsity
parameter $s$, and hence can be applied without knowing it. For the interactive
setting, we propose a novel tree-based estimation scheme and show that the
minimum sample-size needed to achieve dimension-free convergence can be further
reduced to $n^*(s, d, b) = \tilde{O}\left( {s^2\log^2 d}/{2^b} \right)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei-Ning Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kairouz_P/0/1/0/all/0/1"&gt;Peter Kairouz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ozgur_A/0/1/0/all/0/1"&gt;Ayfer &amp;#xd6;zg&amp;#xfc;r&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Unreliable Predictions by Shattering a Neural Network. (arXiv:2106.08365v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08365</id>
        <link href="http://arxiv.org/abs/2106.08365"/>
        <updated>2021-06-17T01:58:42.980Z</updated>
        <summary type="html"><![CDATA[Piecewise linear neural networks can be split into subfunctions, each with
its own activation pattern, domain, and empirical error. Empirical error for
the full network can be written as an expectation over empirical error of
subfunctions. Constructing a generalization bound on subfunction empirical
error indicates that the more densely a subfunction is surrounded by training
samples in representation space, the more reliable its predictions are.
Further, it suggests that models with fewer activation regions generalize
better, and models that abstract knowledge to a greater degree generalize
better, all else equal. We propose not only a theoretical framework to reason
about subfunction error bounds but also a pragmatic way of approximately
evaluating it, which we apply to predicting which samples the network will not
successfully generalize to. We test our method on detection of
misclassification and out-of-distribution samples, finding that it performs
competitively in both cases. In short, some network activation patterns are
associated with higher reliability than others, and these can be identified
using subfunction error bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xu Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1"&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hjelm_D/0/1/0/all/0/1"&gt;Devon Hjelm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1"&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Spiking Neural Network for Image Segmentation. (arXiv:2106.08921v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.08921</id>
        <link href="http://arxiv.org/abs/2106.08921"/>
        <updated>2021-06-17T01:58:42.973Z</updated>
        <summary type="html"><![CDATA[We seek to investigate the scalability of neuromorphic computing for computer
vision, with the objective of replicating non-neuromorphic performance on
computer vision tasks while reducing power consumption. We convert the deep
Artificial Neural Network (ANN) architecture U-Net to a Spiking Neural Network
(SNN) architecture using the Nengo framework. Both rate-based and spike-based
models are trained and optimized for benchmarking performance and power, using
a modified version of the ISBI 2D EM Segmentation dataset consisting of
microscope images of cells. We propose a partitioning method to optimize
inter-chip communication to improve speed and energy efficiency when deploying
multi-chip networks on the Loihi neuromorphic chip. We explore the advantages
of regularizing firing rates of Loihi neurons for converting ANN to SNN with
minimum accuracy loss and optimized energy consumption. We propose a percentile
based regularization loss function to limit the spiking rate of the neuron
between a desired range. The SNN is converted directly from the corresponding
ANN, and demonstrates similar semantic segmentation as the ANN using the same
number of neurons and weights. However, the neuromorphic implementation on the
Intel Loihi neuromorphic chip is over 2x more energy-efficient than
conventional hardware (CPU, GPU) when running online (one image at a time).
These power improvements are achieved without sacrificing the task performance
accuracy of the network, and when all weights (Loihi, CPU, and GPU networks)
are quantized to 8 bits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1"&gt;Kinjal Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hunsberger_E/0/1/0/all/0/1"&gt;Eric Hunsberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batir_S/0/1/0/all/0/1"&gt;Sean Batir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eliasmith_C/0/1/0/all/0/1"&gt;Chris Eliasmith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lorenz System State Stability Identification using Neural Networks. (arXiv:2106.08489v1 [math.DS])]]></title>
        <id>http://arxiv.org/abs/2106.08489</id>
        <link href="http://arxiv.org/abs/2106.08489"/>
        <updated>2021-06-17T01:58:42.956Z</updated>
        <summary type="html"><![CDATA[Nonlinear dynamical systems such as Lorenz63 equations are known to be
chaotic in nature and sensitive to initial conditions. As a result, a small
perturbation in the initial conditions results in deviation in state trajectory
after a few time steps. The algorithms and computational resources needed to
accurately identify the system states vary depending on whether the solution is
in transition region or not. We refer to the transition and non-transition
regions as unstable and stable regions respectively. We label a system state to
be stable if it's immediate past and future states reside in the same regime.
However, at a given time step we don't have the prior knowledge about whether
system is in stable or unstable region. In this paper, we develop and train a
feed forward (multi-layer perceptron) Neural Network to classify the system
states of a Lorenz system as stable and unstable. We pose this task as a
supervised learning problem where we train the neural network on Lorenz system
which have states labeled as stable or unstable. We then test the ability of
the neural network models to identify the stable and unstable states on a
different Lorenz system that is generated using different initial conditions.
We also evaluate the classification performance in the mismatched case i.e.,
when the initial conditions for training and validation data are sampled from
different intervals. We show that certain normalization schemes can greatly
improve the performance of neural networks in especially these mismatched
scenarios. The classification framework developed in the paper can be a
preprocessor for a larger context of sequential decision making framework where
the decision making is performed based on observed stable or unstable states.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Subramanian_M/0/1/0/all/0/1"&gt;Megha Subramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tipireddy_R/0/1/0/all/0/1"&gt;Ramakrishna Tipireddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chatterjee_S/0/1/0/all/0/1"&gt;Samrat Chatterjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask 3D CBCT-to-CT Translation and Organs-at-Risk Segmentation Using Physics-Based Data Augmentation. (arXiv:2103.05690v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05690</id>
        <link href="http://arxiv.org/abs/2103.05690"/>
        <updated>2021-06-17T01:58:42.949Z</updated>
        <summary type="html"><![CDATA[Purpose: In current clinical practice, noisy and artifact-ridden weekly
cone-beam computed tomography (CBCT) images are only used for patient setup
during radiotherapy. Treatment planning is done once at the beginning of the
treatment using high-quality planning CT (pCT) images and manual contours for
organs-at-risk (OARs) structures. If the quality of the weekly CBCT images can
be improved while simultaneously segmenting OAR structures, this can provide
critical information for adapting radiotherapy mid-treatment as well as for
deriving biomarkers for treatment response. Methods: Using a novel
physics-based data augmentation strategy, we synthesize a large dataset of
perfectly/inherently registered planning CT and synthetic-CBCT pairs for
locally advanced lung cancer patient cohort, which are then used in a multitask
3D deep learning framework to simultaneously segment and translate real weekly
CBCT images to high-quality planning CT-like images. Results: We compared the
synthetic CT and OAR segmentations generated by the model to real planning CT
and manual OAR segmentations and showed promising results. The real week 1
(baseline) CBCT images which had an average MAE of 162.77 HU compared to pCT
images are translated to synthetic CT images that exhibit a drastically
improved average MAE of 29.31 HU and average structural similarity of 92% with
the pCT images. The average DICE scores of the 3D organs-at-risk segmentations
are: lungs 0.96, heart 0.88, spinal cord 0.83 and esophagus 0.66. Conclusions:
We demonstrate an approach to translate artifact-ridden CBCT images to high
quality synthetic CT images while simultaneously generating good quality
segmentation masks for different organs-at-risk. This approach could allow
clinicians to adjust treatment plans using only the routine low-quality CBCT
images, potentially improving patient outcomes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dahiya_N/0/1/0/all/0/1"&gt;Navdeep Dahiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1"&gt;Sadegh R Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengpeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Si-Yuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yezzi_A/0/1/0/all/0/1"&gt;Anthony Yezzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1"&gt;Saad Nadeem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning with Uncertain Feedback Graphs. (arXiv:2106.08441v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08441</id>
        <link href="http://arxiv.org/abs/2106.08441"/>
        <updated>2021-06-17T01:58:42.942Z</updated>
        <summary type="html"><![CDATA[Online learning with expert advice is widely used in various machine learning
tasks. It considers the problem where a learner chooses one from a set of
experts to take advice and make a decision. In many learning problems, experts
may be related, henceforth the learner can observe the losses associated with a
subset of experts that are related to the chosen one. In this context, the
relationship among experts can be captured by a feedback graph, which can be
used to assist the learner's decision making. However, in practice, the nominal
feedback graph often entails uncertainties, which renders it impossible to
reveal the actual relationship among experts. To cope with this challenge, the
present work studies various cases of potential uncertainties, and develops
novel online learning algorithms to deal with uncertainties while making use of
the uncertain feedback graph. The proposed algorithms are proved to enjoy
sublinear regret under mild conditions. Experiments on real datasets are
presented to demonstrate the effectiveness of the novel algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghari_P/0/1/0/all/0/1"&gt;Pouya M Ghari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yanning Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Attacks on Deep Models for Financial Transaction Records. (arXiv:2106.08361v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08361</id>
        <link href="http://arxiv.org/abs/2106.08361"/>
        <updated>2021-06-17T01:58:42.936Z</updated>
        <summary type="html"><![CDATA[Machine learning models using transaction records as inputs are popular among
financial institutions. The most efficient models use deep-learning
architectures similar to those in the NLP community, posing a challenge due to
their tremendous number of parameters and limited robustness. In particular,
deep-learning models are vulnerable to adversarial attacks: a little change in
the input harms the model's output.

In this work, we examine adversarial attacks on transaction records data and
defences from these attacks. The transaction records data have a different
structure than the canonical NLP or time series data, as neighbouring records
are less connected than words in sentences, and each record consists of both
discrete merchant code and continuous transaction amount. We consider a
black-box attack scenario, where the attack doesn't know the true decision
model, and pay special attention to adding transaction tokens to the end of a
sequence. These limitations provide more realistic scenario, previously
unexplored in NLP world.

The proposed adversarial attacks and the respective defences demonstrate
remarkable performance using relevant datasets from the financial industry. Our
results show that a couple of generated transactions are sufficient to fool a
deep-learning model. Further, we improve model robustness via adversarial
training or separate adversarial examples detection. This work shows that
embedding protection from adversarial attacks improves model robustness,
allowing a wider adoption of deep models for transaction records in banking and
finance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fursov_I/0/1/0/all/0/1"&gt;Ivan Fursov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morozov_M/0/1/0/all/0/1"&gt;Matvey Morozov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaploukhaya_N/0/1/0/all/0/1"&gt;Nina Kaploukhaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovtun_E/0/1/0/all/0/1"&gt;Elizaveta Kovtun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rivera_Castro_R/0/1/0/all/0/1"&gt;Rodrigo Rivera-Castro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gusev_G/0/1/0/all/0/1"&gt;Gleb Gusev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babaev_D/0/1/0/all/0/1"&gt;Dmitry Babaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kireev_I/0/1/0/all/0/1"&gt;Ivan Kireev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1"&gt;Alexey Zaytsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainers in the Wild: Making Surrogate Explainers Robust to Distortions through Perception. (arXiv:2102.10951v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10951</id>
        <link href="http://arxiv.org/abs/2102.10951"/>
        <updated>2021-06-17T01:58:42.901Z</updated>
        <summary type="html"><![CDATA[Explaining the decisions of models is becoming pervasive in the image
processing domain, whether it is by using post-hoc methods or by creating
inherently interpretable models. While the widespread use of surrogate
explainers is a welcome addition to inspect and understand black-box models,
assessing the robustness and reliability of the explanations is key for their
success. Additionally, whilst existing work in the explainability field
proposes various strategies to address this problem, the challenges of working
with data in the wild is often overlooked. For instance, in image
classification, distortions to images can not only affect the predictions
assigned by the model, but also the explanation. Given a clean and a distorted
version of an image, even if the prediction probabilities are similar, the
explanation may still be different. In this paper we propose a methodology to
evaluate the effect of distortions in explanations by embedding perceptual
distances that tailor the neighbourhoods used to training surrogate explainers.
We also show that by operating in this way, we can make the explanations more
robust to distortions. We generate explanations for images in the Imagenet-C
dataset and demonstrate how using a perceptual distances in the surrogate
explainer creates more coherent explanations for the distorted and reference
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1"&gt;Alexander Hepburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1"&gt;Raul Santos-Rodriguez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of Automated Machine Learning Tools for SMS Spam Message Filtering. (arXiv:2106.08671v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08671</id>
        <link href="http://arxiv.org/abs/2106.08671"/>
        <updated>2021-06-17T01:58:42.886Z</updated>
        <summary type="html"><![CDATA[Short Message Service (SMS) is a very popular service used for communication
by mobile users. However, this popular service can be abused by executing
illegal activities and influencing security risks. Nowadays, many automatic
machine learning (AutoML) tools exist which can help domain experts and lay
users to build high-quality ML models with little or no machine learning
knowledge. In this work, a classification performance comparison was conducted
between three automatic ML tools for SMS spam message filtering. These tools
are mljar-supervised AutoML, H2O AutoML, and Tree-based Pipeline Optimization
Tool (TPOT) AutoML. Experimental results showed that ensemble models achieved
the best classification performance. The Stacked Ensemble model, which was
built using H2O AutoML, achieved the best performance in terms of Log Loss
(0.8370), true positive (1088/1116), and true negative (281/287) metrics. There
is a 19.05\% improvement in Log Loss with respect to TPOT AutoML and 10.53\%
improvement with respect to mljar-supervised AutoML. The satisfactory filtering
performance achieved with AutoML tools provides a potential application for
AutoML tools to automatically determine the best ML model that can perform best
for SMS spam message filtering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saeed_W/0/1/0/all/0/1"&gt;Waddah Saeed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imperfect ImaGANation: Implications of GANs Exacerbating Biases on Facial Data Augmentation and Snapchat Selfie Lenses. (arXiv:2001.09528v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09528</id>
        <link href="http://arxiv.org/abs/2001.09528"/>
        <updated>2021-06-17T01:58:42.880Z</updated>
        <summary type="html"><![CDATA[In this paper, we show that popular Generative Adversarial Networks (GANs)
exacerbate biases along the axes of gender and skin tone when given a skewed
distribution of face-shots. While practitioners celebrate synthetic data
generation using GANs as an economical way to augment data for training
data-hungry machine learning models, it is unclear whether they recognize the
perils of such techniques when applied to real world datasets biased along
latent dimensions. Specifically, we show that (1) traditional GANs further skew
the distribution of a dataset consisting of engineering faculty headshots,
generating minority modes less often and of worse quality and (2)
image-to-image translation (conditional) GANs also exacerbate biases by
lightening skin color of non-white faces and transforming female facial
features to be masculine when generating faces of engineering professors. Thus,
our study is meant to serve as a cautionary tale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1"&gt;Niharika Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olmo_A/0/1/0/all/0/1"&gt;Alberto Olmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1"&gt;Sailik Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manikonda_L/0/1/0/all/0/1"&gt;Lydia Manikonda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1"&gt;Subbarao Kambhampati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Developing a Fidelity Evaluation Approach for Interpretable Machine Learning. (arXiv:2106.08492v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08492</id>
        <link href="http://arxiv.org/abs/2106.08492"/>
        <updated>2021-06-17T01:58:42.871Z</updated>
        <summary type="html"><![CDATA[Although modern machine learning and deep learning methods allow for complex
and in-depth data analytics, the predictive models generated by these methods
are often highly complex, and lack transparency. Explainable AI (XAI) methods
are used to improve the interpretability of these complex models, and in doing
so improve transparency. However, the inherent fitness of these explainable
methods can be hard to evaluate. In particular, methods to evaluate the
fidelity of the explanation to the underlying black box require further
development, especially for tabular data. In this paper, we (a) propose a three
phase approach to developing an evaluation method; (b) adapt an existing
evaluation method primarily for image and text data to evaluate models trained
on tabular data; and (c) evaluate two popular explainable methods using this
evaluation method. Our evaluations suggest that the internal mechanism of the
underlying predictive model, the internal mechanism of the explainable method
used and model and data complexity all affect explanation fidelity. Given that
explanation fidelity is so sensitive to context and tools and data used, we
could not clearly identify any specific explainable method as being superior to
another.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Velmurugan_M/0/1/0/all/0/1"&gt;Mythreyi Velmurugan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1"&gt;Chun Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreira_C/0/1/0/all/0/1"&gt;Catarina Moreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sindhgatta_R/0/1/0/all/0/1"&gt;Renuka Sindhgatta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Evaluating Racial Biases in Image Captioning. (arXiv:2106.08503v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08503</id>
        <link href="http://arxiv.org/abs/2106.08503"/>
        <updated>2021-06-17T01:58:42.865Z</updated>
        <summary type="html"><![CDATA[Image captioning is an important task for benchmarking visual reasoning and
for enabling accessibility for people with vision impairments. However, as in
many machine learning settings, social biases can influence image captioning in
undesirable ways. In this work, we study bias propagation pathways within image
captioning, focusing specifically on the COCO dataset. Prior work has analyzed
gender bias in captions using automatically-derived gender labels; here we
examine racial and intersectional biases using manual annotations. Our first
contribution is in annotating the perceived gender and skin color of 28,315 of
the depicted people after obtaining IRB approval. Using these annotations, we
compare racial biases present in both manual and automatically-generated image
captions. We demonstrate differences in caption performance, sentiment, and
word choice between images of lighter versus darker-skinned people. Further, we
find the magnitude of these differences to be greater in modern captioning
systems compared to older ones, thus leading to concerns that without proper
consideration and mitigation these differences will only become increasingly
prevalent. Code and data is available at
https://princetonvisualai.github.io/imagecaptioning-bias .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dora Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Angelina Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1"&gt;Olga Russakovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evidence-based Factual Error Correction. (arXiv:2012.15788v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15788</id>
        <link href="http://arxiv.org/abs/2012.15788"/>
        <updated>2021-06-17T01:58:42.859Z</updated>
        <summary type="html"><![CDATA[This paper introduces the task of factual error correction: performing edits
to a claim so that the generated rewrite is better supported by evidence. This
extends the well-studied task of fact verification by providing a mechanism to
correct written texts that are refuted or only partially supported by evidence.
We demonstrate that it is feasible to train factual error correction systems
from existing fact checking datasets which only contain labeled claims
accompanied by evidence, but not the correction. We achieve this by employing a
two-stage distant supervision approach that incorporates evidence into masked
claims when generating corrections. Our approach, based on the T5 transformer
and using retrieved evidence, achieved better results than existing work which
used a pointer copy network and gold evidence, producing accurate factual error
corrections for 5x more instances in human evaluation and a .125 increase in
SARI score. The evaluation is conducted on a dataset of 65,000 instances based
on a recent fact verification shared task and we release it to enable further
work on the task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1"&gt;James Thorne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1"&gt;Andreas Vlachos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LaneAF: Robust Multi-Lane Detection with Affinity Fields. (arXiv:2103.12040v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12040</id>
        <link href="http://arxiv.org/abs/2103.12040"/>
        <updated>2021-06-17T01:58:42.840Z</updated>
        <summary type="html"><![CDATA[This study presents an approach to lane detection involving the prediction of
binary segmentation masks and per-pixel affinity fields. These affinity fields,
along with the binary masks, can then be used to cluster lane pixels
horizontally and vertically into corresponding lane instances in a
post-processing step. This clustering is achieved through a simple row-by-row
decoding process with little overhead; such an approach allows LaneAF to detect
a variable number of lanes without assuming a fixed or maximum number of lanes.
Moreover, this form of clustering is more interpretable in comparison to
previous visual clustering approaches, and can be analyzed to identify and
correct sources of error. Qualitative and quantitative results obtained on
popular lane detection datasets demonstrate the model's ability to detect and
cluster lanes effectively and robustly. Our proposed approach sets a new
state-of-the-art on the challenging CULane dataset and the recently introduced
Unsupervised LLAMAS dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abualsaud_H/0/1/0/all/0/1"&gt;Hala Abualsaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sean Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1"&gt;David Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Situ_K/0/1/0/all/0/1"&gt;Kenny Situ&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1"&gt;Akshay Rangesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1"&gt;Mohan M. Trivedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Silhouettes and quasi residual plots for neural nets and tree-based classifiers. (arXiv:2106.08814v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08814</id>
        <link href="http://arxiv.org/abs/2106.08814"/>
        <updated>2021-06-17T01:58:42.820Z</updated>
        <summary type="html"><![CDATA[Classification by neural nets and by tree-based methods are powerful tools of
machine learning. There exist interesting visualizations of the inner workings
of these and other classifiers. Here we pursue a different goal, which is to
visualize the cases being classified, either in training data or in test data.
An important aspect is whether a case has been classified to its given class
(label) or whether the classifier wants to assign it to different class. This
is reflected in the (conditional and posterior) probability of the alternative
class (PAC). A high PAC indicates label bias, i.e. the possibility that the
case was mislabeled. The PAC is used to construct a silhouette plot which is
similar in spirit to the silhouette plot for cluster analysis (Rousseeuw,
1987). The average silhouette width can be used to compare different
classifications of the same dataset. We will also draw quasi residual plots of
the PAC versus a data feature, which may lead to more insight in the data. One
of these data features is how far each case lies from its given class. The
graphical displays are illustrated and interpreted on benchmark data sets
containing images, mixed features, and tweets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Raymaekers_J/0/1/0/all/0/1"&gt;Jakob Raymaekers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rousseeuw_P/0/1/0/all/0/1"&gt;Peter J. Rousseeuw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion Dynamics in Movie Dialogues. (arXiv:2103.01345v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01345</id>
        <link href="http://arxiv.org/abs/2103.01345"/>
        <updated>2021-06-17T01:58:42.811Z</updated>
        <summary type="html"><![CDATA[Emotion dynamics is a framework for measuring how an individual's emotions
change over time. It is a powerful tool for understanding how we behave and
interact with the world. In this paper, we introduce a framework to track
emotion dynamics through one's utterances. Specifically we introduce a number
of utterance emotion dynamics (UED) metrics inspired by work in Psychology. We
use this approach to trace emotional arcs of movie characters. We analyze
thousands of such character arcs to test hypotheses that inform our broader
understanding of stories. Notably, we show that there is a tendency for
characters to use increasingly more negative words and become increasingly
emotionally discordant with each other until about 90 percent of the narrative
length. UED also has applications in behavior studies, social sciences, and
public health.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hipson_W/0/1/0/all/0/1"&gt;Will E. Hipson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1"&gt;Saif M. Mohammad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the long-term learning ability of LSTM LMs. (arXiv:2106.08927v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08927</id>
        <link href="http://arxiv.org/abs/2106.08927"/>
        <updated>2021-06-17T01:58:42.805Z</updated>
        <summary type="html"><![CDATA[We inspect the long-term learning ability of Long Short-Term Memory language
models (LSTM LMs) by evaluating a contextual extension based on the Continuous
Bag-of-Words (CBOW) model for both sentence- and discourse-level LSTM LMs and
by analyzing its performance. We evaluate on text and speech. Sentence-level
models using the long-term contextual module perform comparably to vanilla
discourse-level LSTM LMs. On the other hand, the extension does not provide
gains for discourse-level models. These findings indicate that discourse-level
LSTM LMs already rely on contextual information to perform long-term learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boes_W/0/1/0/all/0/1"&gt;Wim Boes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rompaey_R/0/1/0/all/0/1"&gt;Robbe Van Rompaey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verwimp_L/0/1/0/all/0/1"&gt;Lyan Verwimp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelemans_J/0/1/0/all/0/1"&gt;Joris Pelemans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+hamme_H/0/1/0/all/0/1"&gt;Hugo Van hamme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wambacq_P/0/1/0/all/0/1"&gt;Patrick Wambacq&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Black-Box Dissector: Towards Erasing-based Hard-Label Model Stealing Attack. (arXiv:2105.00623v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00623</id>
        <link href="http://arxiv.org/abs/2105.00623"/>
        <updated>2021-06-17T01:58:42.779Z</updated>
        <summary type="html"><![CDATA[Previous studies have verified that the functionality of black-box models can
be stolen with full probability outputs. However, under the more practical
hard-label setting, we observe that existing methods suffer from catastrophic
performance degradation. We argue this is due to the lack of rich information
in the probability prediction and the overfitting caused by hard labels. To
this end, we propose a novel hard-label model stealing method termed
\emph{black-box dissector}, which consists of two erasing-based modules. One is
a CAM-driven erasing strategy that is designed to increase the information
capacity hidden in hard labels from the victim model. The other is a
random-erasing-based self-knowledge distillation module that utilizes soft
labels from the substitute model to mitigate overfitting. Extensive experiments
on four widely-used datasets consistently demonstrate that our method
outperforms state-of-the-art methods, with an improvement of at most $8.27\%$.
We also validate the effectiveness and practical potential of our method on
real-world APIs and defense methods. Furthermore, our method promotes other
downstream tasks, \emph{i.e.}, transfer adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongjian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-trained Weights in Wide Neural Networks Align Layerwise to Error-scaled Input Correlations. (arXiv:2106.08453v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08453</id>
        <link href="http://arxiv.org/abs/2106.08453"/>
        <updated>2021-06-17T01:58:42.765Z</updated>
        <summary type="html"><![CDATA[Recent works have examined how deep neural networks, which can solve a
variety of difficult problems, incorporate the statistics of training data to
achieve their success. However, existing results have been established only in
limited settings. In this work, we derive the layerwise weight dynamics of
infinite-width neural networks with nonlinear activations trained by gradient
descent. We show theoretically that weight updates are aligned with input
correlations from intermediate layers weighted by error, and demonstrate
empirically that the result also holds in finite-width wide networks. The
alignment result allows us to formulate backpropagation-free learning rules,
named Align-zero and Align-ada, that theoretically achieve the same alignment
as backpropagation. Finally, we test these learning rules on benchmark problems
in feedforward and recurrent neural networks and demonstrate, in wide networks,
comparable performance to backpropagation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boopathy_A/0/1/0/all/0/1"&gt;Akhilan Boopathy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiete_I/0/1/0/all/0/1"&gt;Ila Fiete&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaze Preserving CycleGANs for Eyeglass Removal & Persistent Gaze Estimation. (arXiv:2002.02077v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02077</id>
        <link href="http://arxiv.org/abs/2002.02077"/>
        <updated>2021-06-17T01:58:42.757Z</updated>
        <summary type="html"><![CDATA[A driver's gaze is critical for determining their attention, state,
situational awareness, and readiness to take over control from partially
automated vehicles. Estimating the gaze direction is the most obvious way to
gauge a driver's state under ideal conditions when limited to using
non-intrusive imaging sensors. Unfortunately, the vehicular environment
introduces a variety of challenges that are usually unaccounted for - harsh
illumination, nighttime conditions, and reflective eyeglasses. Relying on head
pose alone under such conditions can prove to be unreliable and erroneous. In
this study, we offer solutions to address these problems encountered in the
real world. To solve issues with lighting, we demonstrate that using an
infrared camera with suitable equalization and normalization suffices. To
handle eyeglasses and their corresponding artifacts, we adopt image-to-image
translation using generative adversarial networks to pre-process images prior
to gaze estimation. Our proposed Gaze Preserving CycleGAN (GPCycleGAN) is
trained to preserve the driver's gaze while removing potential eyeglasses from
face images. GPCycleGAN is based on the well-known CycleGAN approach - with the
addition of a gaze classifier and a gaze consistency loss for additional
supervision. Our approach exhibits improved performance, interpretability,
robustness and superior qualitative results on challenging real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1"&gt;Akshay Rangesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bowen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1"&gt;Mohan M. Trivedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LARNet: Lie Algebra Residual Network for Face Recognition. (arXiv:2103.08147v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08147</id>
        <link href="http://arxiv.org/abs/2103.08147"/>
        <updated>2021-06-17T01:58:42.751Z</updated>
        <summary type="html"><![CDATA[Face recognition is an important yet challenging problem in computer vision.
A major challenge in practical face recognition applications lies in
significant variations between profile and frontal faces. Traditional
techniques address this challenge either by synthesizing frontal faces or by
pose invariant learning. In this paper, we propose a novel method with Lie
algebra theory to explore how face rotation in the 3D space affects the deep
feature generation process of convolutional neural networks (CNNs). We prove
that face rotation in the image space is equivalent to an additive residual
component in the feature space of CNNs, which is determined solely by the
rotation. Based on this theoretical finding, we further design a Lie Algebraic
Residual Network (LARNet) for tackling pose robust face recognition. Our LARNet
consists of a residual subnet for decoding rotation information from input face
images, and a gating subnet to learn rotation magnitude for controlling the
strength of the residual component contributing to the feature learning
process. Comprehensive experimental evaluations on both frontal-profile face
datasets and general face recognition datasets convincingly demonstrate that
our method consistently outperforms the state-of-the-art ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaolong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaohong Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1"&gt;Dihong Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1"&gt;Dong-Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Feature Alignment for Adversarial Training. (arXiv:2105.15157v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15157</id>
        <link href="http://arxiv.org/abs/2105.15157"/>
        <updated>2021-06-17T01:58:42.744Z</updated>
        <summary type="html"><![CDATA[Recent studies reveal that Convolutional Neural Networks (CNNs) are typically
vulnerable to adversarial attacks, which pose a threat to security-sensitive
applications. Many adversarial defense methods improve robustness at the cost
of accuracy, raising the contradiction between standard and adversarial
accuracies. In this paper, we observe an interesting phenomenon that feature
statistics change monotonically and smoothly w.r.t the rising of attacking
strength. Based on this observation, we propose the adaptive feature alignment
(AFA) to generate features of arbitrary attacking strengths. Our method is
trained to automatically align features of arbitrary attacking strength. This
is done by predicting a fusing weight in a dual-BN architecture. Unlike
previous works that need to either retrain the model or manually tune a
hyper-parameters for different attacking strengths, our method can deal with
arbitrary attacking strengths with a single model without introducing any
hyper-parameter. Importantly, our method improves the model robustness against
adversarial samples without incurring much loss in standard accuracy.
Experiments on CIFAR-10, SVHN, and tiny-ImageNet datasets demonstrate that our
method outperforms the state-of-the-art under a wide range of attacking
strengths.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruixin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xingyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuge Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaoxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Social Distance Estimation From Images: Performance Evaluation, Test Benchmark, and Algorithm. (arXiv:2103.06759v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06759</id>
        <link href="http://arxiv.org/abs/2103.06759"/>
        <updated>2021-06-17T01:58:42.733Z</updated>
        <summary type="html"><![CDATA[The COVID-19 virus has caused a global pandemic since March 2020. The World
Health Organization (WHO) has provided guidelines on how to reduce the spread
of the virus and one of the most important measures is social distancing.
Maintaining a minimum of one meter distance from other people is strongly
suggested to reduce the risk of infection. This has created a strong interest
in monitoring the social distances either as a safety measure or to study how
the measures have affected human behavior and country-wise differences in this.
The need for automatic social distance estimation algorithms is evident, but
there is no suitable test benchmark for such algorithms. Collecting images with
measured ground-truth pair-wise distances between all the people using
different camera settings is cumbersome. Furthermore, performance evaluation
for social distance estimation algorithms is not straightforward and there is
no widely accepted evaluation protocol. In this paper, we provide a dataset of
varying images with measured pair-wise social distances under different camera
positionings and focal length values. We suggest a performance evaluation
protocol and provide a benchmark to easily evaluate social distance estimation
algorithms. We also propose a method for automatic social distance estimation.
Our method takes advantage of object detection and human pose estimation. It
can be applied on any single image as long as focal length and sensor size
information are known. The results on our benchmark are encouraging with 92%
human detection rate and only 28.9% average error in distance estimation among
the detected people.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seker_M/0/1/0/all/0/1"&gt;Mert Seker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannisto_A/0/1/0/all/0/1"&gt;Anssi M&amp;#xe4;nnist&amp;#xf6;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1"&gt;Jenni Raitoharju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-MAN: Explaining multiple sources of anomalies in video. (arXiv:2106.08856v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08856</id>
        <link href="http://arxiv.org/abs/2106.08856"/>
        <updated>2021-06-17T01:58:42.690Z</updated>
        <summary type="html"><![CDATA[Our objective is to detect anomalies in video while also automatically
explaining the reason behind the detector's response. In a practical sense,
explainability is crucial for this task as the required response to an anomaly
depends on its nature and severity. However, most leading methods (based on
deep neural networks) are not interpretable and hide the decision making
process in uninterpretable feature representations. In an effort to tackle this
problem we make the following contributions: (1) we show how to build
interpretable feature representations suitable for detecting anomalies with
state of the art performance, (2) we propose an interpretable probabilistic
anomaly detector which can describe the reason behind it's response using high
level concepts, (3) we are the first to directly consider object interactions
for anomaly detection and (4) we propose a new task of explaining anomalies and
release a large dataset for evaluating methods on this task. Our method
competes well with the state of the art on public datasets while also providing
anomaly explanation based on objects and their interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Szymanowicz_S/0/1/0/all/0/1"&gt;Stanislaw Szymanowicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charles_J/0/1/0/all/0/1"&gt;James Charles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1"&gt;Roberto Cipolla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Person Re-identification via Multi-Label Prediction and Classification based on Graph-Structural Insight. (arXiv:2106.08798v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08798</id>
        <link href="http://arxiv.org/abs/2106.08798"/>
        <updated>2021-06-17T01:58:42.664Z</updated>
        <summary type="html"><![CDATA[This paper addresses unsupervised person re-identification (Re-ID) using
multi-label prediction and classification based on graph-structural insight.
Our method extracts features from person images and produces a graph that
consists of the features and a pairwise similarity of them as nodes and edges,
respectively. Based on the graph, the proposed graph structure based
multi-label prediction (GSMLP) method predicts multi-labels by considering the
pairwise similarity and the adjacency node distribution of each node. The
multi-labels created by GSMLP are applied to the proposed selective multi-label
classification (SMLC) loss. SMLC integrates a hard-sample mining scheme and a
multi-label classification. The proposed GSMLP and SMLC boost the performance
of unsupervised person Re-ID without any pre-labelled dataset. Experimental
results justify the superiority of the proposed method in unsupervised person
Re-ID by producing state-of-the-art performance. The source code for this paper
is publicly available on 'https://github.com/uknownpioneer/GSMLP-SMLC.git'.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jongmin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1"&gt;Hyeontaek Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Disentangle GAN Fingerprint for Fake Image Attribution. (arXiv:2106.08749v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08749</id>
        <link href="http://arxiv.org/abs/2106.08749"/>
        <updated>2021-06-17T01:58:42.651Z</updated>
        <summary type="html"><![CDATA[Rapid pace of generative models has brought about new threats to visual
forensics such as malicious personation and digital copyright infringement,
which promotes works on fake image attribution. Existing works on fake image
attribution mainly rely on a direct classification framework. Without
additional supervision, the extracted features could include many
content-relevant components and generalize poorly. Meanwhile, how to obtain an
interpretable GAN fingerprint to explain the decision remains an open question.
Adopting a multi-task framework, we propose a GAN Fingerprint Disentangling
Network (GFD-Net) to simultaneously disentangle the fingerprint from
GAN-generated images and produce a content-irrelevant representation for fake
image attribution. A series of constraints are provided to guarantee the
stability and discriminability of the fingerprint, which in turn helps
content-irrelevant feature extraction. Further, we perform comprehensive
analysis on GAN fingerprint, providing some clues about the properties of GAN
fingerprint and which factors dominate the fingerprint in GAN architecture.
Experiments show that our GFD-Net achieves superior fake image attribution
performance in both closed-world and open-world testing. We also apply our
method in binary fake image detection and exhibit a significant generalization
ability on unseen generators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianyun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Juan Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1"&gt;Qiang Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1"&gt;Jiaqi Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xirong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Sheng Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PGMAN: An Unsupervised Generative Multi-adversarial Network for Pan-sharpening. (arXiv:2012.09054v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09054</id>
        <link href="http://arxiv.org/abs/2012.09054"/>
        <updated>2021-06-17T01:58:42.627Z</updated>
        <summary type="html"><![CDATA[Pan-sharpening aims at fusing a low-resolution (LR) multi-spectral (MS) image
and a high-resolution (HR) panchromatic (PAN) image acquired by a satellite to
generate an HR MS image. Many deep learning based methods have been developed
in the past few years. However, since there are no intended HR MS images as
references for learning, almost all of the existing methods down-sample the MS
and PAN images and regard the original MS images as targets to form a
supervised setting for training. These methods may perform well on the
down-scaled images, however, they generalize poorly to the full-resolution
images. To conquer this problem, we design an unsupervised framework that is
able to learn directly from the full-resolution images without any
preprocessing. The model is built based on a novel generative multi-adversarial
network. We use a two-stream generator to extract the modality-specific
features from the PAN and MS images, respectively, and develop a
dual-discriminator to preserve the spectral and spatial information of the
inputs when performing fusion. Furthermore, a novel loss function is introduced
to facilitate training under the unsupervised setting. Experiments and
comparisons with other state-of-the-art methods on GaoFen-2 and QuickBird
images demonstrate that the proposed method can obtain much better fusion
results on the full-resolution images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huanyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Category- and Instance-Aware Pixel Embedding for Fast Panoptic Segmentation. (arXiv:2009.13342v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13342</id>
        <link href="http://arxiv.org/abs/2009.13342"/>
        <updated>2021-06-17T01:58:42.617Z</updated>
        <summary type="html"><![CDATA[Panoptic segmentation (PS) is a complex scene understanding task that
requires providing high-quality segmentation for both thing objects and stuff
regions. Previous methods handle these two classes with semantic and instance
segmentation modules separately, following with heuristic fusion or additional
modules to resolve the conflicts between the two outputs. This work simplifies
this pipeline of PS by consistently modeling the two classes with a novel PS
framework, which extends a detection model with an extra module to predict
category- and instance-aware pixel embedding (CIAE). CIAE is a novel pixel-wise
embedding feature that encodes both semantic-classification and
instance-distinction information. At the inference process, PS results are
simply derived by assigning each pixel to a detected instance or a stuff class
according to the learned embedding. Our method not only demonstrates fast
inference speed but also the first one-stage method to achieve comparable
performance to two-stage methods on the challenging COCO benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1"&gt;Naiyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Yanhu Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaiqi Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment Analysis Methods. (arXiv:2106.08829v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.08829</id>
        <link href="http://arxiv.org/abs/2106.08829"/>
        <updated>2021-06-17T01:58:42.610Z</updated>
        <summary type="html"><![CDATA[Opinion and sentiment analysis is a vital task to characterize subjective
information in social media posts. In this paper, we present a comprehensive
experimental evaluation and comparison with six state-of-the-art methods, from
which we have re-implemented one of them. In addition, we investigate different
textual and visual feature embeddings that cover different aspects of the
content, as well as the recently introduced multimodal CLIP embeddings.
Experimental results are presented for two different publicly available
benchmark datasets of tweets and corresponding images. In contrast to the
evaluation methodology of previous work, we introduce a reproducible and fair
evaluation scheme to make results comparable. Finally, we conduct an error
analysis to outline the limitations of the methods and possibilities for the
future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheema_G/0/1/0/all/0/1"&gt;Gullal S. Cheema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1"&gt;Sherzod Hakimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1"&gt;Eric M&amp;#xfc;ller-Budack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1"&gt;Ralph Ewerth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions. (arXiv:2012.04293v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04293</id>
        <link href="http://arxiv.org/abs/2012.04293"/>
        <updated>2021-06-17T01:58:42.604Z</updated>
        <summary type="html"><![CDATA[Humans are able to perceive, understand and reason about physical events.
Developing models with similar physical understanding capabilities is a
long-standing goal of artificial intelligence. As a step towards this goal, in
this work, we introduce CRAFT, a new visual question answering dataset that
requires causal reasoning about physical forces and object interactions. It
contains 58K video and question pairs that are generated from 10K videos from
20 different virtual environments, containing various objects in motion that
interact with each other and the scene. Two question categories from CRAFT
include previously studied descriptive and counterfactual questions. Besides,
inspired by the theories of force dynamics in cognitive linguistics, we
introduce new question categories that involve understanding the interactions
of objects through the notions of cause, enable, and prevent. Our results
demonstrate that even though these tasks seem to be simple and intuitive for
humans, the evaluated baseline models, including existing state-of-the-art
methods, do not yet deal with the challenges posed in our benchmark dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ates_T/0/1/0/all/0/1"&gt;Tayfun Ates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atesoglu_M/0/1/0/all/0/1"&gt;Muhammed Samil Atesoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yigit_C/0/1/0/all/0/1"&gt;Cagatay Yigit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kesen_I/0/1/0/all/0/1"&gt;Ilker Kesen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobas_M/0/1/0/all/0/1"&gt;Mert Kobas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1"&gt;Erkut Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1"&gt;Aykut Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goksun_T/0/1/0/all/0/1"&gt;Tilbe Goksun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1"&gt;Deniz Yuret&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured DropConnect for Uncertainty Inference in Image Classification. (arXiv:2106.08624v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08624</id>
        <link href="http://arxiv.org/abs/2106.08624"/>
        <updated>2021-06-17T01:58:42.598Z</updated>
        <summary type="html"><![CDATA[With the complexity of the network structure, uncertainty inference has
become an important task to improve the classification accuracy for artificial
intelligence systems. For image classification tasks, we propose a structured
DropConnect (SDC) framework to model the output of a deep neural network by a
Dirichlet distribution. We introduce a DropConnect strategy on weights in the
fully connected layers during training. In test, we split the network into
several sub-networks, and then model the Dirichlet distribution by match its
moments with the mean and variance of the outputs of these sub-networks. The
entropy of the estimated Dirichlet distribution is finally utilized for
uncertainty inference. In this paper, this framework is implemented on LeNet$5$
and VGG$16$ models for misclassification detection and out-of-distribution
detection on MNIST and CIFAR-$10$ datasets. Experimental results show that the
performance of the proposed SDC can be comparable to other uncertainty
inference methods. Furthermore, the SDC is adapted well to different network
structures with certain generalization capabilities and research prospects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wenqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weidong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhanyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking. (arXiv:2106.08816v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08816</id>
        <link href="http://arxiv.org/abs/2106.08816"/>
        <updated>2021-06-17T01:58:42.591Z</updated>
        <summary type="html"><![CDATA[Recently, the Siamese-based method has stood out from multitudinous tracking
methods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to
various special challenges in UAV tracking, \textit{e.g.}, severe occlusion,
and fast motion, most existing Siamese-based trackers hardly combine superior
performance with high efficiency. To this concern, in this paper, a novel
attentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking.
By virtue of the attention mechanism, the attentional aggregation network (AAN)
is conducted with self-AAN and cross-AAN, raising the expression ability of
features eventually. The former AAN aggregates and models the self-semantic
interdependencies of the single feature map via spatial and channel dimensions.
The latter aims to aggregate the cross-interdependencies of different semantic
features including the location information of anchors. In addition, the dual
features version of the anchor proposal network is proposed to raise the
robustness of proposing anchors, increasing the perception ability to objects
with various scales. Experiments on two well-known authoritative benchmarks are
conducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA
trackers. Besides, real-world tests onboard a typical embedded platform
demonstrate that SiamAPN++ achieves promising tracking results with real-time
speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Ziang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Junjie Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiming Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LCDNet: Deep Loop Closure Detection and Point Cloud Registration for LiDAR SLAM. (arXiv:2103.05056v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05056</id>
        <link href="http://arxiv.org/abs/2103.05056"/>
        <updated>2021-06-17T01:58:42.584Z</updated>
        <summary type="html"><![CDATA[Loop closure detection is an essential component of Simultaneous Localization
and Mapping (SLAM) systems, which reduces the drift accumulated over time. Over
the years, several deep learning approaches have been proposed to address this
task, however their performance has been subpar compared to handcrafted
techniques, especially while dealing with reverse loops. In this paper, we
introduce the novel LCDNet that effectively detects loop closures in LiDAR
point clouds by simultaneously identifying previously visited places and
estimating the 6-DoF relative transformation between the current scan and the
map. LCDNet is composed of a shared encoder, a place recognition head that
extracts global descriptors, and a relative pose head that estimates the
transformation between two point clouds. We introduce a novel relative pose
head based on the unbalanced optimal transport theory that we implement in a
differentiable manner to allow for end-to-end training. Extensive evaluations
of LCDNet on multiple real-world autonomous driving datasets show that our
approach outperforms state-of-the-art loop closure detection and point cloud
registration techniques by a large margin, especially while dealing with
reverse loops. Moreover, we integrate our proposed loop closure detection
approach into a LiDAR SLAM library to provide a complete mapping system and
demonstrate the generalization ability using different sensor setup in an
unseen city.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1"&gt;Daniele Cattaneo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1"&gt;Matteo Vaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1"&gt;Abhinav Valada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSRN: an Efficient Deep Network for Image Relighting. (arXiv:2102.09242v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09242</id>
        <link href="http://arxiv.org/abs/2102.09242"/>
        <updated>2021-06-17T01:58:42.566Z</updated>
        <summary type="html"><![CDATA[Custom and natural lighting conditions can be emulated in images of the scene
during post-editing. Extraordinary capabilities of the deep learning framework
can be utilized for such purpose. Deep image relighting allows automatic photo
enhancement by illumination-specific retouching. Most of the state-of-the-art
methods for relighting are run-time intensive and memory inefficient. In this
paper, we propose an efficient, real-time framework Deep Stacked Relighting
Network (DSRN) for image relighting by utilizing the aggregated features from
input image at different scales. Our model is very lightweight with total size
of about 42 MB and has an average inference time of about 0.0116s for image of
resolution $1024 \times 1024$ which is faster as compared to other multi-scale
models. Our solution is quite robust for translating image color temperature
from input image to target image and also performs moderately for light
gradient generation with respect to the target image. Additionally, we show
that if images illuminated from opposite directions are used as input, the
qualitative results improve over using a single input image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Sourya Dipta Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1"&gt;Nisarg A. Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Saikat Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_H/0/1/0/all/0/1"&gt;Himanshu Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLAWS: Clustering Assisted Weakly Supervised Learning with Normalcy Suppression for Anomalous Event Detection. (arXiv:2011.12077v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12077</id>
        <link href="http://arxiv.org/abs/2011.12077"/>
        <updated>2021-06-17T01:58:42.537Z</updated>
        <summary type="html"><![CDATA[Learning to detect real-world anomalous events through video-level labels is
a challenging task due to the rare occurrence of anomalies as well as noise in
the labels. In this work, we propose a weakly supervised anomaly detection
method which has manifold contributions including1) a random batch based
training procedure to reduce inter-batch correlation, 2) a normalcy suppression
mechanism to minimize anomaly scores of the normal regions of a video by taking
into account the overall information available in one training batch, and 3) a
clustering distance based loss to contribute towards mitigating the label noise
and to produce better anomaly representations by encouraging our model to
generate distinct normal and anomalous clusters. The proposed method
obtains83.03% and 89.67% frame-level AUC performance on the UCF Crime and
ShanghaiTech datasets respectively, demonstrating its superiority over the
existing state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Muhammad Zaigham Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1"&gt;Arif Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1"&gt;Marcella Astrid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seung-Ik Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Diffusion for Dense Depth Estimation from Multi-view Images. (arXiv:2106.08917v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08917</id>
        <link href="http://arxiv.org/abs/2106.08917"/>
        <updated>2021-06-17T01:58:42.529Z</updated>
        <summary type="html"><![CDATA[We present a method to estimate dense depth by optimizing a sparse set of
points such that their diffusion into a depth map minimizes a multi-view
reprojection error from RGB supervision. We optimize point positions, depths,
and weights with respect to the loss by differential splatting that models
points as Gaussians with analytic transmittance. Further, we develop an
efficient optimization routine that can simultaneously optimize the 50k+ points
required for complex scene reconstruction. We validate our routine using ground
truth data and show high reconstruction quality. Then, we apply this to light
field and wider baseline images via self supervision, and show improvements in
both average and outlier error for depth maps diffused from inaccurate sparse
points. Finally, we compare qualitative and quantitative results to image
processing and deep learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Numair Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Min H. Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1"&gt;James Tompkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point and Ask: Incorporating Pointing into Visual Question Answering. (arXiv:2011.13681v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13681</id>
        <link href="http://arxiv.org/abs/2011.13681"/>
        <updated>2021-06-17T01:58:42.514Z</updated>
        <summary type="html"><![CDATA[Visual Question Answering (VQA) has become one of the key benchmarks of
visual recognition progress. Multiple VQA extensions have been explored to
better simulate real-world settings: different question formulations, changing
training and test distributions, conversational consistency in dialogues, and
explanation-based answering. In this work, we further expand this space by
considering visual questions that include a spatial point of reference.
Pointing is a nearly universal gesture among humans, and real-world VQA is
likely to involve a gesture towards the target region.

Concretely, we (1) introduce and motivate point-input questions as an
extension of VQA, (2) define three novel classes of questions within this
space, and (3) for each class, introduce both a benchmark dataset and a series
of baseline models to handle its unique challenges. There are two key
distinctions from prior work. First, we explicitly design the benchmarks to
require the point input, i.e., we ensure that the visual question cannot be
answered accurately without the spatial reference. Second, we explicitly
explore the more realistic point spatial input rather than the standard but
unnatural bounding box input. Through our exploration we uncover and address
several visual recognition challenges, including the ability to infer human
intent, reason both locally and globally about the image, and effectively
combine visual, language and spatial inputs. Code is available at:
https://github.com/princetonvisualai/pointingqa .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mani_A/0/1/0/all/0/1"&gt;Arjun Mani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hinthorn_W/0/1/0/all/0/1"&gt;Will Hinthorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_N/0/1/0/all/0/1"&gt;Nobline Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1"&gt;Olga Russakovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptation with Variational Approximation for Cardiac Segmentation. (arXiv:2106.08752v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08752</id>
        <link href="http://arxiv.org/abs/2106.08752"/>
        <updated>2021-06-17T01:58:42.492Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation is useful in medical image segmentation.
Particularly, when ground truths of the target images are not available, domain
adaptation can train a target-specific model by utilizing the existing labeled
images from other modalities. Most of the reported works mapped images of both
the source and target domains into a common latent feature space, and then
reduced their discrepancy either implicitly with adversarial training or
explicitly by directly minimizing a discrepancy metric. In this work, we
propose a new framework, where the latent features of both domains are driven
towards a common and parameterized variational form, whose conditional
distribution given the image is Gaussian. This is achieved by two networks
based on variational auto-encoders (VAEs) and a regularization for this
variational approximation. Both of the VAEs, each for one domain, contain a
segmentation module, where the source segmentation is trained in a supervised
manner, while the target one is trained unsupervisedly. We validated the
proposed domain adaptation method using two cardiac segmentation tasks, i.e.,
the cross-modality (CT and MR) whole heart segmentation and the cross-sequence
cardiac MR segmentation. Results show that the proposed method achieved better
accuracies compared to two state-of-the-art approaches and demonstrated good
potential for cardiac segmentation. Furthermore, the proposed explicit
regularization was shown to be effective and efficient in narrowing down the
distribution gap between domains, which is useful for unsupervised domain
adaptation. Our code and data has been released via
https://zmiclab.github.io/projects.html.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fuping Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiahai Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Semi-Supervised Object Detection with Soft Teacher. (arXiv:2106.09018v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09018</id>
        <link href="http://arxiv.org/abs/2106.09018"/>
        <updated>2021-06-17T01:58:42.478Z</updated>
        <summary type="html"><![CDATA[This paper presents an end-to-end semi-supervised object detection approach,
in contrast to previous more complex multi-stage methods. The end-to-end
training gradually improves pseudo label qualities during the curriculum, and
the more and more accurate pseudo labels in turn benefit object detection
training. We also propose two simple yet effective techniques within this
framework: a soft teacher mechanism where the classification loss of each
unlabeled bounding box is weighed by the classification score produced by the
teacher network; a box jittering approach to select reliable pseudo boxes for
the learning of box regression. On COCO benchmark, the proposed approach
outperforms previous methods by a large margin under various labeling ratios,
i.e. 1\%, 5\% and 10\%. Moreover, our approach proves to perform also well when
the amount of labeled data is relatively large. For example, it can improve a
40.9 mAP baseline detector trained using the full COCO training set by +3.6
mAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the
state-of-the-art Swin Transformer-based object detector (58.9 mAP on test-dev),
it can still significantly improve the detection accuracy by +1.5 mAP, reaching
60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching
52.4 mAP, pushing the new state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengde Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Fangyun Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiang Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AtrialGeneral: Domain Generalization for Left Atrial Segmentation of Multi-Center LGE MRIs. (arXiv:2106.08727v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08727</id>
        <link href="http://arxiv.org/abs/2106.08727"/>
        <updated>2021-06-17T01:58:42.460Z</updated>
        <summary type="html"><![CDATA[Left atrial (LA) segmentation from late gadolinium enhanced magnetic
resonance imaging (LGE MRI) is a crucial step needed for planning the treatment
of atrial fibrillation. However, automatic LA segmentation from LGE MRI is
still challenging, due to the poor image quality, high variability in LA
shapes, and unclear LA boundary. Though deep learning-based methods can provide
promising LA segmentation results, they often generalize poorly to unseen
domains, such as data from different scanners and/or sites. In this work, we
collect 210 LGE MRIs from different centers with different levels of image
quality. To evaluate the domain generalization ability of models on the LA
segmentation task, we employ four commonly used semantic segmentation networks
for the LA segmentation from multi-center LGE MRIs. Besides, we investigate
three domain generalization strategies, i.e., histogram matching, mutual
information based disentangled representation, and random style transfer, where
a simple histogram matching is proved to be most effective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_V/0/1/0/all/0/1"&gt;Veronika A. Zimmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1"&gt;Julia A. Schnabel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiahai Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$C^3$: Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08914</id>
        <link href="http://arxiv.org/abs/2106.08914"/>
        <updated>2021-06-17T01:58:42.454Z</updated>
        <summary type="html"><![CDATA[Video-grounded dialogue systems aim to integrate video understanding and
dialogue understanding to generate responses that are relevant to both the
dialogue and video context. Most existing approaches employ deep learning
models and have achieved remarkable performance, given the relatively small
datasets available. However, the results are partly accomplished by exploiting
biases in the datasets rather than developing multimodal reasoning, resulting
in limited generalization. In this paper, we propose a novel approach of
Compositional Counterfactual Contrastive Learning ($C^3$) to develop
contrastive training between factual and counterfactual samples in
video-grounded dialogues. Specifically, we design factual/counterfactual
sampling based on the temporal steps in videos and tokens in dialogues and
propose contrastive loss functions that exploit object-level or action-level
variance. Different from prior approaches, we focus on contrastive hidden state
representations among compositional output tokens to optimize the
representation space in a generation setting. We achieved promising performance
gains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the
benefits of our approach in grounding video and dialogue context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hung Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nancy F. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1"&gt;Steven C.H. Hoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds. (arXiv:2007.07978v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.07978</id>
        <link href="http://arxiv.org/abs/2007.07978"/>
        <updated>2021-06-17T01:58:42.448Z</updated>
        <summary type="html"><![CDATA[Forecasting the formation and development of clouds is a central element of
modern weather forecasting systems. Incorrect clouds forecasts can lead to
major uncertainty in the overall accuracy of weather forecasts due to their
intrinsic role in the Earth's climate system. Few studies have tackled this
challenging problem from a machine learning point-of-view due to a shortage of
high-resolution datasets with many historical observations globally. In this
paper, we present a novel satellite-based dataset called ``CloudCast''. It
consists of 70,080 images with 10 different cloud types for multiple layers of
the atmosphere annotated on a pixel level. The spatial resolution of the
dataset is 928 x 1530 pixels (3x3 km per pixel) with 15-min intervals between
frames for the period 2017-01-01 to 2018-12-31. All frames are centered and
projected over Europe. To supplement the dataset, we conduct an evaluation
study with current state-of-the-art video prediction methods such as
convolutional long short-term memory networks, generative adversarial networks,
and optical flow-based extrapolation methods. As the evaluation of video
prediction is difficult in practice, we aim for a thorough evaluation in the
spatial and temporal domain. Our benchmark models show promising results but
with ample room for improvement. This is the first publicly available
global-scale dataset with high-resolution cloud types on a high temporal
granularity to the authors' best knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_A/0/1/0/all/0/1"&gt;A. H. Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;A. Iosifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karstoft_H/0/1/0/all/0/1"&gt;H. Karstoft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Generative Adversarial Networks in One Stage. (arXiv:2103.00430v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00430</id>
        <link href="http://arxiv.org/abs/2103.00430"/>
        <updated>2021-06-17T01:58:42.440Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have demonstrated unprecedented
success in various image generation tasks. The encouraging results, however,
come at the price of a cumbersome training process, during which the generator
and discriminator are alternately updated in two stages. In this paper, we
investigate a general training scheme that enables training GANs efficiently in
only one stage. Based on the adversarial losses of the generator and
discriminator, we categorize GANs into two classes, Symmetric GANs and
Asymmetric GANs, and introduce a novel gradient decomposition method to unify
the two, allowing us to train both classes in one stage and hence alleviate the
training effort. We also computationally analyze the efficiency of the proposed
method, and empirically demonstrate that, the proposed method yields a solid
$1.5\times$ acceleration across various datasets and network architectures.
Furthermore, we show that the proposed method is readily applicable to other
adversarial-training scenarios, such as data-free knowledge distillation. The
code is available at https://github.com/zju-vipa/OSGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chengchao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Youtan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xubin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jie Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Mingli Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Naturalness Evaluation Database for Video Prediction Models. (arXiv:2005.00356v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00356</id>
        <link href="http://arxiv.org/abs/2005.00356"/>
        <updated>2021-06-17T01:58:42.434Z</updated>
        <summary type="html"><![CDATA[The study of video prediction models is believed to be a fundamental approach
to representation learning for videos. While a plethora of generative models
for predicting the future frame pixel values given the past few frames exist,
the quantitative evaluation of the predicted frames has been found to be
extremely challenging. In this context, we introduce the problem of naturalness
evaluation, which refers to how natural or realistic a predicted video looks.
We create the Indian Institute of Science VIdeo Naturalness Evaluation (IISc
VINE) Database consisting of 300 videos, obtained by applying different
prediction models on different datasets, and accompanying human opinion scores.
We collected subjective ratings of naturalness from 50 human participants for
these videos. Our subjective study reveals that human observers were highly
consistent in their judgments of naturalness. We benchmark several popularly
used measures for evaluating video prediction and show that they do not
adequately correlate with these subjective scores. We introduce two new
features to effectively capture naturalness, motion-compensated cosine
similarities of deep features of predicted frames with past frames, and deep
features extracted from rescaled frame differences. We show that our feature
design leads to state of the art naturalness prediction in accordance with
human judgments on our IISc VINE Database. The database and code are publicly
available on our project website:
https://nagabhushansn95.github.io/publications/2020/vine]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Somraj_N/0/1/0/all/0/1"&gt;Nagabhushan Somraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kashi_M/0/1/0/all/0/1"&gt;Manoj Surya Kashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arun_S/0/1/0/all/0/1"&gt;S. P. Arun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soundararajan_R/0/1/0/all/0/1"&gt;Rajiv Soundararajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JRDB-Act: A Large-scale Multi-modal Dataset for Spatio-temporal Action, Social Group and Activity Detection. (arXiv:2106.08827v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08827</id>
        <link href="http://arxiv.org/abs/2106.08827"/>
        <updated>2021-06-17T01:58:42.411Z</updated>
        <summary type="html"><![CDATA[The availability of large-scale video action understanding datasets has
facilitated advances in the interpretation of visual scenes containing people.
However, learning to recognize human activities in an unconstrained real-world
environment, with potentially highly unbalanced and long-tailed distributed
data remains a significant challenge, not least owing to the lack of a
reflective large-scale dataset. Most existing large-scale datasets are either
collected from a specific or constrained environment, e.g. kitchens or rooms,
or video sharing platforms such as YouTube. In this paper, we introduce
JRDB-Act, a multi-modal dataset, as an extension of the existing JRDB, which is
captured by asocial mobile manipulator and reflects a real distribution of
human daily life actions in a university campus environment. JRDB-Act has been
densely annotated with atomic actions, comprises over 2.8M action labels,
constituting a large-scale spatio-temporal action detection dataset. Each human
bounding box is labelled with one pose-based action label and multiple
(optional) interaction-based action labels. Moreover JRDB-Act comes with social
group identification annotations conducive to the task of grouping individuals
based on their interactions in the scene to infer their social activities
(common activities in each social group).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ehsanpour_M/0/1/0/all/0/1"&gt;Mahsa Ehsanpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saleh_F/0/1/0/all/0/1"&gt;Fatemeh Saleh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1"&gt;Ian Reid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1"&gt;Hamid Rezatofighi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SEOVER: Sentence-level Emotion Orientation Vector based Conversation Emotion Recognition Model. (arXiv:2106.08785v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08785</id>
        <link href="http://arxiv.org/abs/2106.08785"/>
        <updated>2021-06-17T01:58:42.405Z</updated>
        <summary type="html"><![CDATA[For the task of conversation emotion recognition, recent works focus on
speaker relationship modeling but ignore the role of utterance's emotional
tendency.In this paper, we propose a new expression paradigm of sentence-level
emotion orientation vector to model the potential correlation of emotions
between sentence vectors. Based on it, we design an emotion recognition model,
which extracts the sentence-level emotion orientation vectors from the language
model and jointly learns from the dialogue sentiment analysis model and
extracted sentence-level emotion orientation vectors to identify the speaker's
emotional orientation during the conversation. We conduct experiments on two
benchmark datasets and compare them with the five baseline models.The
experimental results show that our model has better performance on all data
sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zaijing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1"&gt;Fengxiao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1"&gt;Tieyu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yusen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Ming Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predictive coding feedback results in perceived illusory contours in a recurrent neural network. (arXiv:2102.01955v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01955</id>
        <link href="http://arxiv.org/abs/2102.01955"/>
        <updated>2021-06-17T01:58:42.399Z</updated>
        <summary type="html"><![CDATA[Modern feedforward convolutional neural networks (CNNs) can now solve some
computer vision tasks at super-human levels. However, these networks only
roughly mimic human visual perception. One difference from human vision is that
they do not appear to perceive illusory contours (e.g. Kanizsa squares) in the
same way humans do. Physiological evidence from visual cortex suggests that the
perception of illusory contours could involve feedback connections. Would
recurrent feedback neural networks perceive illusory contours like humans? In
this work we equip a deep feedforward convolutional network with brain-inspired
recurrent dynamics. The network was first pretrained with an unsupervised
reconstruction objective on a natural image dataset, to expose it to natural
object contour statistics. Then, a classification decision layer was added and
the model was finetuned on a form discrimination task: squares vs. randomly
oriented inducer shapes (no illusory contour). Finally, the model was tested
with the unfamiliar ''illusory contour'' configuration: inducer shapes oriented
to form an illusory square. Compared with feedforward baselines, the iterative
''predictive coding'' feedback resulted in more illusory contours being
classified as physical squares. The perception of the illusory contour was
measurable in the luminance profile of the image reconstructions produced by
the model, demonstrating that the model really ''sees'' the illusion. Ablation
studies revealed that natural image pretraining and feedback error correction
are both critical to the perception of the illusion. Finally we validated our
conclusions in a deeper network (VGG): adding the same predictive coding
feedback dynamics again leads to the perception of illusory contours.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_Z/0/1/0/all/0/1"&gt;Zhaoyang Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OMay_C/0/1/0/all/0/1"&gt;Callum Biggs O&amp;#x27;May&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choksi_B/0/1/0/all/0/1"&gt;Bhavin Choksi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1"&gt;Rufin VanRullen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathological voice adaptation with autoencoder-based voice conversion. (arXiv:2106.08427v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08427</id>
        <link href="http://arxiv.org/abs/2106.08427"/>
        <updated>2021-06-17T01:58:42.393Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a new approach to pathological speech synthesis.
Instead of using healthy speech as a source, we customise an existing
pathological speech sample to a new speaker's voice characteristics. This
approach alleviates the evaluation problem one normally has when converting
typical speech to pathological speech, as in our approach, the voice conversion
(VC) model does not need to be optimised for speech degradation but only for
the speaker change. This change in the optimisation ensures that any
degradation found in naturalness is due to the conversion process and not due
to the model exaggerating characteristics of a speech pathology. To show a
proof of concept of this method, we convert dysarthric speech using the
UASpeech database and an autoencoder-based VC technique. Subjective evaluation
results show reasonable naturalness for high intelligibility dysarthric
speakers, though lower intelligibility seems to introduce a marginal
degradation in naturalness scores for mid and low intelligibility speakers
compared to ground truth. Conversion of speaker characteristics for low and
high intelligibility speakers is successful, but not for mid. Whether the
differences in the results for the different intelligibility levels is due to
the intelligibility levels or due to the speakers needs to be further
investigated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Illa_M/0/1/0/all/0/1"&gt;Marc Illa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halpern_B/0/1/0/all/0/1"&gt;Bence Mark Halpern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Son_R/0/1/0/all/0/1"&gt;Rob van Son&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moro_Velazquez_L/0/1/0/all/0/1"&gt;Laureano Moro-Velazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scharenborg_O/0/1/0/all/0/1"&gt;Odette Scharenborg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved CNN-based Learning of Interpolation Filters for Low-Complexity Inter Prediction in Video Coding. (arXiv:2106.08936v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08936</id>
        <link href="http://arxiv.org/abs/2106.08936"/>
        <updated>2021-06-17T01:58:42.374Z</updated>
        <summary type="html"><![CDATA[The versatility of recent machine learning approaches makes them ideal for
improvement of next generation video compression solutions. Unfortunately,
these approaches typically bring significant increases in computational
complexity and are difficult to interpret into explainable models, affecting
their potential for implementation within practical video coding applications.
This paper introduces a novel explainable neural network-based inter-prediction
scheme, to improve the interpolation of reference samples needed for fractional
precision motion compensation. The approach requires a single neural network to
be trained from which a full quarter-pixel interpolation filter set is derived,
as the network is easily interpretable due to its linear structure. A novel
training framework enables each network branch to resemble a specific
fractional shift. This practical solution makes it very efficient to use
alongside conventional video coding schemes. When implemented in the context of
the state-of-the-art Versatile Video Coding (VVC) test model, 0.77%, 1.27% and
2.25% BD-rate savings can be achieved on average for lower resolution sequences
under the random access, low-delay B and low-delay P configurations,
respectively, while the complexity of the learned interpolation schemes is
significantly reduced compared to the interpolation with full CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1"&gt;Luka Murn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blasi_S/0/1/0/all/0/1"&gt;Saverio Blasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smeaton_A/0/1/0/all/0/1"&gt;Alan F. Smeaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1"&gt;Marta Mrak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-and-Under Complete Convolutional RNN for MRI Reconstruction. (arXiv:2106.08886v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08886</id>
        <link href="http://arxiv.org/abs/2106.08886"/>
        <updated>2021-06-17T01:58:42.368Z</updated>
        <summary type="html"><![CDATA[Reconstructing magnetic resonance (MR) images from undersampled data is a
challenging problem due to various artifacts introduced by the under-sampling
operation. Recent deep learning-based methods for MR image reconstruction
usually leverage a generic auto-encoder architecture which captures low-level
features at the initial layers and high?level features at the deeper layers.
Such networks focus much on global features which may not be optimal to
reconstruct the fully-sampled image. In this paper, we propose an
Over-and-Under Complete Convolu?tional Recurrent Neural Network (OUCR), which
consists of an overcomplete and an undercomplete Convolutional Recurrent Neural
Network(CRNN). The overcomplete branch gives special attention in learning
local structures by restraining the receptive field of the network. Combining
it with the undercomplete branch leads to a network which focuses more on
low-level features without losing out on the global structures. Extensive
experiments on two datasets demonstrate that the proposed method achieves
significant improvements over the compressed sensing and popular deep
learning-based methods with less number of trainable parameters. Our code is
available at https://github.com/guopengf/OUCR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1"&gt;Pengfei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1"&gt;Jeya Maria Jose Valanarasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Puyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shanshan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided interactive image segmentation using machine learning and color based data set clustering. (arXiv:2005.07662v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07662</id>
        <link href="http://arxiv.org/abs/2005.07662"/>
        <updated>2021-06-17T01:58:42.361Z</updated>
        <summary type="html"><![CDATA[We present a novel approach that combines machine learning based interactive
image segmentation with a two-stage clustering method to identify similarly
colored images for efficient batch image segmentation by guided reuse of
classifiers. The segmentation task is formulated as a supervised machine
learning problem working on homogeneous groups of voxels termed supervoxels.
Classifiers are interactively trained from sparse annotations in an iterative
process of annotation refinement. Resulting models can be used for batch
processing of previously unseen images. By clustering images into subsets of
similar colorization, we identify a minimal set of prototype images and
demonstrate that using only classifiers trained on these prototype images for
their color-cluster significantly improves the average segmentation performance
of batch processing. The presented methods are applicable for almost any image
type and therefore represent a useful tool for image analysis tasks in general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Friebel_A/0/1/0/all/0/1"&gt;Adrian Friebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johann_T/0/1/0/all/0/1"&gt;Tim Johann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drasdo_D/0/1/0/all/0/1"&gt;Dirk Drasdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoehme_S/0/1/0/all/0/1"&gt;Stefan Hoehme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch. (arXiv:2106.08970v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08970</id>
        <link href="http://arxiv.org/abs/2106.08970"/>
        <updated>2021-06-17T01:58:42.347Z</updated>
        <summary type="html"><![CDATA[As the curation of data for machine learning becomes increasingly automated,
dataset tampering is a mounting threat. Backdoor attackers tamper with training
data to embed a vulnerability in models that are trained on that data. This
vulnerability is then activated at inference time by placing a "trigger" into
the model's input. Typical backdoor attacks insert the trigger directly into
the training data, although the presence of such an attack may be visible upon
inspection. In contrast, the Hidden Trigger Backdoor Attack achieves poisoning
without placing a trigger into the training data at all. However, this hidden
trigger attack is ineffective at poisoning neural networks trained from
scratch. We develop a new hidden trigger attack, Sleeper Agent, which employs
gradient matching, data selection, and target model re-training during the
crafting process. Sleeper Agent is the first hidden trigger backdoor attack to
be effective against neural networks trained from scratch. We demonstrate its
effectiveness on ImageNet and in black-box settings. Our implementation code
can be found at https://github.com/hsouri/Sleeper-Agent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Souri_H/0/1/0/all/0/1"&gt;Hossein Souri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1"&gt;Liam Fowl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1"&gt;Rama Chellappa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-independent User Simulation with Transformers for Task-oriented Dialogue Systems. (arXiv:2106.08838v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08838</id>
        <link href="http://arxiv.org/abs/2106.08838"/>
        <updated>2021-06-17T01:58:42.340Z</updated>
        <summary type="html"><![CDATA[Dialogue policy optimisation via reinforcement learning requires a large
number of training interactions, which makes learning with real users time
consuming and expensive. Many set-ups therefore rely on a user simulator
instead of humans. These user simulators have their own problems. While
hand-coded, rule-based user simulators have been shown to be sufficient in
small, simple domains, for complex domains the number of rules quickly becomes
intractable. State-of-the-art data-driven user simulators, on the other hand,
are still domain-dependent. This means that adaptation to each new domain
requires redesigning and retraining. In this work, we propose a
domain-independent transformer-based user simulator (TUS). The structure of our
TUS is not tied to a specific domain, enabling domain generalisation and
learning of cross-domain user behaviour from data. We compare TUS with the
state of the art using automatic as well as human evaluations. TUS can compete
with rule-based user simulators on pre-defined domains and is able to
generalise to unseen domains in a zero-shot fashion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hsien-chin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1"&gt;Nurul Lubis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Songbo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1"&gt;Carel van Niekerk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1"&gt;Christian Geishauser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1"&gt;Michael Heck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shutong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1"&gt;Milica Ga&amp;#x161;i&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving filling level classification with adversarial training. (arXiv:2102.04057v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04057</id>
        <link href="http://arxiv.org/abs/2102.04057"/>
        <updated>2021-06-17T01:58:42.334Z</updated>
        <summary type="html"><![CDATA[We investigate the problem of classifying - from a single image - the level
of content in a cup or a drinking glass. This problem is made challenging by
several ambiguities caused by transparencies, shape variations and partial
occlusions, and by the availability of only small training datasets. In this
paper, we tackle this problem with an appropriate strategy for transfer
learning. Specifically, we use adversarial training in a generic source dataset
and then refine the training with a task-specific dataset. We also discuss and
experimentally evaluate several training strategies and their combination on a
range of container types of the CORSMAL Containers Manipulation dataset. We
show that transfer learning with adversarial training in the source domain
consistently improves the classification accuracy on the test set and limits
the overfitting of the classifier to specific features of the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Modas_A/0/1/0/all/0/1"&gt;Apostolos Modas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1"&gt;Alessio Xompero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Matilla_R/0/1/0/all/0/1"&gt;Ricardo Sanchez-Matilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1"&gt;Pascal Frossard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1"&gt;Andrea Cavallaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GelSight Wedge: Measuring High-Resolution 3D Contact Geometry with a Compact Robot Finger. (arXiv:2106.08851v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.08851</id>
        <link href="http://arxiv.org/abs/2106.08851"/>
        <updated>2021-06-17T01:58:42.317Z</updated>
        <summary type="html"><![CDATA[Vision-based tactile sensors have the potential to provide important contact
geometry to localize the objective with visual occlusion. However, it is
challenging to measure high-resolution 3D contact geometry for a compact robot
finger, to simultaneously meet optical and mechanical constraints. In this
work, we present the GelSight Wedge sensor, which is optimized to have a
compact shape for robot fingers, while achieving high-resolution 3D
reconstruction. We evaluate the 3D reconstruction under different lighting
configurations, and extend the method from 3 lights to 1 or 2 lights. We
demonstrate the flexibility of the design by shrinking the sensor to the size
of a human finger for fine manipulation tasks. We also show the effectiveness
and potential of the reconstructed 3D geometry for pose tracking in the 3D
space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shaoxiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+She_Y/0/1/0/all/0/1"&gt;Yu She&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romero_B/0/1/0/all/0/1"&gt;Branden Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adelson_E/0/1/0/all/0/1"&gt;Edward Adelson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent. (arXiv:2009.04709v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04709</id>
        <link href="http://arxiv.org/abs/2009.04709"/>
        <updated>2021-06-17T01:58:42.311Z</updated>
        <summary type="html"><![CDATA[Adversarial training, especially projected gradient descent (PGD), has been a
successful approach for improving robustness against adversarial attacks. After
adversarial training, gradients of models with respect to their inputs have a
preferential direction. However, the direction of alignment is not
mathematically well established, making it difficult to evaluate
quantitatively. We propose a novel definition of this direction as the
direction of the vector pointing toward the closest point of the support of the
closest inaccurate class in decision space. To evaluate the alignment with this
direction after adversarial training, we apply a metric that uses generative
adversarial networks to produce the smallest residual needed to change the
class present in the image. We show that PGD-trained models have a higher
alignment than the baseline according to our definition, that our metric
presents higher alignment values than a competing metric formulation, and that
enforcing this alignment increases the robustness of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lanfredi_R/0/1/0/all/0/1"&gt;Ricardo Bigolin Lanfredi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schroeder_J/0/1/0/all/0/1"&gt;Joyce D. Schroeder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tasdizen_T/0/1/0/all/0/1"&gt;Tolga Tasdizen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning with Continuous Proxy Meta-Data for 3D MRI Classification. (arXiv:2106.08808v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08808</id>
        <link href="http://arxiv.org/abs/2106.08808"/>
        <updated>2021-06-17T01:58:42.304Z</updated>
        <summary type="html"><![CDATA[Traditional supervised learning with deep neural networks requires a
tremendous amount of labelled data to converge to a good solution. For 3D
medical images, it is often impractical to build a large homogeneous annotated
dataset for a specific pathology. Self-supervised methods offer a new way to
learn a representation of the images in an unsupervised manner with a neural
network. In particular, contrastive learning has shown great promises by
(almost) matching the performance of fully-supervised CNN on vision tasks.
Nonetheless, this method does not take advantage of available meta-data, such
as participant's age, viewed as prior knowledge. Here, we propose to leverage
continuous proxy metadata, in the contrastive learning framework, by
introducing a new loss called y-Aware InfoNCE loss. Specifically, we improve
the positive sampling during pre-training by adding more positive examples with
similar proxy meta-data with the anchor, assuming they share similar
discriminative semantic features.With our method, a 3D CNN model pre-trained on
$10^4$ multi-site healthy brain MRI scans can extract relevant features for
three classification tasks: schizophrenia, bipolar diagnosis and Alzheimer's
detection. When fine-tuned, it also outperforms 3D CNN trained from scratch on
these tasks, as well as state-of-the-art self-supervised methods. Our code is
made publicly available here.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dufumier_B/0/1/0/all/0/1"&gt;Benoit Dufumier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1"&gt;Pietro Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Victor_J/0/1/0/all/0/1"&gt;Julie Victor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grigis_A/0/1/0/all/0/1"&gt;Antoine Grigis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wessa_M/0/1/0/all/0/1"&gt;Michel Wessa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brambilla_P/0/1/0/all/0/1"&gt;Paolo Brambilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favre_P/0/1/0/all/0/1"&gt;Pauline Favre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polosan_M/0/1/0/all/0/1"&gt;Mircea Polosan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_C/0/1/0/all/0/1"&gt;Colm McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piguet_C/0/1/0/all/0/1"&gt;Camille Marie Piguet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duchesnay_E/0/1/0/all/0/1"&gt;Edouard Duchesnay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An unifying point of view on expressive power of GNNs. (arXiv:2106.08992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08992</id>
        <link href="http://arxiv.org/abs/2106.08992"/>
        <updated>2021-06-17T01:58:42.297Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are a wide class of connectionist models for
graph processing. They perform an iterative message passing operation on each
node and its neighbors, to solve classification/ clustering tasks --- on some
nodes or on the whole graph --- collecting all such messages, regardless of
their order. Despite the differences among the various models belonging to this
class, most of them adopt the same computation scheme, based on a local
aggregation mechanism and, intuitively, the local computation framework is
mainly responsible for the expressive power of GNNs. In this paper, we prove
that the Weisfeiler--Lehman test induces an equivalence relationship on the
graph nodes that exactly corresponds to the unfolding equivalence, defined on
the original GNN model. Therefore, the results on the expressive power of the
original GNNs can be extended to general GNNs which, under mild conditions, can
be proved capable of approximating, in probability and up to any precision, any
function on graphs that respects the unfolding equivalence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DInverno_G/0/1/0/all/0/1"&gt;Giuseppe Alessio D&amp;#x27;Inverno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bianchini_M/0/1/0/all/0/1"&gt;Monica Bianchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sampoli_M/0/1/0/all/0/1"&gt;Maria Lucia Sampoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1"&gt;Franco Scarselli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing. (arXiv:2011.09899v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09899</id>
        <link href="http://arxiv.org/abs/2011.09899"/>
        <updated>2021-06-17T01:58:42.280Z</updated>
        <summary type="html"><![CDATA[User data confidentiality protection is becoming a rising challenge in the
present deep learning research. Without access to data, conventional
data-driven model compression faces a higher risk of performance degradation.
Recently, some works propose to generate images from a specific pretrained
model to serve as training data. However, the inversion process only utilizes
biased feature statistics stored in one model and is from low-dimension to
high-dimension. As a consequence, it inevitably encounters the difficulties of
generalizability and inexact inversion, which leads to unsatisfactory
performance. To address these problems, we propose MixMix based on two simple
yet effective techniques: (1) Feature Mixing: utilizes various models to
construct a universal feature space for generalized inversion; (2) Data Mixing:
mixes the synthesized images and labels to generate exact label information. We
prove the effectiveness of MixMix from both theoretical and empirical
perspectives. Extensive experiments show that MixMix outperforms existing
methods on the mainstream compression tasks, including quantization, knowledge
distillation, and pruning. Specifically, MixMix achieves up to 4% and 20%
accuracy uplift on quantization and pruning, respectively, compared to existing
data-free compression work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Feng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Ruihao Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1"&gt;Mingzhu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fengwei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shaoqing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shi Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local plasticity rules can learn deep representations using self-supervised contrastive predictions. (arXiv:2010.08262v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08262</id>
        <link href="http://arxiv.org/abs/2010.08262"/>
        <updated>2021-06-17T01:58:42.274Z</updated>
        <summary type="html"><![CDATA[Learning in the brain is poorly understood and learning rules that respect
biological constraints, yet yield deep hierarchical representations, are still
unknown. Here, we propose a learning rule that takes inspiration from
neuroscience and recent advances in self-supervised deep learning. Learning
minimizes a simple layer-specific loss function and does not need to
back-propagate error signals within or between layers. Instead, weight updates
follow a local, Hebbian, learning rule that only depends on pre- and
post-synaptic neuronal activity, predictive dendritic input and widely
broadcasted modulation factors which are identical for large groups of neurons.
The learning rule applies contrastive predictive learning to a causal,
biological setting using saccades (i.e. rapid shifts in gaze direction). We
find that networks trained with this self-supervised and local rule build deep
hierarchical representations of images, speech and video.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Illing_B/0/1/0/all/0/1"&gt;Bernd Illing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventura_J/0/1/0/all/0/1"&gt;Jean Ventura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellec_G/0/1/0/all/0/1"&gt;Guillaume Bellec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1"&gt;Wulfram Gerstner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolving Image Compositions for Feature Representation Learning. (arXiv:2106.09011v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09011</id>
        <link href="http://arxiv.org/abs/2106.09011"/>
        <updated>2021-06-17T01:58:42.253Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks for visual recognition require large amounts of
training samples and usually benefit from data augmentation. This paper
proposes PatchMix, a data augmentation method that creates new samples by
composing patches from pairs of images in a grid-like pattern. These new
samples' ground truth labels are set as proportional to the number of patches
from each image. We then add a set of additional losses at the patch-level to
regularize and to encourage good representations at both the patch and image
levels. A ResNet-50 model trained on ImageNet using PatchMix exhibits superior
transfer learning capabilities across a wide array of benchmarks. Although
PatchMix can rely on random pairings and random grid-like patterns for mixing,
we explore evolutionary search as a guiding strategy to discover optimal
grid-like patterns and image pairing jointly. For this purpose, we conceive a
fitness function that bypasses the need to re-train a model to evaluate each
choice. In this way, PatchMix outperforms a base model on CIFAR-10 (+1.91),
CIFAR-100 (+5.31), Tiny Imagenet (+3.52), and ImageNet (+1.16) by significant
margins, also outperforming previous state-of-the-art pairwise augmentation
strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cascante_Bonilla_P/0/1/0/all/0/1"&gt;Paola Cascante-Bonilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1"&gt;Arshdeep Sekhon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yanjun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1"&gt;Vicente Ordonez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape from Blur: Recovering Textured 3D Shape and Motion of Fast Moving Objects. (arXiv:2106.08762v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08762</id>
        <link href="http://arxiv.org/abs/2106.08762"/>
        <updated>2021-06-17T01:58:42.247Z</updated>
        <summary type="html"><![CDATA[We address the novel task of jointly reconstructing the 3D shape, texture,
and motion of an object from a single motion-blurred image. While previous
approaches address the deblurring problem only in the 2D image domain, our
proposed rigorous modeling of all object properties in the 3D domain enables
the correct description of arbitrary object motion. This leads to significantly
better image decomposition and sharper deblurring results. We model the
observed appearance of a motion-blurred object as a combination of the
background and a 3D object with constant translation and rotation. Our method
minimizes a loss on reconstructing the input image via differentiable rendering
with suitable regularizers. This enables estimating the textured 3D mesh of the
blurred object with high fidelity. Our method substantially outperforms
competing approaches on several benchmarks for fast moving objects deblurring.
Qualitative results show that the reconstructed 3D mesh generates high-quality
temporal super-resolution and novel views of the deblurred object.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rozumnyi_D/0/1/0/all/0/1"&gt;Denys Rozumnyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1"&gt;Martin R. Oswald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1"&gt;Vittorio Ferrari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1"&gt;Marc Pollefeys&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Robotic Weed Control: Detection of Nutsedge Weed in Bermudagrass Turf Using Inaccurate and Insufficient Training Data. (arXiv:2106.08897v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08897</id>
        <link href="http://arxiv.org/abs/2106.08897"/>
        <updated>2021-06-17T01:58:42.230Z</updated>
        <summary type="html"><![CDATA[To enable robotic weed control, we develop algorithms to detect nutsedge weed
from bermudagrass turf. Due to the similarity between the weed and the
background turf, manual data labeling is expensive and error-prone.
Consequently, directly applying deep learning methods for object detection
cannot generate satisfactory results. Building on an instance detection
approach (i.e. Mask R-CNN), we combine synthetic data with raw data to train
the network. We propose an algorithm to generate high fidelity synthetic data,
adopting different levels of annotations to reduce labeling cost. Moreover, we
construct a nutsedge skeleton-based probabilistic map (NSPM) as the neural
network input to reduce the reliance on pixel-wise precise labeling. We also
modify loss function from cross entropy to Kullback-Leibler divergence which
accommodates uncertainty in the labeling process. We implement the proposed
algorithm and compare it with both Faster R-CNN and Mask R-CNN. The results
show that our design can effectively overcome the impact of imprecise and
insufficient training sample issues and significantly outperform the Faster
R-CNN counterpart with a false negative rate of only 0.4%. In particular, our
approach also reduces labeling time by 95% while achieving better performance
if comparing with the original Mask R-CNN approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Shuangyu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Chengsong Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagavathiannan_M/0/1/0/all/0/1"&gt;Muthukumar Bagavathiannan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dezhen Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence. (arXiv:2106.08710v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.08710</id>
        <link href="http://arxiv.org/abs/2106.08710"/>
        <updated>2021-06-17T01:58:42.224Z</updated>
        <summary type="html"><![CDATA[Mobile Augmented Reality (MAR) integrates computer-generated virtual objects
with physical environments for mobile devices. MAR systems enable users to
interact with MAR devices, such as smartphones and head-worn wearables, and
performs seamless transitions from the physical world to a mixed world with
digital entities. These MAR systems support user experiences by using MAR
devices to provide universal accessibility to digital contents. Over the past
20 years, a number of MAR systems have been developed, however, the studies and
design of MAR frameworks have not yet been systematically reviewed from the
perspective of user-centric design. This article presents the first effort of
surveying existing MAR frameworks (count: 37) and further discusses the latest
studies on MAR through a top-down approach: 1) MAR applications; 2) MAR
visualisation techniques adaptive to user mobility and contexts; 3) systematic
evaluation of MAR frameworks including supported platforms and corresponding
features such as tracking, feature extraction plus sensing capabilities; and 4)
underlying machine learning approaches supporting intelligent operations within
MAR systems. Finally, we summarise the development of emerging research fields,
current state-of-the-art, and discuss the important open challenges and
possible theoretical and technical directions. This survey aims to benefit both
researchers and MAR system developers alike.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jacky Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1"&gt;Kit-Yung Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1"&gt;Lik-Hang Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_P/0/1/0/all/0/1"&gt;Pan Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xiang Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimality of short-term synaptic plasticity in modelling certain dynamic environments. (arXiv:2009.06808v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06808</id>
        <link href="http://arxiv.org/abs/2009.06808"/>
        <updated>2021-06-17T01:58:42.206Z</updated>
        <summary type="html"><![CDATA[Biological neurons and their in-silico emulations for neuromorphic artificial
intelligence (AI) use extraordinarily energy-efficient mechanisms, such as
spike-based communication and local synaptic plasticity. It remains unclear
whether these neuronal mechanisms only offer efficiency or also underlie the
superiority of biological intelligence. Here, we prove rigorously that, indeed,
the Bayes-optimal prediction and inference of randomly but continuously
transforming environments, a common natural setting, relies on short-term
spike-timing-dependent plasticity, a hallmark of biological synapses. Further,
this dynamic Bayesian inference through plasticity enables circuits of the
cerebral cortex in simulations to recognize previously unseen, highly distorted
dynamic stimuli. Strikingly, this also introduces a biologically-modelled AI,
the first to overcome multiple limitations of deep learning and outperform
artificial neural networks in a visual task. The cortical-like network is
spiking and event-based, trained only with unsupervised and local plasticity,
on a small, narrow, and static training dataset, but achieves recognition of
unseen, transformed, and dynamic data better than deep neural networks with
continuous activations, trained with supervised backpropagation on the
transforming data. These results link short-term plasticity to high-level
cortical function, suggest optimality of natural intelligence for natural
environments, and repurpose neuromorphic AI from mere efficiency to
computational supremacy altogether.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1"&gt;Timoleon Moraitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1"&gt;Abu Sebastian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eleftheriou_E/0/1/0/all/0/1"&gt;Evangelos Eleftheriou&lt;/a&gt; (IBM Research - Zurich)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Oxford Road Boundaries Dataset. (arXiv:2106.08983v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08983</id>
        <link href="http://arxiv.org/abs/2106.08983"/>
        <updated>2021-06-17T01:58:42.191Z</updated>
        <summary type="html"><![CDATA[In this paper we present the Oxford Road Boundaries Dataset, designed for
training and testing machine-learning-based road-boundary detection and
inference approaches. We have hand-annotated two of the 10 km-long forays from
the Oxford Robotcar Dataset and generated from other forays several thousand
further examples with semi-annotated road-boundary masks. To boost the number
of training samples in this way, we used a vision-based localiser to project
labels from the annotated datasets to other traversals at different times and
weather conditions. As a result, we release 62605 labelled samples, of which
47639 samples are curated. Each of these samples contains both raw and
classified masks for left and right lenses. Our data contains images from a
diverse set of scenarios such as straight roads, parked cars, junctions, etc.
Files for download and tools for manipulating the labelled data are available
at: oxford-robotics-institute.github.io/road-boundaries-dataset]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suleymanov_T/0/1/0/all/0/1"&gt;Tarlan Suleymanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadd_M/0/1/0/all/0/1"&gt;Matthew Gadd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martini_D/0/1/0/all/0/1"&gt;Daniele De Martini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newman_P/0/1/0/all/0/1"&gt;Paul Newman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The shape and simplicity biases of adversarially robust ImageNet-trained CNNs. (arXiv:2006.09373v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09373</id>
        <link href="http://arxiv.org/abs/2006.09373"/>
        <updated>2021-06-17T01:58:42.186Z</updated>
        <summary type="html"><![CDATA[Adversarial training has been the topic of dozens of studies and a leading
method for defending against adversarial attacks. Yet, it remains largely
unknown (a) how adversarially-robust ImageNet classifiers (R classifiers)
generalize to out-of-distribution examples; and (b) how their generalization
capability relates to their hidden representations. In this paper, we perform a
thorough, systematic study to answer these two questions across AlexNet,
GoogLeNet, and ResNet-50 architectures. We found that while standard ImageNet
classifiers have a strong texture bias, their R counterparts rely heavily on
shapes. Remarkably, adversarial training induces three simplicity biases into
hidden neurons in the process of 'robustifying' the network. That is, each
convolutional neuron in R networks often changes to detecting (1) pixel-wise
smoother patterns i.e. a mechanism that blocks high-frequency noise from
passing through the network; (2) more lower-level features i.e. textures and
colors (instead of objects); and (3) fewer types of inputs. Our findings reveal
the interesting mechanisms that made networks more adversarially robust and
also explain some recent findings. Our findings reveal the interesting
mechanisms that made networks more adversarially robust and also explain some
recent findings e.g. why R networks benefit from much larger capacity (Xie and
Yuille, 2020) and can act as a strong image prior in image synthesis (Santurkar
et al., 2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Peijie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1"&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invertible Attention. (arXiv:2106.09003v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09003</id>
        <link href="http://arxiv.org/abs/2106.09003"/>
        <updated>2021-06-17T01:58:42.143Z</updated>
        <summary type="html"><![CDATA[Attention has been proved to be an efficient mechanism to capture long-range
dependencies. However, so far it has not been deployed in invertible networks.
This is due to the fact that in order to make a network invertible, every
component within the network needs to be a bijective transformation, but a
normal attention block is not. In this paper, we propose invertible attention
that can be plugged into existing invertible models. We mathematically and
experimentally prove that the invertibility of an attention model can be
achieved by carefully constraining its Lipschitz constant. We validate the
invertibility of our invertible attention on image reconstruction task with 3
popular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible
attention achieves similar performance in comparison with normal non-invertible
attention on dense prediction tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zha_J/0/1/0/all/0/1"&gt;Jiajun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yiran Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1"&gt;Richard Hartley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Split and Expand: An inference-time improvement for Weakly Supervised Cell Instance Segmentation. (arXiv:2007.10817v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10817</id>
        <link href="http://arxiv.org/abs/2007.10817"/>
        <updated>2021-06-17T01:58:42.082Z</updated>
        <summary type="html"><![CDATA[We consider the problem of segmenting cell nuclei instances from Hematoxylin
and Eosin (H&E) stains with dot annotations only. While most recent works focus
on improving the segmentation quality, this is usually insufficient for
instance segmentation of cell instances clustered together or with a small
size. In this work, we propose a simple two-step post-processing procedure,
Split and Expand, that directly improves the conversion of segmentation maps to
instances. In the splitting step, we generate fine-grained cell instances from
the segmentation map with the guidance of cell-center predictions. For the
expansion step, we utilize Layer-wise Relevance Propagation (LRP) explanation
results to add small cells that are not captured in the segmentation map.
Although we additionally train an output head to predict cell-centers, the
post-processing procedure itself is not explicitly trained and is executed at
inference-time only. A feature re-weighting loss based on LRP is proposed to
improve our method even further. We test our procedure on the MoNuSeg and TNBC
datasets and show quantitatively and qualitatively that our proposed method
improves object-level metrics substantially.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Foo_L/0/1/0/all/0/1"&gt;Lin Geng Foo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiamei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1"&gt;Alexander Binder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ParticleAugment: Sampling-Based Data Augmentation. (arXiv:2106.08693v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08693</id>
        <link href="http://arxiv.org/abs/2106.08693"/>
        <updated>2021-06-17T01:58:42.075Z</updated>
        <summary type="html"><![CDATA[We present an automated data augmentation approach for image classification.
We formulate the problem as Monte Carlo sampling where our goal is to
approximate the optimal augmentation policies. We propose a particle filtering
formulation to find optimal augmentation policies and their schedules during
model training. Our performance measurement procedure relies on a validation
subset of our training set, while the policy transition model depends on a
Gaussian prior and an optional augmentation velocity parameter. In our
experiments, we show that our formulation for automated augmentation reaches
promising results on CIFAR-10, CIFAR-100, and ImageNet datasets using the
standard network architectures for this problem. By comparing with the related
work, we also show that our method reaches a balance between the computational
cost of policy search and the model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsaregorodtsev_A/0/1/0/all/0/1"&gt;Alexander Tsaregorodtsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1"&gt;Vasileios Belagiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polynomial Trajectory Predictions for Improved Learning Performance. (arXiv:2101.12616v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12616</id>
        <link href="http://arxiv.org/abs/2101.12616"/>
        <updated>2021-06-17T01:58:42.068Z</updated>
        <summary type="html"><![CDATA[The rising demand for Active Safety systems in automotive applications
stresses the need for a reliable short to mid-term trajectory prediction.
Anticipating the unfolding path of road users, one can act to increase the
overall safety. In this work, we propose to train artificial neural networks
for movement understanding by predicting trajectories in their natural form, as
a function of time. Predicting polynomial coefficients allows us to increased
accuracy and improve generalisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_I/0/1/0/all/0/1"&gt;Ido Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kummert_A/0/1/0/all/0/1"&gt;Anton Kummert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization. (arXiv:2102.01670v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01670</id>
        <link href="http://arxiv.org/abs/2102.01670"/>
        <updated>2021-06-17T01:58:42.062Z</updated>
        <summary type="html"><![CDATA[Training sparse networks to converge to the same performance as dense neural
architectures has proven to be elusive. Recent work suggests that
initialization is the key. However, while this direction of research has had
some success, focusing on initialization alone appears to be inadequate. In
this paper, we take a broader view of training sparse networks and consider the
role of regularization, optimization, and architecture choices on sparse
models. We propose a simple experimental framework, Same Capacity Sparse vs
Dense Comparison (SC-SDC), that allows for a fair comparison of sparse and
dense networks. Furthermore, we propose a new measure of gradient flow,
Effective Gradient Flow (EGF), that better correlates to performance in sparse
networks. Using top-line metrics, SC-SDC and EGF, we show that default choices
of optimizers, activation functions and regularizers used for dense networks
can disadvantage sparse networks. Based upon these findings, we show that
gradient flow in sparse networks can be improved by reconsidering aspects of
the architecture design and the training regime. Our work suggests that
initialization is only one piece of the puzzle and taking a wider view of
tailoring optimization to sparse networks yields promising results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1"&gt;Kale-ab Tessera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1"&gt;Sara Hooker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosman_B/0/1/0/all/0/1"&gt;Benjamin Rosman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Semantic-to-visual Confusion for Zero-shot Learning. (arXiv:2106.08605v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08605</id>
        <link href="http://arxiv.org/abs/2106.08605"/>
        <updated>2021-06-17T01:58:42.047Z</updated>
        <summary type="html"><![CDATA[Using generative models to synthesize visual features from semantic
distribution is one of the most popular solutions to ZSL image classification
in recent years. The triplet loss (TL) is popularly used to generate realistic
visual distributions from semantics by automatically searching discriminative
representations. However, the traditional TL cannot search reliable unseen
disentangled representations due to the unavailability of unseen classes in
ZSL. To alleviate this drawback, we propose in this work a multi-modal triplet
loss (MMTL) which utilizes multimodal information to search a disentangled
representation space. As such, all classes can interplay which can benefit
learning disentangled class representations in the searched space. Furthermore,
we develop a novel model called Disentangling Class Representation Generative
Adversarial Network (DCR-GAN) focusing on exploiting the disentangled
representations in training, feature synthesis, and final recognition stages.
Benefiting from the disentangled representations, DCR-GAN could fit a more
realistic distribution over both seen and unseen features. Extensive
experiments show that our proposed model can lead to superior performance to
the state-of-the-arts on four benchmark datasets. Our code is available at
https://github.com/FouriYe/DCRGAN-TMM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1"&gt;Zihan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1"&gt;Fuyuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_F/0/1/0/all/0/1"&gt;Fan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the proper role of linguistically-oriented deep net analysis in linguistic theorizing. (arXiv:2106.08694v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08694</id>
        <link href="http://arxiv.org/abs/2106.08694"/>
        <updated>2021-06-17T01:58:42.035Z</updated>
        <summary type="html"><![CDATA[A lively research field has recently emerged that uses experimental methods
to probe the linguistic behavior of modern deep networks. While work in this
tradition often reports intriguing results about the grammatical skills of deep
nets, it is not clear what their implications for linguistic theorizing should
be. As a consequence, linguistically-oriented deep net analysis has had very
little impact on linguistics at large. In this chapter, I suggest that deep
networks should be treated as theories making explicit predictions about the
acceptability of linguistic utterances. I argue that, if we overcome some
obstacles standing in the way of seriously pursuing this idea, we will gain a
powerful new theoretical tool, complementary to mainstream algebraic
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1"&gt;Marco Baroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Silent Speech and Emotion Recognition from Vocal Tract Shape Dynamics in Real-Time MRI. (arXiv:2106.08706v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08706</id>
        <link href="http://arxiv.org/abs/2106.08706"/>
        <updated>2021-06-17T01:58:42.010Z</updated>
        <summary type="html"><![CDATA[Speech sounds of spoken language are obtained by varying configuration of the
articulators surrounding the vocal tract. They contain abundant information
that can be utilized to better understand the underlying mechanism of human
speech production. We propose a novel deep neural network-based learning
framework that understands acoustic information in the variable-length sequence
of vocal tract shaping during speech production, captured by real-time magnetic
resonance imaging (rtMRI), and translate it into text. The proposed framework
comprises of spatiotemporal convolutions, a recurrent network, and the
connectionist temporal classification loss, trained entirely end-to-end. On the
USC-TIMIT corpus, the model achieved a 40.6% PER at sentence-level, much better
compared to the existing models. To the best of our knowledge, this is the
first study that demonstrates the recognition of entire spoken sentence based
on an individual's articulatory motions captured by rtMRI video. We also
performed an analysis of variations in the geometry of articulation in each
sub-regions of the vocal tract (i.e., pharyngeal, velar and dorsal, hard
palate, labial constriction region) with respect to different emotions and
genders. Results suggest that each sub-regions distortion is affected by both
emotion and gender.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pandey_L/0/1/0/all/0/1"&gt;Laxmi Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arif_A/0/1/0/all/0/1"&gt;Ahmed Sabbir Arif&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shuffle Transformer with Feature Alignment for Video Face Parsing. (arXiv:2106.08650v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08650</id>
        <link href="http://arxiv.org/abs/2106.08650"/>
        <updated>2021-06-17T01:58:41.996Z</updated>
        <summary type="html"><![CDATA[This is a short technical report introducing the solution of the Team
TCParser for Short-video Face Parsing Track of The 3rd Person in Context (PIC)
Workshop and Challenge at CVPR 2021. In this paper, we introduce a strong
backbone which is cross-window based Shuffle Transformer for presenting
accurate face parsing representation. To further obtain the finer segmentation
results, especially on the edges, we introduce a Feature Alignment Aggregation
(FAA) module. It can effectively relieve the feature misalignment issue caused
by multi-resolution feature aggregation. Benefiting from the stronger backbone
and better feature aggregation, the proposed method achieves 86.9519% score in
the Short-video Face Parsing track of the 3rd Person in Context (PIC) Workshop
and Challenge, ranked the first place.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zilong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Pei Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1"&gt;Guozhong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1"&gt;Gang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1"&gt;Bin Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CMF: Cascaded Multi-model Fusion for Referring Image Segmentation. (arXiv:2106.08617v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08617</id>
        <link href="http://arxiv.org/abs/2106.08617"/>
        <updated>2021-06-17T01:58:41.990Z</updated>
        <summary type="html"><![CDATA[In this work, we address the task of referring image segmentation (RIS),
which aims at predicting a segmentation mask for the object described by a
natural language expression. Most existing methods focus on establishing
unidirectional or directional relationships between visual and linguistic
features to associate two modalities together, while the multi-scale context is
ignored or insufficiently modeled. Multi-scale context is crucial to localize
and segment those objects that have large scale variations during the
multi-modal fusion process. To solve this problem, we propose a simple yet
effective Cascaded Multi-modal Fusion (CMF) module, which stacks multiple
atrous convolutional layers in parallel and further introduces a cascaded
branch to fuse visual and linguistic features. The cascaded branch can
progressively integrate multi-scale contextual information and facilitate the
alignment of two modalities during the multi-modal fusion process. Experimental
results on four benchmark datasets demonstrate that our method outperforms most
state-of-the-art methods. Code is available at
https://github.com/jianhua2022/CMF-Refseg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianhua Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhanyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[2nd Place Solution for Waymo Open Dataset Challenge - Real-time 2D Object Detection. (arXiv:2106.08713v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08713</id>
        <link href="http://arxiv.org/abs/2106.08713"/>
        <updated>2021-06-17T01:58:41.983Z</updated>
        <summary type="html"><![CDATA[In an autonomous driving system, it is essential to recognize vehicles,
pedestrians and cyclists from images. Besides the high accuracy of the
prediction, the requirement of real-time running brings new challenges for
convolutional network models. In this report, we introduce a real-time method
to detect the 2D objects from images. We aggregate several popular one-stage
object detectors and train the models of variety input strategies
independently, to yield better performance for accurate multi-scale detection
of each category, especially for small objects. For model acceleration, we
leverage TensorRT to optimize the inference time of our detection pipeline. As
shown in the leaderboard, our proposed detection framework ranks the 2nd place
with 75.00% L1 mAP and 69.72% L2 mAP in the real-time 2D detection track of the
Waymo Open Dataset Challenges, while our framework achieves the latency of
45.8ms/frame on an Nvidia Tesla V100 GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yueming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xiaolin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_B/0/1/0/all/0/1"&gt;Bing Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_T/0/1/0/all/0/1"&gt;Tengfei Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhihui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yawei Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1"&gt;Haojin Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guoshan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Pengfei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Semi-supervised Medical Image Classification via Inter-client Relation Matching. (arXiv:2106.08600v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08600</id>
        <link href="http://arxiv.org/abs/2106.08600"/>
        <updated>2021-06-17T01:58:41.968Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has emerged with increasing popularity to collaborate
distributed medical institutions for training deep networks. However, despite
existing FL algorithms only allow the supervised training setting, most
hospitals in realistic usually cannot afford the intricate data labeling due to
absence of budget or expertise. This paper studies a practical yet challenging
FL problem, named \textit{Federated Semi-supervised Learning} (FSSL), which
aims to learn a federated model by jointly utilizing the data from both labeled
and unlabeled clients (i.e., hospitals). We present a novel approach for this
problem, which improves over traditional consistency regularization mechanism
with a new inter-client relation matching scheme. The proposed learning scheme
explicitly connects the learning across labeled and unlabeled clients by
aligning their extracted disease relationships, thereby mitigating the
deficiency of task knowledge at unlabeled clients and promoting discriminative
information from unlabeled samples. We validate our method on two large-scale
medical image classification datasets. The effectiveness of our method has been
demonstrated with the clear improvements over state-of-the-arts as well as the
thorough ablation analysis on both tasks\footnote{Code will be made available
at \url{https://github.com/liuquande/FedIRM}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quande Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongzheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1"&gt;Qi Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng-Ann Heng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection in Video Sequences: A Benchmark and Computational Model. (arXiv:2106.08570v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08570</id>
        <link href="http://arxiv.org/abs/2106.08570"/>
        <updated>2021-06-17T01:58:41.938Z</updated>
        <summary type="html"><![CDATA[Anomaly detection has attracted considerable search attention. However,
existing anomaly detection databases encounter two major problems. Firstly,
they are limited in scale. Secondly, training sets contain only video-level
labels indicating the existence of an abnormal event during the full video
while lacking annotations of precise time durations. To tackle these problems,
we contribute a new Large-scale Anomaly Detection (LAD) database as the
benchmark for anomaly detection in video sequences, which is featured in two
aspects. 1) It contains 2000 video sequences including normal and abnormal
video clips with 14 anomaly categories including crash, fire, violence, etc.
with large scene varieties, making it the largest anomaly analysis database to
date. 2) It provides the annotation data, including video-level labels
(abnormal/normal video, anomaly type) and frame-level labels (abnormal/normal
video frame) to facilitate anomaly detection. Leveraging the above benefits
from the LAD database, we further formulate anomaly detection as a
fully-supervised learning problem and propose a multi-task deep neural network
to solve it. We first obtain the local spatiotemporal contextual feature by
using an Inflated 3D convolutional (I3D) network. Then we construct a recurrent
convolutional neural network fed the local spatiotemporal contextual feature to
extract the spatiotemporal contextual feature. With the global spatiotemporal
contextual feature, the anomaly type and score can be computed simultaneously
by a multi-task neural network. Experimental results show that the proposed
method outperforms the state-of-the-art anomaly detection methods on our
database and other public databases of anomaly detection. Codes are available
at https://github.com/wanboyang/anomaly_detection_LAD2000.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_B/0/1/0/all/0/1"&gt;Boyang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wenhui Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuming Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhiyuan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1"&gt;Guanqun Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compound Frechet Inception Distance for Quality Assessment of GAN Created Images. (arXiv:2106.08575v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08575</id>
        <link href="http://arxiv.org/abs/2106.08575"/>
        <updated>2021-06-17T01:58:41.929Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks or GANs are a type of generative modeling
framework. GANs involve a pair of neural networks engaged in a competition in
iteratively creating fake data, indistinguishable from the real data. One
notable application of GANs is developing fake human faces, also known as "deep
fakes," due to the deep learning algorithms at the core of the GAN framework.
Measuring the quality of the generated images is inherently subjective but
attempts to objectify quality using standardized metrics have been made. One
example of objective metrics is the Frechet Inception Distance (FID), which
measures the difference between distributions of feature vectors for two
separate datasets of images. There are situations that images with low
perceptual qualities are not assigned appropriate FID scores. We propose to
improve the robustness of the evaluation process by integrating lower-level
features to cover a wider array of visual defects. Our proposed method
integrates three levels of feature abstractions to evaluate the quality of
generated images. Experimental evaluations show better performance of the
proposed method for distorted images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nunn_E/0/1/0/all/0/1"&gt;Eric J. Nunn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khadivi_P/0/1/0/all/0/1"&gt;Pejman Khadivi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samavi_S/0/1/0/all/0/1"&gt;Shadrokh Samavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PatchNet: Unsupervised Object Discovery based on Patch Embedding. (arXiv:2106.08599v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08599</id>
        <link href="http://arxiv.org/abs/2106.08599"/>
        <updated>2021-06-17T01:58:41.922Z</updated>
        <summary type="html"><![CDATA[We demonstrate that frequently appearing objects can be discovered by
training randomly sampled patches from a small number of images (100 to 200) by
self-supervision. Key to this approach is the pattern space, a latent space of
patterns that represents all possible sub-images of the given image data. The
distance structure in the pattern space captures the co-occurrence of patterns
due to the frequent objects. The pattern space embedding is learned by
minimizing the contrastive loss between randomly generated adjacent patches. To
prevent the embedding from learning the background, we modulate the contrastive
loss by color-based object saliency and background dissimilarity. The learned
distance structure serves as object memory, and the frequent objects are simply
discovered by clustering the pattern vectors from the random patches sampled
for inference. Our image representation based on image patches naturally
handles the position and scale invariance property that is crucial to
multi-object discovery. The method has been proven surprisingly effective, and
successfully applied to finding multiple human faces and bodies from natural
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1"&gt;Hankyu Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1"&gt;Heng Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Didari_S/0/1/0/all/0/1"&gt;Sima Didari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jae Oh Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bangert_P/0/1/0/all/0/1"&gt;Patrick Bangert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastAno: Fast Anomaly Detection via Spatio-temporal Patch Transformation. (arXiv:2106.08613v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08613</id>
        <link href="http://arxiv.org/abs/2106.08613"/>
        <updated>2021-06-17T01:58:41.911Z</updated>
        <summary type="html"><![CDATA[Video anomaly detection has gained significant attention due to the
increasing requirements of automatic monitoring for surveillance videos.
Especially, the prediction based approach is one of the most studied methods to
detect anomalies by predicting frames that include abnormal events in the test
set after learning with the normal frames of the training set. However, a lot
of prediction networks are computationally expensive owing to the use of
pre-trained optical flow networks, or fail to detect abnormal situations
because of their strong generative ability to predict even the anomalies. To
address these shortcomings, we propose spatial rotation transformation (SRT)
and temporal mixing transformation (TMT) to generate irregular patch cuboids
within normal frame cuboids in order to enhance the learning of normal
features. Additionally, the proposed patch transformation is used only during
the training phase, allowing our model to detect abnormal frames at fast speed
during inference. Our model is evaluated on three anomaly detection benchmarks,
achieving competitive accuracy and surpassing all the previous works in terms
of speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chaewon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1"&gt;MyeongAh Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Minhyeok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangyoun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Consistency Regularization for Unsupervised Multi-source Domain Adaptive Classification. (arXiv:2106.08590v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08590</id>
        <link href="http://arxiv.org/abs/2106.08590"/>
        <updated>2021-06-17T01:58:41.895Z</updated>
        <summary type="html"><![CDATA[Deep learning-based multi-source unsupervised domain adaptation (MUDA) has
been actively studied in recent years. Compared with single-source unsupervised
domain adaptation (SUDA), domain shift in MUDA exists not only between the
source and target domains but also among multiple source domains. Most existing
MUDA algorithms focus on extracting domain-invariant representations among all
domains whereas the task-specific decision boundaries among classes are largely
neglected. In this paper, we propose an end-to-end trainable network that
exploits domain Consistency Regularization for unsupervised Multi-source domain
Adaptive classification (CRMA). CRMA aligns not only the distributions of each
pair of source and target domains but also that of all domains. For each pair
of source and target domains, we employ an intra-domain consistency to
regularize a pair of domain-specific classifiers to achieve intra-domain
alignment. In addition, we design an inter-domain consistency that targets
joint inter-domain alignment among all domains. To address different
similarities between multiple source domains and the target domain, we design
an authorization strategy that assigns different authorities to domain-specific
classifiers adaptively for optimal pseudo label prediction and self-training.
Extensive experiments show that CRMA tackles unsupervised domain adaptation
effectively under a multi-source setup and achieves superior adaptation
consistently across multiple MUDA datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhipeng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaobing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1"&gt;Shuai Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisit Visual Representation in Analytics Taxonomy: A Compression Perspective. (arXiv:2106.08512v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08512</id>
        <link href="http://arxiv.org/abs/2106.08512"/>
        <updated>2021-06-17T01:58:41.871Z</updated>
        <summary type="html"><![CDATA[Visual analytics have played an increasingly critical role in the Internet of
Things, where massive visual signals have to be compressed and fed into
machines. But facing such big data and constrained bandwidth capacity, existing
image/video compression methods lead to very low-quality representations, while
existing feature compression techniques fail to support diversified visual
analytics applications/tasks with low-bit-rate representations. In this paper,
we raise and study the novel problem of supporting multiple machine vision
analytics tasks with the compressed visual representation, namely, the
information compression problem in analytics taxonomy. By utilizing the
intrinsic transferability among different tasks, our framework successfully
constructs compact and expressive representations at low bit-rates to support a
diversified set of machine vision tasks, including both high-level
semantic-related tasks and mid-level geometry analytic tasks. In order to
impose compactness in the representations, we propose a codebook-based
hyperprior, which helps map the representation into a low-dimensional manifold.
As it well fits the signal structure of the deep visual feature, it facilitates
more accurate entropy estimation, and results in higher compression efficiency.
With the proposed framework and the codebook-based hyperprior, we further
investigate the relationship of different task features owning different levels
of abstraction granularity. Experimental results demonstrate that with the
proposed scheme, a set of diversified tasks can be supported at a significantly
lower bit-rate, compared with existing compression schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yueyu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenhan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haofeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised-learning-based method for chest MRI-CT transformation using structure constrained unsupervised generative attention networks. (arXiv:2106.08557v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08557</id>
        <link href="http://arxiv.org/abs/2106.08557"/>
        <updated>2021-06-17T01:58:41.862Z</updated>
        <summary type="html"><![CDATA[The integrated positron emission tomography/magnetic resonance imaging
(PET/MRI) scanner facilitates the simultaneous acquisition of metabolic
information via PET and morphological information with high soft-tissue
contrast using MRI. Although PET/MRI facilitates the capture of high-accuracy
fusion images, its major drawback can be attributed to the difficulty
encountered when performing attenuation correction, which is necessary for
quantitative PET evaluation. The combined PET/MRI scanning requires the
generation of attenuation-correction maps from MRI owing to no direct
relationship between the gamma-ray attenuation information and MRIs. While
MRI-based bone-tissue segmentation can be readily performed for the head and
pelvis regions, the realization of accurate bone segmentation via chest CT
generation remains a challenging task. This can be attributed to the
respiratory and cardiac motions occurring in the chest as well as its
anatomically complicated structure and relatively thin bone cortex. This paper
presents a means to minimise the anatomical structural changes without human
annotation by adding structural constraints using a modality-independent
neighbourhood descriptor (MIND) to a generative adversarial network (GAN) that
can transform unpaired images. The results obtained in this study revealed the
proposed U-GAT-IT + MIND approach to outperform all other competing approaches.
The findings of this study hint towards possibility of synthesising clinically
acceptable CT images from chest MRI without human annotation, thereby
minimising the changes in the anatomical structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matsuo_H/0/1/0/all/0/1"&gt;Hidetoshi Matsuo&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Nishio_M/0/1/0/all/0/1"&gt;Mizuho Nishio&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Nogami_M/0/1/0/all/0/1"&gt;Munenobu Nogami&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1"&gt;Feibi Zeng&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Kurimoto_T/0/1/0/all/0/1"&gt;Takako Kurimoto&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1"&gt;Sandeep Kaushik&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Wiesinger_F/0/1/0/all/0/1"&gt;Florian Wiesinger&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Kono_A/0/1/0/all/0/1"&gt;Atsushi K Kono&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1"&gt;Takamichi Murakami&lt;/a&gt; (1) ((1) Department of Radiology, Kobe University Graduate School of Medicine, Kobe, Japan, (2) GE Healthcare, Hino, Japan and (3) GE Healthcare, Munich, Germany)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Watching Too Much Television is Good: Self-Supervised Audio-Visual Representation Learning from Movies and TV Shows. (arXiv:2106.08513v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08513</id>
        <link href="http://arxiv.org/abs/2106.08513"/>
        <updated>2021-06-17T01:58:41.855Z</updated>
        <summary type="html"><![CDATA[The abundance and ease of utilizing sound, along with the fact that auditory
clues reveal so much about what happens in the scene, make the audio-visual
space a perfectly intuitive choice for self-supervised representation learning.
However, the current literature suggests that training on \textit{uncurated}
data yields considerably poorer representations compared to the
\textit{curated} alternatives collected in supervised manner, and the gap only
narrows when the volume of data significantly increases. Furthermore, the
quality of learned representations is known to be heavily influenced by the
size and taxonomy of the curated datasets used for self-supervised training.
This begs the question of whether we are celebrating too early on catching up
with supervised learning when our self-supervised efforts still rely almost
exclusively on curated data. In this paper, we study the efficacy of learning
from Movies and TV Shows as forms of uncurated data for audio-visual
self-supervised learning. We demonstrate that a simple model based on
contrastive learning, trained on a collection of movies and TV shows, not only
dramatically outperforms more complex methods which are trained on orders of
magnitude larger uncurated datasets, but also performs very competitively with
the state-of-the-art that learns from large-scale curated data. We identify
that audiovisual patterns like the appearance of the main character or
prominent scenes and mise-en-sc\`ene which frequently occur through the whole
duration of a movie, lead to an overabundance of easy negative instances in the
contrastive learning formulation. Capitalizing on such observation, we propose
a hierarchical sampling policy, which despite its simplicity, effectively
improves the performance, particularly when learning from TV shows which
naturally face less semantic diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalayeh_M/0/1/0/all/0/1"&gt;Mahdi M. Kalayeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamath_N/0/1/0/all/0/1"&gt;Nagendra Kamath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrashekar_A/0/1/0/all/0/1"&gt;Ashok Chandrashekar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning-based analysis of hyperspectral images for automated sepsis diagnosis. (arXiv:2106.08445v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08445</id>
        <link href="http://arxiv.org/abs/2106.08445"/>
        <updated>2021-06-17T01:58:41.843Z</updated>
        <summary type="html"><![CDATA[Sepsis is a leading cause of mortality and critical illness worldwide. While
robust biomarkers for early diagnosis are still missing, recent work indicates
that hyperspectral imaging (HSI) has the potential to overcome this bottleneck
by monitoring microcirculatory alterations. Automated machine learning-based
diagnosis of sepsis based on HSI data, however, has not been explored to date.
Given this gap in the literature, we leveraged an existing data set to (1)
investigate whether HSI-based automated diagnosis of sepsis is possible and (2)
put forth a list of possible confounders relevant for HSI-based tissue
classification. While we were able to classify sepsis with an accuracy of over
$98\,\%$ using the existing data, our research also revealed several subject-,
therapy- and imaging-related confounders that may lead to an overestimation of
algorithm performance when not balanced across the patient groups. We conclude
that further prospective studies, carefully designed with respect to these
confounders, are necessary to confirm the preliminary results obtained in this
study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dietrich_M/0/1/0/all/0/1"&gt;Maximilian Dietrich&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Seidlitz_S/0/1/0/all/0/1"&gt;Silvia Seidlitz&lt;/a&gt; (2, 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Schreck_N/0/1/0/all/0/1"&gt;Nicholas Schreck&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Wiesenfarth_M/0/1/0/all/0/1"&gt;Manuel Wiesenfarth&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Godau_P/0/1/0/all/0/1"&gt;Patrick Godau&lt;/a&gt; (2, 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1"&gt;Minu Tizabi&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Sellner_J/0/1/0/all/0/1"&gt;Jan Sellner&lt;/a&gt; (2, 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Marx_S/0/1/0/all/0/1"&gt;Sebastian Marx&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Knodler_S/0/1/0/all/0/1"&gt;Samuel Kn&amp;#xf6;dler&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Allers_M/0/1/0/all/0/1"&gt;Michael M. Allers&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Ayala_L/0/1/0/all/0/1"&gt;Leonardo Ayala&lt;/a&gt; (2, 7), &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_K/0/1/0/all/0/1"&gt;Karsten Schmidt&lt;/a&gt; (8), &lt;a href="http://arxiv.org/find/cs/1/au:+Brenner_T/0/1/0/all/0/1"&gt;Thorsten Brenner&lt;/a&gt; (8), &lt;a href="http://arxiv.org/find/cs/1/au:+Studier_Fischer_A/0/1/0/all/0/1"&gt;Alexander Studier-Fischer&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Nickel_F/0/1/0/all/0/1"&gt;Felix Nickel&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1"&gt;Beat P. M&amp;#xfc;ller-Stich&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Kopp_Schneider_A/0/1/0/all/0/1"&gt;Annette Kopp-Schneider&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Weigand_M/0/1/0/all/0/1"&gt;Markus A. Weigand&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt; (2, 6, 7) ((1) Department of Anesthesiology, Heidelberg University Hospital, Heidelberg, Germany, (2) Division of Computer Assisted Medical Interventions, German Cancer Research Center (DKFZ), Heidelberg, Germany, (3) HIDSS4Health - Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany (4) Division of Biostatistics, German Cancer Research Center (DKFZ), Heidelberg, Germany, (5) Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany, (6) Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany, (7) Medical Faculty, Heidelberg University, Heidelberg, Germany, (8) Department of Anesthesiology and Intensive Care Medicine, University Hospital Essen, University Duisburg-Essen, Essen, Germany)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Achieving Domain Robustness in Stereo Matching Networks by Removing Shortcut Learning. (arXiv:2106.08486v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08486</id>
        <link href="http://arxiv.org/abs/2106.08486"/>
        <updated>2021-06-17T01:58:41.823Z</updated>
        <summary type="html"><![CDATA[Learning-based stereo matching and depth estimation networks currently excel
on public benchmarks with impressive results. However, state-of-the-art
networks often fail to generalize from synthetic imagery to more challenging
real data domains. This paper is an attempt to uncover hidden secrets of
achieving domain robustness and in particular, discovering the important
ingredients of generalization success of stereo matching networks by analyzing
the effect of synthetic image learning on real data performance. We provide
evidence that demonstrates that learning of features in the synthetic domain by
a stereo matching network is heavily influenced by two "shortcuts" presented in
the synthetic data: (1) identical local statistics (RGB colour features)
between matching pixels in the synthetic stereo images and (2) lack of realism
in synthetic textures on 3D objects simulated in game engines. We will show
that by removing such shortcuts, we can achieve domain robustness in the
state-of-the-art stereo matching frameworks and produce a remarkable
performance on multiple realistic datasets, despite the fact that the networks
were trained on synthetic data, only. Our experimental results point to the
fact that eliminating shortcuts from the synthetic data is key to achieve
domain-invariant generalization between synthetic and real data domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chuah_W/0/1/0/all/0/1"&gt;WeiQin Chuah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tennakoon_R/0/1/0/all/0/1"&gt;Ruwan Tennakoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bab_Hadiashar_A/0/1/0/all/0/1"&gt;Alireza Bab-Hadiashar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1"&gt;David Suter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Implicit Glyph Shape Representation. (arXiv:2106.08573v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08573</id>
        <link href="http://arxiv.org/abs/2106.08573"/>
        <updated>2021-06-17T01:58:41.817Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel implicit glyph shape representation, which
models glyphs as shape primitives enclosed by quadratic curves, and naturally
enables generating glyph images at arbitrary high resolutions. Experiments on
font reconstruction and interpolation tasks verified that this structured
implicit representation is suitable for describing both structure and style
features of glyphs. Furthermore, based on the proposed representation, we
design a simple yet effective disentangled network for the challenging one-shot
font style transfer problem, and achieve the best results comparing to
state-of-the-art alternatives in both quantitative and qualitative comparisons.
Benefit from this representation, our generated glyphs have the potential to be
converted to vector fonts through post-processing, reducing the gap between
rasterized images and vector graphics. We hope this work can provide a powerful
tool for 2D shape analysis and synthesis, and inspire further exploitation in
implicit representations for 2D shape modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Ying-Tian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuan-Chen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yi-Xiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Song-Hai Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Convolution Networks with Positional Encoding for Evoked Expression Estimation. (arXiv:2106.08596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08596</id>
        <link href="http://arxiv.org/abs/2106.08596"/>
        <updated>2021-06-17T01:58:41.801Z</updated>
        <summary type="html"><![CDATA[This paper presents an approach for Evoked Expressions from Videos (EEV)
challenge, which aims to predict evoked facial expressions from video. We take
advantage of pre-trained models on large-scale datasets in computer vision and
audio signals to extract the deep representation of timestamps in the video. A
temporal convolution network, rather than an RNN like architecture, is used to
explore temporal relationships due to its advantage in memory consumption and
parallelism. Furthermore, to address the missing annotations of some
timestamps, positional encoding is employed to ensure continuity of input data
when discarding these timestamps during training. We achieved state-of-the-art
results on the EEV challenge with a Pearson correlation coefficient of 0.05477,
the first ranked performance in the EEV 2021 challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_V/0/1/0/all/0/1"&gt;VanThong Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Guee-Sang Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hyung-Jeong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Soo-Huyng Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Resolution Continuous Normalizing Flows. (arXiv:2106.08462v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08462</id>
        <link href="http://arxiv.org/abs/2106.08462"/>
        <updated>2021-06-17T01:58:41.795Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that Neural Ordinary Differential Equations (ODEs) can
serve as generative models of images using the perspective of Continuous
Normalizing Flows (CNFs). Such models offer exact likelihood calculation, and
invertible generation/density estimation. In this work we introduce a
Multi-Resolution variant of such models (MRCNF), by characterizing the
conditional distribution over the additional information required to generate a
fine image that is consistent with the coarse image. We introduce a
transformation between resolutions that allows for no change in the log
likelihood. We show that this approach yields comparable likelihood values for
various image datasets, with improved performance at higher resolutions, with
fewer parameters, using only 1 GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1"&gt;Vikram Voleti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finlay_C/0/1/0/all/0/1"&gt;Chris Finlay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1"&gt;Adam Oberman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling the Challenges in Scene Graph Generation with Local-to-Global Interactions. (arXiv:2106.08543v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08543</id>
        <link href="http://arxiv.org/abs/2106.08543"/>
        <updated>2021-06-17T01:58:41.788Z</updated>
        <summary type="html"><![CDATA[In this work, we seek new insights into the underlying challenges of the
Scene Graph Generation (SGG) task. Quantitative and qualitative analysis of the
Visual Genome dataset implies -- 1) Ambiguity: even if inter-object
relationship contains the same object (or predicate), they may not be visually
or semantically similar, 2) Asymmetry: despite the nature of the relationship
that embodied the direction, it was not well addressed in previous studies, and
3) Higher-order contexts: leveraging the identities of certain graph elements
can help to generate accurate scene graphs. Motivated by the analysis, we
design a novel SGG framework, Local-to-Global Interaction Networks (LOGIN).
Locally, interactions extract the essence between three instances - subject,
object, and background - while baking direction awareness into the network by
constraining the input order. Globally, interactions encode the contexts
between every graph components -- nodes and edges. Also we introduce Attract &
Repel loss which finely adjusts predicate embeddings. Our framework enables
predicting the scene graph in a local-to-global manner by design, leveraging
the possible complementariness. To quantify how much LOGIN is aware of
relational direction, we propose a new diagnostic task called Bidirectional
Relationship Classification (BRC). We see that LOGIN can successfully
distinguish relational direction than existing methods (in BRC task) while
showing state-of-the-art results on the Visual Genome benchmark (in SGG task).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1"&gt;Sangmin Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1"&gt;Junhyug Noh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kangil Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICDAR 2021 Competition on Components Segmentation Task of Document Photos. (arXiv:2106.08499v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08499</id>
        <link href="http://arxiv.org/abs/2106.08499"/>
        <updated>2021-06-17T01:58:41.771Z</updated>
        <summary type="html"><![CDATA[This paper describes the short-term competition on Components Segmentation
Task of Document Photos that was prepared in the context of the 16th
International Conference on Document Analysis and Recognition (ICDAR 2021).
This competition aims to bring together researchers working on the filed of
identification document image processing and provides them a suitable benchmark
to compare their techniques on the component segmentation task of document
images. Three challenge tasks were proposed entailing different segmentation
assignments to be performed on a provided dataset. The collected data are from
several types of Brazilian ID documents, whose personal information was
conveniently replaced. There were 16 participants whose results obtained for
some or all the three tasks show different rates for the adopted metrics, like
Dice Similarity Coefficient ranging from 0.06 to 0.99. Different Deep Learning
models were applied by the entrants with diverse strategies to achieve the best
results in each of the tasks. Obtained results show that the current applied
methods for solving one of the proposed tasks (document boundary detection) are
already well stablished. However, for the other two challenge tasks (text zone
and handwritten sign detection) research and development of more robust
approaches are still required to achieve acceptable results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Junior_C/0/1/0/all/0/1"&gt;Celso A. M. Lopes Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junior_R/0/1/0/all/0/1"&gt;Ricardo B. das Neves Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bezerra_B/0/1/0/all/0/1"&gt;Byron L. D. Bezerra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toselli_A/0/1/0/all/0/1"&gt;Alejandro H. Toselli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Impedovo_D/0/1/0/all/0/1"&gt;Donato Impedovo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECKPN: Explicit Class Knowledge Propagation Network for Transductive Few-shot Learning. (arXiv:2106.08523v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08523</id>
        <link href="http://arxiv.org/abs/2106.08523"/>
        <updated>2021-06-17T01:58:41.765Z</updated>
        <summary type="html"><![CDATA[Recently, the transductive graph-based methods have achieved great success in
the few-shot classification task. However, most existing methods ignore
exploring the class-level knowledge that can be easily learned by humans from
just a handful of samples. In this paper, we propose an Explicit Class
Knowledge Propagation Network (ECKPN), which is composed of the comparison,
squeeze and calibration modules, to address this problem. Specifically, we
first employ the comparison module to explore the pairwise sample relations to
learn rich sample representations in the instance-level graph. Then, we squeeze
the instance-level graph to generate the class-level graph, which can help
obtain the class-level visual knowledge and facilitate modeling the relations
of different classes. Next, the calibration module is adopted to characterize
the relations of the classes explicitly to obtain the more discriminative
class-level knowledge representations. Finally, we combine the class-level
knowledge with the instance-level sample representations to guide the inference
of the query samples. We conduct extensive experiments on four few-shot
classification benchmarks, and the experimental results show that the proposed
ECKPN significantly outperforms the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaofan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoshan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Changsheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuhui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhe Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scene Transformer: A unified multi-task model for behavior prediction and planning. (arXiv:2106.08417v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08417</id>
        <link href="http://arxiv.org/abs/2106.08417"/>
        <updated>2021-06-17T01:58:41.759Z</updated>
        <summary type="html"><![CDATA[Predicting the future motion of multiple agents is necessary for planning in
dynamic environments. This task is challenging for autonomous driving since
agents (e.g., vehicles and pedestrians) and their associated behaviors may be
diverse and influence each other. Most prior work has focused on first
predicting independent futures for each agent based on all past motion, and
then planning against these independent predictions. However, planning against
fixed predictions can suffer from the inability to represent the future
interaction possibilities between different agents, leading to sub-optimal
planning. In this work, we formulate a model for predicting the behavior of all
agents jointly in real-world driving environments in a unified manner. Inspired
by recent language modeling approaches, we use a masking strategy as the query
to our model, enabling one to invoke a single model to predict agent behavior
in many ways, such as potentially conditioned on the goal or full future
trajectory of the autonomous vehicle or the behavior of other agents in the
environment. Our model architecture fuses heterogeneous world state in a
unified Transformer architecture by employing attention across road elements,
agent interactions and time steps. We evaluate our approach on autonomous
driving datasets for behavior prediction, and achieve state-of-the-art
performance. Our work demonstrates that formulating the problem of behavior
prediction in a unified architecture with a masking strategy may allow us to
have a single model that can perform multiple motion prediction and planning
related tasks effectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngiam_J/0/1/0/all/0/1"&gt;Jiquan Ngiam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1"&gt;Benjamin Caine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1"&gt;Vijay Vasudevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1"&gt;Hao-Tien Lewis Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1"&gt;Jeffrey Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1"&gt;Rebecca Roelofs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1"&gt;Alex Bewley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chenxi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venugopal_A/0/1/0/all/0/1"&gt;Ashish Venugopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_D/0/1/0/all/0/1"&gt;David Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1"&gt;Ben Sapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhifeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1"&gt;Jonathon Shlens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing Through Clouds in Satellite Images. (arXiv:2106.08408v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08408</id>
        <link href="http://arxiv.org/abs/2106.08408"/>
        <updated>2021-06-17T01:58:41.752Z</updated>
        <summary type="html"><![CDATA[This paper presents a neural-network-based solution to recover pixels
occluded by clouds in satellite images. We leverage radio frequency (RF)
signals in the ultra/super-high frequency band that penetrate clouds to help
reconstruct the occluded regions in multispectral images. We introduce the
first multi-modal multi-temporal cloud removal model. Our model uses publicly
available satellite observations and produces daily cloud-free images.
Experimental results show that our system significantly outperforms baselines
by 8dB in PSNR. We also demonstrate use cases of our system in digital
agriculture, flood monitoring, and wildfire detection. We will release the
processed dataset to facilitate future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mingmin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olsen_P/0/1/0/all/0/1"&gt;Peder A. Olsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Ranveer Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmenting Part-of-speech Tagging with Syntactic Information for Vietnamese and Chinese. (arXiv:2102.12136v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12136</id>
        <link href="http://arxiv.org/abs/2102.12136"/>
        <updated>2021-06-17T01:58:41.744Z</updated>
        <summary type="html"><![CDATA[Word segmentation and part-of-speech tagging are two critical preliminary
steps for downstream tasks in Vietnamese natural language processing. In
reality, people tend to consider also the phrase boundary when performing word
segmentation and part of speech tagging rather than solely process word by word
from left to right. In this paper, we implement this idea to improve word
segmentation and part of speech tagging the Vietnamese language by employing a
simplified constituency parser. Our neural model for joint word segmentation
and part-of-speech tagging has the architecture of the syllable-based CRF
constituency parser. To reduce the complexity of parsing, we replace all
constituent labels with a single label indicating for phrases. This model can
be augmented with predicted word boundary and part-of-speech tags by other
tools. Because Vietnamese and Chinese have some similar linguistic phenomena,
we evaluated the proposed model and its augmented versions on three Vietnamese
benchmark datasets and six Chinese benchmark datasets. Our experimental results
show that the proposed model achieves higher performances than previous works
for both languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Duc-Vu Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Kiet Van Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Ngan Luu-Thuy Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-scale Neural ODEs for 3D Medical Image Registration. (arXiv:2106.08493v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08493</id>
        <link href="http://arxiv.org/abs/2106.08493"/>
        <updated>2021-06-17T01:58:41.726Z</updated>
        <summary type="html"><![CDATA[Image registration plays an important role in medical image analysis.
Conventional optimization based methods provide an accurate estimation due to
the iterative process at the cost of expensive computation. Deep learning
methods such as learn-to-map are much faster but either iterative or
coarse-to-fine approach is required to improve accuracy for handling large
motions. In this work, we proposed to learn a registration optimizer via a
multi-scale neural ODE model. The inference consists of iterative gradient
updates similar to a conventional gradient descent optimizer but in a much
faster way, because the neural ODE learns from the training data to adapt the
gradient efficiently at each iteration. Furthermore, we proposed to learn a
modal-independent similarity metric to address image appearance variations
across different image contrasts. We performed evaluations through extensive
experiments in the context of multi-contrast 3D MR images from both public and
private data sources and demonstrate the superior performance of our proposed
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Junshen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Eric Z. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Terrence Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shanhui Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GKNet: grasp keypoint network for grasp candidates detection. (arXiv:2106.08497v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.08497</id>
        <link href="http://arxiv.org/abs/2106.08497"/>
        <updated>2021-06-17T01:58:41.720Z</updated>
        <summary type="html"><![CDATA[Contemporary grasp detection approaches employ deep learning to achieve
robustness to sensor and object model uncertainty. The two dominant approaches
design either grasp-quality scoring or anchor-based grasp recognition networks.
This paper presents a different approach to grasp detection by treating it as
keypoint detection. The deep network detects each grasp candidate as a pair of
keypoints, convertible to the grasp representation g = {x, y, w, {\theta}}^T,
rather than a triplet or quartet of corner points. Decreasing the detection
difficulty by grouping keypoints into pairs boosts performance. To further
promote dependencies between keypoints, the general non-local module is
incorporated into the proposed learning framework. A final filtering strategy
based on discrete and continuous orientation prediction removes false
correspondences and further improves grasp detection performance. GKNet, the
approach presented here, achieves the best balance of accuracy and speed on the
Cornell and the abridged Jacquard dataset (96.9% and 98.39% at 41.67 and 23.26
fps). Follow-up experiments on a manipulator evaluate GKNet using 4 types of
grasping experiments reflecting different nuisance sources: static grasping,
dynamic grasping, grasping at varied camera angles, and bin picking. GKNet
outperforms reference baselines in static and dynamic grasping experiments
while showing robustness to varied camera viewpoints and bin picking
experiments. The results confirm the hypothesis that grasp keypoints are an
effective output representation for deep grasp networks that provide robustness
to expected nuisance factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruinian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1"&gt;Fu-Jen Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vela_P/0/1/0/all/0/1"&gt;Patricio A. Vela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-Layered Approach for Measuring the Simulation-to-Reality Gap of Radar Perception for Autonomous Driving. (arXiv:2106.08372v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.08372</id>
        <link href="http://arxiv.org/abs/2106.08372"/>
        <updated>2021-06-17T01:58:41.714Z</updated>
        <summary type="html"><![CDATA[With the increasing safety validation requirements for the release of a
self-driving car, alternative approaches, such as simulation-based testing, are
emerging in addition to conventional real-world testing. In order to rely on
virtual tests the employed sensor models have to be validated. For this reason,
it is necessary to quantify the discrepancy between simulation and reality in
order to determine whether a certain fidelity is sufficient for a desired
intended use. There exists no sound method to measure this
simulation-to-reality gap of radar perception for autonomous driving. We
address this problem by introducing a multi-layered evaluation approach, which
consists of a combination of an explicit and an implicit sensor model
evaluation. The former directly evaluates the realism of the synthetically
generated sensor data, while the latter refers to an evaluation of a downstream
target application. In order to demonstrate the method, we evaluated the
fidelity of three typical radar model types (ideal, data-driven, ray
tracing-based) and their applicability for virtually testing radar-based
multi-object tracking. We have shown the effectiveness of the proposed approach
in terms of providing an in-depth sensor model assessment that renders existing
disparities visible and enables a realistic estimation of the overall model
fidelity across different scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_A/0/1/0/all/0/1"&gt;Anthony Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1"&gt;Max Paul Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Resch_M/0/1/0/all/0/1"&gt;Michael Resch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining. (arXiv:2012.15525v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15525</id>
        <link href="http://arxiv.org/abs/2012.15525"/>
        <updated>2021-06-17T01:58:41.703Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose BANG, a new pretraining model to Bridge the gap
between Autoregressive (AR) and Non-autoregressive (NAR) Generation. AR and NAR
generation can be uniformly regarded as to what extent previous tokens can be
attended, and BANG bridges AR and NAR generation by designing a novel model
structure for large-scale pretraining. The pretrained BANG model can
simultaneously support AR, NAR and semi-NAR generation to meet different
requirements. Experiments on question generation (SQuAD 1.1), summarization
(XSum) and dialogue generation (PersonaChat) show that BANG improves NAR and
semi-NAR performance significantly as well as attaining comparable performance
with strong AR pretrained models. Compared with the semi-NAR strong baselines,
BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of
SQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute
improvements of 10.73, 6.39 and 5.90 in the overall scores of SQuAD, XSUM and
PersonaChat respectively compared with the strong NAR baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1"&gt;Weizhen Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yeyun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1"&gt;Jian Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dayiheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Kewen Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiusheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruofei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Ming Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining decision of model from its prediction. (arXiv:2106.08366v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08366</id>
        <link href="http://arxiv.org/abs/2106.08366"/>
        <updated>2021-06-17T01:58:41.677Z</updated>
        <summary type="html"><![CDATA[This document summarizes different visual explanations methods such as CAM,
Grad-CAM, Localization using Multiple Instance Learning - Saliency-based
methods, Saliency-driven Class-Impressions, Muting pixels in input image -
Adversarial methods and Activation visualization, Convolution filter
visualization - Feature-based methods. We have also shown the results produced
by different methods and a comparison between CAM, GradCAM, and Guided
Backpropagation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tamboli_D/0/1/0/all/0/1"&gt;Dipesh Tamboli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DMSANet: Dual Multi Scale Attention Network. (arXiv:2106.08382v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08382</id>
        <link href="http://arxiv.org/abs/2106.08382"/>
        <updated>2021-06-17T01:58:41.661Z</updated>
        <summary type="html"><![CDATA[Attention mechanism of late has been quite popular in the computer vision
community. A lot of work has been done to improve the performance of the
network, although almost always it results in increased computational
complexity. In this paper, we propose a new attention module that not only
achieves the best performance but also has lesser parameters compared to most
existing models. Our attention module can easily be integrated with other
convolutional neural networks because of its lightweight nature. The proposed
network named Dual Multi Scale Attention Network (DMSANet) is comprised of two
parts: the first part is used to extract features at various scales and
aggregate them, the second part uses spatial and channel attention modules in
parallel to adaptively integrate local features with their global dependencies.
We benchmark our network performance for Image Classification on ImageNet
dataset, Object Detection and Instance Segmentation both on MS COCO dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DialogSum: A Real-Life Scenario Dialogue Summarization Dataset. (arXiv:2105.06762v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06762</id>
        <link href="http://arxiv.org/abs/2105.06762"/>
        <updated>2021-06-17T01:58:41.617Z</updated>
        <summary type="html"><![CDATA[Proposal of large-scale datasets has facilitated research on deep neural
models for news summarization. Deep learning can also be potentially useful for
spoken dialogue summarization, which can benefit a range of real-life scenarios
including customer service management and medication tracking. To this end, we
propose DialogSum, a large-scale labeled dialogue summarization dataset. We
conduct empirical analysis on DialogSum using state-of-the-art neural
summarizers. Experimental results show unique challenges in dialogue
summarization, such as spoken terms, special discourse structures, coreferences
and ellipsis, pragmatics and social common sense, which require specific
representation learning technologies to better deal with.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yulong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QuatDE: Dynamic Quaternion Embedding for Knowledge Graph Completion. (arXiv:2105.09002v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09002</id>
        <link href="http://arxiv.org/abs/2105.09002"/>
        <updated>2021-06-17T01:58:41.610Z</updated>
        <summary type="html"><![CDATA[Knowledge graph embedding has been an active research topic for knowledge
base completion (KGC), with progressive improvement from the initial TransE,
TransH, RotatE et al to the current state-of-the-art QuatE. However, QuatE
ignores the multi-faceted nature of the entity and the complexity of the
relation, only using rigorous operation on quaternion space to capture the
interaction between entitiy pair and relation, leaving opportunities for better
knowledge representation which will finally help KGC. In this paper, we propose
a novel model, QuatDE, with a dynamic mapping strategy to explicitly capture
the variety of relational patterns and separate different semantic information
of the entity, using transition vectors to adjust the point position of the
entity embedding vectors in the quaternion space via Hamilton product,
enhancing the feature interaction capability between elements of the triplet.
Experiment results show QuatDE achieves state-of-the-art performance on three
well-established knowledge graph completion benchmarks. In particular, the MR
evaluation has relatively increased by 26% on WN18 and 15% on WN18RR, which
proves the generalization of QuatDE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1"&gt;Haipeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuxue Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zakari_R/0/1/0/all/0/1"&gt;Rufai Yusuf Zakari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Owusu_J/0/1/0/all/0/1"&gt;Jim Wilson Owusu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1"&gt;Ke Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset. (arXiv:2104.08459v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08459</id>
        <link href="http://arxiv.org/abs/2104.08459"/>
        <updated>2021-06-17T01:58:41.604Z</updated>
        <summary type="html"><![CDATA[This paper introduces a high-quality open-source speech synthesis dataset for
Kazakh, a low-resource language spoken by over 13 million people worldwide. The
dataset consists of about 93 hours of transcribed audio recordings spoken by
two professional speakers (female and male). It is the first publicly available
large-scale dataset developed to promote Kazakh text-to-speech (TTS)
applications in both academia and industry. In this paper, we share our
experience by describing the dataset development procedures and faced
challenges, and discuss important future directions. To demonstrate the
reliability of our dataset, we built baseline end-to-end TTS models and
evaluated them using the subjective mean opinion score (MOS) measure.
Evaluation results show that the best TTS models trained on our dataset achieve
MOS above 4 for both speakers, which makes them applicable for practical use.
The dataset, training recipe, and pretrained TTS models are freely available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mussakhojayeva_S/0/1/0/all/0/1"&gt;Saida Mussakhojayeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Janaliyeva_A/0/1/0/all/0/1"&gt;Aigerim Janaliyeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mirzakhmetov_A/0/1/0/all/0/1"&gt;Almas Mirzakhmetov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khassanov_Y/0/1/0/all/0/1"&gt;Yerbolat Khassanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1"&gt;Huseyin Atakan Varol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Information Divergence Measure Between Neural Text and Human Text. (arXiv:2102.01454v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01454</id>
        <link href="http://arxiv.org/abs/2102.01454"/>
        <updated>2021-06-17T01:58:41.552Z</updated>
        <summary type="html"><![CDATA[As major progress is made in open-ended text generation, measuring how close
machine-generated text is to human language remains a critical open problem. We
propose Mauve, a comparison measure for open-ended text generation, which
directly compares a generation model's distribution to that of human-written
text. Mauve measures the mean area under a divergence curve for the two
distributions, exploring the trade-off between two types of errors: those
arising from parts of the human distribution that the model distribution
approximates well, and those it does not. Mauve extends a family of information
divergence metrics, introducing a tractable approximation based on computing
the KL divergence in a quantized embedding space. This yields an efficient
implementation that scales up to modern text generation models. Through an
extensive empirical study on three open-ended generation tasks, we find that
Mauve identifies known properties of generated text, scales naturally with
model size, and correlates with human judgments, with fewer restrictions than
existing distributional evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pillutla_K/0/1/0/all/0/1"&gt;Krishna Pillutla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1"&gt;Swabha Swayamdipta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1"&gt;Rowan Zellers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1"&gt;John Thickstun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1"&gt;Sean Welleck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harchaoui_Z/0/1/0/all/0/1"&gt;Zaid Harchaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Topic Coverage Approach to Evaluation of Topic Models. (arXiv:2012.06274v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06274</id>
        <link href="http://arxiv.org/abs/2012.06274"/>
        <updated>2021-06-17T01:58:41.545Z</updated>
        <summary type="html"><![CDATA[Topic models are widely used unsupervised models of text capable of learning
topics - weighted lists of words and documents - from large collections of text
documents. When topic models are used for discovery of topics in text
collections, a question that arises naturally is how well the model-induced
topics correspond to topics of interest to the analyst. In this paper we
revisit and extend a so far neglected approach to topic model evaluation based
on measuring topic coverage - computationally matching model topics with a set
of reference topics that models are expected to uncover. The approach is well
suited for analyzing models' performance in topic discovery and for large-scale
analysis of both topic models and measures of model quality. We propose new
measures of coverage and evaluate, in a series of experiments, different types
of topic models on two distinct text domains for which interest for topic
discovery exists. The experiments include evaluation of model quality, analysis
of coverage of distinct topic categories, and the analysis of the relationship
between coverage and other methods of topic model evaluation. The contributions
of the paper include new measures of coverage, insights into both topic models
and other methods of model evaluation, and the datasets and code for
facilitating future research of both topic coverage and other approaches to
topic model evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korencic_D/0/1/0/all/0/1"&gt;Damir Koren&amp;#x10d;i&amp;#x107;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ristov_S/0/1/0/all/0/1"&gt;Strahil Ristov&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Repar_J/0/1/0/all/0/1"&gt;Jelena Repar&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1"&gt;Jan &amp;#x160;najder&lt;/a&gt; (2) ((1) Rudjer Bo&amp;#x161;kovi&amp;#x107; Institute, Croatia, (2) University of Zagreb, Faculty of Electrical Engineering and Computing, Croatia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Impact of ASR on the Automatic Analysis of Linguistic Complexity and Sophistication in Spontaneous L2 Speech. (arXiv:2104.08529v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08529</id>
        <link href="http://arxiv.org/abs/2104.08529"/>
        <updated>2021-06-17T01:58:41.539Z</updated>
        <summary type="html"><![CDATA[In recent years, automated approaches to assessing linguistic complexity in
second language (L2) writing have made significant progress in gauging learner
performance, predicting human ratings of the quality of learner productions,
and benchmarking L2 development. In contrast, there is comparatively little
work in the area of speaking, particularly with respect to fully automated
approaches to assessing L2 spontaneous speech. While the importance of a
well-performing ASR system is widely recognized, little research has been
conducted to investigate the impact of its performance on subsequent automatic
text analysis. In this paper, we focus on this issue and examine the impact of
using a state-of-the-art ASR system for subsequent automatic analysis of
linguistic complexity in spontaneously produced L2 speech. A set of 30 selected
measures were considered, falling into four categories: syntactic, lexical,
n-gram frequency, and information-theoretic measures. The agreement between the
scores for these measures obtained on the basis of ASR-generated vs. manual
transcriptions was determined through correlation analysis. A more differential
effect of ASR performance on specific types of complexity measures when
controlling for task type effects is also presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerz_E/0/1/0/all/0/1"&gt;Elma Kerz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Refining Language Models with Compositional Explanations. (arXiv:2103.10415v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10415</id>
        <link href="http://arxiv.org/abs/2103.10415"/>
        <updated>2021-06-17T01:58:41.531Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models have been successful on text classification
tasks, but are prone to learning spurious correlations from biased datasets,
and are thus vulnerable when making inferences in a new domain. Prior works
reveal such spurious patterns via post-hoc explanation algorithms which compute
the importance of input features. Further, the model is regularized to align
the importance scores with human knowledge, so that the unintended model
behaviors are eliminated. However, such a regularization technique lacks
flexibility and coverage, since only importance scores towards a pre-defined
list of features are adjusted, while more complex human knowledge such as
feature interaction and pattern generalization can hardly be incorporated. In
this work, we propose to refine a learned language model for a target domain by
collecting human-provided compositional explanations regarding observed biases.
By parsing these explanations into executable logic rules, the human-specified
refinement advice from a small set of explanations can be generalized to more
training examples. We additionally introduce a regularization term allowing
adjustments for both importance and interaction of features to better rectify
model behavior. We demonstrate the effectiveness of the proposed approach on
two text classification tasks by showing improved performance in target domain
as well as improved model fairness after refinement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Huihan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qinyuan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xisen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Earnings-21: A Practical Benchmark for ASR in the Wild. (arXiv:2104.11348v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11348</id>
        <link href="http://arxiv.org/abs/2104.11348"/>
        <updated>2021-06-17T01:58:41.524Z</updated>
        <summary type="html"><![CDATA[Commonly used speech corpora inadequately challenge academic and commercial
ASR systems. In particular, speech corpora lack metadata needed for detailed
analysis and WER measurement. In response, we present Earnings-21, a 39-hour
corpus of earnings calls containing entity-dense speech from nine different
financial sectors. This corpus is intended to benchmark ASR systems in the wild
with special attention towards named entity recognition. We benchmark four
commercial ASR models, two internal models built with open-source tools, and an
open-source LibriSpeech model and discuss their differences in performance on
Earnings-21. Using our recently released fstalign tool, we provide a candid
analysis of each model's recognition capabilities under different partitions.
Our analysis finds that ASR accuracy for certain NER categories is poor,
presenting a significant impediment to transcript comprehension and usage.
Earnings-21 bridges academic and commercial ASR system evaluation and enables
further research on entity modeling and WER on real world audio.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rio_M/0/1/0/all/0/1"&gt;Miguel Del Rio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delworth_N/0/1/0/all/0/1"&gt;Natalie Delworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Westerman_R/0/1/0/all/0/1"&gt;Ryan Westerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Michelle Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1"&gt;Nishchal Bhandari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palakapilly_J/0/1/0/all/0/1"&gt;Joseph Palakapilly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McNamara_Q/0/1/0/all/0/1"&gt;Quinten McNamara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Joshua Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelasko_P/0/1/0/all/0/1"&gt;Piotr Zelasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jette_M/0/1/0/all/0/1"&gt;Miguel Jette&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Variational Attention Models for Language Generation. (arXiv:2004.09764v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09764</id>
        <link href="http://arxiv.org/abs/2004.09764"/>
        <updated>2021-06-17T01:58:41.517Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders have been widely applied for natural language
generation, however, there are two long-standing problems: information
under-representation and posterior collapse. The former arises from the fact
that only the last hidden state from the encoder is transformed to the latent
space, which is insufficient to summarize data. The latter comes as a result of
the imbalanced scale between the reconstruction loss and the KL divergence in
the objective function. To tackle these issues, in this paper we propose the
discrete variational attention model with categorical distribution over the
attention mechanism owing to the discrete nature in languages. Our approach is
combined with an auto-regressive prior to capture the sequential dependency
from observations, which can enhance the latent space for language generation.
Moreover, thanks to the property of discreteness, the training of our proposed
approach does not suffer from posterior collapse. Furthermore, we carefully
analyze the superiority of discrete latent space over the continuous space with
the common Gaussian distribution. Extensive experiments on language generation
demonstrate superior advantages of our proposed approach in comparison with the
state-of-the-art counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subword Sampling for Low Resource Word Alignment. (arXiv:2012.11657v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11657</id>
        <link href="http://arxiv.org/abs/2012.11657"/>
        <updated>2021-06-17T01:58:41.489Z</updated>
        <summary type="html"><![CDATA[Annotation projection is an important area in NLP that can greatly contribute
to creating language resources for low-resource languages. Word alignment plays
a key role in this setting. However, most of the existing word alignment
methods are designed for a high resource setting in machine translation where
millions of parallel sentences are available. This amount reduces to a few
thousands of sentences when dealing with low-resource languages failing the
existing established IBM models. In this paper, we propose subword
sampling-based alignment of text units. This method's hypothesis is that the
aggregation of different granularities of text for certain language pairs can
help word-level alignment. For certain languages for which gold-standard
alignments exist, we propose an iterative Bayesian optimization framework to
optimize selecting possible subwords from the space of possible subword
representations of the source and target sentences. We show that the subword
sampling method consistently outperforms word-level alignment on six language
pairs: English-German, English-French, English-Romanian, English-Persian,
English-Hindi, and English-Inuktitut. In addition, we show that the
hyperparameters learned for certain language pairs can be applied to other
languages at no supervision and consistently improve the alignment results. We
observe that using $5K$ parallel sentences together with our proposed subword
sampling approach, we obtain similar F1 scores to the use of $100K$'s of
parallel sentences in existing word-level fast-align/eflomal alignment methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asgari_E/0/1/0/all/0/1"&gt;Ehsaneddin Asgari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1"&gt;Masoud Jalili Sabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1"&gt;Philipp Dufter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ringlstetter_C/0/1/0/all/0/1"&gt;Christopher Ringlstetter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1"&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phoneme-BERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript. (arXiv:2102.00804v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00804</id>
        <link href="http://arxiv.org/abs/2102.00804"/>
        <updated>2021-06-17T01:58:41.472Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed significant improvement in ASR systems to
recognize spoken utterances. However, it is still a challenging task for noisy
and out-of-domain data, where substitution and deletion errors are prevalent in
the transcribed text. These errors significantly degrade the performance of
downstream tasks. In this work, we propose a BERT-style language model,
referred to as PhonemeBERT, that learns a joint language model with phoneme
sequence and ASR transcript to learn phonetic-aware representations that are
robust to ASR errors. We show that PhonemeBERT can be used on downstream tasks
using phoneme sequences as additional features, and also in low-resource setup
where we only have ASR-transcripts for the downstream tasks with no phoneme
information available. We evaluate our approach extensively by generating noisy
data for three benchmark datasets - Stanford Sentiment Treebank, TREC and ATIS
for sentiment, question and intent classification tasks respectively. The
results of the proposed approach beats the state-of-the-art baselines
comprehensively on each dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sundararaman_M/0/1/0/all/0/1"&gt;Mukuntha Narayanan Sundararaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Ayush Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vepa_J/0/1/0/all/0/1"&gt;Jithendra Vepa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models. (arXiv:2010.08566v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08566</id>
        <link href="http://arxiv.org/abs/2010.08566"/>
        <updated>2021-06-17T01:58:41.430Z</updated>
        <summary type="html"><![CDATA[Publicly available, large pretrained LanguageModels (LMs) generate text with
remarkable quality, but only sequentially from left to right. As a result, they
are not immediately applicable to generation tasks that break the
unidirectional assumption, such as paraphrasing or text-infilling,
necessitating task-specific supervision.

In this paper, we present Reflective Decoding, a novel unsupervised algorithm
that allows for direct application of unidirectional LMs to non-sequential
tasks. Our 2-step approach requires no supervision or even parallel corpora,
only two off-the-shelf pretrained LMs in opposite directions: forward and
backward. First, in the contextualization step, we use LMs to generate
ensembles of past and future contexts which collectively capture the input
(e.g. the source sentence for paraphrasing). Second, in the reflection step, we
condition on these "context ensembles", generating outputs that are compatible
with them. Comprehensive empirical results demonstrate that Reflective Decoding
outperforms strong unsupervised baselines on both paraphrasing and abductive
text infilling, significantly narrowing the gap between unsupervised and
supervised methods. Reflective Decoding surpasses multiple supervised baselines
on various metrics including human evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1"&gt;Peter West&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Ximing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1"&gt;Ari Holtzman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1"&gt;Chandra Bhagavatula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Jena Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions. (arXiv:2012.04293v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04293</id>
        <link href="http://arxiv.org/abs/2012.04293"/>
        <updated>2021-06-17T01:58:41.390Z</updated>
        <summary type="html"><![CDATA[Humans are able to perceive, understand and reason about physical events.
Developing models with similar physical understanding capabilities is a
long-standing goal of artificial intelligence. As a step towards this goal, in
this work, we introduce CRAFT, a new visual question answering dataset that
requires causal reasoning about physical forces and object interactions. It
contains 58K video and question pairs that are generated from 10K videos from
20 different virtual environments, containing various objects in motion that
interact with each other and the scene. Two question categories from CRAFT
include previously studied descriptive and counterfactual questions. Besides,
inspired by the theories of force dynamics in cognitive linguistics, we
introduce new question categories that involve understanding the interactions
of objects through the notions of cause, enable, and prevent. Our results
demonstrate that even though these tasks seem to be simple and intuitive for
humans, the evaluated baseline models, including existing state-of-the-art
methods, do not yet deal with the challenges posed in our benchmark dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ates_T/0/1/0/all/0/1"&gt;Tayfun Ates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atesoglu_M/0/1/0/all/0/1"&gt;Muhammed Samil Atesoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yigit_C/0/1/0/all/0/1"&gt;Cagatay Yigit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kesen_I/0/1/0/all/0/1"&gt;Ilker Kesen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobas_M/0/1/0/all/0/1"&gt;Mert Kobas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1"&gt;Erkut Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1"&gt;Aykut Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goksun_T/0/1/0/all/0/1"&gt;Tilbe Goksun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1"&gt;Deniz Yuret&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automated Website Classification by Deep Learning. (arXiv:1910.09991v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.09991</id>
        <link href="http://arxiv.org/abs/1910.09991"/>
        <updated>2021-06-17T01:58:41.379Z</updated>
        <summary type="html"><![CDATA[In recent years, the interest in Big Data sources has been steadily growing
within the Official Statistic community. The Italian National Institute of
Statistics (Istat) is currently carrying out several Big Data pilot studies.
One of these studies, the ICT Big Data pilot, aims at exploiting massive
amounts of textual data automatically scraped from the websites of Italian
enterprises in order to predict a set of target variables (e.g. e-commerce)
that are routinely observed by the traditional ICT Survey. In this paper, we
show that Deep Learning techniques can successfully address this problem.
Essentially, we tackle a text classification task: an algorithm must learn to
infer whether an Italian enterprise performs e-commerce from the textual
content of its website. To reach this goal, we developed a sophisticated
processing pipeline and evaluated its performance through extensive
experiments. Our pipeline uses Convolutional Neural Networks and relies on Word
Embeddings to encode raw texts into grayscale images (i.e. normalized numeric
matrices). Web-scraped texts are huge and have very low signal to noise ratio:
to overcome these issues, we adopted a framework known as False Positive
Reduction, which has seldom (if ever) been applied before to text
classification tasks. Several original contributions enable our processing
pipeline to reach good classification results. Empirical evidence shows that
our proposal outperforms all the alternative Machine Learning solutions already
tested in Istat for the same task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fausti_F/0/1/0/all/0/1"&gt;Fabrizio De Fausti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pugliese_F/0/1/0/all/0/1"&gt;Francesco Pugliese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zardetto_D/0/1/0/all/0/1"&gt;Diego Zardetto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$C^3$: Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08914</id>
        <link href="http://arxiv.org/abs/2106.08914"/>
        <updated>2021-06-17T01:58:41.308Z</updated>
        <summary type="html"><![CDATA[Video-grounded dialogue systems aim to integrate video understanding and
dialogue understanding to generate responses that are relevant to both the
dialogue and video context. Most existing approaches employ deep learning
models and have achieved remarkable performance, given the relatively small
datasets available. However, the results are partly accomplished by exploiting
biases in the datasets rather than developing multimodal reasoning, resulting
in limited generalization. In this paper, we propose a novel approach of
Compositional Counterfactual Contrastive Learning ($C^3$) to develop
contrastive training between factual and counterfactual samples in
video-grounded dialogues. Specifically, we design factual/counterfactual
sampling based on the temporal steps in videos and tokens in dialogues and
propose contrastive loss functions that exploit object-level or action-level
variance. Different from prior approaches, we focus on contrastive hidden state
representations among compositional output tokens to optimize the
representation space in a generation setting. We achieved promising performance
gains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the
benefits of our approach in grounding video and dialogue context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hung Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nancy F. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1"&gt;Steven C.H. Hoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Training of Acoustic Encoders for Speech Recognition. (arXiv:2106.08960v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08960</id>
        <link href="http://arxiv.org/abs/2106.08960"/>
        <updated>2021-06-17T01:58:41.287Z</updated>
        <summary type="html"><![CDATA[On-device speech recognition requires training models of different sizes for
deploying on devices with various computational budgets. When building such
different models, we can benefit from training them jointly to take advantage
of the knowledge shared between them. Joint training is also efficient since it
reduces the redundancy in the training procedure's data handling operations. We
propose a method for collaboratively training acoustic encoders of different
sizes for speech recognition. We use a sequence transducer setup where
different acoustic encoders share a common predictor and joiner modules. The
acoustic encoders are also trained using co-distillation through an auxiliary
task for frame level chenone prediction, along with the transducer loss. We
perform experiments using the LibriSpeech corpus and demonstrate that the
collaboratively trained acoustic encoders can provide up to a 11% relative
improvement in the word error rate on both the test partitions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagaraja_V/0/1/0/all/0/1"&gt;Varun Nagaraja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yangyang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1"&gt;Ganesh Venkatesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1"&gt;Ozlem Kalinli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1"&gt;Michael L. Seltzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1"&gt;Vikas Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RefBERT: Compressing BERT by Referencing to Pre-computed Representations. (arXiv:2106.08898v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08898</id>
        <link href="http://arxiv.org/abs/2106.08898"/>
        <updated>2021-06-17T01:58:41.281Z</updated>
        <summary type="html"><![CDATA[Recently developed large pre-trained language models, e.g., BERT, have
achieved remarkable performance in many downstream natural language processing
applications. These pre-trained language models often contain hundreds of
millions of parameters and suffer from high computation and latency in
real-world applications. It is desirable to reduce the computation overhead of
the models for fast training and inference while keeping the model performance
in downstream applications. Several lines of work utilize knowledge
distillation to compress the teacher model to a smaller student model. However,
they usually discard the teacher's knowledge when in inference. Differently, in
this paper, we propose RefBERT to leverage the knowledge learned from the
teacher, i.e., facilitating the pre-computed BERT representation on the
reference sample and compressing BERT into a smaller student model. To
guarantee our proposal, we provide theoretical justification on the loss
function and the usage of reference samples. Significantly, the theoretical
result shows that including the pre-computed teacher's representations on the
reference samples indeed increases the mutual information in learning the
student model. Finally, we conduct the empirical evaluation and show that our
RefBERT can beat the vanilla TinyBERT over 8.1\% and achieves more than 94\% of
the performance of $\BERTBASE$ on the GLUE benchmark. Meanwhile, RefBERT is
7.4x smaller and 9.5x faster on inference than BERT$_{\rm BASE}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haiqin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1"&gt;Yang Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jianping Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Spoken Language Understanding for Generalized Voice Assistants. (arXiv:2106.09009v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09009</id>
        <link href="http://arxiv.org/abs/2106.09009"/>
        <updated>2021-06-17T01:58:41.271Z</updated>
        <summary type="html"><![CDATA[End-to-end (E2E) spoken language understanding (SLU) systems predict
utterance semantics directly from speech using a single model. Previous work in
this area has focused on targeted tasks in fixed domains, where the output
semantic structure is assumed a priori and the input speech is of limited
complexity. In this work we present our approach to developing an E2E model for
generalized SLU in commercial voice assistants (VAs). We propose a fully
differentiable, transformer-based, hierarchical system that can be pretrained
at both the ASR and NLU levels. This is then fine-tuned on both transcription
and semantic classification losses to handle a diverse set of intent and
argument combinations. This leads to an SLU system that achieves significant
improvements over baselines on a complex internal generalized VA dataset with a
43% improvement in accuracy, while still meeting the 99% accuracy benchmark on
the popular Fluent Speech Commands dataset. We further evaluate our model on a
hard test set, exclusively containing slot arguments unseen in training, and
demonstrate a nearly 20% improvement, showing the efficacy of our approach in
truly demanding VA scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1"&gt;Michael Saxon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1"&gt;Samridhi Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKenna_J/0/1/0/all/0/1"&gt;Joseph P. McKenna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1"&gt;Athanasios Mouchtaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08977</id>
        <link href="http://arxiv.org/abs/2106.08977"/>
        <updated>2021-06-17T01:58:41.262Z</updated>
        <summary type="html"><![CDATA[Weak supervision has shown promising results in many natural language
processing tasks, such as Named Entity Recognition (NER). Existing work mainly
focuses on learning deep NER models only with weak supervision, i.e., without
any human annotation, and shows that by merely using weakly labeled data, one
can achieve good performance, though still underperforms fully supervised NER
with manually/strongly labeled data. In this paper, we consider a more
practical scenario, where we have both a small amount of strongly labeled data
and a large amount of weakly labeled data. Unfortunately, we observe that
weakly labeled data does not necessarily improve, or even deteriorate the model
performance (due to the extensive noise in the weak labels) when we train deep
NER models over a simple or weighted combination of the strongly labeled and
weakly labeled data. To address this issue, we propose a new multi-stage
computational framework -- NEEDLE with three essential ingredients: (1) weak
label completion, (2) noise-aware loss function, and (3) final fine-tuning over
the strongly labeled data. Through experiments on E-commerce query NER and
Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise
of the weak labels and outperforms existing methods. In particular, we achieve
new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,
BC5CDR-disease 90.69, NCBI-disease 92.28.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Danqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tianyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Bing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Discourse to Narrative: Knowledge Projection for Event Relation Extraction. (arXiv:2106.08629v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08629</id>
        <link href="http://arxiv.org/abs/2106.08629"/>
        <updated>2021-06-17T01:58:41.255Z</updated>
        <summary type="html"><![CDATA[Current event-centric knowledge graphs highly rely on explicit connectives to
mine relations between events. Unfortunately, due to the sparsity of
connectives, these methods severely undermine the coverage of EventKGs. The
lack of high-quality labelled corpora further exacerbates that problem. In this
paper, we propose a knowledge projection paradigm for event relation
extraction: projecting discourse knowledge to narratives by exploiting the
commonalities between them. Specifically, we propose Multi-tier Knowledge
Projection Network (MKPNet), which can leverage multi-tier discourse knowledge
effectively for event relation extraction. In this way, the labelled data
requirement is significantly reduced, and implicit event relations can be
effectively extracted. Intrinsic experimental results show that MKPNet achieves
the new state-of-the-art performance, and extrinsic experimental results verify
the value of the extracted event relations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jialong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hongyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1"&gt;Meng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yaojie Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xianpei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Le Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Weijian Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding Spatio-Temporal Language with Transformers. (arXiv:2106.08858v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.08858</id>
        <link href="http://arxiv.org/abs/2106.08858"/>
        <updated>2021-06-17T01:58:41.228Z</updated>
        <summary type="html"><![CDATA[Language is an interface to the outside world. In order for embodied agents
to use it, language must be grounded in other, sensorimotor modalities. While
there is an extended literature studying how machines can learn grounded
language, the topic of how to learn spatio-temporal linguistic concepts is
still largely uncharted. To make progress in this direction, we here introduce
a novel spatio-temporal language grounding task where the goal is to learn the
meaning of spatio-temporal descriptions of behavioral traces of an embodied
agent. This is achieved by training a truth function that predicts if a
description matches a given history of observations. The descriptions involve
time-extended predicates in past and present tense as well as spatio-temporal
references to objects in the scene. To study the role of architectural biases
in this task, we train several models including multimodal Transformer
architectures; the latter implement different attention computations between
words and objects across space and time. We test models on two classes of
generalization: 1) generalization to randomly held-out sentences; 2)
generalization to grammar primitives. We observe that maintaining object
identity in the attention computation of our Transformers is instrumental to
achieving good performance on generalization overall, and that summarizing
object traces in a single token has little influence on performance. We then
discuss how this opens new perspectives for language-guided autonomous embodied
agents. We also release our code under open-source license as well as
pretrained models and datasets to encourage the wider community to build upon
and extend our work in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1"&gt;Tristan Karch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1"&gt;Laetitia Teodorescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1"&gt;Katja Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Moulin-Frier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1"&gt;Pierre-Yves Oudeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alzheimer's Disease Detection from Spontaneous Speech through Combining Linguistic Complexity and (Dis)Fluency Features with Pretrained Language Models. (arXiv:2106.08689v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08689</id>
        <link href="http://arxiv.org/abs/2106.08689"/>
        <updated>2021-06-17T01:58:41.215Z</updated>
        <summary type="html"><![CDATA[In this paper, we combined linguistic complexity and (dis)fluency features
with pretrained language models for the task of Alzheimer's disease detection
of the 2021 ADReSSo (Alzheimer's Dementia Recognition through Spontaneous
Speech) challenge. An accuracy of 83.1% was achieved on the test set, which
amounts to an improvement of 4.23% over the baseline model. Our best-performing
model that integrated component models using a stacking ensemble technique
performed equally well on cross-validation and test data, indicating that it is
robust against overfitting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xuefeng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiechmann_D/0/1/0/all/0/1"&gt;Daniel Wiechmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerz_E/0/1/0/all/0/1"&gt;Elma Kerz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coreference Augmentation for Multi-Domain Task-Oriented Dialogue State Tracking. (arXiv:2106.08723v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08723</id>
        <link href="http://arxiv.org/abs/2106.08723"/>
        <updated>2021-06-17T01:58:41.207Z</updated>
        <summary type="html"><![CDATA[Dialogue State Tracking (DST), which is the process of inferring user goals
by estimating belief states given the dialogue history, plays a critical role
in task-oriented dialogue systems. A coreference phenomenon observed in
multi-turn conversations is not addressed by existing DST models, leading to
sub-optimal performances. In this paper, we propose Coreference Dialogue State
Tracker (CDST) that explicitly models the coreference feature. In particular,
at each turn, the proposed model jointly predicts the coreferred domain-slot
pair and extracts the coreference values from the dialogue context.
Experimental results on MultiWOZ 2.1 dataset show that the proposed model
achieves the state-of-the-art joint goal accuracy of 56.47%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1"&gt;Ting Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chongxuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-Based Keyword Localisation in Speech using Visual Grounding. (arXiv:2106.08859v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08859</id>
        <link href="http://arxiv.org/abs/2106.08859"/>
        <updated>2021-06-17T01:58:41.200Z</updated>
        <summary type="html"><![CDATA[Visually grounded speech models learn from images paired with spoken
captions. By tagging images with soft text labels using a trained visual
classifier with a fixed vocabulary, previous work has shown that it is possible
to train a model that can detect whether a particular text keyword occurs in
speech utterances or not. Here we investigate whether visually grounded speech
models can also do keyword localisation: predicting where, within an utterance,
a given textual keyword occurs without any explicit text-based or alignment
supervision. We specifically consider whether incorporating attention into a
convolutional model is beneficial for localisation. Although absolute
localisation performance with visually supervised models is still modest
(compared to using unordered bag-of-word text labels for supervision), we show
that attention provides a large gain in performance over previous visually
grounded models. As in many other speech-image studies, we find that many of
the incorrect localisations are due to semantic confusions, e.g. locating the
word 'backstroke' for the query keyword 'swimming'.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olaleye_K/0/1/0/all/0/1"&gt;Kayode Olaleye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1"&gt;Herman Kamper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation. (arXiv:2106.08942v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08942</id>
        <link href="http://arxiv.org/abs/2106.08942"/>
        <updated>2021-06-17T01:58:41.183Z</updated>
        <summary type="html"><![CDATA[Policy gradient algorithms have found wide adoption in NLP, but have recently
become subject to criticism, doubting their suitability for NMT. Choshen et al.
(2020) identify multiple weaknesses and suspect that their success is
determined by the shape of output distributions rather than the reward. In this
paper, we revisit these claims and study them under a wider range of
configurations. Our experiments on in-domain and cross-domain adaptation reveal
the importance of exploration and reward scaling, and provide empirical
counter-evidence to these claims.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kiegeland_S/0/1/0/all/0/1"&gt;Samuel Kiegeland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1"&gt;Julia Kreutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the expressiveness of neural vocoding with non-affine Normalizing Flows. (arXiv:2106.08649v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.08649</id>
        <link href="http://arxiv.org/abs/2106.08649"/>
        <updated>2021-06-17T01:58:41.166Z</updated>
        <summary type="html"><![CDATA[This paper proposes a general enhancement to the Normalizing Flows (NF) used
in neural vocoding. As a case study, we improve expressive speech vocoding with
a revamped Parallel Wavenet (PW). Specifically, we propose to extend the affine
transformation of PW to the more expressive invertible non-affine function. The
greater expressiveness of the improved PW leads to better-perceived signal
quality and naturalness in the waveform reconstruction and text-to-speech (TTS)
tasks. We evaluate the model across different speaking styles on a
multi-speaker, multi-lingual dataset. In the waveform reconstruction task, the
proposed model closes the naturalness and signal quality gap from the original
PW to recordings by $10\%$, and from other state-of-the-art neural vocoding
systems by more than $60\%$. We also demonstrate improvements in objective
metrics on the evaluation test set with L2 Spectral Distance and Cross-Entropy
reduced by $3\%$ and $6\unicode{x2030}$ comparing to the affine PW.
Furthermore, we extend the probability density distillation procedure proposed
by the original PW paper, so that it works with any non-affine invertible and
differentiable function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gabrys_A/0/1/0/all/0/1"&gt;Adam Gabry&amp;#x15b;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yunlong Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klimkov_V/0/1/0/all/0/1"&gt;Viacheslav Klimkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korzekwa_D/0/1/0/all/0/1"&gt;Daniel Korzekwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barra_Chicote_R/0/1/0/all/0/1"&gt;Roberto Barra-Chicote&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithm to Compilation Codesign: An Integrated View of Neural Network Sparsity. (arXiv:2106.08846v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08846</id>
        <link href="http://arxiv.org/abs/2106.08846"/>
        <updated>2021-06-17T01:58:41.071Z</updated>
        <summary type="html"><![CDATA[Reducing computation cost, inference latency, and memory footprint of neural
networks are frequently cited as research motivations for pruning and sparsity.
However, operationalizing those benefits and understanding the end-to-end
effect of algorithm design and regularization on the runtime execution is not
often examined in depth.

Here we apply structured and unstructured pruning to attention weights of
transformer blocks of the BERT language model, while also expanding block
sparse representation (BSR) operations in the TVM compiler. Integration of BSR
operations enables the TVM runtime execution to leverage structured pattern
sparsity induced by model regularization.

This integrated view of pruning algorithms enables us to study relationships
between modeling decisions and their direct impact on sparsity-enhanced
execution. Our main findings are: 1) we validate that performance benefits of
structured sparsity block regularization must be enabled by the BSR
augmentations to TVM, with 4x speedup relative to vanilla PyTorch and 2.2x
speedup relative to standard TVM compilation (without expanded BSR support). 2)
for BERT attention weights, the end-to-end optimal block sparsity shape in this
CPU inference context is not a square block (as in \cite{gray2017gpu}) but
rather a linear 32x1 block 3) the relationship between performance and block
size / shape is is suggestive of how model regularization parameters interact
with task scheduler optimizations resulting in the observed end-to-end
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1"&gt;Fu-Ming Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1"&gt;Austin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic sentence similarity: size does not always matter. (arXiv:2106.08648v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08648</id>
        <link href="http://arxiv.org/abs/2106.08648"/>
        <updated>2021-06-17T01:58:41.037Z</updated>
        <summary type="html"><![CDATA[This study addresses the question whether visually grounded speech
recognition (VGS) models learn to capture sentence semantics without access to
any prior linguistic knowledge. We produce synthetic and natural spoken
versions of a well known semantic textual similarity database and show that our
VGS model produces embeddings that correlate well with human semantic
similarity judgements. Our results show that a model trained on a small
image-caption database outperforms two models trained on much larger databases,
indicating that database size is not all that matters. We also investigate the
importance of having multiple captions per image and find that this is indeed
helpful even if the total number of images is lower, suggesting that
paraphrasing is a valuable learning signal. While the general trend in the
field is to create ever larger datasets to train models on, our findings
indicate other characteristics of the database can just as important important.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Merkx_D/0/1/0/all/0/1"&gt;Danny Merkx&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_S/0/1/0/all/0/1"&gt;Stefan L. Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ernestus_M/0/1/0/all/0/1"&gt;Mirjam Ernestus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Gender Bias in Hindi-English Machine Translation. (arXiv:2106.08680v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08680</id>
        <link href="http://arxiv.org/abs/2106.08680"/>
        <updated>2021-06-17T01:58:41.028Z</updated>
        <summary type="html"><![CDATA[With language models being deployed increasingly in the real world, it is
essential to address the issue of the fairness of their outputs. The word
embedding representations of these language models often implicitly draw
unwanted associations that form a social bias within the model. The nature of
gendered languages like Hindi, poses an additional problem to the
quantification and mitigation of bias, owing to the change in the form of the
words in the sentence, based on the gender of the subject. Additionally, there
is sparse work done in the realm of measuring and debiasing systems for Indic
languages. In our work, we attempt to evaluate and quantify the gender bias
within a Hindi-English machine translation system. We implement a modified
version of the existing TGBI metric based on the grammatical considerations for
Hindi. We also compare and contrast the resulting bias measurements across
multiple metrics for pre-trained embeddings and the ones learned by our machine
translation model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1"&gt;Gauri Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1"&gt;Krithika Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sanjay Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alternated Training with Synthetic and Authentic Data for Neural Machine Translation. (arXiv:2106.08582v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08582</id>
        <link href="http://arxiv.org/abs/2106.08582"/>
        <updated>2021-06-17T01:58:41.007Z</updated>
        <summary type="html"><![CDATA[While synthetic bilingual corpora have demonstrated their effectiveness in
low-resource neural machine translation (NMT), adding more synthetic data often
deteriorates translation performance. In this work, we propose alternated
training with synthetic and authentic data for NMT. The basic idea is to
alternate synthetic and authentic corpora iteratively during training. Compared
with previous work, we introduce authentic data as guidance to prevent the
training of NMT models from being disturbed by noisy synthetic data.
Experiments on Chinese-English and German-English translation tasks show that
our approach improves the performance over several strong baselines. We
visualize the BLEU landscape to further investigate the role of authentic and
synthetic data during alternated training. From the visualization, we find that
authentic data helps to direct the NMT model parameters towards points with
higher BLEU scores and leads to consistent translation performance improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_R/0/1/0/all/0/1"&gt;Rui Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zonghan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coreference-Aware Dialogue Summarization. (arXiv:2106.08556v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08556</id>
        <link href="http://arxiv.org/abs/2106.08556"/>
        <updated>2021-06-17T01:58:41.001Z</updated>
        <summary type="html"><![CDATA[Summarizing conversations via neural approaches has been gaining research
traction lately, yet it is still challenging to obtain practical solutions.
Examples of such challenges include unstructured information exchange in
dialogues, informal interactions between speakers, and dynamic role changes of
speakers as the dialogue evolves. Many of such challenges result in complex
coreference links. Therefore, in this work, we investigate different approaches
to explicitly incorporate coreference information in neural abstractive
dialogue summarization models to tackle the aforementioned challenges.
Experimental results show that the proposed approaches achieve state-of-the-art
performance, implying it is useful to utilize coreference information in
dialogue summarization. Evaluation results on factual correctness suggest such
coreference-aware models are better at tracing the information flow among
interlocutors and associating accurate status/actions with the corresponding
interlocutors and person mentions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1"&gt;Ke Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nancy F. Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PRASEMap: A Probabilistic Reasoning and Semantic Embedding based Knowledge Graph Alignment System. (arXiv:2106.08801v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08801</id>
        <link href="http://arxiv.org/abs/2106.08801"/>
        <updated>2021-06-17T01:58:40.968Z</updated>
        <summary type="html"><![CDATA[Knowledge Graph (KG) alignment aims at finding equivalent entities and
relations (i.e., mappings) between two KGs. The existing approaches utilize
either reasoning-based or semantic embedding-based techniques, but few studies
explore their combination. In this demonstration, we present PRASEMap, an
unsupervised KG alignment system that iteratively computes the Mappings with
both Probabilistic Reasoning (PR) And Semantic Embedding (SE) techniques.
PRASEMap can support various embedding-based KG alignment approaches as the SE
module, and enables easy human computer interaction that additionally provides
an option for users to feed the mapping annotations back to the system for
better results. The demonstration showcases these features via a stand-alone
Web application with user friendly interfaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhiyuan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Context Features Can Transformer Language Models Use?. (arXiv:2106.08367v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08367</id>
        <link href="http://arxiv.org/abs/2106.08367"/>
        <updated>2021-06-17T01:58:40.892Z</updated>
        <summary type="html"><![CDATA[Transformer-based language models benefit from conditioning on contexts of
hundreds to thousands of previous tokens. What aspects of these contexts
contribute to accurate model prediction? We describe a series of experiments
that measure usable information by selectively ablating lexical and structural
information in transformer language models trained on English Wikipedia. In
both mid- and long-range contexts, we find that several extremely destructive
context manipulations -- including shuffling word order within sentences and
deleting all words other than nouns -- remove less than 15% of the usable
information. Our results suggest that long contexts, but not their detailed
syntactic and propositional content, are important for the low perplexity of
current transformer language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+OConnor_J/0/1/0/all/0/1"&gt;Joe O&amp;#x27;Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Enrichment of Persona-grounded Dialog with Background Stories. (arXiv:2106.08364v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08364</id>
        <link href="http://arxiv.org/abs/2106.08364"/>
        <updated>2021-06-17T01:58:40.885Z</updated>
        <summary type="html"><![CDATA[Humans often refer to personal narratives, life experiences, and events to
make a conversation more engaging and rich. While persona-grounded dialog
models are able to generate responses that follow a given persona, they often
miss out on stating detailed experiences or events related to a persona, often
leaving conversations shallow and dull. In this work, we equip dialog models
with 'background stories' related to a persona by leveraging fictional
narratives from existing story datasets (e.g. ROCStories). Since current dialog
datasets do not contain such narratives as responses, we perform an
unsupervised adaptation of a retrieved story for generating a dialog response
using a gradient-based rewriting technique. Our proposed method encourages the
generated response to be fluent (i.e., highly likely) with the dialog history,
minimally different from the retrieved story to preserve event ordering and
consistent with the original persona. We demonstrate that our method can
generate responses that are more diverse, and are rated more engaging and
human-like by human evaluators, compared to outputs from existing dialog
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1"&gt;Bodhisattwa Prasad Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1"&gt;Harsh Jhamtani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Entity Linking through Semantic Reinforced Entity Embeddings. (arXiv:2106.08495v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08495</id>
        <link href="http://arxiv.org/abs/2106.08495"/>
        <updated>2021-06-17T01:58:40.825Z</updated>
        <summary type="html"><![CDATA[Entity embeddings, which represent different aspects of each entity with a
single vector like word embeddings, are a key component of neural entity
linking models. Existing entity embeddings are learned from canonical Wikipedia
articles and local contexts surrounding target entities. Such entity embeddings
are effective, but too distinctive for linking models to learn contextual
commonality. We propose a simple yet effective method, FGS2EE, to inject
fine-grained semantic information into entity embeddings to reduce the
distinctiveness and facilitate the learning of contextual commonality. FGS2EE
first uses the embeddings of semantic type words to generate semantic
embeddings, and then combines them with existing entity embeddings through
linear aggregation. Extensive experiments show the effectiveness of such
embeddings. Based on our entity embeddings, we achieved new sate-of-the-art
performance on entity linking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1"&gt;Feng Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruili Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jun He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yi Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Auto-regressive Variational Attention Models for Text Modeling. (arXiv:2106.08571v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08571</id>
        <link href="http://arxiv.org/abs/2106.08571"/>
        <updated>2021-06-17T01:58:40.807Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) have been widely applied for text modeling.
In practice, however, they are troubled by two challenges: information
underrepresentation and posterior collapse. The former arises as only the last
hidden state of LSTM encoder is transformed into the latent space, which is
generally insufficient to summarize the data. The latter is a long-standing
problem during the training of VAEs as the optimization is trapped to a
disastrous local optimum. In this paper, we propose Discrete Auto-regressive
Variational Attention Model (DAVAM) to address the challenges. Specifically, we
introduce an auto-regressive variational attention approach to enrich the
latent space by effectively capturing the semantic dependency from the input.
We further design discrete latent space for the variational attention and
mathematically show that our model is free from posterior collapse. Extensive
experiments on language modeling tasks demonstrate the superiority of DAVAM
against several VAE counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code to Comment Translation: A Comparative Study on Model Effectiveness & Errors. (arXiv:2106.08415v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.08415</id>
        <link href="http://arxiv.org/abs/2106.08415"/>
        <updated>2021-06-17T01:58:40.800Z</updated>
        <summary type="html"><![CDATA[Automated source code summarization is a popular software engineering
research topic wherein machine translation models are employed to "translate"
code snippets into relevant natural language descriptions. Most evaluations of
such models are conducted using automatic reference-based metrics. However,
given the relatively large semantic gap between programming languages and
natural language, we argue that this line of research would benefit from a
qualitative investigation into the various error modes of current
state-of-the-art models. Therefore, in this work, we perform both a
quantitative and qualitative comparison of three recently proposed source code
summarization models. In our quantitative evaluation, we compare the models
based on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics,
and in our qualitative evaluation, we perform a manual open-coding of the most
common errors committed by the models when compared to ground truth captions.
Our investigation reveals new insights into the relationship between
metric-based performance and model prediction errors grounded in an empirically
derived error taxonomy that can be used to drive future research efforts]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmud_J/0/1/0/all/0/1"&gt;Junayed Mahmud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1"&gt;Fahim Faisal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnob_R/0/1/0/all/0/1"&gt;Raihan Islam Arnob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1"&gt;Antonios Anastasopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moran_K/0/1/0/all/0/1"&gt;Kevin Moran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Eider: Evidence-enhanced Document-level Relation Extraction. (arXiv:2106.08657v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08657</id>
        <link href="http://arxiv.org/abs/2106.08657"/>
        <updated>2021-06-17T01:58:40.790Z</updated>
        <summary type="html"><![CDATA[Document-level relation extraction (DocRE) aims at extracting the semantic
relations among entity pairs in a document. In DocRE, a subset of the sentences
in a document, called the evidence sentences, might be sufficient for
predicting the relation between a specific entity pair. To make better use of
the evidence sentences, in this paper, we propose a three-stage
evidence-enhanced DocRE framework consisting of joint relation and evidence
extraction, evidence-centered relation extraction (RE), and fusion of
extraction results. We first jointly train an RE model with a simple and
memory-efficient evidence extraction model. Then, we construct pseudo documents
based on the extracted evidence sentences and run the RE model again. Finally,
we fuse the extraction results of the first two stages using a blending layer
and make a final prediction. Extensive experiments show that our proposed
framework achieves state-of-the-art performance on the DocRED dataset,
outperforming the second-best method by 0.76/0.82 Ign F1/F1. In particular, our
method significantly improves the performance on inter-sentence relations by
1.23 Inter F1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yiqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jiaming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sha Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Yuning Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Conversational Networks. (arXiv:2106.08484v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08484</id>
        <link href="http://arxiv.org/abs/2106.08484"/>
        <updated>2021-06-17T01:58:40.780Z</updated>
        <summary type="html"><![CDATA[Inspired by recent work in meta-learning and generative teaching networks, we
propose a framework called Generative Conversational Networks, in which
conversational agents learn to generate their own labelled training data (given
some seed data) and then train themselves from that data to perform a given
task. We use reinforcement learning to optimize the data generation process
where the reward signal is the agent's performance on the task. The task can be
any language-related task, from intent detection to full task-oriented
conversations. In this work, we show that our approach is able to generalise
from seed data and performs well in limited data and limited computation
settings, with significant gains for intent detection and slot tagging across
multiple datasets: ATIS, TOD, SNIPS, and Restaurants8k. We show an average
improvement of 35% in intent detection and 21% in slot tagging over a baseline
model trained from the seed data. We also conduct an analysis of the novelty of
the generated data and provide generated examples for intent detection, slot
tagging, and non-goal oriented conversations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1"&gt;Alexandros Papangelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1"&gt;Karthik Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1"&gt;Aishwarya Padmakumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seokhwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1"&gt;Gokhan Tur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1"&gt;Dilek Hakkani-Tur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RyanSpeech: A Corpus for Conversational Text-to-Speech Synthesis. (arXiv:2106.08468v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08468</id>
        <link href="http://arxiv.org/abs/2106.08468"/>
        <updated>2021-06-17T01:58:40.772Z</updated>
        <summary type="html"><![CDATA[This paper introduces RyanSpeech, a new speech corpus for research on
automated text-to-speech (TTS) systems. Publicly available TTS corpora are
often noisy, recorded with multiple speakers, or lack quality male speech data.
In order to meet the need for a high quality, publicly available male speech
corpus within the field of speech recognition, we have designed and created
RyanSpeech which contains textual materials from real-world conversational
settings. These materials contain over 10 hours of a professional male voice
actor's speech recorded at 44.1 kHz. This corpus's design and pipeline make
RyanSpeech ideal for developing TTS systems in real-world applications. To
provide a baseline for future research, protocols, and benchmarks, we trained 4
state-of-the-art speech models and a vocoder on RyanSpeech. The results show
3.36 in mean opinion scores (MOS) in our best model. We have made both the
corpus and trained models for public use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zandie_R/0/1/0/all/0/1"&gt;Rohola Zandie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoor_M/0/1/0/all/0/1"&gt;Mohammad H. Mahoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madsen_J/0/1/0/all/0/1"&gt;Julia Madsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emamian_E/0/1/0/all/0/1"&gt;Eshrat S. Emamian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training. (arXiv:2106.08616v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08616</id>
        <link href="http://arxiv.org/abs/2106.08616"/>
        <updated>2021-06-17T01:58:40.754Z</updated>
        <summary type="html"><![CDATA[Out-of-scope intent detection is of practical importance in task-oriented
dialogue systems. Since the distribution of outlier utterances is arbitrary and
unknown in the training stage, existing methods commonly rely on strong
assumptions on data distribution such as mixture of Gaussians to make
inference, resulting in either complex multi-step training procedures or
hand-crafted rules such as confidence threshold selection for outlier
detection. In this paper, we propose a simple yet effective method to train an
out-of-scope intent classifier in a fully end-to-end manner by simulating the
test scenario in training, which requires no assumption on data distribution
and no additional post-processing or threshold setting. Specifically, we
construct a set of pseudo outliers in the training stage, by generating
synthetic outliers using inliner features via self-supervision and sampling
out-of-scope sentences from easily available open-domain datasets. The pseudo
outliers are used to train a discriminative classifier that can be directly
applied to and generalize well on the test task. We evaluate our method
extensively on four benchmark dialogue datasets and observe significant
improvements over state-of-the-art approaches. Our code has been released at
https://github.com/liam0949/DCLOOS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1"&gt;Li-Ming Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Haowen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lu Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiao-Ming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_A/0/1/0/all/0/1"&gt;Albert Y.S. Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study. (arXiv:2106.08686v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08686</id>
        <link href="http://arxiv.org/abs/2106.08686"/>
        <updated>2021-06-17T01:58:40.737Z</updated>
        <summary type="html"><![CDATA[Several variants of deep neural networks have been successfully employed for
building parametric models that project variable-duration spoken word segments
onto fixed-size vector representations, or acoustic word embeddings (AWEs).
However, it remains unclear to what degree we can rely on the distance in the
emerging AWE space as an estimate of word-form similarity. In this paper, we
ask: does the distance in the acoustic embedding space correlate with
phonological dissimilarity? To answer this question, we empirically investigate
the performance of supervised approaches for AWEs with different neural
architectures and learning objectives. We train AWE models in controlled
settings for two languages (German and Czech) and evaluate the embeddings on
two tasks: word discrimination and phonological similarity. Our experiments
show that (1) the distance in the embedding space in the best cases only
moderately correlates with phonological distance, and (2) improving the
performance on the word discrimination task does not necessarily yield models
that better reflect word phonological similarity. Our findings highlight the
necessity to rethink the current intrinsic evaluations for AWEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdullah_B/0/1/0/all/0/1"&gt;Badr M. Abdullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1"&gt;Marius Mosbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaitova_I/0/1/0/all/0/1"&gt;Iuliia Zaitova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mobius_B/0/1/0/all/0/1"&gt;Bernd M&amp;#xf6;bius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1"&gt;Dietrich Klakow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Better Understanding of Linear Models for Recommendation. (arXiv:2105.12937v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12937</id>
        <link href="http://arxiv.org/abs/2105.12937"/>
        <updated>2021-06-17T01:58:40.720Z</updated>
        <summary type="html"><![CDATA[Recently, linear regression models, such as EASE and SLIM, have shown to
often produce rather competitive results against more sophisticated deep
learning models. On the other side, the (weighted) matrix factorization
approaches have been popular choices for recommendation in the past and widely
adopted in the industry. In this work, we aim to theoretically understand the
relationship between these two approaches, which are the cornerstones of
model-based recommendations. Through the derivation and analysis of the
closed-form solutions for two basic regression and matrix factorization
approaches, we found these two approaches are indeed inherently related but
also diverge in how they "scale-down" the singular values of the original
user-item interaction matrix. This analysis also helps resolve the questions
related to the regularization parameter range and model complexities. We
further introduce a new learning algorithm in searching (hyper)parameters for
the closed-form solution and utilize it to discover the nearby models of the
existing solutions. The experimental results demonstrate that the basic models
and their closed-form solutions are indeed quite competitive against the
state-of-the-art models, thus, confirming the validity of studying the basic
models. The effectiveness of exploring the nearby models are also
experimentally validated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Ruoming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jing Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yang Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Topic Coverage Approach to Evaluation of Topic Models. (arXiv:2012.06274v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06274</id>
        <link href="http://arxiv.org/abs/2012.06274"/>
        <updated>2021-06-17T01:58:40.701Z</updated>
        <summary type="html"><![CDATA[Topic models are widely used unsupervised models of text capable of learning
topics - weighted lists of words and documents - from large collections of text
documents. When topic models are used for discovery of topics in text
collections, a question that arises naturally is how well the model-induced
topics correspond to topics of interest to the analyst. In this paper we
revisit and extend a so far neglected approach to topic model evaluation based
on measuring topic coverage - computationally matching model topics with a set
of reference topics that models are expected to uncover. The approach is well
suited for analyzing models' performance in topic discovery and for large-scale
analysis of both topic models and measures of model quality. We propose new
measures of coverage and evaluate, in a series of experiments, different types
of topic models on two distinct text domains for which interest for topic
discovery exists. The experiments include evaluation of model quality, analysis
of coverage of distinct topic categories, and the analysis of the relationship
between coverage and other methods of topic model evaluation. The contributions
of the paper include new measures of coverage, insights into both topic models
and other methods of model evaluation, and the datasets and code for
facilitating future research of both topic coverage and other approaches to
topic model evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korencic_D/0/1/0/all/0/1"&gt;Damir Koren&amp;#x10d;i&amp;#x107;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ristov_S/0/1/0/all/0/1"&gt;Strahil Ristov&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Repar_J/0/1/0/all/0/1"&gt;Jelena Repar&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1"&gt;Jan &amp;#x160;najder&lt;/a&gt; (2) ((1) Rudjer Bo&amp;#x161;kovi&amp;#x107; Institute, Croatia, (2) University of Zagreb, Faculty of Electrical Engineering and Computing, Croatia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal and specific features of Ukrainian economic research: publication analysis based on Crossref data. (arXiv:2106.08701v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.08701</id>
        <link href="http://arxiv.org/abs/2106.08701"/>
        <updated>2021-06-17T01:58:40.608Z</updated>
        <summary type="html"><![CDATA[Our study is one of the first examples of multidimensional and longitudinal
disciplinary analysis at the national level based on Crossref data. We present
a large-scale quantitative analysis of Ukrainian economics. This study is not
yet another example of research aimed at ranking of local journals, authors or
institutions, but rather exploring general tendencies that can be compared to
other countries or regions. We study different aspects of Ukrainian economics
output. In particular, the collaborative nature, geographic landscape and some
peculiarities of citation statistics are investigated. We have found that
Ukrainian economics is characterized by a comparably small share of co-authored
publications, however, it demonstrates the tendency towards more collaborative
output. Based on our analysis, we discuss specific and universal features of
Ukrainian economic research. The importance of supporting various initiatives
aimed at enriching open scholarly metadata is considered. A comprehensive and
high-quality meta description of publications is probably the shortest path to
a better understanding of national trends, especially for non-English speaking
countries. The results of our analysis can be used to better understand
Ukrainian economic research and support research policy decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mryglod_O/0/1/0/all/0/1"&gt;O. Mryglod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nazarovets_S/0/1/0/all/0/1"&gt;S. Nazarovets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kozmenko_S/0/1/0/all/0/1"&gt;S. Kozmenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSSuBERT: Tweet Stream Summarization Using BERT. (arXiv:2106.08770v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08770</id>
        <link href="http://arxiv.org/abs/2106.08770"/>
        <updated>2021-06-17T01:58:40.264Z</updated>
        <summary type="html"><![CDATA[The development of deep neural networks and the emergence of pre-trained
language models such as BERT allow to increase performance on many NLP tasks.
However, these models do not meet the same popularity for tweet summarization,
which can probably be explained by the lack of existing collections for
training and evaluation. Our contribution in this paper is twofold : (1) we
introduce a large dataset for Twitter event summarization, and (2) we propose a
neural model to automatically summarize huge tweet streams. This extractive
model combines in an original way pre-trained language models and vocabulary
frequency-based representations to predict tweet salience. An additional
advantage of the model is that it automatically adapts the size of the output
summary according to the input tweet stream. We conducted experiments using two
different Twitter collections, and promising results are observed in comparison
with state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dusart_A/0/1/0/all/0/1"&gt;Alexis Dusart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinel_Sauvagnat_K/0/1/0/all/0/1"&gt;Karen Pinel-Sauvagnat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubert_G/0/1/0/all/0/1"&gt;Gilles Hubert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized News Recommendation: A Survey. (arXiv:2106.08934v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08934</id>
        <link href="http://arxiv.org/abs/2106.08934"/>
        <updated>2021-06-17T01:58:40.220Z</updated>
        <summary type="html"><![CDATA[Personalized news recommendation is an important technique to help users find
their interested news information and alleviate their information overload. It
has been extensively studied over decades and has achieved notable success in
improving users' news reading experience. However, there are still many
unsolved problems and challenges that need to be further studied. To help
researchers master the advances in personalized news recommendation over the
past years, in this paper we present a comprehensive overview of personalized
news recommendation. Instead of following the conventional taxonomy of news
recommendation methods, in this paper we propose a novel perspective to
understand personalized news recommendation based on its core problems and the
associated techniques and challenges. We first review the techniques for
tackling each core problem in a personalized news recommender system and the
challenges they face. Next, we introduce the public datasets and evaluation
metrics used for personalized news recommendation. We then discuss the key
points on improving the responsibility of personalized news recommender
systems. Finally, we raise several research directions that are worth
investigating in future. This paper can provide up-to-date and comprehensive
views to help readers understand the personalized news recommendation field. We
hope this paper can facilitate research on personalized news recommendation and
as well as related fields in natural language processing and data mining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TUTA: Tree-based Transformers for Generally Structured Table Pre-training. (arXiv:2010.12537v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12537</id>
        <link href="http://arxiv.org/abs/2010.12537"/>
        <updated>2021-06-17T01:58:40.200Z</updated>
        <summary type="html"><![CDATA[Tables are widely used with various structures to organize and present data.
Recent attempts on table understanding mainly focus on relational tables, yet
overlook to other common table structures. In this paper, we propose TUTA, a
unified pre-training architecture for understanding generally structured
tables. Noticing that understanding a table requires spatial, hierarchical, and
semantic information, we enhance transformers with three novel structure-aware
mechanisms. First, we devise a unified tree-based structure, called a
bi-dimensional coordinate tree, to describe both the spatial and hierarchical
information of generally structured tables. Upon this, we propose tree-based
attention and position embedding to better capture the spatial and hierarchical
information. Moreover, we devise three progressive pre-training objectives to
enable representations at the token, cell, and table levels. We pre-train TUTA
on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two
critical tasks in the field of table structure understanding: cell type
classification and table type classification. Experiments show that TUTA is
highly effective, achieving state-of-the-art on five widely-studied datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiruo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Haoyu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ran Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhiyi Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Mappings: Generating Open-Ended Expressive Mappings Using Variational Autoencoders. (arXiv:2106.08867v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.08867</id>
        <link href="http://arxiv.org/abs/2106.08867"/>
        <updated>2021-06-17T01:58:40.177Z</updated>
        <summary type="html"><![CDATA[In many contexts, creating mappings for gestural interactions can form part
of an artistic process. Creators seeking a mapping that is expressive, novel,
and affords them a sense of authorship may not know how to program it up in a
signal processing patch. Tools like Wekinator and MIMIC allow creators to use
supervised machine learning to learn mappings from example input/output
pairings. However, a creator may know a good mapping when they encounter it yet
start with little sense of what the inputs or outputs should be. We call this
an open-ended mapping process. Addressing this need, we introduce the latent
mapping, which leverages the latent space of an unsupervised machine learning
algorithm such as a Variational Autoencoder trained on a corpus of unlabelled
gestural data from the creator. We illustrate it with Sonified Body, a system
mapping full-body movement to sound which we explore in a residency with three
dancers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murray_Browne_T/0/1/0/all/0/1"&gt;Tim Murray-Browne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tigas_P/0/1/0/all/0/1"&gt;Panagiotis Tigas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved CNN-based Learning of Interpolation Filters for Low-Complexity Inter Prediction in Video Coding. (arXiv:2106.08936v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08936</id>
        <link href="http://arxiv.org/abs/2106.08936"/>
        <updated>2021-06-17T01:58:40.151Z</updated>
        <summary type="html"><![CDATA[The versatility of recent machine learning approaches makes them ideal for
improvement of next generation video compression solutions. Unfortunately,
these approaches typically bring significant increases in computational
complexity and are difficult to interpret into explainable models, affecting
their potential for implementation within practical video coding applications.
This paper introduces a novel explainable neural network-based inter-prediction
scheme, to improve the interpolation of reference samples needed for fractional
precision motion compensation. The approach requires a single neural network to
be trained from which a full quarter-pixel interpolation filter set is derived,
as the network is easily interpretable due to its linear structure. A novel
training framework enables each network branch to resemble a specific
fractional shift. This practical solution makes it very efficient to use
alongside conventional video coding schemes. When implemented in the context of
the state-of-the-art Versatile Video Coding (VVC) test model, 0.77%, 1.27% and
2.25% BD-rate savings can be achieved on average for lower resolution sequences
under the random access, low-delay B and low-delay P configurations,
respectively, while the complexity of the learned interpolation schemes is
significantly reduced compared to the interpolation with full CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1"&gt;Luka Murn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blasi_S/0/1/0/all/0/1"&gt;Saverio Blasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smeaton_A/0/1/0/all/0/1"&gt;Alan F. Smeaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1"&gt;Marta Mrak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topology Distillation for Recommender System. (arXiv:2106.08700v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08700</id>
        <link href="http://arxiv.org/abs/2106.08700"/>
        <updated>2021-06-17T01:58:40.114Z</updated>
        <summary type="html"><![CDATA[Recommender Systems (RS) have employed knowledge distillation which is a
model compression technique training a compact student model with the knowledge
transferred from a pre-trained large teacher model. Recent work has shown that
transferring knowledge from the teacher's intermediate layer significantly
improves the recommendation quality of the student. However, they transfer the
knowledge of individual representation point-wise and thus have a limitation in
that primary information of RS lies in the relations in the representation
space. This paper proposes a new topology distillation approach that guides the
student by transferring the topological structure built upon the relations in
the teacher space. We first observe that simply making the student learn the
whole topological structure is not always effective and even degrades the
student's performance. We demonstrate that because the capacity of the student
is highly limited compared to that of the teacher, learning the whole
topological structure is daunting for the student. To address this issue, we
propose a novel method named Hierarchical Topology Distillation (HTD) which
distills the topology hierarchically to cope with the large capacity gap. Our
extensive experiments on real-world datasets show that the proposed method
significantly outperforms the state-of-the-art competitors. We also provide
in-depth analyses to ascertain the benefit of distilling the topology for RS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;SeongKu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Junyoung Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kweon_W/0/1/0/all/0/1"&gt;Wonbin Kweon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hwanjo Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FAIR: Fairness-Aware Information Retrieval Evaluation. (arXiv:2106.08527v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08527</id>
        <link href="http://arxiv.org/abs/2106.08527"/>
        <updated>2021-06-17T01:58:40.089Z</updated>
        <summary type="html"><![CDATA[With the emerging needs of creating fairness-aware solutions for search and
recommendation systems, a daunting challenge exists of evaluating such
solutions. While many of the traditional information retrieval (IR) metrics can
capture the relevance, diversity and novelty for the utility with respect to
users, they are not suitable for inferring whether the presented results are
fair from the perspective of responsible information exposure. On the other
hand, various fairness metrics have been proposed but they do not account for
the user utility or do not measure it adequately. To address this problem, we
propose a new metric called Fairness-Aware IR (FAIR). By unifying standard IR
metrics and fairness measures into an integrated metric, this metric offers a
new perspective for evaluating fairness-aware ranking results. Based on this
metric, we developed an effective ranking algorithm that jointly optimized user
utility and fairness. The experimental results showed that our FAIR metric
could highlight results with good user utility and fair information exposure.
We showed how FAIR related to existing metrics and demonstrated the
effectiveness of our FAIR-based algorithm. We believe our work opens up a new
direction of pursuing a computationally feasible metric for evaluating and
implementing the fairness-aware IR systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruoyuan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1"&gt;Chirag Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysing Dense Passage Retrieval for Multi-hop Question Answering. (arXiv:2106.08433v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08433</id>
        <link href="http://arxiv.org/abs/2106.08433"/>
        <updated>2021-06-17T01:58:40.067Z</updated>
        <summary type="html"><![CDATA[We analyse the performance of passage retrieval models in the presence of
complex (multi-hop) questions to provide a better understanding of how
retrieval systems behave when multiple hops of reasoning are needed. In simple
open-domain question answering (QA), dense passage retrieval has become one of
the standard approaches for retrieving the relevant passages to infer an
answer. Recently, dense passage retrieval also achieved state-of-the-art
results in multi-hop QA, where aggregating information from multiple documents
and reasoning over them is required. However, so far, the dense retrieval
models are not evaluated properly concerning the multi-hop nature of the
problem: models are typically evaluated by the end result of the retrieval
pipeline, which leaves unclear where their success lies. In this work, we
provide an in-depth evaluation of such models not only unveiling the reasons
behind their success but also their limitations. Moreover, we introduce a
hybrid (lexical and dense) retrieval approach that is highly competitive with
the state-of-the-art dense retrieval model, while requiring substantially less
computational resources. Furthermore, we also perform qualitative analysis to
better understand the challenges behind passage retrieval for multi-hop QA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sidiropoulos_G/0/1/0/all/0/1"&gt;Georgios Sidiropoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voskarides_N/0/1/0/all/0/1"&gt;Nikos Voskarides&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakulenko_S/0/1/0/all/0/1"&gt;Svitlana Vakulenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1"&gt;Evangelos Kanoulas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections. (arXiv:2106.08908v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08908</id>
        <link href="http://arxiv.org/abs/2106.08908"/>
        <updated>2021-06-17T01:58:40.029Z</updated>
        <summary type="html"><![CDATA[Question answering (QA) systems for large document collections typically use
pipelines that (i) retrieve possibly relevant documents, (ii) re-rank them,
(iii) rank paragraphs or other snippets of the top-ranked documents, and (iv)
select spans of the top-ranked snippets as exact answers. Pipelines are
conceptually simple, but errors propagate from one component to the next,
without later components being able to revise earlier decisions. We present an
architecture for joint document and snippet ranking, the two middle stages,
which leverages the intuition that relevant documents have good snippets and
good snippets come from relevant documents. The architecture is general and can
be used with any neural text relevance ranker. We experiment with two main
instantiations of the architecture, based on POSIT-DRMM (PDRMM) and a
BERT-based ranker. Experiments on biomedical data from BIOASQ show that our
joint models vastly outperform the pipelines in snippet retrieval, the main
goal for QA, with fewer trainable parameters, also remaining competitive in
document retrieval. Furthermore, our joint PDRMM-based model is competitive
with BERT-based models, despite using orders of magnitude fewer parameters.
These claims are also supported by human evaluation on two test batches of
BIOASQ. To test our key findings on another dataset, we modified the Natural
Questions dataset so that it can also be used for document and snippet
retrieval. Our joint PDRMM-based model again outperforms the corresponding
pipeline in snippet retrieval on the modified Natural Questions dataset, even
though it performs worse than the pipeline in document retrieval. We make our
code and the modified Natural Questions dataset publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pappas_D/0/1/0/all/0/1"&gt;Dimitris Pappas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1"&gt;Ion Androutsopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Latent Vector Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation. (arXiv:2106.08188v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08188</id>
        <link href="http://arxiv.org/abs/2106.08188"/>
        <updated>2021-06-16T01:21:12.922Z</updated>
        <summary type="html"><![CDATA[This paper addresses the domain shift problem for segmentation. As a
solution, we propose OLVA, a novel and lightweight unsupervised domain
adaptation method based on a Variational Auto-Encoder (VAE) and Optimal
Transport (OT) theory. Thanks to the VAE, our model learns a shared
cross-domain latent space that follows a normal distribution, which reduces the
domain shift. To guarantee valid segmentations, our shared latent space is
designed to model the shape rather than the intensity variations. We further
rely on an OT loss to match and align the remaining discrepancy between the two
domains in the latent space. We demonstrate OLVA's effectiveness for the
segmentation of multiple cardiac structures on the public Multi-Modality Whole
Heart Segmentation (MM-WHS) dataset, where the source domain consists of
annotated 3D MR images and the unlabelled target domain of 3D CTs. Our results
show remarkable improvements with an additional margin of 12.5\% dice score
over concurrent generative training approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chanti_D/0/1/0/all/0/1"&gt;Dawood Al Chanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mateus_D/0/1/0/all/0/1"&gt;Diana Mateus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-16T01:21:12.915Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-the-Air Decentralized Federated Learning. (arXiv:2106.08011v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.08011</id>
        <link href="http://arxiv.org/abs/2106.08011"/>
        <updated>2021-06-16T01:21:12.908Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider decentralized federated learning (FL) over
wireless networks, where over-the-air computation (AirComp) is adopted to
facilitate the local model consensus in a device-to-device (D2D) communication
manner. However, the AirComp-based consensus phase brings the additive noise in
each algorithm iterate and the consensus needs to be robust to wireless network
topology changes, which introduce a coupled and novel challenge of establishing
the convergence for wireless decentralized FL algorithm. To facilitate
consensus phase, we propose an AirComp-based DSGD with gradient tracking and
variance reduction (DSGT-VR) algorithm, where both precoding and decoding
strategies are developed for D2D communication. Furthermore, we prove that the
proposed algorithm converges linearly and establish the optimality gap for
strongly convex and smooth loss functions, taking into account the channel
fading and noise. The theoretical result shows that the additional error bound
in the optimality gap depends on the number of devices. Extensive simulations
verify the theoretical results and show that the proposed algorithm outperforms
other benchmark decentralized FL algorithms over wireless networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yandong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuanming Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis. (arXiv:2106.07049v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07049</id>
        <link href="http://arxiv.org/abs/2106.07049"/>
        <updated>2021-06-16T01:21:12.902Z</updated>
        <summary type="html"><![CDATA[In the last few years, deep learning classifiers have shown promising results
in image-based medical diagnosis. However, interpreting the outputs of these
models remains a challenge. In cancer diagnosis, interpretability can be
achieved by localizing the region of the input image responsible for the
output, i.e. the location of a lesion. Alternatively, segmentation or detection
models can be trained with pixel-wise annotations indicating the locations of
malignant lesions. Unfortunately, acquiring such labels is labor-intensive and
requires medical expertise. To overcome this difficulty, weakly-supervised
localization can be utilized. These methods allow neural network classifiers to
output saliency maps highlighting the regions of the input most relevant to the
classification task (e.g. malignant lesions in mammograms) using only
image-level labels (e.g. whether the patient has cancer or not) during
training. When applied to high-resolution images, existing methods produce
low-resolution saliency maps. This is problematic in applications in which
suspicious lesions are small in relation to the image size. In this work, we
introduce a novel neural network architecture to perform weakly-supervised
segmentation of high-resolution images. The proposed model selects regions of
interest via coarse-level localization, and then performs fine-grained
segmentation of those regions. We apply this model to breast cancer diagnosis
with screening mammography, and validate it on a large clinically-realistic
dataset. Measured by Dice similarity score, our approach outperforms existing
methods by a large margin in terms of localization performance of benign and
malignant lesions, relatively improving the performance by 39.6% and 20.0%,
respectively. Code and the weights of some of the models are available at
https://github.com/nyukat/GLAM]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kangning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yiqiu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1"&gt;Nan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1"&gt;Jakub Ch&amp;#x142;&amp;#x119;dowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1"&gt;Carlos Fernandez-Granda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1"&gt;Krzysztof J. Geras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Reinforcement Learning from Demonstrations. (arXiv:2106.08050v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08050</id>
        <link href="http://arxiv.org/abs/2106.08050"/>
        <updated>2021-06-16T01:21:12.895Z</updated>
        <summary type="html"><![CDATA[Residual reinforcement learning (RL) has been proposed as a way to solve
challenging robotic tasks by adapting control actions from a conventional
feedback controller to maximize a reward signal. We extend the residual
formulation to learn from visual inputs and sparse rewards using
demonstrations. Learning from images, proprioceptive inputs and a sparse
task-completion reward relaxes the requirement of accessing full state
features, such as object and target positions. In addition, replacing the base
controller with a policy learned from demonstrations removes the dependency on
a hand-engineered controller in favour of a dataset of demonstrations, which
can be provided by non-experts. Our experimental evaluation on simulated
manipulation tasks on a 6-DoF UR5 arm and a 28-DoF dexterous hand demonstrates
that residual RL from demonstrations is able to generalize to unseen
environment conditions more flexibly than either behavioral cloning or RL
fine-tuning, and is capable of solving high-dimensional, sparse-reward tasks
out of reach for RL from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alakuijala_M/0/1/0/all/0/1"&gt;Minttu Alakuijala&lt;/a&gt; (WILLOW, Thoth), &lt;a href="http://arxiv.org/find/cs/1/au:+Dulac_Arnold_G/0/1/0/all/0/1"&gt;Gabriel Dulac-Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1"&gt;Julien Mairal&lt;/a&gt; (Thoth), &lt;a href="http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1"&gt;Jean Ponce&lt;/a&gt; (WILLOW), &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1"&gt;Cordelia Schmid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascading Convolutional Temporal Colour Constancy. (arXiv:2106.07955v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07955</id>
        <link href="http://arxiv.org/abs/2106.07955"/>
        <updated>2021-06-16T01:21:12.878Z</updated>
        <summary type="html"><![CDATA[Computational Colour Constancy (CCC) consists of estimating the colour of one
or more illuminants in a scene and using them to remove unwanted chromatic
distortions. Much research has focused on illuminant estimation for CCC on
single images, with few attempts of leveraging the temporal information
intrinsic in sequences of correlated images (e.g., the frames in a video), a
task known as Temporal Colour Constancy (TCC). The state-of-the-art for TCC is
TCCNet, a deep-learning architecture that uses a ConvLSTM for aggregating the
encodings produced by CNN submodules for each image in a sequence. We extend
this architecture with different models obtained by (i) substituting the TCCNet
submodules with C4, the state-of-the-art method for CCC targeting images; (ii)
adding a cascading strategy to perform an iterative improvement of the estimate
of the illuminant. We tested our models on the recently released TCC benchmark
and achieved results that surpass the state-of-the-art. Analyzing the impact of
the number of frames involved in illuminant estimation on performance, we show
that it is possible to reduce inference time by training the models on few
selected frames from the sequences while retaining comparable accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rizzo_M/0/1/0/all/0/1"&gt;Matteo Rizzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conati_C/0/1/0/all/0/1"&gt;Cristina Conati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1"&gt;Daesik Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hui Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Combinatorial Node Labeling Algorithms. (arXiv:2106.03594v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03594</id>
        <link href="http://arxiv.org/abs/2106.03594"/>
        <updated>2021-06-16T01:21:12.871Z</updated>
        <summary type="html"><![CDATA[We present a graph neural network to learn graph coloring heuristics using
reinforcement learning. Our learned deterministic heuristics give better
solutions than classical degree-based greedy heuristics and only take seconds
to evaluate on graphs with tens of thousands of vertices. As our approach is
based on policy-gradients, it also learns a probabilistic policy as well. These
probabilistic policies outperform all greedy coloring baselines and a machine
learning baseline. Our approach generalizes several previous machine-learning
frameworks, which applied to problems like minimum vertex cover. We also
demonstrate that our approach outperforms two greedy heuristics on minimum
vertex cover.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gianinazzi_L/0/1/0/all/0/1"&gt;Lukas Gianinazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fries_M/0/1/0/all/0/1"&gt;Maximilian Fries&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dryden_N/0/1/0/all/0/1"&gt;Nikoli Dryden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Nun_T/0/1/0/all/0/1"&gt;Tal Ben-Nun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besta_M/0/1/0/all/0/1"&gt;Maciej Besta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1"&gt;Torsten Hoefler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Fair Averaging. (arXiv:2104.14937v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14937</id>
        <link href="http://arxiv.org/abs/2104.14937"/>
        <updated>2021-06-16T01:21:12.865Z</updated>
        <summary type="html"><![CDATA[Fairness has emerged as a critical problem in federated learning (FL). In
this work, we identify a cause of unfairness in FL -- conflicting gradients
with large differences in the magnitudes. To address this issue, we propose the
federated fair averaging (FedFV) algorithm to mitigate potential conflicts
among clients before averaging their gradients. We first use the cosine
similarity to detect gradient conflicts, and then iteratively eliminate such
conflicts by modifying both the direction and the magnitude of the gradients.
We further show the theoretical foundation of FedFV to mitigate the issue
conflicting gradients and converge to Pareto stationary solutions. Extensive
experiments on a suite of federated datasets confirm that FedFV compares
favorably against state-of-the-art methods in terms of fairness, accuracy and
efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiaoliang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jianzhong Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1"&gt;Chenglu Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Rongshan Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Assessment of Federated Learning using Private Personalized Layers. (arXiv:2106.08060v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.08060</id>
        <link href="http://arxiv.org/abs/2106.08060"/>
        <updated>2021-06-16T01:21:12.859Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a collaborative scheme to train a learning model
across multiple participants without sharing data. While FL is a clear step
forward towards enforcing users' privacy, different inference attacks have been
developed. In this paper, we quantify the utility and privacy trade-off of a FL
scheme using private personalized layers. While this scheme has been proposed
as local adaptation to improve the accuracy of the model through local
personalization, it has also the advantage to minimize the information about
the model exchanged with the server. However, the privacy of such a scheme has
never been quantified. Our evaluations using motion sensor dataset show that
personalized layers speed up the convergence of the model and slightly improve
the accuracy for all users compared to a standard FL scheme while better
preventing both attribute and membership inferences compared to a FL scheme
using local differential privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jourdan_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Jourdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boutet_A/0/1/0/all/0/1"&gt;Antoine Boutet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frindel_C/0/1/0/all/0/1"&gt;Carole Frindel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Transformers. (arXiv:2106.04554v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04554</id>
        <link href="http://arxiv.org/abs/2106.04554"/>
        <updated>2021-06-16T01:21:12.833Z</updated>
        <summary type="html"><![CDATA[Transformers have achieved great success in many artificial intelligence
fields, such as natural language processing, computer vision, and audio
processing. Therefore, it is natural to attract lots of interest from academic
and industry researchers. Up to the present, a great variety of Transformer
variants (a.k.a. X-formers) have been proposed, however, a systematic and
comprehensive literature review on these Transformer variants is still missing.
In this survey, we provide a comprehensive review of various X-formers. We
first briefly introduce the vanilla Transformer and then propose a new taxonomy
of X-formers. Next, we introduce the various X-formers from three perspectives:
architectural modification, pre-training, and applications. Finally, we outline
some potential directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Characterization of Fair Machine Learning For Clinical Risk Prediction. (arXiv:2007.10306v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10306</id>
        <link href="http://arxiv.org/abs/2007.10306"/>
        <updated>2021-06-16T01:21:12.815Z</updated>
        <summary type="html"><![CDATA[The use of machine learning to guide clinical decision making has the
potential to worsen existing health disparities. Several recent works frame the
problem as that of algorithmic fairness, a framework that has attracted
considerable attention and criticism. However, the appropriateness of this
framework is unclear due to both ethical as well as technical considerations,
the latter of which include trade-offs between measures of fairness and model
performance that are not well-understood for predictive models of clinical
outcomes. To inform the ongoing debate, we conduct an empirical study to
characterize the impact of penalizing group fairness violations on an array of
measures of model performance and group fairness. We repeat the analyses across
multiple observational healthcare databases, clinical outcomes, and sensitive
attributes. We find that procedures that penalize differences between the
distributions of predictions across groups induce nearly-universal degradation
of multiple performance metrics within groups. On examining the secondary
impact of these procedures, we observe heterogeneity of the effect of these
procedures on measures of fairness in calibration and ranking across
experimental conditions. Beyond the reported trade-offs, we emphasize that
analyses of algorithmic fairness in healthcare lack the contextual grounding
and causal awareness necessary to reason about the mechanisms that lead to
health disparities, as well as about the potential of algorithmic fairness
methods to counteract those mechanisms. In light of these limitations, we
encourage researchers building predictive models for clinical use to step
outside the algorithmic fairness frame and engage critically with the broader
sociotechnical context surrounding the use of machine learning in healthcare.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pfohl_S/0/1/0/all/0/1"&gt;Stephen R. Pfohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Foryciarz_A/0/1/0/all/0/1"&gt;Agata Foryciarz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shah_N/0/1/0/all/0/1"&gt;Nigam H. Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Mixture of Posteriors for Counterfactual Inference, Data Integration and Fairness. (arXiv:2106.08161v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08161</id>
        <link href="http://arxiv.org/abs/2106.08161"/>
        <updated>2021-06-16T01:21:12.807Z</updated>
        <summary type="html"><![CDATA[Learning meaningful representations of data that can address challenges such
as batch effect correction, data integration and counterfactual inference is a
central problem in many domains including computational biology. Adopting a
Conditional VAE framework, we identify the mathematical principle that unites
these challenges: learning a representation that is marginally independent of a
condition variable. We therefore propose the Contrastive Mixture of Posteriors
(CoMP) method that uses a novel misalignment penalty to enforce this
independence. This penalty is defined in terms of mixtures of the variational
posteriors themselves, unlike prior work which uses external discrepancy
measures such as MMD to ensure independence in latent space. We show that CoMP
has attractive theoretical properties compared to previous approaches,
especially when there is complex global structure in latent space. We further
demonstrate state of the art performance on a number of real-world problems,
including the challenging tasks of aligning human tumour samples with cancer
cell-lines and performing counterfactual inference on single-cell RNA
sequencing data. Incidentally, we find parallels with the fair representation
learning literature, and demonstrate CoMP has competitive performance in
learning fair yet expressive latent representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1"&gt;Adam Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vezer_A/0/1/0/all/0/1"&gt;&amp;#xc1;rpi Vez&amp;#xe9;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Glastonbury_C/0/1/0/all/0/1"&gt;Craig A Glastonbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Creed_P/0/1/0/all/0/1"&gt;P&amp;#xe1;id&amp;#xed; Creed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Abujudeh_S/0/1/0/all/0/1"&gt;Sam Abujudeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sim_A/0/1/0/all/0/1"&gt;Aaron Sim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07306</id>
        <link href="http://arxiv.org/abs/2106.07306"/>
        <updated>2021-06-16T01:21:12.800Z</updated>
        <summary type="html"><![CDATA[In structured prediction, a major challenge for models is to represent the
interdependencies within their output structures. For the common case where
outputs are structured as a sequence, linear-chain conditional random fields
(CRFs) are a widely used model class which can learn local dependencies in
output sequences. However, the CRF's Markov assumption makes it impossible for
these models to capture nonlocal dependencies, and standard CRFs are unable to
respect nonlocal constraints of the data (such as global arity constraints on
output labels). We present a generalization of CRFs that can enforce a broad
class of constraints, including nonlocal ones, by specifying the space of
possible output structures as a regular language $\mathcal{L}$. The resulting
regular-constrained CRF (RegCCRF) has the same formal properties as a standard
CRF, but assigns zero probability to all label sequences not in $\mathcal{L}$.
Notably, RegCCRFs can incorporate their constraints during training, while
related models only enforce constraints during decoding. We prove that
constrained training is never worse than constrained decoding, and show using
synthetic data that it can be substantially better in practice. Additionally,
we demonstrate a practical benefit on downstream tasks by incorporating a
RegCCRF into a deep neural model for semantic role labeling, exceeding
state-of-the-art results on a standard dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papay_S/0/1/0/all/0/1"&gt;Sean Papay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1"&gt;Roman Klinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1"&gt;Sebastian Pad&amp;#xf3;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal consistency of Wasserstein $k$-NN classifier. (arXiv:2009.04651v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04651</id>
        <link href="http://arxiv.org/abs/2009.04651"/>
        <updated>2021-06-16T01:21:12.794Z</updated>
        <summary type="html"><![CDATA[The Wasserstein distance provides a notion of dissimilarities between
probability measures, which has recent applications in learning of structured
data with varying size such as images and text documents. In this work, we
analyze the $k$-nearest neighbor classifier ($k$-NN) under the Wasserstein
distance and establish the universal consistency on families of distributions.
Using previous known results on the consistency of the $k$-NN classifier on
infinite dimensional metric spaces, it suffices to show that the families is a
countable union of finite dimension sets. As a result, we show that the $k$-NN
classifier is universally consistent on spaces of finitely supported measures,
the space of Gaussian measures, and the space of measures with finite wavelet
densities. In addition, we give a counterexample to show that the universal
consistency does not hold on $\mathcal{W}_p((0,1))$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ponnoprat_D/0/1/0/all/0/1"&gt;Donlapark Ponnoprat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A baseline for semi-supervised learning of efficient semantic segmentation models. (arXiv:2106.07075v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07075</id>
        <link href="http://arxiv.org/abs/2106.07075"/>
        <updated>2021-06-16T01:21:12.771Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning is especially interesting in the dense prediction
context due to high cost of pixel-level ground truth. Unfortunately, most such
approaches are evaluated on outdated architectures which hamper research due to
very slow training and high requirements on GPU RAM. We address this concern by
presenting a simple and effective baseline which works very well both on
standard and efficient architectures. Our baseline is based on one-way
consistency and non-linear geometric and photometric perturbations. We show
advantage of perturbing only the student branch and present a plausible
explanation of such behaviour. Experiments on Cityscapes and CIFAR-10
demonstrate competitive performance with respect to prior work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1"&gt;Ivan Grubi&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orsic_M/0/1/0/all/0/1"&gt;Marin Or&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1"&gt;Sini&amp;#x161;a &amp;#x160;egvi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Epidemic modelling of multiple virus strains:a case study of SARS-CoV-2 B.1.1.7 in Moscow. (arXiv:2106.08048v1 [q-bio.PE])]]></title>
        <id>http://arxiv.org/abs/2106.08048</id>
        <link href="http://arxiv.org/abs/2106.08048"/>
        <updated>2021-06-16T01:21:12.744Z</updated>
        <summary type="html"><![CDATA[During a long-running pandemic a pathogen can mutate, producing new strains
with different epidemiological parameters. Existing approaches to epidemic
modelling only consider one virus strain. We have developed a modified SEIR
model to simulate multiple virus strains within the same population. As a case
study, we investigate the potential effects of SARS-CoV-2 strain B.1.1.7 on the
city of Moscow. Our analysis indicates a high risk of a new wave of infections
in September-October 2021 with up to 35 000 daily infections at peak. We
open-source our code and data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Tseytlin_B/0/1/0/all/0/1"&gt;Boris Tseytlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Makarov_I/0/1/0/all/0/1"&gt;Ilya Makarov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PRANK: motion Prediction based on RANKing. (arXiv:2010.12007v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12007</id>
        <link href="http://arxiv.org/abs/2010.12007"/>
        <updated>2021-06-16T01:21:12.602Z</updated>
        <summary type="html"><![CDATA[Predicting the motion of agents such as pedestrians or human-driven vehicles
is one of the most critical problems in the autonomous driving domain. The
overall safety of driving and the comfort of a passenger directly depend on its
successful solution. The motion prediction problem also remains one of the most
challenging problems in autonomous driving engineering, mainly due to high
variance of the possible agent's future behavior given a situation. The two
phenomena responsible for the said variance are the multimodality caused by the
uncertainty of the agent's intent (e.g., turn right or move forward) and
uncertainty in the realization of a given intent (e.g., which lane to turn
into). To be useful within a real-time autonomous driving pipeline, a motion
prediction system must provide efficient ways to describe and quantify this
uncertainty, such as computing posterior modes and their probabilities or
estimating density at the point corresponding to a given trajectory. It also
should not put substantial density on physically impossible trajectories, as
they can confuse the system processing the predictions. In this paper, we
introduce the PRANK method, which satisfies these requirements. PRANK takes
rasterized bird-eye images of agent's surroundings as an input and extracts
features of the scene with a convolutional neural network. It then produces the
conditional distribution of agent's trajectories plausible in the given scene.
The key contribution of PRANK is a way to represent that distribution using
nearest-neighbor methods in latent trajectory space, which allows for efficient
inference in real time. We evaluate PRANK on the in-house and Argoverse
datasets, where it shows competitive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biktairov_Y/0/1/0/all/0/1"&gt;Yuriy Biktairov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stebelev_M/0/1/0/all/0/1"&gt;Maxim Stebelev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudenko_I/0/1/0/all/0/1"&gt;Irina Rudenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shliazhko_O/0/1/0/all/0/1"&gt;Oleh Shliazhko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yangel_B/0/1/0/all/0/1"&gt;Boris Yangel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Beyond Classification Accuracy Metrics in Model Compression. (arXiv:2012.01604v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01604</id>
        <link href="http://arxiv.org/abs/2012.01604"/>
        <updated>2021-06-16T01:21:12.595Z</updated>
        <summary type="html"><![CDATA[With the rise in edge-computing devices, there has been an increasing demand
to deploy energy and resource-efficient models. A large body of research has
been devoted to developing methods that can reduce the size of the model
considerably without affecting the standard metrics such as top-1 accuracy.
However, these pruning approaches tend to result in a significant mismatch in
other metrics such as fairness across classes and explainability. To combat
such misalignment, we propose a novel multi-part loss function inspired by the
knowledge-distillation literature. Through extensive experiments, we
demonstrate the effectiveness of our approach across different compression
algorithms, architectures, tasks as well as datasets. In particular, we obtain
up to $4.1\times$ reduction in the number of prediction mismatches between the
compressed and reference models, and up to $5.7\times$ in cases where the
reference model makes the correct prediction; all while making no changes to
the compression algorithm, and minor modifications to the loss function.
Furthermore, we demonstrate how inducing simple alignment between the
predictions of the models naturally improves the alignment on other metrics
including fairness and attributions. Our framework can thus serve as a simple
plug-and-play component for compression algorithms in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_V/0/1/0/all/0/1"&gt;Vinu Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1"&gt;Shoaib Ahmed Siddiqui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhaskara_A/0/1/0/all/0/1"&gt;Aditya Bhaskara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muralidharan_S/0/1/0/all/0/1"&gt;Saurav Muralidharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garland_M/0/1/0/all/0/1"&gt;Michael Garland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sheraz Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1"&gt;Andreas Dengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploration in Online Advertising Systems with Deep Uncertainty-Aware Learning. (arXiv:2012.02298v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02298</id>
        <link href="http://arxiv.org/abs/2012.02298"/>
        <updated>2021-06-16T01:21:12.587Z</updated>
        <summary type="html"><![CDATA[Modern online advertising systems inevitably rely on personalization methods,
such as click-through rate (CTR) prediction. Recent progress in CTR prediction
enjoys the rich representation capabilities of deep learning and achieves great
success in large-scale industrial applications. However, these methods can
suffer from lack of exploration. Another line of prior work addresses the
exploration-exploitation trade-off problem with contextual bandit methods,
which are recently less studied in the industry due to the difficulty in
extending their flexibility with deep models. In this paper, we propose a novel
Deep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on
Gaussian processes, which can provide predictive uncertainty estimations while
maintaining the flexibility of deep neural networks. DUAL can be easily
implemented on existing models and deployed in real-time systems with minimal
extra computational overhead. By linking the predictive uncertainty estimation
ability of DUAL to well-known bandit algorithms, we further present DUAL-based
Ad-ranking strategies to boost up long-term utilities such as the social
welfare in advertising systems. Experimental results on several public datasets
demonstrate the effectiveness of our methods. Remarkably, an online A/B test
deployed in the Alibaba display advertising platform shows an 8.2% social
welfare improvement and an 8.0% revenue lift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1"&gt;Chao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhifeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1"&gt;Shuo Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lining Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1"&gt;Yifan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1"&gt;Kun Gai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kuang-chih Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Optimization Methods for Extreme Similarity Learning with Nonlinear Embeddings. (arXiv:2010.13511v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13511</id>
        <link href="http://arxiv.org/abs/2010.13511"/>
        <updated>2021-06-16T01:21:12.532Z</updated>
        <summary type="html"><![CDATA[We study the problem of learning similarity by using nonlinear embedding
models (e.g., neural networks) from all possible pairs. This problem is
well-known for its difficulty of training with the extreme number of pairs. For
the special case of using linear embeddings, many studies have addressed this
issue of handling all pairs by considering certain loss functions and
developing efficient optimization algorithms. This paper aims to extend results
for general nonlinear embeddings. First, we finish detailed derivations and
provide clean formulations for efficiently calculating some building blocks of
optimization algorithms such as function, gradient evaluation, and
Hessian-vector product. The result enables the use of many optimization methods
for extreme similarity learning with nonlinear embeddings. Second, we study
some optimization methods in detail. Due to the use of nonlinear embeddings,
implementation issues different from linear cases are addressed. In the end,
some methods are shown to be highly efficient for extreme similarity learning
with nonlinear embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yuan_B/0/1/0/all/0/1"&gt;Bowen Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu-Sheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Quan_P/0/1/0/all/0/1"&gt;Pengrui Quan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chih-Jen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Regret Bounds for Online Submodular Maximization. (arXiv:2106.07836v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07836</id>
        <link href="http://arxiv.org/abs/2106.07836"/>
        <updated>2021-06-16T01:21:11.497Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider an online optimization problem over $T$ rounds
where at each step $t\in[T]$, the algorithm chooses an action $x_t$ from the
fixed convex and compact domain set $\mathcal{K}$. A utility function
$f_t(\cdot)$ is then revealed and the algorithm receives the payoff $f_t(x_t)$.
This problem has been previously studied under the assumption that the
utilities are adversarially chosen monotone DR-submodular functions and
$\mathcal{O}(\sqrt{T})$ regret bounds have been derived. We first characterize
the class of strongly DR-submodular functions and then, we derive regret bounds
for the following new online settings: $(1)$ $\{f_t\}_{t=1}^T$ are monotone
strongly DR-submodular and chosen adversarially, $(2)$ $\{f_t\}_{t=1}^T$ are
monotone submodular (while the average $\frac{1}{T}\sum_{t=1}^T f_t$ is
strongly DR-submodular) and chosen by an adversary but they arrive in a
uniformly random order, $(3)$ $\{f_t\}_{t=1}^T$ are drawn i.i.d. from some
unknown distribution $f_t\sim \mathcal{D}$ where the expected function
$f(\cdot)=\mathbb{E}_{f_t\sim\mathcal{D}}[f_t(\cdot)]$ is monotone
DR-submodular. For $(1)$, we obtain the first logarithmic regret bounds. In
terms of the second framework, we show that it is possible to obtain similar
logarithmic bounds with high probability. Finally, for the i.i.d. model, we
provide algorithms with $\tilde{\mathcal{O}}(\sqrt{T})$ stochastic regret
bound, both in expectation and with high probability. Experimental results
demonstrate that our algorithms outperform the previous techniques in the
aforementioned three settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadeghi_O/0/1/0/all/0/1"&gt;Omid Sadeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raut_P/0/1/0/all/0/1"&gt;Prasanna Raut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazel_M/0/1/0/all/0/1"&gt;Maryam Fazel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsuitability of NOTEARS for Causal Graph Discovery. (arXiv:2104.05441v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05441</id>
        <link href="http://arxiv.org/abs/2104.05441"/>
        <updated>2021-06-16T01:21:11.489Z</updated>
        <summary type="html"><![CDATA[Causal Discovery methods aim to identify a DAG structure that represents
causal relationships from observational data. In this article, we stress that
it is important to test such methods for robustness in practical settings. As
our main example, we analyze the NOTEARS method, for which we demonstrate a
lack of scale-invariance. We show that NOTEARS is a method that aims to
identify a parsimonious DAG from the data that explains the residual variance.
We conclude that NOTEARS is not suitable for identifying truly causal
relationships from the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kaiser_M/0/1/0/all/0/1"&gt;Marcus Kaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sipos_M/0/1/0/all/0/1"&gt;Maksim Sipos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exponential Reduction in Sample Complexity with Learning of Ising Model Dynamics. (arXiv:2104.00995v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00995</id>
        <link href="http://arxiv.org/abs/2104.00995"/>
        <updated>2021-06-16T01:21:11.483Z</updated>
        <summary type="html"><![CDATA[The usual setting for learning the structure and parameters of a graphical
model assumes the availability of independent samples produced from the
corresponding multivariate probability distribution. However, for many models
the mixing time of the respective Markov chain can be very large and i.i.d.
samples may not be obtained. We study the problem of reconstructing binary
graphical models from correlated samples produced by a dynamical process, which
is natural in many applications. We analyze the sample complexity of two
estimators that are based on the interaction screening objective and the
conditional likelihood loss. We observe that for samples coming from a
dynamical process far from equilibrium, the sample complexity reduces
exponentially compared to a dynamical process that mixes quickly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutt_A/0/1/0/all/0/1"&gt;Arkopal Dutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lokhov_A/0/1/0/all/0/1"&gt;Andrey Y. Lokhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vuffray_M/0/1/0/all/0/1"&gt;Marc Vuffray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_S/0/1/0/all/0/1"&gt;Sidhant Misra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Program Synthesis for Images By Sampling Without Replacement. (arXiv:2001.10119v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10119</id>
        <link href="http://arxiv.org/abs/2001.10119"/>
        <updated>2021-06-16T01:21:11.476Z</updated>
        <summary type="html"><![CDATA[Program synthesis has emerged as a successful approach to the image parsing
task. Most prior works rely on a two-step scheme involving supervised
pretraining of a Seq2Seq model with synthetic programs followed by
reinforcement learning (RL) for fine-tuning with real reference images. Fully
unsupervised approaches promise to train the model directly on the target
images without requiring curated pretraining datasets. However, they struggle
with the inherent sparsity of meaningful programs in the search space. In this
paper, we present the first unsupervised algorithm capable of parsing
constructive solid geometry (CSG) images into context-free grammar (CFG)
without pretraining via non-differentiable renderer. To tackle the
\emph{non-Markovian} sparse reward problem, we combine three key ingredients --
(i) a grammar-encoded tree LSTM ensuring program validity (ii) entropy
regularization and (iii) sampling without replacement from the CFG syntax tree.
Empirically, our algorithm recovers meaningful programs in large search spaces
(up to $3.8 \times 10^{28}$). Further, even though our approach is fully
unsupervised, it generalizes better than supervised methods on the synthetic 2D
CSG dataset. On the 2D computer aided design (CAD) dataset, our approach
significantly outperforms the supervised pretrained model and is competitive to
the refined model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chenghui Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1"&gt;Barnabas Poczos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-efficient Hindsight Off-policy Option Learning. (arXiv:2007.15588v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15588</id>
        <link href="http://arxiv.org/abs/2007.15588"/>
        <updated>2021-06-16T01:21:11.448Z</updated>
        <summary type="html"><![CDATA[We introduce Hindsight Off-policy Options (HO2), a data-efficient option
learning algorithm. Given any trajectory, HO2 infers likely option choices and
backpropagates through the dynamic programming inference procedure to robustly
train all policy components off-policy and end-to-end. The approach outperforms
existing option learning methods on common benchmarks. To better understand the
option framework and disentangle benefits from both temporal and action
abstraction, we evaluate ablations with flat policies and mixture policies with
comparable optimization. The results highlight the importance of both types of
abstraction as well as off-policy training and trust-region constraints,
particularly in challenging, simulated 3D robot manipulation tasks from raw
pixel inputs. Finally, we intuitively adapt the inference step to investigate
the effect of increased temporal abstraction on training with pre-trained
options and from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wulfmeier_M/0/1/0/all/0/1"&gt;Markus Wulfmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1"&gt;Dushyant Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hafner_R/0/1/0/all/0/1"&gt;Roland Hafner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampe_T/0/1/0/all/0/1"&gt;Thomas Lampe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdolmaleki_A/0/1/0/all/0/1"&gt;Abbas Abdolmaleki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hertweck_T/0/1/0/all/0/1"&gt;Tim Hertweck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neunert_M/0/1/0/all/0/1"&gt;Michael Neunert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tirumala_D/0/1/0/all/0/1"&gt;Dhruva Tirumala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siegel_N/0/1/0/all/0/1"&gt;Noah Siegel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1"&gt;Nicolas Heess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1"&gt;Martin Riedmiller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcomplete Representations Against Adversarial Videos. (arXiv:2012.04262v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04262</id>
        <link href="http://arxiv.org/abs/2012.04262"/>
        <updated>2021-06-16T01:21:11.441Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness of deep neural networks is an extensively studied
problem in the literature and various methods have been proposed to defend
against adversarial images. However, only a handful of defense methods have
been developed for defending against attacked videos. In this paper, we propose
a novel Over-and-Under complete restoration network for Defending against
adversarial videos (OUDefend). Most restoration networks adopt an
encoder-decoder architecture that first shrinks spatial dimension then expands
it back. This approach learns undercomplete representations, which have large
receptive fields to collect global information but overlooks local details. On
the other hand, overcomplete representations have opposite properties. Hence,
OUDefend is designed to balance local and global features by learning those two
representations. We attach OUDefend to target video recognition models as a
feature restoration block and train the entire network end-to-end. Experimental
results show that the defenses focusing on images may be ineffective to videos,
while OUDefend enhances robustness against different types of adversarial
videos, ranging from additive attacks, multiplicative attacks to physically
realizable attacks. Code: https://github.com/shaoyuanlo/OUDefend]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shao-Yuan Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1"&gt;Jeya Maria Jose Valanarasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modern Hopfield Networks for Few- and Zero-Shot Reaction Template Prediction. (arXiv:2104.03279v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03279</id>
        <link href="http://arxiv.org/abs/2104.03279"/>
        <updated>2021-06-16T01:21:11.434Z</updated>
        <summary type="html"><![CDATA[Finding synthesis routes for molecules of interest is an essential step in
the discovery of new drugs and materials. To find such routes,
computer-assisted synthesis planning (CASP) methods are employed which rely on
a model of chemical reactivity. In this study, we model single-step
retrosynthesis in a template-based approach using modern Hopfield networks
(MHNs). We adapt MHNs to associate different modalities, reaction templates and
molecules, which allows the model to leverage structural information about
reaction templates. This approach significantly improves the performance of
template relevance prediction, especially for templates with few or zero
training examples. With inference speed several times faster than that of
baseline methods, we improve predictive performance for top-k exact match
accuracy for $\mathrm{k}\geq5$ in the retrosynthesis benchmark USPTO-50k.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seidl_P/0/1/0/all/0/1"&gt;Philipp Seidl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renz_P/0/1/0/all/0/1"&gt;Philipp Renz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyubankova_N/0/1/0/all/0/1"&gt;Natalia Dyubankova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neves_P/0/1/0/all/0/1"&gt;Paulo Neves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verhoeven_J/0/1/0/all/0/1"&gt;Jonas Verhoeven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segler_M/0/1/0/all/0/1"&gt;Marwin Segler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg K. Wegner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1"&gt;Sepp Hochreiter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klambauer_G/0/1/0/all/0/1"&gt;G&amp;#xfc;nter Klambauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothness Analysis of Adversarial Training. (arXiv:2103.01400v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01400</id>
        <link href="http://arxiv.org/abs/2103.01400"/>
        <updated>2021-06-16T01:21:11.427Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are vulnerable to adversarial attacks. Recent studies
about adversarial robustness focus on the loss landscape in the parameter space
since it is related to optimization and generalization performance. These
studies conclude that the difficulty of adversarial training is caused by the
non-smoothness of the loss function: i.e., its gradient is not Lipschitz
continuous. However, this analysis ignores the dependence of adversarial
attacks on model parameters. Since adversarial attacks are optimized for
models, they should depend on the parameters. Considering this dependence, we
analyze the smoothness of the loss function of adversarial training using the
optimal attacks for the model parameter in more detail. We reveal that the
constraint of adversarial attacks is one cause of the non-smoothness and that
the smoothness depends on the types of the constraints. Specifically, the
$L_\infty$ constraint can cause non-smoothness more than the $L_2$ constraint.
Moreover, our analysis implies that if we flatten the loss function with
respect to input data, the Lipschitz constant of the gradient of adversarial
loss tends to increase. To address the non-smoothness, we show that EntropySGD
smoothens the non-smooth loss and improves the performance of adversarial
training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1"&gt;Masanori Yamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_H/0/1/0/all/0/1"&gt;Hiroshi Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamanaka_Y/0/1/0/all/0/1"&gt;Yuki Yamanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ida_Y/0/1/0/all/0/1"&gt;Yasutoshi Ida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composing Normalizing Flows for Inverse Problems. (arXiv:2002.11743v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.11743</id>
        <link href="http://arxiv.org/abs/2002.11743"/>
        <updated>2021-06-16T01:21:11.421Z</updated>
        <summary type="html"><![CDATA[Given an inverse problem with a normalizing flow prior, we wish to estimate
the distribution of the underlying signal conditioned on the observations. We
approach this problem as a task of conditional inference on the pre-trained
unconditional flow model. We first establish that this is computationally hard
for a large class of flow models. Motivated by this, we propose a framework for
approximate inference that estimates the target conditional as a composition of
two flow models. This formulation leads to a stable variational inference
training procedure that avoids adversarial training. Our method is evaluated on
a variety of inverse problems and is shown to produce high-quality samples with
uncertainty quantification. We further demonstrate that our approach can be
amortized for zero-shot inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Whang_J/0/1/0/all/0/1"&gt;Jay Whang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lindgren_E/0/1/0/all/0/1"&gt;Erik M. Lindgren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amortized Probabilistic Detection of Communities in Graphs. (arXiv:2010.15727v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15727</id>
        <link href="http://arxiv.org/abs/2010.15727"/>
        <updated>2021-06-16T01:21:11.383Z</updated>
        <summary type="html"><![CDATA[Learning community structures in graphs has broad applications across
scientific domains. While graph neural networks (GNNs) have been successful in
encoding graph structures, existing GNN-based methods for community detection
are limited by requiring knowledge of the number of communities in advance, in
addition to lacking a proper probabilistic formulation to handle uncertainty.
We propose a simple framework for amortized community detection, which
addresses both of these issues by combining the expressive power of GNNs with
recent methods for amortized clustering. Our models consist of a graph
representation backbone that extracts structural information and an amortized
clustering network that naturally handles variable numbers of clusters. Both
components combine into well-defined models of the posterior distribution of
graph communities and are jointly optimized given labeled graphs. At inference
time, the models yield parallel samples from the posterior of community labels,
quantifying uncertainty in a principled way. We evaluate several models from
our framework on synthetic and real datasets and demonstrate superior
performance to previous methods. As a separate contribution, we extend recent
amortized probabilistic clustering architectures by adding attention modules,
which yield further improvements on community detection tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yueqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yoonho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Basu_P/0/1/0/all/0/1"&gt;Pallab Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1"&gt;Juho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Paninski_L/0/1/0/all/0/1"&gt;Liam Paninski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pakman_A/0/1/0/all/0/1"&gt;Ari Pakman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bringing Differential Private SGD to Practice: On the Independence of Gaussian Noise and the Number of Training Rounds. (arXiv:2102.09030v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09030</id>
        <link href="http://arxiv.org/abs/2102.09030"/>
        <updated>2021-06-16T01:21:11.364Z</updated>
        <summary type="html"><![CDATA[In the context of DP-SGD each round communicates a local SGD update which
leaks some new information about the underlying local data set to the outside
world. In order to provide privacy, Gaussian noise is added to local SGD
updates. However, privacy leakage still aggregates over multiple training
rounds. Therefore, in order to control privacy leakage over an increasing
number of training rounds, we need to increase the added Gaussian noise per
local SGD update. This dependence of the amount of Gaussian noise $\sigma$ on
the number of training rounds $T$ may impose an impractical upper bound on $T$
(because $\sigma$ cannot be too large) leading to a low accuracy global model
(because the global model receives too few local SGD updates). DP-SGD much less
competitive compared to other existing privacy techniques.

We show for the first time that for $(\epsilon,\delta)$-differential privacy
$\sigma$ can be chosen equal to $\sqrt{2(\epsilon +\ln(1/\delta))/\epsilon}$
regardless the total number of training rounds $T$. In other words, $\sigma$
does not depend on $T$ anymore (and aggregation of privacy leakage increases to
a limit). This important discovery brings DP-SGD to practice because $\sigma$
can remain small to make the trained model have high accuracy even for large
$T$ as usually happens in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1"&gt;Marten van Dijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Nhuong V. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Toan N. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Lam M. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong Ha Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Stochastic Gradient Langevin Dynamics. (arXiv:2004.11231v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.11231</id>
        <link href="http://arxiv.org/abs/2004.11231"/>
        <updated>2021-06-16T01:21:11.356Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient MCMC methods, such as stochastic gradient Langevin
dynamics (SGLD), employ fast but noisy gradient estimates to enable large-scale
posterior sampling. Although we can easily extend SGLD to distributed settings,
it suffers from two issues when applied to federated non-IID data. First, the
variance of these estimates increases significantly. Second, delaying
communication causes the Markov chains to diverge from the true posterior even
for very simple models. To alleviate both these problems, we propose conducive
gradients, a simple mechanism that combines local likelihood approximations to
correct gradient updates. Notably, conducive gradients are easy to compute, and
since we only calculate the approximations once, they incur negligible
overhead. We apply conducive gradients to distributed stochastic gradient
Langevin dynamics (DSGLD) and call the resulting method federated stochastic
gradient Langevin dynamics (FSGLD). We demonstrate that our approach can handle
delayed communication rounds, converging to the target posterior in cases where
DSGLD fails. We also show that FSGLD outperforms DSGLD for non-IID federated
data with experiments on metric learning and neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mekkaoui_K/0/1/0/all/0/1"&gt;Khaoula El Mekkaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mesquita_D/0/1/0/all/0/1"&gt;Diego Mesquita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Blomstedt_P/0/1/0/all/0/1"&gt;Paul Blomstedt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10005</id>
        <link href="http://arxiv.org/abs/2105.10005"/>
        <updated>2021-06-16T01:21:11.349Z</updated>
        <summary type="html"><![CDATA[Physical processes, camera movement, and unpredictable environmental
conditions like the presence of dust can induce noise and artifacts in video
feeds. We observe that popular unsupervised MOT methods are dependent on
noise-free inputs. We show that the addition of a small amount of artificial
random noise causes a sharp degradation in model performance on benchmark
metrics. We resolve this problem by introducing a robust unsupervised
multi-object tracking (MOT) model: AttU-Net. The proposed single-head attention
model helps limit the negative impact of noise by learning visual
representations at different segment scales. AttU-Net shows better unsupervised
MOT tracking performance over variational inference-based state-of-the-art
baselines. We evaluate our method in the MNIST-MOT and the Atari game video
benchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''
which consists of moving Japanese characters and ``Fashion-MNIST MOT'' to
validate the effectiveness of the MOT models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;C.-H. Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1"&gt;Mohit Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Y.-C. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quan Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1"&gt;Tomoaki Yoshinaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1"&gt;Tomokazu Murakami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Based Domain Generalization. (arXiv:2102.11436v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11436</id>
        <link href="http://arxiv.org/abs/2102.11436"/>
        <updated>2021-06-16T01:21:11.342Z</updated>
        <summary type="html"><![CDATA[Despite remarkable success in a variety of applications, it is well-known
that deep learning can fail catastrophically when presented with
out-of-distribution data. Toward addressing this challenge, we consider the
domain generalization problem, wherein predictors are trained using data drawn
from a family of related training domains and then evaluated on a distinct and
unseen test domain. We show that under a natural model of data generation and a
concomitant invariance condition, the domain generalization problem is
equivalent to an infinite-dimensional constrained statistical learning problem;
this problem forms the basis of our approach, which we call Model-Based Domain
Generalization. Due to the inherent challenges in solving constrained
optimization problems in deep learning, we exploit nonconvex duality theory to
develop unconstrained relaxations of this statistical problem with tight bounds
on the duality gap. Based on this theoretical motivation, we propose a novel
domain generalization algorithm with convergence guarantees. In our
experiments, we report improvements of up to 30 percentage points over
state-of-the-art domain generalization baselines on several benchmarks
including ColoredMNIST, Camelyon17-WILDS, FMoW-WILDS, and PACS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Robey_A/0/1/0/all/0/1"&gt;Alexander Robey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pappas_G/0/1/0/all/0/1"&gt;George J. Pappas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hassani_H/0/1/0/all/0/1"&gt;Hamed Hassani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantized Adam with Error Feedback. (arXiv:2004.14180v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.14180</id>
        <link href="http://arxiv.org/abs/2004.14180"/>
        <updated>2021-06-16T01:21:11.321Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a distributed variant of adaptive stochastic
gradient method for training deep neural networks in the parameter-server
model. To reduce the communication cost among the workers and server, we
incorporate two types of quantization schemes, i.e., gradient quantization and
weight quantization, into the proposed distributed Adam. Besides, to reduce the
bias introduced by quantization operations, we propose an error-feedback
technique to compensate for the quantized gradient. Theoretically, in the
stochastic nonconvex setting, we show that the distributed adaptive gradient
method with gradient quantization and error-feedback converges to the
first-order stationary point, and that the distributed adaptive gradient method
with weight quantization and error-feedback converges to the point related to
the quantized level under both the single-worker and multi-worker modes. At
last, we apply the proposed distributed adaptive gradient methods to train deep
neural networks. Experimental results demonstrate the efficacy of our methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Congliang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haozhi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Lossless Compression Rates via Monte Carlo Bits-Back Coding. (arXiv:2102.11086v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11086</id>
        <link href="http://arxiv.org/abs/2102.11086"/>
        <updated>2021-06-16T01:21:11.305Z</updated>
        <summary type="html"><![CDATA[Latent variable models have been successfully applied in lossless compression
with the bits-back coding algorithm. However, bits-back suffers from an
increase in the bitrate equal to the KL divergence between the approximate
posterior and the true posterior. In this paper, we show how to remove this gap
asymptotically by deriving bits-back coding algorithms from tighter variational
bounds. The key idea is to exploit extended space representations of Monte
Carlo estimators of the marginal likelihood. Naively applied, our schemes would
require more initial bits than the standard bits-back coder, but we show how to
drastically reduce this additional cost with couplings in the latent space.
When parallel architectures can be exploited, our coders can achieve better
rates than bits-back with little additional cost. We demonstrate improved
lossless compression rates in a variety of settings, especially in
out-of-distribution or sequential data compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1"&gt;Yangjun Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1"&gt;Karen Ullrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Severo_D/0/1/0/all/0/1"&gt;Daniel Severo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Townsend_J/0/1/0/all/0/1"&gt;James Townsend&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khisti_A/0/1/0/all/0/1"&gt;Ashish Khisti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makhzani_A/0/1/0/all/0/1"&gt;Alireza Makhzani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1"&gt;Chris J. Maddison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multivariate Uncertainty in Deep Learning. (arXiv:1910.14215v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.14215</id>
        <link href="http://arxiv.org/abs/1910.14215"/>
        <updated>2021-06-16T01:21:11.298Z</updated>
        <summary type="html"><![CDATA[Deep learning has the potential to dramatically impact navigation and
tracking state estimation problems critical to autonomous vehicles and
robotics. Measurement uncertainties in state estimation systems based on Kalman
and other Bayes filters are typically assumed to be a fixed covariance matrix.
This assumption is risky, particularly for "black box" deep learning models, in
which uncertainty can vary dramatically and unexpectedly. Accurate
quantification of multivariate uncertainty will allow for the full potential of
deep learning to be used more safely and reliably in these applications. We
show how to model multivariate uncertainty for regression problems with neural
networks, incorporating both aleatoric and epistemic sources of heteroscedastic
uncertainty. We train a deep uncertainty covariance matrix model in two ways:
directly using a multivariate Gaussian density loss function, and indirectly
using end-to-end training through a Kalman filter. We experimentally show in a
visual tracking problem the large impact that accurate multivariate uncertainty
quantification can have on Kalman filter performance for both in-domain and
out-of-domain evaluation data. We additionally show in a challenging visual
odometry problem how end-to-end filter training can allow uncertainty
predictions to compensate for filter weaknesses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Russell_R/0/1/0/all/0/1"&gt;Rebecca L. Russell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reale_C/0/1/0/all/0/1"&gt;Christopher Reale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Quantiles. (arXiv:2102.08244v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08244</id>
        <link href="http://arxiv.org/abs/2102.08244"/>
        <updated>2021-06-16T01:21:11.291Z</updated>
        <summary type="html"><![CDATA[Quantiles are often used for summarizing and understanding data. If that data
is sensitive, it may be necessary to compute quantiles in a way that is
differentially private, providing theoretical guarantees that the result does
not reveal private information. However, when multiple quantiles are needed,
existing differentially private algorithms fare poorly: they either compute
quantiles individually, splitting the privacy budget, or summarize the entire
distribution, wasting effort. In either case the result is reduced accuracy. In
this work we propose an instance of the exponential mechanism that
simultaneously estimates exactly $m$ quantiles from $n$ data points while
guaranteeing differential privacy. The utility function is carefully structured
to allow for an efficient implementation that returns estimates of all $m$
quantiles in time $O(mn\log(n) + m^2n)$. Experiments show that our method
significantly outperforms the current state of the art on both real and
synthetic data while remaining efficient enough to be practical.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gillenwater_J/0/1/0/all/0/1"&gt;Jennifer Gillenwater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_M/0/1/0/all/0/1"&gt;Matthew Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulesza_A/0/1/0/all/0/1"&gt;Alex Kulesza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning for Unknown Partially Observable MDPs. (arXiv:2102.12661v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12661</id>
        <link href="http://arxiv.org/abs/2102.12661"/>
        <updated>2021-06-16T01:21:11.275Z</updated>
        <summary type="html"><![CDATA[Solving Partially Observable Markov Decision Processes (POMDPs) is hard.
Learning optimal controllers for POMDPs when the model is unknown is harder.
Online learning of optimal controllers for unknown POMDPs, which requires
efficient learning using regret-minimizing algorithms that effectively tradeoff
exploration and exploitation, is even harder, and no solution exists currently.
In this paper, we consider infinite-horizon average-cost POMDPs with unknown
transition model, though a known observation model. We propose a natural
posterior sampling-based reinforcement learning algorithm (PSRL-POMDP) and show
that it achieves a regret bound of $O(\log T)$, where $T$ is the time horizon,
when the parameter set is finite. In the general case (continuous parameter
set), we show that the algorithm achieves $O (T^{2/3})$ regret under two
technical assumptions. To the best of our knowledge, this is the first online
RL algorithm for POMDPs and has sub-linear regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jafarnia_Jahromi_M/0/1/0/all/0/1"&gt;Mehdi Jafarnia-Jahromi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1"&gt;Rahul Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nayyar_A/0/1/0/all/0/1"&gt;Ashutosh Nayyar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Field-Embedded Factorization Machines for Click-through rate prediction. (arXiv:2009.09931v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09931</id>
        <link href="http://arxiv.org/abs/2009.09931"/>
        <updated>2021-06-16T01:21:11.264Z</updated>
        <summary type="html"><![CDATA[Click-through rate (CTR) prediction models are common in many online
applications such as digital advertising and recommender systems. Field-Aware
Factorization Machine (FFM) and Field-weighted Factorization Machine (FwFM) are
state-of-the-art among the shallow models for CTR prediction. Recently, many
deep learning-based models have also been proposed. Among deeper models,
DeepFM, xDeepFM, AutoInt+, and FiBiNet are state-of-the-art models. The deeper
models combine a core architectural component, which learns explicit feature
interactions, with a deep neural network (DNN) component. We propose a novel
shallow Field-Embedded Factorization Machine (FEFM) and its deep counterpart
Deep Field-Embedded Factorization Machine (DeepFEFM). FEFM learns symmetric
matrix embeddings for each field pair along with the usual single vector
embeddings for each feature. FEFM has significantly lower model complexity than
FFM and roughly the same complexity as FwFM. FEFM also has insightful
mathematical properties about important fields and field interactions. DeepFEFM
combines the FEFM interaction vectors learned by the FEFM component with a DNN
and is thus able to learn higher order interactions. We conducted comprehensive
experiments over a wide range of hyperparameters on two large publicly
available real-world datasets. When comparing test AUC and log loss, the
results show that FEFM and DeepFEFM outperform the existing state-of-the-art
shallow and deep models for CTR prediction tasks. We have made the code of FEFM
and DeepFEFM available in the DeepCTR library
(https://github.com/shenweichen/DeepCTR).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pande_H/0/1/0/all/0/1"&gt;Harshit Pande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled Gradient Descent. (arXiv:2005.08898v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.08898</id>
        <link href="http://arxiv.org/abs/2005.08898"/>
        <updated>2021-06-16T01:21:11.246Z</updated>
        <summary type="html"><![CDATA[Low-rank matrix estimation is a canonical problem that finds numerous
applications in signal processing, machine learning and imaging science. A
popular approach in practice is to factorize the matrix into two compact
low-rank factors, and then optimize these factors directly via simple iterative
methods such as gradient descent and alternating minimization. Despite
nonconvexity, recent literatures have shown that these simple heuristics in
fact achieve linear convergence when initialized properly for a growing number
of problems of interest. However, upon closer examination, existing approaches
can still be computationally expensive especially for ill-conditioned matrices:
the convergence rate of gradient descent depends linearly on the condition
number of the low-rank matrix, while the per-iteration cost of alternating
minimization is often prohibitive for large matrices. The goal of this paper is
to set forth a competitive algorithmic approach dubbed Scaled Gradient Descent
(ScaledGD) which can be viewed as pre-conditioned or diagonally-scaled gradient
descent, where the pre-conditioners are adaptive and iteration-varying with a
minimal computational overhead. With tailored variants for low-rank matrix
sensing, robust principal component analysis and matrix completion, we
theoretically show that ScaledGD achieves the best of both worlds: it converges
linearly at a rate independent of the condition number of the low-rank matrix
similar as alternating minimization, while maintaining the low per-iteration
cost of gradient descent. Our analysis is also applicable to general loss
functions that are restricted strongly convex and smooth over low-rank
matrices. To the best of our knowledge, ScaledGD is the first algorithm that
provably has such properties over a wide range of low-rank matrix estimation
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tong_T/0/1/0/all/0/1"&gt;Tian Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Cong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1"&gt;Yuejie Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top-Related Meta-Learning Method for Few-Shot Object Detection. (arXiv:2007.06837v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06837</id>
        <link href="http://arxiv.org/abs/2007.06837"/>
        <updated>2021-06-16T01:21:11.214Z</updated>
        <summary type="html"><![CDATA[Many meta-learning methods are proposed for few-shot detection. However,
previous most methods have two main problems, poor detection APs, and strong
bias because of imbalance and insufficient datasets. Previous works mainly
alleviate these issues by additional datasets, multi-relation attention
mechanisms and sub-modules. However, they require more cost. In this work, for
meta-learning, we find that the main challenges focus on related or irrelevant
semantic features between categories. Therefore, based on semantic features, we
propose a Top-C classification loss (i.e., TCL-C) for classification task and a
category-based grouping mechanism for category-based meta-features obtained by
the meta-model. The TCL-C exploits the true-label prediction and the most
likely C-1 false classification predictions to improve detection performance on
few-shot classes. According to similar appearance (i.e., visual appearance,
shape, and limbs etc.) and environment in which objects often appear, the
category-based grouping mechanism splits categories into disjoint groups to
make similar semantic features more compact between categories within a group
and obtain more significant difference between groups, alleviating the strong
bias problem and further improving detection APs. The whole training consists
of the base model and the fine-tuning phases. According to grouping mechanism,
we group the meta-features vectors obtained by meta-model, so that the
distribution difference between groups is obvious, and the one within each
group is less. Extensive experiments on Pascal VOC dataset demonstrate that
ours which combines the TCL-C with category-based grouping significantly
outperforms previous state-of-the-art methods for few-shot detection. Compared
with previous competitive baseline, ours improves detection APs by almost 4%
for few-shot detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1"&gt;Nan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaochun Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Duo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Dongrui Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhimin Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-Associated Differential Learning. (arXiv:2102.05246v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05246</id>
        <link href="http://arxiv.org/abs/2102.05246"/>
        <updated>2021-06-16T01:21:11.201Z</updated>
        <summary type="html"><![CDATA[Conventional Supervised Learning approaches focus on the mapping from input
features to output labels. After training, the learnt models alone are adapted
onto testing features to predict testing labels in isolation, with training
data wasted and their associations ignored. To take full advantage of the vast
number of training data and their associations, we propose a novel learning
paradigm called Memory-Associated Differential (MAD) Learning. We first
introduce an additional component called Memory to memorize all the training
data. Then we learn the differences of labels as well as the associations of
features in the combination of a differential equation and some sampling
methods. Finally, in the evaluating phase, we predict unknown labels by
inferencing from the memorized facts plus the learnt differences and
associations in a geometrically meaningful manner. We gently build this theory
in unary situations and apply it on Image Recognition, then extend it into Link
Prediction as a binary situation, in which our method outperforms strong
state-of-the-art baselines on ogbl-ddi dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Aiguo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1"&gt;Bei Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1"&gt;Ke Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Recurrent Neural Tangent Kernel. (arXiv:2006.10246v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10246</id>
        <link href="http://arxiv.org/abs/2006.10246"/>
        <updated>2021-06-16T01:21:11.176Z</updated>
        <summary type="html"><![CDATA[The study of deep neural networks (DNNs) in the infinite-width limit, via the
so-called neural tangent kernel (NTK) approach, has provided new insights into
the dynamics of learning, generalization, and the impact of initialization. One
key DNN architecture remains to be kernelized, namely, the recurrent neural
network (RNN). In this paper we introduce and study the Recurrent Neural
Tangent Kernel (RNTK), which provides new insights into the behavior of
overparametrized RNNs. A key property of the RNTK should greatly benefit
practitioners is its ability to compare inputs of different length. To this
end, we characterize how the RNTK weights different time steps to form its
output under different initialization parameters and nonlinearity choices. A
synthetic and 56 real-world data experiments demonstrate that the RNTK offers
significant performance gains over other kernels, including standard NTKs,
across a wide array of data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alemohammad_S/0/1/0/all/0/1"&gt;Sina Alemohammad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zichao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1"&gt;Randall Balestriero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1"&gt;Richard Baraniuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Sparse Regression with Clustering: An Invex Relaxation for a Combinatorial Problem. (arXiv:2102.09704v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09704</id>
        <link href="http://arxiv.org/abs/2102.09704"/>
        <updated>2021-06-16T01:21:11.157Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of fair sparse regression on a biased
dataset where bias depends upon a hidden binary attribute. The presence of a
hidden attribute adds an extra layer of complexity to the problem by combining
sparse regression and clustering with unknown binary labels. The corresponding
optimization problem is combinatorial, but we propose a novel relaxation of it
as an \emph{invex} optimization problem. To the best of our knowledge, this is
the first invex relaxation for a combinatorial problem. We show that the
inclusion of the debiasing/fairness constraint in our model has no adverse
effect on the performance. Rather, it enables the recovery of the hidden
attribute. The support of our recovered regression parameter vector matches
exactly with the true parameter vector. Moreover, we simultaneously solve the
clustering problem by recovering the exact value of the hidden attribute for
each sample. Our method uses carefully constructed primal dual witnesses to
provide theoretical guarantees for the combinatorial problem. To that end, we
show that the sample complexity of our method is logarithmic in terms of the
dimension of the regression parameter vector.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barik_A/0/1/0/all/0/1"&gt;Adarsh Barik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honorio_J/0/1/0/all/0/1"&gt;Jean Honorio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reasoning Over Virtual Knowledge Bases With Open Predicate Relations. (arXiv:2102.07043v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07043</id>
        <link href="http://arxiv.org/abs/2102.07043"/>
        <updated>2021-06-16T01:21:11.151Z</updated>
        <summary type="html"><![CDATA[We present the Open Predicate Query Language (OPQL); a method for
constructing a virtual KB (VKB) trained entirely from text. Large Knowledge
Bases (KBs) are indispensable for a wide-range of industry applications such as
question answering and recommendation. Typically, KBs encode world knowledge in
a structured, readily accessible form derived from laborious human annotation
efforts. Unfortunately, while they are extremely high precision, KBs are
inevitably highly incomplete and automated methods for enriching them are far
too inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set
of relation mentions in a way that naturally enables reasoning and can be
trained without any structured supervision. We demonstrate that OPQL
outperforms prior VKB methods on two different KB reasoning tasks and,
additionally, can be used as an external memory integrated into a language
model (OPQL-LM) leading to improvements on two open-domain question answering
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haitian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1"&gt;Pat Verga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1"&gt;Bhuwan Dhingra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"&gt;William W. Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Semantic Guidance and Deep Reinforcement Learning For Generating Human Level Paintings. (arXiv:2011.12589v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12589</id>
        <link href="http://arxiv.org/abs/2011.12589"/>
        <updated>2021-06-16T01:21:11.144Z</updated>
        <summary type="html"><![CDATA[Generation of stroke-based non-photorealistic imagery, is an important
problem in the computer vision community. As an endeavor in this direction,
substantial recent research efforts have been focused on teaching machines "how
to paint", in a manner similar to a human painter. However, the applicability
of previous methods has been limited to datasets with little variation in
position, scale and saliency of the foreground object. As a consequence, we
find that these methods struggle to cover the granularity and diversity
possessed by real world images. To this end, we propose a Semantic Guidance
pipeline with 1) a bi-level painting procedure for learning the distinction
between foreground and background brush strokes at training time. 2) We also
introduce invariance to the position and scale of the foreground object through
a neural alignment model, which combines object localization and spatial
transformer networks in an end to end manner, to zoom into a particular
semantic instance. 3) The distinguishing features of the in-focus object are
then amplified by maximizing a novel guided backpropagation based focus reward.
The proposed agent does not require any supervision on human stroke-data and
successfully handles variations in foreground object attributes, thus,
producing much higher quality canvases for the CUB-200 Birds and Stanford
Cars-196 datasets. Finally, we demonstrate the further efficacy of our method
on complex datasets with multiple foreground object instances by evaluating an
extension of our method on the challenging Virtual-KITTI dataset. Source code
and models are available at https://github.com/1jsingh/semantic-guidance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Jaskirat Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Navigation by Continuous-time Neural Networks. (arXiv:2106.08314v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08314</id>
        <link href="http://arxiv.org/abs/2106.08314"/>
        <updated>2021-06-16T01:21:11.123Z</updated>
        <summary type="html"><![CDATA[Imitation learning enables high-fidelity, vision-based learning of policies
within rich, photorealistic environments. However, such techniques often rely
on traditional discrete-time neural models and face difficulties in
generalizing to domain shifts by failing to account for the causal
relationships between the agent and the environment. In this paper, we propose
a theoretical and experimental framework for learning causal representations
using continuous-time neural networks, specifically over their discrete-time
counterparts. We evaluate our method in the context of visual-control learning
of drones over a series of complex tasks, ranging from short- and long-term
navigation, to chasing static and dynamic objects through photorealistic
environments. Our results demonstrate that causal continuous-time deep models
can perform robust navigation tasks, where advanced recurrent models fail.
These models learn complex causal control representations directly from raw
visual inputs and scale to solve a variety of tasks using imitation learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vorbach_C/0/1/0/all/0/1"&gt;Charles Vorbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1"&gt;Ramin Hasani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1"&gt;Alexander Amini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1"&gt;Mathias Lechner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Keyphrase Detection using Speaker and Environment Information. (arXiv:2104.13970v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13970</id>
        <link href="http://arxiv.org/abs/2104.13970"/>
        <updated>2021-06-16T01:21:11.104Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a streaming keyphrase detection system that can
be easily customized to accurately detect any phrase composed of words from a
large vocabulary. The system is implemented with an end-to-end trained
automatic speech recognition (ASR) model and a text-independent speaker
verification model. To address the challenge of detecting these keyphrases
under various noisy conditions, a speaker separation model is added to the
feature frontend of the speaker verification model, and an adaptive noise
cancellation (ANC) algorithm is included to exploit cross-microphone noise
coherence. Our experiments show that the text-independent speaker verification
model largely reduces the false triggering rate of the keyphrase detection,
while the speaker separation model and adaptive noise cancellation largely
reduce false rejections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rikhye_R/0/1/0/all/0/1"&gt;Rajeev Rikhye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Quan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_Q/0/1/0/all/0/1"&gt;Qiao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1"&gt;Yanzhang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yiteng/0/1/0/all/0/1"&gt;Yiteng&lt;/a&gt; (Arden) &lt;a href="http://arxiv.org/find/eess/1/au:+Huang/0/1/0/all/0/1"&gt;Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Narayanan_A/0/1/0/all/0/1"&gt;Arun Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+McGraw_I/0/1/0/all/0/1"&gt;Ian McGraw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analytic function approximation by path norm regularized deep networks. (arXiv:2104.02095v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02095</id>
        <link href="http://arxiv.org/abs/2104.02095"/>
        <updated>2021-06-16T01:21:11.045Z</updated>
        <summary type="html"><![CDATA[We provide an entropy bound for the spaces of path norm regularized neural
networks with piecewise linear activation functions, such as the ReLU and the
absolute value functions. This bound generalizes the known entropy bound for
the spaces of linear functions on $\mathbb{R}^d$. Keeping the path norm
together with the depth, width and the weights of networks to have logarithmic
dependence on $1/\varepsilon$, we $\varepsilon$-approximate functions that are
analytic on certain regions of $\mathbb{C}^d$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1"&gt;Aleksandr Beknazaryan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust and Sample Optimal Algorithms for PSD Low-Rank Approximation. (arXiv:1912.04177v5 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.04177</id>
        <link href="http://arxiv.org/abs/1912.04177"/>
        <updated>2021-06-16T01:21:11.035Z</updated>
        <summary type="html"><![CDATA[Recently, Musco and Woodruff (FOCS, 2017) showed that given an $n \times n$
positive semidefinite (PSD) matrix $A$, it is possible to compute a
$(1+\epsilon)$-approximate relative-error low-rank approximation to $A$ by
querying $O(nk/\epsilon^{2.5})$ entries of $A$ in time $O(nk/\epsilon^{2.5} +n
k^{\omega-1}/\epsilon^{2(\omega-1)})$. They also showed that any relative-error
low-rank approximation algorithm must query $\Omega(nk/\epsilon)$ entries of
$A$, this gap has since remained open. Our main result is to resolve this
question by obtaining an optimal algorithm that queries $O(nk/\epsilon)$
entries of $A$ and outputs a relative-error low-rank approximation in
$O(n(k/\epsilon)^{\omega-1})$ time. Note, our running time improves that of
Musco and Woodruff, and matches the information-theoretic lower bound if the
matrix-multiplication exponent $\omega$ is $2$.

We then extend our techniques to negative-type distance matrices. Bakshi and
Woodruff (NeurIPS, 2018) showed a bi-criteria, relative-error low-rank
approximation which queries $O(nk/\epsilon^{2.5})$ entries and outputs a
rank-$(k+4)$ matrix. We show that the bi-criteria guarantee is not necessary
and obtain an $O(nk/\epsilon)$ query algorithm, which is optimal. Our algorithm
applies to all distance matrices that arise from metrics satisfying
negative-type inequalities, including $\ell_1, \ell_2,$ spherical metrics and
hypermetrics.

Next, we introduce a new robust low-rank approximation model which captures
PSD matrices that have been corrupted with noise. While a sample complexity
lower bound precludes sublinear algorithms for arbitrary PSD matrices, we
provide the first sublinear time and query algorithms when the corruption on
the diagonal entries is bounded. As a special case, we show sample-optimal
sublinear time algorithms for low-rank approximation of correlation matrices
corrupted by noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bakshi_A/0/1/0/all/0/1"&gt;Ainesh Bakshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chepurko_N/0/1/0/all/0/1"&gt;Nadiia Chepurko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mind Your Weight(s): A Large-scale Study on Insufficient Machine Learning Model Protection in Mobile Apps. (arXiv:2002.07687v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.07687</id>
        <link href="http://arxiv.org/abs/2002.07687"/>
        <updated>2021-06-16T01:21:11.018Z</updated>
        <summary type="html"><![CDATA[On-device machine learning (ML) is quickly gaining popularity among mobile
apps. It allows offline model inference while preserving user privacy. However,
ML models, considered as core intellectual properties of model owners, are now
stored on billions of untrusted devices and subject to potential thefts. Leaked
models can cause both severe financial loss and security consequences. This
paper presents the first empirical study of ML model protection on mobile
devices. Our study aims to answer three open questions with quantitative
evidence: How widely is model protection used in apps? How robust are existing
model protection techniques? What impacts can (stolen) models incur? To that
end, we built a simple app analysis pipeline and analyzed 46,753 popular apps
collected from the US and Chinese app markets. We identified 1,468 ML apps
spanning all popular app categories. We found that, alarmingly, 41% of ML apps
do not protect their models at all, which can be trivially stolen from app
packages. Even for those apps that use model protection or encryption, we were
able to extract the models from 66% of them via unsophisticated dynamic
analysis techniques. The extracted models are mostly commercial products and
used for face recognition, liveness detection, ID/bank card recognition, and
malware detection. We quantitatively estimated the potential financial and
security impact of a leaked model, which can amount to millions of dollars for
different stakeholders. Our study reveals that on-device models are currently
at high risk of being leaked; attackers are highly motivated to steal such
models. Drawn from our large-scale study, we report our insights into this
emerging security problem and discuss the technical challenges, hoping to
inspire future research on robust and practical model protection for mobile
devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhichuang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1"&gt;Ruimin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Long Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mislove_A/0/1/0/all/0/1"&gt;Alan Mislove&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11066</id>
        <link href="http://arxiv.org/abs/2010.11066"/>
        <updated>2021-06-16T01:21:11.011Z</updated>
        <summary type="html"><![CDATA[Spoken conversational question answering (SCQA) requires machines to model
complex dialogue flow given the speech utterances and text corpora. Different
from traditional text question answering (QA) tasks, SCQA involves audio signal
processing, passage comprehension, and contextual understanding. However, ASR
systems introduce unexpected noisy signals to the transcriptions, which result
in performance degradation on SCQA. To overcome the problem, we propose CADNet,
a novel contextualized attention-based distillation approach, which applies
both cross-attention and self-attention to obtain ASR-robust contextualized
embedding representations of the passage and dialogue history for performance
improvements. We also introduce the spoken conventional knowledge distillation
framework to distill the ASR-robust knowledge from the estimated probabilities
of the teacher model to the student. We conduct extensive experiments on the
Spoken-CoQA dataset and demonstrate that our approach achieves remarkable
performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ST-UNet: A Spatio-Temporal U-Network for Graph-structured Time Series Modeling. (arXiv:1903.05631v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.05631</id>
        <link href="http://arxiv.org/abs/1903.05631"/>
        <updated>2021-06-16T01:21:11.004Z</updated>
        <summary type="html"><![CDATA[The spatio-temporal graph learning is becoming an increasingly important
object of graph study. Many application domains involve highly dynamic graphs
where temporal information is crucial, e.g. traffic networks and financial
transaction graphs. Despite the constant progress made on learning structured
data, there is still a lack of effective means to extract dynamic complex
features from spatio-temporal structures. Particularly, conventional models
such as convolutional networks or recurrent neural networks are incapable of
revealing the temporal patterns in short or long terms and exploring the
spatial properties in local or global scope from spatio-temporal graphs
simultaneously. To tackle this problem, we design a novel multi-scale
architecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series
modeling. In this U-shaped network, a paired sampling operation is proposed in
spacetime domain accordingly: the pooling (ST-Pool) coarsens the input graph in
spatial from its deterministic partition while abstracts multi-resolution
temporal dependencies through dilated recurrent skip connections; based on
previous settings in the downsampling, the unpooling (ST-Unpool) restores the
original structure of spatio-temporal graphs and resumes regular intervals
within graph sequences. Experiments on spatio-temporal prediction tasks
demonstrate that our model effectively captures comprehensive features in
multiple scales and achieves substantial improvements over mainstream methods
on several real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Haoteng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhanxing Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation of Teacher Ensembles for Spoken Command Classification. (arXiv:2104.01271v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01271</id>
        <link href="http://arxiv.org/abs/2104.01271"/>
        <updated>2021-06-16T01:21:10.997Z</updated>
        <summary type="html"><![CDATA[We propose using an adversarial autoencoder (AAE) to replace generative
adversarial network (GAN) in the private aggregation of teacher ensembles
(PATE), a solution for ensuring differential privacy in speech applications.
The AAE architecture allows us to obtain good synthetic speech leveraging upon
a discriminative training of latent vectors. Such synthetic speech is used to
build a privacy-preserving classifier when non-sensitive data is not
sufficiently available in the public domain. This classifier follows the PATE
scheme that uses an ensemble of noisy outputs to label the synthetic samples
and guarantee $\varepsilon$-differential privacy (DP) on its derived
classifiers. Our proposed framework thus consists of an AAE-based generator and
a PATE-based classifier (PATE-AAE). Evaluated on the Google Speech Commands
Dataset Version II, the proposed PATE-AAE improves the average classification
accuracy by +$2.11\%$ and +$6.60\%$, respectively, when compared with
alternative privacy-preserving solutions, namely PATE-GAN and DP-GAN, while
maintaining a strong level of privacy target at $\varepsilon$=0.01 with a fixed
$\delta$=10$^{-5}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao-Han Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1"&gt;Sabato Marco Siniscalchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chin-Hui Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double Double Descent: On Generalization Errors in Transfer Learning between Linear Regression Tasks. (arXiv:2006.07002v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07002</id>
        <link href="http://arxiv.org/abs/2006.07002"/>
        <updated>2021-06-16T01:21:10.971Z</updated>
        <summary type="html"><![CDATA[We study the transfer learning process between two linear regression
problems. An important and timely special case is when the regressors are
overparameterized and perfectly interpolate their training data. We examine a
parameter transfer mechanism whereby a subset of the parameters of the target
task solution are constrained to the values learned for a related source task.
We analytically characterize the generalization error of the target task in
terms of the salient factors in the transfer learning architecture, i.e., the
number of examples available, the number of (free) parameters in each of the
tasks, the number of parameters transferred from the source to target task, and
the correlation between the two tasks. Our non-asymptotic analysis shows that
the generalization error of the target task follows a two-dimensional double
descent trend (with respect to the number of free parameters in each of the
tasks) that is controlled by the transfer learning factors. Our analysis points
to specific cases where the transfer of parameters is beneficial. Specifically,
we show that transferring a specific set of parameters that generalizes well on
the respective part of the source task can soften the demand on the task
correlation level that is required for successful transfer learning. Moreover,
we show that the usefulness of a transfer learning setting is fragile and
depends on a delicate interplay among the set of transferred parameters, the
relation between the tasks, and the true solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dar_Y/0/1/0/all/0/1"&gt;Yehuda Dar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1"&gt;Richard G. Baraniuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning. (arXiv:2104.04975v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04975</id>
        <link href="http://arxiv.org/abs/2104.04975"/>
        <updated>2021-06-16T01:21:10.956Z</updated>
        <summary type="html"><![CDATA[Marginal-likelihood based model-selection, even though promising, is rarely
used in deep learning due to estimation difficulties. Instead, most approaches
rely on validation data, which may not be readily available. In this work, we
present a scalable marginal-likelihood estimation method to select both
hyperparameters and network architectures, based on the training data alone.
Some hyperparameters can be estimated online during training, simplifying the
procedure. Our marginal-likelihood estimate is based on Laplace's method and
Gauss-Newton approximations to the Hessian, and it outperforms cross-validation
and manual-tuning on standard regression and image classification datasets,
especially in terms of calibration and out-of-distribution detection. Our work
shows that marginal likelihoods can improve generalization and be useful when
validation data is unavailable (e.g., in nonstationary settings).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Immer_A/0/1/0/all/0/1"&gt;Alexander Immer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bauer_M/0/1/0/all/0/1"&gt;Matthias Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1"&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGENT: A Benchmark for Core Psychological Reasoning. (arXiv:2102.12321v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12321</id>
        <link href="http://arxiv.org/abs/2102.12321"/>
        <updated>2021-06-16T01:21:10.940Z</updated>
        <summary type="html"><![CDATA[For machine agents to successfully interact with humans in real-world
settings, they will need to develop an understanding of human mental life.
Intuitive psychology, the ability to reason about hidden mental variables that
drive observable actions, comes naturally to people: even pre-verbal infants
can tell agents from objects, expecting agents to act efficiently to achieve
goals given constraints. Despite recent interest in machine agents that reason
about other agents, it is not clear if such agents learn or hold the core
psychology principles that drive human reasoning. Inspired by cognitive
development studies on intuitive psychology, we present a benchmark consisting
of a large dataset of procedurally generated 3D animations, AGENT (Action,
Goal, Efficiency, coNstraint, uTility), structured around four scenarios (goal
preferences, action efficiency, unobserved constraints, and cost-reward
trade-offs) that probe key concepts of core intuitive psychology. We validate
AGENT with human-ratings, propose an evaluation protocol emphasizing
generalization, and compare two strong baselines built on Bayesian inverse
planning and a Theory of Mind neural network. Our results suggest that to pass
the designed tests of core intuitive psychology at human levels, a model must
acquire or have built-in representations of how agents plan, combining utility
computations and core knowledge of objects and physics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1"&gt;Tianmin Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1"&gt;Abhishek Bhandwaldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kevin A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shari Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1"&gt;Dan Gutfreund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spelke_E/0/1/0/all/0/1"&gt;Elizabeth Spelke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1"&gt;Tomer D. Ullman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Contextual Bandit Learning for Adaptive Radar Waveform Selection. (arXiv:2103.05541v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05541</id>
        <link href="http://arxiv.org/abs/2103.05541"/>
        <updated>2021-06-16T01:21:10.922Z</updated>
        <summary type="html"><![CDATA[A sequential decision process in which an adaptive radar system repeatedly
interacts with a finite-state target channel is studied. The radar is capable
of passively sensing the spectrum at regular intervals, which provides side
information for the waveform selection process. The radar transmitter uses the
sequence of spectrum observations as well as feedback from a collocated
receiver to select waveforms which accurately estimate target parameters. It is
shown that the waveform selection problem can be effectively addressed using a
linear contextual bandit formulation in a manner that is both computationally
feasible and sample efficient. Stochastic and adversarial linear contextual
bandit models are introduced, allowing the radar to achieve effective
performance in broad classes of physical environments. Simulations in a
radar-communication coexistence scenario, as well as in an adversarial
radar-jammer scenario, demonstrate that the proposed formulation provides a
substantial improvement in target detection performance when Thompson Sampling
and EXP3 algorithms are used to drive the waveform selection process. Further,
it is shown that the harmful impacts of pulse-agile behavior on coherently
processed radar data can be mitigated by adopting a time-varying constraint on
the radar's waveform catalog.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thornton_C/0/1/0/all/0/1"&gt;Charles E. Thornton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buehrer_R/0/1/0/all/0/1"&gt;R. Michael Buehrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martone_A/0/1/0/all/0/1"&gt;Anthony F. Martone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Functionals on the Space of Probabilities with Input Convex Neural Networks. (arXiv:2106.00774v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00774</id>
        <link href="http://arxiv.org/abs/2106.00774"/>
        <updated>2021-06-16T01:21:10.876Z</updated>
        <summary type="html"><![CDATA[Gradient flows are a powerful tool for optimizing functionals in general
metric spaces, including the space of probabilities endowed with the
Wasserstein metric. A typical approach to solving this optimization problem
relies on its connection to the dynamic formulation of optimal transport and
the celebrated Jordan-Kinderlehrer-Otto (JKO) scheme. However, this formulation
involves optimization over convex functions, which is challenging, especially
in high dimensions. In this work, we propose an approach that relies on the
recently introduced input-convex neural networks (ICNN) to parameterize the
space of convex functions in order to approximate the JKO scheme, as well as in
designing functionals over measures that enjoy convergence guarantees. We
derive a computationally efficient implementation of this JKO-ICNN framework
and use various experiments to demonstrate its feasibility and validity in
approximating solutions of low-dimensional partial differential equations with
known solutions. We also explore the use of our JKO-ICNN approach in high
dimensions with an experiment in controlled generation for molecular discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Alvarez_Melis_D/0/1/0/all/0/1"&gt;David Alvarez-Melis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schiff_Y/0/1/0/all/0/1"&gt;Yair Schiff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mroueh_Y/0/1/0/all/0/1"&gt;Youssef Mroueh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks Inspired by Classical Iterative Algorithms. (arXiv:2103.06064v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06064</id>
        <link href="http://arxiv.org/abs/2103.06064"/>
        <updated>2021-06-16T01:21:10.870Z</updated>
        <summary type="html"><![CDATA[Despite the recent success of graph neural networks (GNN), common
architectures often exhibit significant limitations, including sensitivity to
oversmoothing, long-range dependencies, and spurious edges, e.g., as can occur
as a result of graph heterophily or adversarial attacks. To at least partially
address these issues within a simple transparent framework, we consider a new
family of GNN layers designed to mimic and integrate the update rules of two
classical iterative algorithms, namely, proximal gradient descent and iterative
reweighted least squares (IRLS). The former defines an extensible base GNN
architecture that is immune to oversmoothing while nonetheless capturing
long-range dependencies by allowing arbitrary propagation steps. In contrast,
the latter produces a novel attention mechanism that is explicitly anchored to
an underlying end-to-end energy function, contributing stability with respect
to edge uncertainty. When combined we obtain an extremely simple yet robust
model that we evaluate across disparate scenarios including standardized
benchmarks, adversarially-perturbated graphs, graphs with heterophily, and
graphs involving long-range dependencies. In doing so, we compare against SOTA
GNN approaches that have been explicitly designed for the respective task,
achieving competitive or superior node classification accuracy. Our code is
available at https://github.com/FFTYYY/TWIRLS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yangkun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinjing Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1"&gt;Quan Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhewei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zengfeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1"&gt;David Wipf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Language Adversarial Defense through Synonym Encoding. (arXiv:1909.06723v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.06723</id>
        <link href="http://arxiv.org/abs/1909.06723"/>
        <updated>2021-06-16T01:21:10.863Z</updated>
        <summary type="html"><![CDATA[In the area of natural language processing, deep learning models are recently
known to be vulnerable to various types of adversarial perturbations, but
relatively few works are done on the defense side. Especially, there exists few
effective defense method against the successful synonym substitution based
attacks that preserve the syntactic structure and semantic information of the
original text while fooling the deep learning models. We contribute in this
direction and propose a novel adversarial defense method called Synonym
Encoding Method (SEM). Specifically, SEM inserts an encoder before the input
layer of the target model to map each cluster of synonyms to a unique encoding
and trains the model to eliminate possible adversarial perturbations without
modifying the network architecture or adding extra data. Extensive experiments
demonstrate that SEM can effectively defend the current synonym substitution
based attacks and block the transferability of adversarial examples. SEM is
also easy and efficient to scale to large models and big datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaosen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yichen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1"&gt;Kun He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum. (arXiv:2102.07367v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07367</id>
        <link href="http://arxiv.org/abs/2102.07367"/>
        <updated>2021-06-16T01:21:10.857Z</updated>
        <summary type="html"><![CDATA[This paper proposes a new algorithm -- the \underline{S}ingle-timescale
Do\underline{u}ble-momentum \underline{St}ochastic
\underline{A}pprox\underline{i}matio\underline{n} (SUSTAIN) -- for tackling
stochastic unconstrained bilevel optimization problems. We focus on bilevel
problems where the lower level subproblem is strongly-convex and the upper
level objective function is smooth. Unlike prior works which rely on
\emph{two-timescale} or \emph{double loop} techniques, we design a stochastic
momentum-assisted gradient estimator for both the upper and lower level
updates. The latter allows us to control the error in the stochastic gradient
updates due to inaccurate solution to both subproblems. If the upper objective
function is smooth but possibly non-convex, we show that {\aname}~requires
$\mathcal{O}(\epsilon^{-3/2})$ iterations (each using ${\cal O}(1)$ samples) to
find an $\epsilon$-stationary solution. The $\epsilon$-stationary solution is
defined as the point whose squared norm of the gradient of the outer function
is less than or equal to $\epsilon$. The total number of stochastic gradient
samples required for the upper and lower level objective functions matches the
best-known complexity for single-level stochastic gradient algorithms. We also
analyze the case when the upper level objective function is strongly-convex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Khanduri_P/0/1/0/all/0/1"&gt;Prashant Khanduri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Siliang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hong_M/0/1/0/all/0/1"&gt;Mingyi Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wai_H/0/1/0/all/0/1"&gt;Hoi-To Wai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuoran Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-16T01:21:10.832Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08318</id>
        <link href="http://arxiv.org/abs/2106.08318"/>
        <updated>2021-06-16T01:21:10.797Z</updated>
        <summary type="html"><![CDATA[How can neural networks be trained on large-volume temporal data efficiently?
To compute the gradients required to update parameters, backpropagation blocks
computations until the forward and backward passes are completed. For temporal
signals, this introduces high latency and hinders real-time learning. It also
creates a coupling between consecutive layers, which limits model parallelism
and increases memory consumption. In this paper, we build upon Sideways, which
avoids blocking by propagating approximate gradients forward in time, and we
propose mechanisms for temporal integration of information based on different
variants of skip connections. We also show how to decouple computation and
delegate individual neural modules to different devices, allowing distributed
and parallel training. The proposed Skip-Sideways achieves low latency
training, model parallelism, and, importantly, is capable of extracting
temporal features, leading to more stable training and improved performance on
real-world action recognition video datasets such as HMDB51, UCF101, and the
large-scale Kinetics-600. Finally, we also show that models trained with
Skip-Sideways generate better future frames than Sideways models, and hence
they can better utilize motion cues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1"&gt;Dimitrios Vytiniotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1"&gt;Grzegorz Swirszcz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1"&gt;Viorica Patraucean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Joao Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encouraging Intra-Class Diversity Through a Reverse Contrastive Loss for Better Single-Source Domain Generalization. (arXiv:2106.07916v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07916</id>
        <link href="http://arxiv.org/abs/2106.07916"/>
        <updated>2021-06-16T01:21:10.753Z</updated>
        <summary type="html"><![CDATA[Traditional deep learning algorithms often fail to generalize when they are
tested outside of the domain of training data. Because data distributions can
change dynamically in real-life applications once a learned model is deployed,
in this paper we are interested in single-source domain generalization (SDG)
which aims to develop deep learning algorithms able to generalize from a single
training domain where no information about the test domain is available at
training time. Firstly, we design two simple MNISTbased SDG benchmarks, namely
MNIST Color SDG-MP and MNIST Color SDG-UP, which highlight the two different
fundamental SDG issues of increasing difficulties: 1) a class-correlated
pattern in the training domain is missing (SDG-MP), or 2) uncorrelated with the
class (SDG-UP), in the testing data domain. This is in sharp contrast with the
current domain generalization (DG) benchmarks which mix up different
correlation and variation factors and thereby make hard to disentangle success
or failure factors when benchmarking DG algorithms. We further evaluate several
state-of-the-art SDG algorithms through our simple benchmark, namely MNIST
Color SDG-MP, and show that the issue SDG-MP is largely unsolved despite of a
decade of efforts in developing DG algorithms. Finally, we also propose a
partially reversed contrastive loss to encourage intra-class diversity and find
less strongly correlated patterns, to deal with SDG-MP and show that the
proposed approach is very effective on our MNIST Color SDG-MP benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duboudin_T/0/1/0/all/0/1"&gt;Thomas Duboudin&lt;/a&gt; (imagine), &lt;a href="http://arxiv.org/find/cs/1/au:+Dellandrea_E/0/1/0/all/0/1"&gt;Emmanuel Dellandr&amp;#xe9;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abgrall_C/0/1/0/all/0/1"&gt;Corentin Abgrall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henaff_G/0/1/0/all/0/1"&gt;Gilles H&amp;#xe9;naff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProsoBeast Prosody Annotation Tool. (arXiv:2104.02397v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02397</id>
        <link href="http://arxiv.org/abs/2104.02397"/>
        <updated>2021-06-16T01:21:10.731Z</updated>
        <summary type="html"><![CDATA[The labelling of speech corpora is a laborious and time-consuming process.
The ProsoBeast Annotation Tool seeks to ease and accelerate this process by
providing an interactive 2D representation of the prosodic landscape of the
data, in which contours are distributed based on their similarity. This
interactive map allows the user to inspect and label the utterances. The tool
integrates several state-of-the-art methods for dimensionality reduction and
feature embedding, including variational autoencoders. The user can use these
to find a good representation for their data. In addition, as most of these
methods are stochastic, each can be used to generate an unlimited number of
different prosodic maps. The web app then allows the user to seamlessly switch
between these alternative representations in the annotation process.
Experiments with a sample prosodically rich dataset have shown that the tool
manages to find good representations of varied data and is helpful both for
annotation and label correction. The tool is released as free software for use
by the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gerazov_B/0/1/0/all/0/1"&gt;Branislav Gerazov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wagner_M/0/1/0/all/0/1"&gt;Michael Wagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speaker Diarization using Two-pass Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings. (arXiv:2104.02469v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02469</id>
        <link href="http://arxiv.org/abs/2104.02469"/>
        <updated>2021-06-16T01:21:10.709Z</updated>
        <summary type="html"><![CDATA[Many modern systems for speaker diarization, such as the recently-developed
VBx approach, rely on clustering of DNN speaker embeddings followed by
resegmentation. Two problems with this approach are that the DNN is not
directly optimized for this task, and the parameters need significant retuning
for different applications. We have recently presented progress in this
direction with a Leave-One-Out Gaussian PLDA (LGP) clustering algorithm and an
approach to training the DNN such that embeddings directly optimize performance
of this scoring method. This paper presents a new two-pass version of this
system, where the second pass uses finer time resolution to significantly
improve overall performance. For the Callhome corpus, we achieve the first
published error rate below 4% without any task-dependent parameter tuning. We
also show significant progress towards a robust single solution for multiple
diarization tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Karra_K/0/1/0/all/0/1"&gt;Kiran Karra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+McCree_A/0/1/0/all/0/1"&gt;Alan McCree&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NNrepair: Constraint-based Repair of Neural Network Classifiers. (arXiv:2103.12535v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12535</id>
        <link href="http://arxiv.org/abs/2103.12535"/>
        <updated>2021-06-16T01:21:10.700Z</updated>
        <summary type="html"><![CDATA[We present NNrepair, a constraint-based technique for repairing neural
network classifiers. The technique aims to fix the logic of the network at an
intermediate layer or at the last layer. NNrepair first uses fault localization
to find potentially faulty network parameters (such as the weights) and then
performs repair using constraint solving to apply small modifications to the
parameters to remedy the defects. We present novel strategies to enable precise
yet efficient repair such as inferring correctness specifications to act as
oracles for intermediate layer repair, and generation of experts for each
class. We demonstrate the technique in the context of three different
scenarios: (1) Improving the overall accuracy of a model, (2) Fixing security
vulnerabilities caused by poisoning of training data and (3) Improving the
robustness of the network against adversarial attacks. Our evaluation on MNIST
and CIFAR-10 models shows that NNrepair can improve the accuracy by 45.56
percentage points on poisoned data and 10.40 percentage points on adversarial
data. NNrepair also provides small improvement in the overall accuracy of
models, without requiring new data or re-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1"&gt;Muhammad Usman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopinath_D/0/1/0/all/0/1"&gt;Divya Gopinath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Youcheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noller_Y/0/1/0/all/0/1"&gt;Yannic Noller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasareanu_C/0/1/0/all/0/1"&gt;Corina Pasareanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inexact-ADMM Based Federated Meta-Learning for Fast and Continual Edge Learning. (arXiv:2012.08677v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08677</id>
        <link href="http://arxiv.org/abs/2012.08677"/>
        <updated>2021-06-16T01:21:10.617Z</updated>
        <summary type="html"><![CDATA[In order to meet the requirements for performance, safety, and latency in
many IoT applications, intelligent decisions must be made right here right now
at the network edge. However, the constrained resources and limited local data
amount pose significant challenges to the development of edge AI. To overcome
these challenges, we explore continual edge learning capable of leveraging the
knowledge transfer from previous tasks. Aiming to achieve fast and continual
edge learning, we propose a platform-aided federated meta-learning architecture
where edge nodes collaboratively learn a meta-model, aided by the knowledge
transfer from prior tasks. The edge learning problem is cast as a regularized
optimization problem, where the valuable knowledge learned from previous tasks
is extracted as regularization. Then, we devise an ADMM based federated
meta-learning algorithm, namely ADMM-FedMeta, where ADMM offers a natural
mechanism to decompose the original problem into many subproblems which can be
solved in parallel across edge nodes and the platform. Further, a variant of
inexact-ADMM method is employed where the subproblems are `solved' via linear
approximation as well as Hessian estimation to reduce the computational cost
per round to $\mathcal{O}(n)$. We provide a comprehensive analysis of
ADMM-FedMeta, in terms of the convergence properties, the rapid adaptation
performance, and the forgetting effect of prior knowledge transfer, for the
general non-convex case. Extensive experimental studies demonstrate the
effectiveness and efficiency of ADMM-FedMeta, and showcase that it
substantially outperforms the existing baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1"&gt;Sheng Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Ju Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Jiang Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Sen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junshan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoupling Value and Policy for Generalization in Reinforcement Learning. (arXiv:2102.10330v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10330</id>
        <link href="http://arxiv.org/abs/2102.10330"/>
        <updated>2021-06-16T01:21:10.616Z</updated>
        <summary type="html"><![CDATA[Standard deep reinforcement learning algorithms use a shared representation
for the policy and value function, especially when training directly from
images. However, we argue that more information is needed to accurately
estimate the value function than to learn the optimal policy. Consequently, the
use of a shared representation for the policy and value function can lead to
overfitting. To alleviate this problem, we propose two approaches which are
combined to create IDAAC: Invariant Decoupled Advantage Actor-Critic. First,
IDAAC decouples the optimization of the policy and value function, using
separate networks to model them. Second, it introduces an auxiliary loss which
encourages the representation to be invariant to task-irrelevant properties of
the environment. IDAAC shows good generalization to unseen environments,
achieving a new state-of-the-art on the Procgen benchmark and outperforming
popular methods on DeepMind Control tasks with distractors. Our implementation
is available at https://github.com/rraileanu/idaac.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1"&gt;Roberta Raileanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1"&gt;Rob Fergus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust learning under clean-label attack. (arXiv:2103.00671v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00671</id>
        <link href="http://arxiv.org/abs/2103.00671"/>
        <updated>2021-06-16T01:21:10.608Z</updated>
        <summary type="html"><![CDATA[We study the problem of robust learning under clean-label data-poisoning
attacks, where the attacker injects (an arbitrary set of) correctly-labeled
examples to the training set to fool the algorithm into making mistakes on
specific test instances at test time. The learning goal is to minimize the
attackable rate (the probability mass of attackable test instances), which is
more difficult than optimal PAC learning. As we show, any robust algorithm with
diminishing attackable rate can achieve the optimal dependence on $\epsilon$ in
its PAC sample complexity, i.e., $O(1/\epsilon)$. On the other hand, the
attackable rate might be large even for some optimal PAC learners, e.g., SVM
for linear classifiers. Furthermore, we show that the class of linear
hypotheses is not robustly learnable when the data distribution has zero margin
and is robustly learnable in the case of positive margin but requires sample
complexity exponential in the dimension. For a general hypothesis class with
bounded VC dimension, if the attacker is limited to add at most $t>0$ poison
examples, the optimal robust learning sample complexity grows almost linearly
with $t$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blum_A/0/1/0/all/0/1"&gt;Avrim Blum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1"&gt;Steve Hanneke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jian Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1"&gt;Han Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keyword Transformer: A Self-Attention Model for Keyword Spotting. (arXiv:2104.00769v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00769</id>
        <link href="http://arxiv.org/abs/2104.00769"/>
        <updated>2021-06-16T01:21:10.607Z</updated>
        <summary type="html"><![CDATA[The Transformer architecture has been successful across many domains,
including natural language processing, computer vision and speech recognition.
In keyword spotting, self-attention has primarily been used on top of
convolutional or recurrent encoders. We investigate a range of ways to adapt
the Transformer architecture to keyword spotting and introduce the Keyword
Transformer (KWT), a fully self-attentional architecture that exceeds
state-of-the-art performance across multiple tasks without any pre-training or
additional data. Surprisingly, this simple architecture outperforms more
complex models that mix convolutional, recurrent and attentive layers. KWT can
be used as a drop-in replacement for these models, setting two new benchmark
records on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on
the 12 and 35-command tasks respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Berg_A/0/1/0/all/0/1"&gt;Axel Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+OConnor_M/0/1/0/all/0/1"&gt;Mark O&amp;#x27;Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cruz_M/0/1/0/all/0/1"&gt;Miguel Tairum Cruz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. (arXiv:1901.10002v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1901.10002</id>
        <link href="http://arxiv.org/abs/1901.10002"/>
        <updated>2021-06-16T01:21:10.491Z</updated>
        <summary type="html"><![CDATA[As machine learning (ML) increasingly affects people and society, awareness
of its potential unwanted consequences has also grown. To anticipate, prevent,
and mitigate undesirable downstream consequences, it is critical that we
understand when and how harm might be introduced throughout the ML life cycle.
In this paper, we provide a framework that identifies seven distinct potential
sources of downstream harm in machine learning, spanning data collection,
development, and deployment. In doing so, we aim to facilitate more productive
and precise communication around these issues, as well as more direct,
application-grounded ways to mitigate them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_H/0/1/0/all/0/1"&gt;Harini Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1"&gt;John V. Guttag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Accuracy-Efficiency Trade-Offs as a Means for Holding Distributed ML Systems Accountable. (arXiv:2007.02203v5 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02203</id>
        <link href="http://arxiv.org/abs/2007.02203"/>
        <updated>2021-06-16T01:21:10.464Z</updated>
        <summary type="html"><![CDATA[Trade-offs between accuracy and efficiency are found in multiple
non-computing domains, such as law and public health, which have developed
rules and heuristics to guide how to balance the two in conditions of
uncertainty. While accuracy-efficiency trade-offs are also commonly
acknowledged in some areas of computer science, their policy implications
remain poorly examined. Drawing on risk assessment practices in the US, we
argue that, since examining accuracy-efficiency trade-offs has been useful for
guiding governance in other domains, explicitly framing such trade-offs in
computing is similarly useful for the governance of computer systems. Our
discussion focuses on real-time distributed ML systems; understanding the
policy implications in this area is particularly urgent because such systems,
which include autonomous vehicles, tend to be high-stakes and safety-critical.
We describe how the trade-off takes shape for these systems, highlight gaps
between existing US risk assessment standards and what these systems require in
order to be properly assessed, and make specific calls to action to facilitate
accountability when hypothetical risks become realized as accidents in the real
world. We close by discussing how such accountability mechanisms encourage more
just, transparent governance aligned with public values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1"&gt;A. Feder Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1"&gt;Karen Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Label Propagation for Semi-Supervised Speaker Identification. (arXiv:2106.08207v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08207</id>
        <link href="http://arxiv.org/abs/2106.08207"/>
        <updated>2021-06-16T01:21:10.431Z</updated>
        <summary type="html"><![CDATA[Speaker identification in the household scenario (e.g., for smart speakers)
is typically based on only a few enrollment utterances but a much larger set of
unlabeled data, suggesting semisupervised learning to improve speaker profiles.
We propose a graph-based semi-supervised learning approach for speaker
identification in the household scenario, to leverage the unlabeled speech
samples. In contrast to most of the works in speaker recognition that focus on
speaker-discriminative embeddings, this work focuses on speaker label inference
(scoring). Given a pre-trained embedding extractor, graph-based learning allows
us to integrate information about both labeled and unlabeled utterances.
Considering each utterance as a graph node, we represent pairwise utterance
similarity scores as edge weights. Graphs are constructed per household, and
speaker identities are propagated to unlabeled nodes to optimize a global
consistency criterion. We show in experiments on the VoxCeleb dataset that this
approach makes effective use of unlabeled data and improves speaker
identification accuracy compared to two state-of-the-art scoring methods as
well as their semi-supervised variants based on pseudo-labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_V/0/1/0/all/0/1"&gt;Venkatesh Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1"&gt;Andreas Stolcke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Modular Should Neural Module Networks Be for Systematic Generalization?. (arXiv:2106.08170v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08170</id>
        <link href="http://arxiv.org/abs/2106.08170"/>
        <updated>2021-06-16T01:21:10.424Z</updated>
        <summary type="html"><![CDATA[Neural Module Networks (NMNs) aim at Visual Question Answering (VQA) via
composition of modules that tackle a sub-task. NMNs are a promising strategy to
achieve systematic generalization, i.e. overcoming biasing factors in the
training distribution. However, the aspects of NMNs that facilitate systematic
generalization are not fully understood. In this paper, we demonstrate that the
stage and the degree at which modularity is defined has large influence on
systematic generalization. In a series of experiments on three VQA datasets
(MNIST with multiple attributes, SQOOP, and CLEVR-CoGenT), our results reveal
that tuning the degree of modularity in the network, especially at the image
encoder stage, reaches substantially higher systematic generalization. These
findings lead to new NMN architectures that outperform previous ones in terms
of systematic generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1"&gt;Vanessa D&amp;#x27;Amario&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1"&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Identification Through Transformers. (arXiv:2106.08185v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08185</id>
        <link href="http://arxiv.org/abs/2106.08185"/>
        <updated>2021-06-16T01:21:10.417Z</updated>
        <summary type="html"><![CDATA[Kernel selection plays a central role in determining the performance of
Gaussian Process (GP) models, as the chosen kernel determines both the
inductive biases and prior support of functions under the GP prior. This work
addresses the challenge of constructing custom kernel functions for
high-dimensional GP regression models. Drawing inspiration from recent progress
in deep learning, we introduce a novel approach named KITT: Kernel
Identification Through Transformers. KITT exploits a transformer-based
architecture to generate kernel recommendations in under 0.1 seconds, which is
several orders of magnitude faster than conventional kernel search algorithms.
We train our model using synthetic data generated from priors over a vocabulary
of known kernels. By exploiting the nature of the self-attention mechanism,
KITT is able to process datasets with inputs of arbitrary dimension. We
demonstrate that kernels chosen by KITT yield strong performance over a diverse
collection of regression benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Simpson_F/0/1/0/all/0/1"&gt;Fergus Simpson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Davies_I/0/1/0/all/0/1"&gt;Ian Davies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lalchand_V/0/1/0/all/0/1"&gt;Vidhi Lalchand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vullo_A/0/1/0/all/0/1"&gt;Alessandro Vullo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Durrande_N/0/1/0/all/0/1"&gt;Nicolas Durrande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rasmussen_C/0/1/0/all/0/1"&gt;Carl Rasmussen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning with Kernel Dependence Maximization. (arXiv:2106.08320v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08320</id>
        <link href="http://arxiv.org/abs/2106.08320"/>
        <updated>2021-06-16T01:21:10.380Z</updated>
        <summary type="html"><![CDATA[We approach self-supervised learning of image representations from a
statistical dependence perspective, proposing Self-Supervised Learning with the
Hilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes
dependence between representations of transformed versions of an image and the
image identity, while minimizing the kernelized variance of those features.
This self-supervised learning framework yields a new understanding of InfoNCE,
a variational lower bound on the mutual information (MI) between different
transformations. While the MI itself is known to have pathologies which can
result in meaningless representations being learned, its bound is much better
behaved: we show that it implicitly approximates SSL-HSIC (with a slightly
different regularizer). Our approach also gives us insight into BYOL, since
SSL-HSIC similarly learns local neighborhoods of samples. SSL-HSIC allows us to
directly optimize statistical dependence in time linear in the batch size,
without restrictive data assumptions or indirect mutual information estimators.
Trained with or without a target network, SSL-HSIC matches the current
state-of-the-art for standard linear evaluation on ImageNet, semi-supervised
learning and transfer to other classification and vision tasks such as semantic
segmentation, depth estimation and object recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yazhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pogodin_R/0/1/0/all/0/1"&gt;Roman Pogodin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1"&gt;Danica J. Sutherland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evading Malware Classifiers via Monte Carlo Mutant Feature Discovery. (arXiv:2106.07860v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.07860</id>
        <link href="http://arxiv.org/abs/2106.07860"/>
        <updated>2021-06-16T01:21:10.347Z</updated>
        <summary type="html"><![CDATA[The use of Machine Learning has become a significant part of malware
detection efforts due to the influx of new malware, an ever changing threat
landscape, and the ability of Machine Learning methods to discover meaningful
distinctions between malicious and benign software. Antivirus vendors have also
begun to widely utilize malware classifiers based on dynamic and static malware
analysis features. Therefore, a malware author might make evasive binary
modifications against Machine Learning models as part of the malware
development life cycle to execute an attack successfully. This makes the
studying of possible classifier evasion strategies an essential part of cyber
defense against malice. To this extent, we stage a grey box setup to analyze a
scenario where the malware author does not know the target classifier
algorithm, and does not have access to decisions made by the classifier, but
knows the features used in training. In this experiment, a malicious actor
trains a surrogate model using the EMBER-2018 dataset to discover binary
mutations that cause an instance to be misclassified via a Monte Carlo tree
search. Then, mutated malware is sent to the victim model that takes the place
of an antivirus API to test whether it can evade detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boutsikas_J/0/1/0/all/0/1"&gt;John Boutsikas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eren_M/0/1/0/all/0/1"&gt;Maksim E. Eren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varga_C/0/1/0/all/0/1"&gt;Charles Varga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1"&gt;Edward Raff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matuszek_C/0/1/0/all/0/1"&gt;Cynthia Matuszek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nicholas_C/0/1/0/all/0/1"&gt;Charles Nicholas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved SVRG for quadratic functions. (arXiv:2006.01017v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01017</id>
        <link href="http://arxiv.org/abs/2006.01017"/>
        <updated>2021-06-16T01:21:10.341Z</updated>
        <summary type="html"><![CDATA[We analyse an iterative algorithm to minimize quadratic functions whose
Hessian matrix $H$ is the expectation of a random symmetric $d\times d$ matrix.
The algorithm is a variant of the stochastic variance reduced gradient (SVRG).
In several applications, including least-squares regressions, ridge
regressions, linear discriminant analysis and regularized linear discriminant
analysis, the running time of each iteration is proportional to $d$. Under
smoothness and convexity conditions, the algorithm has linear convergence. When
applied to quadratic functions, our analysis improves the state-of-the-art
performance of SVRG up to a logarithmic factor. Furthermore, for
well-conditioned quadratic problems, our analysis improves the state-of-the-art
running times of accelerated SVRG, and is better than the known matching lower
bound, by a logarithmic factor. Our theoretical results are backed with
numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kahale_N/0/1/0/all/0/1"&gt;Nabil Kahale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Multi-objective Policy Optimization as a Tool for Reinforcement Learning. (arXiv:2106.08199v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08199</id>
        <link href="http://arxiv.org/abs/2106.08199"/>
        <updated>2021-06-16T01:21:10.334Z</updated>
        <summary type="html"><![CDATA[Many advances that have improved the robustness and efficiency of deep
reinforcement learning (RL) algorithms can, in one way or another, be
understood as introducing additional objectives, or constraints, in the policy
optimization step. This includes ideas as far ranging as exploration bonuses,
entropy regularization, and regularization toward teachers or data priors when
learning from experts or in offline RL. Often, task reward and auxiliary
objectives are in conflict with each other and it is therefore natural to treat
these examples as instances of multi-objective (MO) optimization problems. We
study the principles underlying MORL and introduce a new algorithm,
Distillation of a Mixture of Experts (DiME), that is intuitive and
scale-invariant under some conditions. We highlight its strengths on standard
MO benchmark problems and consider case studies in which we recast offline RL
and learning from experts as MO problems. This leads to a natural algorithmic
formulation that sheds light on the connection between existing approaches. For
offline RL, we use the MO perspective to derive a simple algorithm, that
optimizes for the standard RL objective plus a behavioral cloning term. This
outperforms state-of-the-art on two established offline RL benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdolmaleki_A/0/1/0/all/0/1"&gt;Abbas Abdolmaleki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Sandy H. Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vezzani_G/0/1/0/all/0/1"&gt;Giulia Vezzani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahriari_B/0/1/0/all/0/1"&gt;Bobak Shahriari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Springenberg_J/0/1/0/all/0/1"&gt;Jost Tobias Springenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Shruti Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+TB_D/0/1/0/all/0/1"&gt;Dhruva TB&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byravan_A/0/1/0/all/0/1"&gt;Arunkumar Byravan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bousmalis_K/0/1/0/all/0/1"&gt;Konstantinos Bousmalis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1"&gt;Andras Gyorgy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesvari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1"&gt;Raia Hadsell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1"&gt;Nicolas Heess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1"&gt;Martin Riedmiller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simon Says: Evaluating and Mitigating Bias in Pruned Neural Networks with Knowledge Distillation. (arXiv:2106.07849v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07849</id>
        <link href="http://arxiv.org/abs/2106.07849"/>
        <updated>2021-06-16T01:21:10.326Z</updated>
        <summary type="html"><![CDATA[In recent years the ubiquitous deployment of AI has posed great concerns in
regards to algorithmic bias, discrimination, and fairness. Compared to
traditional forms of bias or discrimination caused by humans, algorithmic bias
generated by AI is more abstract and unintuitive therefore more difficult to
explain and mitigate. A clear gap exists in the current literature on
evaluating and mitigating bias in pruned neural networks. In this work, we
strive to tackle the challenging issues of evaluating, mitigating, and
explaining induced bias in pruned neural networks. Our paper makes three
contributions. First, we propose two simple yet effective metrics, Combined
Error Variance (CEV) and Symmetric Distance Error (SDE), to quantitatively
evaluate the induced bias prevention quality of pruned models. Second, we
demonstrate that knowledge distillation can mitigate induced bias in pruned
neural networks, even with unbalanced datasets. Third, we reveal that model
similarity has strong correlations with pruning induced bias, which provides a
powerful method to explain why bias occurs in pruned neural networks. Our code
is available at https://github.com/codestar12/pruning-distilation-bias]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blakeney_C/0/1/0/all/0/1"&gt;Cody Blakeney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huish_N/0/1/0/all/0/1"&gt;Nathaniel Huish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yan Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1"&gt;Ziliang Zong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active feature selection discovers minimal gene-sets for classifying cell-types and disease states in single-cell mRNA-seq data. (arXiv:2106.08317v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2106.08317</id>
        <link href="http://arxiv.org/abs/2106.08317"/>
        <updated>2021-06-16T01:21:10.312Z</updated>
        <summary type="html"><![CDATA[Sequencing costs currently prohibit the application of single cell mRNA-seq
for many biological and clinical tasks of interest. Here, we introduce an
active learning framework that constructs compressed gene sets that enable high
accuracy classification of cell-types and physiological states while analyzing
a minimal number of gene transcripts. Our active feature selection procedure
constructs gene sets through an iterative cell-type classification task where
misclassified cells are examined at each round to identify maximally
informative genes through an `active' support vector machine (SVM) classifier.
Our active SVM procedure automatically identifies gene sets that enables
$>90\%$ cell-type classification accuracy in the Tabula Muris mouse tissue
survey as well as a $\sim 40$ gene set that enables classification of multiple
myeloma patient samples with $>95\%$ accuracy. Broadly, the discovery of
compact but highly informative gene sets might enable drastic reductions in
sequencing requirements for applications of single-cell mRNA-seq.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoqiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sisi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Thomson_M/0/1/0/all/0/1"&gt;Matt Thomson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sliced Iterative Normalizing Flows. (arXiv:2007.00674v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00674</id>
        <link href="http://arxiv.org/abs/2007.00674"/>
        <updated>2021-06-16T01:21:10.292Z</updated>
        <summary type="html"><![CDATA[We develop an iterative (greedy) deep learning (DL) algorithm which is able
to transform an arbitrary probability distribution function (PDF) into the
target PDF. The model is based on iterative Optimal Transport of a series of 1D
slices, matching on each slice the marginal PDF to the target. The axes of the
orthogonal slices are chosen to maximize the PDF difference using Wasserstein
distance at each iteration, which enables the algorithm to scale well to high
dimensions. As special cases of this algorithm, we introduce two sliced
iterative Normalizing Flow (SINF) models, which map from the data to the latent
space (GIS) and vice versa (SIG). We show that SIG is able to generate high
quality samples of image datasets, which match the GAN benchmarks, while GIS
obtains competitive results on density estimation tasks compared to the density
trained NFs, and is more stable, faster, and achieves higher $p(x)$ when
trained on small training sets. SINF approach deviates significantly from the
current DL paradigm, as it is greedy and does not use concepts such as
mini-batching, stochastic gradient descent and gradient back-propagation
through deep layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Biwei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seljak_U/0/1/0/all/0/1"&gt;Uros Seljak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08043</id>
        <link href="http://arxiv.org/abs/2106.08043"/>
        <updated>2021-06-16T01:21:10.274Z</updated>
        <summary type="html"><![CDATA[The vast majority of existing methods and systems for causal inference assume
that all variables under consideration are categorical or numerical (e.g.,
gender, price, blood pressure, enrollment). In this paper, we present
CausalNLP, a toolkit for inferring causality from observational data that
includes text in addition to traditional numerical and categorical variables.
CausalNLP employs the use of meta-learners for treatment effect estimation and
supports using raw text and its linguistic properties as both a treatment and a
"controlled-for" variable (e.g., confounder). The library is open-source and
available at: https://github.com/amaiya/causalnlp.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1"&gt;Arun S. Maiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation. (arXiv:1911.01529v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.01529</id>
        <link href="http://arxiv.org/abs/1911.01529"/>
        <updated>2021-06-16T01:21:10.268Z</updated>
        <summary type="html"><![CDATA[Deep learning approaches have become the standard solution to many problems
in computer vision and robotics, but obtaining sufficient training data in high
enough quality is challenging, as human labor is error prone, time consuming,
and expensive. Solutions based on simulation have become more popular in recent
years, but the gap between simulation and reality is still a major issue. In
this paper, we introduce a novel method for augmenting synthetic image data
through unsupervised image-to-image translation by applying the style of real
world images to simulated images with open source frameworks. The generated
dataset is combined with conventional augmentation methods and is then applied
to a neural network model running in real-time on autonomous soccer robots. Our
evaluation shows a significant improvement compared to models trained on images
generated entirely in simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blumenkamp_J/0/1/0/all/0/1"&gt;Jan Blumenkamp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baude_A/0/1/0/all/0/1"&gt;Andreas Baude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laue_T/0/1/0/all/0/1"&gt;Tim Laue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PairConnect: A Compute-Efficient MLP Alternative to Attention. (arXiv:2106.08235v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08235</id>
        <link href="http://arxiv.org/abs/2106.08235"/>
        <updated>2021-06-16T01:21:10.262Z</updated>
        <summary type="html"><![CDATA[Transformer models have demonstrated superior performance in natural language
processing. The dot product self-attention in Transformer allows us to model
interactions between words. However, this modeling comes with significant
computational overhead. In this work, we revisit the memory-compute trade-off
associated with Transformer, particularly multi-head attention, and show a
memory-heavy but significantly more compute-efficient alternative to
Transformer. Our proposal, denoted as PairConnect, a multilayer perceptron
(MLP), models the pairwise interaction between words by explicit pairwise word
embeddings. As a result, PairConnect substitutes self dot product with a simple
embedding lookup. We show mathematically that despite being an MLP, our
compute-efficient PairConnect is strictly more expressive than Transformer. Our
experiment on language modeling tasks suggests that PairConnect could achieve
comparable results with Transformer while reducing the computational cost
associated with inference significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhaozhuo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Minghao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RFpredInterval: An R Package for Prediction Intervals with Random Forests and Boosted Forests. (arXiv:2106.08217v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08217</id>
        <link href="http://arxiv.org/abs/2106.08217"/>
        <updated>2021-06-16T01:21:10.255Z</updated>
        <summary type="html"><![CDATA[Like many predictive models, random forests provide a point prediction for a
new observation. Besides the point prediction, it is important to quantify the
uncertainty in the prediction. Prediction intervals provide information about
the reliability of the point predictions. We have developed a comprehensive R
package, RFpredInterval, that integrates 16 methods to build prediction
intervals with random forests and boosted forests. The methods implemented in
the package are a new method to build prediction intervals with boosted forests
(PIBF) and 15 different variants to produce prediction intervals with random
forests proposed by Roy and Larocque (2020). We perform an extensive simulation
study and apply real data analyses to compare the performance of the proposed
method to ten existing methods to build prediction intervals with random
forests. The results show that the proposed method is very competitive and,
globally, it outperforms the competing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Alakus_C/0/1/0/all/0/1"&gt;Cansu Alakus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Larocque_D/0/1/0/all/0/1"&gt;Denis Larocque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Labbe_A/0/1/0/all/0/1"&gt;Aurelie Labbe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CRFL: Certifiably Robust Federated Learning against Backdoor Attacks. (arXiv:2106.08283v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08283</id>
        <link href="http://arxiv.org/abs/2106.08283"/>
        <updated>2021-06-16T01:21:10.205Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) as a distributed learning paradigm that aggregates
information from diverse clients to train a shared global model, has
demonstrated great success. However, malicious clients can perform poisoning
attacks and model replacement to introduce backdoors into the trained global
model. Although there have been intensive studies designing robust aggregation
methods and empirical robust federated training protocols against backdoors,
existing approaches lack robustness certification. This paper provides the
first general framework, Certifiably Robust Federated Learning (CRFL), to train
certifiably robust FL models against backdoors. Our method exploits clipping
and smoothing on model parameters to control the global model smoothness, which
yields a sample-wise robustness certification on backdoors with limited
magnitude. Our certification also specifies the relation to federated learning
parameters, such as poisoning ratio on instance level, number of attackers, and
training iterations. Practically, we conduct comprehensive experiments across a
range of federated datasets, and provide the first benchmark for certified
robustness against backdoor attacks in federated learning. Our code is
available at https://github.com/AI-secure/CRFL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chulin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minghao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Autoregressive Electron Redistribution Modeling for Reaction Prediction. (arXiv:2106.07801v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.07801</id>
        <link href="http://arxiv.org/abs/2106.07801"/>
        <updated>2021-06-16T01:21:10.185Z</updated>
        <summary type="html"><![CDATA[Reliably predicting the products of chemical reactions presents a fundamental
challenge in synthetic chemistry. Existing machine learning approaches
typically produce a reaction product by sequentially forming its subparts or
intermediate molecules. Such autoregressive methods, however, not only require
a pre-defined order for the incremental construction but preclude the use of
parallel decoding for efficient computation. To address these issues, we devise
a non-autoregressive learning paradigm that predicts reaction in one shot.
Leveraging the fact that chemical reactions can be described as a
redistribution of electrons in molecules, we formulate a reaction as an
arbitrary electron flow and predict it with a novel multi-pointer decoding
network. Experiments on the USPTO-MIT dataset show that our approach has
established a new state-of-the-art top-1 accuracy and achieves at least 27
times inference speedup over the state-of-the-art methods. Also, our
predictions are easier for chemists to interpret owing to predicting the
electron flows.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Bi_H/0/1/0/all/0/1"&gt;Hangrui Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hengyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chence Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Coley_C/0/1/0/all/0/1"&gt;Connor Coley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text Generation with Efficient (Soft) Q-Learning. (arXiv:2106.07704v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07704</id>
        <link href="http://arxiv.org/abs/2106.07704"/>
        <updated>2021-06-16T01:21:10.177Z</updated>
        <summary type="html"><![CDATA[Maximum likelihood estimation (MLE) is the predominant algorithm for training
text generation models. This paradigm relies on direct supervision examples,
which is not applicable to many applications, such as generating adversarial
attacks or generating prompts to control language models. Reinforcement
learning (RL) on the other hand offers a more flexible solution by allowing
users to plug in arbitrary task metrics as reward. Yet previous RL algorithms
for text generation, such as policy gradient (on-policy RL) and Q-learning
(off-policy RL), are often notoriously inefficient or unstable to train due to
the large sequence space and the sparse reward received only at the end of
sequences. In this paper, we introduce a new RL formulation for text generation
from the soft Q-learning perspective. It further enables us to draw from the
latest RL advances, such as path consistency learning, to combine the best of
on-/off-policy updates, and learn effectively from sparse reward. We apply the
approach to a wide range of tasks, including learning from noisy/negative
examples, adversarial attacks, and prompt generation. Experiments show our
approach consistently outperforms both task-specialized algorithms and the
previous RL methods. On standard supervised tasks where MLE prevails, our
approach also achieves competitive performance and stability by training text
generation from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Han Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1"&gt;Bowen Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengzhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"&gt;Eric P. Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhiting Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Prevent Leakage: Privacy-Preserving Inference in the Mobile Cloud. (arXiv:1912.08421v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.08421</id>
        <link href="http://arxiv.org/abs/1912.08421"/>
        <updated>2021-06-16T01:21:10.170Z</updated>
        <summary type="html"><![CDATA[Powered by machine learning services in the cloud, numerous learning-driven
mobile applications are gaining popularity in the market. As deep learning
tasks are mostly computation-intensive, it has become a trend to process raw
data on devices and send the deep neural network (DNN) features to the cloud,
where the features are further processed to return final results. However,
there is always unexpected leakage with the release of features, with which an
adversary could infer a significant amount of information about the original
data. We propose a privacy-preserving reinforcement learning framework on top
of the mobile cloud infrastructure from the perspective of DNN structures. The
framework aims to learn a policy to modify the base DNNs to prevent information
leakage while maintaining high inference accuracy. The policy can also be
readily transferred to large-size DNNs to speed up learning. Extensive
evaluations on a variety of DNNs have shown that our framework can successfully
find privacy-preserving DNN structures to defend different privacy attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1"&gt;Liyao Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Congcong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanshi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence of Deep Learning with Differential Privacy. (arXiv:2106.07830v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07830</id>
        <link href="http://arxiv.org/abs/2106.07830"/>
        <updated>2021-06-16T01:21:10.161Z</updated>
        <summary type="html"><![CDATA[In deep learning with differential privacy (DP), the neural network achieves
the privacy usually at the cost of slower convergence (and thus lower
performance) than its non-private counterpart. This work gives the first
convergence analysis of the DP deep learning, through the lens of training
dynamics and the neural tangent kernel (NTK). Our convergence theory
successfully characterizes the effects of two key components in the DP
training: the per-sample clipping (flat or layerwise) and the noise addition.
Our analysis not only initiates a general principled framework to understand
the DP deep learning with any network architecture and loss function, but also
motivates a new clipping method -- the global clipping, that significantly
improves the convergence while preserving the same privacy guarantee as the
existing local clipping.

In terms of theoretical results, we establish the precise connection between
the per-sample clipping and NTK matrix. We show that in the gradient flow,
i.e., with infinitesimal learning rate, the noise level of DP optimizers does
not affect the convergence. We prove that DP gradient descent (GD) with global
clipping guarantees the monotone convergence to zero loss, which can be
violated by the existing DP-GD with local clipping. Notably, our analysis
framework easily extends to other optimizers, e.g., DP-Adam. Empirically
speaking, DP optimizers equipped with global clipping perform strongly on a
wide range of classification and regression tasks. In particular, our global
clipping is surprisingly effective at learning calibrated classifiers, in
contrast to the existing DP classifiers which are oftentimes over-confident and
unreliable. Implementation-wise, the new clipping can be realized by adding one
line of code into the Opacus library.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1"&gt;Zhiqi Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1"&gt;Qi Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weijie J. Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAN-LOC: Spoofing Detection and Physical Intrusion Localization on an In-Vehicle CAN Bus Based on Deep Features of Voltage Signals. (arXiv:2106.07895v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.07895</id>
        <link href="http://arxiv.org/abs/2106.07895"/>
        <updated>2021-06-16T01:21:10.152Z</updated>
        <summary type="html"><![CDATA[The Controller Area Network (CAN) is used for communication between
in-vehicle devices. The CAN bus has been shown to be vulnerable to remote
attacks. To harden vehicles against such attacks, vehicle manufacturers have
divided in-vehicle networks into sub-networks, logically isolating critical
devices. However, attackers may still have physical access to various
sub-networks where they can connect a malicious device. This threat has not
been adequately addressed, as methods proposed to determine physical intrusion
points have shown weak results, emphasizing the need to develop more advanced
techniques. To address this type of threat, we propose a security hardening
system for in-vehicle networks. The proposed system includes two mechanisms
that process deep features extracted from voltage signals measured on the CAN
bus. The first mechanism uses data augmentation and deep learning to detect and
locate physical intrusions when the vehicle starts; this mechanism can detect
and locate intrusions, even when the connected malicious devices are silent.
This mechanism's effectiveness (100% accuracy) is demonstrated in a wide
variety of insertion scenarios on a CAN bus prototype. The second mechanism is
a continuous device authentication mechanism, which is also based on deep
learning; this mechanism's robustness (99.8% accuracy) is demonstrated on a
real moving vehicle.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levy_E/0/1/0/all/0/1"&gt;Efrat Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1"&gt;Asaf Shabtai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Groza_B/0/1/0/all/0/1"&gt;Bogdan Groza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murvay_P/0/1/0/all/0/1"&gt;Pal-Stefan Murvay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1"&gt;Yuval Elovici&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Robustness of Graph Neural Networks with Heterophily-Inspired Designs. (arXiv:2106.07767v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07767</id>
        <link href="http://arxiv.org/abs/2106.07767"/>
        <updated>2021-06-16T01:21:10.112Z</updated>
        <summary type="html"><![CDATA[Recent studies have exposed that many graph neural networks (GNNs) are
sensitive to adversarial attacks, and can suffer from performance loss if the
graph structure is intentionally perturbed. A different line of research has
shown that many GNN architectures implicitly assume that the underlying graph
displays homophily, i.e., connected nodes are more likely to have similar
features and class labels, and perform poorly if this assumption is not
fulfilled. In this work, we formalize the relation between these two seemingly
different issues. We theoretically show that in the standard scenario in which
node features exhibit homophily, impactful structural attacks always lead to
increased levels of heterophily. Then, inspired by GNN architectures that
target heterophily, we present two designs -- (i) separate aggregators for ego-
and neighbor-embeddings, and (ii) a reduced scope of aggregation -- that can
significantly improve the robustness of GNNs. Our extensive empirical
evaluations show that GNNs featuring merely these two designs can achieve
significantly improved robustness compared to the best-performing unvaccinated
model with 24.99% gain in average performance under targeted attacks, while
having smaller computational overhead than existing defense mechanisms.
Furthermore, these designs can be readily combined with explicit defense
mechanisms to yield state-of-the-art robustness with up to 18.33% increase in
performance under attacks compared to the best-performing vaccinated model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Junchen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaub_M/0/1/0/all/0/1"&gt;Michael T. Schaub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1"&gt;Danai Koutra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Clinically Inspired Approach for Melanoma classification. (arXiv:2106.08021v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08021</id>
        <link href="http://arxiv.org/abs/2106.08021"/>
        <updated>2021-06-16T01:21:10.098Z</updated>
        <summary type="html"><![CDATA[Melanoma is a leading cause of deaths due to skin cancer deaths and hence,
early and effective diagnosis of melanoma is of interest. Current approaches
for automated diagnosis of melanoma either use pattern recognition or
analytical recognition like ABCDE (asymmetry, border, color, diameter and
evolving) criterion. In practice however, a differential approach wherein
outliers (ugly duckling) are detected and used to evaluate nevi/lesions.
Incorporation of differential recognition in Computer Aided Diagnosis (CAD)
systems has not been explored but can be beneficial as it can provide a
clinical justification for the derived decision. We present a method for
identifying and quantifying ugly ducklings by performing Intra-Patient
Comparative Analysis (IPCA) of neighboring nevi. This is then incorporated in a
CAD system design for melanoma detection. This design ensures flexibility to
handle cases where IPCA is not possible. Our experiments on a public dataset
show that the outlier information helps boost the sensitivity of detection by
at least 4.1 % and specificity by 4.0 % to 8.9 %, depending on the use of a
strong (EfficientNet) or moderately strong (VGG or ResNet) classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akundi_P/0/1/0/all/0/1"&gt;Prathyusha Akundi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gun_S/0/1/0/all/0/1"&gt;Soumyasis Gun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivaswamy_J/0/1/0/all/0/1"&gt;Jayanthi Sivaswamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KL Guided Domain Adaptation. (arXiv:2106.07780v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07780</id>
        <link href="http://arxiv.org/abs/2106.07780"/>
        <updated>2021-06-16T01:21:10.092Z</updated>
        <summary type="html"><![CDATA[Domain adaptation is an important problem and often needed for real-world
applications. In this problem, instead of i.i.d. datapoints, we assume that the
source (training) data and the target (testing) data have different
distributions. With that setting, the empirical risk minimization training
procedure often does not perform well, since it does not account for the change
in the distribution. A common approach in the domain adaptation literature is
to learn a representation of the input that has the same distributions over the
source and the target domain. However, these approaches often require
additional networks and/or optimizing an adversarial (minimax) objective, which
can be very expensive or unstable in practice. To tackle this problem, we first
derive a generalization bound for the target loss based on the training loss
and the reverse Kullback-Leibler (KL) divergence between the source and the
target representation distributions. Based on this bound, we derive an
algorithm that minimizes the KL term to obtain a better generalization to the
target domain. We show that with a probabilistic representation network, the KL
term can be estimated efficiently via minibatch samples without any additional
network or a minimax objective. This leads to a theoretically sound alignment
method which is also very efficient and stable in practice. Experimental
results also suggest that our method outperforms other representation-alignment
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;A. Tuan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Toan Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H. S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baydin_A/0/1/0/all/0/1"&gt;At&amp;#x131;l&amp;#x131;m G&amp;#xfc;ne&amp;#x15f; Baydin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.08208</id>
        <link href="http://arxiv.org/abs/2106.08208"/>
        <updated>2021-06-16T01:21:10.083Z</updated>
        <summary type="html"><![CDATA[Adaptive gradient methods have shown excellent performance for solving many
machine learning problems. Although multiple adaptive methods were recently
studied, they mainly focus on either empirical or theoretical aspects and also
only work for specific problems by using specific adaptive learning rates. It
is desired to design a universal framework for practical algorithms of adaptive
gradients with theoretical guarantee to solve general problems. To fill this
gap, we propose a faster and universal framework of adaptive gradients (i.e.,
SUPER-ADAM) by introducing a universal adaptive matrix that includes most
existing adaptive gradient forms. Moreover, our framework can flexibly
integrates the momentum and variance reduced techniques. In particular, our
novel framework provides the convergence analysis support for adaptive gradient
methods under the nonconvex setting. In theoretical analysis, we prove that our
new algorithm can achieve the best known complexity of
$\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point of
nonconvex optimization, which matches the lower bound for stochastic smooth
nonconvex optimization. In numerical experiments, we employ various deep
learning tasks to validate that our algorithm consistently outperforms the
existing adaptive algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Large-Cohort Training for Federated Learning. (arXiv:2106.07820v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07820</id>
        <link href="http://arxiv.org/abs/2106.07820"/>
        <updated>2021-06-16T01:21:10.074Z</updated>
        <summary type="html"><![CDATA[Federated learning methods typically learn a model by iteratively sampling
updates from a population of clients. In this work, we explore how the number
of clients sampled at each round (the cohort size) impacts the quality of the
learned model and the training dynamics of federated learning algorithms. Our
work poses three fundamental questions. First, what challenges arise when
trying to scale federated learning to larger cohorts? Second, what parallels
exist between cohort sizes in federated learning and batch sizes in centralized
learning? Last, how can we design federated learning methods that effectively
utilize larger cohort sizes? We give partial answers to these questions based
on extensive empirical evaluation. Our work highlights a number of challenges
stemming from the use of larger cohorts. While some of these (such as
generalization issues and diminishing returns) are analogs of large-batch
training challenges, others (including training failures and fairness concerns)
are unique to federated learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1"&gt;Zachary Charles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1"&gt;Zachary Garrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1"&gt;Zhouyuan Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmulyian_S/0/1/0/all/0/1"&gt;Sergei Shmulyian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1"&gt;Virginia Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Out-of-Distribution Detection on Deep Probabilistic Generative Models. (arXiv:2106.07903v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07903</id>
        <link href="http://arxiv.org/abs/2106.07903"/>
        <updated>2021-06-16T01:21:10.049Z</updated>
        <summary type="html"><![CDATA[Out-of-distribution (OOD) detection is an important task in machine learning
systems for ensuring their reliability and safety. Deep probabilistic
generative models facilitate OOD detection by estimating the likelihood of a
data sample. However, such models frequently assign a suspiciously high
likelihood to a specific outlier. Several recent works have addressed this
issue by training a neural network with auxiliary outliers, which are generated
by perturbing the input data. In this paper, we discover that these approaches
fail for certain OOD datasets. Thus, we suggest a new detection metric that
operates without outlier exposure. We observe that our metric is robust to
diverse variations of an image compared to the previous outlier-exposing
methods. Furthermore, our proposed score requires neither auxiliary models nor
additional training. Instead, this paper utilizes the likelihood ratio
statistic in a new perspective to extract genuine properties from the given
single deep probabilistic generative model. We also apply a novel numerical
approximation to enable fast implementation. Finally, we demonstrate
comprehensive experiments on various probabilistic generative models and show
that our method achieves state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jaemoo Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1"&gt;Changyeon Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1"&gt;Jeongwoo Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Myungjoo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Micro-Structured Weight Unification for Neural Network Compression. (arXiv:2106.08301v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08301</id>
        <link href="http://arxiv.org/abs/2106.08301"/>
        <updated>2021-06-16T01:21:10.042Z</updated>
        <summary type="html"><![CDATA[Compressing Deep Neural Network (DNN) models to alleviate the storage and
computation requirements is essential for practical applications, especially
for resource limited devices. Although capable of reducing a reasonable amount
of model parameters, previous unstructured or structured weight pruning methods
can hardly truly accelerate inference, either due to the poor hardware
compatibility of the unstructured sparsity or due to the low sparse rate of the
structurally pruned network. Aiming at reducing both storage and computation,
as well as preserving the original task performance, we propose a generalized
weight unification framework at a hardware compatible micro-structured level to
achieve high amount of compression and acceleration. Weight coefficients of a
selected micro-structured block are unified to reduce the storage and
computation of the block without changing the neuron connections, which turns
to a micro-structured pruning special case when all unified coefficients are
set to zero, where neuron connections (hence storage and computation) are
completely removed. In addition, we developed an effective training framework
based on the alternating direction method of multipliers (ADMM), which converts
our complex constrained optimization into separately solvable subproblems.
Through iteratively optimizing the subproblems, the desired micro-structure can
be ensured with high compression ratio and low performance degradation. We
extensively evaluated our method using a variety of benchmark models and
datasets for different applications. Experimental results demonstrate
state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Sheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Songnan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08038</id>
        <link href="http://arxiv.org/abs/2106.08038"/>
        <updated>2021-06-16T01:21:10.030Z</updated>
        <summary type="html"><![CDATA[Averaging predictions over a set of models -- an ensemble -- is widely used
to improve predictive performance and uncertainty estimation of deep learning
models. At the same time, many machine learning systems, such as search,
matching, and recommendation systems, heavily rely on embeddings.
Unfortunately, due to misalignment of features of independently trained models,
embeddings, cannot be improved with a naive deep ensemble like approach. In
this work, we look at the ensembling of representations and propose mean
embeddings with test-time augmentation (MeTTA) simple yet well-performing
recipe for ensembling representations. Empirically we demonstrate that MeTTA
significantly boosts the quality of linear evaluation on ImageNet for both
supervised and self-supervised models. Even more exciting, we draw connections
between MeTTA, image retrieval, and transformation invariant models. We believe
that spreading the success of ensembles to inference higher-quality
representations is the important step that will open many new applications of
ensembling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1"&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1"&gt;Andrei Atanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Next Generation Reservoir Computing. (arXiv:2106.07688v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07688</id>
        <link href="http://arxiv.org/abs/2106.07688"/>
        <updated>2021-06-16T01:21:10.023Z</updated>
        <summary type="html"><![CDATA[Reservoir computing is a best-in-class machine learning algorithm for
processing information generated by dynamical systems using observed
time-series data. Importantly, it requires very small training data sets, uses
linear optimization, and thus requires minimal computing resources. However,
the algorithm uses randomly sampled matrices to define the underlying recurrent
neural network and has a multitude of metaparameters that must be optimized.
Recent results demonstrate the equivalence of reservoir computing to nonlinear
vector autoregression, which requires no random matrices, fewer metaparameters,
and provides interpretable results. Here, we demonstrate that nonlinear vector
autoregression excels at reservoir computing benchmark tasks and requires even
shorter training data sets and training time, heralding the next generation of
reservoir computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1"&gt;Daniel J. Gauthier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bollt_E/0/1/0/all/0/1"&gt;Erik Bollt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffith_A/0/1/0/all/0/1"&gt;Aaron Griffith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbosa_W/0/1/0/all/0/1"&gt;Wendson A.S. Barbosa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Gradient Manifold Neural Network. (arXiv:2106.07905v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07905</id>
        <link href="http://arxiv.org/abs/2106.07905"/>
        <updated>2021-06-16T01:21:10.014Z</updated>
        <summary type="html"><![CDATA[Deep neural network (DNN) generally takes thousands of iterations to optimize
via gradient descent and thus has a slow convergence. In addition, softmax, as
a decision layer, may ignore the distribution information of the data during
classification. Aiming to tackle the referred problems, we propose a novel
manifold neural network based on non-gradient optimization, i.e., the
closed-form solutions. Considering that the activation function is generally
invertible, we reconstruct the network via forward ridge regression and low
rank backward approximation, which achieve the rapid convergence. Moreover, by
unifying the flexible Stiefel manifold and adaptive support vector machine, we
devise the novel decision layer which efficiently fits the manifold structure
of the data and label information. Consequently, a jointly non-gradient
optimization method is designed to generate the network with closed-form
results. Eventually, extensive experiments validate the superior performance of
the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Z/0/1/0/all/0/1"&gt;Ziheng Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuelong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voting for the right answer: Adversarial defense for speaker verification. (arXiv:2106.07868v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07868</id>
        <link href="http://arxiv.org/abs/2106.07868"/>
        <updated>2021-06-16T01:21:10.001Z</updated>
        <summary type="html"><![CDATA[Automatic speaker verification (ASV) is a well developed technology for
biometric identification, and has been ubiquitous implemented in
security-critic applications, such as banking and access control. However,
previous works have shown that ASV is under the radar of adversarial attacks,
which are very similar to their original counterparts from human's perception,
yet will manipulate the ASV render wrong prediction. Due to the very late
emergence of adversarial attacks for ASV, effective countermeasures against
them are limited. Given that the security of ASV is of high priority, in this
work, we propose the idea of "voting for the right answer" to prevent risky
decisions of ASV in blind spot areas, by employing random sampling and voting.
Experimental results show that our proposed method improves the robustness
against both the limited-knowledge attackers by pulling the adversarial samples
out of the blind spots, and the perfect-knowledge attackers by introducing
randomness and increasing the attackers' budgets. The code for reproducing main
results is available at https://github.com/thuhcsi/adsv_voting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haibin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling Neural Networks with Rule Representations. (arXiv:2106.07804v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07804</id>
        <link href="http://arxiv.org/abs/2106.07804"/>
        <updated>2021-06-16T01:21:09.977Z</updated>
        <summary type="html"><![CDATA[We propose a novel training method to integrate rules into deep learning, in
a way their strengths are controllable at inference. Deep Neural Networks with
Controllable Rule Representations (DeepCTRL) incorporates a rule encoder into
the model coupled with a rule-based objective, enabling a shared representation
for decision making. DeepCTRL is agnostic to data type and model architecture.
It can be applied to any kind of rule defined for inputs and outputs. The key
aspect of DeepCTRL is that it does not require retraining to adapt the rule
strength -- at inference, the user can adjust it based on the desired operation
point on accuracy vs. rule verification ratio. In real-world domains where
incorporating rules is critical -- such as Physics, Retail and Healthcare -- we
show the effectiveness of DeepCTRL in teaching rules for deep learning.
DeepCTRL improves the trust and reliability of the trained models by
significantly increasing their rule verification ratio, while also providing
accuracy gains at downstream tasks. Additionally, DeepCTRL enables novel use
cases such as hypothesis testing of the rules on data samples, and unsupervised
adaptation based on shared rules between datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1"&gt;Sungyong Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1"&gt;Sercan O. Arik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jinsung Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kihyuk Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1"&gt;Tomas Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exponential Improvement on the Memorization Capacity of Deep Threshold Networks. (arXiv:2106.07724v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07724</id>
        <link href="http://arxiv.org/abs/2106.07724"/>
        <updated>2021-06-16T01:21:09.963Z</updated>
        <summary type="html"><![CDATA[It is well known that modern deep neural networks are powerful enough to
memorize datasets even when the labels have been randomized. Recently,
Vershynin (2020) settled a long standing question by Baum (1988), proving that
\emph{deep threshold} networks can memorize $n$ points in $d$ dimensions using
$\widetilde{\mathcal{O}}(e^{1/\delta^2}+\sqrt{n})$ neurons and
$\widetilde{\mathcal{O}}(e^{1/\delta^2}(d+\sqrt{n})+n)$ weights, where $\delta$
is the minimum distance between the points. In this work, we improve the
dependence on $\delta$ from exponential to almost linear, proving that
$\widetilde{\mathcal{O}}(\frac{1}{\delta}+\sqrt{n})$ neurons and
$\widetilde{\mathcal{O}}(\frac{d}{\delta}+n)$ weights are sufficient. Our
construction uses Gaussian random weights only in the first layer, while all
the subsequent layers use binary or integer weights. We also prove new lower
bounds by connecting memorization in neural networks to the purely geometric
problem of separating $n$ points on a sphere using hyperplanes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajput_S/0/1/0/all/0/1"&gt;Shashank Rajput&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivasan_K/0/1/0/all/0/1"&gt;Kartik Sreenivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1"&gt;Dimitris Papailiopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1"&gt;Amin Karbasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEiT: BERT Pre-Training of Image Transformers. (arXiv:2106.08254v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08254</id>
        <link href="http://arxiv.org/abs/2106.08254"/>
        <updated>2021-06-16T01:21:09.955Z</updated>
        <summary type="html"><![CDATA[We introduce a self-supervised vision representation model BEiT, which stands
for Bidirectional Encoder representation from Image Transformers. Following
BERT developed in the natural language processing area, we propose a masked
image modeling task to pretrain vision Transformers. Specifically, each image
has two views in our pre-training, i.e, image patches (such as 16x16 pixels),
and visual tokens (i.e., discrete tokens). We first "tokenize" the original
image into visual tokens. Then we randomly mask some image patches and fed them
into the backbone Transformer. The pre-training objective is to recover the
original visual tokens based on the corrupted image patches. After pre-training
BEiT, we directly fine-tune the model parameters on downstream tasks by
appending task layers upon the pretrained encoder. Experimental results on
image classification and semantic segmentation show that our model achieves
competitive results with previous pre-training methods. For example, base-size
BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming
from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size
BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with
supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models
are available at https://aka.ms/beit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hangbo Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Canonical-Correlation-Based Fast Feature Selection. (arXiv:2106.08247v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08247</id>
        <link href="http://arxiv.org/abs/2106.08247"/>
        <updated>2021-06-16T01:21:09.944Z</updated>
        <summary type="html"><![CDATA[This paper proposes a canonical-correlation-based filter method for feature
selection. The sum of squared canonical correlation coefficients is adopted as
the feature ranking criterion. The proposed method boosts the computational
speed of the ranking criterion in greedy search. The supporting theorems
developed for the feature selection method are fundamental to the understanding
of the canonical correlation analysis. In empirical studies, a synthetic
dataset is used to demonstrate the speed advantage of the proposed method, and
eight real datasets are applied to show the effectiveness of the proposed
feature ranking criterion in both classification and regression. The results
show that the proposed method is considerably faster than the definition-based
method, and the proposed ranking criterion is competitive compared with the
seven mutual-information-based criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sikai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tingna Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Worden_K/0/1/0/all/0/1"&gt;Keith Worden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cross_E/0/1/0/all/0/1"&gt;Elizabeth J. Cross&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Efficient Reinforcement Learning In Continuous State Spaces: A Perspective Beyond Linearity. (arXiv:2106.07814v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07814</id>
        <link href="http://arxiv.org/abs/2106.07814"/>
        <updated>2021-06-16T01:21:09.938Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) is empirically successful in complex nonlinear
Markov decision processes (MDPs) with continuous state spaces. By contrast, the
majority of theoretical RL literature requires the MDP to satisfy some form of
linear structure, in order to guarantee sample efficient RL. Such efforts
typically assume the transition dynamics or value function of the MDP are
described by linear functions of the state features. To resolve this
discrepancy between theory and practice, we introduce the Effective Planning
Window (EPW) condition, a structural condition on MDPs that makes no linearity
assumptions. We demonstrate that the EPW condition permits sample efficient RL,
by providing an algorithm which provably solves MDPs satisfying this condition.
Our algorithm requires minimal assumptions on the policy class, which can
include multi-layer neural networks with nonlinear activation functions.
Notably, the EPW condition is directly motivated by popular gaming benchmarks,
and we show that many classic Atari games satisfy this condition. We
additionally show the necessity of conditions like EPW, by demonstrating that
simple MDPs with slight nonlinearities cannot be solved sample efficiently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malik_D/0/1/0/all/0/1"&gt;Dhruv Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_V/0/1/0/all/0/1"&gt;Vishwak Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S-LIME: Stabilized-LIME for Model Explanation. (arXiv:2106.07875v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.07875</id>
        <link href="http://arxiv.org/abs/2106.07875"/>
        <updated>2021-06-16T01:21:09.919Z</updated>
        <summary type="html"><![CDATA[An increasing number of machine learning models have been deployed in domains
with high stakes such as finance and healthcare. Despite their superior
performances, many models are black boxes in nature which are hard to explain.
There are growing efforts for researchers to develop methods to interpret these
black-box models. Post hoc explanations based on perturbations, such as LIME,
are widely used approaches to interpret a machine learning model after it has
been built. This class of methods has been shown to exhibit large instability,
posing serious challenges to the effectiveness of the method itself and harming
user trust. In this paper, we propose S-LIME, which utilizes a hypothesis
testing framework based on central limit theorem for determining the number of
perturbation points needed to guarantee stability of the resulting explanation.
Experiments on both simulated and real world data sets are provided to
demonstrate the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhengze Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hooker_G/0/1/0/all/0/1"&gt;Giles Hooker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tracing Back Music Emotion Predictions to Sound Sources and Intuitive Perceptual Qualities. (arXiv:2106.07787v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.07787</id>
        <link href="http://arxiv.org/abs/2106.07787"/>
        <updated>2021-06-16T01:21:09.912Z</updated>
        <summary type="html"><![CDATA[Music emotion recognition is an important task in MIR (Music Information
Retrieval) research. Owing to factors like the subjective nature of the task
and the variation of emotional cues between musical genres, there are still
significant challenges in developing reliable and generalizable models. One
important step towards better models would be to understand what a model is
actually learning from the data and how the prediction for a particular input
is made. In previous work, we have shown how to derive explanations of model
predictions in terms of spectrogram image segments that connect to the
high-level emotion prediction via a layer of easily interpretable perceptual
features. However, that scheme lacks intuitive musical comprehensibility at the
spectrogram level. In the present work, we bridge this gap by merging audioLIME
-- a source-separation based explainer -- with mid-level perceptual features,
thus forming an intuitive connection chain between the input audio and the
output emotion predictions. We demonstrate the usefulness of this method by
applying it to debug a biased emotion prediction model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Shreyan Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Praher_V/0/1/0/all/0/1"&gt;Verena Praher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1"&gt;Gerhard Widmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Continuized View on Nesterov Acceleration for Stochastic Gradient Descent and Randomized Gossip. (arXiv:2106.07644v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.07644</id>
        <link href="http://arxiv.org/abs/2106.07644"/>
        <updated>2021-06-16T01:21:09.905Z</updated>
        <summary type="html"><![CDATA[We introduce the continuized Nesterov acceleration, a close variant of
Nesterov acceleration whose variables are indexed by a continuous time
parameter. The two variables continuously mix following a linear ordinary
differential equation and take gradient steps at random times. This continuized
variant benefits from the best of the continuous and the discrete frameworks:
as a continuous process, one can use differential calculus to analyze
convergence and obtain analytical expressions for the parameters; and a
discretization of the continuized process can be computed exactly with
convergence rates similar to those of Nesterov original acceleration. We show
that the discretization has the same structure as Nesterov acceleration, but
with random parameters. We provide continuized Nesterov acceleration under
deterministic as well as stochastic gradients, with either additive or
multiplicative noise. Finally, using our continuized framework and expressing
the gossip averaging problem as the stochastic minimization of a certain energy
function, we provide the first rigorous acceleration of asynchronous gossip
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Even_M/0/1/0/all/0/1"&gt;Mathieu Even&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Berthier_R/0/1/0/all/0/1"&gt;Rapha&amp;#xeb;l Berthier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Flammarion_N/0/1/0/all/0/1"&gt;Nicolas Flammarion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gaillard_P/0/1/0/all/0/1"&gt;Pierre Gaillard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hendrikx_H/0/1/0/all/0/1"&gt;Hadrien Hendrikx&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Massoulie_L/0/1/0/all/0/1"&gt;Laurent Massouli&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Taylor_A/0/1/0/all/0/1"&gt;Adrien Taylor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented Tensor Decomposition with Stochastic Optimization. (arXiv:2106.07900v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2106.07900</id>
        <link href="http://arxiv.org/abs/2106.07900"/>
        <updated>2021-06-16T01:21:09.888Z</updated>
        <summary type="html"><![CDATA[Tensor decompositions are powerful tools for dimensionality reduction and
feature interpretation of multidimensional data such as signals. Existing
tensor decomposition objectives (e.g., Frobenius norm) are designed for fitting
raw data under statistical assumptions, which may not align with downstream
classification tasks. Also, real-world tensor data are usually high-ordered and
have large dimensions with millions or billions of entries. Thus, it is
expensive to decompose the whole tensor with traditional algorithms. In
practice, raw tensor data also contains redundant information while data
augmentation techniques may be used to smooth out noise in samples. This paper
addresses the above challenges by proposing augmented tensor decomposition
(ATD), which effectively incorporates data augmentations to boost downstream
classification. To reduce the memory footprint of the decomposition, we propose
a stochastic algorithm that updates the factor matrices in a batch fashion. We
evaluate ATD on multiple signal datasets. It shows comparable or better
performance (e.g., up to 15% in accuracy) over self-supervised and autoencoder
baselines with less than 5% of model parameters, achieves 0.6% ~ 1.3% accuracy
gain over other tensor-based baselines, and reduces the memory footprint by 9X
when compared to standard tensor decomposition algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chaoqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Qian_C/0/1/0/all/0/1"&gt;Cheng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Singh_N/0/1/0/all/0/1"&gt;Navjot Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Cao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Westover_M/0/1/0/all/0/1"&gt;M Brandon Westover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Solomonik_E/0/1/0/all/0/1"&gt;Edgar Solomonik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jimeng Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reverse Engineering of Generative Models: Inferring Model Hyperparameters from Generated Images. (arXiv:2106.07873v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07873</id>
        <link href="http://arxiv.org/abs/2106.07873"/>
        <updated>2021-06-16T01:21:09.877Z</updated>
        <summary type="html"><![CDATA[State-of-the-art (SOTA) Generative Models (GMs) can synthesize
photo-realistic images that are hard for humans to distinguish from genuine
photos. We propose to perform reverse engineering of GMs to infer the model
hyperparameters from the images generated by these models. We define a novel
problem, "model parsing", as estimating GM network architectures and training
loss functions by examining their generated images -- a task seemingly
impossible for human beings. To tackle this problem, we propose a framework
with two components: a Fingerprint Estimation Network (FEN), which estimates a
GM fingerprint from a generated image by training with four constraints to
encourage the fingerprint to have desired properties, and a Parsing Network
(PN), which predicts network architecture and loss functions from the estimated
fingerprints. To evaluate our approach, we collect a fake image dataset with
$100$K images generated by $100$ GMs. Extensive experiments show encouraging
results in parsing the hyperparameters of the unseen models. Finally, our
fingerprint estimation can be leveraged for deepfake detection and image
attribution, as we show by reporting SOTA results on both the recent Celeb-DF
and image attribution benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asnani_V/0/1/0/all/0/1"&gt;Vishal Asnani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xi Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1"&gt;Tal Hassner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoDERT: Distilling Encoder Representations with Co-learning for Transducer-based Speech Recognition. (arXiv:2106.07734v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07734</id>
        <link href="http://arxiv.org/abs/2106.07734"/>
        <updated>2021-06-16T01:21:09.858Z</updated>
        <summary type="html"><![CDATA[We propose a simple yet effective method to compress an RNN-Transducer
(RNN-T) through the well-known knowledge distillation paradigm. We show that
the transducer's encoder outputs naturally have a high entropy and contain rich
information about acoustically similar word-piece confusions. This rich
information is suppressed when combined with the lower entropy decoder outputs
to produce the joint network logits. Consequently, we introduce an auxiliary
loss to distill the encoder logits from a teacher transducer's encoder, and
explore training strategies where this encoder distillation works effectively.
We find that tandem training of teacher and student encoders with an inplace
encoder distillation outperforms the use of a pre-trained and static teacher
transducer. We also report an interesting phenomenon we refer to as implicit
distillation, that occurs when the teacher and student encoders share the same
decoder. Our experiments show 5.37-8.4% relative word error rate reductions
(WERR) on in-house test sets, and 5.05-6.18% relative WERRs on LibriSpeech test
sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Swaminathan_R/0/1/0/all/0/1"&gt;Rupak Vignesh Swaminathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_B/0/1/0/all/0/1"&gt;Brian King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1"&gt;Grant P. Strimel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1"&gt;Jasha Droppo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1"&gt;Athanasios Mouchtaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Revenue-Maximizing Auctions With Differentiable Matching. (arXiv:2106.07877v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.07877</id>
        <link href="http://arxiv.org/abs/2106.07877"/>
        <updated>2021-06-16T01:21:09.842Z</updated>
        <summary type="html"><![CDATA[We propose a new architecture to approximately learn incentive compatible,
revenue-maximizing auctions from sampled valuations. Our architecture uses the
Sinkhorn algorithm to perform a differentiable bipartite matching which allows
the network to learn strategyproof revenue-maximizing mechanisms in settings
not learnable by the previous RegretNet architecture. In particular, our
architecture is able to learn mechanisms in settings without free disposal
where each bidder must be allocated exactly some number of items. In
experiments, we show our approach successfully recovers multiple known optimal
mechanisms and high-revenue, low-regret mechanisms in larger settings where the
optimal mechanism is unknown.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Curry_M/0/1/0/all/0/1"&gt;Michael J. Curry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyi_U/0/1/0/all/0/1"&gt;Uro Lyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1"&gt;John Dickerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CathAI: Fully Automated Interpretation of Coronary Angiograms Using Neural Networks. (arXiv:2106.07708v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07708</id>
        <link href="http://arxiv.org/abs/2106.07708"/>
        <updated>2021-06-16T01:21:09.834Z</updated>
        <summary type="html"><![CDATA[Coronary heart disease (CHD) is the leading cause of adult death in the
United States and worldwide, and for which the coronary angiography procedure
is the primary gateway for diagnosis and clinical management decisions. The
standard-of-care for interpretation of coronary angiograms depends upon ad-hoc
visual assessment by the physician operator. However, ad-hoc visual
interpretation of angiograms is poorly reproducible, highly variable and bias
prone. Here we show for the first time that fully-automated angiogram
interpretation to estimate coronary artery stenosis is possible using a
sequence of deep neural network algorithms. The algorithmic pipeline we
developed--called CathAI--achieves state-of-the art performance across the
sequence of tasks required to accomplish automated interpretation of
unselected, real-world angiograms. CathAI (Algorithms 1-2) demonstrated
positive predictive value, sensitivity and F1 score of >=90% to identify the
projection angle overall and >=93% for left or right coronary artery angiogram
detection, the primary anatomic structures of interest. To predict obstructive
coronary artery stenosis (>=70% stenosis), CathAI (Algorithm 4) exhibited an
area under the receiver operating characteristic curve (AUC) of 0.862 (95% CI:
0.843-0.880). When externally validated in a healthcare system in another
country, CathAI AUC was 0.869 (95% CI: 0.830-0.907) to predict obstructive
coronary artery stenosis. Our results demonstrate that multiple purpose-built
neural networks can function in sequence to accomplish the complex series of
tasks required for automated analysis of real-world angiograms. Deployment of
CathAI may serve to increase standardization and reproducibility in coronary
stenosis assessment, while providing a robust foundation to accomplish future
tasks for algorithmic angiographic interpretation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avram_R/0/1/0/all/0/1"&gt;Robert Avram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olgin_J/0/1/0/all/0/1"&gt;Jeffrey E. Olgin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_A/0/1/0/all/0/1"&gt;Alvin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1"&gt;Zeeshan Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verreault_Julien_L/0/1/0/all/0/1"&gt;Louis Verreault-Julien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abreau_S/0/1/0/all/0/1"&gt;Sean Abreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1"&gt;Derek Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1"&gt;Derek Y. So&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soni_K/0/1/0/all/0/1"&gt;Krishan Soni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tison_G/0/1/0/all/0/1"&gt;Geoffrey H. Tison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GeoMol: Torsional Geometric Generation of Molecular 3D Conformer Ensembles. (arXiv:2106.07802v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.07802</id>
        <link href="http://arxiv.org/abs/2106.07802"/>
        <updated>2021-06-16T01:21:09.826Z</updated>
        <summary type="html"><![CDATA[Prediction of a molecule's 3D conformer ensemble from the molecular graph
holds a key role in areas of cheminformatics and drug discovery. Existing
generative models have several drawbacks including lack of modeling important
molecular geometry elements (e.g. torsion angles), separate optimization stages
prone to error accumulation, and the need for structure fine-tuning based on
approximate classical force-fields or computationally expensive methods such as
metadynamics with approximate quantum mechanics calculations at each geometry.
We propose GeoMol--an end-to-end, non-autoregressive and SE(3)-invariant
machine learning approach to generate distributions of low-energy molecular 3D
conformers. Leveraging the power of message passing neural networks (MPNNs) to
capture local and global graph information, we predict local atomic 3D
structures and torsion angles, avoiding unnecessary over-parameterization of
the geometric degrees of freedom (e.g. one angle per non-terminal bond). Such
local predictions suffice both for the training loss computation, as well as
for the full deterministic conformer assembly (at test time). We devise a
non-adversarial optimal transport based loss function to promote diverse
conformer generation. GeoMol predominantly outperforms popular open-source,
commercial, or state-of-the-art machine learning (ML) models, while achieving
significant speed-ups. We expect such differentiable 3D structure generators to
significantly impact molecular modeling and related applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Ganea_O/0/1/0/all/0/1"&gt;Octavian-Eugen Ganea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pattanaik_L/0/1/0/all/0/1"&gt;Lagnajit Pattanaik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Coley_C/0/1/0/all/0/1"&gt;Connor W. Coley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Jensen_K/0/1/0/all/0/1"&gt;Klavs F. Jensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Green_W/0/1/0/all/0/1"&gt;William H. Green&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Jaakkola_T/0/1/0/all/0/1"&gt;Tommi S. Jaakkola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Credit Assignment in Neural Networks through Deep Feedback Control. (arXiv:2106.07887v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07887</id>
        <link href="http://arxiv.org/abs/2106.07887"/>
        <updated>2021-06-16T01:21:09.818Z</updated>
        <summary type="html"><![CDATA[The success of deep learning sparked interest in whether the brain learns by
using similar techniques for assigning credit to each synaptic weight for its
contribution to the network output. However, the majority of current attempts
at biologically-plausible learning methods are either non-local in time,
require highly specific connectivity motives, or have no clear link to any
known mathematical optimization method. Here, we introduce Deep Feedback
Control (DFC), a new learning method that uses a feedback controller to drive a
deep neural network to match a desired output target and whose control signal
can be used for credit assignment. The resulting learning rule is fully local
in space and time and approximates Gauss-Newton optimization for a wide range
of feedback connectivity patterns. To further underline its biological
plausibility, we relate DFC to a multi-compartment model of cortical pyramidal
neurons with a local voltage-dependent synaptic plasticity rule, consistent
with recent theories of dendritic processing. By combining dynamical system
theory with mathematical optimization theory, we provide a strong theoretical
foundation for DFC that we corroborate with detailed results on toy experiments
and standard computer-vision benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meulemans_A/0/1/0/all/0/1"&gt;Alexander Meulemans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farinha_M/0/1/0/all/0/1"&gt;Matilde Tristany Farinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ordonez_J/0/1/0/all/0/1"&gt;Javier Garc&amp;#xed;a Ord&amp;#xf3;&amp;#xf1;ez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aceituno_P/0/1/0/all/0/1"&gt;Pau Vilimelis Aceituno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sacramento_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Sacramento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grewe_B/0/1/0/all/0/1"&gt;Benjamin F. Grewe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers. (arXiv:2106.07798v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07798</id>
        <link href="http://arxiv.org/abs/2106.07798"/>
        <updated>2021-06-16T01:21:09.796Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a new data poisoning attack and apply it to deep
reinforcement learning agents. Our attack centers on what we call
in-distribution triggers, which are triggers native to the data distributions
the model will be trained on and deployed in. We outline a simple procedure for
embedding these, and other, triggers in deep reinforcement learning agents
following a multi-task learning paradigm, and demonstrate in three common
reinforcement learning environments. We believe that this work has important
implications for the security of deep learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashcraft_C/0/1/0/all/0/1"&gt;Chace Ashcraft&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karra_K/0/1/0/all/0/1"&gt;Kiran Karra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Randomized Exploration for Reinforcement Learning with General Value Function Approximation. (arXiv:2106.07841v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07841</id>
        <link href="http://arxiv.org/abs/2106.07841"/>
        <updated>2021-06-16T01:21:09.777Z</updated>
        <summary type="html"><![CDATA[We propose a model-free reinforcement learning algorithm inspired by the
popular randomized least squares value iteration (RLSVI) algorithm as well as
the optimism principle. Unlike existing upper-confidence-bound (UCB) based
approaches, which are often computationally intractable, our algorithm drives
exploration by simply perturbing the training data with judiciously chosen
i.i.d. scalar noises. To attain optimistic value function estimation without
resorting to a UCB-style bonus, we introduce an optimistic reward sampling
procedure. When the value functions can be represented by a function class
$\mathcal{F}$, our algorithm achieves a worst-case regret bound of
$\widetilde{O}(\mathrm{poly}(d_EH)\sqrt{T})$ where $T$ is the time elapsed, $H$
is the planning horizon and $d_E$ is the $\textit{eluder dimension}$ of
$\mathcal{F}$. In the linear setting, our algorithm reduces to LSVI-PHE, a
variant of RLSVI, that enjoys an $\widetilde{\mathcal{O}}(\sqrt{d^3H^3T})$
regret. We complement the theory with an empirical evaluation across known
difficult exploration tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ishfaq_H/0/1/0/all/0/1"&gt;Haque Ishfaq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1"&gt;Qiwen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Viet Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayoub_A/0/1/0/all/0/1"&gt;Alex Ayoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lin F. Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RETRIEVE: Coreset Selection for Efficient and Robust Semi-Supervised Learning. (arXiv:2106.07760v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07760</id>
        <link href="http://arxiv.org/abs/2106.07760"/>
        <updated>2021-06-16T01:21:09.767Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning (SSL) algorithms have had great success in recent
years in limited labeled data regimes. However, the current state-of-the-art
SSL algorithms are computationally expensive and entail significant compute
time and energy requirements. This can prove to be a huge limitation for many
smaller companies and academic groups. Our main insight is that training on a
subset of unlabeled data instead of entire unlabeled data enables the current
SSL algorithms to converge faster, thereby reducing the computational costs
significantly. In this work, we propose RETRIEVE, a coreset selection framework
for efficient and robust semi-supervised learning. RETRIEVE selects the coreset
by solving a mixed discrete-continuous bi-level optimization problem such that
the selected coreset minimizes the labeled set loss. We use a one-step gradient
approximation and show that the discrete optimization problem is approximately
submodular, thereby enabling simple greedy algorithms to obtain the coreset. We
empirically demonstrate on several real-world datasets that existing SSL
algorithms like VAT, Mean-Teacher, FixMatch, when used with RETRIEVE, achieve
a) faster training times, b) better performance when unlabeled data consists of
Out-of-Distribution(OOD) data and imbalance. More specifically, we show that
with minimal accuracy degradation, RETRIEVE achieves a speedup of around 3X in
the traditional SSL setting and achieves a speedup of 5X compared to
state-of-the-art (SOTA) robust SSL algorithms in the case of imbalance and OOD
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1"&gt;Krishnateja Killamsetty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xujiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Feng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embarrassingly parallel MCMC using deep invertible transformations. (arXiv:1903.04556v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.04556</id>
        <link href="http://arxiv.org/abs/1903.04556"/>
        <updated>2021-06-16T01:21:09.684Z</updated>
        <summary type="html"><![CDATA[While MCMC methods have become a main work-horse for Bayesian inference,
scaling them to large distributed datasets is still a challenge. Embarrassingly
parallel MCMC strategies take a divide-and-conquer stance to achieve this by
writing the target posterior as a product of subposteriors, running MCMC for
each of them in parallel and subsequently combining the results. The challenge
then lies in devising efficient aggregation strategies. Current strategies
trade-off between approximation quality, and costs of communication and
computation. In this work, we introduce a novel method that addresses these
issues simultaneously. Our key insight is to introduce a deep invertible
transformation to approximate each of the subposteriors. These approximations
can be made accurate even for complex distributions and serve as intermediate
representations, keeping the total communication cost limited. Moreover, they
enable us to sample from the product of the subposteriors using an efficient
and stable importance sampling scheme. We demonstrate the approach outperforms
available state-of-the-art methods in a range of challenging scenarios,
including high-dimensional and heterogeneous subposteriors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mesquita_D/0/1/0/all/0/1"&gt;Diego Mesquita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blomstedt_P/0/1/0/all/0/1"&gt;Paul Blomstedt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Flip Side of the Reweighted Coin: Duality of Adaptive Dropout and Regularization. (arXiv:2106.07769v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07769</id>
        <link href="http://arxiv.org/abs/2106.07769"/>
        <updated>2021-06-16T01:21:09.666Z</updated>
        <summary type="html"><![CDATA[Among the most successful methods for sparsifying deep (neural) networks are
those that adaptively mask the network weights throughout training. By
examining this masking, or dropout, in the linear case, we uncover a duality
between such adaptive methods and regularization through the so-called
"$\eta$-trick" that casts both as iteratively reweighted optimizations. We show
that any dropout strategy that adapts to the weights in a monotonic way
corresponds to an effective subquadratic regularization penalty, and therefore
leads to sparse solutions. We obtain the effective penalties for several
popular sparsification strategies, which are remarkably similar to classical
penalties commonly used in sparse optimization. Considering variational dropout
as a case study, we demonstrate similar empirical behavior between the adaptive
dropout method and classical methods on the task of deep network
sparsification, validating our theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+LeJeune_D/0/1/0/all/0/1"&gt;Daniel LeJeune&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javadi_H/0/1/0/all/0/1"&gt;Hamid Javadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1"&gt;Richard G. Baraniuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Q-Rank: New Data Dependent Definition of Tensor Rank. (arXiv:1910.12016v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.12016</id>
        <link href="http://arxiv.org/abs/1910.12016"/>
        <updated>2021-06-16T01:21:09.654Z</updated>
        <summary type="html"><![CDATA[Recently, the \textit{Tensor Nuclear Norm~(TNN)} regularization based on
t-SVD has been widely used in various low tubal-rank tensor recovery tasks.
However, these models usually require smooth change of data along the third
dimension to ensure their low rank structures. In this paper, we propose a new
definition of data dependent tensor rank named \textit{tensor Q-rank} by a
learnable orthogonal matrix $\mathbf{Q}$, and further introduce a unified data
dependent low rank tensor recovery model. According to the low rank hypothesis,
we introduce two explainable selection method of $\mathbf{Q}$, under which the
data tensor may have a more significant low tensor Q-rank structure than that
of low tubal-rank structure. Specifically, maximizing the variance of singular
value distribution leads to Variance Maximization Tensor Q-Nuclear
norm~(VMTQN), while minimizing the value of nuclear norm through manifold
optimization leads to Manifold Optimization Tensor Q-Nuclear norm~(MOTQN).
Moreover, we apply these two models to the low rank tensor completion problem,
and then give an effective algorithm and briefly analyze why our method works
better than TNN based methods in the case of complex data with low sampling
rate. Finally, experimental results on real-world datasets demonstrate the
superiority of our proposed model in the tensor completion problem with respect
to other tensor rank regularization models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1"&gt;Hao Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Canyi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Divergence Frontiers for Generative Models: Sample Complexity, Quantization Level, and Frontier Integral. (arXiv:2106.07898v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.07898</id>
        <link href="http://arxiv.org/abs/2106.07898"/>
        <updated>2021-06-16T01:21:09.647Z</updated>
        <summary type="html"><![CDATA[The spectacular success of deep generative models calls for quantitative
tools to measure their statistical performance. Divergence frontiers have
recently been proposed as an evaluation framework for generative models, due to
their ability to measure the quality-diversity trade-off inherent to deep
generative modeling. However, the statistical behavior of divergence frontiers
estimated from data remains unknown to this day. In this paper, we establish
non-asymptotic bounds on the sample complexity of the plug-in estimator of
divergence frontiers. Along the way, we introduce a novel integral summary of
divergence frontiers. We derive the corresponding non-asymptotic bounds and
discuss the choice of the quantization level by balancing the two types of
approximation errors arisen from its computation. We also augment the
divergence frontier framework by investigating the statistical performance of
smoothed distribution estimators such as the Good-Turing estimator. We
illustrate the theoretical results with numerical examples from natural
language processing and computer vision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pillutla_K/0/1/0/all/0/1"&gt;Krishna Pillutla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Welleck_S/0/1/0/all/0/1"&gt;Sean Welleck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Oh_S/0/1/0/all/0/1"&gt;Sewoong Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1"&gt;Zaid Harchaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning. (arXiv:1910.10897v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.10897</id>
        <link href="http://arxiv.org/abs/1910.10897"/>
        <updated>2021-06-16T01:21:09.640Z</updated>
        <summary type="html"><![CDATA[Meta-reinforcement learning algorithms can enable robots to acquire new
skills much more quickly, by leveraging prior experience to learn how to learn.
However, much of the current research on meta-reinforcement learning focuses on
task distributions that are very narrow. For example, a commonly used
meta-reinforcement learning benchmark uses different running velocities for a
simulated robot as different tasks. When policies are meta-trained on such
narrow task distributions, they cannot possibly generalize to more quickly
acquire entirely new tasks. Therefore, if the aim of these methods is to enable
faster acquisition of entirely new behaviors, we must evaluate them on task
distributions that are sufficiently broad to enable generalization to new
behaviors. In this paper, we propose an open-source simulated benchmark for
meta-reinforcement learning and multi-task learning consisting of 50 distinct
robotic manipulation tasks. Our aim is to make it possible to develop
algorithms that generalize to accelerate the acquisition of entirely new,
held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and
multi-task learning algorithms on these tasks. Surprisingly, while each task
and its variations (e.g., with different object positions) can be learned with
reasonable success, these algorithms struggle to learn with multiple tasks at
the same time, even with as few as ten distinct training tasks. Our analysis
and open-source environments pave the way for future research in multi-task
learning and meta-learning that can enable meaningful generalization, thereby
unlocking the full potential of these methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tianhe Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quillen_D/0/1/0/all/0/1"&gt;Deirdre Quillen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhanpeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1"&gt;Ryan Julian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1"&gt;Avnish Narayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shively_H/0/1/0/all/0/1"&gt;Hayden Shively&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellathur_A/0/1/0/all/0/1"&gt;Adithya Bellathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1"&gt;Karol Hausman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Deep Morphological Networks with Neural Architecture Search. (arXiv:2106.07714v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07714</id>
        <link href="http://arxiv.org/abs/2106.07714"/>
        <updated>2021-06-16T01:21:09.633Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) are generated by sequentially performing linear
and non-linear processes. Using a combination of linear and non-linear
procedures is critical for generating a sufficiently deep feature space. The
majority of non-linear operators are derivations of activation functions or
pooling functions. Mathematical morphology is a branch of mathematics that
provides non-linear operators for a variety of image processing problems. We
investigate the utility of integrating these operations in an end-to-end deep
learning framework in this paper. DNNs are designed to acquire a realistic
representation for a particular job. Morphological operators give topological
descriptors that convey salient information about the shapes of objects
depicted in images. We propose a method based on meta-learning to incorporate
morphological operators into DNNs. The learned architecture demonstrates how
our novel morphological operations significantly increase DNN performance on
various tasks, including picture classification and edge detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yufei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belkhir_N/0/1/0/all/0/1"&gt;Nacim Belkhir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angulo_J/0/1/0/all/0/1"&gt;Jesus Angulo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1"&gt;Angela Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1"&gt;Gianni Franchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Equivariant Energy Based Models with Equivariant Stein Variational Gradient Descent. (arXiv:2106.07832v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07832</id>
        <link href="http://arxiv.org/abs/2106.07832"/>
        <updated>2021-06-16T01:21:09.623Z</updated>
        <summary type="html"><![CDATA[We focus on the problem of efficient sampling and learning of probability
densities by incorporating symmetries in probabilistic models. We first
introduce Equivariant Stein Variational Gradient Descent algorithm -- an
equivariant sampling method based on Stein's identity for sampling from
densities with symmetries. Equivariant SVGD explicitly incorporates symmetry
information in a density through equivariant kernels which makes the resultant
sampler efficient both in terms of sample complexity and the quality of
generated samples. Subsequently, we define equivariant energy based models to
model invariant densities that are learned using contrastive divergence. By
utilizing our equivariant SVGD for training equivariant EBMs, we propose new
ways of improving and scaling up training of energy based models. We apply
these equivariant energy models for modelling joint densities in regression and
classification tasks for image datasets, many-body particle systems and
molecular structure generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaini_P/0/1/0/all/0/1"&gt;Priyank Jaini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holdijk_L/0/1/0/all/0/1"&gt;Lars Holdijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A White Paper on Neural Network Quantization. (arXiv:2106.08295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08295</id>
        <link href="http://arxiv.org/abs/2106.08295"/>
        <updated>2021-06-16T01:21:09.583Z</updated>
        <summary type="html"><![CDATA[While neural networks have advanced the frontiers in many applications, they
often come at a high computational cost. Reducing the power and latency of
neural network inference is key if we want to integrate modern networks into
edge devices with strict power and compute requirements. Neural network
quantization is one of the most effective ways of achieving these savings but
the additional noise it induces can lead to accuracy degradation. In this white
paper, we introduce state-of-the-art algorithms for mitigating the impact of
quantization noise on the network's performance while maintaining low-bit
weights and activations. We start with a hardware motivated introduction to
quantization and then consider two main classes of algorithms: Post-Training
Quantization (PTQ) and Quantization-Aware-Training (QAT). PTQ requires no
re-training or labelled data and is thus a lightweight push-button approach to
quantization. In most cases, PTQ is sufficient for achieving 8-bit quantization
with close to floating-point accuracy. QAT requires fine-tuning and access to
labeled training data but enables lower bit quantization with competitive
results. For both solutions, we provide tested pipelines based on existing
literature and extensive experimentation that lead to state-of-the-art
performance for common deep learning models and tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagel_M/0/1/0/all/0/1"&gt;Markus Nagel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fournarakis_M/0/1/0/all/0/1"&gt;Marios Fournarakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amjad_R/0/1/0/all/0/1"&gt;Rana Ali Amjad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bondarenko_Y/0/1/0/all/0/1"&gt;Yelysei Bondarenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baalen_M/0/1/0/all/0/1"&gt;Mart van Baalen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1"&gt;Tijmen Blankevoort&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.07806</id>
        <link href="http://arxiv.org/abs/2106.07806"/>
        <updated>2021-06-16T01:21:09.475Z</updated>
        <summary type="html"><![CDATA[Machine learning is revolutionizing image-based diagnostics in pathology and
radiology. ML models have shown promising results in research settings, but
their lack of interoperability has been a major barrier for clinical
integration and evaluation. The DICOM a standard specifies Information Object
Definitions and Services for the representation and communication of digital
images and related information, including image-derived annotations and
analysis results. However, the complexity of the standard represents an
obstacle for its adoption in the ML community and creates a need for software
libraries and tools that simplify working with data sets in DICOM format. Here
we present the highdicom library, which provides a high-level application
programming interface for the Python programming language that abstracts
low-level details of the standard and enables encoding and decoding of
image-derived information in DICOM format in a few lines of Python code. The
highdicom library ties into the extensive Python ecosystem for image processing
and machine learning. Simultaneously, by simplifying creation and parsing of
DICOM-compliant files, highdicom achieves interoperability with the medical
imaging systems that hold the data used to train and run ML models, and
ultimately communicate and store model outputs for clinical use. We demonstrate
through experiments with slide microscopy and computed tomography imaging,
that, by bridging these two ecosystems, highdicom enables developers to train
and evaluate state-of-the-art ML models in pathology and radiology while
remaining compliant with the DICOM standard and interoperable with clinical
systems at all stages. To promote standardization of ML research and streamline
the ML model development and deployment process, we made the library available
free and open-source.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bridge_C/0/1/0/all/0/1"&gt;Christopher P. Bridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gorman_C/0/1/0/all/0/1"&gt;Chris Gorman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pieper_S/0/1/0/all/0/1"&gt;Steven Pieper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Doyle_S/0/1/0/all/0/1"&gt;Sean W. Doyle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lennerz_J/0/1/0/all/0/1"&gt;Jochen K. Lennerz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1"&gt;Jayashree Kalpathy-Cramer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Clunie_D/0/1/0/all/0/1"&gt;David A. Clunie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fedorov_A/0/1/0/all/0/1"&gt;Andriy Y. Fedorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Herrmann_M/0/1/0/all/0/1"&gt;Markus D. Herrmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Site-Agnostic 3D Dose Distribution Prediction with Deep Learning Neural Networks. (arXiv:2106.07825v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07825</id>
        <link href="http://arxiv.org/abs/2106.07825"/>
        <updated>2021-06-16T01:21:09.468Z</updated>
        <summary type="html"><![CDATA[Typically, the current dose prediction models are limited to small amounts of
data and require re-training for a specific site, often leading to suboptimal
performance. We propose a site-agnostic, 3D dose distribution prediction model
using deep learning that can leverage data from any treatment site, thus
increasing the total data available to train the model. Applying our proposed
model to a new target treatment site requires only a brief fine-tuning of the
model to the new data and involves no modifications to the model input channels
or its parameters. Thus, it can be efficiently adapted to a different treatment
site, even with a small training dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mashayekhi_M/0/1/0/all/0/1"&gt;Maryam Mashayekhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tapia_I/0/1/0/all/0/1"&gt;Itzel Ramirez Tapia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balagopal_A/0/1/0/all/0/1"&gt;Anjali Balagopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1"&gt;Xinran Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barkousaraie_A/0/1/0/all/0/1"&gt;Azar Sadeghnejad Barkousaraie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McBeth_R/0/1/0/all/0/1"&gt;Rafe McBeth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mu-Han Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Steve Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dan Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HUMAP: Hierarchical Uniform Manifold Approximation and Projection. (arXiv:2106.07718v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07718</id>
        <link href="http://arxiv.org/abs/2106.07718"/>
        <updated>2021-06-16T01:21:09.401Z</updated>
        <summary type="html"><![CDATA[Dimensionality reduction (DR) techniques help analysts to understand patterns
in high-dimensional spaces. These techniques, often represented by scatter
plots, are employed in diverse science domains and facilitate similarity
analysis among clusters and data samples. For datasets containing many
granularities or when analysis follows the information visualization mantra,
hierarchical DR techniques are the most suitable approach since they present
major structures beforehand and details on demand. However, current
hierarchical DR techniques are not fully capable of addressing literature
problems because they do not preserve the projection mental map across
hierarchical levels or are not suitable for most data types. This work presents
HUMAP, a novel hierarchical dimensionality reduction technique designed to be
flexible on preserving local and global structures and preserve the mental map
throughout hierarchical exploration. We provide empirical evidence of our
technique's superiority compared with current hierarchical approaches and show
two case studies to demonstrate its strengths.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+E%2E_W/0/1/0/all/0/1"&gt;Wilson E. Marc&amp;#xed;lio-Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eler_D/0/1/0/all/0/1"&gt;Danilo M. Eler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paulovich_F/0/1/0/all/0/1"&gt;Fernando V. Paulovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_R/0/1/0/all/0/1"&gt;Rafael M. Martins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to find a unicorn: a novel model-free, unsupervised anomaly detection method for time series. (arXiv:2004.11468v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.11468</id>
        <link href="http://arxiv.org/abs/2004.11468"/>
        <updated>2021-06-16T01:21:09.222Z</updated>
        <summary type="html"><![CDATA[Recognition of anomalous events is a challenging but critical task in many
scientific and industrial fields, especially when the properties of anomalies
are unknown. In this paper, we introduce a new anomaly concept called "unicorn"
or unique event and present a new, model-free, unsupervised detection algorithm
to detect unicorns. The key component of the new algorithm is the Temporal
Outlier Factor (TOF) to measure the uniqueness of events in continuous data
sets from dynamic systems. The concept of unique events differs significantly
from traditional outliers in many aspects: while repetitive outliers are no
longer unique events, a unique event is not necessarily an outlier; it does not
necessarily fall out from the distribution of normal activity. The performance
of our algorithm was examined in recognizing unique events on different types
of simulated data sets with anomalies and it was compared with the Local
Outlier Factor (LOF) and discord discovery algorithms. TOF had superior
performance compared to LOF and discord algorithms even in recognizing
traditional outliers and it also recognized unique events that those did not.
The benefits of the unicorn concept and the new detection method were
illustrated by example data sets from very different scientific fields. Our
algorithm successfully recognized unique events in those cases where they were
already known such as the gravitational waves of a binary black hole merger on
LIGO detector data and the signs of respiratory failure on ECG data series.
Furthermore, unique events were found on the LIBOR data set of the last 30
years.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benko_Z/0/1/0/all/0/1"&gt;Zsigmond Benk&amp;#x151;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babel_T/0/1/0/all/0/1"&gt;Tam&amp;#xe1;s B&amp;#xe1;bel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Somogyvari_Z/0/1/0/all/0/1"&gt;Zolt&amp;#xe1;n Somogyv&amp;#xe1;ri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spot the Difference: Topological Anomaly Detection via Geometric Alignment. (arXiv:2106.08233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08233</id>
        <link href="http://arxiv.org/abs/2106.08233"/>
        <updated>2021-06-16T01:21:09.202Z</updated>
        <summary type="html"><![CDATA[Geometric alignment appears in a variety of applications, ranging from domain
adaptation, optimal transport, and normalizing flows in machine learning;
optical flow and learned augmentation in computer vision and deformable
registration within biomedical imaging. A recurring challenge is the alignment
of domains whose topology is not the same; a problem that is routinely ignored,
potentially introducing bias in downstream analysis. As a first step towards
solving such alignment problems, we propose an unsupervised topological
difference detection algorithm. The model is based on a conditional variational
auto-encoder and detects topological anomalies with regards to a reference
alongside the registration step. We consider both a) topological changes in the
image under spatial variation and b) unexpected transformations. Our approach
is validated on a proxy task of unsupervised anomaly detection in images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Czolbe_S/0/1/0/all/0/1"&gt;Steffen Czolbe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1"&gt;Aasa Feragen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1"&gt;Oswin Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08285</id>
        <link href="http://arxiv.org/abs/2106.08285"/>
        <updated>2021-06-16T01:21:09.193Z</updated>
        <summary type="html"><![CDATA[Time-lapse fluorescent microscopy (TLFM) combined with predictive
mathematical modelling is a powerful tool to study the inherently dynamic
processes of life on the single-cell level. Such experiments are costly,
complex and labour intensive. A complimentary approach and a step towards
completely in silico experiments, is to synthesise the imagery itself. Here, we
propose Multi-StyleGAN as a descriptive approach to simulate time-lapse
fluorescence microscopy imagery of living cells, based on a past experiment.
This novel generative adversarial network synthesises a multi-domain sequence
of consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple
live yeast cells in microstructured environments and train on a dataset
recorded in our laboratory. The simulation captures underlying biophysical
factors and time dependencies, such as cell morphology, growth, physical
interactions, as well as the intensity of a fluorescent reporter protein. An
immediate application is to generate additional training and validation data
for feature extraction algorithms or to aid and expedite development of
advanced experimental techniques such as online monitoring or control of cells.

Code and dataset is available at
https://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1"&gt;Tim Prangemeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1"&gt;Christoph Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1"&gt;Christian Wildner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1"&gt;Heinz Koeppl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Data Augmentation samples for Semantic Segmentation of Salt Bodies in a Synthetic Seismic Image Dataset. (arXiv:2106.08269v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08269</id>
        <link href="http://arxiv.org/abs/2106.08269"/>
        <updated>2021-06-16T01:21:09.187Z</updated>
        <summary type="html"><![CDATA[Nowadays, subsurface salt body localization and delineation, also called
semantic segmentation of salt bodies, are among the most challenging
geophysicist tasks. Thus, identifying large salt bodies is notoriously tricky
and is crucial for identifying hydrocarbon reservoirs and drill path planning.
This work proposes a Data Augmentation method based on training two generative
models to augment the number of samples in a seismic image dataset for the
semantic segmentation of salt bodies. Our method uses deep learning models to
generate pairs of seismic image patches and their respective salt masks for the
Data Augmentation. The first model is a Variational Autoencoder and is
responsible for generating patches of salt body masks. The second is a
Conditional Normalizing Flow model, which receives the generated masks as
inputs and generates the associated seismic image patches. We evaluate the
proposed method by comparing the performance of ten distinct state-of-the-art
models for semantic segmentation, trained with and without the generated
augmentations, in a dataset from two synthetic seismic images. The proposed
methodology yields an average improvement of 8.57% in the IoU metric across all
compared models. The best result is achieved by a DeeplabV3+ model variant,
which presents an IoU score of 95.17% when trained with our augmentations.
Additionally, our proposal outperformed six selected data augmentation methods,
and the most significant improvement in the comparison, of 9.77%, is achieved
by composing our DA with augmentations from an elastic transformation. At last,
we show that the proposed method is adaptable for a larger context size by
achieving results comparable to the obtained on the smaller context size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1"&gt;Luis Felipe Henriques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1"&gt;S&amp;#xe9;rgio Colcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1"&gt;Ruy Luiz Milidi&amp;#xfa;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulcao_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Bulc&amp;#xe3;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barros_P/0/1/0/all/0/1"&gt;Pablo Barros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multivariate Business Process Representation Learning utilizing Gramian Angular Fields and Convolutional Neural Networks. (arXiv:2106.08027v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08027</id>
        <link href="http://arxiv.org/abs/2106.08027"/>
        <updated>2021-06-16T01:21:09.180Z</updated>
        <summary type="html"><![CDATA[Learning meaningful representations of data is an important aspect of machine
learning and has recently been successfully applied to many domains like
language understanding or computer vision. Instead of training a model for one
specific task, representation learning is about training a model to capture all
useful information in the underlying data and make it accessible for a
predictor. For predictive process analytics, it is essential to have all
explanatory characteristics of a process instance available when making
predictions about the future, as well as for clustering and anomaly detection.
Due to the large variety of perspectives and types within business process
data, generating a good representation is a challenging task. In this paper, we
propose a novel approach for representation learning of business process
instances which can process and combine most perspectives in an event log. In
conjunction with a self-supervised pre-training method, we show the
capabilities of the approach through a visualization of the representation
space and case retrieval. Furthermore, the pre-trained model is fine-tuned to
multiple process prediction tasks and demonstrates its effectiveness in
comparison with existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_P/0/1/0/all/0/1"&gt;Peter Pfeiffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahann_J/0/1/0/all/0/1"&gt;Johannes Lahann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fettke_P/0/1/0/all/0/1"&gt;Peter Fettke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypergraph Dissimilarity Measures. (arXiv:2106.08206v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08206</id>
        <link href="http://arxiv.org/abs/2106.08206"/>
        <updated>2021-06-16T01:21:09.173Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose two novel approaches for hypergraph comparison. The
first approach transforms the hypergraph into a graph representation for use of
standard graph dissimilarity measures. The second approach exploits the
mathematics of tensors to intrinsically capture multi-way relations. For each
approach, we present measures that assess hypergraph dissimilarity at a
specific scale or provide a more holistic multi-scale comparison. We test these
measures on synthetic hypergraphs and apply them to biological datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Surana_A/0/1/0/all/0/1"&gt;Amit Surana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Can Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajapakse_I/0/1/0/all/0/1"&gt;Indika Rajapakse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EuroCrops: A Pan-European Dataset for Time Series Crop Type Classification. (arXiv:2106.08151v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08151</id>
        <link href="http://arxiv.org/abs/2106.08151"/>
        <updated>2021-06-16T01:21:09.149Z</updated>
        <summary type="html"><![CDATA[We present EuroCrops, a dataset based on self-declared field annotations for
training and evaluating methods for crop type classification and mapping,
together with its process of acquisition and harmonisation. By this, we aim to
enrich the research efforts and discussion for data-driven land cover
classification via Earth observation and remote sensing. Additionally, through
inclusion of self-declarations gathered in the scope of subsidy control from
all countries of the European Union (EU), this dataset highlights the
difficulties and pitfalls one comes across when operating on a transnational
level. We, therefore, also introduce a new taxonomy scheme, HCAT-ID, that
aspires to capture all the aspects of reference data originating from
administrative and agency databases. To address researchers from both the
remote sensing and the computer vision and machine learning communities, we
publish the dataset in different formats and processing levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schneider_M/0/1/0/all/0/1"&gt;Maja Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Broszeit_A/0/1/0/all/0/1"&gt;Amelie Broszeit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korner_M/0/1/0/all/0/1"&gt;Marco K&amp;#xf6;rner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graphical Gaussian Process Regression Model for Aqueous Solvation Free Energy Prediction of Organic Molecules in Redox Flow Battery. (arXiv:2106.08146v1 [cs.CE])]]></title>
        <id>http://arxiv.org/abs/2106.08146</id>
        <link href="http://arxiv.org/abs/2106.08146"/>
        <updated>2021-06-16T01:21:09.140Z</updated>
        <summary type="html"><![CDATA[The solvation free energy of organic molecules is a critical parameter in
determining emergent properties such as solubility, liquid-phase equilibrium
constants, and pKa and redox potentials in an organic redox flow battery. In
this work, we present a machine learning (ML) model that can learn and predict
the aqueous solvation free energy of an organic molecule using Gaussian process
regression method based on a new molecular graph kernel. To investigate the
performance of the ML model on electrostatic interaction, the nonpolar
interaction contribution of solvent and the conformational entropy of solute in
solvation free energy, three data sets with implicit or explicit water solvent
models, and contribution of conformational entropy of solute are tested. We
demonstrate that our ML model can predict the solvation free energy of
molecules at chemical accuracy with a mean absolute error of less than 1
kcal/mol for subsets of the QM9 dataset and the Freesolv database. To solve the
general data scarcity problem for a graph-based ML model, we propose a
dimension reduction algorithm based on the distance between molecular graphs,
which can be used to examine the diversity of the molecular data set. It
provides a promising way to build a minimum training set to improve prediction
for certain test sets where the space of molecular structures is predetermined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peiyuan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yu-Hang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Muqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1"&gt;Amity Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murugesan_V/0/1/0/all/0/1"&gt;Vijayakumar Murugesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hollas_A/0/1/0/all/0/1"&gt;Aaron Hollas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Now You See It, Now You Dont: Adversarial Vulnerabilities in Computational Pathology. (arXiv:2106.08153v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08153</id>
        <link href="http://arxiv.org/abs/2106.08153"/>
        <updated>2021-06-16T01:21:09.112Z</updated>
        <summary type="html"><![CDATA[Deep learning models are routinely employed in computational pathology
(CPath) for solving problems of diagnostic and prognostic significance.
Typically, the generalization performance of CPath models is analyzed using
evaluation protocols such as cross-validation and testing on multi-centric
cohorts. However, to ensure that such CPath solutions are robust and safe for
use in a clinical setting, a critical analysis of their predictive performance
and vulnerability to adversarial attacks is required, which is the focus of
this paper. Specifically, we show that a highly accurate model for
classification of tumour patches in pathology images (AUC > 0.95) can easily be
attacked with minimal perturbations which are imperceptible to lay humans and
trained pathologists alike. Our analytical results show that it is possible to
generate single-instance white-box attacks on specific input images with high
success rate and low perturbation energy. Furthermore, we have also generated a
single universal perturbation matrix using the training dataset only which,
when added to unseen test images, results in forcing the trained neural network
to flip its prediction labels with high confidence at a success rate of > 84%.
We systematically analyze the relationship between perturbation energy of an
adversarial attack, its impact on morphological constructs of clinical
significance, their perceptibility by a trained pathologist and saliency maps
obtained using deep learning models. Based on our analysis, we strongly
recommend that computational pathology models be critically analyzed using the
proposed adversarial validation strategy prior to clinical adoption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Foote_A/0/1/0/all/0/1"&gt;Alex Foote&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asif_A/0/1/0/all/0/1"&gt;Amina Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Azam_A/0/1/0/all/0/1"&gt;Ayesha Azam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1"&gt;Nasir Rajpoot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1"&gt;Fayyaz Minhas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Long-term Non-invasive Monitoring for Epilepsy via Wearable EEG Devices. (arXiv:2106.08008v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.08008</id>
        <link href="http://arxiv.org/abs/2106.08008"/>
        <updated>2021-06-16T01:21:09.103Z</updated>
        <summary type="html"><![CDATA[We present the implementation of seizure detection algorithms based on a
minimal number of EEG channels on a parallel ultra-low-power embedded platform.
The analyses are based on the CHB-MIT dataset, and include explorations of
different classification approaches (Support Vector Machines, Random Forest,
Extra Trees, AdaBoost) and different pre/post-processing techniques to maximize
sensitivity while guaranteeing no false alarms. We analyze global and
subject-specific approaches, considering all 23-electrodes or only 4 temporal
channels. For 8s window size and subject-specific approach, we report zero
false positives and 100% sensitivity. These algorithms are parallelized and
optimized for a parallel ultra-low power (PULP) platform, enabling 300h of
continuous monitoring on a 300 mAh battery, in a wearable form factor and power
budget. These results pave the way for the implementation of affordable,
wearable, long-term epilepsy monitoring solutions with low false-positive rates
and high sensitivity, meeting both patient and caregiver requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ingolfsson_T/0/1/0/all/0/1"&gt;Thorir Mar Ingolfsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cossettini_A/0/1/0/all/0/1"&gt;Andrea Cossettini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tabanelli_E/0/1/0/all/0/1"&gt;Enrico Tabanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tagliavini_G/0/1/0/all/0/1"&gt;Guiseppe Tagliavini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ryvlin_P/0/1/0/all/0/1"&gt;Philippe Ryvlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Benini_L/0/1/0/all/0/1"&gt;Luca Benini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MICo: Learning improved representations via sampling-based state similarity for Markov decision processes. (arXiv:2106.08229v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08229</id>
        <link href="http://arxiv.org/abs/2106.08229"/>
        <updated>2021-06-16T01:21:09.083Z</updated>
        <summary type="html"><![CDATA[We present a new behavioural distance over the state space of a Markov
decision process, and demonstrate the use of this distance as an effective
means of shaping the learnt representations of deep reinforcement learning
agents. While existing notions of state similarity are typically difficult to
learn at scale due to high computational cost and lack of sample-based
algorithms, our newly-proposed distance addresses both of these issues. In
addition to providing detailed theoretical analysis, we provide empirical
evidence that learning this distance alongside the value function yields
structured and informative representations, including strong results on the
Arcade Learning Environment benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1"&gt;Pablo Samuel Castro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kastner_T/0/1/0/all/0/1"&gt;Tyler Kastner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panangaden_P/0/1/0/all/0/1"&gt;Prakash Panangaden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rowland_M/0/1/0/all/0/1"&gt;Mark Rowland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Application of the Quantum Potential Neural Network to multi-electronic atoms. (arXiv:2106.08138v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.08138</id>
        <link href="http://arxiv.org/abs/2106.08138"/>
        <updated>2021-06-16T01:21:09.067Z</updated>
        <summary type="html"><![CDATA[In this report, the application of the Quantum Potential Neural Network
(QPNN) framework to many electron atomic systems is presented. For this study,
full configuration interaction (FCI) one--electron density functions within
predefined limits of accuracy were used to train the QPNN. The obtained results
suggest that this new neural network is capable of learning the effective
potential functions of many electron atoms in a completely unsupervised manner,
and using only limited information from the probability density. Using the
effective potential functions learned for each of the studied systems the QPNN
was able to estimate the total energies of each of the systems (with a maximum
of 10 trials) with a remarkable accuracy when compared to the FCI energies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Corzo_H/0/1/0/all/0/1"&gt;Hector H. Corzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sehanobish_A/0/1/0/all/0/1"&gt;Arijit Sehanobish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kara_O/0/1/0/all/0/1"&gt;Onur Kara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualizing Multiple Tasks via Learning to Decompose. (arXiv:2106.08112v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08112</id>
        <link href="http://arxiv.org/abs/2106.08112"/>
        <updated>2021-06-16T01:21:09.050Z</updated>
        <summary type="html"><![CDATA[One single instance could possess multiple portraits and reveal diverse
relationships with others according to different contexts. Those ambiguities
increase the difficulty of learning a generalizable model when there exists one
concept or mixed concepts in a task. We propose a general approach Learning to
Decompose Network (LeadNet) for both two cases, which contextualizes a model
through meta-learning multiple maps for concepts discovery -- the
representations of instances are decomposed and adapted conditioned on the
contexts. Through taking a holistic view over multiple latent components over
instances in a sampled pseudo task, LeadNet learns to automatically select the
right concept via incorporating those rich semantics inside and between
objects. LeadNet demonstrates its superiority in various applications,
including exploring multiple views of confusing tasks, out-of-distribution
recognition, and few-shot image classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Da-Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lanqing Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiu-Shen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Awardee Solution of KDD Cup 2021 OGB Large-Scale Challenge Graph-Level Track. (arXiv:2106.08279v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08279</id>
        <link href="http://arxiv.org/abs/2106.08279"/>
        <updated>2021-06-16T01:21:09.031Z</updated>
        <summary type="html"><![CDATA[In this technical report, we present our solution of KDD Cup 2021 OGB
Large-Scale Challenge - PCQM4M-LSC Track. We adopt Graphormer and ExpC as our
basic models. We train each model by 8-fold cross-validation, and additionally
train two Graphormer models on the union of training and validation sets with
different random seeds. For final submission, we use a naive ensemble for these
18 models by taking average of their outputs. Using our method, our team
MachineLearning achieved 0.1200 MAE on test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1"&gt;Chengxuan Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mingqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuxin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1"&gt;Guolin Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shengjie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenglin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yanming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the List Decoding Version of the Cyclically Equivariant Neural Decoder. (arXiv:2106.07964v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.07964</id>
        <link href="http://arxiv.org/abs/2106.07964"/>
        <updated>2021-06-16T01:21:09.025Z</updated>
        <summary type="html"><![CDATA[The cyclically equivariant neural decoder was recently proposed in [Chen-Ye,
International Conference on Machine Learning, 2021] to decode cyclic codes. In
the same paper, a list decoding procedure was also introduced for two widely
used classes of cyclic codes -- BCH codes and punctured Reed-Muller (RM) codes.
While the list decoding procedure significantly improves the Frame Error Rate
(FER) of the cyclically equivariant neural decoder, the Bit Error Rate (BER) of
the list decoding procedure is even worse than the unique decoding algorithm
when the list size is small. In this paper, we propose an improved version of
the list decoding algorithm for BCH codes and punctured RM codes. Our new
proposal significantly reduces the BER while maintaining the same (in some
cases even smaller) FER. More specifically, our new decoder provides up to
$2$dB gain over the previous list decoder when measured by BER, and the running
time of our new decoder is $15\%$ smaller. Code available at
https://github.com/improvedlistdecoder/code]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1"&gt;Min Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning for Conservation Decisions. (arXiv:2106.08272v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08272</id>
        <link href="http://arxiv.org/abs/2106.08272"/>
        <updated>2021-06-16T01:21:09.019Z</updated>
        <summary type="html"><![CDATA[Can machine learning help us make better decisions about a changing planet?
In this paper, we illustrate and discuss the potential of a promising corner of
machine learning known as _reinforcement learning_ (RL) to help tackle the most
challenging conservation decision problems. RL is uniquely well suited to
conservation and global change challenges for three reasons: (1) RL explicitly
focuses on designing an agent who _interacts_ with an environment which is
dynamic and uncertain, (2) RL approaches do not require massive amounts of
data, (3) RL approaches would utilize rather than replace existing models,
simulations, and the knowledge they contain. We provide a conceptual and
technical introduction to RL and its relevance to ecological and conservation
challenges, including examples of a problem in setting fisheries quotas and in
managing ecological tipping points. Four appendices with annotated code provide
a tangible introduction to researchers looking to adopt, evaluate, or extend
these approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lapeyrolerie_M/0/1/0/all/0/1"&gt;Marcus Lapeyrolerie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chapman_M/0/1/0/all/0/1"&gt;Melissa S. Chapman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norman_K/0/1/0/all/0/1"&gt;Kari E. A. Norman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boettiger_C/0/1/0/all/0/1"&gt;Carl Boettiger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Representation and Inference for NLP. (arXiv:2106.08117v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08117</id>
        <link href="http://arxiv.org/abs/2106.08117"/>
        <updated>2021-06-16T01:21:09.012Z</updated>
        <summary type="html"><![CDATA[Semantic representation and inference is essential for Natural Language
Processing (NLP). The state of the art for semantic representation and
inference is deep learning, and particularly Recurrent Neural Networks (RNNs),
Convolutional Neural Networks (CNNs), and transformer Self-Attention models.
This thesis investigates the use of deep learning for novel semantic
representation and inference, and makes contributions in the following three
areas: creating training data, improving semantic representations and extending
inference learning. In terms of creating training data, we contribute the
largest publicly available dataset of real-life factual claims for the purpose
of automatic claim verification (MultiFC), and we present a novel inference
model composed of multi-scale CNNs with different kernel sizes that learn from
external sources to infer fact checking labels. In terms of improving semantic
representations, we contribute a novel model that captures non-compositional
semantic indicators. By definition, the meaning of a non-compositional phrase
cannot be inferred from the individual meanings of its composing words (e.g.,
hot dog). Motivated by this, we operationalize the compositionality of a phrase
contextually by enriching the phrase representation with external word
embeddings and knowledge graphs. Finally, in terms of inference learning, we
propose a series of novel deep learning architectures that improve inference by
using syntactic dependencies, by ensembling role guided attention heads,
incorporating gating layers, and concatenating multiple heads in novel and
effective ways. This thesis consists of seven publications (five published and
two under review).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dongsheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness as Equality of Opportunity: Normative Guidance from Political Philosophy. (arXiv:2106.08259v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2106.08259</id>
        <link href="http://arxiv.org/abs/2106.08259"/>
        <updated>2021-06-16T01:21:09.006Z</updated>
        <summary type="html"><![CDATA[Recent interest in codifying fairness in Automated Decision Systems (ADS) has
resulted in a wide range of formulations of what it means for an algorithmic
system to be fair. Most of these propositions are inspired by, but inadequately
grounded in, political philosophy scholarship. This paper aims to correct that
deficit. We introduce a taxonomy of fairness ideals using doctrines of Equality
of Opportunity (EOP) from political philosophy, clarifying their conceptions in
philosophy and the proposed codification in fair machine learning. We arrange
these fairness ideals onto an EOP spectrum, which serves as a useful frame to
guide the design of a fair ADS in a given context.

We use our fairness-as-EOP framework to re-interpret the impossibility
results from a philosophical perspective, as the in-compatibility between
different value systems, and demonstrate the utility of the framework with
several real-world and hypothetical examples. Through our EOP-framework we hope
to answer what it means for an ADS to be fair from a moral and political
philosophy standpoint, and to pave the way for similar scholarship from ethics
and legal experts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Falaah Arif Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manis_E/0/1/0/all/0/1"&gt;Eleni Manis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanovich_J/0/1/0/all/0/1"&gt;Julia Stoyanovich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialectal Speech Recognition and Translation of Swiss German Speech to Standard German Text: Microsoft's Submission to SwissText 2021. (arXiv:2106.08126v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.08126</id>
        <link href="http://arxiv.org/abs/2106.08126"/>
        <updated>2021-06-16T01:21:08.999Z</updated>
        <summary type="html"><![CDATA[This paper describes the winning approach in the public SwissText 2021
competition on dialect recognition and translation of Swiss German speech to
standard German text. Swiss German refers to the multitude of Alemannic
dialects spoken in the German-speaking parts of Switzerland. Swiss German
differs significantly from standard German in pronunciation, word inventory and
grammar. It is mostly incomprehensible to native German speakers. Moreover, it
lacks a standardized written script. To solve the challenging task, we propose
a hybrid automatic speech recognition system with a lexicon that incorporates
translations, a 1st pass language model that deals with Swiss German
particularities, a transfer-learned acoustic model and a strong neural language
model for 2nd pass rescoring. Our submission reaches 46.04% BLEU on a blind
conversational test set and outperforms the second best competitor by a 12%
relative margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Arabskyy_Y/0/1/0/all/0/1"&gt;Yuriy Arabskyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Aashish Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dey_S/0/1/0/all/0/1"&gt;Subhadeep Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Koller_O/0/1/0/all/0/1"&gt;Oscar Koller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Incident Prediction Models Over Large Geographical Areas for Emergency Response Systems. (arXiv:2106.08307v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08307</id>
        <link href="http://arxiv.org/abs/2106.08307"/>
        <updated>2021-06-16T01:21:08.980Z</updated>
        <summary type="html"><![CDATA[Principled decision making in emergency response management necessitates the
use of statistical models that predict the spatial-temporal likelihood of
incident occurrence. These statistical models are then used for proactive
stationing which allocates first responders across the spatial area in order to
reduce overall response time. Traditional methods that simply aggregate past
incidents over space and time fail to make useful short-term predictions when
the spatial region is large and focused on fine-grained spatial entities like
interstate highway networks. This is partially due to the sparsity of incidents
with respect to the area in consideration. Further, accidents are affected by
several covariates, and collecting, cleaning, and managing multiple streams of
data from various sources is challenging for large spatial areas. In this
paper, we highlight how this problem is being solved for the state of
Tennessee, a state in the USA with a total area of over 100,000 sq. km. Our
pipeline, based on a combination of synthetic resampling, non-spatial
clustering, and learning from data can efficiently forecast the spatial and
temporal dynamics of accident occurrence, even under sparse conditions. In the
paper, we describe our pipeline that uses data related to roadway geometry,
weather, historical accidents, and real-time traffic congestion to aid accident
forecasting. To understand how our forecasting model can affect allocation and
dispatch, we improve upon a classical resource allocation approach.
Experimental results show that our approach can significantly reduce response
times in the field in comparison with current approaches followed by first
responders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vazirizade_S/0/1/0/all/0/1"&gt;Sayyed Mohsen Vazirizade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1"&gt;Ayan Mukhopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pettet_G/0/1/0/all/0/1"&gt;Geoffrey Pettet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Said_S/0/1/0/all/0/1"&gt;Said El Said&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baroud_H/0/1/0/all/0/1"&gt;Hiba Baroud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1"&gt;Abhishek Dubey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detect and remove watermark in deep neural networks via generative adversarial networks. (arXiv:2106.08104v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.08104</id>
        <link href="http://arxiv.org/abs/2106.08104"/>
        <updated>2021-06-16T01:21:08.973Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNN) have achieved remarkable performance in various
fields. However, training a DNN model from scratch requires a lot of computing
resources and training data. It is difficult for most individual users to
obtain such computing resources and training data. Model copyright infringement
is an emerging problem in recent years. For instance, pre-trained models may be
stolen or abuse by illegal users without the authorization of the model owner.
Recently, many works on protecting the intellectual property of DNN models have
been proposed. In these works, embedding watermarks into DNN based on backdoor
is one of the widely used methods. However, when the DNN model is stolen, the
backdoor-based watermark may face the risk of being detected and removed by an
adversary. In this paper, we propose a scheme to detect and remove watermark in
deep neural networks via generative adversarial networks (GAN). We demonstrate
that the backdoor-based DNN watermarks are vulnerable to the proposed GAN-based
watermark removal attack. The proposed attack method includes two phases. In
the first phase, we use the GAN and few clean images to detect and reverse the
watermark in the DNN model. In the second phase, we fine-tune the watermarked
DNN based on the reversed backdoor images. Experimental evaluations on the
MNIST and CIFAR10 datasets demonstrate that, the proposed method can
effectively remove about 98% of the watermark in DNN models, as the watermark
retention rate reduces from 100% to less than 2% after applying the proposed
attack. In the meantime, the proposed attack hardly affects the model's
performance. The test accuracy of the watermarked DNN on the MNIST and the
CIFAR10 datasets drops by less than 1% and 3%, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1"&gt;Mingfu Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shichang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yushu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiqiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Employing an Adjusted Stability Measure for Multi-Criteria Model Fitting on Data Sets with Similar Features. (arXiv:2106.08105v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08105</id>
        <link href="http://arxiv.org/abs/2106.08105"/>
        <updated>2021-06-16T01:21:08.964Z</updated>
        <summary type="html"><![CDATA[Fitting models with high predictive accuracy that include all relevant but no
irrelevant or redundant features is a challenging task on data sets with
similar (e.g. highly correlated) features. We propose the approach of tuning
the hyperparameters of a predictive model in a multi-criteria fashion with
respect to predictive accuracy and feature selection stability. We evaluate
this approach based on both simulated and real data sets and we compare it to
the standard approach of single-criteria tuning of the hyperparameters as well
as to the state-of-the-art technique "stability selection". We conclude that
our approach achieves the same or better predictive performance compared to the
two established approaches. Considering the stability during tuning does not
decrease the predictive accuracy of the resulting models. Our approach succeeds
at selecting the relevant features while avoiding irrelevant or redundant
features. The single-criteria approach fails at avoiding irrelevant or
redundant features and the stability selection approach fails at selecting
enough relevant features for achieving acceptable predictive accuracy. For our
approach, for data sets with many similar features, the feature selection
stability must be evaluated with an adjusted stability measure, that is, a
measure that considers similarities between features. For data sets with only
few similar features, an unadjusted stability measure suffices and is faster to
compute.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bommert_A/0/1/0/all/0/1"&gt;Andrea Bommert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rahnenfuhrer_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg Rahnenf&amp;#xfc;hrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lang_M/0/1/0/all/0/1"&gt;Michel Lang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Extraction and Adversarial Attacks on Neural Networks using Switching Power Information. (arXiv:2106.08299v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08299</id>
        <link href="http://arxiv.org/abs/2106.08299"/>
        <updated>2021-06-16T01:21:08.957Z</updated>
        <summary type="html"><![CDATA[Artificial neural networks (ANNs) have gained significant popularity in the
last decade for solving narrow AI problems in domains such as healthcare,
transportation, and defense. As ANNs become more ubiquitous, it is imperative
to understand their associated safety, security, and privacy vulnerabilities.
Recently, it has been shown that ANNs are susceptible to a number of
adversarial evasion attacks--inputs that cause the ANN to make high-confidence
misclassifications despite being almost indistinguishable from the data used to
train and test the network. This work explores to what degree finding these
examples maybe aided by using side-channel information, specifically switching
power consumption, of hardware implementations of ANNs. A black-box threat
scenario is assumed, where an attacker has access to the ANN hardware's input,
outputs, and topology, but the trained model parameters are unknown. Then, a
surrogate model is trained to have similar functional (i.e. input-output
mapping) and switching power characteristics as the oracle (black-box) model.
Our results indicate that the inclusion of power consumption data increases the
fidelity of the model extraction by up to 30 percent based on a mean square
error comparison of the oracle and surrogate weights. However, transferability
of adversarial examples from the surrogate to the oracle model was not
significantly affected.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tommy Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merkel_C/0/1/0/all/0/1"&gt;Cory Merkel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compression Implies Generalization. (arXiv:2106.07989v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07989</id>
        <link href="http://arxiv.org/abs/2106.07989"/>
        <updated>2021-06-16T01:21:08.950Z</updated>
        <summary type="html"><![CDATA[Explaining the surprising generalization performance of deep neural networks
is an active and important line of research in theoretical machine learning.
Influential work by Arora et al. (ICML'18) showed that, noise stability
properties of deep nets occurring in practice can be used to provably compress
model representations. They then argued that the small representations of
compressed networks imply good generalization performance albeit only of the
compressed nets. Extending their compression framework to yield generalization
bounds for the original uncompressed networks remains elusive.

Our main contribution is the establishment of a compression-based framework
for proving generalization bounds. The framework is simple and powerful enough
to extend the generalization bounds by Arora et al. to also hold for the
original network. To demonstrate the flexibility of the framework, we also show
that it allows us to give simple proofs of the strongest known generalization
bounds for other popular machine learning models, namely Support Vector
Machines and Boosting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gronlund_A/0/1/0/all/0/1"&gt;Allan Gr&amp;#xf8;nlund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hogsgaard_M/0/1/0/all/0/1"&gt;Mikael H&amp;#xf8;gsgaard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamma_L/0/1/0/all/0/1"&gt;Lior Kamma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1"&gt;Kasper Green Larsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSMix: Saliency-Based Span Mixup for Text Classification. (arXiv:2106.08062v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08062</id>
        <link href="http://arxiv.org/abs/2106.08062"/>
        <updated>2021-06-16T01:21:08.928Z</updated>
        <summary type="html"><![CDATA[Data augmentation with mixup has shown to be effective on various computer
vision tasks. Despite its great success, there has been a hurdle to apply mixup
to NLP tasks since text consists of discrete tokens with variable length. In
this work, we propose SSMix, a novel mixup method where the operation is
performed on input text rather than on hidden vectors like previous approaches.
SSMix synthesizes a sentence while preserving the locality of two original
texts by span-based mixing and keeping more tokens related to the prediction
relying on saliency information. With extensive experiments, we empirically
validate that our method outperforms hidden-level mixup methods on a wide range
of text classification benchmarks, including textual entailment, sentiment
classification, and question-type classification. Our code is available at
https://github.com/clovaai/ssmix.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Soyoung Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gyuwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyumin Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Very Deep Graph Neural Networks Via Noise Regularisation. (arXiv:2106.07971v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07971</id>
        <link href="http://arxiv.org/abs/2106.07971"/>
        <updated>2021-06-16T01:21:08.921Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) perform learned message passing over an input
graph, but conventional wisdom says performing more than handful of steps makes
training difficult and does not yield improved performance. Here we show the
contrary. We train a deep GNN with up to 100 message passing steps and achieve
several state-of-the-art results on two challenging molecular property
prediction benchmarks, Open Catalyst 2020 IS2RE and QM9. Our approach depends
crucially on a novel but simple regularisation method, which we call ``Noisy
Nodes'', in which we corrupt the input graph with noise and add an auxiliary
node autoencoder loss if the task is graph property prediction. Our results
show this regularisation method allows the model to monotonically improve in
performance with increased message passing steps. Our work opens new
opportunities for reaping the benefits of deep neural networks in the space of
graph and other structured prediction problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Godwin_J/0/1/0/all/0/1"&gt;Jonathan Godwin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaarschmidt_M/0/1/0/all/0/1"&gt;Michael Schaarschmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaunt_A/0/1/0/all/0/1"&gt;Alexander Gaunt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1"&gt;Alvaro Sanchez-Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubanova_Y/0/1/0/all/0/1"&gt;Yulia Rubanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1"&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirkpatrick_J/0/1/0/all/0/1"&gt;James Kirkpatrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1"&gt;Peter Battaglia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural continual learning: success is a journey, not (just) a destination. (arXiv:2106.08085v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08085</id>
        <link href="http://arxiv.org/abs/2106.08085"/>
        <updated>2021-06-16T01:21:08.915Z</updated>
        <summary type="html"><![CDATA[Biological agents are known to learn many different tasks over the course of
their lives, and to be able to revisit previous tasks and behaviors with little
to no loss in performance. In contrast, artificial agents are prone to
'catastrophic forgetting' whereby performance on previous tasks deteriorates
rapidly as new ones are acquired. This shortcoming has recently been addressed
using methods that encourage parameters to stay close to those used for
previous tasks. This can be done by (i) using specific parameter regularizers
that map out suitable destinations in parameter space, or (ii) guiding the
optimization journey by projecting gradients into subspaces that do not
interfere with previous tasks. However, parameter regularization has been shown
to be relatively ineffective in recurrent neural networks (RNNs), a setting
relevant to the study of neural dynamics supporting biological continual
learning. Similarly, projection based methods can reach capacity and fail to
learn any further as the number of tasks increases. To address these
limitations, we propose Natural Continual Learning (NCL), a new method that
unifies weight regularization and projected gradient descent. NCL uses Bayesian
weight regularization to encourage good performance on all tasks at convergence
and combines this with gradient projections designed to prevent catastrophic
forgetting during optimization. NCL formalizes gradient projection as a trust
region algorithm based on the Fisher information metric, and achieves
scalability via a novel Kronecker-factored approximation strategy. Our method
outperforms both standard weight regularization techniques and projection based
approaches when applied to continual learning problems in RNNs. The trained
networks evolve task-specific dynamics that are strongly preserved as new tasks
are learned, similar to experimental findings in biological circuits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kao_T/0/1/0/all/0/1"&gt;Ta-Chu Kao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jensen_K/0/1/0/all/0/1"&gt;Kristopher T. Jensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernacchia_A/0/1/0/all/0/1"&gt;Alberto Bernacchia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennequin_G/0/1/0/all/0/1"&gt;Guillaume Hennequin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S2Engine: A Novel Systolic Architecture for Sparse Convolutional Neural Networks. (arXiv:2106.07894v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2106.07894</id>
        <link href="http://arxiv.org/abs/2106.07894"/>
        <updated>2021-06-16T01:21:08.908Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have achieved great success in
performing cognitive tasks. However, execution of CNNs requires a large amount
of computing resources and generates heavy memory traffic, which imposes a
severe challenge on computing system design. Through optimizing parallel
executions and data reuse in convolution, systolic architecture demonstrates
great advantages in accelerating CNN computations. However, regular internal
data transmission path in traditional systolic architecture prevents the
systolic architecture from completely leveraging the benefits introduced by
neural network sparsity. Deployment of fine-grained sparsity on the existing
systolic architectures is greatly hindered by the incurred computational
overheads. In this work, we propose S2Engine $-$ a novel systolic architecture
that can fully exploit the sparsity in CNNs with maximized data reuse. S2Engine
transmits compressed data internally and allows each processing element to
dynamically select an aligned data from the compressed dataflow in convolution.
Compared to the naive systolic array, S2Engine achieves about $3.2\times$ and
about $3.0\times$ improvements on speed and energy efficiency, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianlei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1"&gt;Wenzhi Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xingzhou Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xucheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1"&gt;Pengcheng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Weisheng Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hotel Recognition via Latent Image Embedding. (arXiv:2106.08042v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08042</id>
        <link href="http://arxiv.org/abs/2106.08042"/>
        <updated>2021-06-16T01:21:08.901Z</updated>
        <summary type="html"><![CDATA[We approach the problem of hotel recognition with deep metric learning. We
overview the existing approaches and propose a modification to Contrastive loss
called Contrastive-Triplet loss. We construct a robust pipeline for
benchmarking metric learning models and perform experiments on Hotels-50K and
CUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval
on Hotels-50k. We open-source our code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tseytlin_B/0/1/0/all/0/1"&gt;Boris Tseytlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1"&gt;Ilya Makarov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Learning of Keypoint Representations for Continuous Control from Images. (arXiv:2106.07995v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07995</id>
        <link href="http://arxiv.org/abs/2106.07995"/>
        <updated>2021-06-16T01:21:08.881Z</updated>
        <summary type="html"><![CDATA[In many control problems that include vision, optimal controls can be
inferred from the location of the objects in the scene. This information can be
represented using keypoints, which is a list of spatial locations in the input
image. Previous works show that keypoint representations learned during
unsupervised pre-training using encoder-decoder architectures can provide good
features for control tasks. In this paper, we show that it is possible to learn
efficient keypoint representations end-to-end, without the need for
unsupervised pre-training, decoders, or additional losses. Our proposed
architecture consists of a differentiable keypoint extractor that feeds the
coordinates of the estimated keypoints directly to a soft actor-critic agent.
The proposed algorithm yields performance competitive to the state-of-the art
on DeepMind Control Suite tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boney_R/0/1/0/all/0/1"&gt;Rinu Boney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1"&gt;Alexander Ilin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1"&gt;Juho Kannala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Contrastive Explanations for Inductive Logic Programming Based on a Near Miss Approach. (arXiv:2106.08064v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08064</id>
        <link href="http://arxiv.org/abs/2106.08064"/>
        <updated>2021-06-16T01:21:08.874Z</updated>
        <summary type="html"><![CDATA[In recent research, human-understandable explanations of machine learning
models have received a lot of attention. Often explanations are given in form
of model simplifications or visualizations. However, as shown in cognitive
science as well as in early AI research, concept understanding can also be
improved by the alignment of a given instance for a concept with a similar
counterexample. Contrasting a given instance with a structurally similar
example which does not belong to the concept highlights what characteristics
are necessary for concept membership. Such near misses have been proposed by
Winston (1970) as efficient guidance for learning in relational domains. We
introduce an explanation generation algorithm for relational concepts learned
with Inductive Logic Programming (\textsc{GeNME}). The algorithm identifies
near miss examples from a given set of instances and ranks these examples by
their degree of closeness to a specific positive instance. A modified rule
which covers the near miss but not the original instance is given as an
explanation. We illustrate \textsc{GeNME} with the well known family domain
consisting of kinship relations, the visual relational Winston arches domain
and a real-world domain dealing with file management. We also present a
psychological experiment comparing human preferences of rule-based,
example-based, and near miss explanations in the family and the arches domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rabold_J/0/1/0/all/0/1"&gt;Johannes Rabold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siebers_M/0/1/0/all/0/1"&gt;Michael Siebers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1"&gt;Ute Schmid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning with Electronic Health Records is vulnerable to Backdoor Trigger Attacks. (arXiv:2106.07925v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07925</id>
        <link href="http://arxiv.org/abs/2106.07925"/>
        <updated>2021-06-16T01:21:08.868Z</updated>
        <summary type="html"><![CDATA[Electronic Health Records (EHRs) provide a wealth of information for machine
learning algorithms to predict the patient outcome from the data including
diagnostic information, vital signals, lab tests, drug administration, and
demographic information. Machine learning models can be built, for example, to
evaluate patients based on their predicted mortality or morbidity and to
predict required resources for efficient resource management in hospitals. In
this paper, we demonstrate that an attacker can manipulate the machine learning
predictions with EHRs easily and selectively at test time by backdoor attacks
with the poisoned training data. Furthermore, the poison we create has
statistically similar features to the original data making it hard to detect,
and can also attack multiple machine learning models without any knowledge of
the models. With less than 5% of the raw EHR data poisoned, we achieve average
attack success rates of 97% on mortality prediction tasks with MIMIC-III
database against Logistic Regression, Multilayer Perceptron, and Long
Short-term Memory models simultaneously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joe_B/0/1/0/all/0/1"&gt;Byunggill Joe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_A/0/1/0/all/0/1"&gt;Akshay Mehra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1"&gt;Insik Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamm_J/0/1/0/all/0/1"&gt;Jihun Hamm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Compensate: A Deep Neural Network Framework for 5G Power Amplifier Compensation. (arXiv:2106.07953v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.07953</id>
        <link href="http://arxiv.org/abs/2106.07953"/>
        <updated>2021-06-16T01:21:08.861Z</updated>
        <summary type="html"><![CDATA[Owing to the complicated characteristics of 5G communication system,
designing RF components through mathematical modeling becomes a challenging
obstacle. Moreover, such mathematical models need numerous manual adjustments
for various specification requirements. In this paper, we present a
learning-based framework to model and compensate Power Amplifiers (PAs) in 5G
communication. In the proposed framework, Deep Neural Networks (DNNs) are used
to learn the characteristics of the PAs, while, correspondent Digital
Pre-Distortions (DPDs) are also learned to compensate for the nonlinear and
memory effects of PAs. On top of the framework, we further propose two
frequency domain losses to guide the learning process to better optimize the
target, compared to naive time domain Mean Square Error (MSE). The proposed
framework serves as a drop-in replacement for the conventional approach. The
proposed approach achieves an average of 56.7% reduction of nonlinear and
memory effects, which converts to an average of 16.3% improvement over a
carefully-designed mathematical model, and even reaches 34% enhancement in
severe distortion scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1"&gt;Po-Yu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yi-Min Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuo_H/0/1/0/all/0/1"&gt;Hsien-Kai Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hantao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hsin-Hung Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_S/0/1/0/all/0/1"&gt;Sheng-Hong Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ou_W/0/1/0/all/0/1"&gt;Wei-Lun Ou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Chia-Ming Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning for Internet of Things: A Federated Learning Framework for On-device Anomaly Data Detection. (arXiv:2106.07976v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07976</id>
        <link href="http://arxiv.org/abs/2106.07976"/>
        <updated>2021-06-16T01:21:08.854Z</updated>
        <summary type="html"><![CDATA[Federated learning can be a promising solution for enabling IoT cybersecurity
(i.e., anomaly detection in the IoT environment) while preserving data privacy
and mitigating the high communication/storage overhead (e.g., high-frequency
data from time-series sensors) of centralized over-the-cloud approaches. In
this paper, to further push forward this direction with a comprehensive study
in both algorithm and system design, we build FedIoT platform that contains a
synthesized dataset using N-BaIoT, FedDetect algorithm, and a system design for
IoT devices. Furthermore, the proposed FedDetect learning framework improves
the performance by utilizing an adaptive optimizer (e.g., Adam) and a
cross-round learning rate scheduler. In a network of realistic IoT devices
(Raspberry PI), we evaluate FedIoT platform and FedDetect algorithm in both
model and system performance. Our results demonstrate the efficacy of federated
learning in detecting a large range of attack types. The system efficiency
analysis indicates that both end-to-end training time and memory cost are
affordable and promising for resource-constrained IoT devices. The source code
is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tuo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1"&gt;Chaoyang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tianhao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Mark Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1"&gt;Salman Avestimehr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequence-Level Training for Non-Autoregressive Neural Machine Translation. (arXiv:2106.08122v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08122</id>
        <link href="http://arxiv.org/abs/2106.08122"/>
        <updated>2021-06-16T01:21:08.846Z</updated>
        <summary type="html"><![CDATA[In recent years, Neural Machine Translation (NMT) has achieved notable
results in various translation tasks. However, the word-by-word generation
manner determined by the autoregressive mechanism leads to high translation
latency of the NMT and restricts its low-latency applications.
Non-Autoregressive Neural Machine Translation (NAT) removes the autoregressive
mechanism and achieves significant decoding speedup through generating target
words independently and simultaneously. Nevertheless, NAT still takes the
word-level cross-entropy loss as the training objective, which is not optimal
because the output of NAT cannot be properly evaluated due to the multimodality
problem. In this paper, we propose using sequence-level training objectives to
train NAT models, which evaluate the NAT outputs as a whole and correlates well
with the real translation quality. Firstly, we propose training NAT models to
optimize sequence-level evaluation metrics (e.g., BLEU) based on several novel
reinforcement algorithms customized for NAT, which outperforms the conventional
method by reducing the variance of gradient estimation. Secondly, we introduce
a novel training objective for NAT models, which aims to minimize the
Bag-of-Ngrams (BoN) difference between the model output and the reference
sentence. The BoN training objective is differentiable and can be calculated
efficiently without doing any approximations. Finally, we apply a three-stage
training strategy to combine these two methods to train the NAT model. We
validate our approach on four translation tasks (WMT14 En$\leftrightarrow$De,
WMT16 En$\leftrightarrow$Ro), which shows that our approach largely outperforms
NAT baselines and achieves remarkable performance on all translation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1"&gt;Chenze Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceptually-inspired super-resolution of compressed videos. (arXiv:2106.08147v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08147</id>
        <link href="http://arxiv.org/abs/2106.08147"/>
        <updated>2021-06-16T01:21:08.826Z</updated>
        <summary type="html"><![CDATA[Spatial resolution adaptation is a technique which has often been employed in
video compression to enhance coding efficiency. This approach encodes a lower
resolution version of the input video and reconstructs the original resolution
during decoding. Instead of using conventional up-sampling filters, recent work
has employed advanced super-resolution methods based on convolutional neural
networks (CNNs) to further improve reconstruction quality. These approaches are
usually trained to minimise pixel-based losses such as Mean-Squared Error
(MSE), despite the fact that this type of loss metric does not correlate well
with subjective opinions. In this paper, a perceptually-inspired
super-resolution approach (M-SRGAN) is proposed for spatial up-sampling of
compressed video using a modified CNN model, which has been trained using a
generative adversarial network (GAN) on compressed content with perceptual loss
functions. The proposed method was integrated with HEVC HM 16.20, and has been
evaluated on the JVET Common Test Conditions (UHD test sequences) using the
Random Access configuration. The results show evident perceptual quality
improvement over the original HM 16.20, with an average bitrate saving of 35.6%
(Bj{\o}ntegaard Delta measurement) based on a perceptual quality metric, VMAF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ma_D/0/1/0/all/0/1"&gt;Di Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Afonso_M/0/1/0/all/0/1"&gt;Mariana Afonso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1"&gt;David R. Bull&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Abstractive Opinion Summarization by Generating Sentences with Tree-Structured Topic Guidance. (arXiv:2106.08007v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08007</id>
        <link href="http://arxiv.org/abs/2106.08007"/>
        <updated>2021-06-16T01:21:08.819Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel unsupervised abstractive summarization method for
opinionated texts. While the basic variational autoencoder-based models assume
a unimodal Gaussian prior for the latent code of sentences, we alternate it
with a recursive Gaussian mixture, where each mixture component corresponds to
the latent code of a topic sentence and is mixed by a tree-structured topic
distribution. By decoding each Gaussian component, we generate sentences with
tree-structured topic guidance, where the root sentence conveys generic
content, and the leaf sentences describe specific topics. Experimental results
demonstrate that the generated topic sentences are appropriate as a summary of
opinionated texts, which are more informative and cover more input contents
than those generated by the recent unsupervised summarization model
(Bra\v{z}inskas et al., 2020). Furthermore, we demonstrate that the variance of
latent Gaussians represents the granularity of sentences, analogous to Gaussian
word embedding (Vilnis and McCallum, 2015).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Isonuma_M/0/1/0/all/0/1"&gt;Masaru Isonuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_J/0/1/0/all/0/1"&gt;Junichiro Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1"&gt;Danushka Bollegala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakata_I/0/1/0/all/0/1"&gt;Ichiro Sakata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coupled Gradient Estimators for Discrete Latent Variables. (arXiv:2106.08056v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08056</id>
        <link href="http://arxiv.org/abs/2106.08056"/>
        <updated>2021-06-16T01:21:08.813Z</updated>
        <summary type="html"><![CDATA[Training models with discrete latent variables is challenging due to the high
variance of unbiased gradient estimators. While low-variance reparameterization
gradients of a continuous relaxation can provide an effective solution, a
continuous relaxation is not always available or tractable. Dong et al. (2020)
and Yin et al. (2020) introduced a performant estimator that does not rely on
continuous relaxations; however, it is limited to binary random variables. We
introduce a novel derivation of their estimator based on importance sampling
and statistical couplings, which we extend to the categorical setting.
Motivated by the construction of a stick-breaking coupling, we introduce
gradient estimators based on reparameterizing categorical variables as
sequences of binary variables and Rao-Blackwellization. In systematic
experiments, we show that our proposed categorical gradient estimators provide
state-of-the-art performance, whereas even with additional
Rao-Blackwellization, previous estimators (Yin et al., 2019) underperform a
simpler REINFORCE with a leave-one-out-baseline estimator (Kool et al., 2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhe Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mnih_A/0/1/0/all/0/1"&gt;Andriy Mnih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tucker_G/0/1/0/all/0/1"&gt;George Tucker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization. (arXiv:2106.07991v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.07991</id>
        <link href="http://arxiv.org/abs/2106.07991"/>
        <updated>2021-06-16T01:21:08.806Z</updated>
        <summary type="html"><![CDATA[Bi-level optimization model is able to capture a wide range of complex
learning tasks with practical interest. Due to the witnessed efficiency in
solving bi-level programs, gradient-based methods have gained popularity in the
machine learning community. In this work, we propose a new gradient-based
solution scheme, namely, the Bi-level Value-Function-based Interior-point
Method (BVFIM). Following the main idea of the log-barrier interior-point
scheme, we penalize the regularized value function of the lower level problem
into the upper level objective. By further solving a sequence of differentiable
unconstrained approximation problems, we consequently derive a sequential
programming scheme. The numerical advantage of our scheme relies on the fact
that, when gradient methods are applied to solve the approximation problem, we
successfully avoid computing any expensive Hessian-vector or Jacobian-vector
product. We prove the convergence without requiring any convexity assumption on
either the upper level or the lower level objective. Experiments demonstrate
the efficiency of the proposed BVFIM on non-convex bi-level problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Liu_R/0/1/0/all/0/1"&gt;Risheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiaoming Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shangzhi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Evaluation of Sequential Machine Learning for Network Intrusion Detection. (arXiv:2106.07961v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.07961</id>
        <link href="http://arxiv.org/abs/2106.07961"/>
        <updated>2021-06-16T01:21:08.800Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning renewed the research interests in machine
learning for Network Intrusion Detection Systems (NIDS). Specifically,
attention has been given to sequential learning models, due to their ability to
extract the temporal characteristics of Network traffic Flows (NetFlows), and
use them for NIDS tasks. However, the applications of these sequential models
often consist of transferring and adapting methodologies directly from other
fields, without an in-depth investigation on how to leverage the specific
circumstances of cybersecurity scenarios; moreover, there is a lack of
comprehensive studies on sequential models that rely on NetFlow data, which
presents significant advantages over traditional full packet captures. We
tackle this problem in this paper. We propose a detailed methodology to extract
temporal sequences of NetFlows that denote patterns of malicious activities.
Then, we apply this methodology to compare the efficacy of sequential learning
models against traditional static learning models. In particular, we perform a
fair comparison of a `sequential' Long Short-Term Memory (LSTM) against a
`static' Feedforward Neural Networks (FNN) in distinct environments represented
by two well-known datasets for NIDS: the CICIDS2017 and the CTU13. Our results
highlight that LSTM achieves comparable performance to FNN in the CICIDS2017
with over 99.5\% F1-score; while obtaining superior performance in the CTU13,
with 95.7\% F1-score against 91.5\%. This paper thus paves the way to future
applications of sequential learning models for NIDS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Corsini_A/0/1/0/all/0/1"&gt;Andrea Corsini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shanchieh Jay Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Apruzzese_G/0/1/0/all/0/1"&gt;Giovanni Apruzzese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting the Calibration of Modern Neural Networks. (arXiv:2106.07998v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07998</id>
        <link href="http://arxiv.org/abs/2106.07998"/>
        <updated>2021-06-16T01:21:08.781Z</updated>
        <summary type="html"><![CDATA[Accurate estimation of predictive uncertainty (model calibration) is
essential for the safe application of neural networks. Many instances of
miscalibration in modern neural networks have been reported, suggesting a trend
that newer, more accurate models produce poorly calibrated predictions. Here,
we revisit this question for recent state-of-the-art image classification
models. We systematically relate model calibration and accuracy, and find that
the most recent models, notably those not using convolutions, are among the
best calibrated. Trends observed in prior model generations, such as decay of
calibration with distribution shift or model size, are less pronounced in
recent architectures. We also show that model size and amount of pretraining do
not fully explain these differences, suggesting that architecture is a major
determinant of calibration properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1"&gt;Matthias Minderer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1"&gt;Josip Djolonga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1"&gt;Rob Romijnders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubis_F/0/1/0/all/0/1"&gt;Frances Hubis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Dustin Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1"&gt;Mario Lucic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Query Embedding on Hyper-relational Knowledge Graphs. (arXiv:2106.08166v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.08166</id>
        <link href="http://arxiv.org/abs/2106.08166"/>
        <updated>2021-06-16T01:21:08.775Z</updated>
        <summary type="html"><![CDATA[Multi-hop logical reasoning is an established problem in the field of
representation learning on knowledge graphs (KGs). It subsumes both one-hop
link prediction as well as other more complex types of logical queries.
Existing algorithms operate only on classical, triple-based graphs, whereas
modern KGs often employ a hyper-relational modeling paradigm. In this paradigm,
typed edges may have several key-value pairs known as qualifiers that provide
fine-grained context for facts. In queries, this context modifies the meaning
of relations, and usually reduces the answer set. Hyper-relational queries are
often observed in real-world KG applications, and existing approaches for
approximate query answering cannot make use of qualifier pairs. In this work,
we bridge this gap and extend the multi-hop reasoning problem to
hyper-relational KGs allowing to tackle this new type of complex queries.
Building upon recent advancements in Graph Neural Networks and query embedding
techniques, we study how to embed and answer hyper-relational conjunctive
queries. Besides that, we propose a method to answer such queries and
demonstrate in our experiments that qualifiers improve query answering on a
diverse set of query patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alivanistos_D/0/1/0/all/0/1"&gt;Dimitrios Alivanistos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1"&gt;Max Berrendorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1"&gt;Michael Cochez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1"&gt;Mikhail Galkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thompson Sampling for Unimodal Bandits. (arXiv:2106.08187v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08187</id>
        <link href="http://arxiv.org/abs/2106.08187"/>
        <updated>2021-06-16T01:21:08.768Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a Thompson Sampling algorithm for \emph{unimodal}
bandits, where the expected reward is unimodal over the partially ordered arms.
To exploit the unimodal structure better, at each step, instead of exploration
from the entire decision space, our algorithm makes decision according to
posterior distribution only in the neighborhood of the arm that has the highest
empirical mean estimate. We theoretically prove that, for Bernoulli rewards,
the regret of our algorithm reaches the lower bound of unimodal bandits, thus
it is asymptotically optimal. For Gaussian rewards, the regret of our algorithm
is $\mathcal{O}(\log T)$, which is far better than standard Thompson Sampling
algorithms. Extensive experiments demonstrate the effectiveness of the proposed
algorithm on both synthetic data sets and the real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Long Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zehong Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1"&gt;Shasha Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shijian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1"&gt;Gang Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hongyang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Autonomy in Management of Wireless Random Networks. (arXiv:2106.07984v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.07984</id>
        <link href="http://arxiv.org/abs/2106.07984"/>
        <updated>2021-06-16T01:21:08.762Z</updated>
        <summary type="html"><![CDATA[This paper presents a machine learning strategy that tackles a distributed
optimization task in a wireless network with an arbitrary number of randomly
interconnected nodes. Individual nodes decide their optimal states with
distributed coordination among other nodes through randomly varying backhaul
links. This poses a technical challenge in distributed universal optimization
policy robust to a random topology of the wireless network, which has not been
properly addressed by conventional deep neural networks (DNNs) with rigid
structural configurations. We develop a flexible DNN formalism termed
distributed message-passing neural network (DMPNN) with forward and backward
computations independent of the network topology. A key enabler of this
approach is an iterative message-sharing strategy through arbitrarily connected
backhaul links. The DMPNN provides a convergent solution for iterative
coordination by learning numerous random backhaul interactions. The DMPNN is
investigated for various configurations of the power control in wireless
networks, and intensive numerical results prove its universality and viability
over conventional optimization and DNN approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sang Hyun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1"&gt;Tony Q. S. Quek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decomposition of Global Feature Importance into Direct and Associative Components (DEDACT). (arXiv:2106.08086v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08086</id>
        <link href="http://arxiv.org/abs/2106.08086"/>
        <updated>2021-06-16T01:21:08.731Z</updated>
        <summary type="html"><![CDATA[Global model-agnostic feature importance measures either quantify whether
features are directly used for a model's predictions (direct importance) or
whether they contain prediction-relevant information (associative importance).
Direct importance provides causal insight into the model's mechanism, yet it
fails to expose the leakage of information from associated but not directly
used variables. In contrast, associative importance exposes information leakage
but does not provide causal insight into the model's mechanism. We introduce
DEDACT - a framework to decompose well-established direct and associative
importance measures into their respective associative and direct components.
DEDACT provides insight into both the sources of prediction-relevant
information in the data and the direct and indirect feature pathways by which
the information enters the model. We demonstrate the method's usefulness on
simulated examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Konig_G/0/1/0/all/0/1"&gt;Gunnar K&amp;#xf6;nig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Freiesleben_T/0/1/0/all/0/1"&gt;Timo Freiesleben&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1"&gt;Bernd Bischl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Casalicchio_G/0/1/0/all/0/1"&gt;Giuseppe Casalicchio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Grosse_Wentrup_M/0/1/0/all/0/1"&gt;Moritz Grosse-Wentrup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Capabilities of Deep Learning Models on Learning Physical Relationships: Case of Rainfall-Runoff Modeling with LSTM. (arXiv:2106.07963v1 [physics.ao-ph])]]></title>
        <id>http://arxiv.org/abs/2106.07963</id>
        <link href="http://arxiv.org/abs/2106.07963"/>
        <updated>2021-06-16T01:21:08.651Z</updated>
        <summary type="html"><![CDATA[This study investigates the relationships which deep learning methods can
identify between the input and output data. As a case study, rainfall-runoff
modeling in a snow-dominated watershed by means of a long- and short-term
memory (LSTM) network is selected. Daily precipitation and mean air temperature
were used as model input to estimate daily flow discharge. After model training
and verification, two experimental simulations were conducted with hypothetical
inputs instead of observed meteorological data to clarify the response of the
trained model to the inputs. The first numerical experiment showed that even
without input precipitation, the trained model generated flow discharge,
particularly winter low flow and high flow during the snow-melting period. The
effects of warmer and colder conditions on the flow discharge were also
replicated by the trained model without precipitation. Additionally, the model
reflected only 17-39% of the total precipitation mass during the snow
accumulation period in the total annual flow discharge, revealing a strong lack
of water mass conservation. The results of this study indicated that a deep
learning method may not properly learn the explicit physical relationships
between input and target variables, although they are still capable of
maintaining strong goodness-of-fit results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Yokoo_K/0/1/0/all/0/1"&gt;Kazuki Yokoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ishida_K/0/1/0/all/0/1"&gt;Kei Ishida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ercan_A/0/1/0/all/0/1"&gt;Ali Ercan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tu_T/0/1/0/all/0/1"&gt;Tongbi Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Nagasato_T/0/1/0/all/0/1"&gt;Takeyoshi Nagasato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kiyama_M/0/1/0/all/0/1"&gt;Masato Kiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Amagasaki_M/0/1/0/all/0/1"&gt;Motoki Amagasaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization-friendly generic mechanisms without money. (arXiv:2106.07752v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.07752</id>
        <link href="http://arxiv.org/abs/2106.07752"/>
        <updated>2021-06-16T01:21:08.578Z</updated>
        <summary type="html"><![CDATA[The goal of this paper is to develop a generic framework for converting
modern optimization algorithms into mechanisms where inputs come from
self-interested agents. We focus on aggregating preferences from $n$ players in
a context without money. Special cases of this setting include voting,
allocation of items by lottery, and matching. Our key technical contribution is
a new meta-algorithm we call \apex (Adaptive Pricing Equalizing Externalities).
The framework is sufficiently general to be combined with any optimization
algorithm that is based on local search. We outline an agenda for studying the
algorithm's properties and its applications. As a special case of applying the
framework to the problem of one-sided assignment with lotteries, we obtain a
strengthening of the 1979 result by Hylland and Zeckhauser on allocation via a
competitive equilibrium from equal incomes (CEEI). The [HZ79] result posits
that there is a (fractional) allocation and a set of item prices such that the
allocation is a competitive equilibrium given prices. We further show that
there is always a reweighing of the players' utility values such that running
unit-demand VCG with reweighed utilities leads to a HZ-equilibrium prices.
Interestingly, not all HZ competitive equilibria come from VCG prices. As part
of our proof, we re-prove the [HZ79] result using only Brouwer's fixed point
theorem (and not the more general Kakutani's theorem). This may be of
independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Braverman_M/0/1/0/all/0/1"&gt;Mark Braverman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time Series Anomaly Detection for Cyber-physical Systems via Neural System Identification and Bayesian Filtering. (arXiv:2106.07992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07992</id>
        <link href="http://arxiv.org/abs/2106.07992"/>
        <updated>2021-06-16T01:21:08.560Z</updated>
        <summary type="html"><![CDATA[Recent advances in AIoT technologies have led to an increasing popularity of
utilizing machine learning algorithms to detect operational failures for
cyber-physical systems (CPS). In its basic form, an anomaly detection module
monitors the sensor measurements and actuator states from the physical plant,
and detects anomalies in these measurements to identify abnormal operation
status. Nevertheless, building effective anomaly detection models for CPS is
rather challenging as the model has to accurately detect anomalies in presence
of highly complicated system dynamics and unknown amount of sensor noise. In
this work, we propose a novel time series anomaly detection method called
Neural System Identification and Bayesian Filtering (NSIBF) in which a
specially crafted neural network architecture is posed for system
identification, i.e., capturing the dynamics of CPS in a dynamical state-space
model; then a Bayesian filtering algorithm is naturally applied on top of the
"identified" state-space model for robust anomaly detection by tracking the
uncertainty of the hidden state of the system recursively over time. We provide
qualitative as well as quantitative experiments with the proposed method on a
synthetic and three real-world CPS datasets, showing that NSIBF compares
favorably to the state-of-the-art methods with considerable improvements on
anomaly detection in CPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Cheng Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1"&gt;Pengwei Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges and Considerations with Code-Mixed NLP for Multilingual Societies. (arXiv:2106.07823v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07823</id>
        <link href="http://arxiv.org/abs/2106.07823"/>
        <updated>2021-06-16T01:21:08.554Z</updated>
        <summary type="html"><![CDATA[Multilingualism refers to the high degree of proficiency in two or more
languages in the written and oral communication modes. It often results in
language mixing, a.k.a. code-mixing, when a multilingual speaker switches
between multiple languages in a single utterance of a text or speech. This
paper discusses the current state of the NLP research, limitations, and
foreseeable pitfalls in addressing five real-world applications for social good
crisis management, healthcare, political campaigning, fake news, and hate
speech for multilingual societies. We also propose futuristic datasets, models,
and tools that can significantly advance the current research in multilingual
NLP applications for the societal good. As a representative example, we
consider English-Hindi code-mixing but draw similar inferences for other
language pairs]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1"&gt;Vivek Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the compromise between accuracy, interpretability and personalization of rule-based machine learning in medical problems. (arXiv:2106.07827v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07827</id>
        <link href="http://arxiv.org/abs/2106.07827"/>
        <updated>2021-06-16T01:21:08.548Z</updated>
        <summary type="html"><![CDATA[One of the key challenges when developing a predictive model is the
capability to describe the domain knowledge and the cause-effect relationships
in a simple way. Decision rules are a useful and important methodology in this
context, justifying their application in several areas, in particular in
clinical practice. Several machine-learning classifiers have exploited the
advantageous properties of decision rules to build intelligent prediction
models, namely decision trees and ensembles of trees (ETs). However, such
methodologies usually suffer from a trade-off between interpretability and
predictive performance. Some procedures consider a simplification of ETs, using
heuristic approaches to select an optimal reduced set of decision rules. In
this paper, we introduce a novel step to those methodologies. We create a new
component to predict if a given rule will be correct or not for a particular
patient, which introduces personalization into the procedure. Furthermore, the
validation results using three public clinical datasets show that it also
allows to increase the predictive performance of the selected set of rules,
improving the mentioned trade-off.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valente_F/0/1/0/all/0/1"&gt;Francisco Valente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paredes_S/0/1/0/all/0/1"&gt;Simao Paredes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1"&gt;Jorge Henriques&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code Integrity Attestation for PLCs using Black Box Neural Network Predictions. (arXiv:2106.07851v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.07851</id>
        <link href="http://arxiv.org/abs/2106.07851"/>
        <updated>2021-06-16T01:21:08.498Z</updated>
        <summary type="html"><![CDATA[Cyber-physical systems (CPSs) are widespread in critical domains, and
significant damage can be caused if an attacker is able to modify the code of
their programmable logic controllers (PLCs). Unfortunately, traditional
techniques for attesting code integrity (i.e. verifying that it has not been
modified) rely on firmware access or roots-of-trust, neither of which
proprietary or legacy PLCs are likely to provide. In this paper, we propose a
practical code integrity checking solution based on privacy-preserving black
box models that instead attest the input/output behaviour of PLC programs.
Using faithful offline copies of the PLC programs, we identify their most
important inputs through an information flow analysis, execute them on multiple
combinations to collect data, then train neural networks able to predict PLC
outputs (i.e. actuator commands) from their inputs. By exploiting the black box
nature of the model, our solution maintains the privacy of the original PLC
code and does not assume that attackers are unaware of its presence. The trust
instead comes from the fact that it is extremely hard to attack the PLC code
and neural networks at the same time and with consistent outcomes. We evaluated
our approach on a modern six-stage water treatment plant testbed, finding that
it could predict actuator states from PLC inputs with near-100% accuracy, and
thus could detect all 120 effective code mutations that we subjected the PLCs
to. Finally, we found that it is not practically possible to simultaneously
modify the PLC code and apply discreet adversarial noise to our attesters in a
way that leads to consistent (mis-)predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poskitt_C/0/1/0/all/0/1"&gt;Christopher M. Poskitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jun Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MobILE: Model-Based Imitation Learning From Observation Alone. (arXiv:2102.10769v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10769</id>
        <link href="http://arxiv.org/abs/2102.10769"/>
        <updated>2021-06-16T01:21:08.466Z</updated>
        <summary type="html"><![CDATA[This paper studies Imitation Learning from Observations alone (ILFO) where
the learner is presented with expert demonstrations that consist only of states
visited by an expert (without access to actions taken by the expert). We
present a provably efficient model-based framework MobILE to solve the ILFO
problem. MobILE involves carefully trading off strategic exploration against
imitation - this is achieved by integrating the idea of optimism in the face of
uncertainty into the distribution matching imitation learning (IL) framework.
We provide a unified analysis for MobILE, and demonstrate that MobILE enjoys
strong performance guarantees for classes of MDP dynamics that satisfy certain
well studied notions of structural complexity. We also show that the ILFO
problem is strictly harder than the standard IL problem by presenting an
exponential sample complexity separation between IL and ILFO. We complement
these theoretical results with experimental simulations on benchmark OpenAI Gym
tasks that indicate the efficacy of MobILE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kidambi_R/0/1/0/all/0/1"&gt;Rahul Kidambi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;Jonathan Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CatBoost model with synthetic features in application to loan risk assessment of small businesses. (arXiv:2106.07954v1 [cs.CE])]]></title>
        <id>http://arxiv.org/abs/2106.07954</id>
        <link href="http://arxiv.org/abs/2106.07954"/>
        <updated>2021-06-16T01:21:08.426Z</updated>
        <summary type="html"><![CDATA[Loan risk for small business has long been a complex problem worthy of
exploring. Predicting the loan risk approximately can benefit entrepreneurship
by developing more jobs for the society. CatBoost (Categorical Boosting) is a
powerful machine learning algorithm that is suitable for dataset with many
categorical variables like the dataset for forecasting loan risk. In this
paper, we identify the important risk factors that contribute to loan status
classification problem. Then we compare the the performance between
boosting-type algorithms(especially CatBoost) with other traditional yet
popular ones. The dataset we adopt in the research comes from the U.S. Small
Business Administration (SBA) and holds a very large sample size (899,164
observations and 27 features). We obtain a high accuracy of 95.74% and
well-performed AUC of 98.59% compared with the existent literature of related
research. In order to make best use of the important features in the dataset,
we propose a technique named "synthetic generation" to develop more combined
features based on arithmetic operation, which ends up improving the accuracy
and AUC of original CatBoost model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Liexing Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoxue Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval. (arXiv:2104.01894v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01894</id>
        <link href="http://arxiv.org/abs/2104.01894"/>
        <updated>2021-06-16T01:21:08.419Z</updated>
        <summary type="html"><![CDATA[Speech-based image retrieval has been studied as a proxy for joint
representation learning, usually without emphasis on retrieval itself. As such,
it is unclear how well speech-based retrieval can work in practice -- both in
an absolute sense and versus alternative strategies that combine automatic
speech recognition (ASR) with strong text encoders. In this work, we
extensively study and expand choices of encoder architectures, training
methodology (including unimodal and multimodal pretraining), and other factors.
Our experiments cover different types of speech in three datasets: Flickr
Audio, Places Audio, and Localized Narratives. Our best model configuration
achieves large gains over state of the art, e.g., pushing recall-at-one from
21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also
show our best speech-based models can match or exceed cascaded ASR-to-text
encoding when speech is spontaneous, accented, or otherwise hard to
automatically transcribe.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1"&gt;Ramon Sanabria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1"&gt;Austin Waters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1"&gt;Jason Baldridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07847</id>
        <link href="http://arxiv.org/abs/2106.07847"/>
        <updated>2021-06-16T01:21:08.412Z</updated>
        <summary type="html"><![CDATA[We study transfer learning in the presence of spurious correlations. We
experimentally demonstrate that directly transferring the stable feature
extractor learned on the source task may not eliminate these biases for the
target task. However, we hypothesize that the unstable features in the source
task and those in the target task are directly related. By explicitly informing
the target classifier of the source task's unstable features, we can regularize
the biases in the target task. Specifically, we derive a representation that
encodes the unstable features by contrasting different data environments in the
source task. On the target task, we cluster data from this representation, and
achieve robustness by minimizing the worst-case risk across all clusters. We
evaluate our method on both text and image classifications. Empirical results
demonstrate that our algorithm is able to maintain robustness on the target
task, outperforming the best baseline by 22.9% in absolute accuracy across 12
transfer settings. Our code is available at https://github.com/YujiaBao/Tofu.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1"&gt;Yujia Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepKoCo: Efficient latent planning with a robust Koopman representation. (arXiv:2011.12690v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12690</id>
        <link href="http://arxiv.org/abs/2011.12690"/>
        <updated>2021-06-16T01:21:08.391Z</updated>
        <summary type="html"><![CDATA[This paper presents DeepKoCo, a novel model-based agent that learns a latent
Koopman representation from images. This representation allows DeepKoCo to plan
efficiently using linear control methods, such as linear model predictive
control. Compared to traditional agents, DeepKoCo is robust to task-irrelevant
dynamics, thanks to the use of a tailored lossy autoencoder network that allows
DeepKoCo to learn latent dynamics that reconstruct and predict only observed
costs, rather than all observed dynamics. As our results show, DeepKoCo
achieves a similar final performance as traditional model-free methods on
complex control tasks, while being considerably more robust to distractor
dynamics, making the proposed agent more amenable for real-life applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heijden_B/0/1/0/all/0/1"&gt;Bas van der Heijden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferranti_L/0/1/0/all/0/1"&gt;Laura Ferranti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1"&gt;Jens Kober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1"&gt;Robert Babuska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation. (arXiv:2011.09757v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09757</id>
        <link href="http://arxiv.org/abs/2011.09757"/>
        <updated>2021-06-16T01:21:08.385Z</updated>
        <summary type="html"><![CDATA[Conventional unsupervised multi-source domain adaptation (UMDA) methods
assume all source domains can be accessed directly. This neglects the
privacy-preserving policy, that is, all the data and computations must be kept
decentralized. There exists three problems in this scenario: (1) Minimizing the
domain distance requires the pairwise calculation of the data from source and
target domains, which is not accessible. (2) The communication cost and privacy
security limit the application of UMDA methods (e.g., the domain adversarial
training). (3) Since users have no authority to check the data quality, the
irrelevant or malicious source domains are more likely to appear, which causes
negative transfer. In this study, we propose a privacy-preserving UMDA paradigm
named Knowledge Distillation based Decentralized Domain Adaptation (KD3A),
which performs domain adaptation through the knowledge distillation on models
from different source domains. KD3A solves the above problems with three
components: (1) A multi-source knowledge distillation method named Knowledge
Vote to learn high-quality domain consensus knowledge. (2) A dynamic weighting
strategy named Consensus Focus to identify both the malicious and irrelevant
domains. (3) A decentralized optimization strategy for domain distance named
BatchNorm MMD. The extensive experiments on DomainNet demonstrate that KD3A is
robust to the negative transfer and brings a 100x reduction of communication
cost compared with other decentralized UMDA methods. Moreover, our KD3A
significantly outperforms state-of-the-art UMDA approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1"&gt;Hao-Zhe Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1"&gt;Zhaoyang You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minghao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianye Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Minfeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Control Variates for Slate Off-Policy Evaluation. (arXiv:2106.07914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07914</id>
        <link href="http://arxiv.org/abs/2106.07914"/>
        <updated>2021-06-16T01:21:08.377Z</updated>
        <summary type="html"><![CDATA[We study the problem of off-policy evaluation from batched contextual bandit
data with multidimensional actions, often termed slates. The problem is common
to recommender systems and user-interface optimization, and it is particularly
challenging because of the combinatorially-sized action space. Swaminathan et
al. (2017) have proposed the pseudoinverse (PI) estimator under the assumption
that the conditional mean rewards are additive in actions. Using control
variates, we consider a large class of unbiased estimators that includes as
specific cases the PI estimator and (asymptotically) its self-normalized
variant. By optimizing over this class, we obtain new estimators with risk
improvement guarantees over both the PI and self-normalized PI estimators.
Experiments with real-world recommender data as well as synthetic data validate
these improvements in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vlassis_N/0/1/0/all/0/1"&gt;Nikos Vlassis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrashekar_A/0/1/0/all/0/1"&gt;Ashok Chandrashekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gil_F/0/1/0/all/0/1"&gt;Fernando Amat Gil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kallus_N/0/1/0/all/0/1"&gt;Nathan Kallus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ShadowNet: A Secure and Efficient System for On-device Model Inference. (arXiv:2011.05905v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05905</id>
        <link href="http://arxiv.org/abs/2011.05905"/>
        <updated>2021-06-16T01:21:08.371Z</updated>
        <summary type="html"><![CDATA[With the increased usage of AI accelerators on mobile and edge devices,
on-device machine learning (ML) is gaining popularity. Consequently, thousands
of proprietary ML models are being deployed on billions of untrusted devices.
This raises serious security concerns about model privacy. However, protecting
the model privacy without losing access to the AI accelerators is a challenging
problem. In this paper, we present a novel on-device model inference system,
ShadowNet. ShadowNet protects the model privacy with Trusted Execution
Environment (TEE) while securely outsourcing the heavy linear layers of the
model to the untrusted hardware accelerators. ShadowNet achieves this by
transforming the weights of the linear layers before outsourcing them and
restoring the results inside the TEE. The nonlinear layers are also kept secure
inside the TEE. The transformation of the weights and the restoration of the
results are designed in a way that can be implemented efficiently. We have
built a ShadowNet prototype based on TensorFlow Lite and applied it on four
popular CNNs, namely, MobileNets, ResNet-44, AlexNet and MiniVGG. Our
evaluation shows that ShadowNet achieves strong security guarantees with
reasonable performance, offering a practical solution for secure on-device
model inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhichuang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1"&gt;Ruimin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Changming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1"&gt;Amrita Roy Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Long Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Explanations as Interventions in Latent Space. (arXiv:2106.07754v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.07754</id>
        <link href="http://arxiv.org/abs/2106.07754"/>
        <updated>2021-06-16T01:21:08.364Z</updated>
        <summary type="html"><![CDATA[Explainable Artificial Intelligence (XAI) is a set of techniques that allows
the understanding of both technical and non-technical aspects of Artificial
Intelligence (AI) systems. XAI is crucial to help satisfying the increasingly
important demand of \emph{trustworthy} Artificial Intelligence, characterized
by fundamental characteristics such as respect of human autonomy, prevention of
harm, transparency, accountability, etc. Within XAI techniques, counterfactual
explanations aim to provide to end users a set of features (and their
corresponding values) that need to be changed in order to achieve a desired
outcome. Current approaches rarely take into account the feasibility of actions
needed to achieve the proposed explanations, and in particular they fall short
of considering the causal impact of such actions. In this paper, we present
Counterfactual Explanations as Interventions in Latent Space (CEILS), a
methodology to generate counterfactual explanations capturing by design the
underlying causal relations from the data, and at the same time to provide
feasible recommendations to reach the proposed profile. Moreover, our
methodology has the advantage that it can be set on top of existing
counterfactuals generator algorithms, thus minimising the complexity of
imposing additional causal constrains. We demonstrate the effectiveness of our
approach with a set of different experiments using synthetic and real datasets
(including a proprietary dataset of the financial domain).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Crupi_R/0/1/0/all/0/1"&gt;Riccardo Crupi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castelnovo_A/0/1/0/all/0/1"&gt;Alessandro Castelnovo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Regoli_D/0/1/0/all/0/1"&gt;Daniele Regoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_B/0/1/0/all/0/1"&gt;Beatriz San Miguel Gonzalez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased Sentence Encoder For Large-Scale Multi-lingual Search Engines. (arXiv:2106.07719v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07719</id>
        <link href="http://arxiv.org/abs/2106.07719"/>
        <updated>2021-06-16T01:21:08.343Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a multi-lingual sentence encoder that can be used
in search engines as a query and document encoder. This embedding enables a
semantic similarity score between queries and documents that can be an
important feature in document ranking and relevancy. To train such a customized
sentence encoder, it is beneficial to leverage users search data in the form of
query-document clicked pairs however, we must avoid relying too much on search
click data as it is biased and does not cover many unseen cases. The search
data is heavily skewed towards short queries and for long queries is small and
often noisy. The goal is to design a universal multi-lingual encoder that works
for all cases and covers both short and long queries. We select a number of
public NLI datasets in different languages and translation data and together
with user search data we train a language model using a multi-task approach. A
challenge is that these datasets are not homogeneous in terms of content, size
and the balance ratio. While the public NLI datasets are usually two-sentence
based with the same portion of positive and negative pairs, the user search
data can contain multi-sentence documents and only positive pairs. We show how
multi-task training enables us to leverage all these datasets and exploit
knowledge sharing across these tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1"&gt;Mahdi Hajiaghayi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1"&gt;Monir Hajiaghayi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolin_M/0/1/0/all/0/1"&gt;Mark Bolin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Local Stochastic Extra-Gradient for Variational Inequalities. (arXiv:2106.08315v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.08315</id>
        <link href="http://arxiv.org/abs/2106.08315"/>
        <updated>2021-06-16T01:21:08.337Z</updated>
        <summary type="html"><![CDATA[We consider decentralized stochastic variational inequalities where the
problem data is distributed across many participating devices (heterogeneous,
or non-IID data setting). We propose a novel method - based on stochastic
extra-gradient - where participating devices can communicate over arbitrary,
possibly time-varying network topologies. This covers both the fully
decentralized optimization setting and the centralized topologies commonly used
in Federated Learning. Our method further supports multiple local updates on
the workers for reducing the communication frequency between workers. We
theoretically analyze the proposed scheme in the strongly monotone, monotone
and non-monotone setting. As a special case, our method and analysis apply in
particular to decentralized stochastic min-max problems which are being studied
with increased interest in Deep Learning. For example, the training objective
of Generative Adversarial Networks (GANs) are typically saddle point problems
and the decentralized training of GANs has been reported to be extremely
challenging. While SOTA techniques rely on either repeated gossip rounds or
proximal updates, we alleviate both of these requirements. Experimental results
for decentralized GAN demonstrate the effectiveness of our proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Beznosikov_A/0/1/0/all/0/1"&gt;Aleksandr Beznosikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Dvurechensky_P/0/1/0/all/0/1"&gt;Pavel Dvurechensky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Koloskova_A/0/1/0/all/0/1"&gt;Anastasia Koloskova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Samokhin_V/0/1/0/all/0/1"&gt;Valentin Samokhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1"&gt;Alexander Gasnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topics to Avoid: Demoting Latent Confounds in Text Classification. (arXiv:1909.00453v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.00453</id>
        <link href="http://arxiv.org/abs/1909.00453"/>
        <updated>2021-06-16T01:21:08.330Z</updated>
        <summary type="html"><![CDATA[Despite impressive performance on many text classification tasks, deep neural
networks tend to learn frequent superficial patterns that are specific to the
training data and do not always generalize well. In this work, we observe this
limitation with respect to the task of native language identification. We find
that standard text classifiers which perform well on the test set end up
learning topical features which are confounds of the prediction task (e.g., if
the input text mentions Sweden, the classifier predicts that the author's
native language is Swedish). We propose a method that represents the latent
topical confounds and a model which "unlearns" confounding features by
predicting both the label of the input text and the confound; but we train the
two predictors adversarially in an alternating fashion to learn a text
representation that predicts the correct label but is less prone to using
information about the confound. We show that this model generalizes better and
learns features that are indicative of the writing style rather than the
content.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sachin Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wintner_S/0/1/0/all/0/1"&gt;Shuly Wintner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aggregating From Multiple Target-Shifted Sources. (arXiv:2105.04051v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04051</id>
        <link href="http://arxiv.org/abs/2105.04051"/>
        <updated>2021-06-16T01:21:08.323Z</updated>
        <summary type="html"><![CDATA[Multi-source domain adaptation aims at leveraging the knowledge from multiple
tasks for predicting a related target domain. Hence, a crucial aspect is to
properly combine different sources based on their relations. In this paper, we
analyzed the problem for aggregating source domains with different label
distributions, where most recent source selection approaches fail. Our proposed
algorithm differs from previous approaches in two key ways: the model
aggregates multiple sources mainly through the similarity of semantic
conditional distribution rather than marginal distribution; the model proposes
a \emph{unified} framework to select relevant sources for three popular
scenarios, i.e., domain adaptation with limited label on target domain,
unsupervised domain adaptation and label partial unsupervised domain adaption.
We evaluate the proposed method through extensive experiments. The empirical
results significantly outperform the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shui_C/0/1/0/all/0/1"&gt;Changjian Shui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zijian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiaqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1"&gt;Christian Gagn&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1"&gt;Charles Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Boyu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coping with Label Shift via Distributionally Robust Optimisation. (arXiv:2010.12230v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12230</id>
        <link href="http://arxiv.org/abs/2010.12230"/>
        <updated>2021-06-16T01:21:08.316Z</updated>
        <summary type="html"><![CDATA[The label shift problem refers to the supervised learning setting where the
train and test label distributions do not match. Existing work addressing label
shift usually assumes access to an \emph{unlabelled} test sample. This sample
may be used to estimate the test label distribution, and to then train a
suitably re-weighted classifier. While approaches using this idea have proven
effective, their scope is limited as it is not always feasible to access the
target domain; further, they require repeated retraining if the model is to be
deployed in \emph{multiple} test environments. Can one instead learn a
\emph{single} classifier that is robust to arbitrary label shifts from a broad
family? In this paper, we answer this question by proposing a model that
minimises an objective based on distributionally robust optimisation (DRO). We
then design and analyse a gradient descent-proximal mirror ascent algorithm
tailored for large-scale problems to optimise the proposed objective. %, and
establish its convergence. Finally, through experiments on CIFAR-100 and
ImageNet, we show that our technique can significantly improve performance over
a number of baselines in settings where label shift is present.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingzhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1"&gt;Andreas Veit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1"&gt;Srinadh Bhojanapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1"&gt;Suvrit Sra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dont Just Divide; Polarize and Conquer!. (arXiv:2102.11872v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11872</id>
        <link href="http://arxiv.org/abs/2102.11872"/>
        <updated>2021-06-16T01:21:08.297Z</updated>
        <summary type="html"><![CDATA[In data containing heterogeneous subpopulations, classification performance
benefits from incorporating the knowledge of cluster structure in the
classifier. Previous methods for such combined clustering and classification
are either 1) classifier-specific and not generic, or 2) independently perform
clustering and classifier training, which may not form clusters that can
potentially benefit classifier performance. The question of how to perform
clustering to improve the performance of classifiers trained on the clusters
has received scant attention in previous literature, despite its importance in
several real-world applications. In this paper, we design a simple and
efficient classification algorithm called Clustering Aware Classification
(CAC), to find clusters that are well suited for being used as training
datasets by classifiers for each underlying subpopulation. Our experiments on
synthetic and real benchmark datasets demonstrate the efficacy of CAC over
previous methods for combined clustering and classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Shivin Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1"&gt;Siddharth Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lingxiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_L/0/1/0/all/0/1"&gt;Lim Jun Heng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_V/0/1/0/all/0/1"&gt;Vaibhav Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting in the Presence of Massart Noise. (arXiv:2106.07779v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07779</id>
        <link href="http://arxiv.org/abs/2106.07779"/>
        <updated>2021-06-16T01:21:08.291Z</updated>
        <summary type="html"><![CDATA[We study the problem of boosting the accuracy of a weak learner in the
(distribution-independent) PAC model with Massart noise. In the Massart noise
model, the label of each example $x$ is independently misclassified with
probability $\eta(x) \leq \eta$, where $\eta<1/2$. The Massart model lies
between the random classification noise model and the agnostic model. Our main
positive result is the first computationally efficient boosting algorithm in
the presence of Massart noise that achieves misclassification error arbitrarily
close to $\eta$. Prior to our work, no non-trivial booster was known in this
setting. Moreover, we show that this error upper bound is best possible for
polynomial-time black-box boosters, under standard cryptographic assumptions.
Our upper and lower bounds characterize the complexity of boosting in the
distribution-independent PAC model with Massart noise. As a simple application
of our positive result, we give the first efficient Massart learner for unions
of high-dimensional rectangles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1"&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Impagliazzo_R/0/1/0/all/0/1"&gt;Russell Impagliazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1"&gt;Daniel Kane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_R/0/1/0/all/0/1"&gt;Rex Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorrell_J/0/1/0/all/0/1"&gt;Jessica Sorrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1"&gt;Christos Tzamos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When does gradient descent with logistic loss find interpolating two-layer networks?. (arXiv:2012.02409v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02409</id>
        <link href="http://arxiv.org/abs/2012.02409"/>
        <updated>2021-06-16T01:21:08.285Z</updated>
        <summary type="html"><![CDATA[We study the training of finite-width two-layer smoothed ReLU networks for
binary classification using the logistic loss. We show that gradient descent
drives the training loss to zero if the initial loss is small enough. When the
data satisfies certain cluster and separation conditions and the network is
wide enough, we show that one step of gradient descent reduces the loss
sufficiently that the first result applies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1"&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Long_P/0/1/0/all/0/1"&gt;Philip M. Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter L. Bartlett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph cuts always find a global optimum for Potts models (with a catch). (arXiv:2011.03639v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03639</id>
        <link href="http://arxiv.org/abs/2011.03639"/>
        <updated>2021-06-16T01:21:08.278Z</updated>
        <summary type="html"><![CDATA[We prove that the $\alpha$-expansion algorithm for MAP inference always
returns a globally optimal assignment for Markov Random Fields with Potts
pairwise potentials, with a catch: the returned assignment is only guaranteed
to be optimal for an instance within a small perturbation of the original
problem instance. In other words, all local minima with respect to expansion
moves are global minima to slightly perturbed versions of the problem. On
"real-world" instances, MAP assignments of small perturbations of the problem
should be very similar to the MAP assignment(s) of the original problem
instance. We design an algorithm that can certify whether this is the case in
practice. On several MAP inference problem instances from computer vision, this
algorithm certifies that MAP solutions to all of these perturbations are very
close to solutions of the original instance. These results taken together give
a cohesive explanation for the good performance of "graph cuts" algorithms in
practice. Every local expansion minimum is a global minimum in a small
perturbation of the problem, and all of these global minima are close to the
original solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lang_H/0/1/0/all/0/1"&gt;Hunter Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sontag_D/0/1/0/all/0/1"&gt;David Sontag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vijayaraghavan_A/0/1/0/all/0/1"&gt;Aravindan Vijayaraghavan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Video Captioning Through Latent Variable Expansion. (arXiv:1910.12019v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.12019</id>
        <link href="http://arxiv.org/abs/1910.12019"/>
        <updated>2021-06-16T01:21:08.269Z</updated>
        <summary type="html"><![CDATA[Automatically describing video content with text description is challenging
but important task, which has been attracting a lot of attention in computer
vision community. Previous works mainly strive for the accuracy of the
generated sentences, while ignoring the sentences diversity, which is
inconsistent with human behavior. In this paper, we aim to caption each video
with multiple descriptions and propose a novel framework. Concretely, for a
given video, the intermediate latent variables of conventional encode-decode
process are utilized as input to the conditional generative adversarial network
(CGAN) with the purpose of generating diverse sentences. We adopt different
Convolutional Neural Networks (CNNs) as our generator that produces
descriptions conditioned on latent variables and discriminator that assesses
the quality of generated sentences. Simultaneously, a novel DCE metric is
designed to assess the diverse captions. We evaluate our method on the
benchmark datasets, where it demonstrates its ability to generate diverse
descriptions and achieves superior results against other state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Huanhou Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jinglun Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning-based conditional mean filter: a generalization of the ensemble Kalman filter for nonlinear data assimilation. (arXiv:2106.07908v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07908</id>
        <link href="http://arxiv.org/abs/2106.07908"/>
        <updated>2021-06-16T01:21:08.252Z</updated>
        <summary type="html"><![CDATA[Filtering is a data assimilation technique that performs the sequential
inference of dynamical systems states from noisy observations. Herein, we
propose a machine learning-based ensemble conditional mean filter (ML-EnCMF)
for tracking possibly high-dimensional non-Gaussian state models with nonlinear
dynamics based on sparse observations. The proposed filtering method is
developed based on the conditional expectation and numerically implemented
using machine learning (ML) techniques combined with the ensemble method. The
contribution of this work is twofold. First, we demonstrate that the ensembles
assimilated using the ensemble conditional mean filter (EnCMF) provide an
unbiased estimator of the Bayesian posterior mean, and their variance matches
the expected conditional variance. Second, we implement the EnCMF using
artificial neural networks, which have a significant advantage in representing
nonlinear functions over high-dimensional domains such as the conditional mean.
Finally, we demonstrate the effectiveness of the ML-EnCMF for tracking the
states of Lorenz-63 and Lorenz-96 systems under the chaotic regime. Numerical
results show that the ML-EnCMF outperforms the ensemble Kalman filter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1"&gt;Truong-Vinh Hoang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Krumscheid_S/0/1/0/all/0/1"&gt;Sebastian Krumscheid&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Matthies_H/0/1/0/all/0/1"&gt;Hermann G. Matthies&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Tempone_R/0/1/0/all/0/1"&gt;Ra&amp;#xfa;l Tempone&lt;/a&gt; (1 and 3) ((1) Chair of Mathematics for Uncertainty Quantification, RWTH Aachen University, (2) Technische Universit&amp;#xe4;t Braunschweig (3) Computer, Electrical and Mathematical Sciences and Engineering, KAUST, and Alexander von Humboldt professor in Mathematics of Uncertainty Quantification, RWTH Aachen University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Neural Tangent Kernels via Sketching and Random Features. (arXiv:2106.07880v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07880</id>
        <link href="http://arxiv.org/abs/2106.07880"/>
        <updated>2021-06-16T01:21:08.245Z</updated>
        <summary type="html"><![CDATA[The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely-wide
neural networks trained under least squares loss by gradient descent. Recent
works also report that NTK regression can outperform finitely-wide neural
networks trained on small-scale datasets. However, the computational complexity
of kernel methods has limited its use in large-scale learning tasks. To
accelerate learning with NTK, we design a near input-sparsity time
approximation algorithm for NTK, by sketching the polynomial expansions of
arc-cosine kernels: our sketch for the convolutional counterpart of NTK (CNTK)
can transform any image using a linear runtime in the number of pixels.
Furthermore, we prove a spectral approximation guarantee for the NTK matrix, by
combining random features (based on leverage score sampling) of the arc-cosine
kernels with a sketching algorithm. We benchmark our methods on various
large-scale regression and classification tasks and show that a linear
regressor trained on our CNTK features matches the accuracy of exact CNTK on
CIFAR-10 dataset while achieving 150x speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zandieh_A/0/1/0/all/0/1"&gt;Amir Zandieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_I/0/1/0/all/0/1"&gt;Insu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avron_H/0/1/0/all/0/1"&gt;Haim Avron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoham_N/0/1/0/all/0/1"&gt;Neta Shoham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1"&gt;Chaewon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Domain Knowledge into Health Recommender Systems using Hyperbolic Embeddings. (arXiv:2106.07720v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07720</id>
        <link href="http://arxiv.org/abs/2106.07720"/>
        <updated>2021-06-16T01:21:08.228Z</updated>
        <summary type="html"><![CDATA[In contrast to many other domains, recommender systems in health services may
benefit particularly from the incorporation of health domain knowledge, as it
helps to provide meaningful and personalised recommendations catering to the
individual's health needs. With recent advances in representation learning
enabling the hierarchical embedding of health knowledge into the hyperbolic
Poincare space, this work proposes a content-based recommender system for
patient-doctor matchmaking in primary care based on patients' health profiles,
enriched by pre-trained Poincare embeddings of the ICD-9 codes through transfer
learning. The proposed model outperforms its conventional counterpart in terms
of recommendation accuracy and has several important business implications for
improving the patient-doctor relationship.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peito_J/0/1/0/all/0/1"&gt;Joel Peito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1"&gt;Qiwei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Analytical Theory of Curriculum Learning in Teacher-Student Networks. (arXiv:2106.08068v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08068</id>
        <link href="http://arxiv.org/abs/2106.08068"/>
        <updated>2021-06-16T01:21:08.207Z</updated>
        <summary type="html"><![CDATA[In humans and animals, curriculum learning -- presenting data in a curated
order - is critical to rapid learning and effective pedagogy. Yet in machine
learning, curricula are not widely used and empirically often yield only
moderate benefits. This stark difference in the importance of curriculum raises
a fundamental theoretical question: when and why does curriculum learning help?

In this work, we analyse a prototypical neural network model of curriculum
learning in the high-dimensional limit, employing statistical physics methods.
Curricula could in principle change both the learning speed and asymptotic
performance of a model. To study the former, we provide an exact description of
the online learning setting, confirming the long-standing experimental
observation that curricula can modestly speed up learning. To study the latter,
we derive performance in a batch learning setting, in which a network trains to
convergence in successive phases of learning on dataset slices of varying
difficulty. With standard training losses, curriculum does not provide
generalisation benefit, in line with empirical observations. However, we show
that by connecting different learning phases through simple Gaussian priors,
curriculum can yield a large improvement in test performance. Taken together,
our reduced analytical descriptions help reconcile apparently conflicting
empirical results and trace regimes where curriculum learning yields the
largest gains. More broadly, our results suggest that fully exploiting a
curriculum may require explicit changes to the loss function at curriculum
boundaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saglietti_L/0/1/0/all/0/1"&gt;Luca Saglietti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannelli_S/0/1/0/all/0/1"&gt;Stefano Sarao Mannelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1"&gt;Andrew Saxe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unique sparse decomposition of low rank matrices. (arXiv:2106.07736v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.07736</id>
        <link href="http://arxiv.org/abs/2106.07736"/>
        <updated>2021-06-16T01:21:08.196Z</updated>
        <summary type="html"><![CDATA[The problem of finding the unique low dimensional decomposition of a given
matrix has been a fundamental and recurrent problem in many areas. In this
paper, we study the problem of seeking a unique decomposition of a low rank
matrix $Y\in \mathbb{R}^{p\times n}$ that admits a sparse representation.
Specifically, we consider $Y = A X\in \mathbb{R}^{p\times n}$ where the matrix
$A\in \mathbb{R}^{p\times r}$ has full column rank, with $r < \min\{n,p\}$, and
the matrix $X\in \mathbb{R}^{r\times n}$ is element-wise sparse. We prove that
this sparse decomposition of $Y$ can be uniquely identified, up to some
intrinsic signed permutation. Our approach relies on solving a nonconvex
optimization problem constrained over the unit sphere. Our geometric analysis
for the nonconvex optimization landscape shows that any {\em strict} local
solution is close to the ground truth solution, and can be recovered by a
simple data-driven initialization followed with any second order descent
algorithm. At last, we corroborate these theoretical results with numerical
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Jin_D/0/1/0/all/0/1"&gt;Dian Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bing_X/0/1/0/all/0/1"&gt;Xin Bing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuqian Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Margins for Instance Reweighting in Adversarial Training. (arXiv:2106.07904v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07904</id>
        <link href="http://arxiv.org/abs/2106.07904"/>
        <updated>2021-06-16T01:21:08.190Z</updated>
        <summary type="html"><![CDATA[Reweighting adversarial data during training has been recently shown to
improve adversarial robustness, where data closer to the current decision
boundaries are regarded as more critical and given larger weights. However,
existing methods measuring the closeness are not very reliable: they are
discrete and can take only a few values, and they are path-dependent, i.e.,
they may change given the same start and end points with different attack
paths. In this paper, we propose three types of probabilistic margin (PM),
which are continuous and path-independent, for measuring the aforementioned
closeness and reweighting adversarial data. Specifically, a PM is defined as
the difference between two estimated class-posterior probabilities, e.g., such
the probability of the true label minus the probability of the most confusing
label given some natural data. Though different PMs capture different geometric
properties, all three PMs share a negative correlation with the vulnerability
of data: data with larger/smaller PMs are safer/riskier and should have
smaller/larger weights. Experiments demonstrate that PMs are reliable
measurements and PM-based reweighting methods outperform state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qizhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chen Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pitfalls of Explainable ML: An Industry Perspective. (arXiv:2106.07758v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07758</id>
        <link href="http://arxiv.org/abs/2106.07758"/>
        <updated>2021-06-16T01:21:08.116Z</updated>
        <summary type="html"><![CDATA[As machine learning (ML) systems take a more prominent and central role in
contributing to life-impacting decisions, ensuring their trustworthiness and
accountability is of utmost importance. Explanations sit at the core of these
desirable attributes of a ML system. The emerging field is frequently called
``Explainable AI (XAI)'' or ``Explainable ML.'' The goal of explainable ML is
to intuitively explain the predictions of a ML system, while adhering to the
needs to various stakeholders. Many explanation techniques were developed with
contributions from both academia and industry. However, there are several
existing challenges that have not garnered enough interest and serve as
roadblocks to widespread adoption of explainable ML. In this short paper, we
enumerate challenges in explainable ML from an industry perspective. We hope
these challenges will serve as promising future research directions, and would
contribute to democratizing explainable ML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1"&gt;Sahil Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahiri_A/0/1/0/all/0/1"&gt;Aditya Lahiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1"&gt;John P. Dickerson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Su-In Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting. (arXiv:2106.07677v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07677</id>
        <link href="http://arxiv.org/abs/2106.07677"/>
        <updated>2021-06-16T01:21:08.108Z</updated>
        <summary type="html"><![CDATA[Restless and collapsing bandits are commonly used to model constrained
resource allocation in settings featuring arms with action-dependent transition
probabilities, such as allocating health interventions among patients [Whittle,
1988; Mate et al., 2020]. However, state-of-the-art Whittle-index-based
approaches to this planning problem either do not consider fairness among arms,
or incentivize fairness without guaranteeing it [Mate et al., 2021].
Additionally, their optimality guarantees only apply when arms are indexable
and threshold-optimal. We demonstrate that the incorporation of hard fairness
constraints necessitates the coupling of arms, which undermines the
tractability, and by extension, indexability of the problem. We then introduce
ProbFair, a probabilistically fair stationary policy that maximizes total
expected reward and satisfies the budget constraint, while ensuring a strictly
positive lower bound on the probability of being pulled at each timestep. We
evaluate our algorithm on a real-world application, where interventions support
continuous positive airway pressure (CPAP) therapy adherence among obstructive
sleep apnea (OSA) patients, as well as simulations on a broader class of
synthetic transition matrices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Herlihy_C/0/1/0/all/0/1"&gt;Christine Herlihy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prins_A/0/1/0/all/0/1"&gt;Aviva Prins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1"&gt;Aravind Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1"&gt;John Dickerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing Structural Regularities of Labeled Data in Overparameterized Models. (arXiv:2002.03206v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.03206</id>
        <link href="http://arxiv.org/abs/2002.03206"/>
        <updated>2021-06-16T01:21:08.101Z</updated>
        <summary type="html"><![CDATA[Humans are accustomed to environments that contain both regularities and
exceptions. For example, at most gas stations, one pays prior to pumping, but
the occasional rural station does not accept payment in advance. Likewise, deep
neural networks can generalize across instances that share common patterns or
structures, yet have the capacity to memorize rare or irregular forms. We
analyze how individual instances are treated by a model via a consistency
score. The score characterizes the expected accuracy for a held-out instance
given training sets of varying size sampled from the data distribution. We
obtain empirical estimates of this score for individual instances in multiple
data sets, and we show that the score identifies out-of-distribution and
mislabeled examples at one end of the continuum and strongly regular examples
at the other end. We identify computationally inexpensive proxies to the
consistency score using statistics collected during training. We show examples
of potential applications to the analysis of deep-learning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Ziheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chiyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1"&gt;Kunal Talwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1"&gt;Michael C. Mozer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition. (arXiv:2104.09106v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09106</id>
        <link href="http://arxiv.org/abs/2104.09106"/>
        <updated>2021-06-16T01:21:08.065Z</updated>
        <summary type="html"><![CDATA[Subword units are commonly used for end-to-end automatic speech recognition
(ASR), while a fully acoustic-oriented subword modeling approach is somewhat
missing. We propose an acoustic data-driven subword modeling (ADSM) approach
that adapts the advantages of several text-based and acoustic-based subword
methods into one pipeline. With a fully acoustic-oriented label design and
learning process, ADSM produces acoustic-structured subword units and
acoustic-matched target sequence for further ASR training. The obtained ADSM
labels are evaluated with different end-to-end ASR approaches including CTC,
RNN-Transducer and attention models. Experiments on the LibriSpeech corpus show
that ADSM clearly outperforms both byte pair encoding (BPE) and
pronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis
shows that ADSM achieves acoustically more logical word segmentation and more
balanced sequence length, and thus, is suitable for both time-synchronous and
label-synchronous models. We also briefly describe how to apply acoustic-based
subword regularization and unseen text segmentation using ADSM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1"&gt;Mohammad Zeineldeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zuoyun Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear-Time Probabilistic Solutions of Boundary Value Problems. (arXiv:2106.07761v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.07761</id>
        <link href="http://arxiv.org/abs/2106.07761"/>
        <updated>2021-06-16T01:21:08.028Z</updated>
        <summary type="html"><![CDATA[We propose a fast algorithm for the probabilistic solution of boundary value
problems (BVPs), which are ordinary differential equations subject to boundary
conditions. In contrast to previous work, we introduce a Gauss--Markov prior
and tailor it specifically to BVPs, which allows computing a posterior
distribution over the solution in linear time, at a quality and cost comparable
to that of well-established, non-probabilistic methods. Our model further
delivers uncertainty quantification, mesh refinement, and hyperparameter
adaptation. We demonstrate how these practical considerations positively impact
the efficiency of the scheme. Altogether, this results in a practically usable
probabilistic BVP solver that is (in contrast to non-probabilistic algorithms)
natively compatible with other parts of the statistical modelling tool-chain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kramer_N/0/1/0/all/0/1"&gt;Nicholas Kr&amp;#xe4;mer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hennig_P/0/1/0/all/0/1"&gt;Philipp Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question Answering Infused Pre-training of General-Purpose Contextualized Representations. (arXiv:2106.08190v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08190</id>
        <link href="http://arxiv.org/abs/2106.08190"/>
        <updated>2021-06-16T01:21:08.021Z</updated>
        <summary type="html"><![CDATA[This paper proposes a pre-training objective based on question answering (QA)
for learning general-purpose contextual representations, motivated by the
intuition that the representation of a phrase in a passage should encode all
questions that the phrase can answer in context. We accomplish this goal by
training a bi-encoder QA model, which independently encodes passages and
questions, to match the predictions of a more accurate cross-encoder model on
80 million synthesized QA pairs. By encoding QA-relevant information, the
bi-encoder's token-level representations are useful for non-QA downstream tasks
without extensive (or in some cases, any) fine-tuning. We show large
improvements over both RoBERTa-large and previous state-of-the-art results on
zero-shot and few-shot paraphrase detection on four datasets, few-shot named
entity recognition on two datasets, and zero-shot sentiment analysis on three
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Robin Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Mike Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OCHADAI-KYOTO at SemEval-2021 Task 1: Enhancing Model Generalization and Robustness for Lexical Complexity Prediction. (arXiv:2105.05535v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05535</id>
        <link href="http://arxiv.org/abs/2105.05535"/>
        <updated>2021-06-16T01:21:08.014Z</updated>
        <summary type="html"><![CDATA[We propose an ensemble model for predicting the lexical complexity of words
and multiword expressions (MWEs). The model receives as input a sentence with a
target word or MWEand outputs its complexity score. Given that a key challenge
with this task is the limited size of annotated data, our model relies on
pretrained contextual representations from different state-of-the-art
transformer-based language models (i.e., BERT and RoBERTa), and on a variety of
training methods for further enhancing model generalization and
robustness:multi-step fine-tuning and multi-task learning, and adversarial
training. Additionally, we propose to enrich contextual representations by
adding hand-crafted features during training. Our model achieved competitive
results and ranked among the top-10 systems in both sub-tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taya_Y/0/1/0/all/0/1"&gt;Yuki Taya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_L/0/1/0/all/0/1"&gt;Lis Kanashiro Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1"&gt;Fei Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_I/0/1/0/all/0/1"&gt;Ichiro Kobayashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistency Regularization for Cross-Lingual Fine-Tuning. (arXiv:2106.08226v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08226</id>
        <link href="http://arxiv.org/abs/2106.08226"/>
        <updated>2021-06-16T01:21:08.008Z</updated>
        <summary type="html"><![CDATA[Fine-tuning pre-trained cross-lingual language models can transfer
task-specific supervision from one language to the others. In this work, we
propose to improve cross-lingual fine-tuning with consistency regularization.
Specifically, we use example consistency regularization to penalize the
prediction sensitivity to four types of data augmentations, i.e., subword
sampling, Gaussian noise, code-switch substitution, and machine translation. In
addition, we employ model consistency to regularize the models trained with two
augmented versions of the same training set. Experimental results on the XTREME
benchmark show that our method significantly improves cross-lingual fine-tuning
across various tasks, including text classification, question answering, and
sequence labeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Bo Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shaohan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1"&gt;Zewen Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1"&gt;Saksham Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1"&gt;Wanxiang Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xia Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynthASR: Unlocking Synthetic Data for Speech Recognition. (arXiv:2106.07803v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07803</id>
        <link href="http://arxiv.org/abs/2106.07803"/>
        <updated>2021-06-16T01:21:08.001Z</updated>
        <summary type="html"><![CDATA[End-to-end (E2E) automatic speech recognition (ASR) models have recently
demonstrated superior performance over the traditional hybrid ASR models.
Training an E2E ASR model requires a large amount of data which is not only
expensive but may also raise dependency on production data. At the same time,
synthetic speech generated by the state-of-the-art text-to-speech (TTS) engines
has advanced to near-human naturalness. In this work, we propose to utilize
synthetic speech for ASR training (SynthASR) in applications where data is
sparse or hard to get for ASR model training. In addition, we apply continual
learning with a novel multi-stage training strategy to address catastrophic
forgetting, achieved by a mix of weighted multi-style training, data
augmentation, encoder freezing, and parameter regularization. In our
experiments conducted on in-house datasets for a new application of recognizing
medication names, training ASR RNN-T models with synthetic audio via the
proposed multi-stage training improved the recognition performance on new
application by more than 65% relative, without degradation on existing general
applications. Our observations show that SynthASR holds great promise in
training the state-of-the-art large-scale E2E ASR models for new applications
while reducing the costs and dependency on production data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fazel_A/0/1/0/all/0/1"&gt;Amin Fazel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yulan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barra_Chicote_R/0/1/0/all/0/1"&gt;Roberto Barra-Chicote&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1"&gt;Yixiong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maas_R/0/1/0/all/0/1"&gt;Roland Maas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1"&gt;Jasha Droppo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Abstractive Opinion Summarization by Generating Sentences with Tree-Structured Topic Guidance. (arXiv:2106.08007v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08007</id>
        <link href="http://arxiv.org/abs/2106.08007"/>
        <updated>2021-06-16T01:21:07.994Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel unsupervised abstractive summarization method for
opinionated texts. While the basic variational autoencoder-based models assume
a unimodal Gaussian prior for the latent code of sentences, we alternate it
with a recursive Gaussian mixture, where each mixture component corresponds to
the latent code of a topic sentence and is mixed by a tree-structured topic
distribution. By decoding each Gaussian component, we generate sentences with
tree-structured topic guidance, where the root sentence conveys generic
content, and the leaf sentences describe specific topics. Experimental results
demonstrate that the generated topic sentences are appropriate as a summary of
opinionated texts, which are more informative and cover more input contents
than those generated by the recent unsupervised summarization model
(Bra\v{z}inskas et al., 2020). Furthermore, we demonstrate that the variance of
latent Gaussians represents the granularity of sentences, analogous to Gaussian
word embedding (Vilnis and McCallum, 2015).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Isonuma_M/0/1/0/all/0/1"&gt;Masaru Isonuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_J/0/1/0/all/0/1"&gt;Junichiro Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1"&gt;Danushka Bollegala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakata_I/0/1/0/all/0/1"&gt;Ichiro Sakata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Learning from Activity Description. (arXiv:2102.07024v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07024</id>
        <link href="http://arxiv.org/abs/2102.07024"/>
        <updated>2021-06-16T01:21:07.976Z</updated>
        <summary type="html"><![CDATA[We present a novel interactive learning protocol that enables training
request-fulfilling agents by verbally describing their activities. Unlike
imitation learning (IL), our protocol allows the teaching agent to provide
feedback in a language that is most appropriate for them. Compared with reward
in reinforcement learning (RL), the description feedback is richer and allows
for improved sample complexity. We develop a probabilistic framework and an
algorithm that practically implements our protocol. Empirical results in two
challenging request-fulfilling problems demonstrate the strengths of our
approach: compared with RL baselines, it is more sample-efficient; compared
with IL baselines, it achieves competitive success rates without requiring the
teaching agent to be able to demonstrate the desired behavior using the
learning agent's actions. Apart from empirical evaluation, we also provide
theoretical guarantees for our algorithm under certain assumptions about the
teacher and the environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khanh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schapire_R/0/1/0/all/0/1"&gt;Robert Schapire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1"&gt;Miro Dud&amp;#xed;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafto_P/0/1/0/all/0/1"&gt;Patrick Shafto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11066</id>
        <link href="http://arxiv.org/abs/2010.11066"/>
        <updated>2021-06-16T01:21:07.968Z</updated>
        <summary type="html"><![CDATA[Spoken conversational question answering (SCQA) requires machines to model
complex dialogue flow given the speech utterances and text corpora. Different
from traditional text question answering (QA) tasks, SCQA involves audio signal
processing, passage comprehension, and contextual understanding. However, ASR
systems introduce unexpected noisy signals to the transcriptions, which result
in performance degradation on SCQA. To overcome the problem, we propose CADNet,
a novel contextualized attention-based distillation approach, which applies
both cross-attention and self-attention to obtain ASR-robust contextualized
embedding representations of the passage and dialogue history for performance
improvements. We also introduce the spoken conventional knowledge distillation
framework to distill the ASR-robust knowledge from the estimated probabilities
of the teacher model to the student. We conduct extensive experiments on the
Spoken-CoQA dataset and demonstrate that our approach achieves remarkable
performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Label Propagation for Semi-Supervised Speaker Identification. (arXiv:2106.08207v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08207</id>
        <link href="http://arxiv.org/abs/2106.08207"/>
        <updated>2021-06-16T01:21:07.961Z</updated>
        <summary type="html"><![CDATA[Speaker identification in the household scenario (e.g., for smart speakers)
is typically based on only a few enrollment utterances but a much larger set of
unlabeled data, suggesting semisupervised learning to improve speaker profiles.
We propose a graph-based semi-supervised learning approach for speaker
identification in the household scenario, to leverage the unlabeled speech
samples. In contrast to most of the works in speaker recognition that focus on
speaker-discriminative embeddings, this work focuses on speaker label inference
(scoring). Given a pre-trained embedding extractor, graph-based learning allows
us to integrate information about both labeled and unlabeled utterances.
Considering each utterance as a graph node, we represent pairwise utterance
similarity scores as edge weights. Graphs are constructed per household, and
speaker identities are propagated to unlabeled nodes to optimize a global
consistency criterion. We show in experiments on the VoxCeleb dataset that this
approach makes effective use of unlabeled data and improves speaker
identification accuracy compared to two state-of-the-art scoring methods as
well as their semi-supervised variants based on pseudo-labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_V/0/1/0/all/0/1"&gt;Venkatesh Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1"&gt;Andreas Stolcke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Commonsense Cues in BERT for Solving Commonsense Tasks. (arXiv:2008.03945v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03945</id>
        <link href="http://arxiv.org/abs/2008.03945"/>
        <updated>2021-06-16T01:21:07.944Z</updated>
        <summary type="html"><![CDATA[BERT has been used for solving commonsense tasks such as CommonsenseQA. While
prior research has found that BERT does contain commonsense information to some
extent, there has been work showing that pre-trained models can rely on
spurious associations (e.g., data bias) rather than key cues in solving
sentiment classification and other problems. We quantitatively investigate the
presence of structural commonsense cues in BERT when solving commonsense tasks,
and the importance of such cues for the model prediction. Using two different
measures, we find that BERT does use relevant knowledge for solving the task,
and the presence of commonsense knowledge is positively correlated to the model
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Leyang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Sijie Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Word Sense Disambiguation in Neural Language Models. (arXiv:2106.07967v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07967</id>
        <link href="http://arxiv.org/abs/2106.07967"/>
        <updated>2021-06-16T01:21:07.936Z</updated>
        <summary type="html"><![CDATA[We present two supervised (pre-)training methods to incorporate gloss
definitions from lexical resources into neural language models (LMs). The
training improves our models' performance for Word Sense Disambiguation (WSD)
but also benefits general language understanding tasks while adding almost no
parameters. We evaluate our techniques with seven different neural LMs and find
that XLNet is more suitable for WSD than BERT. Our best-performing methods
exceeds state-of-the-art WSD techniques on the SemCor 3.0 dataset by 0.5% F1
and increase BERT's performance on the GLUE benchmark by 1.1% on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1"&gt;Jan Philip Wahle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1"&gt;Terry Ruas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1"&gt;Norman Meuschke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1"&gt;Bela Gipp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges and Considerations with Code-Mixed NLP for Multilingual Societies. (arXiv:2106.07823v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07823</id>
        <link href="http://arxiv.org/abs/2106.07823"/>
        <updated>2021-06-16T01:21:07.925Z</updated>
        <summary type="html"><![CDATA[Multilingualism refers to the high degree of proficiency in two or more
languages in the written and oral communication modes. It often results in
language mixing, a.k.a. code-mixing, when a multilingual speaker switches
between multiple languages in a single utterance of a text or speech. This
paper discusses the current state of the NLP research, limitations, and
foreseeable pitfalls in addressing five real-world applications for social good
crisis management, healthcare, political campaigning, fake news, and hate
speech for multilingual societies. We also propose futuristic datasets, models,
and tools that can significantly advance the current research in multilingual
NLP applications for the societal good. As a representative example, we
consider English-Hindi code-mixing but draw similar inferences for other
language pairs]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1"&gt;Vivek Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Representation and Inference for NLP. (arXiv:2106.08117v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08117</id>
        <link href="http://arxiv.org/abs/2106.08117"/>
        <updated>2021-06-16T01:21:07.918Z</updated>
        <summary type="html"><![CDATA[Semantic representation and inference is essential for Natural Language
Processing (NLP). The state of the art for semantic representation and
inference is deep learning, and particularly Recurrent Neural Networks (RNNs),
Convolutional Neural Networks (CNNs), and transformer Self-Attention models.
This thesis investigates the use of deep learning for novel semantic
representation and inference, and makes contributions in the following three
areas: creating training data, improving semantic representations and extending
inference learning. In terms of creating training data, we contribute the
largest publicly available dataset of real-life factual claims for the purpose
of automatic claim verification (MultiFC), and we present a novel inference
model composed of multi-scale CNNs with different kernel sizes that learn from
external sources to infer fact checking labels. In terms of improving semantic
representations, we contribute a novel model that captures non-compositional
semantic indicators. By definition, the meaning of a non-compositional phrase
cannot be inferred from the individual meanings of its composing words (e.g.,
hot dog). Motivated by this, we operationalize the compositionality of a phrase
contextually by enriching the phrase representation with external word
embeddings and knowledge graphs. Finally, in terms of inference learning, we
propose a series of novel deep learning architectures that improve inference by
using syntactic dependencies, by ensembling role guided attention heads,
incorporating gating layers, and concatenating multiple heads in novel and
effective ways. This thesis consists of seven publications (five published and
two under review).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dongsheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARTA: Collection and Classification of Ambiguous Requests and Thoughtful Actions. (arXiv:2106.07999v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07999</id>
        <link href="http://arxiv.org/abs/2106.07999"/>
        <updated>2021-06-16T01:21:07.911Z</updated>
        <summary type="html"><![CDATA[Human-assisting systems such as dialogue systems must take thoughtful,
appropriate actions not only for clear and unambiguous user requests, but also
for ambiguous user requests, even if the users themselves are not aware of
their potential requirements. To construct such a dialogue agent, we collected
a corpus and developed a model that classifies ambiguous user requests into
corresponding system actions. In order to collect a high-quality corpus, we
asked workers to input antecedent user requests whose pre-defined actions could
be regarded as thoughtful. Although multiple actions could be identified as
thoughtful for a single user request, annotating all combinations of user
requests and system actions is impractical. For this reason, we fully annotated
only the test data and left the annotation of the training data incomplete. In
order to train the classification model on such training data, we applied the
positive/unlabeled (PU) learning method, which assumes that only a part of the
data is labeled with positive examples. The experimental results show that the
PU learning method achieved better performance than the general
positive/negative (PN) learning method to classify thoughtful actions given an
ambiguous user request.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_S/0/1/0/all/0/1"&gt;Shohei Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshino_K/0/1/0/all/0/1"&gt;Koichiro Yoshino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1"&gt;Katsuhito Sudoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1"&gt;Satoshi Nakamura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Explanations for Machine Learning: Challenges Revisited. (arXiv:2106.07756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07756</id>
        <link href="http://arxiv.org/abs/2106.07756"/>
        <updated>2021-06-16T01:21:07.904Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations (CFEs) are an emerging technique under the
umbrella of interpretability of machine learning (ML) models. They provide
``what if'' feedback of the form ``if an input datapoint were $x'$ instead of
$x$, then an ML model's output would be $y'$ instead of $y$.'' Counterfactual
explainability for ML models has yet to see widespread adoption in industry. In
this short paper, we posit reasons for this slow uptake. Leveraging recent work
outlining desirable properties of CFEs and our experience running the ML wing
of a model monitoring startup, we identify outstanding obstacles hindering CFE
deployment in industry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1"&gt;Sahil Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1"&gt;John Dickerson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hines_K/0/1/0/all/0/1"&gt;Keegan Hines&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissecting User-Perceived Latency of On-Device E2E Speech Recognition. (arXiv:2104.02207v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02207</id>
        <link href="http://arxiv.org/abs/2104.02207"/>
        <updated>2021-06-16T01:21:07.898Z</updated>
        <summary type="html"><![CDATA[As speech-enabled devices such as smartphones and smart speakers become
increasingly ubiquitous, there is growing interest in building automatic speech
recognition (ASR) systems that can run directly on-device; end-to-end (E2E)
speech recognition models such as recurrent neural network transducers and
their variants have recently emerged as prime candidates for this task. Apart
from being accurate and compact, such systems need to decode speech with low
user-perceived latency (UPL), producing words as soon as they are spoken. This
work examines the impact of various techniques - model architectures, training
criteria, decoding hyperparameters, and endpointer parameters - on UPL. Our
analyses suggest that measures of model size (parameters, input chunk sizes),
or measures of computation (e.g., FLOPS, RTF) that reflect the model's ability
to process input frames are not always strongly correlated with observed UPL.
Thus, conventional algorithmic latency measurements might be inadequate in
accurately capturing latency observed when models are deployed on embedded
devices. Instead, we find that factors affecting token emission latency, and
endpointing behavior have a larger impact on UPL. We achieve the best trade-off
between latency and word error rate when performing ASR jointly with
endpointing, while utilizing the recently proposed alignment regularization
mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1"&gt;Yuan Shangguan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1"&gt;Rohit Prabhavalkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahadeokar_J/0/1/0/all/0/1"&gt;Jay Mahadeokar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yangyang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiatong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chunyang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1"&gt;Duc Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1"&gt;Ozlem Kalinli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuegen_C/0/1/0/all/0/1"&gt;Christian Fuegen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1"&gt;Michael L. Seltzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Transfer Learning for Brain Magnetic Resonance Image Multi-class Classification. (arXiv:2106.07333v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07333</id>
        <link href="http://arxiv.org/abs/2106.07333"/>
        <updated>2021-06-16T01:21:07.891Z</updated>
        <summary type="html"><![CDATA[Magnetic Resonance Imaging (MRI) is a principal diagnostic approach used in
the field of radiology to create images of the anatomical and physiological
structure of patients. MRI is the prevalent medical imaging practice to find
abnormalities in soft tissues. Traditionally they are analyzed by a radiologist
to detect abnormalities in soft tissues, especially the brain. The process of
interpreting a massive volume of patient's MRI is laborious. Hence, the use of
Machine Learning methodologies can aid in detecting abnormalities in soft
tissues with considerable accuracy. In this research, we have curated a novel
dataset and developed a framework that uses Deep Transfer Learning to perform a
multi-classification of tumors in the brain MRI images. In this paper, we
adopted the Deep Residual Convolutional Neural Network (ResNet50) architecture
for the experiments along with discriminative learning techniques to train the
model. Using the novel dataset and two publicly available MRI brain datasets,
this proposed approach attained a classification accuracy of 86.40% on the
curated dataset, 93.80% on the Harvard Whole Brain Atlas dataset, and 97.05%
accuracy on the School of Biomedical Engineering dataset. Results of our
experiments significantly demonstrate our proposed framework for transfer
learning is a potential and effective method for brain tumor
multi-classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brima_Y/0/1/0/all/0/1"&gt;Yusuf Brima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tushar_M/0/1/0/all/0/1"&gt;Mossadek Hossain Kamal Tushar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabir_U/0/1/0/all/0/1"&gt;Upama Kabir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1"&gt;Tariqul Islam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequence-Level Training for Non-Autoregressive Neural Machine Translation. (arXiv:2106.08122v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08122</id>
        <link href="http://arxiv.org/abs/2106.08122"/>
        <updated>2021-06-16T01:21:07.859Z</updated>
        <summary type="html"><![CDATA[In recent years, Neural Machine Translation (NMT) has achieved notable
results in various translation tasks. However, the word-by-word generation
manner determined by the autoregressive mechanism leads to high translation
latency of the NMT and restricts its low-latency applications.
Non-Autoregressive Neural Machine Translation (NAT) removes the autoregressive
mechanism and achieves significant decoding speedup through generating target
words independently and simultaneously. Nevertheless, NAT still takes the
word-level cross-entropy loss as the training objective, which is not optimal
because the output of NAT cannot be properly evaluated due to the multimodality
problem. In this paper, we propose using sequence-level training objectives to
train NAT models, which evaluate the NAT outputs as a whole and correlates well
with the real translation quality. Firstly, we propose training NAT models to
optimize sequence-level evaluation metrics (e.g., BLEU) based on several novel
reinforcement algorithms customized for NAT, which outperforms the conventional
method by reducing the variance of gradient estimation. Secondly, we introduce
a novel training objective for NAT models, which aims to minimize the
Bag-of-Ngrams (BoN) difference between the model output and the reference
sentence. The BoN training objective is differentiable and can be calculated
efficiently without doing any approximations. Finally, we apply a three-stage
training strategy to combine these two methods to train the NAT model. We
validate our approach on four translation tasks (WMT14 En$\leftrightarrow$De,
WMT16 En$\leftrightarrow$Ro), which shows that our approach largely outperforms
NAT baselines and achieves remarkable performance on all translation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1"&gt;Chenze Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedNILM: Applying Federated Learning to NILM Applications at the Edge. (arXiv:2106.07751v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07751</id>
        <link href="http://arxiv.org/abs/2106.07751"/>
        <updated>2021-06-16T01:21:07.847Z</updated>
        <summary type="html"><![CDATA[Non-intrusive load monitoring (NILM) helps disaggregate the household's main
electricity consumption to energy usages of individual appliances, thus greatly
cutting down the cost in fine-grained household load monitoring. To address the
arisen privacy concern in NILM applications, federated learning (FL) could be
leveraged for NILM model training and sharing. When applying the FL paradigm in
real-world NILM applications, however, we are faced with the challenges of edge
resource restriction, edge model personalization and edge training data
scarcity.

In this paper we present FedNILM, a practical FL paradigm for NILM
applications at the edge client. Specifically, FedNILM is designed to deliver
privacy-preserving and personalized NILM services to large-scale edge clients,
by leveraging i) secure data aggregation through federated learning, ii)
efficient cloud model compression via filter pruning and multi-task learning,
and iii) personalized edge model building with unsupervised transfer learning.
Our experiments on real-world energy data show that, FedNILM is able to achieve
personalized energy disaggregation with the state-of-the-art accuracy, while
ensuring privacy preserving at the edge client.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1"&gt;Guoming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qianyi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xudong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1"&gt;Jiadong Lou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teacher-Student MixIT for Unsupervised and Semi-supervised Speech Separation. (arXiv:2106.07843v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.07843</id>
        <link href="http://arxiv.org/abs/2106.07843"/>
        <updated>2021-06-16T01:21:07.836Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a novel semi-supervised learning framework for
end-to-end speech separation. The proposed method first uses mixtures of
unseparated sources and the mixture invariant training (MixIT) criterion to
train a teacher model. The teacher model then estimates separated sources that
are used to train a student model with standard permutation invariant training
(PIT). The student model can be fine-tuned with supervised data, i.e., paired
artificial mixtures and clean speech sources, and further improved via model
distillation. Experiments with single and multi channel mixtures show that the
teacher-student training resolves the over-separation problem observed in the
original MixIT method. Further, the semisupervised performance is comparable to
a fully-supervised separation system trained using ten times the amount of
supervised data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jisi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zorila_C/0/1/0/all/0/1"&gt;Catalin Zorila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1"&gt;Rama Doddipatla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barker_J/0/1/0/all/0/1"&gt;Jon Barker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Generate Task-Specific Adapters from Task Description. (arXiv:2101.00420v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00420</id>
        <link href="http://arxiv.org/abs/2101.00420"/>
        <updated>2021-06-16T01:21:07.822Z</updated>
        <summary type="html"><![CDATA[Pre-trained text-to-text transformers such as BART have achieved impressive
performance across a range of NLP tasks. Recent study further shows that they
can learn to generalize to novel tasks, by including task descriptions as part
of the source sequence and training the model with (source, target) examples.
At test time, these fine-tuned models can make inferences on new tasks using
the new task descriptions as part of the input. However, this approach has
potential limitations, as the model learns to solve individual (source, target)
examples (i.e., at the instance level), instead of learning to solve tasks by
taking all examples within a task as a whole (i.e., at the task level). To this
end, we introduce Hypter, a framework that improves text-to-text transformer's
generalization ability to unseen tasks by training a hypernetwork to generate
task-specific, light-weight adapters from task descriptions. Experiments on
ZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves
upon fine-tuning baselines. Notably, when using BART-Large as the main network,
Hypter brings 11.3% comparative improvement on ZEST dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qinyuan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Launching into clinical space with medspaCy: a new clinical text processing toolkit in Python. (arXiv:2106.07799v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07799</id>
        <link href="http://arxiv.org/abs/2106.07799"/>
        <updated>2021-06-16T01:21:07.797Z</updated>
        <summary type="html"><![CDATA[Despite impressive success of machine learning algorithms in clinical natural
language processing (cNLP), rule-based approaches still have a prominent role.
In this paper, we introduce medspaCy, an extensible, open-source cNLP library
based on spaCy framework that allows flexible integration of rule-based and
machine learning-based algorithms adapted to clinical text. MedspaCy includes a
variety of components that meet common cNLP needs such as context analysis and
mapping to standard terminologies. By utilizing spaCy's clear and easy-to-use
conventions, medspaCy enables development of custom pipelines that integrate
easily with other spaCy-based modules. Our toolkit includes several core
components and facilitates rapid development of pipelines for clinical text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eyre_H/0/1/0/all/0/1"&gt;Hannah Eyre&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Chapman_A/0/1/0/all/0/1"&gt;Alec B Chapman&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Peterson_K/0/1/0/all/0/1"&gt;Kelly S Peterson&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jianlin Shi&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Alba_P/0/1/0/all/0/1"&gt;Patrick R Alba&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1"&gt;Makoto M Jones&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Box_T/0/1/0/all/0/1"&gt;Tamara L Box&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+DuVall_S/0/1/0/all/0/1"&gt;Scott L DuVall&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Patterson_O/0/1/0/all/0/1"&gt;Olga V Patterson&lt;/a&gt; (1 and 2) ((1) VA Salt Lake City Health Care System, (2) University of Utah, Salt Lake City, UT, USA, (3) Veterans Health Administration Office of Analytics and Performance Integration)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using heterogeneity in semi-supervised transcription hypotheses to improve code-switched speech recognition. (arXiv:2106.07699v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07699</id>
        <link href="http://arxiv.org/abs/2106.07699"/>
        <updated>2021-06-16T01:21:07.767Z</updated>
        <summary type="html"><![CDATA[Modeling code-switched speech is an important problem in automatic speech
recognition (ASR). Labeled code-switched data are rare, so monolingual data are
often used to model code-switched speech. These monolingual data may be more
closely matched to one of the languages in the code-switch pair. We show that
such asymmetry can bias prediction toward the better-matched language and
degrade overall model performance. To address this issue, we propose a
semi-supervised approach for code-switched ASR. We consider the case of
English-Mandarin code-switching, and the problem of using monolingual data to
build bilingual "transcription models'' for annotation of unlabeled
code-switched data. We first build multiple transcription models so that their
individual predictions are variously biased toward either English or Mandarin.
We then combine these biased transcriptions using confidence-based selection.
This strategy generates a superior transcript for semi-supervised training, and
obtains a 19% relative improvement compared to a semi-supervised system that
relies on a transcription model built with only the best-matched monolingual
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slottje_A/0/1/0/all/0/1"&gt;Andrew Slottje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wotherspoon_S/0/1/0/all/0/1"&gt;Shannon Wotherspoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartmann_W/0/1/0/all/0/1"&gt;William Hartmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snover_M/0/1/0/all/0/1"&gt;Matthew Snover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kimball_O/0/1/0/all/0/1"&gt;Owen Kimball&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07306</id>
        <link href="http://arxiv.org/abs/2106.07306"/>
        <updated>2021-06-16T01:21:07.743Z</updated>
        <summary type="html"><![CDATA[In structured prediction, a major challenge for models is to represent the
interdependencies within their output structures. For the common case where
outputs are structured as a sequence, linear-chain conditional random fields
(CRFs) are a widely used model class which can learn local dependencies in
output sequences. However, the CRF's Markov assumption makes it impossible for
these models to capture nonlocal dependencies, and standard CRFs are unable to
respect nonlocal constraints of the data (such as global arity constraints on
output labels). We present a generalization of CRFs that can enforce a broad
class of constraints, including nonlocal ones, by specifying the space of
possible output structures as a regular language $\mathcal{L}$. The resulting
regular-constrained CRF (RegCCRF) has the same formal properties as a standard
CRF, but assigns zero probability to all label sequences not in $\mathcal{L}$.
Notably, RegCCRFs can incorporate their constraints during training, while
related models only enforce constraints during decoding. We prove that
constrained training is never worse than constrained decoding, and show using
synthetic data that it can be substantially better in practice. Additionally,
we demonstrate a practical benefit on downstream tasks by incorporating a
RegCCRF into a deep neural model for semantic role labeling, exceeding
state-of-the-art results on a standard dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papay_S/0/1/0/all/0/1"&gt;Sean Papay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1"&gt;Roman Klinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1"&gt;Sebastian Pad&amp;#xf3;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased Sentence Encoder For Large-Scale Multi-lingual Search Engines. (arXiv:2106.07719v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07719</id>
        <link href="http://arxiv.org/abs/2106.07719"/>
        <updated>2021-06-16T01:21:07.736Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a multi-lingual sentence encoder that can be used
in search engines as a query and document encoder. This embedding enables a
semantic similarity score between queries and documents that can be an
important feature in document ranking and relevancy. To train such a customized
sentence encoder, it is beneficial to leverage users search data in the form of
query-document clicked pairs however, we must avoid relying too much on search
click data as it is biased and does not cover many unseen cases. The search
data is heavily skewed towards short queries and for long queries is small and
often noisy. The goal is to design a universal multi-lingual encoder that works
for all cases and covers both short and long queries. We select a number of
public NLI datasets in different languages and translation data and together
with user search data we train a language model using a multi-task approach. A
challenge is that these datasets are not homogeneous in terms of content, size
and the balance ratio. While the public NLI datasets are usually two-sentence
based with the same portion of positive and negative pairs, the user search
data can contain multi-sentence documents and only positive pairs. We show how
multi-task training enables us to leverage all these datasets and exploit
knowledge sharing across these tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1"&gt;Mahdi Hajiaghayi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1"&gt;Monir Hajiaghayi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolin_M/0/1/0/all/0/1"&gt;Mark Bolin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Diffusion Halftoning Against Adversarial Examples. (arXiv:2101.09451v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09451</id>
        <link href="http://arxiv.org/abs/2101.09451"/>
        <updated>2021-06-16T01:21:07.729Z</updated>
        <summary type="html"><![CDATA[Adversarial examples contain carefully crafted perturbations that can fool
deep neural networks (DNNs) into making wrong predictions. Enhancing the
adversarial robustness of DNNs has gained considerable interest in recent
years. Although image transformation-based defenses were widely considered at
an earlier time, most of them have been defeated by adaptive attacks. In this
paper, we propose a new image transformation defense based on error diffusion
halftoning, and combine it with adversarial training to defend against
adversarial examples. Error diffusion halftoning projects an image into a 1-bit
space and diffuses quantization error to neighboring pixels. This process can
remove adversarial perturbations from a given image while maintaining
acceptable image quality in the meantime in favor of recognition. Experimental
results demonstrate that the proposed method is able to improve adversarial
robustness even under advanced adaptive attacks, while most of the other image
transformation-based defenses do not. We show that a proper image
transformation can still be an effective defense approach. Code:
https://github.com/shaoyuanlo/Halftoning-Defense]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shao-Yuan Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks with Heterophily. (arXiv:2009.13566v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13566</id>
        <link href="http://arxiv.org/abs/2009.13566"/>
        <updated>2021-06-16T01:21:07.710Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have proven to be useful for many different
practical applications. However, many existing GNN models have implicitly
assumed homophily among the nodes connected in the graph, and therefore have
largely overlooked the important setting of heterophily, where most connected
nodes are from different classes. In this work, we propose a novel framework
called CPGNN that generalizes GNNs for graphs with either homophily or
heterophily. The proposed framework incorporates an interpretable compatibility
matrix for modeling the heterophily or homophily level in the graph, which can
be learned in an end-to-end fashion, enabling it to go beyond the assumption of
strong homophily. Theoretically, we show that replacing the compatibility
matrix in our framework with the identity (which represents pure homophily)
reduces to GCN. Our extensive experiments demonstrate the effectiveness of our
approach in more realistic and challenging experimental settings with
significantly less training data compared to previous works: CPGNN variants
achieve state-of-the-art results in heterophily settings with or without
contextual node features, while maintaining comparable performance in homophily
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1"&gt;Ryan A. Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1"&gt;Anup Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1"&gt;Tung Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1"&gt;Nedim Lipka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1"&gt;Nesreen K. Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1"&gt;Danai Koutra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient and Accurate Gradients for Neural SDEs. (arXiv:2105.13493v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13493</id>
        <link href="http://arxiv.org/abs/2105.13493"/>
        <updated>2021-06-16T01:21:07.703Z</updated>
        <summary type="html"><![CDATA[Neural SDEs combine many of the best qualities of both RNNs and SDEs: memory
efficient training, high-capacity function approximation, and strong priors on
model space. This makes them a natural choice for modelling many types of
temporal dynamics. Training a Neural SDE (either as a VAE or as a GAN) requires
backpropagating through an SDE solve. This may be done by solving a
backwards-in-time SDE whose solution is the desired parameter gradients.
However, this has previously suffered from severe speed and accuracy issues,
due to high computational cost and numerical truncation errors. Here, we
overcome these issues through several technical innovations. First, we
introduce the \textit{reversible Heun method}. This is a new SDE solver that is
\textit{algebraically reversible}: eliminating numerical gradient errors, and
the first such solver of which we are aware. Moreover it requires half as many
function evaluations as comparable solvers, giving up to a $1.98\times$
speedup. Second, we introduce the \textit{Brownian Interval}: a new, fast,
memory efficient, and exact way of sampling \textit{and reconstructing}
Brownian motion. With this we obtain up to a $10.6\times$ speed improvement
over previous techniques, which in contrast are both approximate and relatively
slow. Third, when specifically training Neural SDEs as GANs (Kidger et al.
2021), we demonstrate how SDE-GANs may be trained through careful weight
clipping and choice of activation function. This reduces computational cost
(giving up to a $1.87\times$ speedup) and removes the numerical truncation
errors associated with gradient penalty. Altogether, we outperform the
state-of-the-art by substantial margins, with respect to training speed, and
with respect to classification, prediction, and MMD test metrics. We have
contributed implementations of all of our techniques to the torchsde library to
help facilitate their adoption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kidger_P/0/1/0/all/0/1"&gt;Patrick Kidger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1"&gt;James Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuechen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyons_T/0/1/0/all/0/1"&gt;Terry Lyons&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selfish Sparse RNN Training. (arXiv:2101.09048v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09048</id>
        <link href="http://arxiv.org/abs/2101.09048"/>
        <updated>2021-06-16T01:21:07.696Z</updated>
        <summary type="html"><![CDATA[Sparse neural networks have been widely applied to reduce the computational
demands of training and deploying over-parameterized deep neural networks. For
inference acceleration, methods that discover a sparse network from a
pre-trained dense network (dense-to-sparse training) work effectively.
Recently, dynamic sparse training (DST) has been proposed to train sparse
neural networks without pre-training a dense model (sparse-to-sparse training),
so that the training process can also be accelerated. However, previous
sparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs)
and Convolutional Neural Networks (CNNs), failing to match the performance of
dense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting. In
this paper, we propose an approach to train intrinsically sparse RNNs with a
fixed parameter count in one single run, without compromising performance.
During training, we allow RNN layers to have a non-uniform redistribution
across cell gates for better regularization. Further, we propose SNT-ASGD, a
novel variant of the averaged stochastic gradient optimizer, which
significantly improves the performance of all sparse training methods for RNNs.
Using these strategies, we achieve state-of-the-art sparse training results,
better than the dense-to-sparse methods, with various types of RNNs on Penn
TreeBank and Wikitext-2 datasets. Our codes are available at
https://github.com/Shiweiliuiiiiiii/Selfish-RNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1"&gt;Decebal Constantin Mocanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1"&gt;Yulong Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate spectral clustering using both reference vectors and topology of the network generated by growing neural gas. (arXiv:2009.07101v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07101</id>
        <link href="http://arxiv.org/abs/2009.07101"/>
        <updated>2021-06-16T01:21:07.690Z</updated>
        <summary type="html"><![CDATA[Spectral clustering (SC) is one of the most popular clustering methods and
often outperforms traditional clustering methods. SC uses the eigenvectors of a
Laplacian matrix calculated from a similarity matrix of a dataset. SC has the
serious drawbacks that are the significant increases in the time complexity
derived from the computation of eigenvectors and the memory space complexity to
store the similarity matrix. To address the issues, I develop a new approximate
spectral clustering using the network generated by growing neural gas (GNG),
called ASC with GNG in this study. ASC with GNG uses not only reference vectors
for vector quantization but also the topology of the network for extraction of
the topological relationship between data points in a dataset. ASC with GNG
makes the similarity matrix from both the reference vectors and the topology of
the network generated by GNG. Using the network generated from a dataset by
GNG, ASC with GNG achieves to reduce the computational and space complexities
and improve clustering quality. In this study, I demonstrate that ASC with GNG
effectively reduces computational time. Moreover, this study shows that ASC
with GNG displays equal to or better clustering performance than SC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujita_K/0/1/0/all/0/1"&gt;Kazuhisa Fujita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coded Machine Unlearning. (arXiv:2012.15721v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15721</id>
        <link href="http://arxiv.org/abs/2012.15721"/>
        <updated>2021-06-16T01:21:07.683Z</updated>
        <summary type="html"><![CDATA[There are applications that may require removing the trace of a sample from
the system, e.g., a user requests their data to be deleted, or corrupted data
is discovered. Simply removing a sample from storage units does not necessarily
remove its entire trace since downstream machine learning models may store some
information about the samples used to train them. A sample can be perfectly
unlearned if we retrain all models that used it from scratch with that sample
removed from their training dataset. When multiple such unlearning requests are
expected to be served, unlearning by retraining becomes prohibitively
expensive. Ensemble learning enables the training data to be split into smaller
disjoint shards that are assigned to non-communicating weak learners. Each
shard is used to produce a weak model. These models are then aggregated to
produce the final central model. This setup introduces an inherent trade-off
between performance and unlearning cost, as reducing the shard size reduces the
unlearning cost but may cause degradation in performance. In this paper, we
propose a coded learning protocol where we utilize linear encoders to encode
the training data into shards prior to the learning phase. We also present the
corresponding unlearning protocol and show that it satisfies the perfect
unlearning criterion. Our experimental results show that the proposed coded
machine unlearning provides a better performance versus unlearning cost
trade-off compared to the uncoded baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aldaghri_N/0/1/0/all/0/1"&gt;Nasser Aldaghri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahdavifar_H/0/1/0/all/0/1"&gt;Hessam Mahdavifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1"&gt;Ahmad Beirami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can BERT Dig It? -- Named Entity Recognition for Information Retrieval in the Archaeology Domain. (arXiv:2106.07742v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07742</id>
        <link href="http://arxiv.org/abs/2106.07742"/>
        <updated>2021-06-16T01:21:07.661Z</updated>
        <summary type="html"><![CDATA[The amount of archaeological literature is growing rapidly. Until recently,
these data were only accessible through metadata search. We implemented a text
retrieval engine for a large archaeological text collection ($\sim 658$ Million
words). In archaeological IR, domain-specific entities such as locations, time
periods, and artefacts, play a central role. This motivated the development of
a named entity recognition (NER) model to annotate the full collection with
archaeological named entities. In this paper, we present ArcheoBERTje, a BERT
model pre-trained on Dutch archaeological texts. We compare the model's quality
and output on a Named Entity Recognition task to a generic multilingual model
and a generic Dutch model. We also investigate ensemble methods for combining
multiple BERT models, and combining the best BERT model with a domain thesaurus
using Conditional Random Fields (CRF). We find that ArcheoBERTje outperforms
both the multilingual and Dutch model significantly with a smaller standard
deviation between runs, reaching an average F1 score of 0.735. The model also
outperforms ensemble methods combining the three models. Combining ArcheoBERTje
predictions and explicit domain knowledge from the thesaurus did not increase
the F1 score. We quantitatively and qualitatively analyse the differences
between the vocabulary and output of the BERT models on the full collection and
provide some valuable insights in the effect of fine-tuning for specific
domains. Our results indicate that for a highly specific text domain such as
archaeology, further pre-training on domain-specific data increases the model's
quality on NER by a much larger margin than shown for other domains in the
literature, and that domain-specific pre-training makes the addition of domain
knowledge from a thesaurus unnecessary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandsen_A/0/1/0/all/0/1"&gt;Alex Brandsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1"&gt;Suzan Verberne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lambers_K/0/1/0/all/0/1"&gt;Karsten Lambers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wansleeben_M/0/1/0/all/0/1"&gt;Milco Wansleeben&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Modules in Graph Contrastive Learning. (arXiv:2106.08171v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08171</id>
        <link href="http://arxiv.org/abs/2106.08171"/>
        <updated>2021-06-16T01:21:07.652Z</updated>
        <summary type="html"><![CDATA[The recent emergence of contrastive learning approaches facilitates the
research on graph representation learning (GRL), introducing graph contrastive
learning (GCL) into the literature. These methods contrast semantically similar
and dissimilar sample pairs to encode the semantics into node or graph
embeddings. However, most existing works only performed model-level evaluation,
and did not explore the combination space of modules for more comprehensive and
systematic studies. For effective module-level evaluation, we propose a
framework that decomposes GCL models into four modules: (1) a sampler to
generate anchor, positive and negative data samples (nodes or graphs); (2) an
encoder and a readout function to get sample embeddings; (3) a discriminator to
score each sample pair (anchor-positive and anchor-negative); and (4) an
estimator to define the loss function. Based on this framework, we conduct
controlled experiments over a wide range of architectural designs and
hyperparameter settings on node and graph classification tasks. Specifically,
we manage to quantify the impact of a single module, investigate the
interaction between modules, and compare the overall performance with current
model architectures. Our key findings include a set of module-level guidelines
for GCL, e.g., simple samplers from LINE and DeepWalk are strong and robust; an
MLP encoder associated with Sum readout could achieve competitive performance
on graph classification. Finally, we release our implementations and results as
OpenGCL, a modularized toolkit that allows convenient reproduction, standard
model and module evaluation, and easy extension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1"&gt;Ganqu Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yufeng Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lifeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training. (arXiv:2102.02887v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02887</id>
        <link href="http://arxiv.org/abs/2102.02887"/>
        <updated>2021-06-16T01:21:07.644Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a new perspective on training deep neural
networks capable of state-of-the-art performance without the need for the
expensive over-parameterization by proposing the concept of In-Time
Over-Parameterization (ITOP) in sparse training. By starting from a random
sparse network and continuously exploring sparse connectivities during
training, we can perform an Over-Parameterization in the space-time manifold,
closing the gap in the expressibility between sparse training and dense
training. We further use ITOP to understand the underlying mechanism of Dynamic
Sparse Training (DST) and indicate that the benefits of DST come from its
ability to consider across time all possible parameters when searching for the
optimal sparse connectivity. As long as there are sufficient parameters that
have been reliably explored during training, DST can outperform the dense
neural network by a large margin. We present a series of experiments to support
our conjecture and achieve the state-of-the-art sparse training performance
with ResNet-50 on ImageNet. More impressively, our method achieves dominant
performance over the overparameterization-based sparse methods at extreme
sparsity levels. When trained on CIFAR-100, our method can match the
performance of the dense model even at an extreme sparsity (98%). Code can be
found https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1"&gt;Lu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1"&gt;Decebal Constantin Mocanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-script Handwritten Digit Recognition Using Multi-task Learning. (arXiv:2106.08267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08267</id>
        <link href="http://arxiv.org/abs/2106.08267"/>
        <updated>2021-06-16T01:21:07.638Z</updated>
        <summary type="html"><![CDATA[Handwritten digit recognition is one of the extensively studied area in
machine learning. Apart from the wider research on handwritten digit
recognition on MNIST dataset, there are many other research works on various
script recognition. However, it is not very common for multi-script digit
recognition which encourage the development of robust and multipurpose systems.
Additionally working on multi-script digit recognition enables multi-task
learning, considering the script classification as a related task for instance.
It is evident that multi-task learning improves model performance through
inductive transfer using the information contained in related tasks. Therefore,
in this study multi-script handwritten digit recognition using multi-task
learning will be investigated. As a specific case of demonstrating the solution
to the problem, Amharic handwritten character recognition will also be
experimented. The handwritten digits of three scripts including Latin, Arabic
and Kannada are studied to show that multi-task models with reformulation of
the individual tasks have shown promising results. In this study a novel way of
using the individual tasks predictions was proposed to help classification
performance and regularize the different loss for the purpose of the main task.
This finding has outperformed the baseline and the conventional multi-task
learning models. More importantly, it avoided the need for weighting the
different losses of the tasks, which is one of the challenges in multi-task
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gondere_M/0/1/0/all/0/1"&gt;Mesay Samuel Gondere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1"&gt;Lars Schmidt-Thieme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1"&gt;Durga Prasad Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholz_R/0/1/0/all/0/1"&gt;Randolf Scholz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Generate Task-Specific Adapters from Task Description. (arXiv:2101.00420v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00420</id>
        <link href="http://arxiv.org/abs/2101.00420"/>
        <updated>2021-06-16T01:21:07.630Z</updated>
        <summary type="html"><![CDATA[Pre-trained text-to-text transformers such as BART have achieved impressive
performance across a range of NLP tasks. Recent study further shows that they
can learn to generalize to novel tasks, by including task descriptions as part
of the source sequence and training the model with (source, target) examples.
At test time, these fine-tuned models can make inferences on new tasks using
the new task descriptions as part of the input. However, this approach has
potential limitations, as the model learns to solve individual (source, target)
examples (i.e., at the instance level), instead of learning to solve tasks by
taking all examples within a task as a whole (i.e., at the task level). To this
end, we introduce Hypter, a framework that improves text-to-text transformer's
generalization ability to unseen tasks by training a hypernetwork to generate
task-specific, light-weight adapters from task descriptions. Experiments on
ZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves
upon fine-tuning baselines. Notably, when using BART-Large as the main network,
Hypter brings 11.3% comparative improvement on ZEST dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qinyuan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keyword Transformer: A Self-Attention Model for Keyword Spotting. (arXiv:2104.00769v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00769</id>
        <link href="http://arxiv.org/abs/2104.00769"/>
        <updated>2021-06-16T01:21:07.623Z</updated>
        <summary type="html"><![CDATA[The Transformer architecture has been successful across many domains,
including natural language processing, computer vision and speech recognition.
In keyword spotting, self-attention has primarily been used on top of
convolutional or recurrent encoders. We investigate a range of ways to adapt
the Transformer architecture to keyword spotting and introduce the Keyword
Transformer (KWT), a fully self-attentional architecture that exceeds
state-of-the-art performance across multiple tasks without any pre-training or
additional data. Surprisingly, this simple architecture outperforms more
complex models that mix convolutional, recurrent and attentive layers. KWT can
be used as a drop-in replacement for these models, setting two new benchmark
records on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on
the 12 and 35-command tasks respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Berg_A/0/1/0/all/0/1"&gt;Axel Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+OConnor_M/0/1/0/all/0/1"&gt;Mark O&amp;#x27;Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cruz_M/0/1/0/all/0/1"&gt;Miguel Tairum Cruz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Syntax and Semantics in the Brain with Deep Networks. (arXiv:2103.01620v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01620</id>
        <link href="http://arxiv.org/abs/2103.01620"/>
        <updated>2021-06-16T01:21:07.603Z</updated>
        <summary type="html"><![CDATA[The activations of language transformers like GPT-2 have been shown to
linearly map onto brain activity during speech comprehension. However, the
nature of these activations remains largely unknown and presumably conflate
distinct linguistic classes. Here, we propose a taxonomy to factorize the
high-dimensional activations of language models into four combinatorial
classes: lexical, compositional, syntactic, and semantic representations. We
then introduce a statistical method to decompose, through the lens of GPT-2's
activations, the brain activity of 345 subjects recorded with functional
magnetic resonance imaging (fMRI) during the listening of ~4.6 hours of
narrated text. The results highlight two findings. First, compositional
representations recruit a more widespread cortical network than lexical ones,
and encompass the bilateral temporal, parietal and prefrontal cortices. Second,
contrary to previous claims, syntax and semantics are not associated with
separated modules, but, instead, appear to share a common and distributed
neural substrate. Overall, this study introduces a versatile framework to
isolate, in the brain activity, the distributed representations of linguistic
constructs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caucheteux_C/0/1/0/all/0/1"&gt;Charlotte Caucheteux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gramfort_A/0/1/0/all/0/1"&gt;Alexandre Gramfort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_J/0/1/0/all/0/1"&gt;Jean-Remi King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Power of Multitask Representation Learning in Linear MDP. (arXiv:2106.08053v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08053</id>
        <link href="http://arxiv.org/abs/2106.08053"/>
        <updated>2021-06-16T01:21:07.596Z</updated>
        <summary type="html"><![CDATA[While multitask representation learning has become a popular approach in
reinforcement learning (RL), theoretical understanding of why and when it works
remains limited. This paper presents analyses for the statistical benefit of
multitask representation learning in linear Markov Decision Process (MDP) under
a generative model. In this paper, we consider an agent to learn a
representation function $\phi$ out of a function class $\Phi$ from $T$ source
tasks with $N$ data per task, and then use the learned $\hat{\phi}$ to reduce
the required number of sample for a new task. We first discover a
\emph{Least-Activated-Feature-Abundance} (LAFA) criterion, denoted as $\kappa$,
with which we prove that a straightforward least-square algorithm learns a
policy which is $\tilde{O}(H^2\sqrt{\frac{\mathcal{C}(\Phi)^2 \kappa
d}{NT}+\frac{\kappa d}{n}})$ sub-optimal. Here $H$ is the planning horizon,
$\mathcal{C}(\Phi)$ is $\Phi$'s complexity measure, $d$ is the dimension of the
representation (usually $d\ll \mathcal{C}(\Phi)$) and $n$ is the number of
samples for the new task. Thus the required $n$ is $O(\kappa d H^4)$ for the
sub-optimality to be close to zero, which is much smaller than
$O(\mathcal{C}(\Phi)^2\kappa d H^4)$ in the setting without multitask
representation learning, whose sub-optimality gap is
$\tilde{O}(H^2\sqrt{\frac{\kappa \mathcal{C}(\Phi)^2d}{n}})$. This
theoretically explains the power of multitask representation learning in
reducing sample complexity. Further, we note that to ensure high sample
efficiency, the LAFA criterion $\kappa$ should be small. In fact, $\kappa$
varies widely in magnitude depending on the different sampling distribution for
new task. This indicates adaptive sampling technique is important to make
$\kappa$ solely depend on $d$. Finally, we provide empirical results of a noisy
grid-world environment to corroborate our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1"&gt;Rui Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Training Data from Large Language Models. (arXiv:2012.07805v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07805</id>
        <link href="http://arxiv.org/abs/2012.07805"/>
        <updated>2021-06-16T01:21:07.588Z</updated>
        <summary type="html"><![CDATA[It has become common to publish large (billion parameter) language models
that have been trained on private datasets. This paper demonstrates that in
such settings, an adversary can perform a training data extraction attack to
recover individual training examples by querying the language model.

We demonstrate our attack on GPT-2, a language model trained on scrapes of
the public Internet, and are able to extract hundreds of verbatim text
sequences from the model's training data. These extracted examples include
(public) personally identifiable information (names, phone numbers, and email
addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible
even though each of the above sequences are included in just one document in
the training data.

We comprehensively evaluate our extraction attack to understand the factors
that contribute to its success. Worryingly, we find that larger models are more
vulnerable than smaller models. We conclude by drawing lessons and discussing
possible safeguards for training large language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1"&gt;Florian Tramer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1"&gt;Eric Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jagielski_M/0/1/0/all/0/1"&gt;Matthew Jagielski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herbert_Voss_A/0/1/0/all/0/1"&gt;Ariel Herbert-Voss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Katherine Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1"&gt;Adam Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1"&gt;Tom Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erlingsson_U/0/1/0/all/0/1"&gt;Ulfar Erlingsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oprea_A/0/1/0/all/0/1"&gt;Alina Oprea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1"&gt;Colin Raffel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Model Stitching to Compare Neural Representations. (arXiv:2106.07682v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07682</id>
        <link href="http://arxiv.org/abs/2106.07682"/>
        <updated>2021-06-16T01:21:07.579Z</updated>
        <summary type="html"><![CDATA[We revisit and extend model stitching (Lenc & Vedaldi 2015) as a methodology
to study the internal representations of neural networks. Given two trained and
frozen models $A$ and $B$, we consider a "stitched model'' formed by connecting
the bottom-layers of $A$ to the top-layers of $B$, with a simple trainable
layer between them. We argue that model stitching is a powerful and perhaps
under-appreciated tool, which reveals aspects of representations that measures
such as centered kernel alignment (CKA) cannot. Through extensive experiments,
we use model stitching to obtain quantitative verifications for intuitive
statements such as "good networks learn similar representations'', by
demonstrating that good networks of the same architecture, but trained in very
different ways (e.g.: supervised vs. self-supervised learning), can be stitched
to each other without drop in performance. We also give evidence for the
intuition that "more is better'' by showing that representations learnt with
(1) more data, (2) bigger width, or (3) more training time can be "plugged in''
to weaker models to improve performance. Finally, our experiments reveal a new
structural property of SGD which we call "stitching connectivity'', akin to
mode-connectivity: typical minima reached by SGD can all be stitched to each
other with minimal change in accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_Y/0/1/0/all/0/1"&gt;Yamini Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1"&gt;Preetum Nakkiran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1"&gt;Boaz Barak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Learning from Activity Description. (arXiv:2102.07024v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07024</id>
        <link href="http://arxiv.org/abs/2102.07024"/>
        <updated>2021-06-16T01:21:07.559Z</updated>
        <summary type="html"><![CDATA[We present a novel interactive learning protocol that enables training
request-fulfilling agents by verbally describing their activities. Unlike
imitation learning (IL), our protocol allows the teaching agent to provide
feedback in a language that is most appropriate for them. Compared with reward
in reinforcement learning (RL), the description feedback is richer and allows
for improved sample complexity. We develop a probabilistic framework and an
algorithm that practically implements our protocol. Empirical results in two
challenging request-fulfilling problems demonstrate the strengths of our
approach: compared with RL baselines, it is more sample-efficient; compared
with IL baselines, it achieves competitive success rates without requiring the
teaching agent to be able to demonstrate the desired behavior using the
learning agent's actions. Apart from empirical evaluation, we also provide
theoretical guarantees for our algorithm under certain assumptions about the
teacher and the environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khanh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schapire_R/0/1/0/all/0/1"&gt;Robert Schapire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1"&gt;Miro Dud&amp;#xed;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafto_P/0/1/0/all/0/1"&gt;Patrick Shafto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Global Dynamics of Loss Landscape in Deep Learning Models. (arXiv:2106.07683v1 [math.DS])]]></title>
        <id>http://arxiv.org/abs/2106.07683</id>
        <link href="http://arxiv.org/abs/2106.07683"/>
        <updated>2021-06-16T01:21:07.551Z</updated>
        <summary type="html"><![CDATA[Deep learning models evolve through training to learn the manifold in which
the data exists to satisfy an objective. It is well known that evolution leads
to different final states which produce inconsistent predictions of the same
test data points. This calls for techniques to be able to empirically quantify
the difference in the trajectories and highlight problematic regions. While
much focus is placed on discovering what models learn, the question of how a
model learns is less studied beyond theoretical landscape characterizations and
local geometric approximations near optimal conditions. Here, we present a
toolkit for the Dynamical Organization Of Deep Learning Loss Landscapes, or
DOODL3. DOODL3 formulates the training of neural networks as a dynamical
system, analyzes the learning process, and presents an interpretable global
view of trajectories in the loss landscape. Our approach uses the coarseness of
topology to capture the granularity of geometry to mitigate against states of
instability or elongated training. Overall, our analysis presents an empirical
framework to extract the global dynamics of a model and to use that information
to guide the training of neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Eslami_M/0/1/0/all/0/1"&gt;Mohammed Eslami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Eramian_H/0/1/0/all/0/1"&gt;Hamed Eramian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gameiro_M/0/1/0/all/0/1"&gt;Marcio Gameiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kalies_W/0/1/0/all/0/1"&gt;William Kalies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mischaikow_K/0/1/0/all/0/1"&gt;Konstantin Mischaikow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Inference with Continuously-Indexed Normalizing Flows. (arXiv:2007.05426v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05426</id>
        <link href="http://arxiv.org/abs/2007.05426"/>
        <updated>2021-06-16T01:21:07.543Z</updated>
        <summary type="html"><![CDATA[Continuously-indexed flows (CIFs) have recently achieved improvements over
baseline normalizing flows on a variety of density estimation tasks. CIFs do
not possess a closed-form marginal density, and so, unlike standard flows,
cannot be plugged in directly to a variational inference (VI) scheme in order
to produce a more expressive family of approximate posteriors. However, we show
here how CIFs can be used as part of an auxiliary VI scheme to formulate and
train expressive posterior approximations in a natural way. We exploit the
conditional independence structure of multi-layer CIFs to build the required
auxiliary inference models, which we show empirically yield low-variance
estimators of the model evidence. We then demonstrate the advantages of CIFs
over baseline flows in VI problems when the posterior distribution of interest
possesses a complicated topology, obtaining improved results in both the
Bayesian inference and surrogate maximum likelihood settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Caterini_A/0/1/0/all/0/1"&gt;Anthony Caterini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cornish_R/0/1/0/all/0/1"&gt;Rob Cornish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sejdinovic_D/0/1/0/all/0/1"&gt;Dino Sejdinovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallel Training of Deep Networks with Local Updates. (arXiv:2012.03837v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03837</id>
        <link href="http://arxiv.org/abs/2012.03837"/>
        <updated>2021-06-16T01:21:07.536Z</updated>
        <summary type="html"><![CDATA[Deep learning models trained on large data sets have been widely successful
in both vision and language domains. As state-of-the-art deep learning
architectures have continued to grow in parameter count so have the compute
budgets and times required to train them, increasing the need for
compute-efficient methods that parallelize training. Two common approaches to
parallelize the training of deep networks have been data and model parallelism.
While useful, data and model parallelism suffer from diminishing returns in
terms of compute efficiency for large batch sizes. In this paper, we
investigate how to continue scaling compute efficiently beyond the point of
diminishing returns for large batches through local parallelism, a framework
which parallelizes training of individual layers in deep networks by replacing
global backpropagation with truncated layer-wise backpropagation. Local
parallelism enables fully asynchronous layer-wise parallelism with a low memory
footprint, and requires little communication overhead compared with model
parallelism. We show results in both vision and language domains across a
diverse set of architectures, and find that local parallelism is particularly
effective in the high-compute regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laskin_M/0/1/0/all/0/1"&gt;Michael Laskin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metz_L/0/1/0/all/0/1"&gt;Luke Metz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nabarro_S/0/1/0/all/0/1"&gt;Seth Nabarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saroufim_M/0/1/0/all/0/1"&gt;Mark Saroufim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noune_B/0/1/0/all/0/1"&gt;Badreddine Noune&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1"&gt;Carlo Luschi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1"&gt;Jascha Sohl-Dickstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantics Altering Modifications for Evaluating Comprehension in Machine Reading. (arXiv:2012.04056v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04056</id>
        <link href="http://arxiv.org/abs/2012.04056"/>
        <updated>2021-06-16T01:21:07.529Z</updated>
        <summary type="html"><![CDATA[Advances in NLP have yielded impressive results for the task of machine
reading comprehension (MRC), with approaches having been reported to achieve
performance comparable to that of humans. In this paper, we investigate whether
state-of-the-art MRC models are able to correctly process Semantics Altering
Modifications (SAM): linguistically-motivated phenomena that alter the
semantics of a sentence while preserving most of its lexical surface form. We
present a method to automatically generate and align challenge sets featuring
original and altered examples. We further propose a novel evaluation
methodology to correctly assess the capability of MRC systems to process these
examples independent of the data they were optimised on, by discounting for
effects introduced by domain shift. In a large-scale empirical study, we apply
the methodology in order to evaluate extractive MRC models with regard to their
capability to correctly process SAM-enriched data. We comprehensively cover 12
different state-of-the-art neural architecture configurations and four training
datasets and find that -- despite their well-known remarkable performance --
optimised models consistently struggle to correctly process semantically
altered data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schlegel_V/0/1/0/all/0/1"&gt;Viktor Schlegel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1"&gt;Goran Nenadic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batista_Navarro_R/0/1/0/all/0/1"&gt;Riza Batista-Navarro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Principle of Least Action for the Training of Neural Networks. (arXiv:2009.08372v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08372</id>
        <link href="http://arxiv.org/abs/2009.08372"/>
        <updated>2021-06-16T01:21:07.521Z</updated>
        <summary type="html"><![CDATA[Neural networks have been achieving high generalization performance on many
tasks despite being highly over-parameterized. Since classical statistical
learning theory struggles to explain this behavior, much effort has recently
been focused on uncovering the mechanisms behind it, in the hope of developing
a more adequate theoretical framework and having a better control over the
trained models. In this work, we adopt an alternate perspective, viewing the
neural network as a dynamical system displacing input particles over time. We
conduct a series of experiments and, by analyzing the network's behavior
through its displacements, we show the presence of a low kinetic energy
displacement bias in the transport map of the network, and link this bias with
generalization performance. From this observation, we reformulate the learning
problem as follows: finding neural networks which solve the task while
transporting the data as efficiently as possible. This offers a novel
formulation of the learning problem which allows us to provide regularity
results for the solution network, based on Optimal Transport theory. From a
practical viewpoint, this allows us to propose a new learning algorithm, which
automatically adapts to the complexity of the given task, and leads to networks
with a high generalization ability even in low data regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Karkar_S/0/1/0/all/0/1"&gt;Skander Karkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ibrahim Ayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bezenac_E/0/1/0/all/0/1"&gt;Emmanuel de B&amp;#xe9;zenac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gallinari_P/0/1/0/all/0/1"&gt;Patrick Gallinari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phase Transitions, Distance Functions, and Implicit Neural Representations. (arXiv:2106.07689v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07689</id>
        <link href="http://arxiv.org/abs/2106.07689"/>
        <updated>2021-06-16T01:21:07.502Z</updated>
        <summary type="html"><![CDATA[Representing surfaces as zero level sets of neural networks recently emerged
as a powerful modeling paradigm, named Implicit Neural Representations (INRs),
serving numerous downstream applications in geometric deep learning and 3D
vision. Training INRs previously required choosing between occupancy and
distance function representation and different losses with unknown limit
behavior and/or bias. In this paper we draw inspiration from the theory of
phase transitions of fluids and suggest a loss for training INRs that learns a
density function that converges to a proper occupancy function, while its log
transform converges to a distance function. Furthermore, we analyze the limit
minimizer of this loss showing it satisfies the reconstruction constraints and
has minimal surface perimeter, a desirable inductive bias for surface
reconstruction. Training INRs with this new loss leads to state-of-the-art
reconstructions on a standard benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1"&gt;Yaron Lipman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast decentralized non-convex finite-sum optimization with recursive variance reduction. (arXiv:2008.07428v5 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07428</id>
        <link href="http://arxiv.org/abs/2008.07428"/>
        <updated>2021-06-16T01:21:07.488Z</updated>
        <summary type="html"><![CDATA[This paper considers decentralized minimization of $N:=nm$ smooth non-convex
cost functions equally divided over a directed network of $n$ nodes.
Specifically, we describe a stochastic first-order gradient method, called
GT-SARAH, that employs a SARAH-type variance reduction technique and gradient
tracking (GT) to address the stochastic and decentralized nature of the
problem. We show that GT-SARAH, with appropriate algorithmic parameters, finds
an $\epsilon$-accurate first-order stationary point with
$O\big(\max\big\{N^{\frac{1}{2}},n(1-\lambda)^{-2},n^{\frac{2}{3}}m^{\frac{1}{3}}(1-\lambda)^{-1}\big\}L\epsilon^{-2}\big)$
gradient complexity, where ${(1-\lambda)\in(0,1]}$ is the spectral gap of the
network weight matrix and $L$ is the smoothness parameter of the cost
functions. This gradient complexity outperforms that of the existing
decentralized stochastic gradient methods. In particular, in a big-data regime
such that ${n = O(N^{\frac{1}{2}}(1-\lambda)^{3})}$, this gradient complexity
furthers reduces to ${O(N^{\frac{1}{2}}L\epsilon^{-2})}$, independent of the
network topology, and matches that of the centralized near-optimal
variance-reduced methods. Moreover, in this regime GT-SARAH achieves a
non-asymptotic linear speedup, in that, the total number of gradient
computations at each node is reduced by a factor of $1/n$ compared to the
centralized near-optimal algorithms that perform all gradient computations at a
single node. To the best of our knowledge, GT-SARAH is the first algorithm that
achieves this property. In addition, we show that appropriate choices of local
minibatch size balance the trade-offs between the gradient and communication
complexity of GT-SARAH. Over infinite time horizon, we establish that all nodes
in GT-SARAH asymptotically achieve consensus and converge to a first-order
stationary point in the almost sure and mean-squared sense.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Xin_R/0/1/0/all/0/1"&gt;Ran Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Khan_U/0/1/0/all/0/1"&gt;Usman A. Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kar_S/0/1/0/all/0/1"&gt;Soummya Kar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Potato Crop Stress Identification in Aerial Images using Deep Learning-based Object Detection. (arXiv:2106.07770v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07770</id>
        <link href="http://arxiv.org/abs/2106.07770"/>
        <updated>2021-06-16T01:21:07.474Z</updated>
        <summary type="html"><![CDATA[Recent research on the application of remote sensing and deep learning-based
analysis in precision agriculture demonstrated a potential for improved crop
management and reduced environmental impacts of agricultural production.
Despite the promising results, the practical relevance of these technologies
for actual field deployment requires novel algorithms that are customized for
analysis of agricultural images and robust to implementation on natural field
imagery. The paper presents an approach for analyzing aerial images of a potato
crop using deep neural networks. The main objective is to demonstrate automated
spatial recognition of a healthy versus stressed crop at a plant level.
Specifically, we examine premature plant senescence resulting in drought stress
on Russet Burbank potato plants. The proposed deep learning model, named
Retina-UNet-Ag, is a variant of Retina-UNet (Jaeger et al., 2018) and includes
connections from low-level semantic dense representation maps to the feature
pyramid network. The paper also introduces a dataset of field images acquired
with a Parrot Sequoia camera carried by a Solo unmanned aerial vehicle.
Experimental validation demonstrated the ability for distinguishing healthy and
stressed plants in field images, achieving an average Dice score coefficient of
0.74. A comparison to related state-of-the-art deep learning models for object
detection revealed that the presented approach is effective for the task at
hand. The method applied here is conducive toward the assessment and
recognition of potato crop stress (early plant senescence resulting from
drought stress in this case) in natural aerial field images collected under
real conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Butte_S/0/1/0/all/0/1"&gt;Sujata Butte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakanski_A/0/1/0/all/0/1"&gt;Aleksandar Vakanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duellman_K/0/1/0/all/0/1"&gt;Kasia Duellman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirkouei_A/0/1/0/all/0/1"&gt;Amin Mirkouei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-Informed Neural Network for Modelling the Thermochemical Curing Process of Composite-Tool Systems During Manufacture. (arXiv:2011.13511v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13511</id>
        <link href="http://arxiv.org/abs/2011.13511"/>
        <updated>2021-06-16T01:21:07.467Z</updated>
        <summary type="html"><![CDATA[We present a Physics-Informed Neural Network (PINN) to simulate the
thermochemical evolution of a composite material on a tool undergoing cure in
an autoclave. In particular, we solve the governing coupled system of
differential equations -- including conductive heat transfer and resin cure
kinetics -- by optimizing the parameters of a deep neural network (DNN) using a
physics-based loss function. To account for the vastly different behaviour of
thermal conduction and resin cure, we design a PINN consisting of two
disconnected subnetworks, and develop a sequential training algorithm that
mitigates instability present in traditional training methods. Further, we
incorporate explicit discontinuities into the DNN at the composite-tool
interface and enforce known physical behaviour directly in the loss function to
improve the solution near the interface. We train the PINN with a technique
that automatically adapts the weights on the loss terms corresponding to PDE,
boundary, interface, and initial conditions. Finally, we demonstrate that one
can include problem parameters as an input to the model -- resulting in a
surrogate that provides real-time simulation for a range of problem settings --
and that one can use transfer learning to significantly reduce the training
time for problem settings similar to that of an initial trained model. The
performance of the proposed PINN is demonstrated in multiple scenarios with
different material thicknesses and thermal boundary conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niaki_S/0/1/0/all/0/1"&gt;Sina Amini Niaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haghighat_E/0/1/0/all/0/1"&gt;Ehsan Haghighat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campbell_T/0/1/0/all/0/1"&gt;Trevor Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poursartip_A/0/1/0/all/0/1"&gt;Anoush Poursartip&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaziri_R/0/1/0/all/0/1"&gt;Reza Vaziri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Audio-Visual Dereverberation. (arXiv:2106.07732v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.07732</id>
        <link href="http://arxiv.org/abs/2106.07732"/>
        <updated>2021-06-16T01:21:07.443Z</updated>
        <summary type="html"><![CDATA[Reverberation from audio reflecting off surfaces and objects in the
environment not only degrades the quality of speech for human perception, but
also severely impacts the accuracy of automatic speech recognition. Prior work
attempts to remove reverberation based on the audio modality only. Our idea is
to learn to dereverberate speech from audio-visual observations. The visual
environment surrounding a human speaker reveals important cues about the room
geometry, materials, and speaker location, all of which influence the precise
reverberation effects in the audio stream. We introduce Visually-Informed
Dereverberation of Audio (VIDA), an end-to-end approach that learns to remove
reverberation based on both the observed sounds and visual scene. In support of
this new task, we develop a large-scale dataset that uses realistic acoustic
renderings of speech in real-world 3D scans of homes offering a variety of room
acoustics. Demonstrating our approach on both simulated and real imagery for
speech enhancement, speech recognition, and speaker identification, we show it
achieves state-of-the-art performance and substantially improves over
traditional audio-only methods. Project page:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1"&gt;David Harwath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Training Data from Large Language Models. (arXiv:2012.07805v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07805</id>
        <link href="http://arxiv.org/abs/2012.07805"/>
        <updated>2021-06-16T01:21:07.435Z</updated>
        <summary type="html"><![CDATA[It has become common to publish large (billion parameter) language models
that have been trained on private datasets. This paper demonstrates that in
such settings, an adversary can perform a training data extraction attack to
recover individual training examples by querying the language model.

We demonstrate our attack on GPT-2, a language model trained on scrapes of
the public Internet, and are able to extract hundreds of verbatim text
sequences from the model's training data. These extracted examples include
(public) personally identifiable information (names, phone numbers, and email
addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible
even though each of the above sequences are included in just one document in
the training data.

We comprehensively evaluate our extraction attack to understand the factors
that contribute to its success. Worryingly, we find that larger models are more
vulnerable than smaller models. We conclude by drawing lessons and discussing
possible safeguards for training large language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1"&gt;Florian Tramer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1"&gt;Eric Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jagielski_M/0/1/0/all/0/1"&gt;Matthew Jagielski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herbert_Voss_A/0/1/0/all/0/1"&gt;Ariel Herbert-Voss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Katherine Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1"&gt;Adam Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1"&gt;Tom Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erlingsson_U/0/1/0/all/0/1"&gt;Ulfar Erlingsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oprea_A/0/1/0/all/0/1"&gt;Alina Oprea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1"&gt;Colin Raffel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Software Engineering Perspective on Engineering Machine Learning Systems: State of the Art and Challenges. (arXiv:2012.07919v3 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07919</id>
        <link href="http://arxiv.org/abs/2012.07919"/>
        <updated>2021-06-16T01:21:07.427Z</updated>
        <summary type="html"><![CDATA[Context: Advancements in machine learning (ML) lead to a shift from the
traditional view of software development, where algorithms are hard-coded by
humans, to ML systems materialized through learning from data. Therefore, we
need to revisit our ways of developing software systems and consider the
particularities required by these new types of systems. Objective: The purpose
of this study is to systematically identify, analyze, summarize, and synthesize
the current state of software engineering (SE) research for engineering ML
systems. Method: I performed a systematic literature review (SLR). I
systematically selected a pool of 141 studies from SE venues and then conducted
a quantitative and qualitative analysis using the data extracted from these
studies. Results: The non-deterministic nature of ML systems complicates all SE
aspects of engineering ML systems. Despite increasing interest from 2018
onwards, the results reveal that none of the SE aspects have a mature set of
tools and techniques. Testing is by far the most popular area among
researchers. Even for testing ML systems, engineers have only some tool
prototypes and solution proposals with weak experimental proof. Many of the
challenges of ML systems engineering were identified through surveys and
interviews. Researchers should conduct experiments and case studies, ideally in
industrial environments, to further understand these challenges and propose
solutions. Conclusion: The results may benefit (1) practitioners in foreseeing
the challenges of ML systems engineering; (2) researchers and academicians in
identifying potential research questions; and (3) educators in designing or
updating SE courses to cover ML systems engineering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giray_G/0/1/0/all/0/1"&gt;G&amp;#xf6;rkem Giray&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Biomedical Entity Linking with Contrastive Context Matching. (arXiv:2106.07583v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07583</id>
        <link href="http://arxiv.org/abs/2106.07583"/>
        <updated>2021-06-16T01:21:07.402Z</updated>
        <summary type="html"><![CDATA[We introduce BioCoM, a contrastive learning framework for biomedical entity
linking that uses only two resources: a small-sized dictionary and a large
number of raw biomedical articles. Specifically, we build the training
instances from raw PubMed articles by dictionary matching and use them to train
a context-aware entity linking model with contrastive learning. We predict the
normalized biomedical entity at inference time through a nearest-neighbor
search. Results found that BioCoM substantially outperforms state-of-the-art
models, especially in low-resource settings, by effectively using the context
of the entities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ujiie_S/0/1/0/all/0/1"&gt;Shogo Ujiie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1"&gt;Hayate Iso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aramaki_E/0/1/0/all/0/1"&gt;Eiji Aramaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-Trained Models: Past, Present and Future. (arXiv:2106.07139v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07139</id>
        <link href="http://arxiv.org/abs/2106.07139"/>
        <updated>2021-06-16T01:21:07.396Z</updated>
        <summary type="html"><![CDATA[Large-scale pre-trained models (PTMs) such as BERT and GPT have recently
achieved great success and become a milestone in the field of artificial
intelligence (AI). Owing to sophisticated pre-training objectives and huge
model parameters, large-scale PTMs can effectively capture knowledge from
massive labeled and unlabeled data. By storing knowledge into huge parameters
and fine-tuning on specific tasks, the rich knowledge implicitly encoded in
huge parameters can benefit a variety of downstream tasks, which has been
extensively demonstrated via experimental verification and empirical analysis.
It is now the consensus of the AI community to adopt PTMs as backbone for
downstream tasks rather than learning models from scratch. In this paper, we
take a deep look into the history of pre-training, especially its special
relation with transfer learning and self-supervised learning, to reveal the
crucial position of PTMs in the AI development spectrum. Further, we
comprehensively review the latest breakthroughs of PTMs. These breakthroughs
are driven by the surge of computational power and the increasing availability
of data, towards four important directions: designing effective architectures,
utilizing rich contexts, improving computational efficiency, and conducting
interpretation and theoretical analysis. Finally, we discuss a series of open
problems and research directions of PTMs, and hope our view can inspire and
advance the future study of PTMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1"&gt;Ning Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yuxian Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuqi Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1"&gt;Jiezhong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1"&gt;Wentao Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yanyan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhiwu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Ruihua Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jinhui Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Language Adversarial Defense through Synonym Encoding. (arXiv:1909.06723v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.06723</id>
        <link href="http://arxiv.org/abs/1909.06723"/>
        <updated>2021-06-16T01:21:07.353Z</updated>
        <summary type="html"><![CDATA[In the area of natural language processing, deep learning models are recently
known to be vulnerable to various types of adversarial perturbations, but
relatively few works are done on the defense side. Especially, there exists few
effective defense method against the successful synonym substitution based
attacks that preserve the syntactic structure and semantic information of the
original text while fooling the deep learning models. We contribute in this
direction and propose a novel adversarial defense method called Synonym
Encoding Method (SEM). Specifically, SEM inserts an encoder before the input
layer of the target model to map each cluster of synonyms to a unique encoding
and trains the model to eliminate possible adversarial perturbations without
modifying the network architecture or adding extra data. Extensive experiments
demonstrate that SEM can effectively defend the current synonym substitution
based attacks and block the transferability of adversarial examples. SEM is
also easy and efficient to scale to large models and big datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaosen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yichen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1"&gt;Kun He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders. (arXiv:2105.05752v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05752</id>
        <link href="http://arxiv.org/abs/2105.05752"/>
        <updated>2021-06-16T01:21:07.328Z</updated>
        <summary type="html"><![CDATA[Encoder pre-training is promising in end-to-end Speech Translation (ST),
given the fact that speech-to-translation data is scarce. But ST encoders are
not simple instances of Automatic Speech Recognition (ASR) or Machine
Translation (MT) encoders. For example, we find that ASR encoders lack the
global context representation, which is necessary for translation, whereas MT
encoders are not designed to deal with long but locally attentive acoustic
sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding
(SATE) method for speech translation. Our encoder begins with processing the
acoustic sequence as usual, but later behaves more like an MT encoder for a
global representation of the input sequence. In this way, it is straightforward
to incorporate the pre-trained models into the system. Also, we develop an
adaptor module to alleviate the representation inconsistency between the
pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge
distillation method to preserve the pre-training knowledge. Experimental
results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method
achieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we
are the first to develop an end-to-end ST system that achieves comparable or
even better BLEU performance than the cascaded ST counterpart when large-scale
ASR and MT data is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1"&gt;Bojie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+huang_s/0/1/0/all/0/1"&gt;shen huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_Q/0/1/0/all/0/1"&gt;Qi Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1"&gt;Tong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jingbo Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Determinantal Beam Search. (arXiv:2106.07400v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07400</id>
        <link href="http://arxiv.org/abs/2106.07400"/>
        <updated>2021-06-16T01:21:07.284Z</updated>
        <summary type="html"><![CDATA[Beam search is a go-to strategy for decoding neural sequence models. The
algorithm can naturally be viewed as a subset optimization problem, albeit one
where the corresponding set function does not reflect interactions between
candidates. Empirically, this leads to sets often exhibiting high overlap,
e.g., strings may differ by only a single word. Yet in use-cases that call for
multiple solutions, a diverse or representative set is often desired. To
address this issue, we propose a reformulation of beam search, which we call
determinantal beam search. Determinantal beam search has a natural relationship
to determinantal point processes (DPPs), models over sets that inherently
encode intra-set interactions. By posing iterations in beam search as a series
of subdeterminant maximization problems, we can turn the algorithm into a
diverse subset selection process. In a case study, we use the string
subsequence kernel to explicitly encourage n-gram coverage in text generated
from a sequence model. We observe that our algorithm offers competitive
performance against other diverse set generation strategies in the context of
language generation, while providing a more general approach to optimizing for
diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1"&gt;Clara Meister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forster_M/0/1/0/all/0/1"&gt;Martina Forster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alternate Endings: Improving Prosody for Incremental Neural TTS with Predicted Future Text Input. (arXiv:2102.09914v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09914</id>
        <link href="http://arxiv.org/abs/2102.09914"/>
        <updated>2021-06-16T01:21:07.267Z</updated>
        <summary type="html"><![CDATA[The prosody of a spoken word is determined by its surrounding context. In
incremental text-to-speech synthesis, where the synthesizer produces an output
before it has access to the complete input, the full context is often unknown
which can result in a loss of naturalness in the synthesized speech. In this
paper, we investigate whether the use of predicted future text can attenuate
this loss. We compare several test conditions of next future word: (a) unknown
(zero-word), (b) language model predicted, (c) randomly predicted and (d)
ground-truth. We measure the prosodic features (pitch, energy and duration) and
find that predicted text provides significant improvements over a zero-word
lookahead, but only slight gains over random-word lookahead. We confirm these
results with a perceptive test.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stephenson_B/0/1/0/all/0/1"&gt;Brooke Stephenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hueber_T/0/1/0/all/0/1"&gt;Thomas Hueber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girin_L/0/1/0/all/0/1"&gt;Laurent Girin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1"&gt;Laurent Besacier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deriving Word Vectors from Contextualized Language Models using Topic-Aware Mention Selection. (arXiv:2106.07947v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07947</id>
        <link href="http://arxiv.org/abs/2106.07947"/>
        <updated>2021-06-16T01:21:07.233Z</updated>
        <summary type="html"><![CDATA[One of the long-standing challenges in lexical semantics consists in learning
representations of words which reflect their semantic properties. The
remarkable success of word embeddings for this purpose suggests that
high-quality representations can be obtained by summarizing the sentence
contexts of word mentions. In this paper, we propose a method for learning word
representations that follows this basic strategy, but differs from standard
word embeddings in two important ways. First, we take advantage of
contextualized language models (CLMs) rather than bags of word vectors to
encode contexts. Second, rather than learning a word vector directly, we use a
topic model to partition the contexts in which words appear, and then learn
different topic-specific vectors for each word. Finally, we use a task-specific
supervision signal to make a soft selection of the resulting vectors. We show
that this simple strategy leads to high-quality word vectors, which are more
predictive of semantic properties than word embeddings and existing CLM-based
strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouraoui_Z/0/1/0/all/0/1"&gt;Zied Bouraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anke_L/0/1/0/all/0/1"&gt;Luis Espinosa Anke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1"&gt;Steven Schockaert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Syntax and Semantics in the Brain with Deep Networks. (arXiv:2103.01620v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01620</id>
        <link href="http://arxiv.org/abs/2103.01620"/>
        <updated>2021-06-16T01:21:07.206Z</updated>
        <summary type="html"><![CDATA[The activations of language transformers like GPT-2 have been shown to
linearly map onto brain activity during speech comprehension. However, the
nature of these activations remains largely unknown and presumably conflate
distinct linguistic classes. Here, we propose a taxonomy to factorize the
high-dimensional activations of language models into four combinatorial
classes: lexical, compositional, syntactic, and semantic representations. We
then introduce a statistical method to decompose, through the lens of GPT-2's
activations, the brain activity of 345 subjects recorded with functional
magnetic resonance imaging (fMRI) during the listening of ~4.6 hours of
narrated text. The results highlight two findings. First, compositional
representations recruit a more widespread cortical network than lexical ones,
and encompass the bilateral temporal, parietal and prefrontal cortices. Second,
contrary to previous claims, syntax and semantics are not associated with
separated modules, but, instead, appear to share a common and distributed
neural substrate. Overall, this study introduces a versatile framework to
isolate, in the brain activity, the distributed representations of linguistic
constructs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caucheteux_C/0/1/0/all/0/1"&gt;Charlotte Caucheteux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gramfort_A/0/1/0/all/0/1"&gt;Alexandre Gramfort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_J/0/1/0/all/0/1"&gt;Jean-Remi King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topics to Avoid: Demoting Latent Confounds in Text Classification. (arXiv:1909.00453v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.00453</id>
        <link href="http://arxiv.org/abs/1909.00453"/>
        <updated>2021-06-16T01:21:07.198Z</updated>
        <summary type="html"><![CDATA[Despite impressive performance on many text classification tasks, deep neural
networks tend to learn frequent superficial patterns that are specific to the
training data and do not always generalize well. In this work, we observe this
limitation with respect to the task of native language identification. We find
that standard text classifiers which perform well on the test set end up
learning topical features which are confounds of the prediction task (e.g., if
the input text mentions Sweden, the classifier predicts that the author's
native language is Swedish). We propose a method that represents the latent
topical confounds and a model which "unlearns" confounding features by
predicting both the label of the input text and the confound; but we train the
two predictors adversarially in an alternating fashion to learn a text
representation that predicts the correct label but is less prone to using
information about the confound. We show that this model generalizes better and
learns features that are indicative of the writing style rather than the
content.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sachin Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wintner_S/0/1/0/all/0/1"&gt;Shuly Wintner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Multilingual TEDx Corpus for Speech Recognition and Translation. (arXiv:2102.01757v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01757</id>
        <link href="http://arxiv.org/abs/2102.01757"/>
        <updated>2021-06-16T01:21:07.191Z</updated>
        <summary type="html"><![CDATA[We present the Multilingual TEDx corpus, built to support speech recognition
(ASR) and speech translation (ST) research across many non-English source
languages. The corpus is a collection of audio recordings from TEDx talks in 8
source languages. We segment transcripts into sentences and align them to the
source-language audio and target-language translations. The corpus is released
along with open-sourced code enabling extension to new talks and languages as
they become available. Our corpus creation methodology can be applied to more
languages than previous work, and creates multi-way parallel evaluation sets.
We provide baselines in multiple ASR and ST settings, including multilingual
models to improve translation performance for low-resource language pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1"&gt;Elizabeth Salesky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiesner_M/0/1/0/all/0/1"&gt;Matthew Wiesner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bremerman_J/0/1/0/all/0/1"&gt;Jacob Bremerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cattoni_R/0/1/0/all/0/1"&gt;Roldano Cattoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1"&gt;Matteo Negri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1"&gt;Marco Turchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1"&gt;Douglas W. Oard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1"&gt;Matt Post&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reasoning Over Virtual Knowledge Bases With Open Predicate Relations. (arXiv:2102.07043v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07043</id>
        <link href="http://arxiv.org/abs/2102.07043"/>
        <updated>2021-06-16T01:21:07.184Z</updated>
        <summary type="html"><![CDATA[We present the Open Predicate Query Language (OPQL); a method for
constructing a virtual KB (VKB) trained entirely from text. Large Knowledge
Bases (KBs) are indispensable for a wide-range of industry applications such as
question answering and recommendation. Typically, KBs encode world knowledge in
a structured, readily accessible form derived from laborious human annotation
efforts. Unfortunately, while they are extremely high precision, KBs are
inevitably highly incomplete and automated methods for enriching them are far
too inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set
of relation mentions in a way that naturally enables reasoning and can be
trained without any structured supervision. We demonstrate that OPQL
outperforms prior VKB methods on two different KB reasoning tasks and,
additionally, can be used as an external memory integrated into a language
model (OPQL-LM) leading to improvements on two open-domain question answering
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haitian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1"&gt;Pat Verga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1"&gt;Bhuwan Dhingra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"&gt;William W. Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling morphology with Linear Discriminative Learning: considerations and design choices. (arXiv:2106.07936v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07936</id>
        <link href="http://arxiv.org/abs/2106.07936"/>
        <updated>2021-06-16T01:21:07.178Z</updated>
        <summary type="html"><![CDATA[This study addresses a series of methodological questions that arise when
modeling inflectional morphology with Linear Discriminative Learning. Taking
the semi-productive German noun system as example, we illustrate how decisions
made about the representation of form and meaning influence model performance.
We clarify that for modeling frequency effects in learning, it is essential to
make use of incremental learning rather than the endstate of learning. We also
discuss how the model can be set up to approximate the learning of inflected
words in context. In addition, we illustrate how in this approach the wug task
can be modeled in considerable detail. In general, the model provides an
excellent memory for known words, but appropriately shows more limited
performance for unseen data, in line with the semi-productivity of German noun
inflection and generalization performance of native German speakers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heitmeier_M/0/1/0/all/0/1"&gt;Maria Heitmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1"&gt;Yu-Ying Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baayen_R/0/1/0/all/0/1"&gt;R. Harald Baayen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSMix: Saliency-Based Span Mixup for Text Classification. (arXiv:2106.08062v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08062</id>
        <link href="http://arxiv.org/abs/2106.08062"/>
        <updated>2021-06-16T01:21:07.158Z</updated>
        <summary type="html"><![CDATA[Data augmentation with mixup has shown to be effective on various computer
vision tasks. Despite its great success, there has been a hurdle to apply mixup
to NLP tasks since text consists of discrete tokens with variable length. In
this work, we propose SSMix, a novel mixup method where the operation is
performed on input text rather than on hidden vectors like previous approaches.
SSMix synthesizes a sentence while preserving the locality of two original
texts by span-based mixing and keeping more tokens related to the prediction
relying on saliency information. With extensive experiments, we empirically
validate that our method outperforms hidden-level mixup methods on a wide range
of text classification benchmarks, including textual entailment, sentiment
classification, and question-type classification. Our code is available at
https://github.com/clovaai/ssmix.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Soyoung Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gyuwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyumin Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialectal Speech Recognition and Translation of Swiss German Speech to Standard German Text: Microsoft's Submission to SwissText 2021. (arXiv:2106.08126v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.08126</id>
        <link href="http://arxiv.org/abs/2106.08126"/>
        <updated>2021-06-16T01:21:07.152Z</updated>
        <summary type="html"><![CDATA[This paper describes the winning approach in the public SwissText 2021
competition on dialect recognition and translation of Swiss German speech to
standard German text. Swiss German refers to the multitude of Alemannic
dialects spoken in the German-speaking parts of Switzerland. Swiss German
differs significantly from standard German in pronunciation, word inventory and
grammar. It is mostly incomprehensible to native German speakers. Moreover, it
lacks a standardized written script. To solve the challenging task, we propose
a hybrid automatic speech recognition system with a lexicon that incorporates
translations, a 1st pass language model that deals with Swiss German
particularities, a transfer-learned acoustic model and a strong neural language
model for 2nd pass rescoring. Our submission reaches 46.04% BLEU on a blind
conversational test set and outperforms the second best competitor by a 12%
relative margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Arabskyy_Y/0/1/0/all/0/1"&gt;Yuriy Arabskyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Aashish Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dey_S/0/1/0/all/0/1"&gt;Subhadeep Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Koller_O/0/1/0/all/0/1"&gt;Oscar Koller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10005</id>
        <link href="http://arxiv.org/abs/2105.10005"/>
        <updated>2021-06-16T01:21:07.143Z</updated>
        <summary type="html"><![CDATA[Physical processes, camera movement, and unpredictable environmental
conditions like the presence of dust can induce noise and artifacts in video
feeds. We observe that popular unsupervised MOT methods are dependent on
noise-free inputs. We show that the addition of a small amount of artificial
random noise causes a sharp degradation in model performance on benchmark
metrics. We resolve this problem by introducing a robust unsupervised
multi-object tracking (MOT) model: AttU-Net. The proposed single-head attention
model helps limit the negative impact of noise by learning visual
representations at different segment scales. AttU-Net shows better unsupervised
MOT tracking performance over variational inference-based state-of-the-art
baselines. We evaluate our method in the MNIST-MOT and the Atari game video
benchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''
which consists of moving Japanese characters and ``Fashion-MNIST MOT'' to
validate the effectiveness of the MOT models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;C.-H. Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1"&gt;Mohit Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Y.-C. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quan Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1"&gt;Tomoaki Yoshinaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1"&gt;Tomokazu Murakami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivalence of Segmental and Neural Transducer Modeling: A Proof of Concept. (arXiv:2104.06104v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06104</id>
        <link href="http://arxiv.org/abs/2104.06104"/>
        <updated>2021-06-16T01:21:07.137Z</updated>
        <summary type="html"><![CDATA[With the advent of direct models in automatic speech recognition (ASR), the
formerly prevalent frame-wise acoustic modeling based on hidden Markov models
(HMM) diversified into a number of modeling architectures like encoder-decoder
attention models, transducer models and segmental models (direct HMM). While
transducer models stay with a frame-level model definition, segmental models
are defined on the level of label segments directly. While
(soft-)attention-based models avoid explicit alignment, transducer and
segmental approach internally do model alignment, either by segment hypotheses
or, more implicitly, by emitting so-called blank symbols. In this work, we
prove that the widely used class of RNN-Transducer models and segmental models
(direct HMM) are equivalent and therefore show equal modeling power. It is
shown that blank probabilities translate into segment length probabilities and
vice versa. In addition, we provide initial experiments investigating decoding
and beam-pruning, comparing time-synchronous and label-/segment-synchronous
search strategies and their properties using the same underlying model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1"&gt;Albert Zeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merboldt_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Merboldt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Self-supervised Multi-task Learning for COVID-19 Information Retrieval and Extraction. (arXiv:2106.08252v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08252</id>
        <link href="http://arxiv.org/abs/2106.08252"/>
        <updated>2021-06-16T01:21:07.129Z</updated>
        <summary type="html"><![CDATA[The rapidly evolving literature of COVID-19 related articles makes it
challenging for NLP models to be effectively trained for information retrieval
and extraction with the corresponding labeled data that follows the current
distribution of the pandemic. On the other hand, due to the uncertainty of the
situation, human experts' supervision would always be required to double check
the decision making of these models highlighting the importance of
interpretability. In the light of these challenges, this study proposes an
interpretable self-supervised multi-task learning model to jointly and
effectively tackle the tasks of information retrieval (IR) and extraction (IE)
during the current emergency health crisis situation. Our results show that our
model effectively leverage the multi-task and self-supervised learning to
improve generalization, data efficiency and robustness to the ongoing dataset
shift problem. Our model outperforms baselines in IE and IR tasks, respectively
by micro-f score of 0.08 (LCA-F score of 0.05), and MAP of 0.05 on average. In
IE the zero- and few-shot learning performances are on average 0.32 and 0.19
micro-f score higher than those of the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ebadi_N/0/1/0/all/0/1"&gt;Nima Ebadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1"&gt;Peyman Najafirad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Margin Circle Loss for Speaker Verification. (arXiv:2106.08004v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08004</id>
        <link href="http://arxiv.org/abs/2106.08004"/>
        <updated>2021-06-16T01:21:07.110Z</updated>
        <summary type="html"><![CDATA[Deep-Neural-Network (DNN) based speaker verification sys-tems use the angular
softmax loss with margin penalties toenhance the intra-class compactness of
speaker embeddings,which achieved remarkable performance. In this paper, we
pro-pose a novel angular loss function called adaptive margin cir-cle loss for
speaker verification. The stage-based margin andchunk-based margin are applied
to improve the angular discrim-ination of circle loss on the training set. The
analysis on gradi-ents shows that, compared with the previous angular loss
likeAdditive Margin Softmax(Am-Softmax), circle loss has flexi-ble optimization
and definite convergence status. Experimentsare carried out on the Voxceleb and
SITW. By applying adap-tive margin circle loss, our best system achieves
1.31%EER onVoxceleb1 and 2.13% on SITW core-core.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1"&gt;Runqiu Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Transformers. (arXiv:2106.04554v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04554</id>
        <link href="http://arxiv.org/abs/2106.04554"/>
        <updated>2021-06-16T01:21:07.104Z</updated>
        <summary type="html"><![CDATA[Transformers have achieved great success in many artificial intelligence
fields, such as natural language processing, computer vision, and audio
processing. Therefore, it is natural to attract lots of interest from academic
and industry researchers. Up to the present, a great variety of Transformer
variants (a.k.a. X-formers) have been proposed, however, a systematic and
comprehensive literature review on these Transformer variants is still missing.
In this survey, we provide a comprehensive review of various X-formers. We
first briefly introduce the vanilla Transformer and then propose a new taxonomy
of X-formers. Next, we introduce the various X-formers from three perspectives:
architectural modification, pre-training, and applications. Finally, we outline
some potential directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PairConnect: A Compute-Efficient MLP Alternative to Attention. (arXiv:2106.08235v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08235</id>
        <link href="http://arxiv.org/abs/2106.08235"/>
        <updated>2021-06-16T01:21:07.098Z</updated>
        <summary type="html"><![CDATA[Transformer models have demonstrated superior performance in natural language
processing. The dot product self-attention in Transformer allows us to model
interactions between words. However, this modeling comes with significant
computational overhead. In this work, we revisit the memory-compute trade-off
associated with Transformer, particularly multi-head attention, and show a
memory-heavy but significantly more compute-efficient alternative to
Transformer. Our proposal, denoted as PairConnect, a multilayer perceptron
(MLP), models the pairwise interaction between words by explicit pairwise word
embeddings. As a result, PairConnect substitutes self dot product with a simple
embedding lookup. We show mathematically that despite being an MLP, our
compute-efficient PairConnect is strictly more expressive than Transformer. Our
experiment on language modeling tasks suggests that PairConnect could achieve
comparable results with Transformer while reducing the computational cost
associated with inference significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhaozhuo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Minghao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Spanning Trees Are Invariant to Temperature Scaling in Graph-based Dependency Parsing. (arXiv:2106.08159v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08159</id>
        <link href="http://arxiv.org/abs/2106.08159"/>
        <updated>2021-06-16T01:21:07.091Z</updated>
        <summary type="html"><![CDATA[Modern graph-based syntactic dependency parsers operate by predicting, for
each token within a sentence, a probability distribution over its possible
syntactic heads (i.e., all other tokens) and then extracting a maximum spanning
tree from the resulting log-probabilities. Nowadays, virtually all such parsers
utilize deep neural networks and may thus be susceptible to miscalibration (in
particular, overconfident predictions). In this paper, we prove that
temperature scaling, a popular technique for post-hoc calibration of neural
networks, cannot change the output of the aforementioned procedure. We conclude
that other techniques are needed to tackle miscalibration in graph-based
dependency parsers in a way that improves parsing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grunewald_S/0/1/0/all/0/1"&gt;Stefan Gr&amp;#xfc;newald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Three-part diachronic semantic change dataset for Russian. (arXiv:2106.08294v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08294</id>
        <link href="http://arxiv.org/abs/2106.08294"/>
        <updated>2021-06-16T01:21:07.085Z</updated>
        <summary type="html"><![CDATA[We present a manually annotated lexical semantic change dataset for Russian:
RuShiftEval. Its novelty is ensured by a single set of target words annotated
for their diachronic semantic shifts across three time periods, while the
previous work either used only two time periods, or different sets of target
words. The paper describes the composition and annotation procedure for the
dataset. In addition, it is shown how the ternary nature of RuShiftEval allows
to trace specific diachronic trajectories: `changed at a particular time period
and stable afterwards' or `was changing throughout all time periods'. Based on
the analysis of the submissions to the recent shared task on semantic change
detection for Russian, we argue that correctly identifying such trajectories
can be an interesting sub-task itself.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kutuzov_A/0/1/0/all/0/1"&gt;Andrey Kutuzov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pivovarova_L/0/1/0/all/0/1"&gt;Lidia Pivovarova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark. (arXiv:2106.08087v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08087</id>
        <link href="http://arxiv.org/abs/2106.08087"/>
        <updated>2021-06-16T01:21:07.078Z</updated>
        <summary type="html"><![CDATA[Artificial Intelligence (AI), along with the recent progress in biomedical
language understanding, is gradually changing medical practice. With the
development of biomedical language understanding benchmarks, AI applications
are widely used in the medical field. However, most benchmarks are limited to
English, which makes it challenging to replicate many of the successes in
English for other languages. To facilitate research in this direction, we
collect real-world biomedical data and present the first Chinese Biomedical
Language Understanding Evaluation (CBLUE) benchmark: a collection of natural
language understanding tasks including named entity recognition, information
extraction, clinical diagnosis normalization, single-sentence/sentence-pair
classification, and an associated online platform for model evaluation,
comparison, and analysis. To establish evaluation on these tasks, we report
empirical results with the current 11 pre-trained Chinese models, and
experimental results show that state-of-the-art neural models perform by far
worse than the human ceiling. Our benchmark is released at
\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&lang=en-us}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1"&gt;Zhen Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaozhuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Luoqiu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Hongbin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1"&gt;Xin Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1"&gt;Kangping Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chuanqi Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mosha Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Luo Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1"&gt;Yuan Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1"&gt;Guotong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1"&gt;Zhifang Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zong_H/0/1/0/all/0/1"&gt;Hui Zong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1"&gt;Zheng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linfeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zan_H/0/1/0/all/0/1"&gt;Hongying Zan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kunli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huajun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1"&gt;Buzhou Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qingcai Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direction is what you need: Improving Word Embedding Compression in Large Language Models. (arXiv:2106.08181v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08181</id>
        <link href="http://arxiv.org/abs/2106.08181"/>
        <updated>2021-06-16T01:21:07.059Z</updated>
        <summary type="html"><![CDATA[The adoption of Transformer-based models in natural language processing (NLP)
has led to great success using a massive number of parameters. However, due to
deployment constraints in edge devices, there has been a rising interest in the
compression of these models to improve their inference time and memory
footprint. This paper presents a novel loss objective to compress token
embeddings in the Transformer-based models by leveraging an AutoEncoder
architecture. More specifically, we emphasize the importance of the direction
of compressed embeddings with respect to original uncompressed embeddings. The
proposed method is task-agnostic and does not require further language modeling
pre-training. Our method significantly outperforms the commonly used SVD-based
matrix-factorization approach in terms of initial language model Perplexity.
Moreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several
downstream tasks from the GLUE benchmark, where we also outperform the baseline
in most scenarios. Our code is public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balazy_K/0/1/0/all/0/1"&gt;Klaudia Ba&amp;#x142;azy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banaei_M/0/1/0/all/0/1"&gt;Mohammadreza Banaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lebret_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Lebret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1"&gt;Jacek Tabor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1"&gt;Karl Aberer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08043</id>
        <link href="http://arxiv.org/abs/2106.08043"/>
        <updated>2021-06-16T01:21:07.053Z</updated>
        <summary type="html"><![CDATA[The vast majority of existing methods and systems for causal inference assume
that all variables under consideration are categorical or numerical (e.g.,
gender, price, blood pressure, enrollment). In this paper, we present
CausalNLP, a toolkit for inferring causality from observational data that
includes text in addition to traditional numerical and categorical variables.
CausalNLP employs the use of meta-learners for treatment effect estimation and
supports using raw text and its linguistic properties as both a treatment and a
"controlled-for" variable (e.g., confounder). The library is open-source and
available at: https://github.com/amaiya/causalnlp.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1"&gt;Arun S. Maiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kaizen: Continuously improving teacher using Exponential Moving Average for semi-supervised speech recognition. (arXiv:2106.07759v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.07759</id>
        <link href="http://arxiv.org/abs/2106.07759"/>
        <updated>2021-06-16T01:21:07.045Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce the Kaizen framework that uses a continuously
improving teacher to generate pseudo-labels for semi-supervised training. The
proposed approach uses a teacher model which is updated as the exponential
moving average of the student model parameters. This can be seen as a
continuous version of the iterative pseudo-labeling approach for
semi-supervised training. It is applicable for different training criteria, and
in this paper we demonstrate it for frame-level hybrid hidden Markov model -
deep neural network (HMM-DNN) models and sequence-level connectionist temporal
classification (CTC) based models. The proposed approach shows more than 10%
word error rate (WER) reduction over standard teacher-student training and more
than 50\% relative WER reduction over 10 hour supervised baseline when using
large scale realistic unsupervised public videos in UK English and Italian
languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Manohar_V/0/1/0/all/0/1"&gt;Vimal Manohar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Likhomanenko_T/0/1/0/all/0/1"&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiantong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wei-Ning Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Collobert_R/0/1/0/all/0/1"&gt;Ronan Collobert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saraf_Y/0/1/0/all/0/1"&gt;Yatharth Saraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zweig_G/0/1/0/all/0/1"&gt;Geoffrey Zweig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Abdelrahman Mohamed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval. (arXiv:2104.01894v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01894</id>
        <link href="http://arxiv.org/abs/2104.01894"/>
        <updated>2021-06-16T01:21:07.030Z</updated>
        <summary type="html"><![CDATA[Speech-based image retrieval has been studied as a proxy for joint
representation learning, usually without emphasis on retrieval itself. As such,
it is unclear how well speech-based retrieval can work in practice -- both in
an absolute sense and versus alternative strategies that combine automatic
speech recognition (ASR) with strong text encoders. In this work, we
extensively study and expand choices of encoder architectures, training
methodology (including unimodal and multimodal pretraining), and other factors.
Our experiments cover different types of speech in three datasets: Flickr
Audio, Places Audio, and Localized Narratives. Our best model configuration
achieves large gains over state of the art, e.g., pushing recall-at-one from
21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also
show our best speech-based models can match or exceed cascaded ASR-to-text
encoding when speech is spontaneous, accented, or otherwise hard to
automatically transcribe.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1"&gt;Ramon Sanabria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1"&gt;Austin Waters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1"&gt;Jason Baldridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing. (arXiv:2106.08037v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08037</id>
        <link href="http://arxiv.org/abs/2106.08037"/>
        <updated>2021-06-16T01:21:07.012Z</updated>
        <summary type="html"><![CDATA[Modality is the linguistic ability to describe events with added information
such as how desirable, plausible, or feasible they are. Modality is important
for many NLP downstream tasks such as the detection of hedging, uncertainty,
speculation, and more. Previous studies that address modality detection in NLP
often restrict modal expressions to a closed syntactic class, and the modal
sense labels are vastly different across different studies, lacking an accepted
standard. Furthermore, these senses are often analyzed independently of the
events that they modify. This work builds on the theoretical foundations of the
Georgetown Gradable Modal Expressions (GME) work by Rubinstein et al. (2013) to
propose an event-based modality detection task where modal expressions can be
words of any syntactic class and sense labels are drawn from a comprehensive
taxonomy which harmonizes the modal concepts contributed by the different
studies. We present experiments on the GME corpus aiming to detect and classify
fine-grained modal concepts and associate them with their modified events. We
show that detecting and classifying modal expressions is not only feasible, but
also improves the detection of modal events in their own right.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1"&gt;Valentina Pyatkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadde_S/0/1/0/all/0/1"&gt;Shoval Sadde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1"&gt;Aynat Rubinstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portner_P/0/1/0/all/0/1"&gt;Paul Portner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1"&gt;Reut Tsarfaty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07847</id>
        <link href="http://arxiv.org/abs/2106.07847"/>
        <updated>2021-06-16T01:21:07.005Z</updated>
        <summary type="html"><![CDATA[We study transfer learning in the presence of spurious correlations. We
experimentally demonstrate that directly transferring the stable feature
extractor learned on the source task may not eliminate these biases for the
target task. However, we hypothesize that the unstable features in the source
task and those in the target task are directly related. By explicitly informing
the target classifier of the source task's unstable features, we can regularize
the biases in the target task. Specifically, we derive a representation that
encodes the unstable features by contrasting different data environments in the
source task. On the target task, we cluster data from this representation, and
achieve robustness by minimizing the worst-case risk across all clusters. We
evaluate our method on both text and image classifications. Empirical results
demonstrate that our algorithm is able to maintain robustness on the target
task, outperforming the best baseline by 22.9% in absolute accuracy across 12
transfer settings. Our code is available at https://github.com/YujiaBao/Tofu.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1"&gt;Yujia Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Semantic Guidance and Deep Reinforcement Learning For Generating Human Level Paintings. (arXiv:2011.12589v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12589</id>
        <link href="http://arxiv.org/abs/2011.12589"/>
        <updated>2021-06-16T01:21:06.998Z</updated>
        <summary type="html"><![CDATA[Generation of stroke-based non-photorealistic imagery, is an important
problem in the computer vision community. As an endeavor in this direction,
substantial recent research efforts have been focused on teaching machines "how
to paint", in a manner similar to a human painter. However, the applicability
of previous methods has been limited to datasets with little variation in
position, scale and saliency of the foreground object. As a consequence, we
find that these methods struggle to cover the granularity and diversity
possessed by real world images. To this end, we propose a Semantic Guidance
pipeline with 1) a bi-level painting procedure for learning the distinction
between foreground and background brush strokes at training time. 2) We also
introduce invariance to the position and scale of the foreground object through
a neural alignment model, which combines object localization and spatial
transformer networks in an end to end manner, to zoom into a particular
semantic instance. 3) The distinguishing features of the in-focus object are
then amplified by maximizing a novel guided backpropagation based focus reward.
The proposed agent does not require any supervision on human stroke-data and
successfully handles variations in foreground object attributes, thus,
producing much higher quality canvases for the CUB-200 Birds and Stanford
Cars-196 datasets. Finally, we demonstrate the further efficacy of our method
on complex datasets with multiple foreground object instances by evaluating an
extension of our method on the challenging Virtual-KITTI dataset. Source code
and models are available at https://github.com/1jsingh/semantic-guidance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Jaskirat Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StockBabble: A Conversational Financial Agent to support Stock Market Investors. (arXiv:2106.08298v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.08298</id>
        <link href="http://arxiv.org/abs/2106.08298"/>
        <updated>2021-06-16T01:21:06.989Z</updated>
        <summary type="html"><![CDATA[We introduce StockBabble, a conversational agent designed to support
understanding and engagement with the stock market. StockBabble's value and
novelty is in its ability to empower retail investors -- many of which may be
new to investing -- and supplement their informational needs using a
user-friendly agent. Users have the ability to query information on companies
to retrieve a general and financial overview of a stock, including accessing
the latest news and trading recommendations. They can also request charts which
contain live prices and technical investment indicators, and add shares to a
personal portfolio to allow performance monitoring over time. To evaluate our
agent's potential, we conducted a user study with 15 participants. In total,
73% (11/15) of respondents said that they felt more confident in investing
after using StockBabble, and all 15 would consider recommending it to others.
These results are encouraging and suggest a wider appeal for such agents.
Moreover, we believe this research can help to inform the design and
development of future intelligent, financial personal assistants.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Suraj Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brennan_J/0/1/0/all/0/1"&gt;Joseph Brennan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nurse_J/0/1/0/all/0/1"&gt;Jason R. C. Nurse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Transfer Learning for Brain Magnetic Resonance Image Multi-class Classification. (arXiv:2106.07333v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07333</id>
        <link href="http://arxiv.org/abs/2106.07333"/>
        <updated>2021-06-16T01:21:06.943Z</updated>
        <summary type="html"><![CDATA[Magnetic Resonance Imaging (MRI) is a principal diagnostic approach used in
the field of radiology to create images of the anatomical and physiological
structure of patients. MRI is the prevalent medical imaging practice to find
abnormalities in soft tissues. Traditionally they are analyzed by a radiologist
to detect abnormalities in soft tissues, especially the brain. The process of
interpreting a massive volume of patient's MRI is laborious. Hence, the use of
Machine Learning methodologies can aid in detecting abnormalities in soft
tissues with considerable accuracy. In this research, we have curated a novel
dataset and developed a framework that uses Deep Transfer Learning to perform a
multi-classification of tumors in the brain MRI images. In this paper, we
adopted the Deep Residual Convolutional Neural Network (ResNet50) architecture
for the experiments along with discriminative learning techniques to train the
model. Using the novel dataset and two publicly available MRI brain datasets,
this proposed approach attained a classification accuracy of 86.40% on the
curated dataset, 93.80% on the Harvard Whole Brain Atlas dataset, and 97.05%
accuracy on the School of Biomedical Engineering dataset. Results of our
experiments significantly demonstrate our proposed framework for transfer
learning is a potential and effective method for brain tumor
multi-classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brima_Y/0/1/0/all/0/1"&gt;Yusuf Brima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tushar_M/0/1/0/all/0/1"&gt;Mossadek Hossain Kamal Tushar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabir_U/0/1/0/all/0/1"&gt;Upama Kabir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1"&gt;Tariqul Islam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Automated Quality Evaluation Framework of Psychotherapy Conversations with Local Quality Estimates. (arXiv:2106.07922v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07922</id>
        <link href="http://arxiv.org/abs/2106.07922"/>
        <updated>2021-06-16T01:21:06.932Z</updated>
        <summary type="html"><![CDATA[Computational approaches for assessing the quality of conversation-based
psychotherapy, such as Cognitive Behavioral Therapy (CBT) and Motivational
Interviewing (MI), have been developed recently to support quality assurance
and clinical training. However, due to the long session lengths and limited
modeling resources, computational methods largely rely on frequency-based
lexical features or distribution of dialogue acts. In this work, we propose a
hierarchical framework to automatically evaluate the quality of a CBT
interaction. We divide each psychotherapy session into conversation segments
and input those into a BERT-based model to produce segment embeddings. We first
fine-tune BERT for predicting segment-level (local) quality scores and then use
segment embeddings as lower-level input to a Bidirectional LSTM-based neural
network to predict session-level (global) quality estimates. In particular, the
segment-level quality scores are initialized with the session-level scores and
we model the global quality as a function of the local quality scores to
achieve the accurate segment-level quality estimates. These estimated
segment-level scores benefit theBERT fine-tuning and in learning better segment
embeddings. We evaluate the proposed framework on data drawn from real-world
CBT clinical session recordings to predict multiple session-level behavior
codes. The results indicate that our approach leads to improved evaluation
accuracy for most codes in both regression and classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhuohao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flemotomos_N/0/1/0/all/0/1"&gt;Nikolaos Flemotomos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singla_K/0/1/0/all/0/1"&gt;Karan Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Creed_T/0/1/0/all/0/1"&gt;Torrey A. Creed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atkins_D/0/1/0/all/0/1"&gt;David C. Atkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Shrikanth Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Paraphrase Detection with the Adversarial Paraphrasing Task. (arXiv:2106.07691v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07691</id>
        <link href="http://arxiv.org/abs/2106.07691"/>
        <updated>2021-06-16T01:21:06.889Z</updated>
        <summary type="html"><![CDATA[If two sentences have the same meaning, it should follow that they are
equivalent in their inferential properties, i.e., each sentence should
textually entail the other. However, many paraphrase datasets currently in
widespread use rely on a sense of paraphrase based on word overlap and syntax.
Can we teach them instead to identify paraphrases in a way that draws on the
inferential properties of the sentences, and is not over-reliant on lexical and
syntactic similarities of a sentence pair? We apply the adversarial paradigm to
this question, and introduce a new adversarial method of dataset creation for
paraphrase identification: the Adversarial Paraphrasing Task (APT), which asks
participants to generate semantically equivalent (in the sense of mutually
implicative) but lexically and syntactically disparate paraphrases. These
sentence pairs can then be used both to test paraphrase identification models
(which get barely random accuracy) and then improve their performance. To
accelerate dataset generation, we explore automation of APT using T5, and show
that the resulting dataset also improves accuracy. We discuss implications for
paraphrase detection and release our dataset in the hope of making paraphrase
detection models better able to detect sentence-level meaning equivalence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nighojkar_A/0/1/0/all/0/1"&gt;Animesh Nighojkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Licato_J/0/1/0/all/0/1"&gt;John Licato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An enriched category theory of language: from syntax to semantics. (arXiv:2106.07890v1 [math.CT])]]></title>
        <id>http://arxiv.org/abs/2106.07890</id>
        <link href="http://arxiv.org/abs/2106.07890"/>
        <updated>2021-06-16T01:21:06.883Z</updated>
        <summary type="html"><![CDATA[Given a piece of text, the ability to generate a coherent extension of it
implies some sophistication, including a knowledge of grammar and semantics. In
this paper, we propose a mathematical framework for passing from probability
distributions on extensions of given texts to an enriched category containing
semantic information. Roughly speaking, we model probability distributions on
texts as a category enriched over the unit interval. Objects of this category
are expressions in language and hom objects are conditional probabilities that
one expression is an extension of another. This category is syntactical: it
describes what goes with what. We then pass to the enriched category of unit
interval-valued copresheaves on this syntactical category to find semantic
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Bradley_T/0/1/0/all/0/1"&gt;Tai-Danae Bradley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Terilla_J/0/1/0/all/0/1"&gt;John Terilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Vlassopoulos_Y/0/1/0/all/0/1"&gt;Yiannis Vlassopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoDERT: Distilling Encoder Representations with Co-learning for Transducer-based Speech Recognition. (arXiv:2106.07734v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07734</id>
        <link href="http://arxiv.org/abs/2106.07734"/>
        <updated>2021-06-16T01:21:06.826Z</updated>
        <summary type="html"><![CDATA[We propose a simple yet effective method to compress an RNN-Transducer
(RNN-T) through the well-known knowledge distillation paradigm. We show that
the transducer's encoder outputs naturally have a high entropy and contain rich
information about acoustically similar word-piece confusions. This rich
information is suppressed when combined with the lower entropy decoder outputs
to produce the joint network logits. Consequently, we introduce an auxiliary
loss to distill the encoder logits from a teacher transducer's encoder, and
explore training strategies where this encoder distillation works effectively.
We find that tandem training of teacher and student encoders with an inplace
encoder distillation outperforms the use of a pre-trained and static teacher
transducer. We also report an interesting phenomenon we refer to as implicit
distillation, that occurs when the teacher and student encoders share the same
decoder. Our experiments show 5.37-8.4% relative word error rate reductions
(WERR) on in-house test sets, and 5.05-6.18% relative WERRs on LibriSpeech test
sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Swaminathan_R/0/1/0/all/0/1"&gt;Rupak Vignesh Swaminathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_B/0/1/0/all/0/1"&gt;Brian King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1"&gt;Grant P. Strimel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1"&gt;Jasha Droppo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1"&gt;Athanasios Mouchtaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilateral Personalized Dialogue Generation with Dynamic Persona-Aware Fusion. (arXiv:2106.07857v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07857</id>
        <link href="http://arxiv.org/abs/2106.07857"/>
        <updated>2021-06-16T01:21:06.797Z</updated>
        <summary type="html"><![CDATA[Generating personalized responses is one of the major challenges in natural
human-robot interaction. Current researches in this field mainly focus on
generating responses consistent with the robot's pre-assigned persona, while
ignoring the user's persona. Such responses may be inappropriate or even
offensive, which may lead to the bad user experience. Therefore, we propose a
bilateral personalized dialogue generation (BPDG) method with dynamic
persona-aware fusion via multi-task transfer learning to generate responses
consistent with both personas. The proposed method aims to accomplish three
learning tasks: 1) an encoder is trained with dialogue utterances added with
corresponded personalized attributes and relative position (language model
task), 2) a dynamic persona-aware fusion module predicts the persona presence
to adaptively fuse the contextual and bilateral personas encodings (persona
prediction task) and 3) a decoder generates natural, fluent and personalized
responses (dialogue generation task). To make the generated responses more
personalized and bilateral persona-consistent, the Conditional Mutual
Information Maximum (CMIM) criterion is adopted to select the final response
from the generated candidates. The experimental results show that the proposed
method outperforms several state-of-the-art methods in terms of both automatic
and manual evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Bin Sun&lt;/a&gt; (Member, IEEE), &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shutao Li&lt;/a&gt; (Fellow, IEEE)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Tags Matter for Zero-Shot Neural Machine Translation. (arXiv:2106.07930v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07930</id>
        <link href="http://arxiv.org/abs/2106.07930"/>
        <updated>2021-06-16T01:21:06.788Z</updated>
        <summary type="html"><![CDATA[Multilingual Neural Machine Translation (MNMT) has aroused widespread
interest due to its efficiency. An exciting advantage of MNMT models is that
they could also translate between unsupervised (zero-shot) language directions.
Language tag (LT) strategies are often adopted to indicate the translation
directions in MNMT. In this paper, we demonstrate that the LTs are not only
indicators for translation directions but also crucial to zero-shot translation
qualities. Unfortunately, previous work tends to ignore the importance of LT
strategies. We demonstrate that a proper LT strategy could enhance the
consistency of semantic representations and alleviate the off-target issue in
zero-shot directions. Experimental results show that by ignoring the source
language tag (SLT) and adding the target language tag (TLT) to the encoder, the
zero-shot translations could achieve a +8 BLEU score difference over other LT
strategies in IWSLT17, Europarl, TED talks translation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Shanbo Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Targeted Data Acquisition for Evolving Negotiation Agents. (arXiv:2106.07728v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.07728</id>
        <link href="http://arxiv.org/abs/2106.07728"/>
        <updated>2021-06-16T01:21:06.781Z</updated>
        <summary type="html"><![CDATA[Successful negotiators must learn how to balance optimizing for self-interest
and cooperation. Yet current artificial negotiation agents often heavily depend
on the quality of the static datasets they were trained on, limiting their
capacity to fashion an adaptive response balancing self-interest and
cooperation. For this reason, we find that these agents can achieve either high
utility or cooperation, but not both. To address this, we introduce a targeted
data acquisition framework where we guide the exploration of a reinforcement
learning agent using annotations from an expert oracle. The guided exploration
incentivizes the learning agent to go beyond its static dataset and develop new
negotiation strategies. We show that this enables our agents to obtain
higher-reward and more Pareto-optimal solutions when negotiating with both
simulated and human partners compared to standard supervised learning and
reinforcement learning methods. This trend additionally holds when comparing
agents using our targeted data acquisition framework to variants of agents
trained with a mix of supervised learning and reinforcement learning, or to
agents using tailored reward functions that explicitly optimize for utility and
Pareto-optimality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1"&gt;Minae Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1"&gt;Siddharth Karamcheti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuellar_M/0/1/0/all/0/1"&gt;Mariano-Florentino Cuellar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1"&gt;Dorsa Sadigh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Motion Vector Extrapolation for Video Object Detection. (arXiv:2104.08918v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08918</id>
        <link href="http://arxiv.org/abs/2104.08918"/>
        <updated>2021-06-16T01:21:06.774Z</updated>
        <summary type="html"><![CDATA[Despite the continued successes of computationally efficient deep neural
network architectures for video object detection, performance continually
arrives at the great trilemma of speed versus accuracy versus computational
resources (pick two). Current attempts to exploit temporal information in video
data to overcome this trilemma are bottlenecked by the state-of-the-art in
object detection models. We present, a technique which performs video object
detection through the use of off-the-shelf object detectors alongside existing
optical flow based motion estimation techniques in parallel. Through a set of
experiments on the benchmark MOT20 dataset, we demonstrate that our approach
significantly reduces the baseline latency of any given object detector without
sacrificing any accuracy. Further latency reduction, up to 25x lower than the
original latency, can be achieved with minimal accuracy loss. MOVEX enables low
latency video object detection on common CPU based systems, thus allowing for
high performance video object detection beyond the domain of GPU computing. The
code is available at https://github.com/juliantrue/movex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+True_J/0/1/0/all/0/1"&gt;Julian True&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naimul Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Beyond Classification Accuracy Metrics in Model Compression. (arXiv:2012.01604v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01604</id>
        <link href="http://arxiv.org/abs/2012.01604"/>
        <updated>2021-06-16T01:21:06.764Z</updated>
        <summary type="html"><![CDATA[With the rise in edge-computing devices, there has been an increasing demand
to deploy energy and resource-efficient models. A large body of research has
been devoted to developing methods that can reduce the size of the model
considerably without affecting the standard metrics such as top-1 accuracy.
However, these pruning approaches tend to result in a significant mismatch in
other metrics such as fairness across classes and explainability. To combat
such misalignment, we propose a novel multi-part loss function inspired by the
knowledge-distillation literature. Through extensive experiments, we
demonstrate the effectiveness of our approach across different compression
algorithms, architectures, tasks as well as datasets. In particular, we obtain
up to $4.1\times$ reduction in the number of prediction mismatches between the
compressed and reference models, and up to $5.7\times$ in cases where the
reference model makes the correct prediction; all while making no changes to
the compression algorithm, and minor modifications to the loss function.
Furthermore, we demonstrate how inducing simple alignment between the
predictions of the models naturally improves the alignment on other metrics
including fairness and attributions. Our framework can thus serve as a simple
plug-and-play component for compression algorithms in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_V/0/1/0/all/0/1"&gt;Vinu Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1"&gt;Shoaib Ahmed Siddiqui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhaskara_A/0/1/0/all/0/1"&gt;Aditya Bhaskara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muralidharan_S/0/1/0/all/0/1"&gt;Saurav Muralidharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garland_M/0/1/0/all/0/1"&gt;Michael Garland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sheraz Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1"&gt;Andreas Dengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge-Rich BERT Embeddings for Readability Assessment. (arXiv:2106.07935v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07935</id>
        <link href="http://arxiv.org/abs/2106.07935"/>
        <updated>2021-06-16T01:21:06.744Z</updated>
        <summary type="html"><![CDATA[Automatic readability assessment (ARA) is the task of evaluating the level of
ease or difficulty of text documents for a target audience. For researchers,
one of the many open problems in the field is to make such models trained for
the task show efficacy even for low-resource languages. In this study, we
propose an alternative way of utilizing the information-rich embeddings of BERT
models through a joint-learning method combined with handcrafted linguistic
features for readability assessment. Results show that the proposed method
outperforms classical approaches in readability assessment using English and
Filipino datasets, and obtaining as high as 12.4% increase in F1 performance.
We also show that the knowledge encoded in BERT embeddings can be used as a
substitute feature set for low-resource languages like Filipino with limited
semantic and syntactic NLP tools to explicitly extract feature values for the
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1"&gt;Joseph Marvin Imperial&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text Generation with Efficient (Soft) Q-Learning. (arXiv:2106.07704v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07704</id>
        <link href="http://arxiv.org/abs/2106.07704"/>
        <updated>2021-06-16T01:21:06.738Z</updated>
        <summary type="html"><![CDATA[Maximum likelihood estimation (MLE) is the predominant algorithm for training
text generation models. This paradigm relies on direct supervision examples,
which is not applicable to many applications, such as generating adversarial
attacks or generating prompts to control language models. Reinforcement
learning (RL) on the other hand offers a more flexible solution by allowing
users to plug in arbitrary task metrics as reward. Yet previous RL algorithms
for text generation, such as policy gradient (on-policy RL) and Q-learning
(off-policy RL), are often notoriously inefficient or unstable to train due to
the large sequence space and the sparse reward received only at the end of
sequences. In this paper, we introduce a new RL formulation for text generation
from the soft Q-learning perspective. It further enables us to draw from the
latest RL advances, such as path consistency learning, to combine the best of
on-/off-policy updates, and learn effectively from sparse reward. We apply the
approach to a wide range of tasks, including learning from noisy/negative
examples, adversarial attacks, and prompt generation. Experiments show our
approach consistently outperforms both task-specialized algorithms and the
previous RL methods. On standard supervised tasks where MLE prevails, our
approach also achieves competitive performance and stability by training text
generation from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Han Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1"&gt;Bowen Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengzhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"&gt;Eric P. Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhiting Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compact and adaptive multiplane images for view synthesis. (arXiv:2102.10086v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10086</id>
        <link href="http://arxiv.org/abs/2102.10086"/>
        <updated>2021-06-16T01:21:06.729Z</updated>
        <summary type="html"><![CDATA[Recently, learning methods have been designed to create Multiplane Images
(MPIs) for view synthesis. While MPIs are extremely powerful and facilitate
high quality renderings, a great amount of memory is required, making them
impractical for many applications. In this paper, we propose a learning method
that optimizes the available memory to render compact and adaptive MPIs. Our
MPIs avoid redundant information and take into account the scene geometry to
determine the depth sampling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Navarro_J/0/1/0/all/0/1"&gt;Julia Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabater_N/0/1/0/all/0/1"&gt;Neus Sabater&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGENT: A Benchmark for Core Psychological Reasoning. (arXiv:2102.12321v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12321</id>
        <link href="http://arxiv.org/abs/2102.12321"/>
        <updated>2021-06-16T01:21:06.722Z</updated>
        <summary type="html"><![CDATA[For machine agents to successfully interact with humans in real-world
settings, they will need to develop an understanding of human mental life.
Intuitive psychology, the ability to reason about hidden mental variables that
drive observable actions, comes naturally to people: even pre-verbal infants
can tell agents from objects, expecting agents to act efficiently to achieve
goals given constraints. Despite recent interest in machine agents that reason
about other agents, it is not clear if such agents learn or hold the core
psychology principles that drive human reasoning. Inspired by cognitive
development studies on intuitive psychology, we present a benchmark consisting
of a large dataset of procedurally generated 3D animations, AGENT (Action,
Goal, Efficiency, coNstraint, uTility), structured around four scenarios (goal
preferences, action efficiency, unobserved constraints, and cost-reward
trade-offs) that probe key concepts of core intuitive psychology. We validate
AGENT with human-ratings, propose an evaluation protocol emphasizing
generalization, and compare two strong baselines built on Bayesian inverse
planning and a Theory of Mind neural network. Our results suggest that to pass
the designed tests of core intuitive psychology at human levels, a model must
acquire or have built-in representations of how agents plan, combining utility
computations and core knowledge of objects and physics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1"&gt;Tianmin Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1"&gt;Abhishek Bhandwaldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kevin A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shari Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1"&gt;Dan Gutfreund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spelke_E/0/1/0/all/0/1"&gt;Elizabeth Spelke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1"&gt;Tomer D. Ullman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing the Use of Prosody in Constituency Parsing of Imperfect Transcripts. (arXiv:2106.07794v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07794</id>
        <link href="http://arxiv.org/abs/2106.07794"/>
        <updated>2021-06-16T01:21:06.702Z</updated>
        <summary type="html"><![CDATA[This work explores constituency parsing on automatically recognized
transcripts of conversational speech. The neural parser is based on a sentence
encoder that leverages word vectors contextualized with prosodic features,
jointly learning prosodic feature extraction with parsing. We assess the
utility of the prosody in parsing on imperfect transcripts, i.e. transcripts
with automatic speech recognition (ASR) errors, by applying the parser in an
N-best reranking framework. In experiments on Switchboard, we obtain 13-15% of
the oracle N-best gain relative to parsing the 1-best ASR output, with
insignificant impact on word recognition error rate. Prosody provides a
significant part of the gain, and analyses suggest that it leads to more
grammatical utterances via recovering function words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Trang Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1"&gt;Mari Ostendorf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Book Cover Design via Layout Graphs. (arXiv:2105.11088v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11088</id>
        <link href="http://arxiv.org/abs/2105.11088"/>
        <updated>2021-06-16T01:21:06.692Z</updated>
        <summary type="html"><![CDATA[Book covers are intentionally designed and provide an introduction to a book.
However, they typically require professional skills to design and produce the
cover images. Thus, we propose a generative neural network that can produce
book covers based on an easy-to-use layout graph. The layout graph contains
objects such as text, natural scene objects, and solid color spaces. This
layout graph is embedded using a graph convolutional neural network and then
used with a mask proposal generator and a bounding-box generator and filled
using an object proposal generator. Next, the objects are compiled into a
single image and the entire network is trained using a combination of
adversarial training, perceptual training, and reconstruction. Finally, a Style
Retention Network (SRNet) is used to transfer the learned font style onto the
desired text. Using the proposed method allows for easily controlled and unique
book covers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wensheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miyazono_T/0/1/0/all/0/1"&gt;Taiga Miyazono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1"&gt;Brian Kenji Iwana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DISCO: Dynamic and Invariant Sensitive Channel Obfuscation for deep neural networks. (arXiv:2012.11025v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11025</id>
        <link href="http://arxiv.org/abs/2012.11025"/>
        <updated>2021-06-16T01:21:06.676Z</updated>
        <summary type="html"><![CDATA[Recent deep learning models have shown remarkable performance in image
classification. While these deep learning systems are getting closer to
practical deployment, the common assumption made about data is that it does not
carry any sensitive information. This assumption may not hold for many
practical cases, especially in the domain where an individual's personal
information is involved, like healthcare and facial recognition systems. We
posit that selectively removing features in this latent space can protect the
sensitive information and provide a better privacy-utility trade-off.
Consequently, we propose DISCO which learns a dynamic and data driven pruning
filter to selectively obfuscate sensitive information in the feature space. We
propose diverse attack schemes for sensitive inputs \& attributes and
demonstrate the effectiveness of DISCO against state-of-the-art methods through
quantitative and qualitative evaluation. Finally, we also release an evaluation
benchmark dataset of 1 million sensitive representations to encourage rigorous
exploration of novel attack schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Abhishek Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1"&gt;Ayush Chopra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1"&gt;Vivek Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garza_E/0/1/0/all/0/1"&gt;Ethan Garza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1"&gt;Emily Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vepakomma_P/0/1/0/all/0/1"&gt;Praneeth Vepakomma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1"&gt;Ramesh Raskar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A baseline for semi-supervised learning of efficient semantic segmentation models. (arXiv:2106.07075v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07075</id>
        <link href="http://arxiv.org/abs/2106.07075"/>
        <updated>2021-06-16T01:21:06.669Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning is especially interesting in the dense prediction
context due to high cost of pixel-level ground truth. Unfortunately, most such
approaches are evaluated on outdated architectures which hamper research due to
very slow training and high requirements on GPU RAM. We address this concern by
presenting a simple and effective baseline which works very well both on
standard and efficient architectures. Our baseline is based on one-way
consistency and non-linear geometric and photometric perturbations. We show
advantage of perturbing only the student branch and present a plausible
explanation of such behaviour. Experiments on Cityscapes and CIFAR-10
demonstrate competitive performance with respect to prior work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1"&gt;Ivan Grubi&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orsic_M/0/1/0/all/0/1"&gt;Marin Or&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1"&gt;Sini&amp;#x161;a &amp;#x160;egvi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework. (arXiv:2010.04879v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04879</id>
        <link href="http://arxiv.org/abs/2010.04879"/>
        <updated>2021-06-16T01:21:06.659Z</updated>
        <summary type="html"><![CDATA[Most neural network pruning methods, such as filter-level and layer-level
prunings, prune the network model along one dimension (depth, width, or
resolution) solely to meet a computational budget. However, such a pruning
policy often leads to excessive reduction of that dimension, thus inducing a
huge accuracy loss. To alleviate this issue, we argue that pruning should be
conducted along three dimensions comprehensively. For this purpose, our pruning
framework formulates pruning as an optimization problem. Specifically, it first
casts the relationships between a certain model's accuracy and
depth/width/resolution into a polynomial regression and then maximizes the
polynomial to acquire the optimal values for the three dimensions. Finally, the
model is pruned along the three optimal dimensions accordingly. In this
framework, since collecting too much data for training the regression is very
time-costly, we propose two approaches to lower the cost: 1) specializing the
polynomial to ensure an accurate regression even with less training data; 2)
employing iterative pruning and fine-tuning to collect the data faster.
Extensive experiments show that our proposed algorithm surpasses
state-of-the-art pruning algorithms and even neural architecture search-based
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenxiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minghao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jinming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haifeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coping with Label Shift via Distributionally Robust Optimisation. (arXiv:2010.12230v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12230</id>
        <link href="http://arxiv.org/abs/2010.12230"/>
        <updated>2021-06-16T01:21:06.650Z</updated>
        <summary type="html"><![CDATA[The label shift problem refers to the supervised learning setting where the
train and test label distributions do not match. Existing work addressing label
shift usually assumes access to an \emph{unlabelled} test sample. This sample
may be used to estimate the test label distribution, and to then train a
suitably re-weighted classifier. While approaches using this idea have proven
effective, their scope is limited as it is not always feasible to access the
target domain; further, they require repeated retraining if the model is to be
deployed in \emph{multiple} test environments. Can one instead learn a
\emph{single} classifier that is robust to arbitrary label shifts from a broad
family? In this paper, we answer this question by proposing a model that
minimises an objective based on distributionally robust optimisation (DRO). We
then design and analyse a gradient descent-proximal mirror ascent algorithm
tailored for large-scale problems to optimise the proposed objective. %, and
establish its convergence. Finally, through experiments on CIFAR-100 and
ImageNet, we show that our technique can significantly improve performance over
a number of baselines in settings where label shift is present.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingzhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1"&gt;Andreas Veit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1"&gt;Srinadh Bhojanapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1"&gt;Suvrit Sra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[1$\times$N Block Pattern for Network Sparsity. (arXiv:2105.14713v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14713</id>
        <link href="http://arxiv.org/abs/2105.14713"/>
        <updated>2021-06-16T01:21:06.630Z</updated>
        <summary type="html"><![CDATA[Though network sparsity emerges as a promising direction to overcome the
drastically increasing size of neural networks, it remains an open problem to
concurrently maintain model accuracy as well as achieve significant speedups on
general CPUs. In this paper, we propose one novel concept of $1\times N$ block
sparsity pattern (block pruning) to break this limitation. In particular,
consecutive $N$ output kernels with the same input channel index are grouped
into one block, which serves as a basic pruning granularity of our pruning
pattern. Our $1 \times N$ sparsity pattern prunes these blocks considered
unimportant. We also provide a workflow of filter rearrangement that first
rearranges the weight matrix in the output channel dimension to derive more
influential blocks for accuracy improvements, and then applies similar
rearrangement to the next-layer weights in the input channel dimension to
ensure correct convolutional operations. Moreover, the output computation after
our $1 \times N$ block sparsity can be realized via a parallelized block-wise
vectorized operation, leading to significant speedups on general CPUs-based
platforms. The efficacy of our pruning pattern is proved with experiments on
ILSVRC-2012. For example, in the case of 50% sparsity and $N=4$, our pattern
obtains about 3.0% improvements over filter pruning in the top-1 accuracy of
MobileNet-V2. Meanwhile, it obtains 56.04ms inference savings on Cortex-A7 CPU
over weight pruning. Code is available at https://github.com/lmbxmu/1xN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bohong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1"&gt;Fei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training. (arXiv:2102.02887v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02887</id>
        <link href="http://arxiv.org/abs/2102.02887"/>
        <updated>2021-06-16T01:21:06.622Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a new perspective on training deep neural
networks capable of state-of-the-art performance without the need for the
expensive over-parameterization by proposing the concept of In-Time
Over-Parameterization (ITOP) in sparse training. By starting from a random
sparse network and continuously exploring sparse connectivities during
training, we can perform an Over-Parameterization in the space-time manifold,
closing the gap in the expressibility between sparse training and dense
training. We further use ITOP to understand the underlying mechanism of Dynamic
Sparse Training (DST) and indicate that the benefits of DST come from its
ability to consider across time all possible parameters when searching for the
optimal sparse connectivity. As long as there are sufficient parameters that
have been reliably explored during training, DST can outperform the dense
neural network by a large margin. We present a series of experiments to support
our conjecture and achieve the state-of-the-art sparse training performance
with ResNet-50 on ImageNet. More impressively, our method achieves dominant
performance over the overparameterization-based sparse methods at extreme
sparsity levels. When trained on CIFAR-100, our method can match the
performance of the dense model even at an extreme sparsity (98%). Code can be
found https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1"&gt;Lu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1"&gt;Decebal Constantin Mocanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EPICURE Ensemble Pretrained Models for Extracting Cancer Mutations from Literature. (arXiv:2106.07722v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07722</id>
        <link href="http://arxiv.org/abs/2106.07722"/>
        <updated>2021-06-16T01:21:06.615Z</updated>
        <summary type="html"><![CDATA[To interpret the genetic profile present in a patient sample, it is necessary
to know which mutations have important roles in the development of the
corresponding cancer type. Named entity recognition is a core step in the text
mining pipeline which facilitates mining valuable cancer information from the
scientific literature. However, due to the scarcity of related datasets,
previous NER attempts in this domain either suffer from low performance when
deep learning based models are deployed, or they apply feature based machine
learning models or rule based models to tackle this problem, which requires
intensive efforts from domain experts, and limit the model generalization
capability. In this paper, we propose EPICURE, an ensemble pre trained model
equipped with a conditional random field pattern layer and a span prediction
pattern layer to extract cancer mutations from text. We also adopt a data
augmentation strategy to expand our training set from multiple datasets.
Experimental results on three benchmark datasets show competitive results
compared to the baseline models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jiarun Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veen_E/0/1/0/all/0/1"&gt;Elke M van Veen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peek_N/0/1/0/all/0/1"&gt;Niels Peek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renehan_A/0/1/0/all/0/1"&gt;Andrew G Renehan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1"&gt;Sophia Ananiadou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcoming Domain Mismatch in Low Resource Sequence-to-Sequence ASR Models using Hybrid Generated Pseudotranscripts. (arXiv:2106.07716v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07716</id>
        <link href="http://arxiv.org/abs/2106.07716"/>
        <updated>2021-06-16T01:21:06.609Z</updated>
        <summary type="html"><![CDATA[Sequence-to-sequence (seq2seq) models are competitive with hybrid models for
automatic speech recognition (ASR) tasks when large amounts of training data
are available. However, data sparsity and domain adaptation are more
problematic for seq2seq models than their hybrid counterparts. We examine
corpora of five languages from the IARPA MATERIAL program where the transcribed
data is conversational telephone speech (CTS) and evaluation data is broadcast
news (BN). We show that there is a sizable initial gap in such a data condition
between hybrid and seq2seq models, and the hybrid model is able to further
improve through the use of additional language model (LM) data. We use an
additional set of untranscribed data primarily in the BN domain for
semisupervised training. In semisupervised training, a seed model trained on
transcribed data generates hypothesized transcripts for unlabeled
domain-matched data for further training. By using a hybrid model with an
expanded language model for pseudotranscription, we are able to improve our
seq2seq model from an average word error rate (WER) of 66.7% across all five
languages to 29.0% WER. While this puts the seq2seq model at a competitive
operating point, hybrid models are still able to use additional LM data to
maintain an advantage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chak-Fai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keith_F/0/1/0/all/0/1"&gt;Francis Keith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartmann_W/0/1/0/all/0/1"&gt;William Hartmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snover_M/0/1/0/all/0/1"&gt;Matthew Snover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kimball_O/0/1/0/all/0/1"&gt;Owen Kimball&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval. (arXiv:2104.01894v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01894</id>
        <link href="http://arxiv.org/abs/2104.01894"/>
        <updated>2021-06-16T01:21:06.601Z</updated>
        <summary type="html"><![CDATA[Speech-based image retrieval has been studied as a proxy for joint
representation learning, usually without emphasis on retrieval itself. As such,
it is unclear how well speech-based retrieval can work in practice -- both in
an absolute sense and versus alternative strategies that combine automatic
speech recognition (ASR) with strong text encoders. In this work, we
extensively study and expand choices of encoder architectures, training
methodology (including unimodal and multimodal pretraining), and other factors.
Our experiments cover different types of speech in three datasets: Flickr
Audio, Places Audio, and Localized Narratives. Our best model configuration
achieves large gains over state of the art, e.g., pushing recall-at-one from
21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also
show our best speech-based models can match or exceed cascaded ASR-to-text
encoding when speech is spontaneous, accented, or otherwise hard to
automatically transcribe.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1"&gt;Ramon Sanabria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1"&gt;Austin Waters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1"&gt;Jason Baldridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis. (arXiv:2106.07049v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07049</id>
        <link href="http://arxiv.org/abs/2106.07049"/>
        <updated>2021-06-16T01:21:06.580Z</updated>
        <summary type="html"><![CDATA[In the last few years, deep learning classifiers have shown promising results
in image-based medical diagnosis. However, interpreting the outputs of these
models remains a challenge. In cancer diagnosis, interpretability can be
achieved by localizing the region of the input image responsible for the
output, i.e. the location of a lesion. Alternatively, segmentation or detection
models can be trained with pixel-wise annotations indicating the locations of
malignant lesions. Unfortunately, acquiring such labels is labor-intensive and
requires medical expertise. To overcome this difficulty, weakly-supervised
localization can be utilized. These methods allow neural network classifiers to
output saliency maps highlighting the regions of the input most relevant to the
classification task (e.g. malignant lesions in mammograms) using only
image-level labels (e.g. whether the patient has cancer or not) during
training. When applied to high-resolution images, existing methods produce
low-resolution saliency maps. This is problematic in applications in which
suspicious lesions are small in relation to the image size. In this work, we
introduce a novel neural network architecture to perform weakly-supervised
segmentation of high-resolution images. The proposed model selects regions of
interest via coarse-level localization, and then performs fine-grained
segmentation of those regions. We apply this model to breast cancer diagnosis
with screening mammography, and validate it on a large clinically-realistic
dataset. Measured by Dice similarity score, our approach outperforms existing
methods by a large margin in terms of localization performance of benign and
malignant lesions, relatively improving the performance by 39.6% and 20.0%,
respectively. Code and the weights of some of the models are available at
https://github.com/nyukat/GLAM]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kangning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yiqiu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1"&gt;Nan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1"&gt;Jakub Ch&amp;#x142;&amp;#x119;dowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1"&gt;Carlos Fernandez-Granda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1"&gt;Krzysztof J. Geras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Traffic Scenario Clustering by Iterative Optimisation of Self-Supervised Networks Using a Random Forest Activation Pattern Similarity. (arXiv:2105.07639v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07639</id>
        <link href="http://arxiv.org/abs/2105.07639"/>
        <updated>2021-06-16T01:21:06.573Z</updated>
        <summary type="html"><![CDATA[Traffic scenario categorisation is an essential component of automated
driving, for e.\,g., in motion planning algorithms and their validation.
Finding new relevant scenarios without handcrafted steps reduce the required
resources for the development of autonomous driving dramatically. In this work,
a method is proposed to address this challenge by introducing a clustering
technique based on a novel data-adaptive similarity measure, called Random
Forest Activation Pattern (RFAP) similarity. The RFAP similarity is generated
using a tree encoding scheme in a Random Forest algorithm. The clustering
method proposed in this work takes into account that there are labelled
scenarios available and the information from the labelled scenarios can help to
guide the clustering of unlabelled scenarios. It consists of three steps.
First, a self-supervised Convolutional Neural Network~(CNN) is trained on all
available traffic scenarios using a defined self-supervised objective. Second,
the CNN is fine-tuned for classification of the labelled scenarios. Third,
using the labelled and unlabelled scenarios an iterative optimisation procedure
is performed for clustering. In the third step at each epoch of the iterative
optimisation, the CNN is used as a feature generator for an unsupervised Random
Forest. The trained forest, in turn, provides the RFAP similarity to adapt
iteratively the feature generation process implemented by the CNN. Extensive
experiments and ablation studies have been done on the highD dataset. The
proposed method shows superior performance compared to baseline clustering
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_L/0/1/0/all/0/1"&gt;Lakshman Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wurst_J/0/1/0/all/0/1"&gt;Jonas Wurst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Botsch_M/0/1/0/all/0/1"&gt;Michael Botsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1"&gt;Ke Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S2R-DepthNet: Learning a Generalizable Depth-specific Structural Representation. (arXiv:2104.00877v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00877</id>
        <link href="http://arxiv.org/abs/2104.00877"/>
        <updated>2021-06-16T01:21:06.566Z</updated>
        <summary type="html"><![CDATA[Human can infer the 3D geometry of a scene from a sketch instead of a
realistic image, which indicates that the spatial structure plays a fundamental
role in understanding the depth of scenes. We are the first to explore the
learning of a depth-specific structural representation, which captures the
essential feature for depth estimation and ignores irrelevant style
information. Our S2R-DepthNet (Synthetic to Real DepthNet) can be well
generalized to unseen real-world data directly even though it is only trained
on synthetic data. S2R-DepthNet consists of: a) a Structure Extraction (STE)
module which extracts a domaininvariant structural representation from an image
by disentangling the image into domain-invariant structure and domain-specific
style components, b) a Depth-specific Attention (DSA) module, which learns
task-specific knowledge to suppress depth-irrelevant structures for better
depth estimation and generalization, and c) a depth prediction module (DP) to
predict depth from the depth-specific representation. Without access of any
real-world images, our method even outperforms the state-of-the-art
unsupervised domain adaptation methods which use real-world images of the
target domain for training. In addition, when using a small amount of labeled
real-world data, we achieve the state-ofthe-art performance under the
semi-supervised setting. The code and trained models are available at
https://github.com/microsoft/S2R-DepthNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaotian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuwang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuejin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepKoCo: Efficient latent planning with a robust Koopman representation. (arXiv:2011.12690v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12690</id>
        <link href="http://arxiv.org/abs/2011.12690"/>
        <updated>2021-06-16T01:21:06.559Z</updated>
        <summary type="html"><![CDATA[This paper presents DeepKoCo, a novel model-based agent that learns a latent
Koopman representation from images. This representation allows DeepKoCo to plan
efficiently using linear control methods, such as linear model predictive
control. Compared to traditional agents, DeepKoCo is robust to task-irrelevant
dynamics, thanks to the use of a tailored lossy autoencoder network that allows
DeepKoCo to learn latent dynamics that reconstruct and predict only observed
costs, rather than all observed dynamics. As our results show, DeepKoCo
achieves a similar final performance as traditional model-free methods on
complex control tasks, while being considerably more robust to distractor
dynamics, making the proposed agent more amenable for real-life applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heijden_B/0/1/0/all/0/1"&gt;Bas van der Heijden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferranti_L/0/1/0/all/0/1"&gt;Laura Ferranti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1"&gt;Jens Kober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1"&gt;Robert Babuska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Nondestructive Wear Assessment in Large Internal Combustion Engines. (arXiv:2103.08482v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08482</id>
        <link href="http://arxiv.org/abs/2103.08482"/>
        <updated>2021-06-16T01:21:06.552Z</updated>
        <summary type="html"><![CDATA[Digitalization offers a large number of promising tools for large internal
combustion engines such as condition monitoring or condition-based maintenance.
This includes the status evaluation of key engine components such as cylinder
liners, whose inner surfaces are subject to constant wear due to their movement
relative to the pistons. Existing state-of-the-art methods for quantifying wear
require disassembly and cutting of the examined liner followed by a
high-resolution microscopic surface depth measurement that quantitatively
evaluates wear based on bearing load curves (also known as Abbott-Firestone
curves). Such reference methods are destructive, time-consuming and costly. The
goal of the research presented here is to develop nondestructive yet reliable
methods for quantifying the surface condition. A deep-learning framework is
proposed that allows computation of the bearing load curves from reflection RGB
images of the liner surface that can be collected with a wide variety of simple
imaging devices, without the need to remove and destroy the investigated liner.
For this purpose, a convolutional neural network is trained to predict the
bearing load curve of the corresponding depth profile from the collected RGB
images, which in turn can be used for further wear evaluation. Training of the
network is performed using a custom-built database containing depth profiles
and reflection images of liner surfaces of large gas engines. The results of
the proposed method are visually examined and quantified considering several
probabilistic distance metrics and comparison of roughness indicators between
ground truth and model predictions. The observed success of the proposed method
suggests its great potential for quantitative wear assessment on engines during
service directly on site.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Angermann_C/0/1/0/all/0/1"&gt;Christoph Angermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jonsson_S/0/1/0/all/0/1"&gt;Steinbj&amp;#xf6;rn J&amp;#xf3;nsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1"&gt;Markus Haltmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moravova_A/0/1/0/all/0/1"&gt;Ad&amp;#xe9;la Moravov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laubichler_C/0/1/0/all/0/1"&gt;Christian Laubichler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiesling_C/0/1/0/all/0/1"&gt;Constantin Kiesling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kober_M/0/1/0/all/0/1"&gt;Martin Kober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fimml_W/0/1/0/all/0/1"&gt;Wolfgang Fimml&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning and Quantization for Deep Neural Network Acceleration: A Survey. (arXiv:2101.09671v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09671</id>
        <link href="http://arxiv.org/abs/2101.09671"/>
        <updated>2021-06-16T01:21:06.531Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been applied in many applications exhibiting
extraordinary abilities in the field of computer vision. However, complex
network architectures challenge efficient real-time deployment and require
significant computation resources and energy costs. These challenges can be
overcome through optimizations such as network compression. Network compression
can often be realized with little loss of accuracy. In some cases accuracy may
even improve. This paper provides a survey on two types of network compression:
pruning and quantization. Pruning can be categorized as static if it is
performed offline or dynamic if it is performed at run-time. We compare pruning
techniques and describe criteria used to remove redundant computations. We
discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise,
layer-wise and even network-wise pruning. Quantization reduces computations by
reducing the precision of the datatype. Weights, biases, and activations may be
quantized typically to 8-bit integers although lower bit width implementations
are also discussed including binary neural networks. Both pruning and
quantization can be used independently or combined. We compare current
techniques, analyze their strengths and weaknesses, present compressed network
accuracy results on a number of frameworks, and provide practical guidance for
compressing networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1"&gt;Tailin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glossner_J/0/1/0/all/0/1"&gt;John Glossner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shaobo Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaotong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Diffusion Halftoning Against Adversarial Examples. (arXiv:2101.09451v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09451</id>
        <link href="http://arxiv.org/abs/2101.09451"/>
        <updated>2021-06-16T01:21:06.523Z</updated>
        <summary type="html"><![CDATA[Adversarial examples contain carefully crafted perturbations that can fool
deep neural networks (DNNs) into making wrong predictions. Enhancing the
adversarial robustness of DNNs has gained considerable interest in recent
years. Although image transformation-based defenses were widely considered at
an earlier time, most of them have been defeated by adaptive attacks. In this
paper, we propose a new image transformation defense based on error diffusion
halftoning, and combine it with adversarial training to defend against
adversarial examples. Error diffusion halftoning projects an image into a 1-bit
space and diffuses quantization error to neighboring pixels. This process can
remove adversarial perturbations from a given image while maintaining
acceptable image quality in the meantime in favor of recognition. Experimental
results demonstrate that the proposed method is able to improve adversarial
robustness even under advanced adaptive attacks, while most of the other image
transformation-based defenses do not. We show that a proper image
transformation can still be an effective defense approach. Code:
https://github.com/shaoyuanlo/Halftoning-Defense]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shao-Yuan Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Technical Report: Temporal Aggregate Representations. (arXiv:2106.03152v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03152</id>
        <link href="http://arxiv.org/abs/2106.03152"/>
        <updated>2021-06-16T01:21:06.515Z</updated>
        <summary type="html"><![CDATA[This technical report extends our work presented in [9] with more
experiments. In [9], we tackle long-term video understanding, which requires
reasoning from current and past or future observations and raises several
fundamental questions. How should temporal or sequential relationships be
modelled? What temporal extent of information and context needs to be
processed? At what temporal scale should they be derived? [9] addresses these
questions with a flexible multi-granular temporal aggregation framework. In
this report, we conduct further experiments with this framework on different
tasks and a new dataset, EPIC-KITCHENS-100.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sener_F/0/1/0/all/0/1"&gt;Fadime Sener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_D/0/1/0/all/0/1"&gt;Dibyadip Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1"&gt;Angela Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v6 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10762</id>
        <link href="http://arxiv.org/abs/2104.10762"/>
        <updated>2021-06-16T01:21:06.508Z</updated>
        <summary type="html"><![CDATA[Random field and random cluster theory are used to prove certain mathematical
results concerning the probability distribution of image pixel intensities
characterized as generic $2D$ integer arrays. The size of the smallest bounded
region within an image is estimated for segmenting an image, from which, the
equilibrium distribution of intensities can be recovered. From the estimated
bounded regions, properties of the sub-optimal and equilibrium distributions of
intensities are derived, which leads to an image compression methodology
whereby only slightly more than half of all pixels are required for a
worst-case reconstruction of the original image. An example in unsupervised
object detection illustrates the mathematical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1"&gt;Robert A. Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Video Captioning Through Latent Variable Expansion. (arXiv:1910.12019v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.12019</id>
        <link href="http://arxiv.org/abs/1910.12019"/>
        <updated>2021-06-16T01:21:06.484Z</updated>
        <summary type="html"><![CDATA[Automatically describing video content with text description is challenging
but important task, which has been attracting a lot of attention in computer
vision community. Previous works mainly strive for the accuracy of the
generated sentences, while ignoring the sentences diversity, which is
inconsistent with human behavior. In this paper, we aim to caption each video
with multiple descriptions and propose a novel framework. Concretely, for a
given video, the intermediate latent variables of conventional encode-decode
process are utilized as input to the conditional generative adversarial network
(CGAN) with the purpose of generating diverse sentences. We adopt different
Convolutional Neural Networks (CNNs) as our generator that produces
descriptions conditioned on latent variables and discriminator that assesses
the quality of generated sentences. Simultaneously, a novel DCE metric is
designed to assess the diverse captions. We evaluate our method on the
benchmark datasets, where it demonstrates its ability to generate diverse
descriptions and achieves superior results against other state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Huanhou Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jinglun Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Domain Facial Expression Recognition: A Unified Evaluation Benchmark and Adversarial Graph Learning. (arXiv:2008.00923v7 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00923</id>
        <link href="http://arxiv.org/abs/2008.00923"/>
        <updated>2021-06-16T01:21:06.432Z</updated>
        <summary type="html"><![CDATA[To address the problem of data inconsistencies among different facial
expression recognition (FER) datasets, many cross-domain FER methods (CD-FERs)
have been extensively devised in recent years. Although each declares to
achieve superior performance, fair comparisons are lacking due to the
inconsistent choices of the source/target datasets and feature extractors. In
this work, we first analyze the performance effect caused by these inconsistent
choices, and then re-implement some well-performing CD-FER and recently
published domain adaptation algorithms. We ensure that all these algorithms
adopt the same source datasets and feature extractors for fair CD-FER
evaluations. We find that most of the current leading algorithms use
adversarial learning to learn holistic domain-invariant features to mitigate
domain shifts. However, these algorithms ignore local features, which are more
transferable across different datasets and carry more detailed content for
fine-grained adaptation. To address these issues, we integrate graph
representation propagation with adversarial learning for cross-domain
holistic-local feature co-adaptation by developing a novel adversarial graph
representation adaptation (AGRA) framework. Specifically, it first builds two
graphs to correlate holistic and local regions within each domain and across
different domains, respectively. Then, it extracts holistic-local features from
the input image and uses learnable per-class statistical distributions to
initialize the corresponding graph nodes. Finally, two stacked graph
convolution networks (GCNs) are adopted to propagate holistic-local features
within each domain to explore their interaction and across different domains
for holistic-local feature co-adaptation. We conduct extensive and fair
evaluations on several popular benchmarks and show that the proposed AGRA
framework outperforms previous state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianshui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_T/0/1/0/all/0/1"&gt;Tao Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hefeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Psychophysical Oriented Saliency Map Prediction Model. (arXiv:2011.04076v9 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04076</id>
        <link href="http://arxiv.org/abs/2011.04076"/>
        <updated>2021-06-16T01:21:06.287Z</updated>
        <summary type="html"><![CDATA[Visual attention is one of the most significant characteristics for selecting
and understanding the outside redundancy world. The nature of complex scenes
includes enormous redundancy. The human vision system can not process all
information simultaneously because of visual information bottleneck. The human
visual system mainly focuses on dominant parts of the scenes to reduce the
input visual redundancy information. It is commonly known as visual attention
prediction or visual saliency map. This paper proposes a new psychophysical
saliency prediction architecture, WECSF, inspired by human low-level visual
cortex function. The model consists of opponent color channels, wavelet
transform, wavelet energy map, and contrast sensitivity function for extracting
low-level image features and maximum approximation to the human visual system.
The proposed model is evaluated several datasets, including MIT1003, MIT300,
TORONTO, SID4VAM and UCF Sports dataset to explain its efficiency. We also
quantitatively and qualitatively compared the performance of saliency
prediction with other state-of-the-art models. Our model achieved very stable
and good performance. Second, we also confirmed that Fourier and
spectral-inspired saliency prediction models achieved outperformance compared
to other start-of-the-art non-neural networks and even deep neural network
models on psychophysical synthesis images. Finally, the proposed model also can
be applied to spatial-temporal saliency prediction and got better performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A White Paper on Neural Network Quantization. (arXiv:2106.08295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08295</id>
        <link href="http://arxiv.org/abs/2106.08295"/>
        <updated>2021-06-16T01:21:06.270Z</updated>
        <summary type="html"><![CDATA[While neural networks have advanced the frontiers in many applications, they
often come at a high computational cost. Reducing the power and latency of
neural network inference is key if we want to integrate modern networks into
edge devices with strict power and compute requirements. Neural network
quantization is one of the most effective ways of achieving these savings but
the additional noise it induces can lead to accuracy degradation. In this white
paper, we introduce state-of-the-art algorithms for mitigating the impact of
quantization noise on the network's performance while maintaining low-bit
weights and activations. We start with a hardware motivated introduction to
quantization and then consider two main classes of algorithms: Post-Training
Quantization (PTQ) and Quantization-Aware-Training (QAT). PTQ requires no
re-training or labelled data and is thus a lightweight push-button approach to
quantization. In most cases, PTQ is sufficient for achieving 8-bit quantization
with close to floating-point accuracy. QAT requires fine-tuning and access to
labeled training data but enables lower bit quantization with competitive
results. For both solutions, we provide tested pipelines based on existing
literature and extensive experimentation that lead to state-of-the-art
performance for common deep learning models and tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagel_M/0/1/0/all/0/1"&gt;Markus Nagel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fournarakis_M/0/1/0/all/0/1"&gt;Marios Fournarakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amjad_R/0/1/0/all/0/1"&gt;Rana Ali Amjad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bondarenko_Y/0/1/0/all/0/1"&gt;Yelysei Bondarenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baalen_M/0/1/0/all/0/1"&gt;Mart van Baalen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1"&gt;Tijmen Blankevoort&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Modular Should Neural Module Networks Be for Systematic Generalization?. (arXiv:2106.08170v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08170</id>
        <link href="http://arxiv.org/abs/2106.08170"/>
        <updated>2021-06-16T01:21:06.263Z</updated>
        <summary type="html"><![CDATA[Neural Module Networks (NMNs) aim at Visual Question Answering (VQA) via
composition of modules that tackle a sub-task. NMNs are a promising strategy to
achieve systematic generalization, i.e. overcoming biasing factors in the
training distribution. However, the aspects of NMNs that facilitate systematic
generalization are not fully understood. In this paper, we demonstrate that the
stage and the degree at which modularity is defined has large influence on
systematic generalization. In a series of experiments on three VQA datasets
(MNIST with multiple attributes, SQOOP, and CLEVR-CoGenT), our results reveal
that tuning the degree of modularity in the network, especially at the image
encoder stage, reaches substantially higher systematic generalization. These
findings lead to new NMN architectures that outperform previous ones in terms
of systematic generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1"&gt;Vanessa D&amp;#x27;Amario&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1"&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08318</id>
        <link href="http://arxiv.org/abs/2106.08318"/>
        <updated>2021-06-16T01:21:06.254Z</updated>
        <summary type="html"><![CDATA[How can neural networks be trained on large-volume temporal data efficiently?
To compute the gradients required to update parameters, backpropagation blocks
computations until the forward and backward passes are completed. For temporal
signals, this introduces high latency and hinders real-time learning. It also
creates a coupling between consecutive layers, which limits model parallelism
and increases memory consumption. In this paper, we build upon Sideways, which
avoids blocking by propagating approximate gradients forward in time, and we
propose mechanisms for temporal integration of information based on different
variants of skip connections. We also show how to decouple computation and
delegate individual neural modules to different devices, allowing distributed
and parallel training. The proposed Skip-Sideways achieves low latency
training, model parallelism, and, importantly, is capable of extracting
temporal features, leading to more stable training and improved performance on
real-world action recognition video datasets such as HMDB51, UCF101, and the
large-scale Kinetics-600. Finally, we also show that models trained with
Skip-Sideways generate better future frames than Sideways models, and hence
they can better utilize motion cues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1"&gt;Dimitrios Vytiniotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1"&gt;Grzegorz Swirszcz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1"&gt;Viorica Patraucean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Joao Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Color2Style: Real-Time Exemplar-Based Image Colorization with Self-Reference Learning and Deep Feature Modulation. (arXiv:2106.08017v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08017</id>
        <link href="http://arxiv.org/abs/2106.08017"/>
        <updated>2021-06-16T01:21:06.233Z</updated>
        <summary type="html"><![CDATA[Legacy black-and-white photos are riddled with people's nostalgia and
glorious memories of the past. To better relive the elapsed frozen moments, in
this paper, we present a deep exemplar-based image colorization approach named
Color2Style to resurrect these grayscale image media by filling them with
vibrant colors. Generally, for exemplar-based colorization, unsupervised and
unpaired training are usually adopted, due to the difficulty of obtaining input
and ground truth image pairs. To train an exemplar-based colorization model,
current algorithms usually strive to achieve two procedures: i) retrieving a
large number of reference images with high similarity in advance, which is
inevitably time-consuming and tedious; ii) designing complicated modules to
transfer the colors of the reference image to the grayscale image, by
calculating and leveraging the deep semantic correspondence between them (e.g.,
non-local operation). Contrary to the previous methods, we solve and simplify
the above two steps in one end-to-end learning procedure. First, we adopt a
self-augmented self-reference training scheme, where the reference image is
generated by graphical transformations from the original colorful one whereby
the training can be formulated in a paired manner. Second, instead of computing
complex and inexplicable correspondence maps, our method exploits a simple yet
effective deep feature modulation (DFM) module, which injects the color
embeddings extracted from the reference image into the deep representations of
the input grayscale image. Such design is much more lightweight and
intelligible, achieving appealing performance with real-time processing speed.
Moreover, our model does not require multifarious loss functions and
regularization terms like existing methods, but only two widely used loss
functions. Codes and models will be available at
https://github.com/zhaohengyuan1/Color2Style.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hengyuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning with Kernel Dependence Maximization. (arXiv:2106.08320v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08320</id>
        <link href="http://arxiv.org/abs/2106.08320"/>
        <updated>2021-06-16T01:21:06.223Z</updated>
        <summary type="html"><![CDATA[We approach self-supervised learning of image representations from a
statistical dependence perspective, proposing Self-Supervised Learning with the
Hilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes
dependence between representations of transformed versions of an image and the
image identity, while minimizing the kernelized variance of those features.
This self-supervised learning framework yields a new understanding of InfoNCE,
a variational lower bound on the mutual information (MI) between different
transformations. While the MI itself is known to have pathologies which can
result in meaningless representations being learned, its bound is much better
behaved: we show that it implicitly approximates SSL-HSIC (with a slightly
different regularizer). Our approach also gives us insight into BYOL, since
SSL-HSIC similarly learns local neighborhoods of samples. SSL-HSIC allows us to
directly optimize statistical dependence in time linear in the batch size,
without restrictive data assumptions or indirect mutual information estimators.
Trained with or without a target network, SSL-HSIC matches the current
state-of-the-art for standard linear evaluation on ImageNet, semi-supervised
learning and transfer to other classification and vision tasks such as semantic
segmentation, depth estimation and object recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yazhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pogodin_R/0/1/0/all/0/1"&gt;Roman Pogodin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1"&gt;Danica J. Sutherland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simon Says: Evaluating and Mitigating Bias in Pruned Neural Networks with Knowledge Distillation. (arXiv:2106.07849v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07849</id>
        <link href="http://arxiv.org/abs/2106.07849"/>
        <updated>2021-06-16T01:21:06.215Z</updated>
        <summary type="html"><![CDATA[In recent years the ubiquitous deployment of AI has posed great concerns in
regards to algorithmic bias, discrimination, and fairness. Compared to
traditional forms of bias or discrimination caused by humans, algorithmic bias
generated by AI is more abstract and unintuitive therefore more difficult to
explain and mitigate. A clear gap exists in the current literature on
evaluating and mitigating bias in pruned neural networks. In this work, we
strive to tackle the challenging issues of evaluating, mitigating, and
explaining induced bias in pruned neural networks. Our paper makes three
contributions. First, we propose two simple yet effective metrics, Combined
Error Variance (CEV) and Symmetric Distance Error (SDE), to quantitatively
evaluate the induced bias prevention quality of pruned models. Second, we
demonstrate that knowledge distillation can mitigate induced bias in pruned
neural networks, even with unbalanced datasets. Third, we reveal that model
similarity has strong correlations with pruning induced bias, which provides a
powerful method to explain why bias occurs in pruned neural networks. Our code
is available at https://github.com/codestar12/pruning-distilation-bias]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blakeney_C/0/1/0/all/0/1"&gt;Cody Blakeney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huish_N/0/1/0/all/0/1"&gt;Nathaniel Huish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yan Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1"&gt;Ziliang Zong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Data Augmentation samples for Semantic Segmentation of Salt Bodies in a Synthetic Seismic Image Dataset. (arXiv:2106.08269v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08269</id>
        <link href="http://arxiv.org/abs/2106.08269"/>
        <updated>2021-06-16T01:21:06.208Z</updated>
        <summary type="html"><![CDATA[Nowadays, subsurface salt body localization and delineation, also called
semantic segmentation of salt bodies, are among the most challenging
geophysicist tasks. Thus, identifying large salt bodies is notoriously tricky
and is crucial for identifying hydrocarbon reservoirs and drill path planning.
This work proposes a Data Augmentation method based on training two generative
models to augment the number of samples in a seismic image dataset for the
semantic segmentation of salt bodies. Our method uses deep learning models to
generate pairs of seismic image patches and their respective salt masks for the
Data Augmentation. The first model is a Variational Autoencoder and is
responsible for generating patches of salt body masks. The second is a
Conditional Normalizing Flow model, which receives the generated masks as
inputs and generates the associated seismic image patches. We evaluate the
proposed method by comparing the performance of ten distinct state-of-the-art
models for semantic segmentation, trained with and without the generated
augmentations, in a dataset from two synthetic seismic images. The proposed
methodology yields an average improvement of 8.57% in the IoU metric across all
compared models. The best result is achieved by a DeeplabV3+ model variant,
which presents an IoU score of 95.17% when trained with our augmentations.
Additionally, our proposal outperformed six selected data augmentation methods,
and the most significant improvement in the comparison, of 9.77%, is achieved
by composing our DA with augmentations from an elastic transformation. At last,
we show that the proposed method is adaptable for a larger context size by
achieving results comparable to the obtained on the smaller context size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1"&gt;Luis Felipe Henriques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1"&gt;S&amp;#xe9;rgio Colcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1"&gt;Ruy Luiz Milidi&amp;#xfa;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulcao_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Bulc&amp;#xe3;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barros_P/0/1/0/all/0/1"&gt;Pablo Barros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relation Modeling in Spatio-Temporal Action Localization. (arXiv:2106.08061v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08061</id>
        <link href="http://arxiv.org/abs/2106.08061"/>
        <updated>2021-06-16T01:21:06.201Z</updated>
        <summary type="html"><![CDATA[This paper presents our solution to the AVA-Kinetics Crossover Challenge of
ActivityNet workshop at CVPR 2021. Our solution utilizes multiple types of
relation modeling methods for spatio-temporal action detection and adopts a
training strategy to integrate multiple relation modeling in end-to-end
training over the two large-scale video datasets. Learning with memory bank and
finetuning for long-tailed distribution are also investigated to further
improve the performance. In this paper, we detail the implementations of our
solution and provide experiments results and corresponding discussions. We
finally achieve 40.67 mAP on the test set of AVA-Kinetics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yutong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jianwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1"&gt;Zhiwu Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingqian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Latent Vector Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation. (arXiv:2106.08188v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08188</id>
        <link href="http://arxiv.org/abs/2106.08188"/>
        <updated>2021-06-16T01:21:06.181Z</updated>
        <summary type="html"><![CDATA[This paper addresses the domain shift problem for segmentation. As a
solution, we propose OLVA, a novel and lightweight unsupervised domain
adaptation method based on a Variational Auto-Encoder (VAE) and Optimal
Transport (OT) theory. Thanks to the VAE, our model learns a shared
cross-domain latent space that follows a normal distribution, which reduces the
domain shift. To guarantee valid segmentations, our shared latent space is
designed to model the shape rather than the intensity variations. We further
rely on an OT loss to match and align the remaining discrepancy between the two
domains in the latent space. We demonstrate OLVA's effectiveness for the
segmentation of multiple cardiac structures on the public Multi-Modality Whole
Heart Segmentation (MM-WHS) dataset, where the source domain consists of
annotated 3D MR images and the unlabelled target domain of 3D CTs. Our results
show remarkable improvements with an additional margin of 12.5\% dice score
over concurrent generative training approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chanti_D/0/1/0/all/0/1"&gt;Dawood Al Chanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mateus_D/0/1/0/all/0/1"&gt;Diana Mateus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Head: Unifying Object Detection Heads with Attentions. (arXiv:2106.08322v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08322</id>
        <link href="http://arxiv.org/abs/2106.08322"/>
        <updated>2021-06-16T01:21:06.174Z</updated>
        <summary type="html"><![CDATA[The complex nature of combining localization and classification in object
detection has resulted in the flourished development of methods. Previous works
tried to improve the performance in various object detection heads but failed
to present a unified view. In this paper, we present a novel dynamic head
framework to unify object detection heads with attentions. By coherently
combining multiple self-attention mechanisms between feature levels for
scale-awareness, among spatial locations for spatial-awareness, and within
output channels for task-awareness, the proposed approach significantly
improves the representation ability of object detection heads without any
computational overhead. Further experiments demonstrate that the effectiveness
and efficiency of the proposed dynamic head on the COCO benchmark. With a
standard ResNeXt-101-DCN backbone, we largely improve the performance over
popular object detectors and achieve a new state-of-the-art at 54.0 AP.
Furthermore, with latest transformer backbone and extra data, we can push
current best COCO result to a new record at 60.6 AP. The code will be released
at https://github.com/microsoft/DynamicHead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiyang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinpeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1"&gt;Bin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengchen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Micro-Structured Weight Unification for Neural Network Compression. (arXiv:2106.08301v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08301</id>
        <link href="http://arxiv.org/abs/2106.08301"/>
        <updated>2021-06-16T01:21:06.167Z</updated>
        <summary type="html"><![CDATA[Compressing Deep Neural Network (DNN) models to alleviate the storage and
computation requirements is essential for practical applications, especially
for resource limited devices. Although capable of reducing a reasonable amount
of model parameters, previous unstructured or structured weight pruning methods
can hardly truly accelerate inference, either due to the poor hardware
compatibility of the unstructured sparsity or due to the low sparse rate of the
structurally pruned network. Aiming at reducing both storage and computation,
as well as preserving the original task performance, we propose a generalized
weight unification framework at a hardware compatible micro-structured level to
achieve high amount of compression and acceleration. Weight coefficients of a
selected micro-structured block are unified to reduce the storage and
computation of the block without changing the neuron connections, which turns
to a micro-structured pruning special case when all unified coefficients are
set to zero, where neuron connections (hence storage and computation) are
completely removed. In addition, we developed an effective training framework
based on the alternating direction method of multipliers (ADMM), which converts
our complex constrained optimization into separately solvable subproblems.
Through iteratively optimizing the subproblems, the desired micro-structure can
be ensured with high compression ratio and low performance degradation. We
extensively evaluated our method using a variety of benchmark models and
datasets for different applications. Experimental results demonstrate
state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Sheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Songnan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation. (arXiv:1911.01529v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.01529</id>
        <link href="http://arxiv.org/abs/1911.01529"/>
        <updated>2021-06-16T01:21:06.159Z</updated>
        <summary type="html"><![CDATA[Deep learning approaches have become the standard solution to many problems
in computer vision and robotics, but obtaining sufficient training data in high
enough quality is challenging, as human labor is error prone, time consuming,
and expensive. Solutions based on simulation have become more popular in recent
years, but the gap between simulation and reality is still a major issue. In
this paper, we introduce a novel method for augmenting synthetic image data
through unsupervised image-to-image translation by applying the style of real
world images to simulated images with open source frameworks. The generated
dataset is combined with conventional augmentation methods and is then applied
to a neural network model running in real-time on autonomous soccer robots. Our
evaluation shows a significant improvement compared to models trained on images
generated entirely in simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blumenkamp_J/0/1/0/all/0/1"&gt;Jan Blumenkamp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baude_A/0/1/0/all/0/1"&gt;Andreas Baude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laue_T/0/1/0/all/0/1"&gt;Tim Laue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Thermal Human Faces for Physiological Assessment Using Thermal Sensor Auxiliary Labels. (arXiv:2106.08091v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08091</id>
        <link href="http://arxiv.org/abs/2106.08091"/>
        <updated>2021-06-16T01:21:06.152Z</updated>
        <summary type="html"><![CDATA[Thermal images reveal medically important physiological information about
human stress, signs of inflammation, and emotional mood that cannot be seen on
visible images. Providing a method to generate thermal faces from visible
images would be highly valuable for the telemedicine community in order to show
this medical information. To the best of our knowledge, there are limited works
on visible-to-thermal (VT) face translation, and many current works go the
opposite direction to generate visible faces from thermal surveillance images
(TV) for law enforcement applications. As a result, we introduce favtGAN, a VT
GAN which uses the pix2pix image translation model with an auxiliary sensor
label prediction network for generating thermal faces from visible images.
Since most TV methods are trained on only one data source drawn from one
thermal sensor, we combine datasets from faces and cityscapes. These combined
data are captured from similar sensors in order to bootstrap the training and
transfer learning task, especially valuable because visible-thermal face
datasets are limited. Experiments on these combined datasets show that favtGAN
demonstrates an increase in SSIM and PSNR scores of generated thermal faces,
compared to training on a single face dataset alone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ordun_C/0/1/0/all/0/1"&gt;Catherine Ordun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1"&gt;Edward Raff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purushotham_S/0/1/0/all/0/1"&gt;Sanjay Purushotham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08285</id>
        <link href="http://arxiv.org/abs/2106.08285"/>
        <updated>2021-06-16T01:21:06.132Z</updated>
        <summary type="html"><![CDATA[Time-lapse fluorescent microscopy (TLFM) combined with predictive
mathematical modelling is a powerful tool to study the inherently dynamic
processes of life on the single-cell level. Such experiments are costly,
complex and labour intensive. A complimentary approach and a step towards
completely in silico experiments, is to synthesise the imagery itself. Here, we
propose Multi-StyleGAN as a descriptive approach to simulate time-lapse
fluorescence microscopy imagery of living cells, based on a past experiment.
This novel generative adversarial network synthesises a multi-domain sequence
of consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple
live yeast cells in microstructured environments and train on a dataset
recorded in our laboratory. The simulation captures underlying biophysical
factors and time dependencies, such as cell morphology, growth, physical
interactions, as well as the intensity of a fluorescent reporter protein. An
immediate application is to generate additional training and validation data
for feature extraction algorithms or to aid and expedite development of
advanced experimental techniques such as online monitoring or control of cells.

Code and dataset is available at
https://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1"&gt;Tim Prangemeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1"&gt;Christoph Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1"&gt;Christian Wildner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1"&gt;Heinz Koeppl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top-Related Meta-Learning Method for Few-Shot Object Detection. (arXiv:2007.06837v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06837</id>
        <link href="http://arxiv.org/abs/2007.06837"/>
        <updated>2021-06-16T01:21:06.125Z</updated>
        <summary type="html"><![CDATA[Many meta-learning methods are proposed for few-shot detection. However,
previous most methods have two main problems, poor detection APs, and strong
bias because of imbalance and insufficient datasets. Previous works mainly
alleviate these issues by additional datasets, multi-relation attention
mechanisms and sub-modules. However, they require more cost. In this work, for
meta-learning, we find that the main challenges focus on related or irrelevant
semantic features between categories. Therefore, based on semantic features, we
propose a Top-C classification loss (i.e., TCL-C) for classification task and a
category-based grouping mechanism for category-based meta-features obtained by
the meta-model. The TCL-C exploits the true-label prediction and the most
likely C-1 false classification predictions to improve detection performance on
few-shot classes. According to similar appearance (i.e., visual appearance,
shape, and limbs etc.) and environment in which objects often appear, the
category-based grouping mechanism splits categories into disjoint groups to
make similar semantic features more compact between categories within a group
and obtain more significant difference between groups, alleviating the strong
bias problem and further improving detection APs. The whole training consists
of the base model and the fine-tuning phases. According to grouping mechanism,
we group the meta-features vectors obtained by meta-model, so that the
distribution difference between groups is obvious, and the one within each
group is less. Extensive experiments on Pascal VOC dataset demonstrate that
ours which combines the TCL-C with category-based grouping significantly
outperforms previous state-of-the-art methods for few-shot detection. Compared
with previous competitive baseline, ours improves detection APs by almost 4%
for few-shot detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1"&gt;Nan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaochun Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Duo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Dongrui Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhimin Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is this Harmful? Learning to Predict Harmfulness Ratings from Video. (arXiv:2106.08323v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08323</id>
        <link href="http://arxiv.org/abs/2106.08323"/>
        <updated>2021-06-16T01:21:06.118Z</updated>
        <summary type="html"><![CDATA[Automatically identifying harmful content in video is an important task with
a wide range of applications. However, due to the difficulty of collecting
high-quality labels as well as demanding computational requirements, the task
has not had a satisfying general approach. Typically, only small subsets of the
problem are considered, such as identifying violent content. In cases where the
general problem is tackled, rough approximations and simplifications are made
to deal with the lack of labels and computational complexity. In this work, we
identify and tackle the two main obstacles. First, we create a dataset of
approximately 4000 video clips, annotated by professionals in the field.
Secondly, we demonstrate that advances in video recognition enable training
models on our dataset that consider the full context of the scene. We conduct
an in-depth study on our modeling choices and find that we greatly benefit from
combining the visual and audio modality and that pretraining on large-scale
video recognition datasets and class balanced sampling further improves
performance. We additionally perform a qualitative study that reveals the
heavily multi-modal nature of our dataset. Our dataset will be made available
upon publication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edstedt_J/0/1/0/all/0/1"&gt;Johan Edstedt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlsson_J/0/1/0/all/0/1"&gt;Johan Karlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benavente_F/0/1/0/all/0/1"&gt;Francisca Benavente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novak_A/0/1/0/all/0/1"&gt;Anette Novak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1"&gt;Amanda Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1"&gt;Michael Felsberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-sample surface defect detection and classification based on semantic feedback neural network. (arXiv:2106.07959v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07959</id>
        <link href="http://arxiv.org/abs/2106.07959"/>
        <updated>2021-06-16T01:21:06.111Z</updated>
        <summary type="html"><![CDATA[Defect detection and classification technology has changed from traditional
artificial visual inspection to current intelligent automated inspection, but
most of the current defect detection methods are training related detection
models based on a data-driven approach, taking into account the difficulty of
collecting some sample data in the industrial field. We apply zero-shot
learning technology to the industrial field. Aiming at the problem of the
existing "Latent Feature Guide Attribute Attention" (LFGAA) zero-shot image
classification network, the output latent attributes and artificially defined
attributes are different in the semantic space, which leads to the problem of
model performance degradation, proposed an LGFAA network based on semantic
feedback, and improved model performance by constructing semantic embedded
modules and feedback mechanisms. At the same time, for the common domain shift
problem in zero-shot learning, based on the idea of co-training algorithm using
the difference information between different views of data to learn from each
other, we propose an Ensemble Co-training algorithm, which adaptively reduces
the prediction error in image tag embedding from multiple angles. Various
experiments conducted on the zero-shot dataset and the cylinder liner dataset
in the industrial field provide competitive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yibo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yiming Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1"&gt;Zhiyang Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haidi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_W/0/1/0/all/0/1"&gt;Wenhua Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingliang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixed Model OCR Training on Historical Latin Script for Out-of-the-Box Recognition and Finetuning. (arXiv:2106.07881v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07881</id>
        <link href="http://arxiv.org/abs/2106.07881"/>
        <updated>2021-06-16T01:21:06.103Z</updated>
        <summary type="html"><![CDATA[In order to apply Optical Character Recognition (OCR) to historical printings
of Latin script fully automatically, we report on our efforts to construct a
widely-applicable polyfont recognition model yielding text with a Character
Error Rate (CER) around 2% when applied out-of-the-box. Moreover, we show how
this model can be further finetuned to specific classes of printings with
little manual and computational effort. The mixed or polyfont model is trained
on a wide variety of materials, in terms of age (from the 15th to the 19th
century), typography (various types of Fraktur and Antiqua), and languages
(among others, German, Latin, and French). To optimize the results we combined
established techniques of OCR training like pretraining, data augmentation, and
voting. In addition, we used various preprocessing methods to enrich the
training data and obtain more robust models. We also implemented a two-stage
approach which first trains on all available, considerably unbalanced data and
then refines the output by training on a selected more balanced subset.
Evaluations on 29 previously unseen books resulted in a CER of 1.73%,
outperforming a widely used standard model with a CER of 2.84% by almost 40%.
Training a more specialized model for some unseen Early Modern Latin books
starting from our mixed model led to a CER of 1.47%, an improvement of up to
50% compared to training from scratch and up to 30% compared to training from
the aforementioned standard model. Our new mixed model is made openly available
to the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reul_C/0/1/0/all/0/1"&gt;Christian Reul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wick_C/0/1/0/all/0/1"&gt;Christoph Wick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noth_M/0/1/0/all/0/1"&gt;Maximilian N&amp;#xf6;th&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buttner_A/0/1/0/all/0/1"&gt;Andreas B&amp;#xfc;ttner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wehner_M/0/1/0/all/0/1"&gt;Maximilian Wehner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Springmann_U/0/1/0/all/0/1"&gt;Uwe Springmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Human Pose and Mesh Reconstruction with Transformers. (arXiv:2012.09760v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09760</id>
        <link href="http://arxiv.org/abs/2012.09760"/>
        <updated>2021-06-16T01:21:06.083Z</updated>
        <summary type="html"><![CDATA[We present a new method, called MEsh TRansfOrmer (METRO), to reconstruct 3D
human pose and mesh vertices from a single image. Our method uses a transformer
encoder to jointly model vertex-vertex and vertex-joint interactions, and
outputs 3D joint coordinates and mesh vertices simultaneously. Compared to
existing techniques that regress pose and shape parameters, METRO does not rely
on any parametric mesh models like SMPL, thus it can be easily extended to
other objects such as hands. We further relax the mesh topology and allow the
transformer self-attention mechanism to freely attend between any two vertices,
making it possible to learn non-local relationships among mesh vertices and
joints. With the proposed masked vertex modeling, our method is more robust and
effective in handling challenging situations like partial occlusions. METRO
generates new state-of-the-art results for human mesh reconstruction on the
public Human3.6M and 3DPW datasets. Moreover, we demonstrate the
generalizability of METRO to 3D hand reconstruction in the wild, outperforming
existing state-of-the-art methods on FreiHAND dataset. Code and pre-trained
models are available at https://github.com/microsoft/MeshTransformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kevin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcomplete Representations Against Adversarial Videos. (arXiv:2012.04262v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04262</id>
        <link href="http://arxiv.org/abs/2012.04262"/>
        <updated>2021-06-16T01:21:06.062Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness of deep neural networks is an extensively studied
problem in the literature and various methods have been proposed to defend
against adversarial images. However, only a handful of defense methods have
been developed for defending against attacked videos. In this paper, we propose
a novel Over-and-Under complete restoration network for Defending against
adversarial videos (OUDefend). Most restoration networks adopt an
encoder-decoder architecture that first shrinks spatial dimension then expands
it back. This approach learns undercomplete representations, which have large
receptive fields to collect global information but overlooks local details. On
the other hand, overcomplete representations have opposite properties. Hence,
OUDefend is designed to balance local and global features by learning those two
representations. We attach OUDefend to target video recognition models as a
feature restoration block and train the entire network end-to-end. Experimental
results show that the defenses focusing on images may be ineffective to videos,
while OUDefend enhances robustness against different types of adversarial
videos, ranging from additive attacks, multiplicative attacks to physically
realizable attacks. Code: https://github.com/shaoyuanlo/OUDefend]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shao-Yuan Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1"&gt;Jeya Maria Jose Valanarasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encouraging Intra-Class Diversity Through a Reverse Contrastive Loss for Better Single-Source Domain Generalization. (arXiv:2106.07916v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07916</id>
        <link href="http://arxiv.org/abs/2106.07916"/>
        <updated>2021-06-16T01:21:06.047Z</updated>
        <summary type="html"><![CDATA[Traditional deep learning algorithms often fail to generalize when they are
tested outside of the domain of training data. Because data distributions can
change dynamically in real-life applications once a learned model is deployed,
in this paper we are interested in single-source domain generalization (SDG)
which aims to develop deep learning algorithms able to generalize from a single
training domain where no information about the test domain is available at
training time. Firstly, we design two simple MNISTbased SDG benchmarks, namely
MNIST Color SDG-MP and MNIST Color SDG-UP, which highlight the two different
fundamental SDG issues of increasing difficulties: 1) a class-correlated
pattern in the training domain is missing (SDG-MP), or 2) uncorrelated with the
class (SDG-UP), in the testing data domain. This is in sharp contrast with the
current domain generalization (DG) benchmarks which mix up different
correlation and variation factors and thereby make hard to disentangle success
or failure factors when benchmarking DG algorithms. We further evaluate several
state-of-the-art SDG algorithms through our simple benchmark, namely MNIST
Color SDG-MP, and show that the issue SDG-MP is largely unsolved despite of a
decade of efforts in developing DG algorithms. Finally, we also propose a
partially reversed contrastive loss to encourage intra-class diversity and find
less strongly correlated patterns, to deal with SDG-MP and show that the
proposed approach is very effective on our MNIST Color SDG-MP benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duboudin_T/0/1/0/all/0/1"&gt;Thomas Duboudin&lt;/a&gt; (imagine), &lt;a href="http://arxiv.org/find/cs/1/au:+Dellandrea_E/0/1/0/all/0/1"&gt;Emmanuel Dellandr&amp;#xe9;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abgrall_C/0/1/0/all/0/1"&gt;Corentin Abgrall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henaff_G/0/1/0/all/0/1"&gt;Gilles H&amp;#xe9;naff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demographic Fairness in Face Identification: The Watchlist Imbalance Effect. (arXiv:2106.08049v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08049</id>
        <link href="http://arxiv.org/abs/2106.08049"/>
        <updated>2021-06-16T01:21:05.982Z</updated>
        <summary type="html"><![CDATA[Recently, different researchers have found that the gallery composition of a
face database can induce performance differentials to facial identification
systems in which a probe image is compared against up to all stored reference
images to reach a biometric decision. This negative effect is referred to as
"watchlist imbalance effect". In this work, we present a method to
theoretically estimate said effect for a biometric identification system given
its verification performance across demographic groups and the composition of
the used gallery. Further, we report results for identification experiments on
differently composed demographic subsets, i.e. females and males, of the public
academic MORPH database using the open-source ArcFace face recognition system.
It is shown that the database composition has a huge impact on performance
differentials in biometric identification systems, even if performance
differentials are less pronounced in the verification scenario. This study
represents the first detailed analysis of the watchlist imbalance effect which
is expected to be of high interest for future research in the field of facial
recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1"&gt;Pawel Drozdowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1"&gt;Christian Rathgeb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1"&gt;Christoph Busch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ResDepth: A Deep Prior For 3D Reconstruction From High-resolution Satellite Images. (arXiv:2106.08107v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08107</id>
        <link href="http://arxiv.org/abs/2106.08107"/>
        <updated>2021-06-16T01:21:05.973Z</updated>
        <summary type="html"><![CDATA[Modern optical satellite sensors enable high-resolution stereo reconstruction
from space. But the challenging imaging conditions when observing the Earth
from space push stereo matching to its limits. In practice, the resulting
digital surface models (DSMs) are fairly noisy and often do not attain the
accuracy needed for high-resolution applications such as 3D city modeling.
Arguably, stereo correspondence based on low-level image similarity is
insufficient and should be complemented with a-priori knowledge about the
expected surface geometry beyond basic local smoothness. To that end, we
introduce ResDepth, a convolutional neural network that learns such an
expressive geometric prior from example data. ResDepth refines an initial, raw
stereo DSM while conditioning the refinement on the images. I.e., it acts as a
smart, learned post-processing filter and can seamlessly complement any stereo
matching pipeline. In a series of experiments, we find that the proposed method
consistently improves stereo DSMs both quantitatively and qualitatively. We
show that the prior encoded in the network weights captures meaningful
geometric characteristics of urban design, which also generalize across
different districts and even from one city to another. Moreover, we demonstrate
that, by training on a variety of stereo pairs, ResDepth can acquire a
sufficient degree of invariance against variations in imaging conditions and
acquisition geometry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Stucker_C/0/1/0/all/0/1"&gt;Corinne Stucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting the Calibration of Modern Neural Networks. (arXiv:2106.07998v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07998</id>
        <link href="http://arxiv.org/abs/2106.07998"/>
        <updated>2021-06-16T01:21:05.905Z</updated>
        <summary type="html"><![CDATA[Accurate estimation of predictive uncertainty (model calibration) is
essential for the safe application of neural networks. Many instances of
miscalibration in modern neural networks have been reported, suggesting a trend
that newer, more accurate models produce poorly calibrated predictions. Here,
we revisit this question for recent state-of-the-art image classification
models. We systematically relate model calibration and accuracy, and find that
the most recent models, notably those not using convolutions, are among the
best calibrated. Trends observed in prior model generations, such as decay of
calibration with distribution shift or model size, are less pronounced in
recent architectures. We also show that model size and amount of pretraining do
not fully explain these differences, suggesting that architecture is a major
determinant of calibration properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1"&gt;Matthias Minderer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1"&gt;Josip Djolonga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1"&gt;Rob Romijnders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubis_F/0/1/0/all/0/1"&gt;Frances Hubis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Dustin Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1"&gt;Mario Lucic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection. (arXiv:2106.07852v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07852</id>
        <link href="http://arxiv.org/abs/2106.07852"/>
        <updated>2021-06-16T01:21:05.888Z</updated>
        <summary type="html"><![CDATA[Non-parametric face modeling aims to reconstruct 3D face only from images
without shape assumptions. While plausible facial details are predicted, the
models tend to over-depend on local color appearance and suffer from ambiguous
noise. To address such problem, this paper presents a novel Learning to
Aggregate and Personalize (LAP) framework for unsupervised robust 3D face
modeling. Instead of using controlled environment, the proposed method
implicitly disentangles ID-consistent and scene-specific face from
unconstrained photo set. Specifically, to learn ID-consistent face, LAP
adaptively aggregates intrinsic face factors of an identity based on a novel
curriculum learning approach with relaxed consistency loss. To adapt the face
for a personalized scene, we propose a novel attribute-refining network to
modify ID-consistent face with target attribute and details. Based on the
proposed method, we make unsupervised 3D face modeling benefit from meaningful
image facial structure and possibly higher resolutions. Extensive experiments
on benchmarks show LAP recovers superior or competitive face shape and texture,
compared with state-of-the-art (SOTA) methods with or without prior and
supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yanhao Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Renwang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yan Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-script Handwritten Digit Recognition Using Multi-task Learning. (arXiv:2106.08267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08267</id>
        <link href="http://arxiv.org/abs/2106.08267"/>
        <updated>2021-06-16T01:21:05.878Z</updated>
        <summary type="html"><![CDATA[Handwritten digit recognition is one of the extensively studied area in
machine learning. Apart from the wider research on handwritten digit
recognition on MNIST dataset, there are many other research works on various
script recognition. However, it is not very common for multi-script digit
recognition which encourage the development of robust and multipurpose systems.
Additionally working on multi-script digit recognition enables multi-task
learning, considering the script classification as a related task for instance.
It is evident that multi-task learning improves model performance through
inductive transfer using the information contained in related tasks. Therefore,
in this study multi-script handwritten digit recognition using multi-task
learning will be investigated. As a specific case of demonstrating the solution
to the problem, Amharic handwritten character recognition will also be
experimented. The handwritten digits of three scripts including Latin, Arabic
and Kannada are studied to show that multi-task models with reformulation of
the individual tasks have shown promising results. In this study a novel way of
using the individual tasks predictions was proposed to help classification
performance and regularize the different loss for the purpose of the main task.
This finding has outperformed the baseline and the conventional multi-task
learning models. More importantly, it avoided the need for weighting the
different losses of the tasks, which is one of the challenges in multi-task
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gondere_M/0/1/0/all/0/1"&gt;Mesay Samuel Gondere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1"&gt;Lars Schmidt-Thieme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1"&gt;Durga Prasad Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholz_R/0/1/0/all/0/1"&gt;Randolf Scholz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physion: Evaluating Physical Prediction from Vision in Humans and Machines. (arXiv:2106.08261v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.08261</id>
        <link href="http://arxiv.org/abs/2106.08261"/>
        <updated>2021-06-16T01:21:05.871Z</updated>
        <summary type="html"><![CDATA[While machine learning algorithms excel at many challenging visual tasks, it
is unclear that they can make predictions about commonplace real world physical
events. Here, we present a visual and physical prediction benchmark that
precisely measures this capability. In realistically simulating a wide variety
of physical phenomena -- rigid and soft-body collisions, stable multi-object
configurations, rolling and sliding, projectile motion -- our dataset presents
a more comprehensive challenge than existing benchmarks. Moreover, we have
collected human responses for our stimuli so that model predictions can be
directly compared to human judgments. We compare an array of algorithms --
varying in their architecture, learning objective, input-output structure, and
training data -- on their ability to make diverse physical predictions. We find
that graph neural networks with access to the physical state best capture human
behavior, whereas among models that receive only visual input, those with
object-centric representations or pretraining do best but fall far short of
human accuracy. This suggests that extracting physically meaningful
representations of scenes is the main bottleneck to achieving human-like visual
prediction. We thus demonstrate how our benchmark can identify areas for
improvement and measure progress on this key aspect of physical understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bear_D/0/1/0/all/0/1"&gt;Daniel M. Bear&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Elias Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mrowca_D/0/1/0/all/0/1"&gt;Damian Mrowca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_F/0/1/0/all/0/1"&gt;Felix J. Binder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1"&gt;Hsiau-Yu Fish Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pramod_R/0/1/0/all/0/1"&gt;R.T. Pramod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holdaway_C/0/1/0/all/0/1"&gt;Cameron Holdaway&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1"&gt;Sirui Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kevin Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanwisher_N/0/1/0/all/0/1"&gt;Nancy Kanwisher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1"&gt;Daniel L.K. Yamins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Judith E. Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Lightweight ReLU-Based Feature Fusion for Aerial Scene Classification. (arXiv:2106.07879v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.07879</id>
        <link href="http://arxiv.org/abs/2106.07879"/>
        <updated>2021-06-16T01:21:05.863Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a transfer-learning based model construction
technique for the aerial scene classification problem. The core of our
technique is a layer selection strategy, named ReLU-Based Feature Fusion
(RBFF), that extracts feature maps from a pretrained CNN-based single-object
image classification model, namely MobileNetV2, and constructs a model for the
aerial scene classification task. RBFF stacks features extracted from the batch
normalization layer of a few selected blocks of MobileNetV2, where the
candidate blocks are selected based on the characteristics of the ReLU
activation layers present in those blocks. The feature vector is then
compressed into a low-dimensional feature space using dimension reduction
algorithms on which we train a low-cost SVM classifier for the classification
of the aerial images. We validate our choice of selected features based on the
significance of the extracted features with respect to our classification
pipeline. RBFF remarkably does not involve any training of the base CNN model
except for a few parameters for the classifier, which makes the technique very
cost-effective for practical deployments. The constructed model despite being
lightweight outperforms several recently proposed models in terms of accuracy
for a number of aerial scene datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Arefeen_M/0/1/0/all/0/1"&gt;Md Adnan Arefeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nimi_S/0/1/0/all/0/1"&gt;Sumaiya Tabassum Nimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Uddin_M/0/1/0/all/0/1"&gt;Md Yusuf Sarwar Uddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEiT: BERT Pre-Training of Image Transformers. (arXiv:2106.08254v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08254</id>
        <link href="http://arxiv.org/abs/2106.08254"/>
        <updated>2021-06-16T01:21:05.844Z</updated>
        <summary type="html"><![CDATA[We introduce a self-supervised vision representation model BEiT, which stands
for Bidirectional Encoder representation from Image Transformers. Following
BERT developed in the natural language processing area, we propose a masked
image modeling task to pretrain vision Transformers. Specifically, each image
has two views in our pre-training, i.e, image patches (such as 16x16 pixels),
and visual tokens (i.e., discrete tokens). We first "tokenize" the original
image into visual tokens. Then we randomly mask some image patches and fed them
into the backbone Transformer. The pre-training objective is to recover the
original visual tokens based on the corrupted image patches. After pre-training
BEiT, we directly fine-tune the model parameters on downstream tasks by
appending task layers upon the pretrained encoder. Experimental results on
image classification and semantic segmentation show that our model achieves
competitive results with previous pre-training methods. For example, base-size
BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming
from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size
BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with
supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models
are available at https://aka.ms/beit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hangbo Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08038</id>
        <link href="http://arxiv.org/abs/2106.08038"/>
        <updated>2021-06-16T01:21:05.837Z</updated>
        <summary type="html"><![CDATA[Averaging predictions over a set of models -- an ensemble -- is widely used
to improve predictive performance and uncertainty estimation of deep learning
models. At the same time, many machine learning systems, such as search,
matching, and recommendation systems, heavily rely on embeddings.
Unfortunately, due to misalignment of features of independently trained models,
embeddings, cannot be improved with a naive deep ensemble like approach. In
this work, we look at the ensembling of representations and propose mean
embeddings with test-time augmentation (MeTTA) simple yet well-performing
recipe for ensembling representations. Empirically we demonstrate that MeTTA
significantly boosts the quality of linear evaluation on ImageNet for both
supervised and self-supervised models. Even more exciting, we draw connections
between MeTTA, image retrieval, and transformation invariant models. We believe
that spreading the success of ensembles to inference higher-quality
representations is the important step that will open many new applications of
ensembling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1"&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1"&gt;Andrei Atanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Out-of-Distribution Detection on Deep Probabilistic Generative Models. (arXiv:2106.07903v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07903</id>
        <link href="http://arxiv.org/abs/2106.07903"/>
        <updated>2021-06-16T01:21:05.830Z</updated>
        <summary type="html"><![CDATA[Out-of-distribution (OOD) detection is an important task in machine learning
systems for ensuring their reliability and safety. Deep probabilistic
generative models facilitate OOD detection by estimating the likelihood of a
data sample. However, such models frequently assign a suspiciously high
likelihood to a specific outlier. Several recent works have addressed this
issue by training a neural network with auxiliary outliers, which are generated
by perturbing the input data. In this paper, we discover that these approaches
fail for certain OOD datasets. Thus, we suggest a new detection metric that
operates without outlier exposure. We observe that our metric is robust to
diverse variations of an image compared to the previous outlier-exposing
methods. Furthermore, our proposed score requires neither auxiliary models nor
additional training. Instead, this paper utilizes the likelihood ratio
statistic in a new perspective to extract genuine properties from the given
single deep probabilistic generative model. We also apply a novel numerical
approximation to enable fast implementation. Finally, we demonstrate
comprehensive experiments on various probabilistic generative models and show
that our method achieves state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jaemoo Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1"&gt;Changyeon Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1"&gt;Jeongwoo Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Myungjoo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Total Recall in Industrial Anomaly Detection. (arXiv:2106.08265v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08265</id>
        <link href="http://arxiv.org/abs/2106.08265"/>
        <updated>2021-06-16T01:21:05.822Z</updated>
        <summary type="html"><![CDATA[Being able to spot defective parts is a critical component in large-scale
industrial manufacturing. A particular challenge that we address in this work
is the cold-start problem: fit a model using nominal (non-defective) example
images only. While handcrafted solutions per class are possible, the goal is to
build systems that work well simultaneously on many different tasks
automatically. The best peforming approaches combine embeddings from ImageNet
models with an outlier detection model. In this paper, we extend on this line
of work and propose PatchCore, which uses a maximally representative memory
bank of nominal patch-features. PatchCore offers competitive inference times
while achieving state-of-the-art performance for both detection and
localization. On the standard dataset MVTec AD, PatchCore achieves an
image-level anomaly detection AUROC score of $99.1\%$, more than halving the
error compared to the next best competitor. We further report competitive
results on two additional datasets and also find competitive results in the few
samples regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1"&gt;Karsten Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pemula_L/0/1/0/all/0/1"&gt;Latha Pemula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zepeda_J/0/1/0/all/0/1"&gt;Joaquin Zepeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1"&gt;Thomas Brox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1"&gt;Peter Gehler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object detection and Autoencoder-based 6D pose estimation for highly cluttered Bin Picking. (arXiv:2106.08045v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08045</id>
        <link href="http://arxiv.org/abs/2106.08045"/>
        <updated>2021-06-16T01:21:05.815Z</updated>
        <summary type="html"><![CDATA[Bin picking is a core problem in industrial environments and robotics, with
its main module as 6D pose estimation. However, industrial depth sensors have a
lack of accuracy when it comes to small objects. Therefore, we propose a
framework for pose estimation in highly cluttered scenes with small objects,
which mainly relies on RGB data and makes use of depth information only for
pose refinement. In this work, we compare synthetic data generation approaches
for object detection and pose estimation and introduce a pose filtering
algorithm that determines the most accurate estimated poses. We will make our]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hofer_T/0/1/0/all/0/1"&gt;Timon H&amp;#xf6;fer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamsafar_F/0/1/0/all/0/1"&gt;Faranak Shamsafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benbarka_N/0/1/0/all/0/1"&gt;Nuri Benbarka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1"&gt;Andreas Zell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated triaging of head MRI examinations using convolutional neural networks. (arXiv:2106.08176v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08176</id>
        <link href="http://arxiv.org/abs/2106.08176"/>
        <updated>2021-06-16T01:21:05.796Z</updated>
        <summary type="html"><![CDATA[The growing demand for head magnetic resonance imaging (MRI) examinations,
along with a global shortage of radiologists, has led to an increase in the
time taken to report head MRI scans around the world. For many neurological
conditions, this delay can result in increased morbidity and mortality. An
automated triaging tool could reduce reporting times for abnormal examinations
by identifying abnormalities at the time of imaging and prioritizing the
reporting of these scans. In this work, we present a convolutional neural
network for detecting clinically-relevant abnormalities in
$\text{T}_2$-weighted head MRI scans. Using a validated neuroradiology report
classifier, we generated a labelled dataset of 43,754 scans from two large UK
hospitals for model training, and demonstrate accurate classification (area
under the receiver operating curve (AUC) = 0.943) on a test set of 800 scans
labelled by a team of neuroradiologists. Importantly, when trained on scans
from only a single hospital the model generalized to scans from the other
hospital ($\Delta$AUC $\leq$ 0.02). A simulation study demonstrated that our
model would reduce the mean reporting time for abnormal examinations from 28
days to 14 days and from 9 days to 5 days at the two hospitals, demonstrating
feasibility for use in a clinical triage environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wood_D/0/1/0/all/0/1"&gt;David A. Wood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kafiabadi_S/0/1/0/all/0/1"&gt;Sina Kafiabadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Busaidi_A/0/1/0/all/0/1"&gt;Ayisha Al Busaidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guilhem_E/0/1/0/all/0/1"&gt;Emily Guilhem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Montvila_A/0/1/0/all/0/1"&gt;Antanas Montvila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Siddharth Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lynch_J/0/1/0/all/0/1"&gt;Jeremy Lynch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Townend_M/0/1/0/all/0/1"&gt;Matthew Townend&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barker_G/0/1/0/all/0/1"&gt;Gareth Barker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cole_J/0/1/0/all/0/1"&gt;James H. Cole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Booth_T/0/1/0/all/0/1"&gt;Thomas C. Booth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Photo-realistic Texture Generation for 3D Face Reconstruction. (arXiv:2106.08148v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08148</id>
        <link href="http://arxiv.org/abs/2106.08148"/>
        <updated>2021-06-16T01:21:05.789Z</updated>
        <summary type="html"><![CDATA[Although much progress has been made recently in 3D face reconstruction, most
previous work has been devoted to predicting accurate and fine-grained 3D
shapes. In contrast, relatively little work has focused on generating
high-fidelity face textures. Compared with the prosperity of photo-realistic 2D
face image generation, high-fidelity 3D face texture generation has yet to be
studied. In this paper, we proposed a novel UV map generation model that
predicts the UV map from a single face image. The model consists of a UV
sampler and a UV generator. By selectively sampling the input face image's
pixels and adjusting their relative locations, the UV sampler generates an
incomplete UV map that could faithfully reconstruct the original face. Missing
textures in the incomplete UV map are further full-filled by the UV generator.
The training is based on pseudo ground truth blended by the 3DMM texture and
the input face texture, thus weakly supervised. To deal with the artifacts in
the imperfect pseudo UV map, multiple partial UV map discriminators are
leveraged.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xiangnan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Di Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zehua Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic linear measurements of the fetal brain on MRI with deep neural networks. (arXiv:2106.08174v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08174</id>
        <link href="http://arxiv.org/abs/2106.08174"/>
        <updated>2021-06-16T01:21:05.781Z</updated>
        <summary type="html"><![CDATA[Timely, accurate and reliable assessment of fetal brain development is
essential to reduce short and long-term risks to fetus and mother. Fetal MRI is
increasingly used for fetal brain assessment. Three key biometric linear
measurements important for fetal brain evaluation are Cerebral Biparietal
Diameter (CBD), Bone Biparietal Diameter (BBD), and Trans-Cerebellum Diameter
(TCD), obtained manually by expert radiologists on reference slices, which is
time consuming and prone to human error. The aim of this study was to develop a
fully automatic method computing the CBD, BBD and TCD measurements from fetal
brain MRI. The input is fetal brain MRI volumes which may include the fetal
body and the mother's abdomen. The outputs are the measurement values and
reference slices on which the measurements were computed. The method, which
follows the manual measurements principle, consists of five stages: 1)
computation of a Region Of Interest that includes the fetal brain with an
anisotropic 3D U-Net classifier; 2) reference slice selection with a
Convolutional Neural Network; 3) slice-wise fetal brain structures segmentation
with a multiclass U-Net classifier; 4) computation of the fetal brain
midsagittal line and fetal brain orientation, and; 5) computation of the
measurements. Experimental results on 214 volumes for CBD, BBD and TCD
measurements yielded a mean $L_1$ difference of 1.55mm, 1.45mm and 1.23mm
respectively, and a Bland-Altman 95% confidence interval ($CI_{95}$) of 3.92mm,
3.98mm and 2.25mm respectively. These results are similar to the manual
inter-observer variability. The proposed automatic method for computing
biometric linear measurements of the fetal brain from MR imaging achieves human
level performance. It has the potential of being a useful method for the
assessment of fetal brain biometry in normal and pathological cases, and of
improving routine clinical practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Avisdris_N/0/1/0/all/0/1"&gt;Netanell Avisdris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yehuda_B/0/1/0/all/0/1"&gt;Bossmat Yehuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ben_Zvi_O/0/1/0/all/0/1"&gt;Ori Ben-Zvi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Link_Sourani_D/0/1/0/all/0/1"&gt;Daphna Link-Sourani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ben_Sira_L/0/1/0/all/0/1"&gt;Liat Ben-Sira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Miller_E/0/1/0/all/0/1"&gt;Elka Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zharkov_E/0/1/0/all/0/1"&gt;Elena Zharkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bashat_D/0/1/0/all/0/1"&gt;Dafna Ben Bashat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Joskowicz_L/0/1/0/all/0/1"&gt;Leo Joskowicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending Touch-based Continuous Authentication Systems from Active Adversaries Using Generative Adversarial Networks. (arXiv:2106.07867v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.07867</id>
        <link href="http://arxiv.org/abs/2106.07867"/>
        <updated>2021-06-16T01:21:05.772Z</updated>
        <summary type="html"><![CDATA[Previous studies have demonstrated that commonly studied (vanilla)
touch-based continuous authentication systems (V-TCAS) are susceptible to
population attack. This paper proposes a novel Generative Adversarial Network
assisted TCAS (G-TCAS) framework, which showed more resilience to the
population attack. G-TCAS framework was tested on a dataset of 117 users who
interacted with a smartphone and tablet pair. On average, the increase in the
false accept rates (FARs) for V-TCAS was much higher (22%) than G-TCAS (13%)
for the smartphone. Likewise, the increase in the FARs for V-TCAS was 25%
compared to G-TCAS (6%) for the tablet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_M/0/1/0/all/0/1"&gt;Mohit Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehrotra_P/0/1/0/all/0/1"&gt;Pragyan Mehrotra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Rajesh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1"&gt;Rajiv Ratn Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direction-aware Feature-level Frequency Decomposition for Single Image Deraining. (arXiv:2106.07941v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07941</id>
        <link href="http://arxiv.org/abs/2106.07941"/>
        <updated>2021-06-16T01:21:05.759Z</updated>
        <summary type="html"><![CDATA[We present a novel direction-aware feature-level frequency decomposition
network for single image deraining. Compared with existing solutions, the
proposed network has three compelling characteristics. First, unlike previous
algorithms, we propose to perform frequency decomposition at feature-level
instead of image-level, allowing both low-frequency maps containing structures
and high-frequency maps containing details to be continuously refined during
the training procedure. Second, we further establish communication channels
between low-frequency maps and high-frequency maps to interactively capture
structures from high-frequency maps and add them back to low-frequency maps
and, simultaneously, extract details from low-frequency maps and send them back
to high-frequency maps, thereby removing rain streaks while preserving more
delicate features in the input image. Third, different from existing algorithms
using convolutional filters consistent in all directions, we propose a
direction-aware filter to capture the direction of rain streaks in order to
more effectively and thoroughly purge the input images of rain streaks. We
extensively evaluate the proposed approach in three representative datasets and
experimental results corroborate our approach consistently outperforms
state-of-the-art deraining algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Sen Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yidan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1"&gt;Mingqiang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Haoran Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiping Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jonathan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao-Ping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jing Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cluster-guided Asymmetric Contrastive Learning for Unsupervised Person Re-Identification. (arXiv:2106.07846v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07846</id>
        <link href="http://arxiv.org/abs/2106.07846"/>
        <updated>2021-06-16T01:21:05.751Z</updated>
        <summary type="html"><![CDATA[Unsupervised person re-identification (Re-ID) aims to match pedestrian images
from different camera views in unsupervised setting. Existing methods for
unsupervised person Re-ID are usually built upon the pseudo labels from
clustering. However, the quality of clustering depends heavily on the quality
of the learned features, which are overwhelmingly dominated by the colors in
images especially in the unsupervised setting. In this paper, we propose a
Cluster-guided Asymmetric Contrastive Learning (CACL) approach for unsupervised
person Re-ID, in which cluster structure is leveraged to guide the feature
learning in a properly designed asymmetric contrastive learning framework. To
be specific, we propose a novel cluster-level contrastive loss to help the
siamese network effectively mine the invariance in feature learning with
respect to the cluster structure within and between different data augmentation
views, respectively. Extensive experiments conducted on three benchmark
datasets demonstrate superior performance of our proposal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mingkun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Guang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jun Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization. (arXiv:2106.07991v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.07991</id>
        <link href="http://arxiv.org/abs/2106.07991"/>
        <updated>2021-06-16T01:21:05.732Z</updated>
        <summary type="html"><![CDATA[Bi-level optimization model is able to capture a wide range of complex
learning tasks with practical interest. Due to the witnessed efficiency in
solving bi-level programs, gradient-based methods have gained popularity in the
machine learning community. In this work, we propose a new gradient-based
solution scheme, namely, the Bi-level Value-Function-based Interior-point
Method (BVFIM). Following the main idea of the log-barrier interior-point
scheme, we penalize the regularized value function of the lower level problem
into the upper level objective. By further solving a sequence of differentiable
unconstrained approximation problems, we consequently derive a sequential
programming scheme. The numerical advantage of our scheme relies on the fact
that, when gradient methods are applied to solve the approximation problem, we
successfully avoid computing any expensive Hessian-vector or Jacobian-vector
product. We prove the convergence without requiring any convexity assumption on
either the upper level or the lower level objective. Experiments demonstrate
the efficiency of the proposed BVFIM on non-convex bi-level problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Liu_R/0/1/0/all/0/1"&gt;Risheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiaoming Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shangzhi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wavelength-based Attributed Deep Neural Network for Underwater Image Restoration. (arXiv:2106.07910v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.07910</id>
        <link href="http://arxiv.org/abs/2106.07910"/>
        <updated>2021-06-16T01:21:05.725Z</updated>
        <summary type="html"><![CDATA[Underwater images, in general, suffer from low contrast and high color
distortions due to the non-uniform attenuation of the light as it propagates
through the water. In addition, the degree of attenuation varies with the
wavelength resulting in the asymmetric traversing of colors. Despite the
prolific works for underwater image restoration (UIR) using deep learning, the
above asymmetricity has not been addressed in the respective network
engineering. As the first novelty, this paper shows that attributing the right
receptive field size (context) based on the traversing range of the color
channel may lead to a substantial performance gain for the task of UIR.
Further, it is important to suppress the irrelevant multi-contextual features
and increase the representational power of the model. Therefore, as a second
novelty, we have incorporated an attentive skip mechanism to adaptively refine
the learned multi-contextual features. The proposed framework, called Deep
WaveNet, is optimized using the traditional pixel-wise and feature-based cost
functions. An extensive set of experiments have been carried out to show the
efficacy of the proposed scheme over existing best-published literature on
benchmark datasets. More importantly, we have demonstrated a comprehensive
validation of enhanced images across various high-level vision tasks, e.g.,
underwater image semantic segmentation, and diver's 2D pose estimation. A
sample video to exhibit our real-world performance is available at
\url{https://www.youtube.com/watch?v=8qtuegBdfac}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Prasen Kumar Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bisht_I/0/1/0/all/0/1"&gt;Ira Bisht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sur_A/0/1/0/all/0/1"&gt;Arijit Sur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.08208</id>
        <link href="http://arxiv.org/abs/2106.08208"/>
        <updated>2021-06-16T01:21:05.665Z</updated>
        <summary type="html"><![CDATA[Adaptive gradient methods have shown excellent performance for solving many
machine learning problems. Although multiple adaptive methods were recently
studied, they mainly focus on either empirical or theoretical aspects and also
only work for specific problems by using specific adaptive learning rates. It
is desired to design a universal framework for practical algorithms of adaptive
gradients with theoretical guarantee to solve general problems. To fill this
gap, we propose a faster and universal framework of adaptive gradients (i.e.,
SUPER-ADAM) by introducing a universal adaptive matrix that includes most
existing adaptive gradient forms. Moreover, our framework can flexibly
integrates the momentum and variance reduced techniques. In particular, our
novel framework provides the convergence analysis support for adaptive gradient
methods under the nonconvex setting. In theoretical analysis, we prove that our
new algorithm can achieve the best known complexity of
$\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point of
nonconvex optimization, which matches the lower bound for stochastic smooth
nonconvex optimization. In numerical experiments, we employ various deep
learning tasks to validate that our algorithm consistently outperforms the
existing adaptive algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Consistency Checks to Detect LiDAR Spoofing Attacks on Autonomous Vehicle Perception. (arXiv:2106.07833v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.07833</id>
        <link href="http://arxiv.org/abs/2106.07833"/>
        <updated>2021-06-16T01:21:05.654Z</updated>
        <summary type="html"><![CDATA[LiDAR sensors are used widely in Autonomous Vehicles for better perceiving
the environment which enables safer driving decisions. Recent work has
demonstrated serious LiDAR spoofing attacks with alarming consequences. In
particular, model-level LiDAR spoofing attacks aim to inject fake depth
measurements to elicit ghost objects that are erroneously detected by 3D Object
Detectors, resulting in hazardous driving decisions. In this work, we explore
the use of motion as a physical invariant of genuine objects for detecting such
attacks. Based on this, we propose a general methodology, 3D Temporal
Consistency Check (3D-TC2), which leverages spatio-temporal information from
motion prediction to verify objects detected by 3D Object Detectors. Our
preliminary design and implementation of a 3D-TC2 prototype demonstrates very
promising performance, providing more than 98% attack detection rate with a
recall of 91% for detecting spoofed Vehicle (Car) objects, and is able to
achieve real-time detection at 41Hz]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chengzeng You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hau_Z/0/1/0/all/0/1"&gt;Zhongyuan Hau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demetriou_S/0/1/0/all/0/1"&gt;Soteris Demetriou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spot the Difference: Topological Anomaly Detection via Geometric Alignment. (arXiv:2106.08233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08233</id>
        <link href="http://arxiv.org/abs/2106.08233"/>
        <updated>2021-06-16T01:21:05.635Z</updated>
        <summary type="html"><![CDATA[Geometric alignment appears in a variety of applications, ranging from domain
adaptation, optimal transport, and normalizing flows in machine learning;
optical flow and learned augmentation in computer vision and deformable
registration within biomedical imaging. A recurring challenge is the alignment
of domains whose topology is not the same; a problem that is routinely ignored,
potentially introducing bias in downstream analysis. As a first step towards
solving such alignment problems, we propose an unsupervised topological
difference detection algorithm. The model is based on a conditional variational
auto-encoder and detects topological anomalies with regards to a reference
alongside the registration step. We consider both a) topological changes in the
image under spatial variation and b) unexpected transformations. Our approach
is validated on a proxy task of unsupervised anomaly detection in images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Czolbe_S/0/1/0/all/0/1"&gt;Steffen Czolbe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1"&gt;Aasa Feragen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1"&gt;Oswin Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DFM: A Performance Baseline for Deep Feature Matching. (arXiv:2106.07791v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07791</id>
        <link href="http://arxiv.org/abs/2106.07791"/>
        <updated>2021-06-16T01:21:05.612Z</updated>
        <summary type="html"><![CDATA[A novel image matching method is proposed that utilizes learned features
extracted by an off-the-shelf deep neural network to obtain a promising
performance. The proposed method uses pre-trained VGG architecture as a feature
extractor and does not require any additional training specific to improve
matching. Inspired by well-established concepts in the psychology area, such as
the Mental Rotation paradigm, an initial warping is performed as a result of a
preliminary geometric transformation estimate. These estimates are simply based
on dense matching of nearest neighbors at the terminal layer of VGG network
outputs of the images to be matched. After this initial alignment, the same
approach is repeated again between reference and aligned images in a
hierarchical manner to reach a good localization and matching performance. Our
algorithm achieves 0.57 and 0.80 overall scores in terms of Mean Matching
Accuracy (MMA) for 1 pixel and 2 pixels thresholds respectively on Hpatches
dataset, which indicates a better performance than the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Efe_U/0/1/0/all/0/1"&gt;Ufuk Efe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ince_K/0/1/0/all/0/1"&gt;Kutalmis Gokalp Ince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1"&gt;A. Aydin Alatan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Spacecraft Dataset for Detection, Segmentation and Parts Recognition. (arXiv:2106.08186v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08186</id>
        <link href="http://arxiv.org/abs/2106.08186"/>
        <updated>2021-06-16T01:21:05.585Z</updated>
        <summary type="html"><![CDATA[Virtually all aspects of modern life depend on space technology. Thanks to
the great advancement of computer vision in general and deep learning-based
techniques in particular, over the decades, the world witnessed the growing use
of deep learning in solving problems for space applications, such as
self-driving robot, tracers, insect-like robot on cosmos and health monitoring
of spacecraft. These are just some prominent examples that has advanced space
industry with the help of deep learning. However, the success of deep learning
models requires a lot of training data in order to have decent performance,
while on the other hand, there are very limited amount of publicly available
space datasets for the training of deep learning models. Currently, there is no
public datasets for space-based object detection or instance segmentation,
partly because manually annotating object segmentation masks is very time
consuming as they require pixel-level labelling, not to mention the challenge
of obtaining images from space. In this paper, we aim to fill this gap by
releasing a dataset for spacecraft detection, instance segmentation and part
recognition. The main contribution of this work is the development of the
dataset using images of space stations and satellites, with rich annotations
including bounding boxes of spacecrafts and masks to the level of object parts,
which are obtained with a mixture of automatic processes and manual efforts. We
also provide evaluations with state-of-the-art methods in object detection and
instance segmentation as a benchmark for the dataset. The link for downloading
the proposed dataset can be found on
https://github.com/Yurushia1998/SatelliteDataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1"&gt;Dung Anh Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1"&gt;Tat-Jun Chin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07847</id>
        <link href="http://arxiv.org/abs/2106.07847"/>
        <updated>2021-06-16T01:21:05.578Z</updated>
        <summary type="html"><![CDATA[We study transfer learning in the presence of spurious correlations. We
experimentally demonstrate that directly transferring the stable feature
extractor learned on the source task may not eliminate these biases for the
target task. However, we hypothesize that the unstable features in the source
task and those in the target task are directly related. By explicitly informing
the target classifier of the source task's unstable features, we can regularize
the biases in the target task. Specifically, we derive a representation that
encodes the unstable features by contrasting different data environments in the
source task. On the target task, we cluster data from this representation, and
achieve robustness by minimizing the worst-case risk across all clusters. We
evaluate our method on both text and image classifications. Empirical results
demonstrate that our algorithm is able to maintain robustness on the target
task, outperforming the best baseline by 22.9% in absolute accuracy across 12
transfer settings. Our code is available at https://github.com/YujiaBao/Tofu.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1"&gt;Yujia Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceptually-inspired super-resolution of compressed videos. (arXiv:2106.08147v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08147</id>
        <link href="http://arxiv.org/abs/2106.08147"/>
        <updated>2021-06-16T01:21:05.563Z</updated>
        <summary type="html"><![CDATA[Spatial resolution adaptation is a technique which has often been employed in
video compression to enhance coding efficiency. This approach encodes a lower
resolution version of the input video and reconstructs the original resolution
during decoding. Instead of using conventional up-sampling filters, recent work
has employed advanced super-resolution methods based on convolutional neural
networks (CNNs) to further improve reconstruction quality. These approaches are
usually trained to minimise pixel-based losses such as Mean-Squared Error
(MSE), despite the fact that this type of loss metric does not correlate well
with subjective opinions. In this paper, a perceptually-inspired
super-resolution approach (M-SRGAN) is proposed for spatial up-sampling of
compressed video using a modified CNN model, which has been trained using a
generative adversarial network (GAN) on compressed content with perceptual loss
functions. The proposed method was integrated with HEVC HM 16.20, and has been
evaluated on the JVET Common Test Conditions (UHD test sequences) using the
Random Access configuration. The results show evident perceptual quality
improvement over the original HM 16.20, with an average bitrate saving of 35.6%
(Bj{\o}ntegaard Delta measurement) based on a perceptual quality metric, VMAF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ma_D/0/1/0/all/0/1"&gt;Di Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Afonso_M/0/1/0/all/0/1"&gt;Mariana Afonso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1"&gt;David R. Bull&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualizing Multiple Tasks via Learning to Decompose. (arXiv:2106.08112v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08112</id>
        <link href="http://arxiv.org/abs/2106.08112"/>
        <updated>2021-06-16T01:21:05.552Z</updated>
        <summary type="html"><![CDATA[One single instance could possess multiple portraits and reveal diverse
relationships with others according to different contexts. Those ambiguities
increase the difficulty of learning a generalizable model when there exists one
concept or mixed concepts in a task. We propose a general approach Learning to
Decompose Network (LeadNet) for both two cases, which contextualizes a model
through meta-learning multiple maps for concepts discovery -- the
representations of instances are decomposed and adapted conditioned on the
contexts. Through taking a holistic view over multiple latent components over
instances in a sampled pseudo task, LeadNet learns to automatically select the
right concept via incorporating those rich semantics inside and between
objects. LeadNet demonstrates its superiority in various applications,
including exploring multiple views of confusing tasks, out-of-distribution
recognition, and few-shot image classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Da-Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lanqing Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiu-Shen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Deep Morphological Networks with Neural Architecture Search. (arXiv:2106.07714v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07714</id>
        <link href="http://arxiv.org/abs/2106.07714"/>
        <updated>2021-06-16T01:21:05.545Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) are generated by sequentially performing linear
and non-linear processes. Using a combination of linear and non-linear
procedures is critical for generating a sufficiently deep feature space. The
majority of non-linear operators are derivations of activation functions or
pooling functions. Mathematical morphology is a branch of mathematics that
provides non-linear operators for a variety of image processing problems. We
investigate the utility of integrating these operations in an end-to-end deep
learning framework in this paper. DNNs are designed to acquire a realistic
representation for a particular job. Morphological operators give topological
descriptors that convey salient information about the shapes of objects
depicted in images. We propose a method based on meta-learning to incorporate
morphological operators into DNNs. The learned architecture demonstrates how
our novel morphological operations significantly increase DNN performance on
various tasks, including picture classification and edge detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yufei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belkhir_N/0/1/0/all/0/1"&gt;Nacim Belkhir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angulo_J/0/1/0/all/0/1"&gt;Jesus Angulo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1"&gt;Angela Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1"&gt;Gianni Franchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Canonical Face Embeddings. (arXiv:2106.07822v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07822</id>
        <link href="http://arxiv.org/abs/2106.07822"/>
        <updated>2021-06-16T01:21:05.524Z</updated>
        <summary type="html"><![CDATA[We present evidence that many common convolutional neural networks (CNNs)
trained for face verification learn functions that are nearly equivalent under
rotation. More specifically, we demonstrate that one face verification model's
embeddings (i.e. last--layer activations) can be compared directly to another
model's embeddings after only a rotation or linear transformation, with little
performance penalty. This finding is demonstrated using IJB-C 1:1 verification
across the combinations of ten modern off-the-shelf CNN-based face verification
models which vary in training dataset, CNN architecture, way of using angular
loss, or some combination of the 3, and achieve a mean true accept rate of 0.96
at a false accept rate of 0.01. When instead evaluating embeddings generated
from two CNNs, where one CNN's embeddings are mapped with a linear
transformation, the mean true accept rate drops to 0.95 using the same
verification paradigm. Restricting these linear maps to only perform rotation
produces a mean true accept rate of 0.91. These mappings' existence suggests
that a common representation is learned by models with variation in training or
structure. A discovery such as this likely has broad implications, and we
provide an application in which face embeddings can be de-anonymized using a
limited number of samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McNeely_White_D/0/1/0/all/0/1"&gt;David McNeely-White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sattelberg_B/0/1/0/all/0/1"&gt;Ben Sattelberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanchard_N/0/1/0/all/0/1"&gt;Nathaniel Blanchard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beveridge_R/0/1/0/all/0/1"&gt;Ross Beveridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Learning of Keypoint Representations for Continuous Control from Images. (arXiv:2106.07995v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07995</id>
        <link href="http://arxiv.org/abs/2106.07995"/>
        <updated>2021-06-16T01:21:05.477Z</updated>
        <summary type="html"><![CDATA[In many control problems that include vision, optimal controls can be
inferred from the location of the objects in the scene. This information can be
represented using keypoints, which is a list of spatial locations in the input
image. Previous works show that keypoint representations learned during
unsupervised pre-training using encoder-decoder architectures can provide good
features for control tasks. In this paper, we show that it is possible to learn
efficient keypoint representations end-to-end, without the need for
unsupervised pre-training, decoders, or additional losses. Our proposed
architecture consists of a differentiable keypoint extractor that feeds the
coordinates of the estimated keypoints directly to a soft actor-critic agent.
The proposed algorithm yields performance competitive to the state-of-the art
on DeepMind Control Suite tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boney_R/0/1/0/all/0/1"&gt;Rinu Boney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1"&gt;Alexander Ilin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1"&gt;Juho Kannala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascading Convolutional Temporal Colour Constancy. (arXiv:2106.07955v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07955</id>
        <link href="http://arxiv.org/abs/2106.07955"/>
        <updated>2021-06-16T01:21:05.462Z</updated>
        <summary type="html"><![CDATA[Computational Colour Constancy (CCC) consists of estimating the colour of one
or more illuminants in a scene and using them to remove unwanted chromatic
distortions. Much research has focused on illuminant estimation for CCC on
single images, with few attempts of leveraging the temporal information
intrinsic in sequences of correlated images (e.g., the frames in a video), a
task known as Temporal Colour Constancy (TCC). The state-of-the-art for TCC is
TCCNet, a deep-learning architecture that uses a ConvLSTM for aggregating the
encodings produced by CNN submodules for each image in a sequence. We extend
this architecture with different models obtained by (i) substituting the TCCNet
submodules with C4, the state-of-the-art method for CCC targeting images; (ii)
adding a cascading strategy to perform an iterative improvement of the estimate
of the illuminant. We tested our models on the recently released TCC benchmark
and achieved results that surpass the state-of-the-art. Analyzing the impact of
the number of frames involved in illuminant estimation on performance, we show
that it is possible to reduce inference time by training the models on few
selected frames from the sequences while retaining comparable accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rizzo_M/0/1/0/all/0/1"&gt;Matteo Rizzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conati_C/0/1/0/all/0/1"&gt;Cristina Conati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1"&gt;Daesik Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hui Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with Unlabeled Data. (arXiv:2106.07807v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07807</id>
        <link href="http://arxiv.org/abs/2106.07807"/>
        <updated>2021-06-16T01:21:05.443Z</updated>
        <summary type="html"><![CDATA[Most existing works in few-shot learning rely on meta-learning the network on
a large base dataset which is typically from the same domain as the target
dataset. We tackle the problem of cross-domain few-shot learning where there is
a large shift between the base and target domain. The problem of cross-domain
few-shot recognition with unlabeled target data is largely unaddressed in the
literature. STARTUP was the first method that tackles this problem using
self-training. However, it uses a fixed teacher pretrained on a labeled base
dataset to create soft labels for the unlabeled target samples. As the base
dataset and unlabeled dataset are from different domains, projecting the target
images in the class-domain of the base dataset with a fixed pretrained model
might be sub-optimal. We propose a simple dynamic distillation-based approach
to facilitate unlabeled images from the novel/base dataset. We impose
consistency regularization by calculating predictions from the weakly-augmented
versions of the unlabeled images from a teacher network and matching it with
the strongly augmented versions of the same images from a student network. The
parameters of the teacher network are updated as exponential moving average of
the parameters of the student network. We show that the proposed network learns
representation that can be easily adapted to the target domain even though it
has not been trained with target-specific classes during the pretraining phase.
Our model outperforms the current state-of-the art method by 4.4% for 1-shot
and 3.6% for 5-shot classification in the BSCD-FSL benchmark, and also shows
competitive performance on traditional in-domain few-shot learning task. Our
code will be available at: https://github.com/asrafulashiq/dynamic-cdfsl.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1"&gt;Ashraful Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun-Fu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1"&gt;Rameswar Panda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1"&gt;Leonid Karlinsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radke_R/0/1/0/all/0/1"&gt;Richard J. Radke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Gradient Manifold Neural Network. (arXiv:2106.07905v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07905</id>
        <link href="http://arxiv.org/abs/2106.07905"/>
        <updated>2021-06-16T01:21:05.436Z</updated>
        <summary type="html"><![CDATA[Deep neural network (DNN) generally takes thousands of iterations to optimize
via gradient descent and thus has a slow convergence. In addition, softmax, as
a decision layer, may ignore the distribution information of the data during
classification. Aiming to tackle the referred problems, we propose a novel
manifold neural network based on non-gradient optimization, i.e., the
closed-form solutions. Considering that the activation function is generally
invertible, we reconstruct the network via forward ridge regression and low
rank backward approximation, which achieve the rapid convergence. Moreover, by
unifying the flexible Stiefel manifold and adaptive support vector machine, we
devise the novel decision layer which efficiently fits the manifold structure
of the data and label information. Consequently, a jointly non-gradient
optimization method is designed to generate the network with closed-form
results. Eventually, extensive experiments validate the superior performance of
the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Z/0/1/0/all/0/1"&gt;Ziheng Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuelong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with Adaptive Hybrid Label. (arXiv:2106.08073v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08073</id>
        <link href="http://arxiv.org/abs/2106.08073"/>
        <updated>2021-06-16T01:21:05.429Z</updated>
        <summary type="html"><![CDATA[Unmanned aerial vehicle (UAV) based visual tracking has been confronted with
numerous challenges, e.g., object motion and occlusion. These challenges
generally introduce unexpected mutations of target appearance and result in
tracking failure. However, prevalent discriminative correlation filter (DCF)
based trackers are insensitive to target mutations due to a predefined label,
which concentrates on merely the centre of the training region. Meanwhile,
appearance mutations caused by occlusion or similar objects usually lead to the
inevitable learning of wrong information. To cope with appearance mutations,
this paper proposes a novel DCF-based method to enhance the sensitivity and
resistance to mutations with an adaptive hybrid label, i.e., MSCF. The ideal
label is optimized jointly with the correlation filter and remains temporal
consistency. Besides, a novel measurement of mutations called mutation threat
factor (MTF) is applied to correct the label dynamically. Considerable
experiments are conducted on widely used UAV benchmarks. The results indicate
that the performance of MSCF tracker surpasses other 26 state-of-the-art
DCF-based and deep-based trackers. With a real-time speed of _38 frames/s, the
proposed approach is sufficient for UAV tracking commissions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guangze Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Junjie Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fuling Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1"&gt;Fangqiang Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Clinically Inspired Approach for Melanoma classification. (arXiv:2106.08021v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08021</id>
        <link href="http://arxiv.org/abs/2106.08021"/>
        <updated>2021-06-16T01:21:05.404Z</updated>
        <summary type="html"><![CDATA[Melanoma is a leading cause of deaths due to skin cancer deaths and hence,
early and effective diagnosis of melanoma is of interest. Current approaches
for automated diagnosis of melanoma either use pattern recognition or
analytical recognition like ABCDE (asymmetry, border, color, diameter and
evolving) criterion. In practice however, a differential approach wherein
outliers (ugly duckling) are detected and used to evaluate nevi/lesions.
Incorporation of differential recognition in Computer Aided Diagnosis (CAD)
systems has not been explored but can be beneficial as it can provide a
clinical justification for the derived decision. We present a method for
identifying and quantifying ugly ducklings by performing Intra-Patient
Comparative Analysis (IPCA) of neighboring nevi. This is then incorporated in a
CAD system design for melanoma detection. This design ensures flexibility to
handle cases where IPCA is not possible. Our experiments on a public dataset
show that the outlier information helps boost the sensitivity of detection by
at least 4.1 % and specificity by 4.0 % to 8.9 %, depending on the use of a
strong (EfficientNet) or moderately strong (VGG or ResNet) classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akundi_P/0/1/0/all/0/1"&gt;Prathyusha Akundi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gun_S/0/1/0/all/0/1"&gt;Soumyasis Gun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivaswamy_J/0/1/0/all/0/1"&gt;Jayanthi Sivaswamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Sketch Search. (arXiv:2106.08009v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08009</id>
        <link href="http://arxiv.org/abs/2106.08009"/>
        <updated>2021-06-16T01:21:05.395Z</updated>
        <summary type="html"><![CDATA[We present an algorithm for searching image collections using free-hand
sketches that describe the appearance and relative positions of multiple
objects. Sketch based image retrieval (SBIR) methods predominantly match
queries containing a single, dominant object invariant to its position within
an image. Our work exploits drawings as a concise and intuitive representation
for specifying entire scene compositions. We train a convolutional neural
network (CNN) to encode masked visual features from sketched objects, pooling
these into a spatial descriptor encoding the spatial relationships and
appearances of objects in the composition. Training the CNN backbone as a
Siamese network under triplet loss yields a metric search embedding for
measuring compositional similarity which may be efficiently leveraged for
visual search by applying product quantization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1"&gt;Alexander Black&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Tu Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1"&gt;Long Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hailin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1"&gt;John Collomosse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hotel Recognition via Latent Image Embedding. (arXiv:2106.08042v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08042</id>
        <link href="http://arxiv.org/abs/2106.08042"/>
        <updated>2021-06-16T01:21:05.385Z</updated>
        <summary type="html"><![CDATA[We approach the problem of hotel recognition with deep metric learning. We
overview the existing approaches and propose a modification to Contrastive loss
called Contrastive-Triplet loss. We construct a robust pipeline for
benchmarking metric learning models and perform experiments on Hotels-50K and
CUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval
on Hotels-50k. We open-source our code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tseytlin_B/0/1/0/all/0/1"&gt;Boris Tseytlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1"&gt;Ilya Makarov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EuroCrops: A Pan-European Dataset for Time Series Crop Type Classification. (arXiv:2106.08151v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08151</id>
        <link href="http://arxiv.org/abs/2106.08151"/>
        <updated>2021-06-16T01:21:05.378Z</updated>
        <summary type="html"><![CDATA[We present EuroCrops, a dataset based on self-declared field annotations for
training and evaluating methods for crop type classification and mapping,
together with its process of acquisition and harmonisation. By this, we aim to
enrich the research efforts and discussion for data-driven land cover
classification via Earth observation and remote sensing. Additionally, through
inclusion of self-declarations gathered in the scope of subsidy control from
all countries of the European Union (EU), this dataset highlights the
difficulties and pitfalls one comes across when operating on a transnational
level. We, therefore, also introduce a new taxonomy scheme, HCAT-ID, that
aspires to capture all the aspects of reference data originating from
administrative and agency databases. To address researchers from both the
remote sensing and the computer vision and machine learning communities, we
publish the dataset in different formats and processing levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schneider_M/0/1/0/all/0/1"&gt;Maja Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Broszeit_A/0/1/0/all/0/1"&gt;Amelie Broszeit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korner_M/0/1/0/all/0/1"&gt;Marco K&amp;#xf6;rner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Facial Expression Analysis For Dimensional Affect Recognition Using Geometric Features. (arXiv:2106.07817v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07817</id>
        <link href="http://arxiv.org/abs/2106.07817"/>
        <updated>2021-06-16T01:21:05.366Z</updated>
        <summary type="html"><![CDATA[Despite their continued popularity, categorical approaches to affect
recognition have limitations, especially in real-life situations. Dimensional
models of affect offer important advantages for the recognition of subtle
expressions and more fine-grained analysis. We introduce a simple but effective
facial expression analysis (FEA) system for dimensional affect, solely based on
geometric features and Partial Least Squares (PLS) regression. The system
jointly learns to estimate Arousal and Valence ratings from a set of facial
images. The proposed approach is robust, efficient, and exhibits comparable
performance to contemporary deep learning models, while requiring a fraction of
the computational resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vonikakis_V/0/1/0/all/0/1"&gt;Vassilios Vonikakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1"&gt;Stefan Winkler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cine-MRI detection of abdominal adhesions with spatio-temporal deep learning. (arXiv:2106.08094v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08094</id>
        <link href="http://arxiv.org/abs/2106.08094"/>
        <updated>2021-06-16T01:21:05.343Z</updated>
        <summary type="html"><![CDATA[Adhesions are an important cause of chronic pain following abdominal surgery.
Recent developments in abdominal cine-MRI have enabled the non-invasive
diagnosis of adhesions. Adhesions are identified on cine-MRI by the absence of
sliding motion during movement. Diagnosis and mapping of adhesions improves the
management of patients with pain. Detection of abdominal adhesions on cine-MRI
is challenging from both a radiological and deep learning perspective. We focus
on classifying presence or absence of adhesions in sagittal abdominal cine-MRI
series. We experimented with spatio-temporal deep learning architectures
centered around a ConvGRU architecture. A hybrid architecture comprising a
ResNet followed by a ConvGRU model allows to classify a whole time-series.
Compared to a stand-alone ResNet with a two time-point (inspiration/expiration)
input, we show an increase in classification performance (AUROC) from 0.74 to
0.83 ($p<0.05$). Our full temporal classification approach adds only a small
amount (5%) of parameters to the entire architecture, which may be useful for
other medical imaging problems with a temporal dimension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wilde_B/0/1/0/all/0/1"&gt;Bram de Wilde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Broek_R/0/1/0/all/0/1"&gt;Richard P. G. ten Broek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huisman_H/0/1/0/all/0/1"&gt;Henkjan Huisman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer-aided Interpretable Features for Leaf Image Classification. (arXiv:2106.08077v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08077</id>
        <link href="http://arxiv.org/abs/2106.08077"/>
        <updated>2021-06-16T01:21:05.331Z</updated>
        <summary type="html"><![CDATA[Plant species identification is time consuming, costly, and requires lots of
efforts, and expertise knowledge. In recent, many researchers use deep learning
methods to classify plants directly using plant images. While deep learning
models have achieved a great success, the lack of interpretability limit their
widespread application. To overcome this, we explore the use of interpretable,
measurable and computer-aided features extracted from plant leaf images. Image
processing is one of the most challenging, and crucial steps in
feature-extraction. The purpose of image processing is to improve the leaf
image by removing undesired distortion. The main image processing steps of our
algorithm involves: i) Convert original image to RGB (Red-Green-Blue) image,
ii) Gray scaling, iii) Gaussian smoothing, iv) Binary thresholding, v) Remove
stalk, vi) Closing holes, and vii) Resize image. The next step after image
processing is to extract features from plant leaf images. We introduced 52
computationally efficient features to classify plant species. These features
are mainly classified into four groups as: i) shape-based features, ii)
color-based features, iii) texture-based features, and iv) scagnostic features.
Length, width, area, texture correlation, monotonicity and scagnostics are to
name few of them. We explore the ability of features to discriminate the
classes of interest under supervised learning and unsupervised learning
settings. For that, supervised dimensionality reduction technique, Linear
Discriminant Analysis (LDA), and unsupervised dimensionality reduction
technique, Principal Component Analysis (PCA) are used to convert and visualize
the images from digital-image space to feature space. The results show that the
features are sufficient to discriminate the classes of interest under both
supervised and unsupervised learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lakshika_J/0/1/0/all/0/1"&gt;Jayani P. G. Lakshika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talagala_T/0/1/0/all/0/1"&gt;Thiyanga S. Talagala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Pose and Shape Reconstruction of Two Interacting Hands With a Single Depth Camera. (arXiv:2106.08059v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08059</id>
        <link href="http://arxiv.org/abs/2106.08059"/>
        <updated>2021-06-16T01:21:05.322Z</updated>
        <summary type="html"><![CDATA[We present a novel method for real-time pose and shape reconstruction of two
strongly interacting hands. Our approach is the first two-hand tracking
solution that combines an extensive list of favorable properties, namely it is
marker-less, uses a single consumer-level depth camera, runs in real time,
handles inter- and intra-hand collisions, and automatically adjusts to the
user's hand shape. In order to achieve this, we embed a recent parametric hand
pose and shape model and a dense correspondence predictor based on a deep
neural network into a suitable energy minimization framework. For training the
correspondence prediction network, we synthesize a two-hand dataset based on
physical simulations that includes both hand pose and shape annotations while
at the same time avoiding inter-hand penetrations. To achieve real-time rates,
we phrase the model fitting in terms of a nonlinear least-squares problem so
that the energy can be optimized based on a highly efficient GPU-based
Gauss-Newton optimizer. We show state-of-the-art results in scenes that exceed
the complexity level demonstrated by previous work, including tight two-hand
grasps, significant inter-hand occlusions, and gesture interaction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1"&gt;Franziska Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_M/0/1/0/all/0/1"&gt;Micah Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1"&gt;Florian Bernard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotnychenko_O/0/1/0/all/0/1"&gt;Oleksandr Sotnychenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verschoor_M/0/1/0/all/0/1"&gt;Mickeal Verschoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otaduy_M/0/1/0/all/0/1"&gt;Miguel A. Otaduy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1"&gt;Dan Casas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReS2tAC -- UAV-Borne Real-Time SGM Stereo Optimized for Embedded ARM and CUDA Devices. (arXiv:2106.07927v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07927</id>
        <link href="http://arxiv.org/abs/2106.07927"/>
        <updated>2021-06-16T01:21:05.250Z</updated>
        <summary type="html"><![CDATA[With the emergence of low-cost robotic systems, such as unmanned aerial
vehicle, the importance of embedded high-performance image processing has
increased. For a long time, FPGAs were the only processing hardware that were
capable of high-performance computing, while at the same time preserving a low
power consumption, essential for embedded systems. However, the recently
increasing availability of embedded GPU-based systems, such as the NVIDIA
Jetson series, comprised of an ARM CPU and a NVIDIA Tegra GPU, allows for
massively parallel embedded computing on graphics hardware. With this in mind,
we propose an approach for real-time embedded stereo processing on ARM and
CUDA-enabled devices, which is based on the popular and widely used Semi-Global
Matching algorithm. In this, we propose an optimization of the algorithm for
embedded CUDA GPUs, by using massively parallel computing, as well as using the
NEON intrinsics to optimize the algorithm for vectorized SIMD processing on
embedded ARM CPUs. We have evaluated our approach with different configurations
on two public stereo benchmark datasets to demonstrate that they can reach an
error rate as low as 3.3%. Furthermore, our experiments show that the fastest
configuration of our approach reaches up to 46 FPS on VGA image resolution.
Finally, in a use-case specific qualitative evaluation, we have evaluated the
power consumption of our approach and deployed it on the DJI Manifold 2-G
attached to a DJI Matrix 210v2 RTK unmanned aerial vehicle (UAV), demonstrating
its suitability for real-time stereo processing onboard a UAV.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruf_B/0/1/0/all/0/1"&gt;Boitumelo Ruf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohrs_J/0/1/0/all/0/1"&gt;Jonas Mohrs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1"&gt;Martin Weinmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hinz_S/0/1/0/all/0/1"&gt;Stefan Hinz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyerer_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Beyerer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keep CALM and Improve Visual Feature Attribution. (arXiv:2106.07861v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07861</id>
        <link href="http://arxiv.org/abs/2106.07861"/>
        <updated>2021-06-16T01:21:05.162Z</updated>
        <summary type="html"><![CDATA[The class activation mapping, or CAM, has been the cornerstone of feature
attribution methods for multiple vision tasks. Its simplicity and effectiveness
have led to wide applications in the explanation of visual predictions and
weakly-supervised localization tasks. However, CAM has its own shortcomings.
The computation of attribution maps relies on ad-hoc calibration steps that are
not part of the training computational graph, making it difficult for us to
understand the real meaning of the attribution values. In this paper, we
improve CAM by explicitly incorporating a latent variable encoding the location
of the cue for recognition in the formulation, thereby subsuming the
attribution map into the training computational graph. The resulting model,
class activation latent mapping, or CALM, is trained with the
expectation-maximization algorithm. Our experiments show that CALM identifies
discriminative attributes for image classifiers more accurately than CAM and
other visual attribution baselines. CALM also shows performance improvements
over prior arts on the weakly-supervised object localization benchmarks. Our
code is available at https://github.com/naver-ai/calm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jae Myung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1"&gt;Junsuk Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Seong Joon Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision-Language Navigation with Random Environmental Mixup. (arXiv:2106.07876v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07876</id>
        <link href="http://arxiv.org/abs/2106.07876"/>
        <updated>2021-06-16T01:21:05.154Z</updated>
        <summary type="html"><![CDATA[Vision-language Navigation (VLN) tasks require an agent to navigate
step-by-step while perceiving the visual observations and comprehending a
natural language instruction. Large data bias, which is caused by the disparity
ratio between the small data scale and large navigation space, makes the VLN
task challenging. Previous works have proposed various data augmentation
methods to reduce data bias. However, these works do not explicitly reduce the
data bias across different house scenes. Therefore, the agent would overfit to
the seen scenes and achieve poor navigation performance in the unseen scenes.
To tackle this problem, we propose the Random Environmental Mixup (REM) method,
which generates cross-connected house scenes as augmented data via mixuping
environment. Specifically, we first select key viewpoints according to the room
connection graph for each scene. Then, we cross-connect the key views of
different scenes to construct augmented scenes. Finally, we generate augmented
instruction-path pairs in the cross-connected scenes. The experimental results
on benchmark datasets demonstrate that our augmentation data via REM help the
agent reduce its performance gap between the seen and unseen environment and
improve the overall performance, making our model the best existing approach on
the standard VLN benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Fengda Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xiaojun Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yi-Dong Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Neural Tangent Kernels via Sketching and Random Features. (arXiv:2106.07880v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07880</id>
        <link href="http://arxiv.org/abs/2106.07880"/>
        <updated>2021-06-16T01:21:05.118Z</updated>
        <summary type="html"><![CDATA[The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely-wide
neural networks trained under least squares loss by gradient descent. Recent
works also report that NTK regression can outperform finitely-wide neural
networks trained on small-scale datasets. However, the computational complexity
of kernel methods has limited its use in large-scale learning tasks. To
accelerate learning with NTK, we design a near input-sparsity time
approximation algorithm for NTK, by sketching the polynomial expansions of
arc-cosine kernels: our sketch for the convolutional counterpart of NTK (CNTK)
can transform any image using a linear runtime in the number of pixels.
Furthermore, we prove a spectral approximation guarantee for the NTK matrix, by
combining random features (based on leverage score sampling) of the arc-cosine
kernels with a sketching algorithm. We benchmark our methods on various
large-scale regression and classification tasks and show that a linear
regressor trained on our CNTK features matches the accuracy of exact CNTK on
CIFAR-10 dataset while achieving 150x speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zandieh_A/0/1/0/all/0/1"&gt;Amir Zandieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_I/0/1/0/all/0/1"&gt;Insu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avron_H/0/1/0/all/0/1"&gt;Haim Avron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoham_N/0/1/0/all/0/1"&gt;Neta Shoham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1"&gt;Chaewon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptive SiamRPN++ for Object Tracking in the Wild. (arXiv:2106.07862v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07862</id>
        <link href="http://arxiv.org/abs/2106.07862"/>
        <updated>2021-06-16T01:21:05.109Z</updated>
        <summary type="html"><![CDATA[Benefit from large-scale training data, recent advances in Siamese-based
object tracking have achieved compelling results on the normal sequences.
Whilst Siamese-based trackers assume training and test data follow an identical
distribution. Suppose there is a set of foggy or rainy test sequences, it
cannot be guaranteed that the trackers trained on the normal images perform
well on the data belonging to other domains. The problem of domain shift among
training and test data has already been discussed in object detection and
semantic segmentation areas, which, however, has not been investigated for
visual tracking. To this end, based on SiamRPN++, we introduce a Domain
Adaptive SiamRPN++, namely DASiamRPN++, to improve the cross-domain
transferability and robustness of a tracker. Inspired by A-distance theory, we
present two domain adaptive modules, Pixel Domain Adaptation (PDA) and Semantic
Domain Adaptation (SDA). The PDA module aligns the feature maps of template and
search region images to eliminate the pixel-level domain shift caused by
weather, illumination, etc. The SDA module aligns the feature representations
of the tracking target's appearance to eliminate the semantic-level domain
shift. PDA and SDA modules reduce the domain disparity by learning domain
classifiers in an adversarial training manner. The domain classifiers enforce
the network to learn domain-invariant feature representations. Extensive
experiments are performed on the standard datasets of two different domains,
including synthetic foggy and TIR sequences, which demonstrate the
transferability and domain adaptability of the proposed tracker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhongzhou Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CathAI: Fully Automated Interpretation of Coronary Angiograms Using Neural Networks. (arXiv:2106.07708v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07708</id>
        <link href="http://arxiv.org/abs/2106.07708"/>
        <updated>2021-06-16T01:21:05.088Z</updated>
        <summary type="html"><![CDATA[Coronary heart disease (CHD) is the leading cause of adult death in the
United States and worldwide, and for which the coronary angiography procedure
is the primary gateway for diagnosis and clinical management decisions. The
standard-of-care for interpretation of coronary angiograms depends upon ad-hoc
visual assessment by the physician operator. However, ad-hoc visual
interpretation of angiograms is poorly reproducible, highly variable and bias
prone. Here we show for the first time that fully-automated angiogram
interpretation to estimate coronary artery stenosis is possible using a
sequence of deep neural network algorithms. The algorithmic pipeline we
developed--called CathAI--achieves state-of-the art performance across the
sequence of tasks required to accomplish automated interpretation of
unselected, real-world angiograms. CathAI (Algorithms 1-2) demonstrated
positive predictive value, sensitivity and F1 score of >=90% to identify the
projection angle overall and >=93% for left or right coronary artery angiogram
detection, the primary anatomic structures of interest. To predict obstructive
coronary artery stenosis (>=70% stenosis), CathAI (Algorithm 4) exhibited an
area under the receiver operating characteristic curve (AUC) of 0.862 (95% CI:
0.843-0.880). When externally validated in a healthcare system in another
country, CathAI AUC was 0.869 (95% CI: 0.830-0.907) to predict obstructive
coronary artery stenosis. Our results demonstrate that multiple purpose-built
neural networks can function in sequence to accomplish the complex series of
tasks required for automated analysis of real-world angiograms. Deployment of
CathAI may serve to increase standardization and reproducibility in coronary
stenosis assessment, while providing a robust foundation to accomplish future
tasks for algorithmic angiographic interpretation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avram_R/0/1/0/all/0/1"&gt;Robert Avram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olgin_J/0/1/0/all/0/1"&gt;Jeffrey E. Olgin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_A/0/1/0/all/0/1"&gt;Alvin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1"&gt;Zeeshan Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verreault_Julien_L/0/1/0/all/0/1"&gt;Louis Verreault-Julien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abreau_S/0/1/0/all/0/1"&gt;Sean Abreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1"&gt;Derek Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1"&gt;Derek Y. So&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soni_K/0/1/0/all/0/1"&gt;Krishan Soni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tison_G/0/1/0/all/0/1"&gt;Geoffrey H. Tison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[G$^2$DA: Geometry-Guided Dual-Alignment Learning for RGB-Infrared Person Re-Identification. (arXiv:2106.07853v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07853</id>
        <link href="http://arxiv.org/abs/2106.07853"/>
        <updated>2021-06-16T01:21:05.074Z</updated>
        <summary type="html"><![CDATA[RGB-Infrared (IR) person re-identification aims to retrieve
person-of-interest between heterogeneous modalities, suffering from large
modality discrepancy caused by different sensory devices. Existing methods
mainly focus on global-level modality alignment, whereas neglect sample-level
modality divergence to some extent, leading to performance degradation. This
paper attempts to find RGB-IR ReID solutions from tackling sample-level
modality difference, and presents a Geometry-Guided Dual-Alignment learning
framework (G$^2$DA), which jointly enhances modality-invariance and reinforces
discriminability with human topological structure in features to boost the
overall matching performance. Specifically, G$^2$DA extracts accurate body part
features with a pose estimator, serving as a semantic bridge complementing the
missing local details in global descriptor. Based on extracted local and global
features, a novel distribution constraint derived from optimal transport is
introduced to mitigate the modality gap in a fine-grained sample-level manner.
Beyond pair-wise relations across two modalities, it additionally measures the
structural similarity of different parts, thus both multi-level features and
their relations are kept consistent in the common feature space. Considering
the inherent human-topology information, we further advance a geometry-guided
graph learning module to refine each part features, where relevant regions can
be emphasized while meaningless ones are suppressed, effectively facilitating
robust feature learning. Extensive experiments on two standard benchmark
datasets validate the superiority of our proposed method, yielding competitive
performance over the state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1"&gt;Lin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zongyuan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_Q/0/1/0/all/0/1"&gt;Qianyan Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yehansen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Lijing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhihang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Feature Information Extraction for Interest Point Detection: A Comprehensive Review. (arXiv:2106.07929v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07929</id>
        <link href="http://arxiv.org/abs/2106.07929"/>
        <updated>2021-06-16T01:21:05.064Z</updated>
        <summary type="html"><![CDATA[Interest point detection is one of the most fundamental and critical problems
in computer vision and image processing. In this paper, we carry out a
comprehensive review on image feature information (IFI) extraction techniques
for interest point detection. To systematically introduce how the existing
interest point detection methods extract IFI from an input image, we propose a
taxonomy of the IFI extraction techniques for interest point detection.
According to this taxonomy, we discuss different types of IFI extraction
techniques for interest point detection. Furthermore, we identify the main
unresolved issues related to the existing IFI extraction techniques for
interest point detection and any interest point detection methods that have not
been discussed before. The existing popular datasets and evaluation standards
are provided and the performances for eighteen state-of-the-art approaches are
evaluated and discussed. Moreover, future research directions on IFI extraction
techniques for interest point detection are elaborated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jing_J/0/1/0/all/0/1"&gt;Junfeng Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1"&gt;Tian Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weichuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yongsheng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changming Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAR Image Classification Based on Spiking Neural Network through Spike-Time Dependent Plasticity and Gradient Descent. (arXiv:2106.08005v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08005</id>
        <link href="http://arxiv.org/abs/2106.08005"/>
        <updated>2021-06-16T01:21:05.053Z</updated>
        <summary type="html"><![CDATA[At present, the Synthetic Aperture Radar (SAR) image classification method
based on convolution neural network (CNN) has faced some problems such as poor
noise resistance and generalization ability. Spiking neural network (SNN) is
one of the core components of brain-like intelligence and has good application
prospects. This article constructs a complete SAR image classifier based on
unsupervised and supervised learning of SNN by using spike sequences with
complex spatio-temporal information. We firstly expound the spiking neuron
model, the receptive field of SNN, and the construction of spike sequence. Then
we put forward an unsupervised learning algorithm based on STDP and a
supervised learning algorithm based on gradient descent. The average
classification accuracy of single layer and bilayer unsupervised learning SNN
in three categories images on MSTAR dataset is 80.8\% and 85.1\%, respectively.
Furthermore, the convergent output spike sequences of unsupervised learning can
be used as teaching signals. Based on the TensorFlow framework, a single layer
supervised learning SNN is built from the bottom, and the classification
accuracy reaches 90.05\%. By comparing noise resistance and model parameters
between SNNs and CNNs, the effectiveness and outstanding advantages of SNN are
verified. Code to reproduce our experiments is available at
\url{https://github.com/Jiankun-chen/Supervised-SNN-with-GD}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiankun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xiaolan Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Chibiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yirong Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hybrid mmWave and Camera System for Long-Range Depth Imaging. (arXiv:2106.07856v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07856</id>
        <link href="http://arxiv.org/abs/2106.07856"/>
        <updated>2021-06-16T01:21:05.043Z</updated>
        <summary type="html"><![CDATA[mmWave radars offer excellent depth resolution owing to their high bandwidth
at mmWave radio frequencies. Yet, they suffer intrinsically from poor angular
resolution, that is an order-of-magnitude worse than camera systems, and are
therefore not a capable 3-D imaging solution in isolation. We propose
Metamoran, a system that combines the complimentary strengths of radar and
camera systems to obtain depth images at high azimuthal resolutions at
distances of several tens of meters with high accuracy, all from a single fixed
vantage point. Metamoran enables rich long-range depth imaging outdoors with
applications to roadside safety infrastructure, surveillance and wide-area
mapping. Our key insight is to use the high azimuth resolution from cameras
using computer vision techniques, including image segmentation and monocular
depth estimation, to obtain object shapes and use these as priors for our novel
specular beamforming algorithm. We also design this algorithm to work in
cluttered environments with weak reflections and in partially occluded
scenarios. We perform a detailed evaluation of Metamoran's depth imaging and
sensing capabilities in 200 diverse scenes at a major U.S. city. Our evaluation
shows that Metamoran estimates the depth of an object up to 60~m away with a
median error of 28~cm, an improvement of 13$\times$ compared to a naive
radar+camera baseline and 23$\times$ compared to monocular depth estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Diana Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prabhakara_A/0/1/0/all/0/1"&gt;Akarsh Prabhakara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munir_S/0/1/0/all/0/1"&gt;Sirajum Munir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1"&gt;Aswin Sankaranarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Swarun Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face Age Progression With Attribute Manipulation. (arXiv:2106.07696v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07696</id>
        <link href="http://arxiv.org/abs/2106.07696"/>
        <updated>2021-06-16T01:21:04.986Z</updated>
        <summary type="html"><![CDATA[Face is one of the predominant means of person recognition. In the process of
ageing, human face is prone to many factors such as time, attributes, weather
and other subject specific variations. The impact of these factors were not
well studied in the literature of face aging. In this paper, we propose a novel
holistic model in this regard viz., ``Face Age progression With Attribute
Manipulation (FAWAM)", i.e. generating face images at different ages while
simultaneously varying attributes and other subject specific characteristics.
We address the task in a bottom-up manner, as two submodules i.e. face age
progression and face attribute manipulation. For face aging, we use an
attribute-conscious face aging model with a pyramidal generative adversarial
network that can model age-specific facial changes while maintaining intrinsic
subject specific characteristics. For facial attribute manipulation, the age
processed facial image is manipulated with desired attributes while preserving
other details unchanged, leveraging an attribute generative adversarial network
architecture. We conduct extensive analysis in standard large scale datasets
and our model achieves significant performance both quantitatively and
qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tatikonda_S/0/1/0/all/0/1"&gt;Sinzith Tatikonda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nambiar_A/0/1/0/all/0/1"&gt;Athira Nambiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anurag Mittal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reverse Engineering of Generative Models: Inferring Model Hyperparameters from Generated Images. (arXiv:2106.07873v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07873</id>
        <link href="http://arxiv.org/abs/2106.07873"/>
        <updated>2021-06-16T01:21:04.812Z</updated>
        <summary type="html"><![CDATA[State-of-the-art (SOTA) Generative Models (GMs) can synthesize
photo-realistic images that are hard for humans to distinguish from genuine
photos. We propose to perform reverse engineering of GMs to infer the model
hyperparameters from the images generated by these models. We define a novel
problem, "model parsing", as estimating GM network architectures and training
loss functions by examining their generated images -- a task seemingly
impossible for human beings. To tackle this problem, we propose a framework
with two components: a Fingerprint Estimation Network (FEN), which estimates a
GM fingerprint from a generated image by training with four constraints to
encourage the fingerprint to have desired properties, and a Parsing Network
(PN), which predicts network architecture and loss functions from the estimated
fingerprints. To evaluate our approach, we collect a fake image dataset with
$100$K images generated by $100$ GMs. Extensive experiments show encouraging
results in parsing the hyperparameters of the unseen models. Finally, our
fingerprint estimation can be leveraged for deepfake detection and image
attribution, as we show by reporting SOTA results on both the recent Celeb-DF
and image attribution benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asnani_V/0/1/0/all/0/1"&gt;Vishal Asnani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xi Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1"&gt;Tal Hassner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flow Guided Transformable Bottleneck Networks for Motion Retargeting. (arXiv:2106.07771v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07771</id>
        <link href="http://arxiv.org/abs/2106.07771"/>
        <updated>2021-06-16T01:21:04.784Z</updated>
        <summary type="html"><![CDATA[Human motion retargeting aims to transfer the motion of one person in a
"driving" video or set of images to another person. Existing efforts leverage a
long training video from each target person to train a subject-specific motion
transfer model. However, the scalability of such methods is limited, as each
model can only generate videos for the given target subject, and such training
videos are labor-intensive to acquire and process. Few-shot motion transfer
techniques, which only require one or a few images from a target, have recently
drawn considerable attention. Methods addressing this task generally use either
2D or explicit 3D representations to transfer motion, and in doing so,
sacrifice either accurate geometric modeling or the flexibility of an
end-to-end learned representation. Inspired by the Transformable Bottleneck
Network, which renders novel views and manipulations of rigid objects, we
propose an approach based on an implicit volumetric representation of the image
content, which can then be spatially manipulated using volumetric flow fields.
We address the challenging question of how to aggregate information across
different body poses, learning flow fields that allow for combining content
from the appropriate regions of input images of highly non-rigid human subjects
performing complex motions into a single implicit volumetric representation.
This allows us to learn our 3D representation solely from videos of moving
people. Armed with both 3D object understanding and end-to-end learned
rendering, this categorically novel representation delivers state-of-the-art
image generation quality, as shown by our quantitative and qualitative
evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jian Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1"&gt;Menglei Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodford_O/0/1/0/all/0/1"&gt;Oliver J. Woodford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1"&gt;Kyle Olszewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1"&gt;Sergey Tulyakov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Field-Embedded Factorization Machines for Click-through rate prediction. (arXiv:2009.09931v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09931</id>
        <link href="http://arxiv.org/abs/2009.09931"/>
        <updated>2021-06-16T01:21:04.771Z</updated>
        <summary type="html"><![CDATA[Click-through rate (CTR) prediction models are common in many online
applications such as digital advertising and recommender systems. Field-Aware
Factorization Machine (FFM) and Field-weighted Factorization Machine (FwFM) are
state-of-the-art among the shallow models for CTR prediction. Recently, many
deep learning-based models have also been proposed. Among deeper models,
DeepFM, xDeepFM, AutoInt+, and FiBiNet are state-of-the-art models. The deeper
models combine a core architectural component, which learns explicit feature
interactions, with a deep neural network (DNN) component. We propose a novel
shallow Field-Embedded Factorization Machine (FEFM) and its deep counterpart
Deep Field-Embedded Factorization Machine (DeepFEFM). FEFM learns symmetric
matrix embeddings for each field pair along with the usual single vector
embeddings for each feature. FEFM has significantly lower model complexity than
FFM and roughly the same complexity as FwFM. FEFM also has insightful
mathematical properties about important fields and field interactions. DeepFEFM
combines the FEFM interaction vectors learned by the FEFM component with a DNN
and is thus able to learn higher order interactions. We conducted comprehensive
experiments over a wide range of hyperparameters on two large publicly
available real-world datasets. When comparing test AUC and log loss, the
results show that FEFM and DeepFEFM outperform the existing state-of-the-art
shallow and deep models for CTR prediction tasks. We have made the code of FEFM
and DeepFEFM available in the DeepCTR library
(https://github.com/shenweichen/DeepCTR).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pande_H/0/1/0/all/0/1"&gt;Harshit Pande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Potato Crop Stress Identification in Aerial Images using Deep Learning-based Object Detection. (arXiv:2106.07770v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07770</id>
        <link href="http://arxiv.org/abs/2106.07770"/>
        <updated>2021-06-16T01:21:04.759Z</updated>
        <summary type="html"><![CDATA[Recent research on the application of remote sensing and deep learning-based
analysis in precision agriculture demonstrated a potential for improved crop
management and reduced environmental impacts of agricultural production.
Despite the promising results, the practical relevance of these technologies
for actual field deployment requires novel algorithms that are customized for
analysis of agricultural images and robust to implementation on natural field
imagery. The paper presents an approach for analyzing aerial images of a potato
crop using deep neural networks. The main objective is to demonstrate automated
spatial recognition of a healthy versus stressed crop at a plant level.
Specifically, we examine premature plant senescence resulting in drought stress
on Russet Burbank potato plants. The proposed deep learning model, named
Retina-UNet-Ag, is a variant of Retina-UNet (Jaeger et al., 2018) and includes
connections from low-level semantic dense representation maps to the feature
pyramid network. The paper also introduces a dataset of field images acquired
with a Parrot Sequoia camera carried by a Solo unmanned aerial vehicle.
Experimental validation demonstrated the ability for distinguishing healthy and
stressed plants in field images, achieving an average Dice score coefficient of
0.74. A comparison to related state-of-the-art deep learning models for object
detection revealed that the presented approach is effective for the task at
hand. The method applied here is conducive toward the assessment and
recognition of potato crop stress (early plant senescence resulting from
drought stress in this case) in natural aerial field images collected under
real conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Butte_S/0/1/0/all/0/1"&gt;Sujata Butte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakanski_A/0/1/0/all/0/1"&gt;Aleksandar Vakanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duellman_K/0/1/0/all/0/1"&gt;Kasia Duellman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirkouei_A/0/1/0/all/0/1"&gt;Amin Mirkouei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Audio-Visual Dereverberation. (arXiv:2106.07732v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.07732</id>
        <link href="http://arxiv.org/abs/2106.07732"/>
        <updated>2021-06-16T01:21:04.744Z</updated>
        <summary type="html"><![CDATA[Reverberation from audio reflecting off surfaces and objects in the
environment not only degrades the quality of speech for human perception, but
also severely impacts the accuracy of automatic speech recognition. Prior work
attempts to remove reverberation based on the audio modality only. Our idea is
to learn to dereverberate speech from audio-visual observations. The visual
environment surrounding a human speaker reveals important cues about the room
geometry, materials, and speaker location, all of which influence the precise
reverberation effects in the audio stream. We introduce Visually-Informed
Dereverberation of Audio (VIDA), an end-to-end approach that learns to remove
reverberation based on both the observed sounds and visual scene. In support of
this new task, we develop a large-scale dataset that uses realistic acoustic
renderings of speech in real-world 3D scans of homes offering a variety of room
acoustics. Demonstrating our approach on both simulated and real imagery for
speech enhancement, speech recognition, and speaker identification, we show it
achieves state-of-the-art performance and substantially improves over
traditional audio-only methods. Project page:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1"&gt;David Harwath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.07806</id>
        <link href="http://arxiv.org/abs/2106.07806"/>
        <updated>2021-06-16T01:21:04.724Z</updated>
        <summary type="html"><![CDATA[Machine learning is revolutionizing image-based diagnostics in pathology and
radiology. ML models have shown promising results in research settings, but
their lack of interoperability has been a major barrier for clinical
integration and evaluation. The DICOM a standard specifies Information Object
Definitions and Services for the representation and communication of digital
images and related information, including image-derived annotations and
analysis results. However, the complexity of the standard represents an
obstacle for its adoption in the ML community and creates a need for software
libraries and tools that simplify working with data sets in DICOM format. Here
we present the highdicom library, which provides a high-level application
programming interface for the Python programming language that abstracts
low-level details of the standard and enables encoding and decoding of
image-derived information in DICOM format in a few lines of Python code. The
highdicom library ties into the extensive Python ecosystem for image processing
and machine learning. Simultaneously, by simplifying creation and parsing of
DICOM-compliant files, highdicom achieves interoperability with the medical
imaging systems that hold the data used to train and run ML models, and
ultimately communicate and store model outputs for clinical use. We demonstrate
through experiments with slide microscopy and computed tomography imaging,
that, by bridging these two ecosystems, highdicom enables developers to train
and evaluate state-of-the-art ML models in pathology and radiology while
remaining compliant with the DICOM standard and interoperable with clinical
systems at all stages. To promote standardization of ML research and streamline
the ML model development and deployment process, we made the library available
free and open-source.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bridge_C/0/1/0/all/0/1"&gt;Christopher P. Bridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gorman_C/0/1/0/all/0/1"&gt;Chris Gorman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pieper_S/0/1/0/all/0/1"&gt;Steven Pieper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Doyle_S/0/1/0/all/0/1"&gt;Sean W. Doyle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lennerz_J/0/1/0/all/0/1"&gt;Jochen K. Lennerz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1"&gt;Jayashree Kalpathy-Cramer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Clunie_D/0/1/0/all/0/1"&gt;David A. Clunie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fedorov_A/0/1/0/all/0/1"&gt;Andriy Y. Fedorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Herrmann_M/0/1/0/all/0/1"&gt;Markus D. Herrmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10005</id>
        <link href="http://arxiv.org/abs/2105.10005"/>
        <updated>2021-06-16T01:21:04.389Z</updated>
        <summary type="html"><![CDATA[Physical processes, camera movement, and unpredictable environmental
conditions like the presence of dust can induce noise and artifacts in video
feeds. We observe that popular unsupervised MOT methods are dependent on
noise-free inputs. We show that the addition of a small amount of artificial
random noise causes a sharp degradation in model performance on benchmark
metrics. We resolve this problem by introducing a robust unsupervised
multi-object tracking (MOT) model: AttU-Net. The proposed single-head attention
model helps limit the negative impact of noise by learning visual
representations at different segment scales. AttU-Net shows better unsupervised
MOT tracking performance over variational inference-based state-of-the-art
baselines. We evaluate our method in the MNIST-MOT and the Atari game video
benchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''
which consists of moving Japanese characters and ``Fashion-MNIST MOT'' to
validate the effectiveness of the MOT models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;C.-H. Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1"&gt;Mohit Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Y.-C. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quan Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1"&gt;Tomoaki Yoshinaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1"&gt;Tomokazu Murakami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11066</id>
        <link href="http://arxiv.org/abs/2010.11066"/>
        <updated>2021-06-16T01:21:04.362Z</updated>
        <summary type="html"><![CDATA[Spoken conversational question answering (SCQA) requires machines to model
complex dialogue flow given the speech utterances and text corpora. Different
from traditional text question answering (QA) tasks, SCQA involves audio signal
processing, passage comprehension, and contextual understanding. However, ASR
systems introduce unexpected noisy signals to the transcriptions, which result
in performance degradation on SCQA. To overcome the problem, we propose CADNet,
a novel contextualized attention-based distillation approach, which applies
both cross-attention and self-attention to obtain ASR-robust contextualized
embedding representations of the passage and dialogue history for performance
improvements. We also introduce the spoken conventional knowledge distillation
framework to distill the ASR-robust knowledge from the estimated probabilities
of the teacher model to the student. We conduct extensive experiments on the
Spoken-CoQA dataset and demonstrate that our approach achieves remarkable
performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Self-supervised Multi-task Learning for COVID-19 Information Retrieval and Extraction. (arXiv:2106.08252v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08252</id>
        <link href="http://arxiv.org/abs/2106.08252"/>
        <updated>2021-06-16T01:21:04.341Z</updated>
        <summary type="html"><![CDATA[The rapidly evolving literature of COVID-19 related articles makes it
challenging for NLP models to be effectively trained for information retrieval
and extraction with the corresponding labeled data that follows the current
distribution of the pandemic. On the other hand, due to the uncertainty of the
situation, human experts' supervision would always be required to double check
the decision making of these models highlighting the importance of
interpretability. In the light of these challenges, this study proposes an
interpretable self-supervised multi-task learning model to jointly and
effectively tackle the tasks of information retrieval (IR) and extraction (IE)
during the current emergency health crisis situation. Our results show that our
model effectively leverage the multi-task and self-supervised learning to
improve generalization, data efficiency and robustness to the ongoing dataset
shift problem. Our model outperforms baselines in IE and IR tasks, respectively
by micro-f score of 0.08 (LCA-F score of 0.05), and MAP of 0.05 on average. In
IE the zero- and few-shot learning performances are on average 0.32 and 0.19
micro-f score higher than those of the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ebadi_N/0/1/0/all/0/1"&gt;Nima Ebadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1"&gt;Peyman Najafirad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detect and remove watermark in deep neural networks via generative adversarial networks. (arXiv:2106.08104v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.08104</id>
        <link href="http://arxiv.org/abs/2106.08104"/>
        <updated>2021-06-16T01:21:04.329Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNN) have achieved remarkable performance in various
fields. However, training a DNN model from scratch requires a lot of computing
resources and training data. It is difficult for most individual users to
obtain such computing resources and training data. Model copyright infringement
is an emerging problem in recent years. For instance, pre-trained models may be
stolen or abuse by illegal users without the authorization of the model owner.
Recently, many works on protecting the intellectual property of DNN models have
been proposed. In these works, embedding watermarks into DNN based on backdoor
is one of the widely used methods. However, when the DNN model is stolen, the
backdoor-based watermark may face the risk of being detected and removed by an
adversary. In this paper, we propose a scheme to detect and remove watermark in
deep neural networks via generative adversarial networks (GAN). We demonstrate
that the backdoor-based DNN watermarks are vulnerable to the proposed GAN-based
watermark removal attack. The proposed attack method includes two phases. In
the first phase, we use the GAN and few clean images to detect and reverse the
watermark in the DNN model. In the second phase, we fine-tune the watermarked
DNN based on the reversed backdoor images. Experimental evaluations on the
MNIST and CIFAR10 datasets demonstrate that, the proposed method can
effectively remove about 98% of the watermark in DNN models, as the watermark
retention rate reduces from 100% to less than 2% after applying the proposed
attack. In the meantime, the proposed attack hardly affects the model's
performance. The test accuracy of the watermarked DNN on the MNIST and the
CIFAR10 datasets drops by less than 1% and 3%, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1"&gt;Mingfu Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shichang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yushu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiqiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval. (arXiv:2104.01894v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01894</id>
        <link href="http://arxiv.org/abs/2104.01894"/>
        <updated>2021-06-16T01:21:04.317Z</updated>
        <summary type="html"><![CDATA[Speech-based image retrieval has been studied as a proxy for joint
representation learning, usually without emphasis on retrieval itself. As such,
it is unclear how well speech-based retrieval can work in practice -- both in
an absolute sense and versus alternative strategies that combine automatic
speech recognition (ASR) with strong text encoders. In this work, we
extensively study and expand choices of encoder architectures, training
methodology (including unimodal and multimodal pretraining), and other factors.
Our experiments cover different types of speech in three datasets: Flickr
Audio, Places Audio, and Localized Narratives. Our best model configuration
achieves large gains over state of the art, e.g., pushing recall-at-one from
21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also
show our best speech-based models can match or exceed cascaded ASR-to-text
encoding when speech is spontaneous, accented, or otherwise hard to
automatically transcribe.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1"&gt;Ramon Sanabria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1"&gt;Austin Waters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1"&gt;Jason Baldridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To Infinity and Beyond! Accessibility is the Future for Kids' Search Engines. (arXiv:2106.07813v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07813</id>
        <link href="http://arxiv.org/abs/2106.07813"/>
        <updated>2021-06-16T01:21:04.306Z</updated>
        <summary type="html"><![CDATA[Research in the area of search engines for children remains in its infancy.
Seminal works have studied how children use mainstream search engines, as well
as how to design and evaluate custom search engines explicitly for children.
These works, however, tend to take a one-size-fits-all view, treating children
as a unit. Nevertheless, even at the same age, children are known to possess
and exhibit different capabilities. These differences affect how children
access and use search engines. To better serve children, in this vision paper,
we spotlight accessibility and discuss why current research on children and
search engines does not, but should, focus on this significant matter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milton_A/0/1/0/all/0/1"&gt;Ashlee Milton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allen_G/0/1/0/all/0/1"&gt;Garrett Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pera_M/0/1/0/all/0/1"&gt;Maria Soledad Pera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Query Embedding on Hyper-relational Knowledge Graphs. (arXiv:2106.08166v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.08166</id>
        <link href="http://arxiv.org/abs/2106.08166"/>
        <updated>2021-06-16T01:21:04.296Z</updated>
        <summary type="html"><![CDATA[Multi-hop logical reasoning is an established problem in the field of
representation learning on knowledge graphs (KGs). It subsumes both one-hop
link prediction as well as other more complex types of logical queries.
Existing algorithms operate only on classical, triple-based graphs, whereas
modern KGs often employ a hyper-relational modeling paradigm. In this paradigm,
typed edges may have several key-value pairs known as qualifiers that provide
fine-grained context for facts. In queries, this context modifies the meaning
of relations, and usually reduces the answer set. Hyper-relational queries are
often observed in real-world KG applications, and existing approaches for
approximate query answering cannot make use of qualifier pairs. In this work,
we bridge this gap and extend the multi-hop reasoning problem to
hyper-relational KGs allowing to tackle this new type of complex queries.
Building upon recent advancements in Graph Neural Networks and query embedding
techniques, we study how to embed and answer hyper-relational conjunctive
queries. Besides that, we propose a method to answer such queries and
demonstrate in our experiments that qualifiers improve query answering on a
diverse set of query patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alivanistos_D/0/1/0/all/0/1"&gt;Dimitrios Alivanistos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1"&gt;Max Berrendorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1"&gt;Michael Cochez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1"&gt;Mikhail Galkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploration in Online Advertising Systems with Deep Uncertainty-Aware Learning. (arXiv:2012.02298v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02298</id>
        <link href="http://arxiv.org/abs/2012.02298"/>
        <updated>2021-06-16T01:21:04.267Z</updated>
        <summary type="html"><![CDATA[Modern online advertising systems inevitably rely on personalization methods,
such as click-through rate (CTR) prediction. Recent progress in CTR prediction
enjoys the rich representation capabilities of deep learning and achieves great
success in large-scale industrial applications. However, these methods can
suffer from lack of exploration. Another line of prior work addresses the
exploration-exploitation trade-off problem with contextual bandit methods,
which are recently less studied in the industry due to the difficulty in
extending their flexibility with deep models. In this paper, we propose a novel
Deep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on
Gaussian processes, which can provide predictive uncertainty estimations while
maintaining the flexibility of deep neural networks. DUAL can be easily
implemented on existing models and deployed in real-time systems with minimal
extra computational overhead. By linking the predictive uncertainty estimation
ability of DUAL to well-known bandit algorithms, we further present DUAL-based
Ad-ranking strategies to boost up long-term utilities such as the social
welfare in advertising systems. Experimental results on several public datasets
demonstrate the effectiveness of our methods. Remarkably, an online A/B test
deployed in the Alibaba display advertising platform shows an 8.2% social
welfare improvement and an 8.0% revenue lift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1"&gt;Chao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhifeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1"&gt;Shuo Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lining Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1"&gt;Yifan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1"&gt;Kun Gai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kuang-chih Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Color2Style: Real-Time Exemplar-Based Image Colorization with Self-Reference Learning and Deep Feature Modulation. (arXiv:2106.08017v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08017</id>
        <link href="http://arxiv.org/abs/2106.08017"/>
        <updated>2021-06-16T01:21:04.242Z</updated>
        <summary type="html"><![CDATA[Legacy black-and-white photos are riddled with people's nostalgia and
glorious memories of the past. To better relive the elapsed frozen moments, in
this paper, we present a deep exemplar-based image colorization approach named
Color2Style to resurrect these grayscale image media by filling them with
vibrant colors. Generally, for exemplar-based colorization, unsupervised and
unpaired training are usually adopted, due to the difficulty of obtaining input
and ground truth image pairs. To train an exemplar-based colorization model,
current algorithms usually strive to achieve two procedures: i) retrieving a
large number of reference images with high similarity in advance, which is
inevitably time-consuming and tedious; ii) designing complicated modules to
transfer the colors of the reference image to the grayscale image, by
calculating and leveraging the deep semantic correspondence between them (e.g.,
non-local operation). Contrary to the previous methods, we solve and simplify
the above two steps in one end-to-end learning procedure. First, we adopt a
self-augmented self-reference training scheme, where the reference image is
generated by graphical transformations from the original colorful one whereby
the training can be formulated in a paired manner. Second, instead of computing
complex and inexplicable correspondence maps, our method exploits a simple yet
effective deep feature modulation (DFM) module, which injects the color
embeddings extracted from the reference image into the deep representations of
the input grayscale image. Such design is much more lightweight and
intelligible, achieving appealing performance with real-time processing speed.
Moreover, our model does not require multifarious loss functions and
regularization terms like existing methods, but only two widely used loss
functions. Codes and models will be available at
https://github.com/zhaohengyuan1/Color2Style.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hengyuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hotel Recognition via Latent Image Embedding. (arXiv:2106.08042v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08042</id>
        <link href="http://arxiv.org/abs/2106.08042"/>
        <updated>2021-06-16T01:21:03.979Z</updated>
        <summary type="html"><![CDATA[We approach the problem of hotel recognition with deep metric learning. We
overview the existing approaches and propose a modification to Contrastive loss
called Contrastive-Triplet loss. We construct a robust pipeline for
benchmarking metric learning models and perform experiments on Hotels-50K and
CUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval
on Hotels-50k. We open-source our code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tseytlin_B/0/1/0/all/0/1"&gt;Boris Tseytlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1"&gt;Ilya Makarov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Axiomatic Explanations for Neural Ranking Models. (arXiv:2106.08019v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08019</id>
        <link href="http://arxiv.org/abs/2106.08019"/>
        <updated>2021-06-16T01:21:03.957Z</updated>
        <summary type="html"><![CDATA[Recently, neural networks have been successfully employed to improve upon
state-of-the-art performance in ad-hoc retrieval tasks via machine-learned
ranking functions. While neural retrieval models grow in complexity and impact,
little is understood about their correspondence with well-studied IR
principles. Recent work on interpretability in machine learning has provided
tools and techniques to understand neural models in general, yet there has been
little progress towards explaining ranking models.

We investigate whether one can explain the behavior of neural ranking models
in terms of their congruence with well understood principles of document
ranking by using established theories from axiomatic IR. Axiomatic analysis of
information retrieval models has formalized a set of constraints on ranking
decisions that reasonable retrieval models should fulfill. We operationalize
this axiomatic thinking to reproduce rankings based on combinations of
elementary constraints. This allows us to investigate to what extent the
ranking decisions of neural rankers can be explained in terms of retrieval
axioms, and which axioms apply in which situations. Our experimental study
considers a comprehensive set of axioms over several representative neural
rankers. While the existing axioms can already explain the particularly
confident ranking decisions rather well, future work should extend the axiom
set to also cover the other still "unexplainable" neural IR rank decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Volske_M/0/1/0/all/0/1"&gt;Michael V&amp;#xf6;lske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bondarenko_A/0/1/0/all/0/1"&gt;Alexander Bondarenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frobe_M/0/1/0/all/0/1"&gt;Maik Fr&amp;#xf6;be&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagen_M/0/1/0/all/0/1"&gt;Matthias Hagen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1"&gt;Benno Stein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Jaspreet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Avishek Anand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User-specific Adaptive Fine-tuning for Cross-domain Recommendations. (arXiv:2106.07864v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07864</id>
        <link href="http://arxiv.org/abs/2106.07864"/>
        <updated>2021-06-16T01:21:03.932Z</updated>
        <summary type="html"><![CDATA[Making accurate recommendations for cold-start users has been a longstanding
and critical challenge for recommender systems (RS). Cross-domain
recommendations (CDR) offer a solution to tackle such a cold-start problem when
there is no sufficient data for the users who have rarely used the system. An
effective approach in CDR is to leverage the knowledge (e.g., user
representations) learned from a related but different domain and transfer it to
the target domain. Fine-tuning works as an effective transfer learning
technique for this objective, which adapts the parameters of a pre-trained
model from the source domain to the target domain. However, current methods are
mainly based on the global fine-tuning strategy: the decision of which layers
of the pre-trained model to freeze or fine-tune is taken for all users in the
target domain. In this paper, we argue that users in RS are personalized and
should have their own fine-tuning policies for better preference transfer
learning. As such, we propose a novel User-specific Adaptive Fine-tuning method
(UAF), selecting which layers of the pre-trained network to fine-tune, on a
per-user basis. Specifically, we devise a policy network with three alternative
strategies to automatically decide which layers to be fine-tuned and which
layers to have their parameters frozen for each user. Extensive experiments
show that the proposed UAF exhibits significantly better and more robust
performance for user cold-start recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1"&gt;Fajie Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaxi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Min Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Domain Knowledge into Health Recommender Systems using Hyperbolic Embeddings. (arXiv:2106.07720v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07720</id>
        <link href="http://arxiv.org/abs/2106.07720"/>
        <updated>2021-06-16T01:21:03.919Z</updated>
        <summary type="html"><![CDATA[In contrast to many other domains, recommender systems in health services may
benefit particularly from the incorporation of health domain knowledge, as it
helps to provide meaningful and personalised recommendations catering to the
individual's health needs. With recent advances in representation learning
enabling the hierarchical embedding of health knowledge into the hyperbolic
Poincare space, this work proposes a content-based recommender system for
patient-doctor matchmaking in primary care based on patients' health profiles,
enriched by pre-trained Poincare embeddings of the ICD-9 codes through transfer
learning. The proposed model outperforms its conventional counterpart in terms
of recommendation accuracy and has several important business implications for
improving the patient-doctor relationship.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peito_J/0/1/0/all/0/1"&gt;Joel Peito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1"&gt;Qiwei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does your robot know? Enhancing children's information retrieval through spoken conversation with responsible robots. (arXiv:2106.07931v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07931</id>
        <link href="http://arxiv.org/abs/2106.07931"/>
        <updated>2021-06-16T01:21:03.897Z</updated>
        <summary type="html"><![CDATA[In this paper, we identify challenges in children's current information
retrieval process, and propose conversational robots as an opportunity to ease
this process in a responsible way. Tools children currently use in this
process, such as search engines on a computer or voice agents, do not always
meet their specific needs. The conversational robot we propose maintains
context, asks clarifying questions, and gives suggestions in order to better
meet children's needs. Since children are often too trusting of robots, we
propose to have the robot measure, monitor and adapt to the trust the child has
in the robot. This way, we hope to induce a critical attitude with the children
during their information retrieval process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beelen_T/0/1/0/all/0/1"&gt;T. Beelen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velner_E/0/1/0/all/0/1"&gt;E. Velner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ordelman_R/0/1/0/all/0/1"&gt;R. Ordelman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_K/0/1/0/all/0/1"&gt;K.P. Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evers_V/0/1/0/all/0/1"&gt;V. Evers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huibers_T/0/1/0/all/0/1"&gt;T. Huibers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can BERT Dig It? -- Named Entity Recognition for Information Retrieval in the Archaeology Domain. (arXiv:2106.07742v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07742</id>
        <link href="http://arxiv.org/abs/2106.07742"/>
        <updated>2021-06-16T01:21:03.869Z</updated>
        <summary type="html"><![CDATA[The amount of archaeological literature is growing rapidly. Until recently,
these data were only accessible through metadata search. We implemented a text
retrieval engine for a large archaeological text collection ($\sim 658$ Million
words). In archaeological IR, domain-specific entities such as locations, time
periods, and artefacts, play a central role. This motivated the development of
a named entity recognition (NER) model to annotate the full collection with
archaeological named entities. In this paper, we present ArcheoBERTje, a BERT
model pre-trained on Dutch archaeological texts. We compare the model's quality
and output on a Named Entity Recognition task to a generic multilingual model
and a generic Dutch model. We also investigate ensemble methods for combining
multiple BERT models, and combining the best BERT model with a domain thesaurus
using Conditional Random Fields (CRF). We find that ArcheoBERTje outperforms
both the multilingual and Dutch model significantly with a smaller standard
deviation between runs, reaching an average F1 score of 0.735. The model also
outperforms ensemble methods combining the three models. Combining ArcheoBERTje
predictions and explicit domain knowledge from the thesaurus did not increase
the F1 score. We quantitatively and qualitatively analyse the differences
between the vocabulary and output of the BERT models on the full collection and
provide some valuable insights in the effect of fine-tuning for specific
domains. Our results indicate that for a highly specific text domain such as
archaeology, further pre-training on domain-specific data increases the model's
quality on NER by a much larger margin than shown for other domains in the
literature, and that domain-specific pre-training makes the addition of domain
knowledge from a thesaurus unnecessary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandsen_A/0/1/0/all/0/1"&gt;Alex Brandsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1"&gt;Suzan Verberne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lambers_K/0/1/0/all/0/1"&gt;Karsten Lambers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wansleeben_M/0/1/0/all/0/1"&gt;Milco Wansleeben&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full Bitcoin Blockchain Data Made Easy. (arXiv:2106.08072v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.08072</id>
        <link href="http://arxiv.org/abs/2106.08072"/>
        <updated>2021-06-16T01:21:03.828Z</updated>
        <summary type="html"><![CDATA[Despite the fact that it is publicly available, collecting and processing the
full bitcoin blockchain data is not trivial. Its mere size, history, and other
features indeed raise quite specific challenges, that we address in this paper.
The strengths of our approach are the following: it relies on very basic and
standard tools, which makes the procedure reliable and easily reproducible; it
is a purely lossless procedure ensuring that we catch and preserve all existing
data; it provides additional indexing that makes it easy to further process the
whole data and select appropriate subsets of it. We present our procedure in
details and illustrate its added value on large-scale use cases, like address
clustering. We provide an implementation online, as well as the obtained
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Emery_J/0/1/0/all/0/1"&gt;Jules Azad Emery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Latapy_M/0/1/0/all/0/1"&gt;Matthieu Latapy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup. (arXiv:2101.06983v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06983</id>
        <link href="http://arxiv.org/abs/2101.06983"/>
        <updated>2021-06-16T00:27:38.497Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has been applied successfully to learn vector
representations of text. Previous research demonstrated that learning
high-quality representations benefits from batch-wise contrastive loss with a
large number of negatives. In practice, the technique of in-batch negative is
used, where for each example in a batch, other batch examples' positives will
be taken as its negatives, avoiding encoding extra negatives. This, however,
still conditions each example's loss on all batch examples and requires fitting
the entire large batch into GPU memory. This paper introduces a gradient
caching technique that decouples backpropagation between contrastive loss and
the encoder, removing encoder backward pass data dependency along the batch
dimension. As a result, gradients can be computed for one subset of the batch
at a time, leading to almost constant memory usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Luyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1"&gt;Jamie Callan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup. (arXiv:2101.06983v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06983</id>
        <link href="http://arxiv.org/abs/2101.06983"/>
        <updated>2021-06-16T00:27:38.461Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has been applied successfully to learn vector
representations of text. Previous research demonstrated that learning
high-quality representations benefits from batch-wise contrastive loss with a
large number of negatives. In practice, the technique of in-batch negative is
used, where for each example in a batch, other batch examples' positives will
be taken as its negatives, avoiding encoding extra negatives. This, however,
still conditions each example's loss on all batch examples and requires fitting
the entire large batch into GPU memory. This paper introduces a gradient
caching technique that decouples backpropagation between contrastive loss and
the encoder, removing encoder backward pass data dependency along the batch
dimension. As a result, gradients can be computed for one subset of the batch
at a time, leading to almost constant memory usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Luyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1"&gt;Jamie Callan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Meta-Feature Selection for the Algorithm Recommendation Problem. (arXiv:2106.03954v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03954</id>
        <link href="http://arxiv.org/abs/2106.03954"/>
        <updated>2021-06-15T22:41:25.615Z</updated>
        <summary type="html"><![CDATA[With the popularity of Machine Learning (ML) solutions, algorithms and data
have been released faster than the capacity of processing them. In this
context, the problem of Algorithm Recommendation (AR) is receiving a
significant deal of attention recently. This problem has been addressed in the
literature as a learning task, often as a Meta-Learning problem where the aim
is to recommend the best alternative for a specific dataset. For such, datasets
encoded by meta-features are explored by ML algorithms that try to learn the
mapping between meta-representations and the best technique to be used. One of
the challenges for the successful use of ML is to define which features are the
most valuable for a specific dataset since several meta-features can be used,
which increases the meta-feature dimension. This paper presents an empirical
analysis of Feature Selection and Feature Extraction in the meta-level for the
AR problem. The present study was focused on three criteria: predictive
performance, dimensionality reduction, and pipeline runtime. As we verified,
applying Dimensionality Reduction (DR) methods did not improve predictive
performances in general. However, DR solutions reduced about 80% of the
meta-features, obtaining pretty much the same performance as the original setup
but with lower runtimes. The only exception was PCA, which presented about the
same runtime as the original meta-features. Experimental results also showed
that various datasets have many non-informative meta-features and that it is
possible to obtain high predictive performance using around 20% of the original
meta-features. Therefore, due to their natural trend for high dimensionality,
DR methods should be used for Meta-Feature Selection and Meta-Feature
Extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1"&gt;Geand Trindade Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1"&gt;Moises Rocha dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1"&gt;Andre Carlos Ponce de Leon Ferreira de Carvalho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustifying $\ell_\infty$ Adversarial Training to the Union of Perturbation Models. (arXiv:2105.14710v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14710</id>
        <link href="http://arxiv.org/abs/2105.14710"/>
        <updated>2021-06-15T22:41:25.605Z</updated>
        <summary type="html"><![CDATA[Classical adversarial training (AT) frameworks are designed to achieve high
adversarial accuracy against a single attack type, typically $\ell_\infty$
norm-bounded perturbations. Recent extensions in AT have focused on defending
against the union of multiple perturbations but this benefit is obtained at the
expense of a significant (up to $10\times$) increase in training complexity
over single-attack $\ell_\infty$ AT. In this work, we expand the capabilities
of widely popular single-attack $\ell_\infty$ AT frameworks to provide
robustness to the union of ($\ell_\infty, \ell_2, \ell_1$) perturbations while
preserving their training efficiency. Our technique, referred to as Shaped
Noise Augmented Processing (SNAP), exploits a well-established byproduct of
single-attack AT frameworks -- the reduction in the curvature of the decision
boundary of networks. SNAP prepends a given deep net with a shaped noise
augmentation layer whose distribution is learned along with network parameters
using any standard single-attack AT. As a result, SNAP enhances adversarial
accuracy of ResNet-18 on CIFAR-10 against the union of ($\ell_\infty, \ell_2,
\ell_1$) perturbations by 14%-to-20% for four state-of-the-art (SOTA)
single-attack $\ell_\infty$ AT frameworks, and, for the first time, establishes
a benchmark for ResNet-50 and ResNet-101 on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1"&gt;Ameya D. Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuttle_M/0/1/0/all/0/1"&gt;Michael Tuttle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1"&gt;Alexander G. Schwing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanbhag_N/0/1/0/all/0/1"&gt;Naresh R. Shanbhag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signal Transformer: Complex-valued Attention and Meta-Learning for Signal Recognition. (arXiv:2106.04392v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04392</id>
        <link href="http://arxiv.org/abs/2106.04392"/>
        <updated>2021-06-15T22:41:25.595Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been shown as a class of useful tools for
addressing signal recognition issues in recent years, especially for
identifying the nonlinear feature structures of signals. However, this power of
most deep learning techniques heavily relies on an abundant amount of training
data, so the performance of classic neural nets decreases sharply when the
number of training data samples is small or unseen data are presented in the
testing phase. This calls for an advanced strategy, i.e., model-agnostic
meta-learning (MAML), which is able to capture the invariant representation of
the data samples or signals. In this paper, inspired by the special structure
of the signal, i.e., real and imaginary parts consisted in practical
time-series signals, we propose a Complex-valued Attentional MEta Learner
(CAMEL) for the problem of few-shot signal recognition by leveraging attention
and meta-learning in the complex domain. To the best of our knowledge, this is
also the first complex-valued MAML that can find the first-order stationary
points of general nonconvex problems with theoretical convergence guarantees.
Extensive experiments results showcase the superiority of the proposed CAMEL
compared with the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yihong Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Ying Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Songtao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1"&gt;Qingjiang Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LaplaceNet: A Hybrid Energy-Neural Model for Deep Semi-Supervised Classification. (arXiv:2106.04527v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04527</id>
        <link href="http://arxiv.org/abs/2106.04527"/>
        <updated>2021-06-15T22:41:25.576Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning has received a lot of recent attention as it
alleviates the need for large amounts of labelled data which can often be
expensive, requires expert knowledge and be time consuming to collect. Recent
developments in deep semi-supervised classification have reached unprecedented
performance and the gap between supervised and semi-supervised learning is
ever-decreasing. This improvement in performance has been based on the
inclusion of numerous technical tricks, strong augmentation techniques and
costly optimisation schemes with multi-term loss functions. We propose a new
framework, LaplaceNet, for deep semi-supervised classification that has a
greatly reduced model complexity. We utilise a hybrid energy-neural network
where graph based pseudo-labels, generated by minimising the graphical
Laplacian, are used to iteratively improve a neural-network backbone. Our model
outperforms state-of-the-art methods for deep semi-supervised classification,
over several benchmark datasets. Furthermore, we consider the application of
strong-augmentations to neural networks theoretically and justify the use of a
multi-sampling approach for semi-supervised learning. We demonstrate, through
rigorous experimentation, that a multi-sampling augmentation approach improves
generalisation and reduces the sensitivity of the network to augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sellars_P/0/1/0/all/0/1"&gt;Philip Sellars&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1"&gt;Angelica I. Aviles-Rivero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation. (arXiv:2106.04563v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04563</id>
        <link href="http://arxiv.org/abs/2106.04563"/>
        <updated>2021-06-15T22:41:25.566Z</updated>
        <summary type="html"><![CDATA[While deep and large pre-trained models are the state-of-the-art for various
natural language processing tasks, their huge size poses significant challenges
for practical uses in resource constrained settings. Recent works in knowledge
distillation propose task-agnostic as well as task-specific methods to compress
these models, with task-specific ones often yielding higher compression rate.
In this work, we develop a new task-agnostic distillation framework
XtremeDistilTransformers that leverages the advantage of task-specific methods
for learning a small universal model that can be applied to arbitrary tasks and
languages. To this end, we study the transferability of several source tasks,
augmentation resources and model architecture for distillation. We evaluate our
model performance on multiple tasks, including the General Language
Understanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and
a massive multi-lingual NER dataset with 41 languages. We release three
distilled task-agnostic checkpoints with 13MM, 22MM and 33MM parameters
obtaining SOTA performance in several tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Subhabrata Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Theoretical Framework of Out-of-Distribution Generalization. (arXiv:2106.04496v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04496</id>
        <link href="http://arxiv.org/abs/2106.04496"/>
        <updated>2021-06-15T22:41:25.555Z</updated>
        <summary type="html"><![CDATA[Generalization to out-of-distribution (OOD) data, or domain generalization,
is one of the central problems in modern machine learning. Recently, there is a
surge of attempts to propose algorithms for OOD that mainly build upon the idea
of extracting invariant features. Although intuitively reasonable, theoretical
understanding of what kind of invariance can guarantee OOD generalization is
still limited, and generalization to arbitrary out-of-distribution is clearly
impossible. In this work, we take the first step towards rigorous and
quantitative definitions of 1) what is OOD; and 2) what does it mean by saying
an OOD problem is learnable. We also introduce a new concept of expansion
function, which characterizes to what extent the variance is amplified in the
test domains over the training domains, and therefore give a quantitative
meaning of invariant features. Based on these, we prove OOD generalization
error bounds. It turns out that OOD generalization largely depends on the
expansion function. As recently pointed out by Gulrajani and Lopez-Paz (2020),
any OOD learning algorithm without a model selection module is incomplete. Our
theory naturally induces a model selection criterion. Extensive experiments on
benchmark OOD datasets demonstrate that our model selection criterion has a
significant advantage over baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Haotian Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chuanlong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruichen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principled Hyperedge Prediction with Structural Spectral Features and Neural Networks. (arXiv:2106.04292v4 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04292</id>
        <link href="http://arxiv.org/abs/2106.04292"/>
        <updated>2021-06-15T22:41:25.543Z</updated>
        <summary type="html"><![CDATA[Hypergraph offers a framework to depict the multilateral relationships in
real-world complex data. Predicting higher-order relationships, i.e hyperedge,
becomes a fundamental problem for the full understanding of complicated
interactions. The development of graph neural network (GNN) has greatly
advanced the analysis of ordinary graphs with pair-wise relations. However,
these methods could not be easily extended to the case of hypergraph. In this
paper, we generalize the challenges of GNN in representing higher-order data in
principle, which are edge- and node-level ambiguities. To overcome the
challenges, we present SNALS that utilizes bipartite graph neural network with
structural features to collectively tackle the two ambiguity issues. SNALS
captures the joint interactions of a hyperedge by its local environment, which
is retrieved by collecting the spectrum information of their connections. As a
result, SNALS achieves nearly 30% performance increase compared with most
recent GNN-based models. In addition, we applied SNALS to predict genetic
higher-order interactions on 3D genome organization data. SNALS showed
consistently high prediction accuracy across different chromosomes, and
generated novel findings on 4-way gene interaction, which is further validated
by existing literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1"&gt;Changlin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Muhan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1"&gt;Wei Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Sha Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Heavy-Tail Phenomenon in SGD. (arXiv:2006.04740v5 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04740</id>
        <link href="http://arxiv.org/abs/2006.04740"/>
        <updated>2021-06-15T22:41:25.530Z</updated>
        <summary type="html"><![CDATA[In recent years, various notions of capacity and complexity have been
proposed for characterizing the generalization properties of stochastic
gradient descent (SGD) in deep learning. Some of the popular notions that
correlate well with the performance on unseen data are (i) the `flatness' of
the local minimum found by SGD, which is related to the eigenvalues of the
Hessian, (ii) the ratio of the stepsize $\eta$ to the batch-size $b$, which
essentially controls the magnitude of the stochastic gradient noise, and (iii)
the `tail-index', which measures the heaviness of the tails of the network
weights at convergence. In this paper, we argue that these three seemingly
unrelated perspectives for generalization are deeply linked to each other. We
claim that depending on the structure of the Hessian of the loss at the
minimum, and the choices of the algorithm parameters $\eta$ and $b$, the SGD
iterates will converge to a \emph{heavy-tailed} stationary distribution. We
rigorously prove this claim in the setting of quadratic optimization: we show
that even in a simple linear regression problem with independent and
identically distributed data whose distribution has finite moments of all
order, the iterates can be heavy-tailed with infinite variance. We further
characterize the behavior of the tails with respect to algorithm parameters,
the dimension, and the curvature. We then translate our results into insights
about the behavior of SGD in deep learning. We support our theory with
experiments conducted on synthetic data, fully connected, and convolutional
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1"&gt;Mert Gurbuzbalaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Simsekli_U/0/1/0/all/0/1"&gt;Umut &amp;#x15e;im&amp;#x15f;ekli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingjiong Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We Know What You Want: An Advertising Strategy Recommender System for Online Advertising. (arXiv:2105.14188v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14188</id>
        <link href="http://arxiv.org/abs/2105.14188"/>
        <updated>2021-06-15T22:41:25.502Z</updated>
        <summary type="html"><![CDATA[Advertising expenditures have become the major source of revenue for
e-commerce platforms. Providing good advertising experiences for advertisers by
reducing their costs of trial and error in discovering the optimal advertising
strategies is crucial for the long-term prosperity of online advertising. To
achieve this goal, the advertising platform needs to identify the advertiser's
optimization objectives, and then recommend the corresponding strategies to
fulfill the objectives. In this work, we first deploy a prototype of strategy
recommender system on Taobao display advertising platform, which indeed
increases the advertisers' performance and the platform's revenue, indicating
the effectiveness of strategy recommendation for online advertising. We further
augment this prototype system by explicitly learning the advertisers'
preferences over various advertising performance indicators and then
optimization objectives through their adoptions of different recommending
advertising strategies. We use contextual bandit algorithms to efficiently
learn the advertisers' preferences and maximize the recommendation adoption,
simultaneously. Simulation experiments based on Taobao online bidding data show
that the designed algorithms can effectively optimize the strategy adoption
rate of advertisers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Liyi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Junqi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhenzhe Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhiye Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1"&gt;Zhizhuang Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Fei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1"&gt;Lvyin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuning Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution Homography Estimation. (arXiv:2106.04067v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04067</id>
        <link href="http://arxiv.org/abs/2106.04067"/>
        <updated>2021-06-15T22:41:25.275Z</updated>
        <summary type="html"><![CDATA[Cross-resolution image alignment is a key problem in multiscale gigapixel
photography, which requires to estimate homography matrix using images with
large resolution gap. Existing deep homography methods concatenate the input
images or features, neglecting the explicit formulation of correspondences
between them, which leads to degraded accuracy in cross-resolution challenges.
In this paper, we consider the cross-resolution homography estimation as a
multimodal problem, and propose a local transformer network embedded within a
multiscale structure to explicitly learn correspondences between the multimodal
inputs, namely, input images with different resolutions. The proposed local
transformer adopts a local attention map specifically for each position in the
feature. By combining the local transformer with the multiscale structure, the
network is able to capture long-short range correspondences efficiently and
accurately. Experiments on both the MS-COCO dataset and the real-captured
cross-resolution dataset show that the proposed network outperforms existing
state-of-the-art feature-based and deep-learning-based homography estimation
methods, and is able to accurately align images under $10\times$ resolution
gap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1"&gt;Ruizhi Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gaochang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuemei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Ying Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yebin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation. (arXiv:2106.04563v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04563</id>
        <link href="http://arxiv.org/abs/2106.04563"/>
        <updated>2021-06-15T22:41:24.981Z</updated>
        <summary type="html"><![CDATA[While deep and large pre-trained models are the state-of-the-art for various
natural language processing tasks, their huge size poses significant challenges
for practical uses in resource constrained settings. Recent works in knowledge
distillation propose task-agnostic as well as task-specific methods to compress
these models, with task-specific ones often yielding higher compression rate.
In this work, we develop a new task-agnostic distillation framework
XtremeDistilTransformers that leverages the advantage of task-specific methods
for learning a small universal model that can be applied to arbitrary tasks and
languages. To this end, we study the transferability of several source tasks,
augmentation resources and model architecture for distillation. We evaluate our
model performance on multiple tasks, including the General Language
Understanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and
a massive multi-lingual NER dataset with 41 languages. We release three
distilled task-agnostic checkpoints with 13MM, 22MM and 33MM parameters
obtaining SOTA performance in several tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Subhabrata Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality. (arXiv:2106.04102v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04102</id>
        <link href="http://arxiv.org/abs/2106.04102"/>
        <updated>2021-06-15T22:41:24.969Z</updated>
        <summary type="html"><![CDATA[We release a new benchmark for lexical substitution, the task of finding
appropriate substitutes for a target word in a context. To assist humans with
writing, lexical substitution systems can suggest words that humans cannot
easily think of. However, existing benchmarks depend on human recall as the
only source of data, and therefore lack coverage of the substitutes that would
be most helpful to humans. Furthermore, annotators often provide substitutes of
low quality, which are not actually appropriate in the given context. We
collect higher-coverage and higher-quality data by framing lexical substitution
as a classification problem, guided by the intuition that it is easier for
humans to judge the appropriateness of candidate substitutes than conjure them
from memory. To this end, we use a context-free thesaurus to produce candidates
and rely on human judgement to determine contextual appropriateness. Compared
to the previous largest benchmark, our Swords benchmark has 4.1x more
substitutes per target word for the same level of quality, and its substitutes
are 1.5x more appropriate (based on human judgement) for the same number of
substitutes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mina Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1"&gt;Chris Donahue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Robin Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyabor_A/0/1/0/all/0/1"&gt;Alexander Iyabor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Meta-Feature Selection for the Algorithm Recommendation Problem. (arXiv:2106.03954v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03954</id>
        <link href="http://arxiv.org/abs/2106.03954"/>
        <updated>2021-06-15T22:41:24.942Z</updated>
        <summary type="html"><![CDATA[With the popularity of Machine Learning (ML) solutions, algorithms and data
have been released faster than the capacity of processing them. In this
context, the problem of Algorithm Recommendation (AR) is receiving a
significant deal of attention recently. This problem has been addressed in the
literature as a learning task, often as a Meta-Learning problem where the aim
is to recommend the best alternative for a specific dataset. For such, datasets
encoded by meta-features are explored by ML algorithms that try to learn the
mapping between meta-representations and the best technique to be used. One of
the challenges for the successful use of ML is to define which features are the
most valuable for a specific dataset since several meta-features can be used,
which increases the meta-feature dimension. This paper presents an empirical
analysis of Feature Selection and Feature Extraction in the meta-level for the
AR problem. The present study was focused on three criteria: predictive
performance, dimensionality reduction, and pipeline runtime. As we verified,
applying Dimensionality Reduction (DR) methods did not improve predictive
performances in general. However, DR solutions reduced about 80% of the
meta-features, obtaining pretty much the same performance as the original setup
but with lower runtimes. The only exception was PCA, which presented about the
same runtime as the original meta-features. Experimental results also showed
that various datasets have many non-informative meta-features and that it is
possible to obtain high predictive performance using around 20% of the original
meta-features. Therefore, due to their natural trend for high dimensionality,
DR methods should be used for Meta-Feature Selection and Meta-Feature
Extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1"&gt;Geand Trindade Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1"&gt;Moises Rocha dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1"&gt;Andre Carlos Ponce de Leon Ferreira de Carvalho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We Know What You Want: An Advertising Strategy Recommender System for Online Advertising. (arXiv:2105.14188v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14188</id>
        <link href="http://arxiv.org/abs/2105.14188"/>
        <updated>2021-06-15T22:41:24.921Z</updated>
        <summary type="html"><![CDATA[Advertising expenditures have become the major source of revenue for
e-commerce platforms. Providing good advertising experiences for advertisers by
reducing their costs of trial and error in discovering the optimal advertising
strategies is crucial for the long-term prosperity of online advertising. To
achieve this goal, the advertising platform needs to identify the advertiser's
optimization objectives, and then recommend the corresponding strategies to
fulfill the objectives. In this work, we first deploy a prototype of strategy
recommender system on Taobao display advertising platform, which indeed
increases the advertisers' performance and the platform's revenue, indicating
the effectiveness of strategy recommendation for online advertising. We further
augment this prototype system by explicitly learning the advertisers'
preferences over various advertising performance indicators and then
optimization objectives through their adoptions of different recommending
advertising strategies. We use contextual bandit algorithms to efficiently
learn the advertisers' preferences and maximize the recommendation adoption,
simultaneously. Simulation experiments based on Taobao online bidding data show
that the designed algorithms can effectively optimize the strategy adoption
rate of advertisers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Liyi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Junqi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhenzhe Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhiye Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1"&gt;Zhizhuang Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Fei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1"&gt;Lvyin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuning Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets. (arXiv:2106.04476v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04476</id>
        <link href="http://arxiv.org/abs/2106.04476"/>
        <updated>2021-06-15T22:41:24.890Z</updated>
        <summary type="html"><![CDATA[Semantic parsers map natural language utterances to meaning representations.
The lack of a single standard for meaning representations led to the creation
of a plethora of semantic parsing datasets. To unify different datasets and
train a single model for them, we investigate the use of Multi-Task Learning
(MTL) architectures. We experiment with five datasets (Geoquery, NLMaps, TOP,
Overnight, AMR). We find that an MTL architecture that shares the entire
network across datasets yields competitive or better parsing accuracies than
the single-task baselines, while reducing the total number of parameters by
68%. We further provide evidence that MTL has also better compositional
generalization than single-task models. We also present a comparison of task
sampling methods and propose a competitive alternative to widespread
proportional sampling strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Damonte_M/0/1/0/all/0/1"&gt;Marco Damonte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monti_E/0/1/0/all/0/1"&gt;Emilio Monti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Transferable Learning: A New Approach for Model Verification and Authorization. (arXiv:2106.06916v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06916</id>
        <link href="http://arxiv.org/abs/2106.06916"/>
        <updated>2021-06-15T22:07:49.359Z</updated>
        <summary type="html"><![CDATA[As Artificial Intelligence as a Service gains popularity, protecting
well-trained models as intellectual property is becoming increasingly
important. Generally speaking, there are two common protection methods:
ownership verification and usage authorization. In this paper, we propose
Non-Transferable Learning (NTL), a novel approach that captures the exclusive
data representation in the learned model and restricts the model generalization
ability to certain domains. This approach provides effective solutions to both
model verification and authorization. For ownership verification, watermarking
techniques are commonly used but are often vulnerable to sophisticated
watermark removal methods. Our NTL-based model verification approach instead
provides robust resistance to state-of-the-art watermark removal methods, as
shown in extensive experiments for four of such methods over the digits,
CIFAR10 & STL10, and VisDA datasets. For usage authorization, prior solutions
focus on authorizing specific users to use the model, but authorized users can
still apply the model to any data without restriction. Our NTL-based
authorization approach instead provides data-centric usage protection by
significantly degrading the performance of usage on unauthorized data. Its
effectiveness is also shown through experiments on a variety of datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lixu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shichao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruiqi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qi Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LE-NAS: Learning-based Ensenble with NAS for Dose Prediction. (arXiv:2106.06733v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.06733</id>
        <link href="http://arxiv.org/abs/2106.06733"/>
        <updated>2021-06-15T22:07:49.342Z</updated>
        <summary type="html"><![CDATA[Radiation therapy treatment planning is a complex process, as the target dose
prescription and normal tissue sparing are conflicting objectives. Automated
and accurate dose prediction for radiation therapy planning is in high demand.
In this study, we propose a novel learning-based ensemble approach, named
LE-NAS, which integrates neural architecture search (NAS) with knowledge
distillation for 3D radiotherapy dose prediction. Specifically, the prediction
network first exhaustively searches each block from enormous architecture
space. Then, multiple architectures are selected with promising performance and
diversity. To reduce the inference time, we adopt the teacher-student paradigm
by treating the combination of diverse outputs from multiple searched networks
as supervisions to guide the student network training. In addition, we apply
adversarial learning to optimize the student network to recover the knowledge
in teacher networks. To the best of our knowledge, we are the first to
investigate the combination of NAS and knowledge distillation. The proposed
method has been evaluated on the public OpenKBP dataset, and experimental
results demonstrate the effectiveness of our method and its superior
performance to the state-of-the-art method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yanfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingguang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guocai Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Subject Domain Adaptation for Multi-Frame EEG Images. (arXiv:2106.06769v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06769</id>
        <link href="http://arxiv.org/abs/2106.06769"/>
        <updated>2021-06-15T22:07:49.270Z</updated>
        <summary type="html"><![CDATA[Working memory (WM) is a basic part of human cognition, which plays an
important role in the study of human cognitive load. Among various brain
imaging techniques, electroencephalography has shown its advantage on easy
access and reliability. However, one of the critical challenges is that
individual difference may cause the ineffective results, especially when the
established model meets an unfamiliar subject. In this work, we propose a
cross-subject deep adaptation model with spatial attention (CS-DASA) to
generalize the workload classifications across subjects. First, we transform
time-series EEG data into multi-frame EEG images incorporating more
spatio-temporal information. First, the subject-shared module in CS-DASA
receives multi-frame EEG image data from both source and target subjects and
learns the common feature representations. Then, in subject-specific module,
the maximum mean discrepancy is implemented to measure the domain distribution
divergence in a reproducing kernel Hilbert space, which can add an effective
penalty loss for domain adaptation. Additionally, the subject-to-subject
spatial attention mechanism is employed to focus on the most discriminative
spatial feature in EEG image data. Experiments conducted on a public WM EEG
dataset containing 13 subjects show that the proposed model is capable of
achieve better performance than existing state-of-the art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Junfu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Subject Domain Adaptation for Multi-Frame EEG Images. (arXiv:2106.06769v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06769</id>
        <link href="http://arxiv.org/abs/2106.06769"/>
        <updated>2021-06-15T22:07:49.048Z</updated>
        <summary type="html"><![CDATA[Working memory (WM) is a basic part of human cognition, which plays an
important role in the study of human cognitive load. Among various brain
imaging techniques, electroencephalography has shown its advantage on easy
access and reliability. However, one of the critical challenges is that
individual difference may cause the ineffective results, especially when the
established model meets an unfamiliar subject. In this work, we propose a
cross-subject deep adaptation model with spatial attention (CS-DASA) to
generalize the workload classifications across subjects. First, we transform
time-series EEG data into multi-frame EEG images incorporating more
spatio-temporal information. First, the subject-shared module in CS-DASA
receives multi-frame EEG image data from both source and target subjects and
learns the common feature representations. Then, in subject-specific module,
the maximum mean discrepancy is implemented to measure the domain distribution
divergence in a reproducing kernel Hilbert space, which can add an effective
penalty loss for domain adaptation. Additionally, the subject-to-subject
spatial attention mechanism is employed to focus on the most discriminative
spatial feature in EEG image data. Experiments conducted on a public WM EEG
dataset containing 13 subjects show that the proposed model is capable of
achieve better performance than existing state-of-the art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Junfu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Reversible Steganography: Principles and Insights. (arXiv:2106.06924v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.06924</id>
        <link href="http://arxiv.org/abs/2106.06924"/>
        <updated>2021-06-15T22:07:48.721Z</updated>
        <summary type="html"><![CDATA[Deep-learning\textendash{centric} reversible steganography has emerged as a
promising research paradigm. A direct way of applying deep learning to
reversible steganography is to construct a pair of encoder and decoder, whose
parameters are trained jointly, thereby learning the steganographic system as a
whole. This end-to-end framework, however, falls short of the reversibility
requirement because it is difficult for this kind of monolithic system, as a
black box, to create or duplicate intricate reversible mechanisms. In response
to this issue, a recent approach is to carve up the steganographic system and
work on modules independently. In particular, neural networks are deployed in
an analytics module to learn the data distribution, while an established
mechanism is called upon to handle the remaining tasks. In this paper, we
investigate the modular framework and deploy deep neural networks in a
reversible steganographic scheme referred to as prediction-error modulation, in
which an analytics module serves the purpose of pixel intensity prediction. The
primary focus of this study is on deep-learning\textendash{based} context-aware
pixel intensity prediction. We address the unsolved issues reported in related
literature, including the impact of pixel initialisation on prediction accuracy
and the influence of uncertainty propagation in dual-layer embedding.
Furthermore, we establish a connection between context-aware pixel intensity
prediction and low-level computer vision and analyse the performance of several
advanced neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Ching-Chun Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sisheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1"&gt;Isao Echizen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1"&gt;Victor Sanchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chang-Tsun Li&lt;/a&gt;</name>
        </author>
    </entry>
</feed>