<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-06-17T15:44:17.264Z</updated>
    <generator>osmosfeed 1.10.2</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Network Space Search for Pareto-Efficient Spaces. (arXiv:2104.11014v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11014</id>
        <link href="http://arxiv.org/abs/2104.11014"/>
        <updated>2021-06-17T15:44:17.182Z</updated>
        <summary type="html"><![CDATA[Network spaces have been known as a critical factor in both handcrafted
network designs or defining search spaces for Neural Architecture Search (NAS).
However, an effective space involves tremendous prior knowledge and/or manual
effort, and additional constraints are required to discover efficiency-aware
architectures. In this paper, we define a new problem, Network Space Search
(NSS), as searching for favorable network spaces instead of a single
architecture. We propose an NSS method to directly search for efficient-aware
network spaces automatically, reducing the manual effort and immense cost in
discovering satisfactory ones. The resultant network spaces, named Elite
Spaces, are discovered from Expanded Search Space with minimal human expertise
imposed. The Pareto-efficient Elite Spaces are aligned with the Pareto front
under various complexity constraints and can be further served as NAS search
spaces, benefiting differentiable NAS approaches (e.g. In CIFAR-100, an
averagely 2.3% lower error rate and 3.7% closer to target constraint than the
baseline with around 90% fewer samples required to find satisfactory networks).
Moreover, our NSS approach is capable of searching for superior spaces in
future unexplored spaces, revealing great potential in searching for network
spaces automatically. Website:
https://minhungchen.netlify.app/publication/nss/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1"&gt;Min-Fong Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao-Yun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Min-Hung Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yu-Syuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1"&gt;Hsien-Kai Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yi-Min Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hung-Jen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jou_K/0/1/0/all/0/1"&gt;Kevin Jou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FetReg: Placental Vessel Segmentation and Registration in Fetoscopy Challenge Dataset. (arXiv:2106.05923v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05923</id>
        <link href="http://arxiv.org/abs/2106.05923"/>
        <updated>2021-06-17T15:44:17.153Z</updated>
        <summary type="html"><![CDATA[Fetoscopy laser photocoagulation is a widely used procedure for the treatment
of Twin-to-Twin Transfusion Syndrome (TTTS), that occur in mono-chorionic
multiple pregnancies due to placental vascular anastomoses. This procedure is
particularly challenging due to limited field of view, poor manoeuvrability of
the fetoscope, poor visibility due to fluid turbidity, variability in light
source, and unusual position of the placenta. This may lead to increased
procedural time and incomplete ablation, resulting in persistent TTTS.
Computer-assisted intervention may help overcome these challenges by expanding
the fetoscopic field of view through video mosaicking and providing better
visualization of the vessel network. However, the research and development in
this domain remain limited due to unavailability of high-quality data to encode
the intra- and inter-procedure variability. Through the \textit{Fetoscopic
Placental Vessel Segmentation and Registration (FetReg)} challenge, we present
a large-scale multi-centre dataset for the development of generalized and
robust semantic segmentation and video mosaicking algorithms for the fetal
environment with a focus on creating drift-free mosaics from long duration
fetoscopy videos. In this paper, we provide an overview of the FetReg dataset,
challenge tasks, evaluation metrics and baseline methods for both segmentation
and registration. Baseline methods results on the FetReg dataset shows that our
dataset poses interesting challenges, offering large opportunity for the
creation of novel methods and models through a community effort initiative
guided by the FetReg challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1"&gt;Sophia Bano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casella_A/0/1/0/all/0/1"&gt;Alessandro Casella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1"&gt;Francisco Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moccia_S/0/1/0/all/0/1"&gt;Sara Moccia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Attilakos_G/0/1/0/all/0/1"&gt;George Attilakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wimalasundera_R/0/1/0/all/0/1"&gt;Ruwan Wimalasundera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1"&gt;Anna L. David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paladini_D/0/1/0/all/0/1"&gt;Dario Paladini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deprest_J/0/1/0/all/0/1"&gt;Jan Deprest&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momi_E/0/1/0/all/0/1"&gt;Elena De Momi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattos_L/0/1/0/all/0/1"&gt;Leonardo S. Mattos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separation Results between Fixed-Kernel and Feature-Learning Probability Metrics. (arXiv:2106.05739v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05739</id>
        <link href="http://arxiv.org/abs/2106.05739"/>
        <updated>2021-06-17T15:44:17.143Z</updated>
        <summary type="html"><![CDATA[Several works in implicit and explicit generative modeling empirically
observed that feature-learning discriminators outperform fixed-kernel
discriminators in terms of the sample quality of the models. We provide
separation results between probability metrics with fixed-kernel and
feature-learning discriminators using the function classes $\mathcal{F}_2$ and
$\mathcal{F}_1$ respectively, which were developed to study overparametrized
two-layer neural networks. In particular, we construct pairs of distributions
over hyper-spheres that can not be discriminated by fixed kernel
$(\mathcal{F}_2)$ integral probability metric (IPM) and Stein discrepancy (SD)
in high dimensions, but that can be discriminated by their feature learning
($\mathcal{F}_1$) counterparts. To further study the separation we provide
links between the $\mathcal{F}_1$ and $\mathcal{F}_2$ IPMs with sliced
Wasserstein distances. Our work suggests that fixed-kernel discriminators
perform worse than their feature learning counterparts because their
corresponding metrics are weaker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Domingo_Enrich_C/0/1/0/all/0/1"&gt;Carles Domingo-Enrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mroueh_Y/0/1/0/all/0/1"&gt;Youssef Mroueh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses. (arXiv:2106.05426v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05426</id>
        <link href="http://arxiv.org/abs/2106.05426"/>
        <updated>2021-06-17T15:44:17.131Z</updated>
        <summary type="html"><![CDATA[How related are the representations learned by neural language models,
translation models, and language tagging tasks? We answer this question by
adapting an encoder-decoder transfer learning method from computer vision to
investigate the structure among 100 different feature spaces extracted from
hidden representations of various networks trained on language tasks. This
method reveals a low-dimensional structure where language models and
translation models smoothly interpolate between word embeddings, syntactic and
semantic tasks, and future word embeddings. We call this low-dimensional
structure a language representation embedding because it encodes the
relationships between representations needed to process language for a variety
of NLP tasks. We find that this representation embedding can predict how well
each individual feature space maps to human brain responses to natural language
stimuli recorded using fMRI. Additionally, we find that the principal dimension
of this structure can be used to create a metric which highlights the brain's
natural language processing hierarchy. This suggests that the embedding
captures some part of the brain's natural language representation structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1"&gt;Richard Antonello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1"&gt;Javier Turek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1"&gt;Vy Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1"&gt;Alexander Huth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FetReg: Placental Vessel Segmentation and Registration in Fetoscopy Challenge Dataset. (arXiv:2106.05923v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05923</id>
        <link href="http://arxiv.org/abs/2106.05923"/>
        <updated>2021-06-17T15:44:16.763Z</updated>
        <summary type="html"><![CDATA[Fetoscopy laser photocoagulation is a widely used procedure for the treatment
of Twin-to-Twin Transfusion Syndrome (TTTS), that occur in mono-chorionic
multiple pregnancies due to placental vascular anastomoses. This procedure is
particularly challenging due to limited field of view, poor manoeuvrability of
the fetoscope, poor visibility due to fluid turbidity, variability in light
source, and unusual position of the placenta. This may lead to increased
procedural time and incomplete ablation, resulting in persistent TTTS.
Computer-assisted intervention may help overcome these challenges by expanding
the fetoscopic field of view through video mosaicking and providing better
visualization of the vessel network. However, the research and development in
this domain remain limited due to unavailability of high-quality data to encode
the intra- and inter-procedure variability. Through the \textit{Fetoscopic
Placental Vessel Segmentation and Registration (FetReg)} challenge, we present
a large-scale multi-centre dataset for the development of generalized and
robust semantic segmentation and video mosaicking algorithms for the fetal
environment with a focus on creating drift-free mosaics from long duration
fetoscopy videos. In this paper, we provide an overview of the FetReg dataset,
challenge tasks, evaluation metrics and baseline methods for both segmentation
and registration. Baseline methods results on the FetReg dataset shows that our
dataset poses interesting challenges, offering large opportunity for the
creation of novel methods and models through a community effort initiative
guided by the FetReg challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1"&gt;Sophia Bano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casella_A/0/1/0/all/0/1"&gt;Alessandro Casella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1"&gt;Francisco Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moccia_S/0/1/0/all/0/1"&gt;Sara Moccia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Attilakos_G/0/1/0/all/0/1"&gt;George Attilakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wimalasundera_R/0/1/0/all/0/1"&gt;Ruwan Wimalasundera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1"&gt;Anna L. David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paladini_D/0/1/0/all/0/1"&gt;Dario Paladini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deprest_J/0/1/0/all/0/1"&gt;Jan Deprest&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momi_E/0/1/0/all/0/1"&gt;Elena De Momi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattos_L/0/1/0/all/0/1"&gt;Leonardo S. Mattos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Space Search for Pareto-Efficient Spaces. (arXiv:2104.11014v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11014</id>
        <link href="http://arxiv.org/abs/2104.11014"/>
        <updated>2021-06-17T15:44:16.740Z</updated>
        <summary type="html"><![CDATA[Network spaces have been known as a critical factor in both handcrafted
network designs or defining search spaces for Neural Architecture Search (NAS).
However, an effective space involves tremendous prior knowledge and/or manual
effort, and additional constraints are required to discover efficiency-aware
architectures. In this paper, we define a new problem, Network Space Search
(NSS), as searching for favorable network spaces instead of a single
architecture. We propose an NSS method to directly search for efficient-aware
network spaces automatically, reducing the manual effort and immense cost in
discovering satisfactory ones. The resultant network spaces, named Elite
Spaces, are discovered from Expanded Search Space with minimal human expertise
imposed. The Pareto-efficient Elite Spaces are aligned with the Pareto front
under various complexity constraints and can be further served as NAS search
spaces, benefiting differentiable NAS approaches (e.g. In CIFAR-100, an
averagely 2.3% lower error rate and 3.7% closer to target constraint than the
baseline with around 90% fewer samples required to find satisfactory networks).
Moreover, our NSS approach is capable of searching for superior spaces in
future unexplored spaces, revealing great potential in searching for network
spaces automatically. Website:
https://minhungchen.netlify.app/publication/nss/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1"&gt;Min-Fong Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao-Yun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Min-Hung Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yu-Syuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1"&gt;Hsien-Kai Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yi-Min Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hung-Jen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jou_K/0/1/0/all/0/1"&gt;Kevin Jou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SVMA: A GAN-based model for Monocular 3D Human Pose Estimation. (arXiv:2106.05616v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05616</id>
        <link href="http://arxiv.org/abs/2106.05616"/>
        <updated>2021-06-17T15:44:16.717Z</updated>
        <summary type="html"><![CDATA[Recovering 3D human pose from 2D joints is a highly unconstrained problem,
especially without any video or multi-view information. We present an
unsupervised GAN-based model to recover 3D human pose from 2D joint locations
extracted from a single image. Our model uses a GAN to learn the mapping of
distribution from 2D poses to 3D poses, not the simple 2D-3D correspondence.
Considering the reprojection constraint, our model can estimate the camera so
that we can reproject the estimated 3D pose to the original 2D pose. Based on
this reprojection method, we can rotate and reproject the generated pose to get
our "new" 2D pose and then use a weight sharing generator to estimate the "new"
3D pose and a "new" camera. Through the above estimation process, we can define
the single-view-multi-angle consistency loss during training to simulate
multi-view consistency, which means the 3D poses and cameras estimated from two
angles of a single view should be able to be mixed to generate rich 2D
reprojections, and the 2D reprojections reprojected from the same 3D pose
should be consistent. The experimental results on Human3.6M show that our
method outperforms all the state-of-the-art methods, and results on
MPI-INF-3DHP show that our method outperforms state-of-the-art by approximately
15.0%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yicheng Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yongqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiahui Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses. (arXiv:2106.05426v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05426</id>
        <link href="http://arxiv.org/abs/2106.05426"/>
        <updated>2021-06-17T15:44:16.262Z</updated>
        <summary type="html"><![CDATA[How related are the representations learned by neural language models,
translation models, and language tagging tasks? We answer this question by
adapting an encoder-decoder transfer learning method from computer vision to
investigate the structure among 100 different feature spaces extracted from
hidden representations of various networks trained on language tasks. This
method reveals a low-dimensional structure where language models and
translation models smoothly interpolate between word embeddings, syntactic and
semantic tasks, and future word embeddings. We call this low-dimensional
structure a language representation embedding because it encodes the
relationships between representations needed to process language for a variety
of NLP tasks. We find that this representation embedding can predict how well
each individual feature space maps to human brain responses to natural language
stimuli recorded using fMRI. Additionally, we find that the principal dimension
of this structure can be used to create a metric which highlights the brain's
natural language processing hierarchy. This suggests that the embedding
captures some part of the brain's natural language representation structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1"&gt;Richard Antonello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1"&gt;Javier Turek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1"&gt;Vy Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1"&gt;Alexander Huth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Predictive Control with and without Terminal Weight: Stability and Algorithms. (arXiv:2011.14193v2 [eess.SY] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2011.14193</id>
        <link href="http://arxiv.org/abs/2011.14193"/>
        <updated>2021-06-17T01:58:46.948Z</updated>
        <summary type="html"><![CDATA[This paper presents stability analysis tools for model predictive control
(MPC) with and without terminal weight. Stability analysis of MPC with a
limited horizon but without terminal weight is a long-standing open problem. By
using a modified value function as an Lyapunov function candidate and the
principle of optimality, this paper establishes stability conditions for this
type of widely spread MPC algorithms. A new stability guaranteed MPC algorithm
without terminal weight (MPCS) is presented. With the help of designing a new
sublevel set defined by the value function of one-step ahead stage cost,
conditions for checking its recursive feasibility and stability of the proposed
MPC algorithm are presented. The new stability condition and the derived MPCS
overcome the difficulties arising in the existing terminal weight based MPC
framework, including the need of searching a suitable terminal weight and
possible poor performance caused by an inappropriate terminal weight. This work
is further extended to MPC with a terminal weight for the completeness.
Numerical examples are presented to demonstrate the effectiveness of the
proposed tool, whereas the existing stability analysis tools are either not
applicable or lead to quite conservative results. It shows that the proposed
tools offer a number of mechanisms to achieve stability: adjusting state and/or
control weights, extending the length of horizon, and adding a simple extra
constraint on the first or second state in the optimisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wen-Hua Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold-Aware Deep Clustering: Maximizing Angles between Embedding Vectors Based on Regular Simplex. (arXiv:2106.02331v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02331</id>
        <link href="http://arxiv.org/abs/2106.02331"/>
        <updated>2021-06-17T01:58:46.936Z</updated>
        <summary type="html"><![CDATA[This paper presents a new deep clustering (DC) method called manifold-aware
DC (M-DC) that can enhance hyperspace utilization more effectively than the
original DC. The original DC has a limitation in that a pair of two speakers
has to be embedded having an orthogonal relationship due to its use of the
one-hot vector-based loss function, while our method derives a unique loss
function aimed at maximizing the target angle in the hyperspace based on the
nature of a regular simplex. Our proposed loss imposes a higher penalty than
the original DC when the speaker is assigned incorrectly. The change from DC to
M-DC can be easily achieved by rewriting just one term in the loss function of
DC, without any other modifications to the network architecture or model
parameters. As such, our method has high practicability because it does not
affect the original inference part. The experimental results show that the
proposed method improves the performances of the original DC and its expansion
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tanaka_K/0/1/0/all/0/1"&gt;Keitaro Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sawata_R/0/1/0/all/0/1"&gt;Ryosuke Sawata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Takahashi_S/0/1/0/all/0/1"&gt;Shusuke Takahashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Less is More: A privacy-respecting Android malware classifier using Federated Learning. (arXiv:2007.08319v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08319</id>
        <link href="http://arxiv.org/abs/2007.08319"/>
        <updated>2021-06-17T01:58:46.932Z</updated>
        <summary type="html"><![CDATA[In this paper we present LiM ("Less is More"), a malware classification
framework that leverages Federated Learning to detect and classify malicious
apps in a privacy-respecting manner. Information about newly installed apps is
kept locally on users' devices, so that the provider cannot infer which apps
were installed by users. At the same time, input from all users is taken into
account in the federated learning process and they all benefit from better
classification performance. A key challenge of this setting is that users do
not have access to the ground truth (i.e. they cannot correctly identify
whether an app is malicious). To tackle this, LiM uses a safe semi-supervised
ensemble that maximizes classification accuracy with respect to a baseline
classifier trained by the service provider (i.e. the cloud). We implement LiM
and show that the cloud server has F1 score of 95%, while clients have perfect
recall with only 1 false positive in >100 apps, using a dataset of 25K clean
apps and 25K malicious apps, 200 users and 50 rounds of federation.
Furthermore, we conduct a security analysis and demonstrate that LiM is robust
against both poisoning attacks by adversaries who control half of the clients,
and inference attacks performed by an honest-but-curious cloud server. Further
experiments with MaMaDroid's dataset confirm resistance against poisoning
attacks and a performance improvement due to the federation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galvez_R/0/1/0/all/0/1"&gt;Rafa G&amp;#xe1;lvez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moonsamy_V/0/1/0/all/0/1"&gt;Veelasha Moonsamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_C/0/1/0/all/0/1"&gt;Claudia Diaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Topic Coverage Approach to Evaluation of Topic Models. (arXiv:2012.06274v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06274</id>
        <link href="http://arxiv.org/abs/2012.06274"/>
        <updated>2021-06-17T01:58:46.925Z</updated>
        <summary type="html"><![CDATA[Topic models are widely used unsupervised models of text capable of learning
topics - weighted lists of words and documents - from large collections of text
documents. When topic models are used for discovery of topics in text
collections, a question that arises naturally is how well the model-induced
topics correspond to topics of interest to the analyst. In this paper we
revisit and extend a so far neglected approach to topic model evaluation based
on measuring topic coverage - computationally matching model topics with a set
of reference topics that models are expected to uncover. The approach is well
suited for analyzing models' performance in topic discovery and for large-scale
analysis of both topic models and measures of model quality. We propose new
measures of coverage and evaluate, in a series of experiments, different types
of topic models on two distinct text domains for which interest for topic
discovery exists. The experiments include evaluation of model quality, analysis
of coverage of distinct topic categories, and the analysis of the relationship
between coverage and other methods of topic model evaluation. The contributions
of the paper include new measures of coverage, insights into both topic models
and other methods of model evaluation, and the datasets and code for
facilitating future research of both topic coverage and other approaches to
topic model evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korencic_D/0/1/0/all/0/1"&gt;Damir Koren&amp;#x10d;i&amp;#x107;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ristov_S/0/1/0/all/0/1"&gt;Strahil Ristov&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Repar_J/0/1/0/all/0/1"&gt;Jelena Repar&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1"&gt;Jan &amp;#x160;najder&lt;/a&gt; (2) ((1) Rudjer Bo&amp;#x161;kovi&amp;#x107; Institute, Croatia, (2) University of Zagreb, Faculty of Electrical Engineering and Computing, Croatia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deriving Autism Spectrum Disorder Functional Networks from RS-FMRI Data using Group ICA and Dictionary Learning. (arXiv:2106.09000v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.09000</id>
        <link href="http://arxiv.org/abs/2106.09000"/>
        <updated>2021-06-17T01:58:46.920Z</updated>
        <summary type="html"><![CDATA[The objective of this study is to derive functional networks for the autism
spectrum disorder (ASD) population using the group ICA and dictionary learning
model together and to classify ASD and typically developing (TD) participants
using the functional connectivity calculated from the derived functional
networks. In our experiments, the ASD functional networks were derived from
resting-state functional magnetic resonance imaging (rs-fMRI) data. We
downloaded a total of 120 training samples, including 58 ASD and 62 TD
participants, which were obtained from the public repository: Autism Brain
Imaging Data Exchange I (ABIDE I). Our methodology and results have five main
parts. First, we utilize a group ICA model to extract functional networks from
the ASD group and rank the top 20 regions of interest (ROIs). Second, we
utilize a dictionary learning model to extract functional networks from the ASD
group and rank the top 20 ROIs. Third, we merged the 40 selected ROIs from the
two models together as the ASD functional networks. Fourth, we generate three
corresponding masks based on the 20 selected ROIs from group ICA, the 20 ROIs
selected from dictionary learning, and the 40 combined ROIs selected from both.
Finally, we extract ROIs for all training samples using the above three masks,
and the calculated functional connectivity was used as features for ASD and TD
classification. The classification results showed that the functional networks
derived from ICA and dictionary learning together outperform those derived from
a single ICA model or a single dictionary learning model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ning Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wang_D/0/1/0/all/0/1"&gt;Donglin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outside the Echo Chamber: Optimizing the Performative Risk. (arXiv:2102.08570v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08570</id>
        <link href="http://arxiv.org/abs/2102.08570"/>
        <updated>2021-06-17T01:58:46.914Z</updated>
        <summary type="html"><![CDATA[In performative prediction, predictions guide decision-making and hence can
influence the distribution of future data. To date, work on performative
prediction has focused on finding performatively stable models, which are the
fixed points of repeated retraining. However, stable solutions can be far from
optimal when evaluated in terms of the performative risk, the loss experienced
by the decision maker when deploying a model. In this paper, we shift attention
beyond performative stability and focus on optimizing the performative risk
directly. We identify a natural set of properties of the loss function and
model-induced distribution shift under which the performative risk is convex, a
property which does not follow from convexity of the loss alone. Furthermore,
we develop algorithms that leverage our structural assumptions to optimize the
performative risk with better sample efficiency than generic methods for
derivative-free convex optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1"&gt;John Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perdomo_J/0/1/0/all/0/1"&gt;Juan C. Perdomo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zrnic_T/0/1/0/all/0/1"&gt;Tijana Zrnic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data. (arXiv:2010.03622v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03622</id>
        <link href="http://arxiv.org/abs/2010.03622"/>
        <updated>2021-06-17T01:58:46.909Z</updated>
        <summary type="html"><![CDATA[Self-training algorithms, which train a model to fit pseudolabels predicted
by another previously-learned model, have been very successful for learning
with unlabeled data using neural networks. However, the current theoretical
understanding of self-training only applies to linear models. This work
provides a unified theoretical analysis of self-training with deep networks for
semi-supervised learning, unsupervised domain adaptation, and unsupervised
learning. At the core of our analysis is a simple but realistic "expansion"
assumption, which states that a low probability subset of the data must expand
to a neighborhood with large probability relative to the subset. We also assume
that neighborhoods of examples in different classes have minimal overlap. We
prove that under these assumptions, the minimizers of population objectives
based on self-training and input-consistency regularization will achieve high
accuracy with respect to ground-truth labels. By using off-the-shelf
generalization bounds, we immediately convert this result to sample complexity
guarantees for neural nets that are polynomial in the margin and Lipschitzness.
Our results help explain the empirical successes of recently proposed
self-training algorithms which use input consistency regularization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Colin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1"&gt;Kendrick Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yining Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved CNN-based Learning of Interpolation Filters for Low-Complexity Inter Prediction in Video Coding. (arXiv:2106.08936v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08936</id>
        <link href="http://arxiv.org/abs/2106.08936"/>
        <updated>2021-06-17T01:58:46.903Z</updated>
        <summary type="html"><![CDATA[The versatility of recent machine learning approaches makes them ideal for
improvement of next generation video compression solutions. Unfortunately,
these approaches typically bring significant increases in computational
complexity and are difficult to interpret into explainable models, affecting
their potential for implementation within practical video coding applications.
This paper introduces a novel explainable neural network-based inter-prediction
scheme, to improve the interpolation of reference samples needed for fractional
precision motion compensation. The approach requires a single neural network to
be trained from which a full quarter-pixel interpolation filter set is derived,
as the network is easily interpretable due to its linear structure. A novel
training framework enables each network branch to resemble a specific
fractional shift. This practical solution makes it very efficient to use
alongside conventional video coding schemes. When implemented in the context of
the state-of-the-art Versatile Video Coding (VVC) test model, 0.77%, 1.27% and
2.25% BD-rate savings can be achieved on average for lower resolution sequences
under the random access, low-delay B and low-delay P configurations,
respectively, while the complexity of the learned interpolation schemes is
significantly reduced compared to the interpolation with full CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1"&gt;Luka Murn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blasi_S/0/1/0/all/0/1"&gt;Saverio Blasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smeaton_A/0/1/0/all/0/1"&gt;Alan F. Smeaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1"&gt;Marta Mrak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FGLP: A Federated Fine-Grained Location Prediction System for Mobile Users. (arXiv:2106.08946v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08946</id>
        <link href="http://arxiv.org/abs/2106.08946"/>
        <updated>2021-06-17T01:58:46.882Z</updated>
        <summary type="html"><![CDATA[Fine-grained location prediction on smart phones can be used to improve
app/system performance. Application scenarios include video quality adaptation
as a function of the 5G network quality at predicted user locations, and
augmented reality apps that speed up content rendering based on predicted user
locations. Such use cases require prediction error in the same range as the GPS
error, and no existing works on location prediction can achieve this level of
accuracy. We present a system for fine-grained location prediction (FGLP) of
mobile users, based on GPS traces collected on the phones. FGLP has two
components: a federated learning framework and a prediction model. The
framework runs on the phones of the users and also on a server that coordinates
learning from all users in the system. FGLP represents the user location data
as relative points in an abstract 2D space, which enables learning across
different physical spaces. The model merges Bidirectional Long Short-Term
Memory (BiLSTM) and Convolutional Neural Networks (CNN), where BiLSTM learns
the speed and direction of the mobile users, and CNN learns information such as
user movement preferences. FGLP uses federated learning to protect user privacy
and reduce bandwidth consumption. Our experimental results, using a dataset
with over 600,000 users, demonstrate that FGLP outperforms baseline models in
terms of prediction accuracy. We also demonstrate that FGLP works well in
conjunction with transfer learning, which enables model reusability. Finally,
benchmark results on several types of Android phones demonstrate FGLP's
feasibility in real life.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaopeng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobson_G/0/1/0/all/0/1"&gt;Guy Jacobson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jana_R/0/1/0/all/0/1"&gt;Rittwik Jana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wen-Ling Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talasila_M/0/1/0/all/0/1"&gt;Manoop Talasila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aftab_S/0/1/0/all/0/1"&gt;Syed Anwar Aftab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borcea_C/0/1/0/all/0/1"&gt;Cristian Borcea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RefBERT: Compressing BERT by Referencing to Pre-computed Representations. (arXiv:2106.08898v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08898</id>
        <link href="http://arxiv.org/abs/2106.08898"/>
        <updated>2021-06-17T01:58:46.875Z</updated>
        <summary type="html"><![CDATA[Recently developed large pre-trained language models, e.g., BERT, have
achieved remarkable performance in many downstream natural language processing
applications. These pre-trained language models often contain hundreds of
millions of parameters and suffer from high computation and latency in
real-world applications. It is desirable to reduce the computation overhead of
the models for fast training and inference while keeping the model performance
in downstream applications. Several lines of work utilize knowledge
distillation to compress the teacher model to a smaller student model. However,
they usually discard the teacher's knowledge when in inference. Differently, in
this paper, we propose RefBERT to leverage the knowledge learned from the
teacher, i.e., facilitating the pre-computed BERT representation on the
reference sample and compressing BERT into a smaller student model. To
guarantee our proposal, we provide theoretical justification on the loss
function and the usage of reference samples. Significantly, the theoretical
result shows that including the pre-computed teacher's representations on the
reference samples indeed increases the mutual information in learning the
student model. Finally, we conduct the empirical evaluation and show that our
RefBERT can beat the vanilla TinyBERT over 8.1\% and achieves more than 94\% of
the performance of $\BERTBASE$ on the GLUE benchmark. Meanwhile, RefBERT is
7.4x smaller and 9.5x faster on inference than BERT$_{\rm BASE}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haiqin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1"&gt;Yang Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jianping Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explicitly Encouraging Low Fractional Dimensional Trajectories Via Reinforcement Learning. (arXiv:2012.11662v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11662</id>
        <link href="http://arxiv.org/abs/2012.11662"/>
        <updated>2021-06-17T01:58:46.865Z</updated>
        <summary type="html"><![CDATA[A key limitation in using various modern methods of machine learning in
developing feedback control policies is the lack of appropriate methodologies
to analyze their long-term dynamics, in terms of making any sort of guarantees
(even statistically) about robustness. The central reasons for this are largely
due to the so-called curse of dimensionality, combined with the black-box
nature of the resulting control policies themselves. This paper aims at the
first of these issues. Although the full state space of a system may be quite
large in dimensionality, it is a common feature of most model-based control
methods that the resulting closed-loop systems demonstrate dominant dynamics
that are rapidly driven to some lower-dimensional sub-space within. In this
work we argue that the dimensionality of this subspace is captured by tools
from fractal geometry, namely various notions of a fractional dimension. We
then show that the dimensionality of trajectories induced by model free
reinforcement learning agents can be influenced adding a post processing
function to the agents reward signal. We verify that the dimensionality
reduction is robust to noise being added to the system and show that that the
modified agents are more actually more robust to noise and push disturbances in
general for the systems we examined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gillen_S/0/1/0/all/0/1"&gt;Sean Gillen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byl_K/0/1/0/all/0/1"&gt;Katie Byl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably End-to-end Label-Noise Learning without Anchor Points. (arXiv:2102.02400v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02400</id>
        <link href="http://arxiv.org/abs/2102.02400"/>
        <updated>2021-06-17T01:58:46.844Z</updated>
        <summary type="html"><![CDATA[In label-noise learning, the transition matrix plays a key role in building
statistically consistent classifiers. Existing consistent estimators for the
transition matrix have been developed by exploiting anchor points. However, the
anchor-point assumption is not always satisfied in real scenarios. In this
paper, we propose an end-to-end framework for solving label-noise learning
without anchor points, in which we simultaneously optimize two objectives: the
cross entropy loss between the noisy label and the predicted probability by the
neural network, and the volume of the simplex formed by the columns of the
transition matrix. Our proposed framework can identify the transition matrix if
the clean class-posterior probabilities are sufficiently scattered. This is by
far the mildest assumption under which the transition matrix is provably
identifiable and the learned classifier is statistically consistent.
Experimental results on benchmark datasets demonstrate the effectiveness and
robustness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuefeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning effective stochastic differential equations from microscopic simulations: combining stochastic numerics and deep learning. (arXiv:2106.09004v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2106.09004</id>
        <link href="http://arxiv.org/abs/2106.09004"/>
        <updated>2021-06-17T01:58:46.836Z</updated>
        <summary type="html"><![CDATA[We identify effective stochastic differential equations (SDE) for coarse
observables of fine-grained particle- or agent-based simulations; these SDE
then provide coarse surrogate models of the fine scale dynamics. We approximate
the drift and diffusivity functions in these effective SDE through neural
networks, which can be thought of as effective stochastic ResNets. The loss
function is inspired by, and embodies, the structure of established stochastic
numerical integrators (here, Euler-Maruyama and Milstein); our approximations
can thus benefit from error analysis of these underlying numerical schemes.
They also lend themselves naturally to "physics-informed" gray-box
identification when approximate coarse models, such as mean field equations,
are available. Our approach does not require long trajectories, works on
scattered snapshot data, and is designed to naturally handle different time
steps per snapshot. We consider both the case where the coarse collective
observables are known in advance, as well as the case where they must be found
in a data-driven manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Dietrich_F/0/1/0/all/0/1"&gt;Felix Dietrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Makeev_A/0/1/0/all/0/1"&gt;Alexei Makeev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kevrekidis_G/0/1/0/all/0/1"&gt;George Kevrekidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Evangelou_N/0/1/0/all/0/1"&gt;Nikolaos Evangelou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bertalan_T/0/1/0/all/0/1"&gt;Tom Bertalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Reich_S/0/1/0/all/0/1"&gt;Sebastian Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kevrekidis_I/0/1/0/all/0/1"&gt;Ioannis G. Kevrekidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonparametric Empirical Bayes Estimation and Testing for Sparse and Heteroscedastic Signals. (arXiv:2106.08881v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08881</id>
        <link href="http://arxiv.org/abs/2106.08881"/>
        <updated>2021-06-17T01:58:46.823Z</updated>
        <summary type="html"><![CDATA[Large-scale modern data often involves estimation and testing for
high-dimensional unknown parameters. It is desirable to identify the sparse
signals, ``the needles in the haystack'', with accuracy and false discovery
control. However, the unprecedented complexity and heterogeneity in modern data
structure require new machine learning tools to effectively exploit
commonalities and to robustly adjust for both sparsity and heterogeneity. In
addition, estimates for high-dimensional parameters often lack uncertainty
quantification. In this paper, we propose a novel Spike-and-Nonparametric
mixture prior (SNP) -- a spike to promote the sparsity and a nonparametric
structure to capture signals. In contrast to the state-of-the-art methods, the
proposed methods solve the estimation and testing problem at once with several
merits: 1) an accurate sparsity estimation; 2) point estimates with
shrinkage/soft-thresholding property; 3) credible intervals for uncertainty
quantification; 4) an optimal multiple testing procedure that controls false
discovery rate. Our method exhibits promising empirical performance on both
simulated data and a gene expression case study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Junhui Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritov_Y/0/1/0/all/0/1"&gt;Ya&amp;#x27;acov Ritov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Linda Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Attacks Against Deep Reinforcement Learning Policies. (arXiv:2106.08746v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08746</id>
        <link href="http://arxiv.org/abs/2106.08746"/>
        <updated>2021-06-17T01:58:46.812Z</updated>
        <summary type="html"><![CDATA[Recent work has discovered that deep reinforcement learning (DRL) policies
are vulnerable to adversarial examples. These attacks mislead the policy of DRL
agents by perturbing the state of the environment observed by agents. They are
feasible in principle but too slow to fool DRL policies in real time. We
propose a new attack to fool DRL policies that is both effective and efficient
enough to be mounted in real time. We utilize the Universal Adversarial
Perturbation (UAP) method to compute effective perturbations independent of the
individual inputs to which they are applied. Via an extensive evaluation using
Atari 2600 games, we show that our technique is effective, as it fully degrades
the performance of both deterministic and stochastic policies (up to 100%, even
when the $l_\infty$ bound on the perturbation is as small as 0.005). We also
show that our attack is efficient, incurring an online computational cost of
0.027ms on average. It is faster compared to the response time (0.6ms on
average) of agents with different DRL policies, and considerably faster than
prior attacks (2.7ms on average). Furthermore, we demonstrate that known
defenses are ineffective against universal perturbations. We propose an
effective detection technique which can form the basis for robust defenses
against attacks based on universal perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tekgul_B/0/1/0/all/0/1"&gt;Buse G.A. Tekgul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shelly Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marchal_S/0/1/0/all/0/1"&gt;Samuel Marchal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asokan_N/0/1/0/all/0/1"&gt;N. Asokan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Based Counterfactual Synthesizer for Interpretation. (arXiv:2106.08971v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08971</id>
        <link href="http://arxiv.org/abs/2106.08971"/>
        <updated>2021-06-17T01:58:46.798Z</updated>
        <summary type="html"><![CDATA[Counterfactuals, serving as one of the emerging type of model
interpretations, have recently received attention from both researchers and
practitioners. Counterfactual explanations formalize the exploration of
``what-if'' scenarios, and are an instance of example-based reasoning using a
set of hypothetical data samples. Counterfactuals essentially show how the
model decision alters with input perturbations. Existing methods for generating
counterfactuals are mainly algorithm-based, which are time-inefficient and
assume the same counterfactual universe for different queries. To address these
limitations, we propose a Model-based Counterfactual Synthesizer (MCS)
framework for interpreting machine learning models. We first analyze the
model-based counterfactual process and construct a base synthesizer using a
conditional generative adversarial net (CGAN). To better approximate the
counterfactual universe for those rare queries, we novelly employ the umbrella
sampling technique to conduct the MCS framework training. Besides, we also
enhance the MCS framework by incorporating the causal dependence among
attributes with model inductive bias, and validate its design correctness from
the causality identification perspective. Experimental results on several
datasets demonstrate the effectiveness as well as efficiency of our proposed
MCS framework, and verify the advantages compared with other alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alva_S/0/1/0/all/0/1"&gt;Sahan Suresh Alva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiahao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xia Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Off-Belief Learning. (arXiv:2103.04000v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04000</id>
        <link href="http://arxiv.org/abs/2103.04000"/>
        <updated>2021-06-17T01:58:46.782Z</updated>
        <summary type="html"><![CDATA[The standard problem setting in Dec-POMDPs is self-play, where the goal is to
find a set of policies that play optimally together. Policies learned through
self-play may adopt arbitrary conventions and implicitly rely on multi-step
reasoning based on fragile assumptions about other agents' actions and thus
fail when paired with humans or independently trained agents at test time. To
address this, we present off-belief learning (OBL). At each timestep OBL agents
follow a policy $\pi_1$ that is optimized assuming past actions were taken by a
given, fixed policy ($\pi_0$), but assuming that future actions will be taken
by $\pi_1$. When $\pi_0$ is uniform random, OBL converges to an optimal policy
that does not rely on inferences based on other agents' behavior (an optimal
grounded policy). OBL can be iterated in a hierarchy, where the optimal policy
from one level becomes the input to the next, thereby introducing multi-level
cognitive reasoning in a controlled manner. Unlike existing approaches, which
may converge to any equilibrium policy, OBL converges to a unique policy,
making it suitable for zero-shot coordination (ZSC). OBL can be scaled to
high-dimensional settings with a fictitious transition mechanism and shows
strong performance in both a toy-setting and the benchmark human-AI & ZSC
problem Hanabi.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hengyuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1"&gt;Adam Lerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Brandon Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;David Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1"&gt;Luis Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1"&gt;Noam Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1"&gt;Jakob Foerster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimized ensemble deep learning framework for scalable forecasting of dynamics containing extreme events. (arXiv:2106.08968v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08968</id>
        <link href="http://arxiv.org/abs/2106.08968"/>
        <updated>2021-06-17T01:58:46.776Z</updated>
        <summary type="html"><![CDATA[The remarkable flexibility and adaptability of both deep learning models and
ensemble methods have led to the proliferation for their application in
understanding many physical phenomena. Traditionally, these two techniques have
largely been treated as independent methodologies in practical applications.
This study develops an optimized ensemble deep learning (OEDL) framework
wherein these two machine learning techniques are jointly used to achieve
synergistic improvements in model accuracy, stability, scalability, and
reproducibility prompting a new wave of applications in the forecasting of
dynamics. Unpredictability is considered as one of the key features of chaotic
dynamics, so forecasting such dynamics of nonlinear systems is a relevant issue
in the scientific community. It becomes more challenging when the prediction of
extreme events is the focus issue for us. In this circumstance, the proposed
OEDL model based on a best convex combination of feed-forward neural networks,
reservoir computing, and long short-term memory can play a key role in
advancing predictions of dynamics consisting of extreme events. The combined
framework can generate the best out-of-sample performance than the individual
deep learners and standard ensemble framework for both numerically simulated
and real world data sets. We exhibit the outstanding performance of the OEDL
framework for forecasting extreme events generated from Lienard-type system,
prediction of COVID-19 cases in Brazil, dengue cases in San Juan, and sea
surface temperature in Nino 3.4 region.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1"&gt;Arnob Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanujit Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1"&gt;Dibakar Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Fair Policies in Decentralized Cooperative Multi-Agent Reinforcement Learning. (arXiv:2012.09421v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09421</id>
        <link href="http://arxiv.org/abs/2012.09421"/>
        <updated>2021-06-17T01:58:46.767Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning fair policies in (deep) cooperative
multi-agent reinforcement learning (MARL). We formalize it in a principled way
as the problem of optimizing a welfare function that explicitly encodes two
important aspects of fairness: efficiency and equity. As a solution method, we
propose a novel neural network architecture, which is composed of two
sub-networks specifically designed for taking into account the two aspects of
fairness. In experiments, we demonstrate the importance of the two sub-networks
for fair optimization. Our overall approach is general as it can accommodate
any (sub)differentiable welfare function. Therefore, it is compatible with
various notions of fairness that have been proposed in the literature (e.g.,
lexicographic maximin, generalized Gini social welfare function, proportional
fairness). Our solution method is generic and can be implemented in various
MARL settings: centralized training and decentralized execution, or fully
decentralized. Finally, we experimentally validate our approach in various
domains and show that it can perform much better than previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1"&gt;Matthieu Zimmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glanois_C/0/1/0/all/0/1"&gt;Claire Glanois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddique_U/0/1/0/all/0/1"&gt;Umer Siddique&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_P/0/1/0/all/0/1"&gt;Paul Weng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursive Construction of Stable Assemblies of Recurrent Neural Networks. (arXiv:2106.08928v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08928</id>
        <link href="http://arxiv.org/abs/2106.08928"/>
        <updated>2021-06-17T01:58:46.760Z</updated>
        <summary type="html"><![CDATA[Advanced applications of modern machine learning will likely involve
combinations of trained networks, as are already used in spectacular systems
such as DeepMind's AlphaGo. Recursively building such combinations in an
effective and stable fashion while also allowing for continual refinement of
the individual networks - as nature does for biological networks - will require
new analysis tools. This paper takes a step in this direction by establishing
contraction properties of broad classes of nonlinear recurrent networks and
neural ODEs, and showing how these quantified properties allow in turn to
recursively construct stable networks of networks in a systematic fashion. The
results can also be used to stably combine recurrent networks and physical
systems with quantified contraction properties. Similarly, they may be applied
to modular computational models of cognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ennis_M/0/1/0/all/0/1"&gt;Michaela Ennis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kozachkov_L/0/1/0/all/0/1"&gt;Leo Kozachkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slotine_J/0/1/0/all/0/1"&gt;Jean-Jacques Slotine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$C^3$: Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08914</id>
        <link href="http://arxiv.org/abs/2106.08914"/>
        <updated>2021-06-17T01:58:46.725Z</updated>
        <summary type="html"><![CDATA[Video-grounded dialogue systems aim to integrate video understanding and
dialogue understanding to generate responses that are relevant to both the
dialogue and video context. Most existing approaches employ deep learning
models and have achieved remarkable performance, given the relatively small
datasets available. However, the results are partly accomplished by exploiting
biases in the datasets rather than developing multimodal reasoning, resulting
in limited generalization. In this paper, we propose a novel approach of
Compositional Counterfactual Contrastive Learning ($C^3$) to develop
contrastive training between factual and counterfactual samples in
video-grounded dialogues. Specifically, we design factual/counterfactual
sampling based on the temporal steps in videos and tokens in dialogues and
propose contrastive loss functions that exploit object-level or action-level
variance. Different from prior approaches, we focus on contrastive hidden state
representations among compositional output tokens to optimize the
representation space in a generation setting. We achieved promising performance
gains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the
benefits of our approach in grounding video and dialogue context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hung Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nancy F. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1"&gt;Steven C.H. Hoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligent Tire-Based Slip Ratio Estimation Using Different Machine Learning Algorithms. (arXiv:2106.08961v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08961</id>
        <link href="http://arxiv.org/abs/2106.08961"/>
        <updated>2021-06-17T01:58:46.709Z</updated>
        <summary type="html"><![CDATA[Estimation of the longitudinal slip ratio of tires is important in boosting
the control performance of the vehicle under driving and braking conditions. In
this paper, the slip ratio is estimated using four machine learning algorithms
(Neural Network, Gradient Boosting Machine, Random Forest and Support Vector
Machine) based on the acceleration signals from the tri-axial MEMS
accelerometers utilized in the intelligent tire system. The experimental data
are collected through the MTS experimental platform. The corresponding
acceleration signals within the tire contact patch are extracted after
filtering to be used for the training the aforesaid machine learning
algorithms. A comparison is provided between the implemented ML algorithms
using a 10-fold CV. NRMS errors in the CV results indicate that NN has the
highest accuracy in comparison with other techniques. The NRSM errors of NN,
GBM, RF, and SVM are 2.59\%, 3.30\%, 4.21\%, and 5.34\%, respectively. Among
these techniques, GBM has a more stable results as it has the smallest output
variance. The present study with the fusion of intelligent tire system and
machine learning algorithms paves the way for the accurate estimation of tire
slip ratio, which is critical for the development of reliable vehicle control
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1"&gt;Nan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zepeng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianfeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Askari_H/0/1/0/all/0/1"&gt;Hassan Askari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Tikhonov: Faster Learning with Self-Concordant Losses via Iterative Regularization. (arXiv:2106.08855v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08855</id>
        <link href="http://arxiv.org/abs/2106.08855"/>
        <updated>2021-06-17T01:58:46.703Z</updated>
        <summary type="html"><![CDATA[The theory of spectral filtering is a remarkable tool to understand the
statistical properties of learning with kernels. For least squares, it allows
to derive various regularization schemes that yield faster convergence rates of
the excess risk than with Tikhonov regularization. This is typically achieved
by leveraging classical assumptions called source and capacity conditions,
which characterize the difficulty of the learning task. In order to understand
estimators derived from other loss functions, Marteau-Ferey et al. have
extended the theory of Tikhonov regularization to generalized self concordant
loss functions (GSC), which contain, e.g., the logistic loss. In this paper, we
go a step further and show that fast and optimal rates can be achieved for GSC
by using the iterated Tikhonov regularization scheme, which is intrinsically
related to the proximal point method in optimization, and overcomes the
limitation of the classical Tikhonov regularization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beugnot_G/0/1/0/all/0/1"&gt;Gaspard Beugnot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1"&gt;Julien Mairal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Economic Nowcasting with Long Short-Term Memory Artificial Neural Networks (LSTM). (arXiv:2106.08901v1 [econ.EM])]]></title>
        <id>http://arxiv.org/abs/2106.08901</id>
        <link href="http://arxiv.org/abs/2106.08901"/>
        <updated>2021-06-17T01:58:46.697Z</updated>
        <summary type="html"><![CDATA[Artificial neural networks (ANNs) have been the catalyst to numerous advances
in a variety of fields and disciplines in recent years. Their impact on
economics, however, has been comparatively muted. One type of ANN, the long
short-term memory network (LSTM), is particularly wellsuited to deal with
economic time-series. Here, the architecture's performance and characteristics
are evaluated in comparison with the dynamic factor model (DFM), currently a
popular choice in the field of economic nowcasting. LSTMs are found to produce
superior results to DFMs in the nowcasting of three separate variables; global
merchandise export values and volumes, and global services exports. Further
advantages include their ability to handle large numbers of input features in a
variety of time frequencies. A disadvantage is the inability to ascribe
contributions of input features to model outputs, common to all ANNs. In order
to facilitate continued applied research of the methodology by avoiding the
need for any knowledge of deep-learning libraries, an accompanying Python
library was developed using PyTorch, https://pypi.org/project/nowcast-lstm/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Hopp_D/0/1/0/all/0/1"&gt;Daniel Hopp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandit Modeling of Map Selection in Counter-Strike: Global Offensive. (arXiv:2106.08888v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08888</id>
        <link href="http://arxiv.org/abs/2106.08888"/>
        <updated>2021-06-17T01:58:46.673Z</updated>
        <summary type="html"><![CDATA[Many esports use a pick and ban process to define the parameters of a match
before it starts. In Counter-Strike: Global Offensive (CSGO) matches, two teams
first pick and ban maps, or virtual worlds, to play. Teams typically ban and
pick maps based on a variety of factors, such as banning maps which they do not
practice, or choosing maps based on the team's recent performance. We introduce
a contextual bandit framework to tackle the problem of map selection in CSGO
and to investigate teams' pick and ban decision-making. Using a data set of
over 3,500 CSGO matches and over 25,000 map selection decisions, we consider
different framings for the problem, different contexts, and different reward
metrics. We find that teams have suboptimal map choice policies with respect to
both picking and banning. We also define an approach for rewarding bans, which
has not been explored in the bandit setting, and find that incorporating ban
rewards improves model performance. Finally, we determine that usage of our
model could improve teams' predicted map win probability by up to 11% and raise
overall match win probabilities by 19.8% for evenly-matched teams.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petri_G/0/1/0/all/0/1"&gt;Guido Petri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanley_M/0/1/0/all/0/1"&gt;Michael H. Stanley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hon_A/0/1/0/all/0/1"&gt;Alec B. Hon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_A/0/1/0/all/0/1"&gt;Alexander Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xenopoulos_P/0/1/0/all/0/1"&gt;Peter Xenopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1"&gt;Cl&amp;#xe1;udio Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical and Private (Deep) Learning without Sampling or Shuffling. (arXiv:2103.00039v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00039</id>
        <link href="http://arxiv.org/abs/2103.00039"/>
        <updated>2021-06-17T01:58:46.610Z</updated>
        <summary type="html"><![CDATA[We consider training models with differential privacy (DP) using mini-batch
gradients. The existing state-of-the-art, Differentially Private Stochastic
Gradient Descent (DP-SGD), requires privacy amplification by sampling or
shuffling to obtain the best privacy/accuracy/computation trade-offs.
Unfortunately, the precise requirements on exact sampling and shuffling can be
hard to obtain in important practical scenarios, particularly federated
learning (FL). We design and analyze a DP variant of
Follow-The-Regularized-Leader (DP-FTRL) that compares favorably (both
theoretically and empirically) to amplified DP-SGD, while allowing for much
more flexible data access patterns. DP-FTRL does not use any form of privacy
amplification.

The code is available at
https://github.com/google-research/federated/tree/master/dp_ftrl and
https://github.com/google-research/DP-FTRL .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1"&gt;Peter Kairouz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McMahan_B/0/1/0/all/0/1"&gt;Brendan McMahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shuang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakkar_O/0/1/0/all/0/1"&gt;Om Thakkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakurta_A/0/1/0/all/0/1"&gt;Abhradeep Thakurta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zheng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch. (arXiv:2106.08970v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08970</id>
        <link href="http://arxiv.org/abs/2106.08970"/>
        <updated>2021-06-17T01:58:46.568Z</updated>
        <summary type="html"><![CDATA[As the curation of data for machine learning becomes increasingly automated,
dataset tampering is a mounting threat. Backdoor attackers tamper with training
data to embed a vulnerability in models that are trained on that data. This
vulnerability is then activated at inference time by placing a "trigger" into
the model's input. Typical backdoor attacks insert the trigger directly into
the training data, although the presence of such an attack may be visible upon
inspection. In contrast, the Hidden Trigger Backdoor Attack achieves poisoning
without placing a trigger into the training data at all. However, this hidden
trigger attack is ineffective at poisoning neural networks trained from
scratch. We develop a new hidden trigger attack, Sleeper Agent, which employs
gradient matching, data selection, and target model re-training during the
crafting process. Sleeper Agent is the first hidden trigger backdoor attack to
be effective against neural networks trained from scratch. We demonstrate its
effectiveness on ImageNet and in black-box settings. Our implementation code
can be found at https://github.com/hsouri/Sleeper-Agent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Souri_H/0/1/0/all/0/1"&gt;Hossein Souri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1"&gt;Liam Fowl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1"&gt;Rama Chellappa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super-k: A Piecewise Linear Classifier Based on Voronoi Tessellations. (arXiv:2012.15492v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15492</id>
        <link href="http://arxiv.org/abs/2012.15492"/>
        <updated>2021-06-17T01:58:46.497Z</updated>
        <summary type="html"><![CDATA[Voronoi tessellations are used to partition the Euclidean space into
polyhedral regions, which are called Voronoi cells. Labeling the Voronoi cells
with the class information, we can map any classification problem into a
Voronoi tessellation. In this way, the classification problem changes into a
query of just finding the enclosing Voronoi cell. In order to accomplish this
task, we have developed a new algorithm which generates a labeled Voronoi
tessellation that partitions the training data into polyhedral regions and
obtains interclass boundaries as an indirect result. It is called Supervised
k-Voxels or in short Super-k. We are introducing Super-k as a foundational new
algorithm and opening the possibility of a new family of algorithms. In this
paper, it is shown via comparisons on certain datasets that the Super-k
algorithm has the potential of providing comparable performance of the
well-known SVM family of algorithms with less complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zengin_R/0/1/0/all/0/1"&gt;Rahman Salim Zengin&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Sezer_V/0/1/0/all/0/1"&gt;Volkan Sezer&lt;/a&gt; (1) ((1) Istanbul Technical University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Federated Learning with Compensated Overlap-FedAvg. (arXiv:2012.06706v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06706</id>
        <link href="http://arxiv.org/abs/2012.06706"/>
        <updated>2021-06-17T01:58:46.455Z</updated>
        <summary type="html"><![CDATA[Petabytes of data are generated each day by emerging Internet of Things
(IoT), but only few of them can be finally collected and used for Machine
Learning (ML) purposes due to the apprehension of data & privacy leakage, which
seriously retarding ML's growth. To alleviate this problem, Federated learning
is proposed to perform model training by multiple clients' combined data
without the dataset sharing within the cluster. Nevertheless, federated
learning introduces massive communication overhead as the synchronized data in
each epoch is of the same size as the model, and thereby leading to a low
communication efficiency. Consequently, variant methods mainly focusing on the
communication rounds reduction and data compression are proposed to reduce the
communication overhead of federated learning. In this paper, we propose
Overlap-FedAvg, a framework that parallels the model training phase with model
uploading & downloading phase, so that the latter phase can be totally covered
by the former phase. Compared to vanilla FedAvg, Overlap-FedAvg is further
developed with a hierarchical computing strategy, a data compensation mechanism
and a nesterov accelerated gradients~(NAG) algorithm. Besides, Overlap-FedAvg
is orthogonal to many other compression methods so that they can be applied
together to maximize the utilization of the cluster. Furthermore, the
theoretical analysis is provided to prove the convergence of the proposed
Overlap-FedAvg framework. Extensive experiments on both conventional and
recurrent tasks with multiple models and datasets also demonstrate that the
proposed Overlap-FedAvg framework substantially boosts the federated learning
process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qing_Y/0/1/0/all/0/1"&gt;Ye Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1"&gt;Jiancheng Lv&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing. (arXiv:2011.09899v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09899</id>
        <link href="http://arxiv.org/abs/2011.09899"/>
        <updated>2021-06-17T01:58:46.236Z</updated>
        <summary type="html"><![CDATA[User data confidentiality protection is becoming a rising challenge in the
present deep learning research. Without access to data, conventional
data-driven model compression faces a higher risk of performance degradation.
Recently, some works propose to generate images from a specific pretrained
model to serve as training data. However, the inversion process only utilizes
biased feature statistics stored in one model and is from low-dimension to
high-dimension. As a consequence, it inevitably encounters the difficulties of
generalizability and inexact inversion, which leads to unsatisfactory
performance. To address these problems, we propose MixMix based on two simple
yet effective techniques: (1) Feature Mixing: utilizes various models to
construct a universal feature space for generalized inversion; (2) Data Mixing:
mixes the synthesized images and labels to generate exact label information. We
prove the effectiveness of MixMix from both theoretical and empirical
perspectives. Extensive experiments show that MixMix outperforms existing
methods on the mainstream compression tasks, including quantization, knowledge
distillation, and pruning. Specifically, MixMix achieves up to 4% and 20%
accuracy uplift on quantization and pruning, respectively, compared to existing
data-free compression work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Feng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Ruihao Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1"&gt;Mingzhu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fengwei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shaoqing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shi Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Dimensional Bayesian Optimisation with Variational Autoencoders and Deep Metric Learning. (arXiv:2106.03609v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03609</id>
        <link href="http://arxiv.org/abs/2106.03609"/>
        <updated>2021-06-17T01:58:46.235Z</updated>
        <summary type="html"><![CDATA[We introduce a method based on deep metric learning to perform Bayesian
optimisation over high-dimensional, structured input spaces using variational
autoencoders (VAEs). By extending ideas from supervised deep metric learning,
we address a longstanding problem in high-dimensional VAE Bayesian
optimisation, namely how to enforce a discriminative latent space as an
inductive bias. Importantly, we achieve such an inductive bias using just 1% of
the available labelled data relative to previous work, highlighting the sample
efficiency of our approach. As a theoretical contribution, we present a proof
of vanishing regret for our method. As an empirical contribution, we present
state-of-the-art results on real-world high-dimensional black-box optimisation
problems including property-guided molecule generation. It is the hope that the
results presented in this paper can act as a guiding principle for realising
effective high-dimensional Bayesian optimisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grosnit_A/0/1/0/all/0/1"&gt;Antoine Grosnit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tutunov_R/0/1/0/all/0/1"&gt;Rasul Tutunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maraval_A/0/1/0/all/0/1"&gt;Alexandre Max Maraval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1"&gt;Ryan-Rhys Griffiths&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cowen_Rivers_A/0/1/0/all/0/1"&gt;Alexander I. Cowen-Rivers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_W/0/1/0/all/0/1"&gt;Wenlong Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhitang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jan Peters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bou_Ammar_H/0/1/0/all/0/1"&gt;Haitham Bou-Ammar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated scoring of pre-REM sleep in mice with deep learning. (arXiv:2105.01933v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01933</id>
        <link href="http://arxiv.org/abs/2105.01933"/>
        <updated>2021-06-17T01:58:46.222Z</updated>
        <summary type="html"><![CDATA[Reliable automation of the labor-intensive manual task of scoring animal
sleep can facilitate the analysis of long-term sleep studies. In recent years,
deep-learning-based systems, which learn optimal features from the data,
increased scoring accuracies for the classical sleep stages of Wake, REM, and
Non-REM. Meanwhile, it has been recognized that the statistics of transitional
stages such as pre-REM, found between Non-REM and REM, may hold additional
insight into the physiology of sleep and are now under vivid investigation. We
propose a classification system based on a simple neural network architecture
that scores the classical stages as well as pre-REM sleep in mice. When
restricted to the classical stages, the optimized network showed
state-of-the-art classification performance with an out-of-sample F1 score of
0.95 in male C57BL/6J mice. When unrestricted, the network showed lower F1
scores on pre-REM (0.5) compared to the classical stages. The result is
comparable to previous attempts to score transitional stages in other species
such as transition sleep in rats or N1 sleep in humans. Nevertheless, we
observed that the sequence of predictions including pre-REM typically
transitioned from Non-REM to REM reflecting sleep dynamics observed by human
scorers. Our findings provide further evidence for the difficulty of scoring
transitional sleep stages, likely because such stages of sleep are
under-represented in typical data sets or show large inter-scorer variability.
We further provide our source code and an online platform to run predictions
with our trained network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Grieger_N/0/1/0/all/0/1"&gt;Niklas Grieger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Schwabedal_J/0/1/0/all/0/1"&gt;Justus T. C. Schwabedal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wendel_S/0/1/0/all/0/1"&gt;Stefanie Wendel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ritze_Y/0/1/0/all/0/1"&gt;Yvonne Ritze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bialonski_S/0/1/0/all/0/1"&gt;Stephan Bialonski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local plasticity rules can learn deep representations using self-supervised contrastive predictions. (arXiv:2010.08262v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08262</id>
        <link href="http://arxiv.org/abs/2010.08262"/>
        <updated>2021-06-17T01:58:46.215Z</updated>
        <summary type="html"><![CDATA[Learning in the brain is poorly understood and learning rules that respect
biological constraints, yet yield deep hierarchical representations, are still
unknown. Here, we propose a learning rule that takes inspiration from
neuroscience and recent advances in self-supervised deep learning. Learning
minimizes a simple layer-specific loss function and does not need to
back-propagate error signals within or between layers. Instead, weight updates
follow a local, Hebbian, learning rule that only depends on pre- and
post-synaptic neuronal activity, predictive dendritic input and widely
broadcasted modulation factors which are identical for large groups of neurons.
The learning rule applies contrastive predictive learning to a causal,
biological setting using saccades (i.e. rapid shifts in gaze direction). We
find that networks trained with this self-supervised and local rule build deep
hierarchical representations of images, speech and video.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Illing_B/0/1/0/all/0/1"&gt;Bernd Illing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventura_J/0/1/0/all/0/1"&gt;Jean Ventura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellec_G/0/1/0/all/0/1"&gt;Guillaume Bellec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1"&gt;Wulfram Gerstner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-memory stochastic backpropagation with multi-channel randomized trace estimation. (arXiv:2106.06998v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06998</id>
        <link href="http://arxiv.org/abs/2106.06998"/>
        <updated>2021-06-17T01:58:46.215Z</updated>
        <summary type="html"><![CDATA[Thanks to the combination of state-of-the-art accelerators and highly
optimized open software frameworks, there has been tremendous progress in the
performance of deep neural networks. While these developments have been
responsible for many breakthroughs, progress towards solving large-scale
problems, such as video encoding and semantic segmentation in 3D, is hampered
because access to on-premise memory is often limited. Instead of relying on
(optimal) checkpointing or invertibility of the network layers -- to recover
the activations during backpropagation -- we propose to approximate the
gradient of convolutional layers in neural networks with a multi-channel
randomized trace estimation technique. Compared to other methods, this approach
is simple, amenable to analyses, and leads to a greatly reduced memory
footprint. Even though the randomized trace estimation introduces stochasticity
during training, we argue that this is of little consequence as long as the
induced errors are of the same order as errors in the gradient due to the use
of stochastic gradient descent. We discuss the performance of networks trained
with stochastic backpropagation and how the error can be controlled while
maximizing memory usage and minimizing computational overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Louboutin_M/0/1/0/all/0/1"&gt;Mathias Louboutin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siahkoohi_A/0/1/0/all/0/1"&gt;Ali Siahkoohi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rongrong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herrmann_F/0/1/0/all/0/1"&gt;Felix J. Herrmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automated Website Classification by Deep Learning. (arXiv:1910.09991v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.09991</id>
        <link href="http://arxiv.org/abs/1910.09991"/>
        <updated>2021-06-17T01:58:46.203Z</updated>
        <summary type="html"><![CDATA[In recent years, the interest in Big Data sources has been steadily growing
within the Official Statistic community. The Italian National Institute of
Statistics (Istat) is currently carrying out several Big Data pilot studies.
One of these studies, the ICT Big Data pilot, aims at exploiting massive
amounts of textual data automatically scraped from the websites of Italian
enterprises in order to predict a set of target variables (e.g. e-commerce)
that are routinely observed by the traditional ICT Survey. In this paper, we
show that Deep Learning techniques can successfully address this problem.
Essentially, we tackle a text classification task: an algorithm must learn to
infer whether an Italian enterprise performs e-commerce from the textual
content of its website. To reach this goal, we developed a sophisticated
processing pipeline and evaluated its performance through extensive
experiments. Our pipeline uses Convolutional Neural Networks and relies on Word
Embeddings to encode raw texts into grayscale images (i.e. normalized numeric
matrices). Web-scraped texts are huge and have very low signal to noise ratio:
to overcome these issues, we adopted a framework known as False Positive
Reduction, which has seldom (if ever) been applied before to text
classification tasks. Several original contributions enable our processing
pipeline to reach good classification results. Empirical evidence shows that
our proposal outperforms all the alternative Machine Learning solutions already
tested in Istat for the same task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fausti_F/0/1/0/all/0/1"&gt;Fabrizio De Fausti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pugliese_F/0/1/0/all/0/1"&gt;Francesco Pugliese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zardetto_D/0/1/0/all/0/1"&gt;Diego Zardetto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Partial Response Network: a neural network nomogram. (arXiv:1908.05978v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.05978</id>
        <link href="http://arxiv.org/abs/1908.05978"/>
        <updated>2021-06-17T01:58:46.197Z</updated>
        <summary type="html"><![CDATA[Among interpretable machine learning methods, the class of Generalised
Additive Neural Networks (GANNs) is referred to as Self-Explaining Neural
Networks (SENN) because of the linear dependence on explicit functions of the
inputs. In binary classification this shows the precise weight that each input
contributes towards the logit. The nomogram is a graphical representation of
these weights. We show that functions of individual and pairs of variables can
be derived from a functional Analysis of Variance (ANOVA) representation,
enabling an efficient feature selection to be carried by application of the
logistic Lasso. This process infers the structure of GANNs which otherwise
needs to be predefined. As this method is particularly suited for tabular data,
it starts by fitting a generic flexible model, in this case a Multi-layer
Perceptron (MLP) to which the ANOVA decomposition is applied. This has the
further advantage that the resulting GANN can be replicated as a SENN, enabling
further refinement of the univariate and bivariate component functions to take
place. The component functions are partial responses hence the SENN is a
partial response network. The Partial Response Network (PRN) is equally as
transparent as a traditional logistic regression model, but capable of
non-linear classification with comparable or superior performance to the
original MLP. In other words, the PRN is a fully interpretable representation
of the MLP, at the level of univariate and bivariate effects. The performance
of the PRN is shown to be competitive for benchmark data, against
state-of-the-art machine learning methods including GBM, SVM and Random
Forests. It is also compared with spline-based Sparse Additive Models (SAM)
showing that a semi-parametric representation of the GAM as a neural network
can be as effective as the SAM though less constrained by the need to set
spline nodes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lisboa_P/0/1/0/all/0/1"&gt;Paulo J. G. Lisboa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortega_Martorell_S/0/1/0/all/0/1"&gt;Sandra Ortega-Martorell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cashman_S/0/1/0/all/0/1"&gt;Sadie Cashman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olier_I/0/1/0/all/0/1"&gt;Ivan Olier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automatic Actor-Critic Solutions to Continuous Control. (arXiv:2106.08918v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08918</id>
        <link href="http://arxiv.org/abs/2106.08918"/>
        <updated>2021-06-17T01:58:46.195Z</updated>
        <summary type="html"><![CDATA[Model-free off-policy actor-critic methods are an efficient solution to
complex continuous control tasks. However, these algorithms rely on a number of
design tricks and many hyperparameters, making their applications to new
domains difficult and computationally expensive. This paper creates an
evolutionary approach that automatically tunes these design decisions and
eliminates the RL-specific hyperparameters from the Soft Actor-Critic
algorithm. Our design is sample efficient and provides practical advantages
over baseline approaches, including improved exploration, generalization over
multiple control frequencies, and a robust ensemble of high-performance
policies. Empirically, we show that our agent outperforms well-tuned
hyperparameter settings in popular benchmarks from the DeepMind Control Suite.
We then apply it to new control tasks to find high-performance solutions with
minimal compute and research effort.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grigsby_J/0/1/0/all/0/1"&gt;Jake Grigsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1"&gt;Jin Yong Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yanjun Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Social Distance Estimation From Images: Performance Evaluation, Test Benchmark, and Algorithm. (arXiv:2103.06759v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06759</id>
        <link href="http://arxiv.org/abs/2103.06759"/>
        <updated>2021-06-17T01:58:46.187Z</updated>
        <summary type="html"><![CDATA[The COVID-19 virus has caused a global pandemic since March 2020. The World
Health Organization (WHO) has provided guidelines on how to reduce the spread
of the virus and one of the most important measures is social distancing.
Maintaining a minimum of one meter distance from other people is strongly
suggested to reduce the risk of infection. This has created a strong interest
in monitoring the social distances either as a safety measure or to study how
the measures have affected human behavior and country-wise differences in this.
The need for automatic social distance estimation algorithms is evident, but
there is no suitable test benchmark for such algorithms. Collecting images with
measured ground-truth pair-wise distances between all the people using
different camera settings is cumbersome. Furthermore, performance evaluation
for social distance estimation algorithms is not straightforward and there is
no widely accepted evaluation protocol. In this paper, we provide a dataset of
varying images with measured pair-wise social distances under different camera
positionings and focal length values. We suggest a performance evaluation
protocol and provide a benchmark to easily evaluate social distance estimation
algorithms. We also propose a method for automatic social distance estimation.
Our method takes advantage of object detection and human pose estimation. It
can be applied on any single image as long as focal length and sensor size
information are known. The results on our benchmark are encouraging with 92%
human detection rate and only 28.9% average error in distance estimation among
the detected people.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seker_M/0/1/0/all/0/1"&gt;Mert Seker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannisto_A/0/1/0/all/0/1"&gt;Anssi M&amp;#xe4;nnist&amp;#xf6;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1"&gt;Jenni Raitoharju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamically Grown Generative Adversarial Networks. (arXiv:2106.08505v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08505</id>
        <link href="http://arxiv.org/abs/2106.08505"/>
        <updated>2021-06-17T01:58:46.180Z</updated>
        <summary type="html"><![CDATA[Recent work introduced progressive network growing as a promising way to ease
the training for large GANs, but the model design and architecture-growing
strategy still remain under-explored and needs manual design for different
image data. In this paper, we propose a method to dynamically grow a GAN during
training, optimizing the network architecture and its parameters together with
automation. The method embeds architecture search techniques as an interleaving
step with gradient-based training to periodically seek the optimal
architecture-growing strategy for the generator and discriminator. It enjoys
the benefits of both eased training because of progressive growing and improved
performance because of broader architecture design space. Experimental results
demonstrate new state-of-the-art of image generation. Observations in the
search procedure also provide constructive insights into the GAN model design
such as generator-discriminator balance and convolutional layer choices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lanlan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuting Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jia Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random feature neural networks learn Black-Scholes type PDEs without curse of dimensionality. (arXiv:2106.08900v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08900</id>
        <link href="http://arxiv.org/abs/2106.08900"/>
        <updated>2021-06-17T01:58:46.167Z</updated>
        <summary type="html"><![CDATA[This article investigates the use of random feature neural networks for
learning Kolmogorov partial (integro-)differential equations associated to
Black-Scholes and more general exponential L\'evy models. Random feature neural
networks are single-hidden-layer feedforward neural networks in which only the
output weights are trainable. This makes training particularly simple, but (a
priori) reduces expressivity. Interestingly, this is not the case for
Black-Scholes type PDEs, as we show here. We derive bounds for the prediction
error of random neural networks for learning sufficiently non-degenerate
Black-Scholes type models. A full error analysis is provided and it is shown
that the derived bounds do not suffer from the curse of dimensionality. We also
investigate an application of these results to basket options and validate the
bounds numerically.

These results prove that neural networks are able to \textit{learn} solutions
to Black-Scholes type PDEs without the curse of dimensionality. In addition,
this provides an example of a relevant learning problem in which random feature
neural networks are provably efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gonon_L/0/1/0/all/0/1"&gt;Lukas Gonon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised GANs with Label Augmentation. (arXiv:2106.08601v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08601</id>
        <link href="http://arxiv.org/abs/2106.08601"/>
        <updated>2021-06-17T01:58:46.166Z</updated>
        <summary type="html"><![CDATA[Recently, transformation-based self-supervised learning has been applied to
generative adversarial networks (GANs) to mitigate the catastrophic forgetting
problem of discriminator by learning stable representations. However, the
separate self-supervised tasks in existing self-supervised GANs cause an
inconsistent goal with generative modeling due to the learning of the generator
from their generator distribution-agnostic classifiers. To address this issue,
we propose a novel self-supervised GANs framework with label augmentation,
i.e., augmenting the GAN labels (real or fake) with the self-supervised
pseudo-labels. In particular, the discriminator and the self-supervised
classifier are unified to learn a single task that predicts the augmented label
such that the discriminator/classifier is aware of the generator distribution,
while the generator tries to confuse the discriminator/classifier by optimizing
the discrepancy between the transformed real and generated distributions.
Theoretically, we prove that the generator, at the equilibrium point, converges
to replicate the data distribution. Empirically, we demonstrate that the
proposed method significantly outperforms competitive baselines on both
generative modeling and representation learning across benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1"&gt;Liang Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Huawei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1"&gt;Qi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drum-Aware Ensemble Architecture for Improved Joint Musical Beat and Downbeat Tracking. (arXiv:2106.08685v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08685</id>
        <link href="http://arxiv.org/abs/2106.08685"/>
        <updated>2021-06-17T01:58:46.160Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel system architecture that integrates blind source
separation with joint beat and downbeat tracking in musical audio signals. The
source separation module segregates the percussive and non-percussive
components of the input signal, over which beat and downbeat tracking are
performed separately and then the results are aggregated with a learnable
fusion mechanism. This way, the system can adaptively determine how much the
tracking result for an input signal should depend on the input's percussive or
non-percussive components. Evaluation on four testing sets that feature
different levels of presence of drum sounds shows that the new architecture
consistently outperforms the widely-adopted baseline architecture that does not
employ source separation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1"&gt;Ching-Yu Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_A/0/1/0/all/0/1"&gt;Alvin Wen-Yu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Quantum Property Prediction via Deeper 2D and 3D Graph Networks. (arXiv:2106.08551v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08551</id>
        <link href="http://arxiv.org/abs/2106.08551"/>
        <updated>2021-06-17T01:58:46.159Z</updated>
        <summary type="html"><![CDATA[Molecular property prediction is gaining increasing attention due to its
diverse applications. One task of particular interests and importance is to
predict quantum chemical properties without 3D equilibrium structures. This is
practically favorable since obtaining 3D equilibrium structures requires
extremely expensive calculations. In this work, we design a deep graph neural
network to predict quantum properties by directly learning from 2D molecular
graphs. In addition, we propose a 3D graph neural network to learn from
low-cost conformer sets, which can be obtained with open-source tools using an
affordable budget. We employ our methods to participate in the 2021 KDD Cup on
OGB Large-Scale Challenge (OGB-LSC), which aims to predict the HOMO-LUMO energy
gap of molecules. Final evaluation results reveal that we are one of the
winners with a mean absolute error of 0.1235 on the holdout test set. Our
implementation is available as part of the MoleculeX package
(https://github.com/divelab/MoleculeX).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Meng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Cong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Limei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yaochen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Hao Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Youzhi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shenglong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shuiwang Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Agnostic Federated Averaging. (arXiv:2104.02748v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02748</id>
        <link href="http://arxiv.org/abs/2104.02748"/>
        <updated>2021-06-17T01:58:46.159Z</updated>
        <summary type="html"><![CDATA[In distributed learning settings such as federated learning, the training
algorithm can be potentially biased towards different clients. Mohri et al.
(2019) proposed a domain-agnostic learning algorithm, where the model is
optimized for any target distribution formed by a mixture of the client
distributions in order to overcome this bias. They further proposed an
algorithm for the cross-silo federated learning setting, where the number of
clients is small. We consider this problem in the cross-device setting, where
the number of clients is much larger. We propose a communication-efficient
distributed algorithm called Agnostic Federated Averaging (or AgnosticFedAvg)
to minimize the domain-agnostic objective proposed in Mohri et al. (2019),
which is amenable to other private mechanisms such as secure aggregation. We
highlight two types of naturally occurring domains in federated learning and
argue that AgnosticFedAvg performs well on both. To demonstrate the practical
effectiveness of AgnosticFedAvg, we report positive results for large-scale
language modeling tasks in both simulation and live experiments, where the
latter involves training language models for Spanish virtual keyboard for
millions of user devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ro_J/0/1/0/all/0/1"&gt;Jae Ro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathews_R/0/1/0/all/0/1"&gt;Rajiv Mathews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1"&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ananda Theertha Suresh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Sample Complexity and Metastability of Heavy-tailed Policy Search in Continuous Control. (arXiv:2106.08414v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08414</id>
        <link href="http://arxiv.org/abs/2106.08414"/>
        <updated>2021-06-17T01:58:46.152Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning is a framework for interactive decision-making with
incentives sequentially revealed across time without a system dynamics model.
Due to its scaling to continuous spaces, we focus on policy search where one
iteratively improves a parameterized policy with stochastic policy gradient
(PG) updates. In tabular Markov Decision Problems (MDPs), under persistent
exploration and suitable parameterization, global optimality may be obtained.
By contrast, in continuous space, the non-convexity poses a pathological
challenge as evidenced by existing convergence results being mostly limited to
stationarity or arbitrary local extrema. To close this gap, we step towards
persistent exploration in continuous space through policy parameterizations
defined by distributions of heavier tails defined by tail-index parameter
alpha, which increases the likelihood of jumping in state space. Doing so
invalidates smoothness conditions of the score function common to PG. Thus, we
establish how the convergence rate to stationarity depends on the policy's tail
index alpha, a Holder continuity parameter, integrability conditions, and an
exploration tolerance parameter introduced here for the first time. Further, we
characterize the dependence of the set of local maxima on the tail index
through an exit and transition time analysis of a suitably defined Markov
chain, identifying that policies associated with Levy Processes of a heavier
tail converge to wider peaks. This phenomenon yields improved stability to
perturbations in supervised learning, which we corroborate also manifests in
improved performance of policy search, especially when myopic and farsighted
incentives are misaligned.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bedi_A/0/1/0/all/0/1"&gt;Amrit Singh Bedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parayil_A/0/1/0/all/0/1"&gt;Anjaly Parayil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koppel_A/0/1/0/all/0/1"&gt;Alec Koppel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Evaluating and Training Verifiably Robust Neural Networks. (arXiv:2104.00447v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00447</id>
        <link href="http://arxiv.org/abs/2104.00447"/>
        <updated>2021-06-17T01:58:46.146Z</updated>
        <summary type="html"><![CDATA[Recent works have shown that interval bound propagation (IBP) can be used to
train verifiably robust neural networks. Reseachers observe an intriguing
phenomenon on these IBP trained networks: CROWN, a bounding method based on
tight linear relaxation, often gives very loose bounds on these networks. We
also observe that most neurons become dead during the IBP training process,
which could hurt the representation capability of the network. In this paper,
we study the relationship between IBP and CROWN, and prove that CROWN is always
tighter than IBP when choosing appropriate bounding lines. We further propose a
relaxed version of CROWN, linear bound propagation (LBP), that can be used to
verify large networks to obtain lower verified errors than IBP. We also design
a new activation function, parameterized ramp function (ParamRamp), which has
more diversity of neuron status than ReLU. We conduct extensive experiments on
MNIST, CIFAR-10 and Tiny-ImageNet with ParamRamp activation and achieve
state-of-the-art verified robustness. Code and the appendix are available at
https://github.com/ZhaoyangLyu/VerifiablyRobustNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1"&gt;Zhaoyang Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Minghao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guodong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kehuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dahua Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining the Behavior of Black-Box Prediction Algorithms with Causal Learning. (arXiv:2006.02482v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02482</id>
        <link href="http://arxiv.org/abs/2006.02482"/>
        <updated>2021-06-17T01:58:46.133Z</updated>
        <summary type="html"><![CDATA[We propose to explain the behavior of black-box prediction methods (e.g.,
deep neural networks trained on image pixel data) using causal graphical
models. Specifically, we explore learning the structure of a causal graph where
the nodes represent prediction outcomes along with a set of macro-level
"interpretable" features, while allowing for arbitrary unmeasured confounding
among these variables. The resulting graph may indicate which of the
interpretable features, if any, are possible causes of the prediction outcome
and which may be merely associated with prediction outcomes due to confounding.
The approach is motivated by a counterfactual theory of causal explanation
wherein good explanations point to factors that are "difference-makers" in an
interventionist sense. The resulting analysis may be useful in algorithm
auditing and evaluation, by identifying features which make a causal difference
to the algorithm's output.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sani_N/0/1/0/all/0/1"&gt;Numair Sani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malinsky_D/0/1/0/all/0/1"&gt;Daniel Malinsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shpitser_I/0/1/0/all/0/1"&gt;Ilya Shpitser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thompson Sampling with Information Relaxation Penalties. (arXiv:1902.04251v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1902.04251</id>
        <link href="http://arxiv.org/abs/1902.04251"/>
        <updated>2021-06-17T01:58:46.116Z</updated>
        <summary type="html"><![CDATA[We consider a finite-horizon multi-armed bandit (MAB) problem in a Bayesian
setting, for which we propose an information relaxation sampling framework.
With this framework, we define an intuitive family of control policies that
include Thompson sampling (TS) and the Bayesian optimal policy as endpoints.
Analogous to TS, which, at each decision epoch pulls an arm that is best with
respect to the randomly sampled parameters, our algorithms sample entire future
reward realizations and take the corresponding best action. However, this is
done in the presence of "penalties" that seek to compensate for the
availability of future information.

We develop several novel policies and performance bounds for MAB problems
that vary in terms of improving performance and increasing computational
complexity between the two endpoints. Our policies can be viewed as natural
generalizations of TS that simultaneously incorporate knowledge of the time
horizon and explicitly consider the exploration-exploitation trade-off. We
prove associated structural results on performance bounds and suboptimality
gaps. Numerical experiments suggest that this new class of policies perform
well, in particular in settings where the finite time horizon introduces
significant exploration-exploitation tension into the problem. Finally,
inspired by the finite-horizon Gittins index, we propose an index policy that
builds on our framework that particularly outperforms the state-of-the-art
algorithms in our numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1"&gt;Seungki Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maglaras_C/0/1/0/all/0/1"&gt;Costis Maglaras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moallemi_C/0/1/0/all/0/1"&gt;Ciamac C. Moallemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense for the Price of Sparse: Improved Performance of Sparsely Initialized Networks via a Subspace Offset. (arXiv:2102.07655v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07655</id>
        <link href="http://arxiv.org/abs/2102.07655"/>
        <updated>2021-06-17T01:58:46.110Z</updated>
        <summary type="html"><![CDATA[That neural networks may be pruned to high sparsities and retain high
accuracy is well established. Recent research efforts focus on pruning
immediately after initialization so as to allow the computational savings
afforded by sparsity to extend to the training process. In this work, we
introduce a new `DCT plus Sparse' layer architecture, which maintains
information propagation and trainability even with as little as 0.01% trainable
kernel parameters remaining. We show that standard training of networks built
with these layers, and pruned at initialization, achieves state-of-the-art
accuracy for extreme sparsities on a variety of benchmark network architectures
and datasets. Moreover, these results are achieved using only simple heuristics
to determine the locations of the trainable parameters in the network, and thus
without having to initially store or compute with the full, unpruned network,
as is required by competing prune-at-initialization algorithms. Switching from
standard sparse layers to DCT plus Sparse layers does not increase the storage
footprint of a network and incurs only a small additional computational
overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Price_I/0/1/0/all/0/1"&gt;Ilan Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanner_J/0/1/0/all/0/1"&gt;Jared Tanner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-Layered Approach for Measuring the Simulation-to-Reality Gap of Radar Perception for Autonomous Driving. (arXiv:2106.08372v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.08372</id>
        <link href="http://arxiv.org/abs/2106.08372"/>
        <updated>2021-06-17T01:58:46.104Z</updated>
        <summary type="html"><![CDATA[With the increasing safety validation requirements for the release of a
self-driving car, alternative approaches, such as simulation-based testing, are
emerging in addition to conventional real-world testing. In order to rely on
virtual tests the employed sensor models have to be validated. For this reason,
it is necessary to quantify the discrepancy between simulation and reality in
order to determine whether a certain fidelity is sufficient for a desired
intended use. There exists no sound method to measure this
simulation-to-reality gap of radar perception for autonomous driving. We
address this problem by introducing a multi-layered evaluation approach, which
consists of a combination of an explicit and an implicit sensor model
evaluation. The former directly evaluates the realism of the synthetically
generated sensor data, while the latter refers to an evaluation of a downstream
target application. In order to demonstrate the method, we evaluated the
fidelity of three typical radar model types (ideal, data-driven, ray
tracing-based) and their applicability for virtually testing radar-based
multi-object tracking. We have shown the effectiveness of the proposed approach
in terms of providing an in-depth sensor model assessment that renders existing
disparities visible and enables a realistic estimation of the overall model
fidelity across different scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_A/0/1/0/all/0/1"&gt;Anthony Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1"&gt;Max Paul Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Resch_M/0/1/0/all/0/1"&gt;Michael Resch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Visibility Graph Neural Network and It's Application in Modulation Classification. (arXiv:2106.08564v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08564</id>
        <link href="http://arxiv.org/abs/2106.08564"/>
        <updated>2021-06-17T01:58:46.098Z</updated>
        <summary type="html"><![CDATA[Our digital world is full of time series and graphs which capture the various
aspects of many complex systems. Traditionally, there are respective methods in
processing these two different types of data, e.g., Recurrent Neural Network
(RNN) and Graph Neural Network (GNN), while in recent years, time series could
be mapped to graphs by using the techniques such as Visibility Graph (VG), so
that researchers can use graph algorithms to mine the knowledge in time series.
Such mapping methods establish a bridge between time series and graphs, and
have high potential to facilitate the analysis of various real-world time
series. However, the VG method and its variants are just based on fixed rules
and thus lack of flexibility, largely limiting their application in reality. In
this paper, we propose an Adaptive Visibility Graph (AVG) algorithm that can
adaptively map time series into graphs, based on which we further establish an
end-to-end classification framework AVGNet, by utilizing GNN model DiffPool as
the classifier. We then adopt AVGNet for radio signal modulation classification
which is an important task in the field of wireless communication. The
simulations validate that AVGNet outperforms a series of advanced deep learning
methods, achieving the state-of-the-art performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1"&gt;Qi Xuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1"&gt;Kunfeng Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinchao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhuangzhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dongwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shilian Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoniu Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Refining Language Models with Compositional Explanations. (arXiv:2103.10415v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10415</id>
        <link href="http://arxiv.org/abs/2103.10415"/>
        <updated>2021-06-17T01:58:46.079Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models have been successful on text classification
tasks, but are prone to learning spurious correlations from biased datasets,
and are thus vulnerable when making inferences in a new domain. Prior works
reveal such spurious patterns via post-hoc explanation algorithms which compute
the importance of input features. Further, the model is regularized to align
the importance scores with human knowledge, so that the unintended model
behaviors are eliminated. However, such a regularization technique lacks
flexibility and coverage, since only importance scores towards a pre-defined
list of features are adjusted, while more complex human knowledge such as
feature interaction and pattern generalization can hardly be incorporated. In
this work, we propose to refine a learned language model for a target domain by
collecting human-provided compositional explanations regarding observed biases.
By parsing these explanations into executable logic rules, the human-specified
refinement advice from a small set of explanations can be generalized to more
training examples. We additionally introduce a regularization term allowing
adjustments for both importance and interaction of features to better rectify
model behavior. We demonstrate the effectiveness of the proposed approach on
two text classification tasks by showing improved performance in target domain
as well as improved model fairness after refinement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Huihan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qinyuan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xisen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Wasserstein Minimax Framework for Mixed Linear Regression. (arXiv:2106.07537v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07537</id>
        <link href="http://arxiv.org/abs/2106.07537"/>
        <updated>2021-06-17T01:58:46.072Z</updated>
        <summary type="html"><![CDATA[Multi-modal distributions are commonly used to model clustered data in
statistical learning tasks. In this paper, we consider the Mixed Linear
Regression (MLR) problem. We propose an optimal transport-based framework for
MLR problems, Wasserstein Mixed Linear Regression (WMLR), which minimizes the
Wasserstein distance between the learned and target mixture regression models.
Through a model-based duality analysis, WMLR reduces the underlying MLR task to
a nonconvex-concave minimax optimization problem, which can be provably solved
to find a minimax stationary point by the Gradient Descent Ascent (GDA)
algorithm. In the special case of mixtures of two linear regression models, we
show that WMLR enjoys global convergence and generalization guarantees. We
prove that WMLR's sample complexity grows linearly with the dimension of data.
Finally, we discuss the application of WMLR to the federated learning task
where the training samples are collected by multiple agents in a network.
Unlike the Expectation Maximization algorithm, WMLR directly extends to the
distributed, federated learning setting. We support our theoretical results
through several numerical experiments, which highlight our framework's ability
to handle the federated learning setting with mixture models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Diamandis_T/0/1/0/all/0/1"&gt;Theo Diamandis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Eldar_Y/0/1/0/all/0/1"&gt;Yonina C. Eldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fallah_A/0/1/0/all/0/1"&gt;Alireza Fallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Farnia_F/0/1/0/all/0/1"&gt;Farzan Farnia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ozdaglar_A/0/1/0/all/0/1"&gt;Asuman Ozdaglar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models. (arXiv:2106.03969v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03969</id>
        <link href="http://arxiv.org/abs/2106.03969"/>
        <updated>2021-06-17T01:58:46.066Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning a tree-structured Ising model from data,
such that subsequent predictions computed using the model are accurate.
Concretely, we aim to learn a model such that posteriors $P(X_i|X_S)$ for small
sets of variables $S$ are accurate. Since its introduction more than 50 years
ago, the Chow-Liu algorithm, which efficiently computes the maximum likelihood
tree, has been the benchmark algorithm for learning tree-structured graphical
models. A bound on the sample complexity of the Chow-Liu algorithm with respect
to the prediction-centric local total variation loss was shown in [BK19]. While
those results demonstrated that it is possible to learn a useful model even
when recovering the true underlying graph is impossible, their bound depends on
the maximum strength of interactions and thus does not achieve the
information-theoretic optimum. In this paper, we introduce a new algorithm that
carefully combines elements of the Chow-Liu algorithm with tree metric
reconstruction methods to efficiently and optimally learn tree Ising models
under a prediction-centric loss. Our algorithm is robust to model
misspecification and adversarial corruptions. In contrast, we show that the
celebrated Chow-Liu algorithm can be arbitrarily suboptimal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boix_Adsera_E/0/1/0/all/0/1"&gt;Enric Boix-Adsera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bresler_G/0/1/0/all/0/1"&gt;Guy Bresler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1"&gt;Frederic Koehler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Momentum Pseudo-Labeling for Semi-Supervised Speech Recognition. (arXiv:2106.08922v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.08922</id>
        <link href="http://arxiv.org/abs/2106.08922"/>
        <updated>2021-06-17T01:58:46.060Z</updated>
        <summary type="html"><![CDATA[Pseudo-labeling (PL) has been shown to be effective in semi-supervised
automatic speech recognition (ASR), where a base model is self-trained with
pseudo-labels generated from unlabeled data. While PL can be further improved
by iteratively updating pseudo-labels as the model evolves, most of the
previous approaches involve inefficient retraining of the model or intricate
control of the label update. We present momentum pseudo-labeling (MPL), a
simple yet effective strategy for semi-supervised ASR. MPL consists of a pair
of online and offline models that interact and learn from each other, inspired
by the mean teacher method. The online model is trained to predict
pseudo-labels generated on the fly by the offline model. The offline model
maintains a momentum-based moving average of the online model. MPL is performed
in a single training process and the interaction between the two models
effectively helps them reinforce each other to improve the ASR performance. We
apply MPL to an end-to-end ASR model based on the connectionist temporal
classification. The experimental results demonstrate that MPL effectively
improves over the base model and is scalable to different semi-supervised
scenarios with varying amounts of data or domain mismatch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1"&gt;Yosuke Higuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moritz_N/0/1/0/all/0/1"&gt;Niko Moritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Roux_J/0/1/0/all/0/1"&gt;Jonathan Le Roux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hori_T/0/1/0/all/0/1"&gt;Takaaki Hori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imperfect ImaGANation: Implications of GANs Exacerbating Biases on Facial Data Augmentation and Snapchat Selfie Lenses. (arXiv:2001.09528v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09528</id>
        <link href="http://arxiv.org/abs/2001.09528"/>
        <updated>2021-06-17T01:58:46.054Z</updated>
        <summary type="html"><![CDATA[In this paper, we show that popular Generative Adversarial Networks (GANs)
exacerbate biases along the axes of gender and skin tone when given a skewed
distribution of face-shots. While practitioners celebrate synthetic data
generation using GANs as an economical way to augment data for training
data-hungry machine learning models, it is unclear whether they recognize the
perils of such techniques when applied to real world datasets biased along
latent dimensions. Specifically, we show that (1) traditional GANs further skew
the distribution of a dataset consisting of engineering faculty headshots,
generating minority modes less often and of worse quality and (2)
image-to-image translation (conditional) GANs also exacerbate biases by
lightening skin color of non-white faces and transforming female facial
features to be masculine when generating faces of engineering professors. Thus,
our study is meant to serve as a cautionary tale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1"&gt;Niharika Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olmo_A/0/1/0/all/0/1"&gt;Alberto Olmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1"&gt;Sailik Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manikonda_L/0/1/0/all/0/1"&gt;Lydia Manikonda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1"&gt;Subbarao Kambhampati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code to Comment Translation: A Comparative Study on Model Effectiveness & Errors. (arXiv:2106.08415v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.08415</id>
        <link href="http://arxiv.org/abs/2106.08415"/>
        <updated>2021-06-17T01:58:46.039Z</updated>
        <summary type="html"><![CDATA[Automated source code summarization is a popular software engineering
research topic wherein machine translation models are employed to "translate"
code snippets into relevant natural language descriptions. Most evaluations of
such models are conducted using automatic reference-based metrics. However,
given the relatively large semantic gap between programming languages and
natural language, we argue that this line of research would benefit from a
qualitative investigation into the various error modes of current
state-of-the-art models. Therefore, in this work, we perform both a
quantitative and qualitative comparison of three recently proposed source code
summarization models. In our quantitative evaluation, we compare the models
based on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics,
and in our qualitative evaluation, we perform a manual open-coding of the most
common errors committed by the models when compared to ground truth captions.
Our investigation reveals new insights into the relationship between
metric-based performance and model prediction errors grounded in an empirically
derived error taxonomy that can be used to drive future research efforts]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmud_J/0/1/0/all/0/1"&gt;Junayed Mahmud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1"&gt;Fahim Faisal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnob_R/0/1/0/all/0/1"&gt;Raihan Islam Arnob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1"&gt;Antonios Anastasopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moran_K/0/1/0/all/0/1"&gt;Kevin Moran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Contextual Bandits with Overparameterized Models. (arXiv:2006.15368v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15368</id>
        <link href="http://arxiv.org/abs/2006.15368"/>
        <updated>2021-06-17T01:58:46.033Z</updated>
        <summary type="html"><![CDATA[Recent results in supervised learning suggest that while overparameterized
models have the capacity to overfit, they in fact generalize quite well. We ask
whether the same phenomenon occurs for offline contextual bandits. Our results
are mixed. Value-based algorithms benefit from the same generalization behavior
as overparameterized supervised learning, but policy-based algorithms do not.
We show that this discrepancy is due to the \emph{action-stability} of their
objectives. An objective is action-stable if there exists a prediction
(action-value vector or action distribution) which is optimal no matter which
action is observed. While value-based objectives are action-stable,
policy-based objectives are unstable. We formally prove upper bounds on the
regret of overparameterized value-based learning and lower bounds on the regret
for policy-based algorithms. In our experiments with large neural networks,
this gap between action-stable value-based objectives and unstable policy-based
objectives leads to significant performance differences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandfonbrener_D/0/1/0/all/0/1"&gt;David Brandfonbrener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1"&gt;William F. Whitney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1"&gt;Joan Bruna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge Sparse Basis Network: A Deep Learning Framework for EEG Source Localization. (arXiv:2102.09188v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09188</id>
        <link href="http://arxiv.org/abs/2102.09188"/>
        <updated>2021-06-17T01:58:46.028Z</updated>
        <summary type="html"><![CDATA[EEG source localization is an important technical issue in EEG analysis.
Despite many numerical methods existed for EEG source localization, they all
rely on strong priors and the deep sources are intractable. Here we propose a
deep learning framework using spatial basis function decomposition for EEG
source localization. This framework combines the edge sparsity prior and
Gaussian source basis, called Edge Sparse Basis Network (ESBN). The performance
of ESBN is validated by both synthetic data and real EEG data during motor
tasks. The results suggest that the supervised ESBN outperforms the traditional
numerical methods in synthetic data and the unsupervised fine-tuning provides
more focal and accurate localizations in real data. Our proposed deep learning
framework can be extended to account for other source priors, and the real-time
property of ESBN can facilitate the applications of EEG in brain-computer
interfaces and clinics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Chen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_K/0/1/0/all/0/1"&gt;Kexin Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhengyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mingqi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mantini_D/0/1/0/all/0/1"&gt;Dante Mantini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quanying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Averaging on the Bures-Wasserstein manifold: dimension-free convergence of gradient descent. (arXiv:2106.08502v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.08502</id>
        <link href="http://arxiv.org/abs/2106.08502"/>
        <updated>2021-06-17T01:58:46.022Z</updated>
        <summary type="html"><![CDATA[We study first-order optimization algorithms for computing the barycenter of
Gaussian distributions with respect to the optimal transport metric. Although
the objective is geodesically non-convex, Riemannian GD empirically converges
rapidly, in fact faster than off-the-shelf methods such as Euclidean GD and SDP
solvers. This stands in stark contrast to the best-known theoretical results
for Riemannian GD, which depend exponentially on the dimension. In this work,
we prove new geodesic convexity results which provide stronger control of the
iterates, yielding a dimension-free convergence rate. Our techniques also
enable the analysis of two related notions of averaging, the
entropically-regularized barycenter and the geometric median, providing the
first convergence guarantees for Riemannian GD for these problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Altschuler_J/0/1/0/all/0/1"&gt;Jason M. Altschuler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chewi_S/0/1/0/all/0/1"&gt;Sinho Chewi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gerber_P/0/1/0/all/0/1"&gt;Patrik Gerber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Stromme_A/0/1/0/all/0/1"&gt;Austin J. Stromme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dataset-Level Geometric Framework for Ensemble Classifiers. (arXiv:2106.08658v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08658</id>
        <link href="http://arxiv.org/abs/2106.08658"/>
        <updated>2021-06-17T01:58:46.016Z</updated>
        <summary type="html"><![CDATA[Ensemble classifiers have been investigated by many in the artificial
intelligence and machine learning community. Majority voting and weighted
majority voting are two commonly used combination schemes in ensemble learning.
However, understanding of them is incomplete at best, with some properties even
misunderstood. In this paper, we present a group of properties of these two
schemes formally under a dataset-level geometric framework. Two key factors,
every component base classifier's performance and dissimilarity between each
pair of component classifiers are evaluated by the same metric - the Euclidean
distance. Consequently, ensembling becomes a deterministic problem and the
performance of an ensemble can be calculated directly by a formula. We prove
several theorems of interest and explain their implications for ensembles. In
particular, we compare and contrast the effect of the number of component
classifiers on these two types of ensemble schemes. Empirical investigation is
also conducted to verify the theoretical results when other metrics such as
accuracy are used. We believe that the results from this paper are very useful
for us to understand the fundamental properties of these two combination
schemes and the principles of ensemble classifiers in general. The results are
also helpful for us to investigate some issues in ensemble classifiers, such as
ensemble performance prediction, selecting a small number of base classifiers
to obtain efficient and effective ensembles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shengli Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Weimin Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The shape and simplicity biases of adversarially robust ImageNet-trained CNNs. (arXiv:2006.09373v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09373</id>
        <link href="http://arxiv.org/abs/2006.09373"/>
        <updated>2021-06-17T01:58:45.997Z</updated>
        <summary type="html"><![CDATA[Adversarial training has been the topic of dozens of studies and a leading
method for defending against adversarial attacks. Yet, it remains largely
unknown (a) how adversarially-robust ImageNet classifiers (R classifiers)
generalize to out-of-distribution examples; and (b) how their generalization
capability relates to their hidden representations. In this paper, we perform a
thorough, systematic study to answer these two questions across AlexNet,
GoogLeNet, and ResNet-50 architectures. We found that while standard ImageNet
classifiers have a strong texture bias, their R counterparts rely heavily on
shapes. Remarkably, adversarial training induces three simplicity biases into
hidden neurons in the process of 'robustifying' the network. That is, each
convolutional neuron in R networks often changes to detecting (1) pixel-wise
smoother patterns i.e. a mechanism that blocks high-frequency noise from
passing through the network; (2) more lower-level features i.e. textures and
colors (instead of objects); and (3) fewer types of inputs. Our findings reveal
the interesting mechanisms that made networks more adversarially robust and
also explain some recent findings. Our findings reveal the interesting
mechanisms that made networks more adversarially robust and also explain some
recent findings e.g. why R networks benefit from much larger capacity (Xie and
Yuille, 2020) and can act as a strong image prior in image synthesis (Santurkar
et al., 2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Peijie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1"&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hash Layers For Large Sparse Models. (arXiv:2106.04426v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04426</id>
        <link href="http://arxiv.org/abs/2106.04426"/>
        <updated>2021-06-17T01:58:45.991Z</updated>
        <summary type="html"><![CDATA[We investigate the training of sparse layers that use different parameters
for different inputs based on hashing in large Transformer models.
Specifically, we modify the feedforward layer to hash to different sets of
weights depending on the current token, over all tokens in the sequence. We
show that this procedure either outperforms or is competitive with
learning-to-route mixture-of-expert methods such as Switch Transformers and
BASE Layers, while requiring no routing parameters or extra terms in the
objective function such as a load balancing loss, and no sophisticated
assignment algorithm. We study the performance of different hashing techniques,
hash sizes and input features, and show that balanced and random hashes focused
on the most local features work best, compared to either learning clusters or
using longer-range context. We show our approach works well both on large
language modeling and dialogue tasks, and on downstream fine-tuning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Loss Landscape in Neural Architecture Search. (arXiv:2005.02960v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.02960</id>
        <link href="http://arxiv.org/abs/2005.02960"/>
        <updated>2021-06-17T01:58:45.984Z</updated>
        <summary type="html"><![CDATA[Neural architecture search (NAS) has seen a steep rise in interest over the
last few years. Many algorithms for NAS consist of searching through a space of
architectures by iteratively choosing an architecture, evaluating its
performance by training it, and using all prior evaluations to come up with the
next choice. The evaluation step is noisy - the final accuracy varies based on
the random initialization of the weights. Prior work has focused on devising
new search algorithms to handle this noise, rather than quantifying or
understanding the level of noise in architecture evaluations. In this work, we
show that (1) the simplest hill-climbing algorithm is a powerful baseline for
NAS, and (2), when the noise in popular NAS benchmark datasets is reduced to a
minimum, hill-climbing to outperforms many popular state-of-the-art algorithms.
We further back up this observation by showing that the number of local minima
is substantially reduced as the noise decreases, and by giving a theoretical
characterization of the performance of local search in NAS. Based on our
findings, for NAS research we suggest (1) using local search as a baseline, and
(2) denoising the training pipeline when possible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Colin White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nolen_S/0/1/0/all/0/1"&gt;Sam Nolen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savani_Y/0/1/0/all/0/1"&gt;Yash Savani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting crop yields with little ground truth: A simple statistical model for in-season forecasting. (arXiv:2106.08720v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08720</id>
        <link href="http://arxiv.org/abs/2106.08720"/>
        <updated>2021-06-17T01:58:45.976Z</updated>
        <summary type="html"><![CDATA[We present a fully automated model for in-season crop yield prediction,
designed to work where there is a dearth of sub-national "ground truth"
information. Our approach relies primarily on satellite data and is
characterized by careful feature engineering combined with a simple regression
model. As such, it can work almost anywhere in the world. Applying it to 10
different crop-country pairs (5 cereals -- corn, wheat, sorghum, barley and
millet, in 2 countries -- Ethiopia and Kenya), we achieve RMSEs of 5\%-10\% for
predictions 9 months into the year, and 7\%-14\% for predictions 3 months into
the year. The model outputs daily forecasts for the final yield of the current
year. It is trained using approximately 4 million data points for each
crop-country pair. These consist of: historical country-level annual yields,
crop calendars, crop cover, NDVI, temperature, rainfall, and
evapotransporation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Semret_N/0/1/0/all/0/1"&gt;Nemo Semret&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-Based Vulnerability Analysis of Cyber-Physical Systems. (arXiv:2103.06271v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06271</id>
        <link href="http://arxiv.org/abs/2103.06271"/>
        <updated>2021-06-17T01:58:45.969Z</updated>
        <summary type="html"><![CDATA[This work focuses on the use of deep learning for vulnerability analysis of
cyber-physical systems (CPS). Specifically, we consider a control architecture
widely used in CPS (e.g., robotics), where the low-level control is based on
e.g., the extended Kalman filter (EKF) and an anomaly detector. To facilitate
analyzing the impact potential sensing attacks could have, our objective is to
develop learning-enabled attack generators capable of designing stealthy
attacks that maximally degrade system operation. We show how such problem can
be cast within a learning-based grey-box framework where parts of the runtime
information are known to the attacker, and introduce two models based on
feed-forward neural networks (FNN); both models are trained offline, using a
cost function that combines the attack effects on the estimation error and the
residual signal used for anomaly detection, so that the trained models are
capable of recursively generating such effective sensor attacks in real-time.
The effectiveness of the proposed methods is illustrated on several case
studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khazraei_A/0/1/0/all/0/1"&gt;Amir Khazraei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hallyburton_S/0/1/0/all/0/1"&gt;Spencer Hallyburton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qitong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pajic_M/0/1/0/all/0/1"&gt;Miroslav Pajic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interval-censored Hawkes processes. (arXiv:2104.07932v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07932</id>
        <link href="http://arxiv.org/abs/2104.07932"/>
        <updated>2021-06-17T01:58:45.963Z</updated>
        <summary type="html"><![CDATA[This work builds a novel point process and tools to use the Hawkes process
with interval-censored data. Such data records the aggregated counts of events
solely during specific time intervals -- such as the number of patients
admitted to the hospital or the volume of vehicles passing traffic loop
detectors -- and not the exact occurrence time of the events. First, we
establish the Mean Behavior Poisson (MBP) process, a novel Poisson process with
a direct parameter correspondence to the popular self-exciting Hawkes process.
The event intensity function of the MBP is the expected intensity over all
possible Hawkes realizations with the same parameter set. We fit MBP in the
interval-censored setting using an interval-censored Poisson log-likelihood
(IC-LL). We use the parameter equivalence to uncover the parameters of the
associated Hawkes process. Second, we introduce two novel exogenous functions
to distinguish the exogenous from the endogenous events. We propose the
multi-impulse exogenous function when the exogenous events are observed as
event time and the latent homogeneous Poisson process exogenous function when
the exogenous events are presented as interval-censored volumes. Third, we
provide several approximation methods to estimate the intensity and compensator
function of MBP when no analytical solution exists. Fourth and finally, we
connect the interval-censored loss of MBP to a broader class of Bregman
divergence-based functions. Using the connection, we show that the current
state of the art in popularity estimation (Hawkes Intensity Process (HIP)
(Rizoiu et al.,2017b)) is a particular case of the MBP process. We verify our
models through empirical testing on synthetic data and real-world data. We find
that on real-world datasets that ourMBP process outperforms HIP for the task of
popularity prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1"&gt;Marian-Andrei Rizoiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soen_A/0/1/0/all/0/1"&gt;Alexander Soen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shidi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Leanne Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calderon_P/0/1/0/all/0/1"&gt;Pio Calderon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Krishna Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lexing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PettingZoo: Gym for Multi-Agent Reinforcement Learning. (arXiv:2009.14471v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.14471</id>
        <link href="http://arxiv.org/abs/2009.14471"/>
        <updated>2021-06-17T01:58:45.947Z</updated>
        <summary type="html"><![CDATA[This paper introduces the PettingZoo library and the accompanying Agent
Environment Cycle ("AEC") games model. PettingZoo is a library of diverse sets
of multi-agent environments with a universal, elegant Python API. PettingZoo
was developed with the goal of accelerating research in Multi-Agent
Reinforcement Learning ("MARL"), by making work more interchangeable,
accessible and reproducible akin to what OpenAI's Gym library did for
single-agent reinforcement learning. PettingZoo's API, while inheriting many
features of Gym, is unique amongst MARL APIs in that it's based around the
novel AEC games model. We argue, in part through case studies on major problems
in popular MARL environments, that the popular game models are poor conceptual
models of the games commonly used with MARL, that they promote severe bugs that
are hard to detect, and that the AEC games model addresses these problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Terry_J/0/1/0/all/0/1"&gt;J. K. Terry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_B/0/1/0/all/0/1"&gt;Benjamin Black&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grammel_N/0/1/0/all/0/1"&gt;Nathaniel Grammel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayakumar_M/0/1/0/all/0/1"&gt;Mario Jayakumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hari_A/0/1/0/all/0/1"&gt;Ananth Hari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sullivan_R/0/1/0/all/0/1"&gt;Ryan Sullivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1"&gt;Luis Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_R/0/1/0/all/0/1"&gt;Rodrigo Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horsch_C/0/1/0/all/0/1"&gt;Caroline Horsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dieffendahl_C/0/1/0/all/0/1"&gt;Clemens Dieffendahl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_N/0/1/0/all/0/1"&gt;Niall L. Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lokesh_Y/0/1/0/all/0/1"&gt;Yashas Lokesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravi_P/0/1/0/all/0/1"&gt;Praveen Ravi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Source Separation-based Data Augmentation for Improved Joint Beat and Downbeat Tracking. (arXiv:2106.08703v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08703</id>
        <link href="http://arxiv.org/abs/2106.08703"/>
        <updated>2021-06-17T01:58:45.941Z</updated>
        <summary type="html"><![CDATA[Due to advances in deep learning, the performance of automatic beat and
downbeat tracking in musical audio signals has seen great improvement in recent
years. In training such deep learning based models, data augmentation has been
found an important technique. However, existing data augmentation methods for
this task mainly target at balancing the distribution of the training data with
respect to their tempo. In this paper, we investigate another approach for data
augmentation, to account for the composition of the training data in terms of
the percussive and non-percussive sound sources. Specifically, we propose to
employ a blind drum separation model to segregate the drum and non-drum sounds
from each training audio signal, filtering out training signals that are
drumless, and then use the obtained drum and non-drum stems to augment the
training data. We report experiments on four completely unseen test sets,
validating the effectiveness of the proposed method, and accordingly the
importance of drum sound composition in the training data for beat and downbeat
tracking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1"&gt;Ching-Yu Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1"&gt;Joann Ching&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsiao_W/0/1/0/all/0/1"&gt;Wen-Yi Hsiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Hua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_A/0/1/0/all/0/1"&gt;Alvin Wen-Yu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Biomanufacturing Harvesting Decisions under Limited Historical Data. (arXiv:2101.03735v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.03735</id>
        <link href="http://arxiv.org/abs/2101.03735"/>
        <updated>2021-06-17T01:58:45.933Z</updated>
        <summary type="html"><![CDATA[In biopharmaceutical manufacturing, fermentation processes play a critical
role on productivity and profit. A fermentation process uses living cells with
complex biological mechanisms, and this leads to high variability in the
process outputs. By building on the biological mechanisms of protein and
impurity growth, we introduce a stochastic model to characterize the
accumulation of the protein and impurity levels in the fermentation process.
However, a common challenge in industry is the availability of only very
limited amount of data especially in the development and early stage of
production. This adds an additional layer of uncertainty, referred to as model
risk, due to the difficulty of estimating the model parameters with limited
data. In this paper, we study the harvesting decision for a fermentation
process under model risk. In particular, we adopt a Bayesian approach to update
the unknown parameters of the growth-rate distributions, and use the resulting
posterior distributions to characterize the impact of model risk on
fermentation output variability. The harvesting problem is formulated as a
Markov decision process model with knowledge states that summarize the
posterior distributions and hence incorporate the model risk in
decision-making. The resulting model is solved by using a reinforcement
learning algorithm based on Bayesian sparse sampling. We provide analytical
results on the structure of the optimal policy and its objective function, and
explicitly study the impact of model risk on harvesting decisions. Our case
studies at MSD Animal Health demonstrate that the proposed model and solution
approach improve the harvesting decisions in real life by achieving
substantially higher average output from a fermentation batch along with lower
batch-to-batch variability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Martagan_T/0/1/0/all/0/1"&gt;Tugce Martagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Akcay_A/0/1/0/all/0/1"&gt;Alp Akcay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ravenstein_B/0/1/0/all/0/1"&gt;Bram van Ravenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topology Distillation for Recommender System. (arXiv:2106.08700v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08700</id>
        <link href="http://arxiv.org/abs/2106.08700"/>
        <updated>2021-06-17T01:58:45.925Z</updated>
        <summary type="html"><![CDATA[Recommender Systems (RS) have employed knowledge distillation which is a
model compression technique training a compact student model with the knowledge
transferred from a pre-trained large teacher model. Recent work has shown that
transferring knowledge from the teacher's intermediate layer significantly
improves the recommendation quality of the student. However, they transfer the
knowledge of individual representation point-wise and thus have a limitation in
that primary information of RS lies in the relations in the representation
space. This paper proposes a new topology distillation approach that guides the
student by transferring the topological structure built upon the relations in
the teacher space. We first observe that simply making the student learn the
whole topological structure is not always effective and even degrades the
student's performance. We demonstrate that because the capacity of the student
is highly limited compared to that of the teacher, learning the whole
topological structure is daunting for the student. To address this issue, we
propose a novel method named Hierarchical Topology Distillation (HTD) which
distills the topology hierarchically to cope with the large capacity gap. Our
extensive experiments on real-world datasets show that the proposed method
significantly outperforms the state-of-the-art competitors. We also provide
in-depth analyses to ascertain the benefit of distilling the topology for RS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;SeongKu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Junyoung Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kweon_W/0/1/0/all/0/1"&gt;Wonbin Kweon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hwanjo Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling Self-Knowledge From Contrastive Links to Classify Graph Nodes Without Passing Messages. (arXiv:2106.08541v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08541</id>
        <link href="http://arxiv.org/abs/2106.08541"/>
        <updated>2021-06-17T01:58:45.914Z</updated>
        <summary type="html"><![CDATA[Nowadays, Graph Neural Networks (GNNs) following the Message Passing paradigm
become the dominant way to learn on graphic data. Models in this paradigm have
to spend extra space to look up adjacent nodes with adjacency matrices and
extra time to aggregate multiple messages from adjacent nodes. To address this
issue, we develop a method called LinkDist that distils self-knowledge from
connected node pairs into a Multi-Layer Perceptron (MLP) without the need to
aggregate messages. Experiment with 8 real-world datasets shows the MLP derived
from LinkDist can predict the label of a node without knowing its adjacencies
but achieve comparable accuracy against GNNs in the contexts of semi- and
full-supervised node classification. Moreover, LinkDist benefits from its
Non-Message Passing paradigm that we can also distil self-knowledge from
arbitrarily sampled node pairs in a contrastive way to further boost the
performance of LinkDist.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Aiguo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1"&gt;Ke Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1"&gt;Ling Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum-inspired event reconstruction with Tensor Networks: Matrix Product States. (arXiv:2106.08334v1 [hep-ph])]]></title>
        <id>http://arxiv.org/abs/2106.08334</id>
        <link href="http://arxiv.org/abs/2106.08334"/>
        <updated>2021-06-17T01:58:45.884Z</updated>
        <summary type="html"><![CDATA[Tensor Networks are non-trivial representations of high-dimensional tensors,
originally designed to describe quantum many-body systems. We show that Tensor
Networks are ideal vehicles to connect quantum mechanical concepts to machine
learning techniques, thereby facilitating an improved interpretability of
neural networks. This study presents the discrimination of top quark signal
over QCD background processes using a Matrix Product State classifier. We show
that entanglement entropy can be used to interpret what a network learns, which
can be used to reduce the complexity of the network and feature space without
loss of generality or performance. For the optimisation of the network, we
compare the Density Matrix Renormalization Group (DMRG) algorithm to stochastic
gradient descent (SGD) and propose a joined training algorithm to harness the
explainability of DMRG with the efficiency of SGD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Araz_J/0/1/0/all/0/1"&gt;Jack Y. Araz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Spannowsky_M/0/1/0/all/0/1"&gt;Michael Spannowsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polynomial Trajectory Predictions for Improved Learning Performance. (arXiv:2101.12616v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12616</id>
        <link href="http://arxiv.org/abs/2101.12616"/>
        <updated>2021-06-17T01:58:45.875Z</updated>
        <summary type="html"><![CDATA[The rising demand for Active Safety systems in automotive applications
stresses the need for a reliable short to mid-term trajectory prediction.
Anticipating the unfolding path of road users, one can act to increase the
overall safety. In this work, we propose to train artificial neural networks
for movement understanding by predicting trajectories in their natural form, as
a function of time. Predicting polynomial coefficients allows us to increased
accuracy and improve generalisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_I/0/1/0/all/0/1"&gt;Ido Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kummert_A/0/1/0/all/0/1"&gt;Anton Kummert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-sample Test using Projected Wasserstein Distance: Breaking the Curse of Dimensionality. (arXiv:2010.11970v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11970</id>
        <link href="http://arxiv.org/abs/2010.11970"/>
        <updated>2021-06-17T01:58:45.869Z</updated>
        <summary type="html"><![CDATA[We develop a projected Wasserstein distance for the two-sample test, a
fundamental problem in statistics and machine learning: given two sets of
samples, to determine whether they are from the same distribution. In
particular, we aim to circumvent the curse of dimensionality in Wasserstein
distance: when the dimension is high, it has diminishing testing power, which
is inherently due to the slow concentration property of Wasserstein metrics in
the high dimension space. A key contribution is to couple optimal projection to
find the low dimensional linear mapping to maximize the Wasserstein distance
between projected probability distributions. We characterize the theoretical
property of the finite-sample convergence rate on IPMs and present practical
algorithms for computing this metric. Numerical examples validate our
theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gao_R/0/1/0/all/0/1"&gt;Rui Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yao Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An unifying point of view on expressive power of GNNs. (arXiv:2106.08992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08992</id>
        <link href="http://arxiv.org/abs/2106.08992"/>
        <updated>2021-06-17T01:58:45.863Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are a wide class of connectionist models for
graph processing. They perform an iterative message passing operation on each
node and its neighbors, to solve classification/ clustering tasks --- on some
nodes or on the whole graph --- collecting all such messages, regardless of
their order. Despite the differences among the various models belonging to this
class, most of them adopt the same computation scheme, based on a local
aggregation mechanism and, intuitively, the local computation framework is
mainly responsible for the expressive power of GNNs. In this paper, we prove
that the Weisfeiler--Lehman test induces an equivalence relationship on the
graph nodes that exactly corresponds to the unfolding equivalence, defined on
the original GNN model. Therefore, the results on the expressive power of the
original GNNs can be extended to general GNNs which, under mild conditions, can
be proved capable of approximating, in probability and up to any precision, any
function on graphs that respects the unfolding equivalence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DInverno_G/0/1/0/all/0/1"&gt;Giuseppe Alessio D&amp;#x27;Inverno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bianchini_M/0/1/0/all/0/1"&gt;Monica Bianchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sampoli_M/0/1/0/all/0/1"&gt;Maria Lucia Sampoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1"&gt;Franco Scarselli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Schr\"odinger Bridge with Applications to Score-Based Generative Modeling. (arXiv:2106.01357v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01357</id>
        <link href="http://arxiv.org/abs/2106.01357"/>
        <updated>2021-06-17T01:58:45.856Z</updated>
        <summary type="html"><![CDATA[Progressively applying Gaussian noise transforms complex data distributions
to approximately Gaussian. Reversing this dynamic defines a generative model.
When the forward noising process is given by a Stochastic Differential Equation
(SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the
associated reverse-time SDE may be estimated using score-matching. A limitation
of this approach is that the forward-time SDE must be run for a sufficiently
long time for the final distribution to be approximately Gaussian. In contrast,
solving the Schr\"odinger Bridge problem (SB), i.e. an entropy-regularized
optimal transport problem on path spaces, yields diffusions which generate
samples from the data distribution in finite time. We present Diffusion SB
(DSB), an original approximation of the Iterative Proportional Fitting (IPF)
procedure to solve the SB problem, and provide theoretical analysis along with
generative modeling experiments. The first DSB iteration recovers the
methodology proposed by Song et al. (2021), with the flexibility of using
shorter time intervals, as subsequent DSB iterations reduce the discrepancy
between the final-time marginal of the forward (resp. backward) SDE with
respect to the prior (resp. data) distribution. Beyond generative modeling, DSB
offers a widely applicable computational optimal transport tool as the
continuous state-space analogue of the popular Sinkhorn algorithm (Cuturi,
2013).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bortoli_V/0/1/0/all/0/1"&gt;Valentin De Bortoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thornton_J/0/1/0/all/0/1"&gt;James Thornton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heng_J/0/1/0/all/0/1"&gt;Jeremy Heng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonequilibrium thermodynamics of self-supervised learning. (arXiv:2106.08981v1 [cond-mat.stat-mech])]]></title>
        <id>http://arxiv.org/abs/2106.08981</id>
        <link href="http://arxiv.org/abs/2106.08981"/>
        <updated>2021-06-17T01:58:45.847Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (SSL) of energy based models has an intuitive
relation to equilibrium thermodynamics because the softmax layer, mapping
energies to probabilities, is a Gibbs distribution. However, in what way SSL is
a thermodynamic process? We show that some SSL paradigms behave as a
thermodynamic composite system formed by representations and self-labels in
contact with a nonequilibrium reservoir. Moreover, this system is subjected to
usual thermodynamic cycles, such as adiabatic expansion and isochoric heating,
resulting in a generalized Gibbs ensemble (GGE). In this picture, we show that
learning is seen as a demon that operates in cycles using feedback measurements
to extract negative work from the system. As applications, we examine some SSL
algorithms using this idea.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Salazar_D/0/1/0/all/0/1"&gt;Domingos S. P. Salazar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis. (arXiv:2106.08352v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.08352</id>
        <link href="http://arxiv.org/abs/2106.08352"/>
        <updated>2021-06-17T01:58:45.827Z</updated>
        <summary type="html"><![CDATA[Text does not fully specify the spoken form, so text-to-speech models must be
able to learn from speech data that vary in ways not explained by the
corresponding text. One way to reduce the amount of unexplained variation in
training data is to provide acoustic information as an additional learning
signal. When generating speech, modifying this acoustic information enables
multiple distinct renditions of a text to be produced.

Since much of the unexplained variation is in the prosody, we propose a model
that generates speech explicitly conditioned on the three primary acoustic
correlates of prosody: $F_{0}$, energy and duration. The model is flexible
about how the values of these features are specified: they can be externally
provided, or predicted from text, or predicted then subsequently modified.

Compared to a model that employs a variational auto-encoder to learn
unsupervised latent features, our model provides more interpretable,
temporally-precise, and disentangled control. When automatically predicting the
acoustic features from text, it generates speech that is more natural than that
from a Tacotron 2 model with reference encoder. Subsequent human-in-the-loop
modification of the predicted acoustic features can significantly further
increase naturalness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mohan_D/0/1/0/all/0/1"&gt;Devang S Ram Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_V/0/1/0/all/0/1"&gt;Vivian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Teh_T/0/1/0/all/0/1"&gt;Tian Huey Teh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Torresquintero_A/0/1/0/all/0/1"&gt;Alexandra Torresquintero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wallis_C/0/1/0/all/0/1"&gt;Christopher G. R. Wallis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Staib_M/0/1/0/all/0/1"&gt;Marlene Staib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Foglianti_L/0/1/0/all/0/1"&gt;Lorenzo Foglianti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jiameng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+King_S/0/1/0/all/0/1"&gt;Simon King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Reinforcement Learning Under Minimax Regret for Green Security. (arXiv:2106.08413v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08413</id>
        <link href="http://arxiv.org/abs/2106.08413"/>
        <updated>2021-06-17T01:58:45.817Z</updated>
        <summary type="html"><![CDATA[Green security domains feature defenders who plan patrols in the face of
uncertainty about the adversarial behavior of poachers, illegal loggers, and
illegal fishers. Importantly, the deterrence effect of patrols on adversaries'
future behavior makes patrol planning a sequential decision-making problem.
Therefore, we focus on robust sequential patrol planning for green security
following the minimax regret criterion, which has not been considered in the
literature. We formulate the problem as a game between the defender and nature
who controls the parameter values of the adversarial behavior and design an
algorithm MIRROR to find a robust policy. MIRROR uses two reinforcement
learning-based oracles and solves a restricted game considering limited
defender strategies and parameter values. We evaluate MIRROR on real-world
poaching data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lily Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perrault_A/0/1/0/all/0/1"&gt;Andrew Perrault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1"&gt;Fei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haipeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1"&gt;Milind Tambe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Interpretable Spatio-temporal Logic Properties for Spatially Distributed Systems. (arXiv:2106.08548v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08548</id>
        <link href="http://arxiv.org/abs/2106.08548"/>
        <updated>2021-06-17T01:58:45.810Z</updated>
        <summary type="html"><![CDATA[The Internet-of-Things, complex sensor networks, multi-agent cyber-physical
systems are all examples of spatially distributed systems that continuously
evolve in time. Such systems generate huge amounts of spatio-temporal data, and
system designers are often interested in analyzing and discovering structure
within the data. There has been considerable interest in learning causal and
logical properties of temporal data using logics such as Signal Temporal Logic
(STL); however, there is limited work on discovering such relations on
spatio-temporal data. We propose the first set of algorithms for unsupervised
learning for spatio-temporal data. Our method does automatic feature extraction
from the spatio-temporal data by projecting it onto the parameter space of a
parametric spatio-temporal reach and escape logic (PSTREL). We propose an
agglomerative hierarchical clustering technique that guarantees that each
cluster satisfies a distinct STREL formula. We show that our method generates
STREL formulas of bounded description complexity using a novel decision-tree
approach which generalizes previous unsupervised learning techniques for Signal
Temporal Logic. We demonstrate the effectiveness of our approach on case
studies from diverse domains such as urban transportation, epidemiology, green
infrastructure, and air quality monitoring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadinejad_S/0/1/0/all/0/1"&gt;Sara Mohammadinejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1"&gt;Jyotirmy V. Deshmukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nenzi_L/0/1/0/all/0/1"&gt;Laura Nenzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selection of Source Images Heavily Influences the Effectiveness of Adversarial Attacks. (arXiv:2106.07141v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07141</id>
        <link href="http://arxiv.org/abs/2106.07141"/>
        <updated>2021-06-17T01:58:45.797Z</updated>
        <summary type="html"><![CDATA[Although the adoption rate of deep neural networks (DNNs) has tremendously
increased in recent years, a solution for their vulnerability against
adversarial examples has not yet been found. As a result, substantial research
efforts are dedicated to fix this weakness, with many studies typically using a
subset of source images to generate adversarial examples, treating every image
in this subset as equal. We demonstrate that, in fact, not every source image
is equally suited for this kind of assessment. To do so, we devise a
large-scale model-to-model transferability scenario for which we meticulously
analyze the properties of adversarial examples, generated from every suitable
source image in ImageNet by making use of two of the most frequently deployed
attacks. In this transferability scenario, which involves seven distinct DNN
models, including the recently proposed vision transformers, we reveal that it
is possible to have a difference of up to $12.5\%$ in model-to-model
transferability success, $1.01$ in average $L_2$ perturbation, and $0.03$
($8/225$) in average $L_{\infty}$ perturbation when $1,000$ source images are
sampled randomly among all suitable candidates. We then take one of the first
steps in evaluating the robustness of images used to create adversarial
examples, proposing a number of simple but effective methods to identify
unsuitable source images, thus making it possible to mitigate extreme cases in
experimentation and support high-quality benchmarking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozbulak_U/0/1/0/all/0/1"&gt;Utku Ozbulak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anzaku_E/0/1/0/all/0/1"&gt;Esla Timothy Anzaku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neve_W/0/1/0/all/0/1"&gt;Wesley De Neve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Messem_A/0/1/0/all/0/1"&gt;Arnout Van Messem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Better Understanding of Linear Models for Recommendation. (arXiv:2105.12937v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12937</id>
        <link href="http://arxiv.org/abs/2105.12937"/>
        <updated>2021-06-17T01:58:45.779Z</updated>
        <summary type="html"><![CDATA[Recently, linear regression models, such as EASE and SLIM, have shown to
often produce rather competitive results against more sophisticated deep
learning models. On the other side, the (weighted) matrix factorization
approaches have been popular choices for recommendation in the past and widely
adopted in the industry. In this work, we aim to theoretically understand the
relationship between these two approaches, which are the cornerstones of
model-based recommendations. Through the derivation and analysis of the
closed-form solutions for two basic regression and matrix factorization
approaches, we found these two approaches are indeed inherently related but
also diverge in how they "scale-down" the singular values of the original
user-item interaction matrix. This analysis also helps resolve the questions
related to the regularization parameter range and model complexities. We
further introduce a new learning algorithm in searching (hyper)parameters for
the closed-form solution and utilize it to discover the nearby models of the
existing solutions. The experimental results demonstrate that the basic models
and their closed-form solutions are indeed quite competitive against the
state-of-the-art models, thus, confirming the validity of studying the basic
models. The effectiveness of exploring the nearby models are also
experimentally validated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Ruoming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jing Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yang Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear Classifiers in Product Space Forms. (arXiv:2102.10204v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10204</id>
        <link href="http://arxiv.org/abs/2102.10204"/>
        <updated>2021-06-17T01:58:45.767Z</updated>
        <summary type="html"><![CDATA[Embedding methods for product spaces are powerful techniques for
low-distortion and low-dimensional representation of complex data structures.
Nevertheless, little is known regarding downstream learning and optimization
problems in such spaces. Here, we address the problem of linear classification
in a product space form -- a mix of Euclidean, spherical, and hyperbolic
spaces. First, we describe new formulations for linear classifiers on a
Riemannian manifold using geodesics and Riemannian metrics which generalize
straight lines and inner products in vector spaces, respectively. Second, we
prove that linear classifiers in $d$-dimensional space forms of any curvature
have the same expressive power, i.e., they can shatter exactly $d+1$ points.
Third, we formalize linear classifiers in product space forms, describe the
first corresponding perceptron and SVM classification algorithms, and establish
rigorous convergence results for the former. We support our theoretical
findings with simulation results on several datasets, including synthetic data,
CIFAR-100, MNIST, Omniglot, and single-cell RNA sequencing data. The results
show that learning methods applied to small-dimensional embeddings in product
space forms outperform their algorithmic counterparts in each space form.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1"&gt;Puoya Tabaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1"&gt;Eli Chien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1"&gt;Chao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jianhao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1"&gt;Olgica Milenkovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ideal formulations for constrained convex optimization problems with indicator variables. (arXiv:2007.00107v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00107</id>
        <link href="http://arxiv.org/abs/2007.00107"/>
        <updated>2021-06-17T01:58:45.759Z</updated>
        <summary type="html"><![CDATA[Motivated by modern regression applications, in this paper, we study the
convexification of a class of convex optimization problems with indicator
variables and combinatorial constraints on the indicators. Unlike most of the
previous work on convexification of sparse regression problems, we
simultaneously consider the nonlinear non-separable objective, indicator
variables, and combinatorial constraints. Specifically, we give the convex hull
description of the epigraph of the composition of a one-dimensional convex
function and an affine function under arbitrary combinatorial constraints. As
special cases of this result, we derive ideal convexifications for problems
with hierarchy, multi-collinearity, and sparsity constraints. Moreover, we also
give a short proof that for a separable objective function, the perspective
reformulation is ideal independent from the constraints of the problem. Our
computational experiments with regression problems under hierarchy constraints
on real datasets demonstrate the potential of the proposed approach in
improving the relaxation quality without significant computational overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Wei_L/0/1/0/all/0/1"&gt;Linchuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gomez_A/0/1/0/all/0/1"&gt;Andres Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kucukyavuz_S/0/1/0/all/0/1"&gt;Simge Kucukyavuz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based Support Estimation in Sublinear Time. (arXiv:2106.08396v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08396</id>
        <link href="http://arxiv.org/abs/2106.08396"/>
        <updated>2021-06-17T01:58:45.726Z</updated>
        <summary type="html"><![CDATA[We consider the problem of estimating the number of distinct elements in a
large data set (or, equivalently, the support size of the distribution induced
by the data set) from a random sample of its elements. The problem occurs in
many applications, including biology, genomics, computer systems and
linguistics. A line of research spanning the last decade resulted in algorithms
that estimate the support up to $ \pm \varepsilon n$ from a sample of size
$O(\log^2(1/\varepsilon) \cdot n/\log n)$, where $n$ is the data set size.
Unfortunately, this bound is known to be tight, limiting further improvements
to the complexity of this problem. In this paper we consider estimation
algorithms augmented with a machine-learning-based predictor that, given any
element, returns an estimation of its frequency. We show that if the predictor
is correct up to a constant approximation factor, then the sample complexity
can be reduced significantly, to \[ \ \log (1/\varepsilon) \cdot
n^{1-\Theta(1/\log(1/\varepsilon))}. \] We evaluate the proposed algorithms on
a collection of data sets, using the neural-network based estimators from {Hsu
et al, ICLR'19} as predictors. Our experiments demonstrate substantial (up to
3x) improvements in the estimation accuracy compared to the state of the art
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eden_T/0/1/0/all/0/1"&gt;Talya Eden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1"&gt;Piotr Indyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Shyam Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinfeld_R/0/1/0/all/0/1"&gt;Ronitt Rubinfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1"&gt;Sandeep Silwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1"&gt;Tal Wagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Localization, Convexity, and Star Aggregation. (arXiv:2105.08866v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08866</id>
        <link href="http://arxiv.org/abs/2105.08866"/>
        <updated>2021-06-17T01:58:45.720Z</updated>
        <summary type="html"><![CDATA[Offset Rademacher complexities have been shown to imply sharp, data-dependent
upper bounds for the square loss in a broad class of problems including
improper statistical learning and online learning. We show that in the
statistical setting, the offset complexity upper bound can be generalized to
any loss satisfying a certain uniform convexity condition. Amazingly, this
condition is shown to also capture exponential concavity and self-concordance,
uniting several apparently disparate results. By a unified geometric argument,
these bounds translate directly to improper learning in a non-convex class
using Audibert's "star algorithm." As applications, we recover the optimal
rates for proper and improper learning with the $p$-loss, $1 < p < \infty$ and
show that improper variants of empirical risk minimization can attain fast
rates for logistic regression and other generalized linear models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vijaykumar_S/0/1/0/all/0/1"&gt;Suhas Vijaykumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlation Clustering in Constant Many Parallel Rounds. (arXiv:2106.08448v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.08448</id>
        <link href="http://arxiv.org/abs/2106.08448"/>
        <updated>2021-06-17T01:58:45.700Z</updated>
        <summary type="html"><![CDATA[Correlation clustering is a central topic in unsupervised learning, with many
applications in ML and data mining. In correlation clustering, one receives as
input a signed graph and the goal is to partition it to minimize the number of
disagreements. In this work we propose a massively parallel computation (MPC)
algorithm for this problem that is considerably faster than prior work. In
particular, our algorithm uses machines with memory sublinear in the number of
nodes in the graph and returns a constant approximation while running only for
a constant number of rounds. To the best of our knowledge, our algorithm is the
first that can provably approximate a clustering problem on graphs using only a
constant number of MPC rounds in the sublinear memory regime. We complement our
analysis with an experimental analysis of our techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Addad_V/0/1/0/all/0/1"&gt;Vincent Cohen-Addad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lattanzi_S/0/1/0/all/0/1"&gt;Silvio Lattanzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitrovic_S/0/1/0/all/0/1"&gt;Slobodan Mitrovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norouzi_Fard_A/0/1/0/all/0/1"&gt;Ashkan Norouzi-Fard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parotsidis_N/0/1/0/all/0/1"&gt;Nikos Parotsidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarnawski_J/0/1/0/all/0/1"&gt;Jakub Tarnawski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Pixel Attack Deceives Computer-Assisted Diagnosis of Cancer. (arXiv:2012.00517v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00517</id>
        <link href="http://arxiv.org/abs/2012.00517"/>
        <updated>2021-06-17T01:58:45.694Z</updated>
        <summary type="html"><![CDATA[Computer vision and machine learning can be used to automate various tasks in
cancer diagnostic and detection. If an attacker can manipulate the automated
processing, the results can be devastating and in the worst case lead to wrong
diagnosis and treatment. In this research, the goal is to demonstrate the use
of one-pixel attacks in a real-life scenario with a real pathology dataset,
TUPAC16, which consists of digitized whole-slide images. We attack against the
IBM CODAIT's MAX breast cancer detector using adversarial images. These
adversarial examples are found using differential evolution to perform the
one-pixel modification to the images in the dataset. The results indicate that
a minor one-pixel modification of a whole slide image under analysis can affect
the diagnosis by reversing the automatic diagnosis result. The attack poses a
threat from the cyber security perspective: the one-pixel method can be used as
an attack vector by a motivated attacker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korpihalkola_J/0/1/0/all/0/1"&gt;Joni Korpihalkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sipola_T/0/1/0/all/0/1"&gt;Tuomo Sipola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puuska_S/0/1/0/all/0/1"&gt;Samir Puuska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokkonen_T/0/1/0/all/0/1"&gt;Tero Kokkonen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DMSANet: Dual Multi Scale Attention Network. (arXiv:2106.08382v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08382</id>
        <link href="http://arxiv.org/abs/2106.08382"/>
        <updated>2021-06-17T01:58:45.686Z</updated>
        <summary type="html"><![CDATA[Attention mechanism of late has been quite popular in the computer vision
community. A lot of work has been done to improve the performance of the
network, although almost always it results in increased computational
complexity. In this paper, we propose a new attention module that not only
achieves the best performance but also has lesser parameters compared to most
existing models. Our attention module can easily be integrated with other
convolutional neural networks because of its lightweight nature. The proposed
network named Dual Multi Scale Attention Network (DMSANet) is comprised of two
parts: the first part is used to extract features at various scales and
aggregate them, the second part uses spatial and channel attention modules in
parallel to adaptively integrate local features with their global dependencies.
We benchmark our network performance for Image Classification on ImageNet
dataset, Object Detection and Instance Segmentation both on MS COCO dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental Limits of Reinforcement Learning in Environment with Endogeneous and Exogeneous Uncertainty. (arXiv:2106.08477v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08477</id>
        <link href="http://arxiv.org/abs/2106.08477"/>
        <updated>2021-06-17T01:58:45.615Z</updated>
        <summary type="html"><![CDATA[Online reinforcement learning (RL) has been widely applied in information
processing scenarios, which usually exhibit much uncertainty due to the
intrinsic randomness of channels and service demands. In this paper, we
consider an un-discounted RL in general Markov decision processes (MDPs) with
both endogeneous and exogeneous uncertainty, where both the rewards and state
transition probability are unknown to the RL agent and evolve with the time as
long as their respective variations do not exceed certain dynamic budget (i.e.,
upper bound). We first develop a variation-aware Bernstein-based upper
confidence reinforcement learning (VB-UCRL), which we allow to restart
according to a schedule dependent on the variations. We successfully overcome
the challenges due to the exogeneous uncertainty and establish a regret bound
of saving at most $\sqrt{S}$ or $S^{\frac{1}{6}}T^{\frac{1}{12}}$ compared with
the latest results in the literature, where $S$ denotes the state size of the
MDP and $T$ indicates the iteration index of learning steps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rongpeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascading Modular Network (CAM-Net) for Multimodal Image Synthesis. (arXiv:2106.09015v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09015</id>
        <link href="http://arxiv.org/abs/2106.09015"/>
        <updated>2021-06-17T01:58:45.215Z</updated>
        <summary type="html"><![CDATA[Deep generative models such as GANs have driven impressive advances in
conditional image synthesis in recent years. A persistent challenge has been to
generate diverse versions of output images from the same input image, due to
the problem of mode collapse: because only one ground truth output image is
given per input image, only one mode of the conditional distribution is
modelled. In this paper, we focus on this problem of multimodal conditional
image synthesis and build on the recently proposed technique of Implicit
Maximum Likelihood Estimation (IMLE). Prior IMLE-based methods required
different architectures for different tasks, which limit their applicability,
and were lacking in fine details in the generated images. We propose CAM-Net, a
unified architecture that can be applied to a broad range of tasks.
Additionally, it is capable of generating convincing high frequency details,
achieving a reduction of the Frechet Inception Distance (FID) by up to 45.3%
compared to the baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shichong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moazeni_A/0/1/0/all/0/1"&gt;Alireza Moazeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpeakerStew: Scaling to Many Languages with a Triaged Multilingual Text-Dependent and Text-Independent Speaker Verification System. (arXiv:2104.02125v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02125</id>
        <link href="http://arxiv.org/abs/2104.02125"/>
        <updated>2021-06-17T01:58:45.188Z</updated>
        <summary type="html"><![CDATA[In this paper, we describe SpeakerStew - a hybrid system to perform speaker
verification on 46 languages. Two core ideas were explored in this system: (1)
Pooling training data of different languages together for multilingual
generalization and reducing development cycles; (2) A novel triage mechanism
between text-dependent and text-independent models to reduce runtime cost and
expected latency. To the best of our knowledge, this is the first study of
speaker verification systems at the scale of 46 languages. The problem is
framed from the perspective of using a smart speaker device with interactions
consisting of a wake-up keyword (text-dependent) followed by a speech query
(text-independent). Experimental evidence suggests that training on multiple
languages can generalize to unseen varieties while maintaining performance on
seen varieties. We also found that it can reduce computational requirements for
training models by an order of magnitude. Furthermore, during model inference
on English data, we observe that leveraging a triage framework can reduce the
number of calls to the more computationally expensive text-independent system
by 73% (and reduce latency by 59%) while maintaining an EER no worse than the
text-independent setup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chojnacka_R/0/1/0/all/0/1"&gt;Roza Chojnacka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pelecanos_J/0/1/0/all/0/1"&gt;Jason Pelecanos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Quan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1"&gt;Ignacio Lopez Moreno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Objective Evaluation of Post Hoc Explainers. (arXiv:2106.08376v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08376</id>
        <link href="http://arxiv.org/abs/2106.08376"/>
        <updated>2021-06-17T01:58:45.163Z</updated>
        <summary type="html"><![CDATA[Many applications of data-driven models demand transparency of decisions,
especially in health care, criminal justice, and other high-stakes
environments. Modern trends in machine learning research have led to algorithms
that are increasingly intricate to the degree that they are considered to be
black boxes. In an effort to reduce the opacity of decisions, methods have been
proposed to construe the inner workings of such models in a
human-comprehensible manner. These post hoc techniques are described as being
universal explainers - capable of faithfully augmenting decisions with
algorithmic insight. Unfortunately, there is little agreement about what
constitutes a "good" explanation. Moreover, current methods of explanation
evaluation are derived from either subjective or proxy means. In this work, we
propose a framework for the evaluation of post hoc explainers on ground truth
that is directly derived from the additive structure of a model. We demonstrate
the efficacy of the framework in understanding explainers by evaluating popular
explainers on thousands of synthetic and several real-world tasks. The
framework unveils that explanations may be accurate but misattribute the
importance of individual features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carmichael_Z/0/1/0/all/0/1"&gt;Zachariah Carmichael&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1"&gt;Walter J. Scheirer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Resolution Continuous Normalizing Flows. (arXiv:2106.08462v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08462</id>
        <link href="http://arxiv.org/abs/2106.08462"/>
        <updated>2021-06-17T01:58:45.157Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that Neural Ordinary Differential Equations (ODEs) can
serve as generative models of images using the perspective of Continuous
Normalizing Flows (CNFs). Such models offer exact likelihood calculation, and
invertible generation/density estimation. In this work we introduce a
Multi-Resolution variant of such models (MRCNF), by characterizing the
conditional distribution over the additional information required to generate a
fine image that is consistent with the coarse image. We introduce a
transformation between resolutions that allows for no change in the log
likelihood. We show that this approach yields comparable likelihood values for
various image datasets, with improved performance at higher resolutions, with
fewer parameters, using only 1 GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1"&gt;Vikram Voleti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finlay_C/0/1/0/all/0/1"&gt;Chris Finlay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1"&gt;Adam Oberman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Adversarial Robustness via Transductive Learning. (arXiv:2106.08387v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08387</id>
        <link href="http://arxiv.org/abs/2106.08387"/>
        <updated>2021-06-17T01:58:45.149Z</updated>
        <summary type="html"><![CDATA[There has been emerging interest to use transductive learning for adversarial
robustness (Goldwasser et al., NeurIPS 2020; Wu et al., ICML 2020). Compared to
traditional "test-time" defenses, these defense mechanisms "dynamically
retrain" the model based on test time input via transductive learning; and
theoretically, attacking these defenses boils down to bilevel optimization,
which seems to raise the difficulty for adaptive attacks. In this paper, we
first formalize and analyze modeling aspects of transductive robustness. Then,
we propose the principle of attacking model space for solving bilevel attack
objectives, and present an instantiation of the principle which breaks previous
transductive defenses. These attacks thus point to significant difficulties in
the use of transductive learning to improve adversarial robustness. To this
end, we present new theoretical and empirical evidence in support of the
utility of transductive learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiefeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lao_Q/0/1/0/all/0/1"&gt;Qicheng Lao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yingyu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best of both worlds: local and global explanations with human-understandable concepts. (arXiv:2106.08641v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08641</id>
        <link href="http://arxiv.org/abs/2106.08641"/>
        <updated>2021-06-17T01:58:45.134Z</updated>
        <summary type="html"><![CDATA[Interpretability techniques aim to provide the rationale behind a model's
decision, typically by explaining either an individual prediction (local
explanation, e.g. `why is this patient diagnosed with this condition') or a
class of predictions (global explanation, e.g. `why are patients diagnosed with
this condition in general'). While there are many methods focused on either
one, few frameworks can provide both local and global explanations in a
consistent manner. In this work, we combine two powerful existing techniques,
one local (Integrated Gradients, IG) and one global (Testing with Concept
Activation Vectors), to provide local, and global concept-based explanations.
We first validate our idea using two synthetic datasets with a known ground
truth, and further demonstrate with a benchmark natural image dataset. We test
our method with various concepts, target classes, model architectures and IG
baselines. We show that our method improves global explanations over TCAV when
compared to ground truth, and provides useful insights. We hope our work
provides a step towards building bridges between many existing local and global
methods to get the best of both worlds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schrouff_J/0/1/0/all/0/1"&gt;Jessica Schrouff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baur_S/0/1/0/all/0/1"&gt;Sebastien Baur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1"&gt;Shaobo Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mincu_D/0/1/0/all/0/1"&gt;Diana Mincu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loreaux_E/0/1/0/all/0/1"&gt;Eric Loreaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanes_R/0/1/0/all/0/1"&gt;Ralph Blanes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wexler_J/0/1/0/all/0/1"&gt;James Wexler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1"&gt;Alan Karthikesalingam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Been Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Affective XAI: Facial Affect Analysis for Understanding Explainable Human-AI Interactions. (arXiv:2106.08761v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08761</id>
        <link href="http://arxiv.org/abs/2106.08761"/>
        <updated>2021-06-17T01:58:45.117Z</updated>
        <summary type="html"><![CDATA[As machine learning approaches are increasingly used to augment human
decision-making, eXplainable Artificial Intelligence (XAI) research has
explored methods for communicating system behavior to humans. However, these
approaches often fail to account for the emotional responses of humans as they
interact with explanations. Facial affect analysis, which examines human facial
expressions of emotions, is one promising lens for understanding how users
engage with explanations. Therefore, in this work, we aim to (1) identify which
facial affect features are pronounced when people interact with XAI interfaces,
and (2) develop a multitask feature embedding for linking facial affect signals
with participants' use of explanations. Our analyses and results show that the
occurrence and values of facial AU1 and AU4, and Arousal are heightened when
participants fail to use explanations effectively. This suggests that facial
affect analysis should be incorporated into XAI to personalize explanations to
individuals' interaction styles and to adapt explanations based on the
difficulty of the task performed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guerdan_L/0/1/0/all/0/1"&gt;Luke Guerdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raymond_A/0/1/0/all/0/1"&gt;Alex Raymond&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunes_H/0/1/0/all/0/1"&gt;Hatice Gunes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Pixel Attack Deceives Computer-Assisted Diagnosis of Cancer. (arXiv:2012.00517v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00517</id>
        <link href="http://arxiv.org/abs/2012.00517"/>
        <updated>2021-06-17T01:58:45.110Z</updated>
        <summary type="html"><![CDATA[Computer vision and machine learning can be used to automate various tasks in
cancer diagnostic and detection. If an attacker can manipulate the automated
processing, the results can be devastating and in the worst case lead to wrong
diagnosis and treatment. In this research, the goal is to demonstrate the use
of one-pixel attacks in a real-life scenario with a real pathology dataset,
TUPAC16, which consists of digitized whole-slide images. We attack against the
IBM CODAIT's MAX breast cancer detector using adversarial images. These
adversarial examples are found using differential evolution to perform the
one-pixel modification to the images in the dataset. The results indicate that
a minor one-pixel modification of a whole slide image under analysis can affect
the diagnosis by reversing the automatic diagnosis result. The attack poses a
threat from the cyber security perspective: the one-pixel method can be used as
an attack vector by a motivated attacker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korpihalkola_J/0/1/0/all/0/1"&gt;Joni Korpihalkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sipola_T/0/1/0/all/0/1"&gt;Tuomo Sipola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puuska_S/0/1/0/all/0/1"&gt;Samir Puuska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokkonen_T/0/1/0/all/0/1"&gt;Tero Kokkonen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nystr\"om Method, and Use of Kernels in Machine Learning: Tutorial and Survey. (arXiv:2106.08443v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08443</id>
        <link href="http://arxiv.org/abs/2106.08443"/>
        <updated>2021-06-17T01:58:45.097Z</updated>
        <summary type="html"><![CDATA[This is a tutorial and survey paper on kernels, kernel methods, and related
fields. We start with reviewing the history of kernels in functional analysis
and machine learning. Then, Mercer kernel, Hilbert and Banach spaces,
Reproducing Kernel Hilbert Space (RKHS), Mercer's theorem and its proof,
frequently used kernels, kernel construction from distance metric, important
classes of kernels (including bounded, integrally positive definite, universal,
stationary, and characteristic kernels), kernel centering and normalization,
and eigenfunctions are explained in detail. Then, we introduce types of use of
kernels in machine learning including kernel methods (such as kernel support
vector machines), kernel learning by semi-definite programming, Hilbert-Schmidt
independence criterion, maximum mean discrepancy, kernel mean embedding, and
kernel dimensionality reduction. We also cover rank and factorization of kernel
matrix as well as the approximation of eigenfunctions and kernels using the
Nystr{\"o}m method. This paper can be useful for various fields of science
including machine learning, dimensionality reduction, functional analysis in
mathematics, and mathematical physics in quantum mechanics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation. (arXiv:2106.09017v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09017</id>
        <link href="http://arxiv.org/abs/2106.09017"/>
        <updated>2021-06-17T01:58:45.091Z</updated>
        <summary type="html"><![CDATA[Multi-task learning (MTL) aims to improve the generalization of several
related tasks by learning them jointly. As a comparison, in addition to the
joint training scheme, modern meta-learning allows unseen tasks with limited
labels during the test phase, in the hope of fast adaptation over them. Despite
the subtle difference between MTL and meta-learning in the problem formulation,
both learning paradigms share the same insight that the shared structure
between existing training tasks could lead to better generalization and
adaptation. In this paper, we take one important step further to understand the
close connection between these two learning paradigms, through both theoretical
analysis and empirical investigation. Theoretically, we first demonstrate that
MTL shares the same optimization formulation with a class of gradient-based
meta-learning (GBML) algorithms. We then prove that for over-parameterized
neural networks with sufficient depth, the learned predictive functions of MTL
and GBML are close. In particular, this result implies that the predictions
given by these two models are similar over the same unseen task. Empirically,
we corroborate our theoretical findings by showing that, with proper
implementation, MTL is competitive against state-of-the-art GBML algorithms on
a set of few-shot image classification benchmarks. Since existing GBML
algorithms often involve costly second-order bi-level optimization, our
first-order MTL method is an order of magnitude faster on large-scale datasets
such as mini-ImageNet. We believe this work could help bridge the gap between
these two learning paradigms, and provide a computationally efficient
alternative to GBML that also supports fast task adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Finite-Horizon Approximation and Efficient Optimal Algorithms for Stochastic Shortest Path. (arXiv:2106.08377v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08377</id>
        <link href="http://arxiv.org/abs/2106.08377"/>
        <updated>2021-06-17T01:58:45.086Z</updated>
        <summary type="html"><![CDATA[We introduce a generic template for developing regret minimization algorithms
in the Stochastic Shortest Path (SSP) model, which achieves minimax optimal
regret as long as certain properties are ensured. The key of our analysis is a
new technique called implicit finite-horizon approximation, which approximates
the SSP model by a finite-horizon counterpart only in the analysis without
explicit implementation. Using this template, we develop two new algorithms:
the first one is model-free (the first in the literature to our knowledge) and
minimax optimal under strictly positive costs; the second one is model-based
and minimax optimal even with zero-cost state-action pairs, matching the best
existing result from [Tarbouriech et al., 2021b]. Importantly, both algorithms
admit highly sparse updates, making them computationally more efficient than
all existing algorithms. Moreover, both can be made completely parameter-free.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jafarnia_Jahromi_M/0/1/0/all/0/1"&gt;Mehdi Jafarnia-Jahromi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1"&gt;Rahul Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Optimisation of Bellman Residual Errors with Neural Function Approximation. (arXiv:2106.08774v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08774</id>
        <link href="http://arxiv.org/abs/2106.08774"/>
        <updated>2021-06-17T01:58:45.080Z</updated>
        <summary type="html"><![CDATA[Recent development of Deep Reinforcement Learning has demonstrated superior
performance of neural networks in solving challenging problems with large or
even continuous state spaces. One specific approach is to deploy neural
networks to approximate value functions by minimising the Mean Squared Bellman
Error function. Despite great successes of Deep Reinforcement Learning,
development of reliable and efficient numerical algorithms to minimise the
Bellman Error is still of great scientific interest and practical demand. Such
a challenge is partially due to the underlying optimisation problem being
highly non-convex or using incorrect gradient information as done in
Semi-Gradient algorithms. In this work, we analyse the Mean Squared Bellman
Error from a smooth optimisation perspective combined with a Residual Gradient
formulation. Our contribution is two-fold.

First, we analyse critical points of the error function and provide technical
insights on the optimisation procure and design choices for neural networks.
When the existence of global minima is assumed and the objective fulfils
certain conditions we can eliminate suboptimal local minima when using
over-parametrised neural networks. We can construct an efficient Approximate
Newton's algorithm based on our analysis and confirm theoretical properties of
this algorithm such as being locally quadratically convergent to a global
minimum numerically.

Second, we demonstrate feasibility and generalisation capabilities of the
proposed algorithm empirically using continuous control problems and provide a
numerical verification of our critical point analysis. We outline the short
coming of Semi-Gradients. To benefit from an approximate Newton's algorithm
complete derivatives of the Mean Squared Bellman error must be considered
during training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gottwald_M/0/1/0/all/0/1"&gt;Martin Gottwald&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Gronauer_S/0/1/0/all/0/1"&gt;Sven Gronauer&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Hao Shen&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Diepold_K/0/1/0/all/0/1"&gt;Klaus Diepold&lt;/a&gt; (1) ((1) Technical University of Munich, (2) fortiss)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation for Graph Convolutional Network on Semi-Supervised Classification. (arXiv:2106.08848v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08848</id>
        <link href="http://arxiv.org/abs/2106.08848"/>
        <updated>2021-06-17T01:58:45.074Z</updated>
        <summary type="html"><![CDATA[Data augmentation aims to generate new and synthetic features from the
original data, which can identify a better representation of data and improve
the performance and generalizability of downstream tasks. However, data
augmentation for graph-based models remains a challenging problem, as graph
data is more complex than traditional data, which consists of two features with
different properties: graph topology and node attributes. In this paper, we
study the problem of graph data augmentation for Graph Convolutional Network
(GCN) in the context of improving the node embeddings for semi-supervised node
classification. Specifically, we conduct cosine similarity based cross
operation on the original features to create new graph features, including new
node attributes and new graph topologies, and we combine them as new pairwise
inputs for specific GCNs. Then, we propose an attentional integrating model to
weighted sum the hidden node embeddings encoded by these GCNs into the final
node embeddings. We also conduct a disparity constraint on these hidden node
embeddings when training to ensure that non-redundant information is captured
from different features. Experimental results on five real-world datasets show
that our method improves the classification accuracy with a clear margin (+2.5%
- +84.2%) than the original GCN model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhengzheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Ziyue Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1"&gt;Xuehai Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dharejo_F/0/1/0/all/0/1"&gt;Fayaz Ali Dharejo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuanchun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yi Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Anomaly Detection in Edge Streams. (arXiv:2009.08452v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08452</id>
        <link href="http://arxiv.org/abs/2009.08452"/>
        <updated>2021-06-17T01:58:45.068Z</updated>
        <summary type="html"><![CDATA[Given a stream of graph edges from a dynamic graph, how can we assign anomaly
scores to edges in an online manner, for the purpose of detecting unusual
behavior, using constant time and memory? Existing approaches aim to detect
individually surprising edges. In this work, we propose MIDAS, which focuses on
detecting microcluster anomalies, or suddenly arriving groups of suspiciously
similar edges, such as lockstep behavior, including denial of service attacks
in network traffic data. We further propose MIDAS-F, to solve the problem by
which anomalies are incorporated into the algorithm's internal states, creating
a `poisoning' effect that can allow future anomalies to slip through
undetected. MIDAS-F introduces two modifications: 1) We modify the anomaly
scoring function, aiming to reduce the `poisoning' effect of newly arriving
edges; 2) We introduce a conditional merge step, which updates the algorithm's
data structures after each time tick, but only if the anomaly score is below a
threshold value, also to reduce the `poisoning' effect. Experiments show that
MIDAS-F has significantly higher accuracy than MIDAS. MIDAS has the following
properties: (a) it detects microcluster anomalies while providing theoretical
guarantees about its false positive probability; (b) it is online, thus
processing each edge in constant time and constant memory, and also processes
the data orders-of-magnitude faster than state-of-the-art approaches; (c) it
provides up to 62% higher ROC-AUC than state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1"&gt;Siddharth Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Rui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1"&gt;Bryan Hooi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_M/0/1/0/all/0/1"&gt;Minji Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1"&gt;Kijung Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faloutsos_C/0/1/0/all/0/1"&gt;Christos Faloutsos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Morphed Face Images Using Discriminative Wavelet Sub-bands. (arXiv:2106.08565v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08565</id>
        <link href="http://arxiv.org/abs/2106.08565"/>
        <updated>2021-06-17T01:58:45.062Z</updated>
        <summary type="html"><![CDATA[This work investigates the well-known problem of morphing attacks, which has
drawn considerable attention in the biometrics community. Morphed images have
exposed face recognition systems' susceptibility to false acceptance, resulting
in dire consequences, especially for national security applications. To detect
morphing attacks, we propose a method which is based on a discriminative 2D
Discrete Wavelet Transform (2D-DWT). A discriminative wavelet sub-band can
highlight inconsistencies between a real and a morphed image. We observe that
there is a salient discrepancy between the entropy of a given sub-band in a
bona fide image, and the same sub-band's entropy in a morphed sample.
Considering this dissimilarity between these two entropy values, we find the
Kullback-Leibler divergence between the two distributions, namely the entropy
of the bona fide and the corresponding morphed images. The most discriminative
wavelet sub-bands are those with the highest corresponding KL-divergence
values. Accordingly, 22 sub-bands are selected as the most discriminative ones
in terms of morph detection. We show that a Deep Neural Network (DNN) trained
on the 22 discriminative sub-bands can detect morphed samples precisely. Most
importantly, the effectiveness of our algorithm is validated through
experiments on three datasets: VISAPP17, LMA, and MorGAN. We also performed an
ablation study on the sub-band selection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1"&gt;Poorya Aghdaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1"&gt;Baaria Chaudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1"&gt;Sobhan Soleymani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1"&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1"&gt;Nasser M. Nasrabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness of Object Detectors in Degrading Weather Conditions. (arXiv:2106.08795v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08795</id>
        <link href="http://arxiv.org/abs/2106.08795"/>
        <updated>2021-06-17T01:58:45.055Z</updated>
        <summary type="html"><![CDATA[State-of-the-art object detection systems for autonomous driving achieve
promising results in clear weather conditions. However, such autonomous safety
critical systems also need to work in degrading weather conditions, such as
rain, fog and snow. Unfortunately, most approaches evaluate only on the KITTI
dataset, which consists only of clear weather scenes. In this paper we address
this issue and perform one of the most detailed evaluation on single and dual
modality architectures on data captured in real weather conditions. We analyse
the performance degradation of these architectures in degrading weather
conditions. We demonstrate that an object detection architecture performing
good in clear weather might not be able to handle degrading weather conditions.
We also perform ablation studies on the dual modality architectures and show
their limitations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mirza_M/0/1/0/all/0/1"&gt;Muhammad Jehanzeb Mirza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buerkle_C/0/1/0/all/0/1"&gt;Cornelius Buerkle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jarquin_J/0/1/0/all/0/1"&gt;Julio Jarquin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Opitz_M/0/1/0/all/0/1"&gt;Michael Opitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oboril_F/0/1/0/all/0/1"&gt;Fabian Oboril&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholl_K/0/1/0/all/0/1"&gt;Kay-Ulrich Scholl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bischof_H/0/1/0/all/0/1"&gt;Horst Bischof&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure First Detail Next: Image Inpainting with Pyramid Generator. (arXiv:2106.08905v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08905</id>
        <link href="http://arxiv.org/abs/2106.08905"/>
        <updated>2021-06-17T01:58:45.039Z</updated>
        <summary type="html"><![CDATA[Recent deep generative models have achieved promising performance in image
inpainting. However, it is still very challenging for a neural network to
generate realistic image details and textures, due to its inherent spectral
bias. By our understanding of how artists work, we suggest to adopt a
`structure first detail next' workflow for image inpainting. To this end, we
propose to build a Pyramid Generator by stacking several sub-generators, where
lower-layer sub-generators focus on restoring image structures while the
higher-layer sub-generators emphasize image details. Given an input image, it
will be gradually restored by going through the entire pyramid in a bottom-up
fashion. Particularly, our approach has a learning scheme of progressively
increasing hole size, which allows it to restore large-hole images. In
addition, our method could fully exploit the benefits of learning with
high-resolution images, and hence is suitable for high-resolution image
inpainting. Extensive experimental results on benchmark datasets have validated
the effectiveness of our approach compared with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1"&gt;Shuyi Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1"&gt;Zhenxing Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1"&gt;Matan Protter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimerman_G/0/1/0/all/0/1"&gt;Gadi Zimerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yinghui Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LCDNet: Deep Loop Closure Detection and Point Cloud Registration for LiDAR SLAM. (arXiv:2103.05056v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05056</id>
        <link href="http://arxiv.org/abs/2103.05056"/>
        <updated>2021-06-17T01:58:45.014Z</updated>
        <summary type="html"><![CDATA[Loop closure detection is an essential component of Simultaneous Localization
and Mapping (SLAM) systems, which reduces the drift accumulated over time. Over
the years, several deep learning approaches have been proposed to address this
task, however their performance has been subpar compared to handcrafted
techniques, especially while dealing with reverse loops. In this paper, we
introduce the novel LCDNet that effectively detects loop closures in LiDAR
point clouds by simultaneously identifying previously visited places and
estimating the 6-DoF relative transformation between the current scan and the
map. LCDNet is composed of a shared encoder, a place recognition head that
extracts global descriptors, and a relative pose head that estimates the
transformation between two point clouds. We introduce a novel relative pose
head based on the unbalanced optimal transport theory that we implement in a
differentiable manner to allow for end-to-end training. Extensive evaluations
of LCDNet on multiple real-world autonomous driving datasets show that our
approach outperforms state-of-the-art loop closure detection and point cloud
registration techniques by a large margin, especially while dealing with
reverse loops. Moreover, we integrate our proposed loop closure detection
approach into a LiDAR SLAM library to provide a complete mapping system and
demonstrate the generalization ability using different sensor setup in an
unseen city.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1"&gt;Daniele Cattaneo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1"&gt;Matteo Vaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1"&gt;Abhinav Valada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for Discovering Optimal Solutions in Photonic Inverse Design. (arXiv:2106.08419v1 [physics.optics])]]></title>
        <id>http://arxiv.org/abs/2106.08419</id>
        <link href="http://arxiv.org/abs/2106.08419"/>
        <updated>2021-06-17T01:58:45.006Z</updated>
        <summary type="html"><![CDATA[Photonic inverse design has emerged as an indispensable engineering tool for
complex optical systems. In many instances it is important to optimize for both
material and geometry configurations, which results in complex non-smooth
search spaces with multiple local minima. Finding solutions approaching global
optimum may present a computationally intractable task. Here, we develop a
framework that allows expediting the search of solutions close to global
optimum on complex optimization spaces. We study the way representative black
box optimization algorithms work, including genetic algorithm (GA), particle
swarm optimization (PSO), simulated annealing (SA), and mesh adaptive direct
search (NOMAD). We then propose and utilize a two-step approach that identifies
best performance algorithms on arbitrarily complex search spaces. We reveal a
connection between the search space complexity and algorithm performance and
find that PSO and NOMAD consistently deliver better performance for mixed
integer problems encountered in photonic inverse design, particularly with the
account of material combinations. Our results differ from a commonly
anticipated advantage of GA. Our findings will foster more efficient design of
photonic systems with optimal performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Digani_J/0/1/0/all/0/1"&gt;Jagrit Digani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hon_P/0/1/0/all/0/1"&gt;Phillip Hon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Davoyan_A/0/1/0/all/0/1"&gt;Artur R. Davoyan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HELP: Hardware-Adaptive Efficient Latency Predictor for NAS via Meta-Learning. (arXiv:2106.08630v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08630</id>
        <link href="http://arxiv.org/abs/2106.08630"/>
        <updated>2021-06-17T01:58:44.993Z</updated>
        <summary type="html"><![CDATA[For deployment, neural architecture search should be hardware-aware, in order
to satisfy the device-specific constraints (e.g., memory usage, latency and
energy consumption) and enhance the model efficiency. Existing methods on
hardware-aware NAS collect a large number of samples (e.g., accuracy and
latency) from a target device, either builds a lookup table or a latency
estimator. However, such approach is impractical in real-world scenarios as
there exist numerous devices with different hardware specifications, and
collecting samples from such a large number of devices will require prohibitive
computational and monetary cost. To overcome such limitations, we propose
Hardware-adaptive Efficient Latency Predictor (HELP), which formulates the
device-specific latency estimation problem as a meta-learning problem, such
that we can estimate the latency of a model's performance for a given task on
an unseen device with a few samples. To this end, we introduce novel hardware
embeddings to embed any devices considering them as black-box functions that
output latencies, and meta-learn the hardware-adaptive latency predictor in a
device-dependent manner, using the hardware embeddings. We validate the
proposed HELP for its latency estimation performance on unseen platforms, on
which it achieves high estimation performance with as few as 10 measurement
samples, outperforming all relevant baselines. We also validate end-to-end NAS
frameworks using HELP against ones without it, and show that it largely reduces
the total time cost of the base NAS method, in latency-constrained settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hayeon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sewoong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chong_S/0/1/0/all/0/1"&gt;Song Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascading Modular Network (CAM-Net) for Multimodal Image Synthesis. (arXiv:2106.09015v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09015</id>
        <link href="http://arxiv.org/abs/2106.09015"/>
        <updated>2021-06-17T01:58:44.978Z</updated>
        <summary type="html"><![CDATA[Deep generative models such as GANs have driven impressive advances in
conditional image synthesis in recent years. A persistent challenge has been to
generate diverse versions of output images from the same input image, due to
the problem of mode collapse: because only one ground truth output image is
given per input image, only one mode of the conditional distribution is
modelled. In this paper, we focus on this problem of multimodal conditional
image synthesis and build on the recently proposed technique of Implicit
Maximum Likelihood Estimation (IMLE). Prior IMLE-based methods required
different architectures for different tasks, which limit their applicability,
and were lacking in fine details in the generated images. We propose CAM-Net, a
unified architecture that can be applied to a broad range of tasks.
Additionally, it is capable of generating convincing high frequency details,
achieving a reduction of the Frechet Inception Distance (FID) by up to 45.3%
compared to the baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shichong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moazeni_A/0/1/0/all/0/1"&gt;Alireza Moazeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LassoNet: A Neural Network with Feature Sparsity. (arXiv:1907.12207v10 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.12207</id>
        <link href="http://arxiv.org/abs/1907.12207"/>
        <updated>2021-06-17T01:58:44.962Z</updated>
        <summary type="html"><![CDATA[Much work has been done recently to make neural networks more interpretable,
and one obvious approach is to arrange for the network to use only a subset of
the available features. In linear models, Lasso (or $\ell_1$-regularized)
regression assigns zero weights to the most irrelevant or redundant features,
and is widely used in data science. However the Lasso only applies to linear
models. Here we introduce LassoNet, a neural network framework with global
feature selection. Our approach enforces a hierarchy: specifically a feature
can participate in a hidden unit only if its linear representative is active.
Unlike other approaches to feature selection for neural nets, our method uses a
modified objective function with constraints, and so integrates feature
selection with the parameter learning directly. As a result, it delivers an
entire regularization path of solutions with a range of feature sparsity. On
systematic experiments, LassoNet significantly outperforms state-of-the-art
methods for feature selection and regression. The LassoNet method uses
projected proximal gradient descent, and generalizes directly to deep networks.
It can be implemented by adding just a few lines of code to a standard neural
network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lemhadri_I/0/1/0/all/0/1"&gt;Ismael Lemhadri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ruan_F/0/1/0/all/0/1"&gt;Feng Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Abraham_L/0/1/0/all/0/1"&gt;Louis Abraham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tibshirani_R/0/1/0/all/0/1"&gt;Robert Tibshirani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissecting Hessian: Understanding Common Structure of Hessian in Neural Networks. (arXiv:2010.04261v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04261</id>
        <link href="http://arxiv.org/abs/2010.04261"/>
        <updated>2021-06-17T01:58:44.943Z</updated>
        <summary type="html"><![CDATA[Hessian captures important properties of the deep neural network loss
landscape. Previous works have observed low rank structure in the Hessians of
neural networks. We make several new observations about the top eigenspace of
layer-wise Hessian: top eigenspaces for different models have surprisingly high
overlap, and top eigenvectors form low rank matrices when they are reshaped
into the same shape as the corresponding weight matrix. Towards formally
explaining such structures of the Hessian, we show that the new eigenspace
structure can be explained by approximating the Hessian using Kronecker
factorization; we also prove the low rank structure for random data at random
initialization for over-parametrized two-layer neural nets. Our new
understanding can explain why some of these structures become weaker when the
network is trained with batch normalization. The Kronecker factorization also
leads to better explicit generalization bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yikai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xingyu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Annie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1"&gt;Rong Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the exchange-correlation functional from nature with fully differentiable density functional theory. (arXiv:2102.04229v4 [physics.chem-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04229</id>
        <link href="http://arxiv.org/abs/2102.04229"/>
        <updated>2021-06-17T01:58:44.918Z</updated>
        <summary type="html"><![CDATA[Improving the predictive capability of molecular properties in ab initio
simulations is essential for advanced material discovery. Despite recent
progress making use of machine learning, utilizing deep neural networks to
improve quantum chemistry modelling remains severely limited by the scarcity
and heterogeneity of appropriate experimental data. Here we show how training a
neural network to replace the exchange-correlation functional within a
fully-differentiable three-dimensional Kohn-Sham density functional theory
(DFT) framework can greatly improve simulation accuracy. Using only eight
experimental data points on diatomic molecules, our trained
exchange-correlation networks enable improved prediction accuracy of
atomization energies across a collection of 104 molecules containing new bonds
and atoms that are not present in the training dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Kasim_M/0/1/0/all/0/1"&gt;Muhammad F. Kasim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Vinko_S/0/1/0/all/0/1"&gt;Sam M. Vinko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EdgeConv with Attention Module for Monocular Depth Estimation. (arXiv:2106.08615v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08615</id>
        <link href="http://arxiv.org/abs/2106.08615"/>
        <updated>2021-06-17T01:58:44.910Z</updated>
        <summary type="html"><![CDATA[Monocular depth estimation is an especially important task in robotics and
autonomous driving, where 3D structural information is essential. However,
extreme lighting conditions and complex surface objects make it difficult to
predict depth in a single image. Therefore, to generate accurate depth maps, it
is important for the model to learn structural information about the scene. We
propose a novel Patch-Wise EdgeConv Module (PEM) and EdgeConv Attention Module
(EAM) to solve the difficulty of monocular depth estimation. The proposed
modules extract structural information by learning the relationship between
image patches close to each other in space using edge convolution. Our method
is evaluated on two popular datasets, the NYU Depth V2 and the KITTI Eigen
split, achieving state-of-the-art performance. We prove that the proposed model
predicts depth robustly in challenging scenes through various comparative
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Minhyeok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sangwon Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chaewon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangyoun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational System Identification for Nonlinear State-Space Models. (arXiv:2012.05072v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05072</id>
        <link href="http://arxiv.org/abs/2012.05072"/>
        <updated>2021-06-17T01:58:44.902Z</updated>
        <summary type="html"><![CDATA[This paper considers parameter estimation for nonlinear state-space models,
which is an important but challenging problem. We address this challenge by
employing a variational inference (VI) approach, which is a principled method
that has deep connections to maximum likelihood estimation. This VI approach
ultimately provides estimates of the model as solutions to an optimisation
problem, which is deterministic, tractable and can be solved using standard
optimisation tools. A specialisation of this approach for systems with additive
Gaussian noise is also detailed. The proposed method is examined numerically on
a range of simulated and real examples focusing on the robustness to parameter
initialisation; additionally, favourable comparisons are performed against
state-of-the-art alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Courts_J/0/1/0/all/0/1"&gt;Jarrad Courts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wills_A/0/1/0/all/0/1"&gt;Adrian Wills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schon_T/0/1/0/all/0/1"&gt;Thomas Sch&amp;#xf6;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ninness_B/0/1/0/all/0/1"&gt;Brett Ninness&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hash Layers For Large Sparse Models. (arXiv:2106.04426v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04426</id>
        <link href="http://arxiv.org/abs/2106.04426"/>
        <updated>2021-06-17T01:58:44.889Z</updated>
        <summary type="html"><![CDATA[We investigate the training of sparse layers that use different parameters
for different inputs based on hashing in large Transformer models.
Specifically, we modify the feedforward layer to hash to different sets of
weights depending on the current token, over all tokens in the sequence. We
show that this procedure either outperforms or is competitive with
learning-to-route mixture-of-expert methods such as Switch Transformers and
BASE Layers, while requiring no routing parameters or extra terms in the
objective function such as a load balancing loss, and no sophisticated
assignment algorithm. We study the performance of different hashing techniques,
hash sizes and input features, and show that balanced and random hashes focused
on the most local features work best, compared to either learning clusters or
using longer-range context. We show our approach works well both on large
language modeling and dialogue tasks, and on downstream fine-tuning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Disentanglement for Rare Event Modeling. (arXiv:2009.08541v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08541</id>
        <link href="http://arxiv.org/abs/2009.08541"/>
        <updated>2021-06-17T01:58:44.852Z</updated>
        <summary type="html"><![CDATA[Combining the increasing availability and abundance of healthcare data and
the current advances in machine learning methods have created renewed
opportunities to improve clinical decision support systems. However, in
healthcare risk prediction applications, the proportion of cases with the
condition (label) of interest is often very low relative to the available
sample size. Though very prevalent in healthcare, such imbalanced
classification settings are also common and challenging in many other
scenarios. So motivated, we propose a variational disentanglement approach to
semi-parametrically learn from rare events in heavily imbalanced classification
problems. Specifically, we leverage the imposed extreme-distribution behavior
on a latent space to extract information from low-prevalence events, and
develop a robust prediction arm that joins the merits of the generalized
additive model and isotonic neural nets. Results on synthetic studies and
diverse real-world datasets, including mortality prediction on a COVID-19
cohort, demonstrate that the proposed approach outperforms existing
alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Xiu_Z/0/1/0/all/0/1"&gt;Zidi Xiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chenyang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gao_M/0/1/0/all/0/1"&gt;Michael Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Davis_C/0/1/0/all/0/1"&gt;Connor Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Goldstein_B/0/1/0/all/0/1"&gt;Benjamin A. Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Henao_R/0/1/0/all/0/1"&gt;Ricardo Henao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Causal Semantic Representation for Out-of-Distribution Prediction. (arXiv:2011.01681v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.01681</id>
        <link href="http://arxiv.org/abs/2011.01681"/>
        <updated>2021-06-17T01:58:44.840Z</updated>
        <summary type="html"><![CDATA[Conventional supervised learning methods, especially deep ones, are found to
be sensitive to out-of-distribution (OOD) examples, largely because the learned
representation mixes the semantic factor with the variation factor due to their
domain-specific correlation, while only the semantic factor causes the output.
To address the problem, we propose a Causal Semantic Generative model (CSG)
based on a causal reasoning so that the two factors are modeled separately, and
develop methods for OOD prediction from a single training domain, which is
common and challenging. The methods are based on the causal invariance
principle, with a novel design for both efficient learning and easy prediction.
Theoretically, we prove that under certain conditions, CSG can identify the
semantic factor by fitting training data, and this semantic-identification
guarantees the boundedness of OOD generalization error and the success of
adaptation. Empirical study shows improved OOD performance over prevailing
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xinwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haoyue Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_T/0/1/0/all/0/1"&gt;Tao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Complexity aspects of local minima and related notions. (arXiv:2008.06148v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.06148</id>
        <link href="http://arxiv.org/abs/2008.06148"/>
        <updated>2021-06-17T01:58:44.823Z</updated>
        <summary type="html"><![CDATA[We consider the notions of (i) critical points, (ii) second-order points,
(iii) local minima, and (iv) strict local minima for multivariate polynomials.
For each type of point, and as a function of the degree of the polynomial, we
study the complexity of deciding (1) if a given point is of that type, and (2)
if a polynomial has a point of that type. Our results characterize the
complexity of these two questions for all degrees left open by prior
literature. Our main contributions reveal that many of these questions turn out
to be tractable for cubic polynomials. In particular, we present an
efficiently-checkable necessary and sufficient condition for local minimality
of a point for a cubic polynomial. We also show that a local minimum of a cubic
polynomial can be efficiently found by solving semidefinite programs of size
linear in the number of variables. By contrast, we show that it is strongly
NP-hard to decide if a cubic polynomial has a critical point. We also prove
that the set of second-order points of any cubic polynomial is a spectrahedron,
and conversely that any spectrahedron is the projection of the set of
second-order points of a cubic polynomial. In our final section, we briefly
present a potential application of finding local minima of cubic polynomials to
the design of a third-order Newton method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ahmadi_A/0/1/0/all/0/1"&gt;Amir Ali Ahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jeffrey Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Variational Attention Models for Language Generation. (arXiv:2004.09764v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09764</id>
        <link href="http://arxiv.org/abs/2004.09764"/>
        <updated>2021-06-17T01:58:44.816Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders have been widely applied for natural language
generation, however, there are two long-standing problems: information
under-representation and posterior collapse. The former arises from the fact
that only the last hidden state from the encoder is transformed to the latent
space, which is insufficient to summarize data. The latter comes as a result of
the imbalanced scale between the reconstruction loss and the KL divergence in
the objective function. To tackle these issues, in this paper we propose the
discrete variational attention model with categorical distribution over the
attention mechanism owing to the discrete nature in languages. Our approach is
combined with an auto-regressive prior to capture the sequential dependency
from observations, which can enhance the latent space for language generation.
Moreover, thanks to the property of discreteness, the training of our proposed
approach does not suffer from posterior collapse. Furthermore, we carefully
analyze the superiority of discrete latent space over the continuous space with
the common Gaussian distribution. Extensive experiments on language generation
demonstrate superior advantages of our proposed approach in comparison with the
state-of-the-art counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Development of Quantized DNN Library for Exact Hardware Emulation. (arXiv:2106.08892v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08892</id>
        <link href="http://arxiv.org/abs/2106.08892"/>
        <updated>2021-06-17T01:58:44.802Z</updated>
        <summary type="html"><![CDATA[Quantization is used to speed up execution time and save power when runnning
Deep neural networks (DNNs) on edge devices like AI chips. To investigate the
effect of quantization, we need performing inference after quantizing the
weights of DNN with 32-bit floating-point precision by a some bit width, and
then quantizing them back to 32-bit floating-point precision. This is because
the DNN library can only handle floating-point numbers. However, the accuracy
of the emulation does not provide accurate precision. We need accurate
precision to detect overflow in MAC operations or to verify the operation on
edge de vices. We have developed PyParch, a DNN library that executes quantized
DNNs (QNNs) with exactly the same be havior as hardware. In this paper, we
describe a new proposal and implementation of PyParch. As a result of the
evaluation, the accuracy of QNNs with arbitrary bit widths can be estimated for
la rge and complex DNNs such as YOLOv5, and the overflow can be detected. We
evaluated the overhead of the emulation time and found that it was 5.6 times
slower for QNN and 42

times slower for QNN with overflow detection compared to the normal DNN
execution time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kiyama_M/0/1/0/all/0/1"&gt;Masato Kiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amagasaki_M/0/1/0/all/0/1"&gt;Motoki Amagasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iida_M/0/1/0/all/0/1"&gt;Masahiro Iida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reset-Free Lifelong Learning with Skill-Space Planning. (arXiv:2012.03548v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03548</id>
        <link href="http://arxiv.org/abs/2012.03548"/>
        <updated>2021-06-17T01:58:44.778Z</updated>
        <summary type="html"><![CDATA[The objective of lifelong reinforcement learning (RL) is to optimize agents
which can continuously adapt and interact in changing environments. However,
current RL approaches fail drastically when environments are non-stationary and
interactions are non-episodic. We propose Lifelong Skill Planning (LiSP), an
algorithmic framework for non-episodic lifelong RL based on planning in an
abstract space of higher-order skills. We learn the skills in an unsupervised
manner using intrinsic rewards and plan over the learned skills using a learned
dynamics model. Moreover, our framework permits skill discovery even from
offline data, thereby reducing the need for excessive real-world interactions.
We demonstrate empirically that LiSP successfully enables long-horizon planning
and learns agents that can avoid catastrophic failures even in challenging
non-stationary and non-episodic environments derived from gridworld and MuJoCo
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Kevin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1"&gt;Aditya Grover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1"&gt;Igor Mordatch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GemNet: Universal Directional Graph Neural Networks for Molecules. (arXiv:2106.08903v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2106.08903</id>
        <link href="http://arxiv.org/abs/2106.08903"/>
        <updated>2021-06-17T01:58:44.766Z</updated>
        <summary type="html"><![CDATA[Effectively predicting molecular interactions has the potential to accelerate
molecular dynamics by multiple orders of magnitude and thus revolutionize
chemical simulations. Graph neural networks (GNNs) have recently shown great
successes for this task, overtaking classical methods based on fixed molecular
kernels. However, they still appear very limited from a theoretical
perspective, since regular GNNs cannot distinguish certain types of graphs. In
this work we close this gap between theory and practice. We show that GNNs with
directed edge embeddings and two-hop message passing are indeed universal
approximators for predictions that are invariant to global rotation and
translation, and equivariant to permutation. We then leverage these insights
and multiple structural improvements to propose the geometric message passing
neural network (GemNet). We demonstrate the benefits of the proposed changes in
multiple ablation studies. GemNet outperforms previous models on the COLL and
MD17 molecular dynamics datasets by 36%, performing especially well on the most
challenging molecules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Klicpera_J/0/1/0/all/0/1"&gt;Johannes Klicpera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Becker_F/0/1/0/all/0/1"&gt;Florian Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Generative Adversarial Networks in One Stage. (arXiv:2103.00430v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00430</id>
        <link href="http://arxiv.org/abs/2103.00430"/>
        <updated>2021-06-17T01:58:44.759Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have demonstrated unprecedented
success in various image generation tasks. The encouraging results, however,
come at the price of a cumbersome training process, during which the generator
and discriminator are alternately updated in two stages. In this paper, we
investigate a general training scheme that enables training GANs efficiently in
only one stage. Based on the adversarial losses of the generator and
discriminator, we categorize GANs into two classes, Symmetric GANs and
Asymmetric GANs, and introduce a novel gradient decomposition method to unify
the two, allowing us to train both classes in one stage and hence alleviate the
training effort. We also computationally analyze the efficiency of the proposed
method, and empirically demonstrate that, the proposed method yields a solid
$1.5\times$ acceleration across various datasets and network architectures.
Furthermore, we show that the proposed method is readily applicable to other
adversarial-training scenarios, such as data-free knowledge distillation. The
code is available at https://github.com/zju-vipa/OSGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chengchao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Youtan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xubin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jie Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Mingli Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularized Orthogonal Machine Learning for Nonlinear Semiparametric Models. (arXiv:1806.04823v7 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1806.04823</id>
        <link href="http://arxiv.org/abs/1806.04823"/>
        <updated>2021-06-17T01:58:44.719Z</updated>
        <summary type="html"><![CDATA[This paper proposes a Lasso-type estimator for a high-dimensional sparse
parameter identified by a single index conditional moment restriction (CMR). In
addition to this parameter, the moment function can also depend on a nuisance
function, such as the propensity score or the conditional choice probability,
which we estimate by modern machine learning tools. We first adjust the moment
function so that the gradient of the future loss function is insensitive
(formally, Neyman-orthogonal) with respect to the first-stage regularization
bias, preserving the single index property. We then take the loss function to
be an indefinite integral of the adjusted moment function with respect to the
single index. The proposed Lasso estimator converges at the oracle rate, where
the oracle knows the nuisance function and solves only the parametric problem.
We demonstrate our method by estimating the short-term heterogeneous impact of
Connecticut's Jobs First welfare reform experiment on women's welfare
participation decision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Nekipelov_D/0/1/0/all/0/1"&gt;Denis Nekipelov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Semenova_V/0/1/0/all/0/1"&gt;Vira Semenova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Syrgkanis_V/0/1/0/all/0/1"&gt;Vasilis Syrgkanis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Bellman Operators. (arXiv:2106.05012v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05012</id>
        <link href="http://arxiv.org/abs/2106.05012"/>
        <updated>2021-06-17T01:58:44.690Z</updated>
        <summary type="html"><![CDATA[We introduce a novel perspective on Bayesian reinforcement learning (RL);
whereas existing approaches infer a posterior over the transition distribution
or Q-function, we characterise the uncertainty in the Bellman operator. Our
Bayesian Bellman operator (BBO) framework is motivated by the insight that when
bootstrapping is introduced, model-free approaches actually infer a posterior
over Bellman operators, not value functions. In this paper, we use BBO to
provide a rigorous theoretical analysis of model-free Bayesian RL to better
understand its relationshipto established frequentist RL methodologies. We
prove that Bayesian solutions are consistent with frequentist RL solutions,
even when approximate inference isused, and derive conditions for which
convergence properties hold. Empirically, we demonstrate that algorithms
derived from the BBO framework have sophisticated deep exploration properties
that enable them to solve continuous control tasks at which state-of-the-art
regularised actor-critic algorithms fail catastrophically]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fellows_M/0/1/0/all/0/1"&gt;Matthew Fellows&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartikainen_K/0/1/0/all/0/1"&gt;Kristian Hartikainen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1"&gt;Shimon Whiteson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Communication. (arXiv:1909.05350v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.05350</id>
        <link href="http://arxiv.org/abs/1909.05350"/>
        <updated>2021-06-17T01:58:44.681Z</updated>
        <summary type="html"><![CDATA[We analyze (stochastic) gradient descent (SGD) with delayed updates on smooth
quasi-convex and non-convex functions and derive concise, non-asymptotic,
convergence rates. We show that the rate of convergence in all cases consists
of two terms: (i) a stochastic term which is not affected by the delay, and
(ii) a higher order deterministic term which is only linearly slowed down by
the delay. Thus, in the presence of noise, the effects of the delay become
negligible after a few iterations and the algorithm converges at the same
optimal rate as standard SGD. This result extends a line of research that
showed similar results in the asymptotic regime or for strongly-convex
quadratic functions only. We further show similar results for SGD with more
intricate form of delayed gradients---compressed gradients under error
compensation and for local~SGD where multiple workers perform local steps
before communicating with each other. In all of these settings, we improve upon
the best known rates. These results show that SGD is robust to compressed
and/or delayed stochastic gradient updates. This is in particular important for
distributed parallel implementations, where asynchronous and communication
efficient methods are the key to achieve linear speedups for optimization with
multiple devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1"&gt;Sai Praneeth Karimireddy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better. (arXiv:2106.08962v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08962</id>
        <link href="http://arxiv.org/abs/2106.08962"/>
        <updated>2021-06-17T01:58:44.674Z</updated>
        <summary type="html"><![CDATA[Deep Learning has revolutionized the fields of computer vision, natural
language understanding, speech recognition, information retrieval and more.
However, with the progressive improvements in deep learning models, their
number of parameters, latency, resources required to train, etc. have all have
increased significantly. Consequently, it has become important to pay attention
to these footprint metrics of a model as well, not just its quality. We present
and motivate the problem of efficiency in deep learning, followed by a thorough
survey of the five core areas of model efficiency (spanning modeling
techniques, infrastructure, and hardware) and the seminal work there. We also
present an experiment-based guide along with code, for practitioners to
optimize their model training and deployment. We believe this is the first
comprehensive survey in the efficient deep learning space that covers the
landscape of model efficiency from modeling techniques to hardware support. Our
hope is that this survey would provide the reader with the mental model and the
necessary understanding of the field to apply generic efficiency techniques to
immediately get significant improvements, and also equip them with ideas for
further research and experimentation to achieve additional gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Menghani_G/0/1/0/all/0/1"&gt;Gaurav Menghani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving filling level classification with adversarial training. (arXiv:2102.04057v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04057</id>
        <link href="http://arxiv.org/abs/2102.04057"/>
        <updated>2021-06-17T01:58:44.599Z</updated>
        <summary type="html"><![CDATA[We investigate the problem of classifying - from a single image - the level
of content in a cup or a drinking glass. This problem is made challenging by
several ambiguities caused by transparencies, shape variations and partial
occlusions, and by the availability of only small training datasets. In this
paper, we tackle this problem with an appropriate strategy for transfer
learning. Specifically, we use adversarial training in a generic source dataset
and then refine the training with a task-specific dataset. We also discuss and
experimentally evaluate several training strategies and their combination on a
range of container types of the CORSMAL Containers Manipulation dataset. We
show that transfer learning with adversarial training in the source domain
consistently improves the classification accuracy on the test set and limits
the overfitting of the classifier to specific features of the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Modas_A/0/1/0/all/0/1"&gt;Apostolos Modas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1"&gt;Alessio Xompero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Matilla_R/0/1/0/all/0/1"&gt;Ricardo Sanchez-Matilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1"&gt;Pascal Frossard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1"&gt;Andrea Cavallaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Sample Complexities for Deep Networks and Robust Classification via an All-Layer Margin. (arXiv:1910.04284v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.04284</id>
        <link href="http://arxiv.org/abs/1910.04284"/>
        <updated>2021-06-17T01:58:44.517Z</updated>
        <summary type="html"><![CDATA[For linear classifiers, the relationship between (normalized) output margin
and generalization is captured in a clear and simple bound -- a large output
margin implies good generalization. Unfortunately, for deep models, this
relationship is less clear: existing analyses of the output margin give
complicated bounds which sometimes depend exponentially on depth. In this work,
we propose to instead analyze a new notion of margin, which we call the
"all-layer margin." Our analysis reveals that the all-layer margin has a clear
and direct relationship with generalization for deep models. This enables the
following concrete applications of the all-layer margin: 1) by analyzing the
all-layer margin, we obtain tighter generalization bounds for neural nets which
depend on Jacobian and hidden layer norms and remove the exponential dependency
on depth 2) our neural net results easily translate to the adversarially robust
setting, giving the first direct analysis of robust test error for deep
networks, and 3) we present a theoretically inspired training algorithm for
increasing the all-layer margin. Our algorithm improves both clean and
adversarially robust test performance over strong baselines in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Colin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent. (arXiv:2009.04709v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04709</id>
        <link href="http://arxiv.org/abs/2009.04709"/>
        <updated>2021-06-17T01:58:44.507Z</updated>
        <summary type="html"><![CDATA[Adversarial training, especially projected gradient descent (PGD), has been a
successful approach for improving robustness against adversarial attacks. After
adversarial training, gradients of models with respect to their inputs have a
preferential direction. However, the direction of alignment is not
mathematically well established, making it difficult to evaluate
quantitatively. We propose a novel definition of this direction as the
direction of the vector pointing toward the closest point of the support of the
closest inaccurate class in decision space. To evaluate the alignment with this
direction after adversarial training, we apply a metric that uses generative
adversarial networks to produce the smallest residual needed to change the
class present in the image. We show that PGD-trained models have a higher
alignment than the baseline according to our definition, that our metric
presents higher alignment values than a competing metric formulation, and that
enforcing this alignment increases the robustness of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lanfredi_R/0/1/0/all/0/1"&gt;Ricardo Bigolin Lanfredi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schroeder_J/0/1/0/all/0/1"&gt;Joyce D. Schroeder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tasdizen_T/0/1/0/all/0/1"&gt;Tolga Tasdizen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimality of short-term synaptic plasticity in modelling certain dynamic environments. (arXiv:2009.06808v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06808</id>
        <link href="http://arxiv.org/abs/2009.06808"/>
        <updated>2021-06-17T01:58:44.499Z</updated>
        <summary type="html"><![CDATA[Biological neurons and their in-silico emulations for neuromorphic artificial
intelligence (AI) use extraordinarily energy-efficient mechanisms, such as
spike-based communication and local synaptic plasticity. It remains unclear
whether these neuronal mechanisms only offer efficiency or also underlie the
superiority of biological intelligence. Here, we prove rigorously that, indeed,
the Bayes-optimal prediction and inference of randomly but continuously
transforming environments, a common natural setting, relies on short-term
spike-timing-dependent plasticity, a hallmark of biological synapses. Further,
this dynamic Bayesian inference through plasticity enables circuits of the
cerebral cortex in simulations to recognize previously unseen, highly distorted
dynamic stimuli. Strikingly, this also introduces a biologically-modelled AI,
the first to overcome multiple limitations of deep learning and outperform
artificial neural networks in a visual task. The cortical-like network is
spiking and event-based, trained only with unsupervised and local plasticity,
on a small, narrow, and static training dataset, but achieves recognition of
unseen, transformed, and dynamic data better than deep neural networks with
continuous activations, trained with supervised backpropagation on the
transforming data. These results link short-term plasticity to high-level
cortical function, suggest optimality of natural intelligence for natural
environments, and repurpose neuromorphic AI from mere efficiency to
computational supremacy altogether.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1"&gt;Timoleon Moraitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1"&gt;Abu Sebastian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eleftheriou_E/0/1/0/all/0/1"&gt;Evangelos Eleftheriou&lt;/a&gt; (IBM Research - Zurich)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ditto: Fair and Robust Federated Learning Through Personalization. (arXiv:2012.04221v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04221</id>
        <link href="http://arxiv.org/abs/2012.04221"/>
        <updated>2021-06-17T01:58:44.492Z</updated>
        <summary type="html"><![CDATA[Fairness and robustness are two important concerns for federated learning
systems. In this work, we identify that robustness to data and model poisoning
attacks and fairness, measured as the uniformity of performance across devices,
are competing constraints in statistically heterogeneous networks. To address
these constraints, we propose employing a simple, general framework for
personalized federated learning, Ditto, that can inherently provide fairness
and robustness benefits, and develop a scalable solver for it. Theoretically,
we analyze the ability of Ditto to achieve fairness and robustness
simultaneously on a class of linear problems. Empirically, across a suite of
federated datasets, we show that Ditto not only achieves competitive
performance relative to recent personalization methods, but also enables more
accurate, robust, and fair models relative to state-of-the-art fair or robust
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shengyuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1"&gt;Ahmad Beirami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1"&gt;Virginia Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation. (arXiv:2106.09016v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09016</id>
        <link href="http://arxiv.org/abs/2106.09016"/>
        <updated>2021-06-17T01:58:44.486Z</updated>
        <summary type="html"><![CDATA[Image-to-Image (I2I) multi-domain translation models are usually evaluated
also using the quality of their semantic interpolation results. However,
state-of-the-art models frequently show abrupt changes in the image appearance
during interpolation, and usually perform poorly in interpolations across
domains. In this paper, we propose a new training protocol based on three
specific losses which help a translation network to learn a smooth and
disentangled latent style space in which: 1) Both intra- and inter-domain
interpolations correspond to gradual changes in the generated images and 2) The
content of the source image is better preserved during the translation.
Moreover, we propose a novel evaluation metric to properly measure the
smoothness of latent style space of I2I translation models. The proposed method
can be plugged into existing translation approaches, and our extensive
experiments on different datasets show that it can significantly boost the
quality of the generated images and the graduality of the interpolations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yahui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1"&gt;Enver Sangineto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yajing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1"&gt;Linchao Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoxian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1"&gt;Bruno Lepri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadai_M/0/1/0/all/0/1"&gt;Marco De Nadai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KALE Flow: A Relaxed KL Gradient Flow for Probabilities with Disjoint Support. (arXiv:2106.08929v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08929</id>
        <link href="http://arxiv.org/abs/2106.08929"/>
        <updated>2021-06-17T01:58:44.471Z</updated>
        <summary type="html"><![CDATA[We study the gradient flow for a relaxed approximation to the
Kullback-Leibler (KL) divergence between a moving source and a fixed target
distribution. This approximation, termed the KALE (KL approximate lower-bound
estimator), solves a regularized version of the Fenchel dual problem defining
the KL over a restricted class of functions. When using a Reproducing Kernel
Hilbert Space (RKHS) to define the function class, we show that the KALE
continuously interpolates between the KL and the Maximum Mean Discrepancy
(MMD). Like the MMD and other Integral Probability Metrics, the KALE remains
well defined for mutually singular distributions. Nonetheless, the KALE
inherits from the limiting KL a greater sensitivity to mismatch in the support
of the distributions, compared with the MMD. These two properties make the KALE
gradient flow particularly well suited when the target distribution is
supported on a low-dimensional manifold. Under an assumption of sufficient
smoothness of the trajectories, we show the global convergence of the KALE
flow. We propose a particle implementation of the flow given initial samples
from the source and the target distribution, which we use to empirically
confirm the KALE's properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Glaser_P/0/1/0/all/0/1"&gt;Pierre Glaser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Arbel_M/0/1/0/all/0/1"&gt;Michael Arbel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Tertiary Protein Structures via an Interpretative Variational Autoencoder. (arXiv:2004.07119v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07119</id>
        <link href="http://arxiv.org/abs/2004.07119"/>
        <updated>2021-06-17T01:58:44.465Z</updated>
        <summary type="html"><![CDATA[Much scientific enquiry across disciplines is founded upon a mechanistic
treatment of dynamic systems that ties form to function. A highly visible
instance of this is in molecular biology, where an important goal is to
determine functionally-relevant forms/structures that a protein molecule
employs to interact with molecular partners in the living cell. This goal is
typically pursued under the umbrella of stochastic optimization with algorithms
that optimize a scoring function. Research repeatedly shows that current
scoring function, though steadily improving, correlate weakly with molecular
activity. Inspired by recent momentum in generative deep learning, this paper
proposes and evaluates an alternative approach to generating
functionally-relevant three-dimensional structures of a protein. Though
typically deep generative models struggle with highly-structured data, the work
presented here circumvents this challenge via graph-generative models. A
comprehensive evaluation of several deep architectures shows the promise of
generative models in directly revealing the latent space for sampling novel
tertiary structures, as well as in highlighting axes/factors that carry
structural meaning and open the black box often associated with deep models.
The work presented here is a first step towards interpretative, deep generative
models becoming viable and informative complementary approaches to protein
structure prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaojie Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuanqi Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Tadepalli_S/0/1/0/all/0/1"&gt;Sivani Tadepalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Shehu_A/0/1/0/all/0/1"&gt;Amarda Shehu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting chaos in lineage-trees: A deep learning approach. (arXiv:2106.08956v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08956</id>
        <link href="http://arxiv.org/abs/2106.08956"/>
        <updated>2021-06-17T01:58:44.458Z</updated>
        <summary type="html"><![CDATA[Many complex phenomena, from weather systems to heartbeat rhythm patterns,
are effectively modeled as low-dimensional dynamical systems. Such systems may
behave chaotically under certain conditions, and so the ability to detect chaos
based on empirical measurement is an important step in characterizing and
predicting these processes. Classifying a system as chaotic usually requires
estimating its largest Lyapunov exponent, which quantifies the average rate of
convergence or divergence of initially close trajectories in state space, and
for which a positive value is generally accepted as an operational definition
of chaos. Estimating the largest Lyapunov exponent from observations of a
process is especially challenging in systems affected by dynamical noise, which
is the case for many models of real-world processes, in particular models of
biological systems. We describe a novel method for estimating the largest
Lyapunov exponent from data, based on training Deep Learning models on
synthetically generated trajectories, and demonstrate that this method yields
accurate and noise-robust predictions given relatively short inputs and across
a range of different dynamical systems. Our method is unique in that it can
analyze tree-shaped data, a ubiquitous topology in biological settings, and
specifically in dynamics over lineages of cells or organisms. We also
characterize the types of input information extracted by our models for their
predictions, allowing for a deeper understanding into the different ways by
which chaos can be analyzed in different topologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rappeport_H/0/1/0/all/0/1"&gt;Hagai Rappeport&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reisman_I/0/1/0/all/0/1"&gt;Irit Levin Reisman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tishby_N/0/1/0/all/0/1"&gt;Naftali Tishby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balaban_N/0/1/0/all/0/1"&gt;Nathalie Q. Balaban&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization. (arXiv:2102.01670v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01670</id>
        <link href="http://arxiv.org/abs/2102.01670"/>
        <updated>2021-06-17T01:58:44.439Z</updated>
        <summary type="html"><![CDATA[Training sparse networks to converge to the same performance as dense neural
architectures has proven to be elusive. Recent work suggests that
initialization is the key. However, while this direction of research has had
some success, focusing on initialization alone appears to be inadequate. In
this paper, we take a broader view of training sparse networks and consider the
role of regularization, optimization, and architecture choices on sparse
models. We propose a simple experimental framework, Same Capacity Sparse vs
Dense Comparison (SC-SDC), that allows for a fair comparison of sparse and
dense networks. Furthermore, we propose a new measure of gradient flow,
Effective Gradient Flow (EGF), that better correlates to performance in sparse
networks. Using top-line metrics, SC-SDC and EGF, we show that default choices
of optimizers, activation functions and regularizers used for dense networks
can disadvantage sparse networks. Based upon these findings, we show that
gradient flow in sparse networks can be improved by reconsidering aspects of
the architecture design and the training regime. Our work suggests that
initialization is only one piece of the puzzle and taking a wider view of
tailoring optimization to sparse networks yields promising results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1"&gt;Kale-ab Tessera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1"&gt;Sara Hooker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosman_B/0/1/0/all/0/1"&gt;Benjamin Rosman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LieTransformer: Equivariant self-attention for Lie Groups. (arXiv:2012.10885v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10885</id>
        <link href="http://arxiv.org/abs/2012.10885"/>
        <updated>2021-06-17T01:58:44.433Z</updated>
        <summary type="html"><![CDATA[Group equivariant neural networks are used as building blocks of group
invariant neural networks, which have been shown to improve generalisation
performance and data efficiency through principled parameter sharing. Such
works have mostly focused on group equivariant convolutions, building on the
result that group equivariant linear maps are necessarily convolutions. In this
work, we extend the scope of the literature to self-attention, that is emerging
as a prominent building block of deep learning models. We propose the
LieTransformer, an architecture composed of LieSelfAttention layers that are
equivariant to arbitrary Lie groups and their discrete subgroups. We
demonstrate the generality of our approach by showing experimental results that
are competitive to baseline methods on a wide range of tasks: shape counting on
point clouds, molecular property regression and modelling particle trajectories
under Hamiltonian dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hutchinson_M/0/1/0/all/0/1"&gt;Michael Hutchinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Charline Le Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1"&gt;Sheheryar Zaidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dupont_E/0/1/0/all/0/1"&gt;Emilien Dupont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunjik Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Quasi-Bayesian Inference for Instrumental Variable Regression. (arXiv:2106.08750v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08750</id>
        <link href="http://arxiv.org/abs/2106.08750"/>
        <updated>2021-06-17T01:58:44.353Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed an upsurge of interest in employing flexible
machine learning models for instrumental variable (IV) regression, but the
development of uncertainty quantification methodology is still lacking. In this
work we present a scalable quasi-Bayesian procedure for IV regression, building
upon the recently developed kernelized IV models. Contrary to Bayesian modeling
for IV, our approach does not require additional assumptions on the data
generating process, and leads to a scalable approximate inference algorithm
with time cost comparable to the corresponding point estimation methods. Our
algorithm can be further extended to work with neural network models. We
analyze the theoretical properties of the proposed quasi-posterior, and
demonstrate through empirical evaluation the competitive performance of our
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ren_T/0/1/0/all/0/1"&gt;Tongzheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Banker Online Mirror Descent. (arXiv:2106.08943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08943</id>
        <link href="http://arxiv.org/abs/2106.08943"/>
        <updated>2021-06-17T01:58:44.340Z</updated>
        <summary type="html"><![CDATA[We propose Banker-OMD, a novel framework generalizing the classical Online
Mirror Descent (OMD) technique in online learning algorithm design. Banker-OMD
allows algorithms to robustly handle delayed feedback, and offers a general
methodology for achieving $\tilde{O}(\sqrt{T} + \sqrt{D})$-style regret bounds
in various delayed-feedback online learning tasks, where $T$ is the time
horizon length and $D$ is the total feedback delay. We demonstrate the power of
Banker-OMD with applications to three important bandit scenarios with delayed
feedback, including delayed adversarial Multi-armed bandits (MAB), delayed
adversarial linear bandits, and a novel delayed best-of-both-worlds MAB
setting. Banker-OMD achieves nearly-optimal performance in all the three
settings. In particular, it leads to the first delayed adversarial linear
bandit algorithm achieving $\tilde{O}(\text{poly}(n)(\sqrt{T} + \sqrt{D}))$
regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiatai Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longbo Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry of Similarity Comparisons. (arXiv:2006.09858v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09858</id>
        <link href="http://arxiv.org/abs/2006.09858"/>
        <updated>2021-06-17T01:58:44.335Z</updated>
        <summary type="html"><![CDATA[Many data analysis problems can be cast as distance geometry problems in
\emph{space forms} -- Euclidean, spherical, or hyperbolic spaces. Often,
absolute distance measurements are often unreliable or simply unavailable and
only proxies to absolute distances in the form of similarities are available.
Hence we ask the following: Given only \emph{comparisons} of similarities
amongst a set of entities, what can be said about the geometry of the
underlying space form? To study this question, we introduce the notions of the
\textit{ordinal capacity} of a target space form and \emph{ordinal spread} of
the similarity measurements. The latter is an indicator of complex patterns in
the measurements, while the former quantifies the capacity of a space form to
accommodate a set of measurements with a specific ordinal spread profile. We
prove that the ordinal capacity of a space form is related to its dimension and
the sign of its curvature. This leads to a lower bound on the Euclidean and
spherical embedding dimension of what we term similarity graphs. More
importantly, we show that the statistical behavior of the ordinal spread random
variables defined on a similarity graph can be used to identify its underlying
space form. We support our theoretical claims with experiments on weighted
trees, single-cell RNA expression data and spherical cartographic measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1"&gt;Puoya Tabaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jianhao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1"&gt;Olgica Milenkovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1"&gt;Ivan Dokmani&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08977</id>
        <link href="http://arxiv.org/abs/2106.08977"/>
        <updated>2021-06-17T01:58:44.321Z</updated>
        <summary type="html"><![CDATA[Weak supervision has shown promising results in many natural language
processing tasks, such as Named Entity Recognition (NER). Existing work mainly
focuses on learning deep NER models only with weak supervision, i.e., without
any human annotation, and shows that by merely using weakly labeled data, one
can achieve good performance, though still underperforms fully supervised NER
with manually/strongly labeled data. In this paper, we consider a more
practical scenario, where we have both a small amount of strongly labeled data
and a large amount of weakly labeled data. Unfortunately, we observe that
weakly labeled data does not necessarily improve, or even deteriorate the model
performance (due to the extensive noise in the weak labels) when we train deep
NER models over a simple or weighted combination of the strongly labeled and
weakly labeled data. To address this issue, we propose a new multi-stage
computational framework -- NEEDLE with three essential ingredients: (1) weak
label completion, (2) noise-aware loss function, and (3) final fine-tuning over
the strongly labeled data. Through experiments on E-commerce query NER and
Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise
of the weak labels and outperforms existing methods. In particular, we achieve
new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,
BC5CDR-disease 90.69, NCBI-disease 92.28.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Danqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tianyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Bing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Costs and Benefits of Wasserstein Fair Regression. (arXiv:2106.08812v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08812</id>
        <link href="http://arxiv.org/abs/2106.08812"/>
        <updated>2021-06-17T01:58:44.313Z</updated>
        <summary type="html"><![CDATA[Real-world applications of machine learning tools in high-stakes domains are
often regulated to be fair, in the sense that the predicted target should
satisfy some quantitative notion of parity with respect to a protected
attribute. However, the exact tradeoff between fairness and accuracy with a
real-valued target is not clear. In this paper, we characterize the inherent
tradeoff between statistical parity and accuracy in the regression setting by
providing a lower bound on the error of any fair regressor. Our lower bound is
sharp, algorithm-independent, and admits a simple interpretation: when the
moments of the target differ between groups, any fair algorithm has to make a
large error on at least one of the groups. We further extend this result to
give a lower bound on the joint error of any (approximately) fair algorithm,
using the Wasserstein distance to measure the quality of the approximation. On
the upside, we establish the first connection between individual fairness,
accuracy parity, and the Wasserstein distance by showing that if a regressor is
individually fair, it also approximately verifies the accuracy parity, where
the gap is given by the Wasserstein distance between the two groups. Inspired
by our theoretical results, we develop a practical algorithm for fair
regression through the lens of representation learning, and conduct experiments
on a real-world dataset to corroborate our findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Circa: Stochastic ReLUs for Private Deep Learning. (arXiv:2106.08475v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08475</id>
        <link href="http://arxiv.org/abs/2106.08475"/>
        <updated>2021-06-17T01:58:44.271Z</updated>
        <summary type="html"><![CDATA[The simultaneous rise of machine learning as a service and concerns over user
privacy have increasingly motivated the need for private inference (PI). While
recent work demonstrates PI is possible using cryptographic primitives, the
computational overheads render it impractical. The community is largely
unprepared to address these overheads, as the source of slowdown in PI stems
from the ReLU operator whereas optimizations for plaintext inference focus on
optimizing FLOPs. In this paper we re-think the ReLU computation and propose
optimizations for PI tailored to properties of neural networks. Specifically,
we reformulate ReLU as an approximate sign test and introduce a novel
truncation method for the sign test that significantly reduces the cost per
ReLU. These optimizations result in a specific type of stochastic ReLU. The key
observation is that the stochastic fault behavior is well suited for the
fault-tolerant properties of neural network inference. Thus, we provide
significant savings without impacting accuracy. We collectively call the
optimizations Circa and demonstrate improvements of up to 4.7x storage and 3x
runtime over baseline implementations; we further show that Circa can be used
on top of recent PI optimizations to obtain 1.8x additional speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghodsi_Z/0/1/0/all/0/1"&gt;Zahra Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1"&gt;Nandan Kumar Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reagen_B/0/1/0/all/0/1"&gt;Brandon Reagen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Siddharth Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automating Augmentation Through Random Unidimensional Search. (arXiv:2106.08756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08756</id>
        <link href="http://arxiv.org/abs/2106.08756"/>
        <updated>2021-06-17T01:58:44.265Z</updated>
        <summary type="html"><![CDATA[It is no secret amongst deep learning researchers that finding the right data
augmentation strategy during training can mean the difference between a
state-of-the-art result and a run-of-the-mill ranking. To that end, the
community has seen many efforts to automate the process of finding the perfect
augmentation procedure for any task at hand. Unfortunately, even recent
cutting-edge methods bring massive computational overhead, requiring as many as
100 full model trainings to settle on an ideal configuration. We show how to
achieve even better performance in just 7: with Random Unidimensional
Augmentation. Source code is available at https://github.com/fastestimator/RUA]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaomeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1"&gt;Michael Potter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1"&gt;Gaurav Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yun-Chan Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saripalli_V/0/1/0/all/0/1"&gt;V. Ratna Saripalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning-based analysis of hyperspectral images for automated sepsis diagnosis. (arXiv:2106.08445v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08445</id>
        <link href="http://arxiv.org/abs/2106.08445"/>
        <updated>2021-06-17T01:58:44.256Z</updated>
        <summary type="html"><![CDATA[Sepsis is a leading cause of mortality and critical illness worldwide. While
robust biomarkers for early diagnosis are still missing, recent work indicates
that hyperspectral imaging (HSI) has the potential to overcome this bottleneck
by monitoring microcirculatory alterations. Automated machine learning-based
diagnosis of sepsis based on HSI data, however, has not been explored to date.
Given this gap in the literature, we leveraged an existing data set to (1)
investigate whether HSI-based automated diagnosis of sepsis is possible and (2)
put forth a list of possible confounders relevant for HSI-based tissue
classification. While we were able to classify sepsis with an accuracy of over
$98\,\%$ using the existing data, our research also revealed several subject-,
therapy- and imaging-related confounders that may lead to an overestimation of
algorithm performance when not balanced across the patient groups. We conclude
that further prospective studies, carefully designed with respect to these
confounders, are necessary to confirm the preliminary results obtained in this
study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dietrich_M/0/1/0/all/0/1"&gt;Maximilian Dietrich&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Seidlitz_S/0/1/0/all/0/1"&gt;Silvia Seidlitz&lt;/a&gt; (2, 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Schreck_N/0/1/0/all/0/1"&gt;Nicholas Schreck&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Wiesenfarth_M/0/1/0/all/0/1"&gt;Manuel Wiesenfarth&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Godau_P/0/1/0/all/0/1"&gt;Patrick Godau&lt;/a&gt; (2, 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1"&gt;Minu Tizabi&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Sellner_J/0/1/0/all/0/1"&gt;Jan Sellner&lt;/a&gt; (2, 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Marx_S/0/1/0/all/0/1"&gt;Sebastian Marx&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Knodler_S/0/1/0/all/0/1"&gt;Samuel Kn&amp;#xf6;dler&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Allers_M/0/1/0/all/0/1"&gt;Michael M. Allers&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Ayala_L/0/1/0/all/0/1"&gt;Leonardo Ayala&lt;/a&gt; (2, 7), &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_K/0/1/0/all/0/1"&gt;Karsten Schmidt&lt;/a&gt; (8), &lt;a href="http://arxiv.org/find/cs/1/au:+Brenner_T/0/1/0/all/0/1"&gt;Thorsten Brenner&lt;/a&gt; (8), &lt;a href="http://arxiv.org/find/cs/1/au:+Studier_Fischer_A/0/1/0/all/0/1"&gt;Alexander Studier-Fischer&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Nickel_F/0/1/0/all/0/1"&gt;Felix Nickel&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1"&gt;Beat P. M&amp;#xfc;ller-Stich&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Kopp_Schneider_A/0/1/0/all/0/1"&gt;Annette Kopp-Schneider&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Weigand_M/0/1/0/all/0/1"&gt;Markus A. Weigand&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt; (2, 6, 7) ((1) Department of Anesthesiology, Heidelberg University Hospital, Heidelberg, Germany, (2) Division of Computer Assisted Medical Interventions, German Cancer Research Center (DKFZ), Heidelberg, Germany, (3) HIDSS4Health - Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany (4) Division of Biostatistics, German Cancer Research Center (DKFZ), Heidelberg, Germany, (5) Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany, (6) Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany, (7) Medical Faculty, Heidelberg University, Heidelberg, Germany, (8) Department of Anesthesiology and Intensive Care Medicine, University Hospital Essen, University Duisburg-Essen, Essen, Germany)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cardiovascular Disease Prediction using Recursive Feature Elimination and Gradient Boosting Classification Techniques. (arXiv:2106.08889v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08889</id>
        <link href="http://arxiv.org/abs/2106.08889"/>
        <updated>2021-06-17T01:58:44.248Z</updated>
        <summary type="html"><![CDATA[Cardiovascular diseases (CVDs) are one of the most common chronic illnesses
that affect peoples health. Early detection of CVDs can reduce mortality rates
by preventing or reducing the severity of the disease. Machine learning
algorithms are a promising method for identifying risk factors. This paper
proposes a proposed recursive feature elimination-based gradient boosting
(RFE-GB) algorithm in order to obtain accurate heart disease prediction. The
patients health record with important CVD features has been analyzed for the
evaluation of the results. Several other machine learning methods were also
used to build the prediction model, and the results were compared with the
proposed model. The results of this proposed model infer that the combined
recursive feature elimination and gradient boosting algorithm achieves the
highest accuracy (89.7 %). Further, with an area under the curve of 0.84, the
proposed RFE-GB algorithm was found superior and had obtained a substantial
gain over other techniques. Thus, the proposed RFE-GB algorithm will serve as a
prominent model for CVD estimation and treatment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Theerthagiri_P/0/1/0/all/0/1"&gt;Prasannavenkatesan Theerthagiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+J_V/0/1/0/all/0/1"&gt;Vidya J&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Continuous Control with Episodic Memory. (arXiv:2106.08832v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08832</id>
        <link href="http://arxiv.org/abs/2106.08832"/>
        <updated>2021-06-17T01:58:44.243Z</updated>
        <summary type="html"><![CDATA[Episodic memory lets reinforcement learning algorithms remember and exploit
promising experience from the past to improve agent performance. Previous works
on memory mechanisms show benefits of using episodic-based data structures for
discrete action problems in terms of sample-efficiency. The application of
episodic memory for continuous control with a large action space is not
trivial. Our study aims to answer the question: can episodic memory be used to
improve agent's performance in continuous control? Our proposed algorithm
combines episodic memory with Actor-Critic architecture by modifying critic's
objective. We further improve performance by introducing episodic-based replay
buffer prioritization. We evaluate our algorithm on OpenAI gym domains and show
greater sample-efficiency compared with the state-of-the art model-free
off-policy algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1"&gt;Igor Kuznetsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1"&gt;Andrey Filchenkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Training in High Dimensions via Block Coordinate Geometric Median Descent. (arXiv:2106.08882v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08882</id>
        <link href="http://arxiv.org/abs/2106.08882"/>
        <updated>2021-06-17T01:58:44.227Z</updated>
        <summary type="html"><![CDATA[Geometric median (\textsc{Gm}) is a classical method in statistics for
achieving a robust estimation of the uncorrupted data; under gross corruption,
it achieves the optimal breakdown point of 0.5. However, its computational
complexity makes it infeasible for robustifying stochastic gradient descent
(SGD) for high-dimensional optimization problems. In this paper, we show that
by applying \textsc{Gm} to only a judiciously chosen block of coordinates at a
time and using a memory mechanism, one can retain the breakdown point of 0.5
for smooth non-convex problems, with non-asymptotic convergence rates
comparable to the SGD with \textsc{Gm}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1"&gt;Anish Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1"&gt;Abolfazl Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1"&gt;Sujay Sanghavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Input Invex Neural Network. (arXiv:2106.08748v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08748</id>
        <link href="http://arxiv.org/abs/2106.08748"/>
        <updated>2021-06-17T01:58:44.221Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel method to constrain invexity on Neural
Networks (NN). Invex functions ensure every stationary point is global minima.
Hence, gradient descent commenced from any point will lead to the global
minima. Another advantage of invexity on NN is to divide data space locally
into two connected sets with a highly non-linear decision boundary by simply
thresholding the output. To this end, we formulate a universal invex function
approximator and employ it to enforce invexity in NN. We call it Input Invex
Neural Networks (II-NN). We first fit data with a known invex function,
followed by modification with a NN, compare the direction of the gradient and
penalize the direction of gradient on NN if it contradicts with the direction
of reference invex function. In order to penalize the direction of the gradient
we perform Gradient Clipped Gradient Penalty (GC-GP). We applied our method to
the existing NNs for both image classification and regression tasks. From the
extensive empirical and qualitative experiments, we observe that our method
gives the performance similar to ordinary NN yet having invexity. Our method
outperforms linear NN and Input Convex Neural Network (ICNN) with a large
margin. We publish our code and implementation details at github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sapkota_S/0/1/0/all/0/1"&gt;Suman Sapkota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1"&gt;Binod Bhattarai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridge Networks. (arXiv:2106.08446v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08446</id>
        <link href="http://arxiv.org/abs/2106.08446"/>
        <updated>2021-06-17T01:58:44.216Z</updated>
        <summary type="html"><![CDATA[Despite rapid progress, current deep learning methods face a number of
critical challenges. These include high energy consumption, catastrophic
forgetting, dependance on global losses, and an inability to reason
symbolically. By combining concepts from information bottleneck theory and
vector-symbolic architectures, we propose and implement a novel information
processing architecture, the 'Bridge network.' We show this architecture
provides unique advantages which can address the problem of global losses and
catastrophic forgetting. Furthermore, we argue that it provides a further basis
for increasing energy efficiency of execution and the ability to reason
symbolically.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olin_Ammentorp_W/0/1/0/all/0/1"&gt;Wilkie Olin-Ammentorp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bazhenov_M/0/1/0/all/0/1"&gt;Maxim Bazhenov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased Methods for Multi-Goal Reinforcement Learning. (arXiv:2106.08863v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08863</id>
        <link href="http://arxiv.org/abs/2106.08863"/>
        <updated>2021-06-17T01:58:44.210Z</updated>
        <summary type="html"><![CDATA[In multi-goal reinforcement learning (RL) settings, the reward for each goal
is sparse, and located in a small neighborhood of the goal. In large dimension,
the probability of reaching a reward vanishes and the agent receives little
learning signal. Methods such as Hindsight Experience Replay (HER) tackle this
issue by also learning from realized but unplanned-for goals. But HER is known
to introduce bias, and can converge to low-return policies by overestimating
chancy outcomes. First, we vindicate HER by proving that it is actually
unbiased in deterministic environments, such as many optimal control settings.
Next, for stochastic environments in continuous spaces, we tackle sparse
rewards by directly taking the infinitely sparse reward limit. We fully
formalize the problem of multi-goal RL with infinitely sparse Dirac rewards at
each goal. We introduce unbiased deep Q-learning and actor-critic algorithms
that can handle such infinitely sparse rewards, and test them in toy
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blier_L/0/1/0/all/0/1"&gt;L&amp;#xe9;onard Blier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ollivier_Y/0/1/0/all/0/1"&gt;Yann Ollivier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous Training of Partially Masked Neural Networks. (arXiv:2106.08895v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08895</id>
        <link href="http://arxiv.org/abs/2106.08895"/>
        <updated>2021-06-17T01:58:44.205Z</updated>
        <summary type="html"><![CDATA[For deploying deep learning models to lower end devices, it is necessary to
train less resource-demanding variants of state-of-the-art architectures. This
does not eliminate the need for more expensive models as they have a higher
performance. In order to avoid training two separate models, we show that it is
possible to train neural networks in such a way that a predefined 'core'
subnetwork can be split-off from the trained full network with remarkable good
performance. We extend on prior methods that focused only on core networks of
smaller width, while we focus on supporting arbitrary core network
architectures. Our proposed training scheme switches consecutively between
optimizing only the core part of the network and the full one. The accuracy of
the full model remains comparable, while the core network achieves better
performance than when it is trained in isolation. In particular, we show that
training a Transformer with a low-rank core gives a low-rank model with
superior performance than when training the low-rank model alone. We analyze
our training scheme theoretically, and show its convergence under assumptions
that are either standard or practically justified. Moreover, we show that the
developed theoretical framework allows analyzing many other partial training
schemes for neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohtashami_A/0/1/0/all/0/1"&gt;Amirkeivan Mohtashami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Markovian Bandits: Is Posterior Sampling more Scalable than Optimism?. (arXiv:2106.08771v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08771</id>
        <link href="http://arxiv.org/abs/2106.08771"/>
        <updated>2021-06-17T01:58:44.190Z</updated>
        <summary type="html"><![CDATA[We study learning algorithms for the classical Markovian bandit problem with
discount. We explain how to adapt PSRL [24] and UCRL2 [2] to exploit the
problem structure. These variants are called MB-PSRL and MB-UCRL2. While the
regret bound and runtime of vanilla implementations of PSRL and UCRL2 are
exponential in the number of bandits, we show that the episodic regret of
MB-PSRL and MB-UCRL2 is�(S $\sqrt$ nK) where K is the number of episodes, n is
the number of bandits and S is the number of states of each bandit (the exact
bound in S, n and K is given in the paper). Up to a factor $\sqrt$ S, this
matches the lower bound of $\Omega$($\sqrt$ SnK) that we also derive in the
paper. MB-PSRL is also computationally efficient: its runtime is linear in the
number of bandits. We further show that this linear runtime cannot be achieved
by adapting classical non-Bayesian algorithms such as UCRL2 or UCBVI to
Markovian bandit problems. Finally, we perform numerical experiments that
confirm that MB-PSRL outperforms other existing algorithms in practice, both in
terms of regret and of computation time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gast_N/0/1/0/all/0/1"&gt;Nicolas Gast&lt;/a&gt; (POLARIS), &lt;a href="http://arxiv.org/find/cs/1/au:+Gaujal_B/0/1/0/all/0/1"&gt;Bruno Gaujal&lt;/a&gt; (POLARIS), &lt;a href="http://arxiv.org/find/cs/1/au:+Khun_K/0/1/0/all/0/1"&gt;Kimang Khun&lt;/a&gt; (POLARIS)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Rhythm Style Transfer Without Text Transcriptions. (arXiv:2106.08519v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.08519</id>
        <link href="http://arxiv.org/abs/2106.08519"/>
        <updated>2021-06-17T01:58:44.183Z</updated>
        <summary type="html"><![CDATA[Prosody plays an important role in characterizing the style of a speaker or
an emotion, but most non-parallel voice or emotion style transfer algorithms do
not convert any prosody information. Two major components of prosody are pitch
and rhythm. Disentangling the prosody information, particularly the rhythm
component, from the speech is challenging because it involves breaking the
synchrony between the input speech and the disentangled speech representation.
As a result, most existing prosody style transfer algorithms would need to rely
on some form of text transcriptions to identify the content information, which
confines their application to high-resource languages only. Recently,
SpeechSplit has made sizeable progress towards unsupervised prosody style
transfer, but it is unable to extract high-level global prosody style in an
unsupervised manner. In this paper, we propose AutoPST, which can disentangle
global prosody style from speech without relying on any text transcriptions.
AutoPST is an Autoencoder-based Prosody Style Transfer framework with a
thorough rhythm removal module guided by the self-expressive representation
learning. Experiments on different style transfer tasks show that AutoPST can
effectively convert prosody that correctly reflects the styles of the target
domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Qian_K/0/1/0/all/0/1"&gt;Kaizhi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jinjun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cox_D/0/1/0/all/0/1"&gt;David Cox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1"&gt;Mark Hasegawa-Johnson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Split and Expand: An inference-time improvement for Weakly Supervised Cell Instance Segmentation. (arXiv:2007.10817v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10817</id>
        <link href="http://arxiv.org/abs/2007.10817"/>
        <updated>2021-06-17T01:58:44.177Z</updated>
        <summary type="html"><![CDATA[We consider the problem of segmenting cell nuclei instances from Hematoxylin
and Eosin (H&E) stains with dot annotations only. While most recent works focus
on improving the segmentation quality, this is usually insufficient for
instance segmentation of cell instances clustered together or with a small
size. In this work, we propose a simple two-step post-processing procedure,
Split and Expand, that directly improves the conversion of segmentation maps to
instances. In the splitting step, we generate fine-grained cell instances from
the segmentation map with the guidance of cell-center predictions. For the
expansion step, we utilize Layer-wise Relevance Propagation (LRP) explanation
results to add small cells that are not captured in the segmentation map.
Although we additionally train an output head to predict cell-centers, the
post-processing procedure itself is not explicitly trained and is executed at
inference-time only. A feature re-weighting loss based on LRP is proposed to
improve our method even further. We test our procedure on the MoNuSeg and TNBC
datasets and show quantitatively and qualitatively that our proposed method
improves object-level metrics substantially.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Foo_L/0/1/0/all/0/1"&gt;Lin Geng Foo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiamei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1"&gt;Alexander Binder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Class Classification from Single-Class Data with Confidences. (arXiv:2106.08864v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08864</id>
        <link href="http://arxiv.org/abs/2106.08864"/>
        <updated>2021-06-17T01:58:44.172Z</updated>
        <summary type="html"><![CDATA[Can we learn a multi-class classifier from only data of a single class? We
show that without any assumptions on the loss functions, models, and
optimizers, we can successfully learn a multi-class classifier from only data
of a single class with a rigorous consistency guarantee when confidences (i.e.,
the class-posterior probabilities for all the classes) are available.
Specifically, we propose an empirical risk minimization framework that is
loss-/model-/optimizer-independent. Instead of constructing a boundary between
the given class and other classes, our method can conduct discriminative
classification between all the classes even if no data from the other classes
are provided. We further theoretically and experimentally show that our method
can be Bayes-consistent with a simple modification even if the provided
confidences are highly noisy. Then, we provide an extension of our method for
the case where data from a subset of all the classes are available.
Experimental results demonstrate the effectiveness of our methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yuzhou Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_S/0/1/0/all/0/1"&gt;Senlin Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yitian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1"&gt;Bo An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the long-term learning ability of LSTM LMs. (arXiv:2106.08927v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08927</id>
        <link href="http://arxiv.org/abs/2106.08927"/>
        <updated>2021-06-17T01:58:44.166Z</updated>
        <summary type="html"><![CDATA[We inspect the long-term learning ability of Long Short-Term Memory language
models (LSTM LMs) by evaluating a contextual extension based on the Continuous
Bag-of-Words (CBOW) model for both sentence- and discourse-level LSTM LMs and
by analyzing its performance. We evaluate on text and speech. Sentence-level
models using the long-term contextual module perform comparably to vanilla
discourse-level LSTM LMs. On the other hand, the extension does not provide
gains for discourse-level models. These findings indicate that discourse-level
LSTM LMs already rely on contextual information to perform long-term learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boes_W/0/1/0/all/0/1"&gt;Wim Boes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rompaey_R/0/1/0/all/0/1"&gt;Robbe Van Rompaey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verwimp_L/0/1/0/all/0/1"&gt;Lyan Verwimp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelemans_J/0/1/0/all/0/1"&gt;Joris Pelemans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+hamme_H/0/1/0/all/0/1"&gt;Hugo Van hamme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wambacq_P/0/1/0/all/0/1"&gt;Patrick Wambacq&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections. (arXiv:2106.08908v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08908</id>
        <link href="http://arxiv.org/abs/2106.08908"/>
        <updated>2021-06-17T01:58:44.150Z</updated>
        <summary type="html"><![CDATA[Question answering (QA) systems for large document collections typically use
pipelines that (i) retrieve possibly relevant documents, (ii) re-rank them,
(iii) rank paragraphs or other snippets of the top-ranked documents, and (iv)
select spans of the top-ranked snippets as exact answers. Pipelines are
conceptually simple, but errors propagate from one component to the next,
without later components being able to revise earlier decisions. We present an
architecture for joint document and snippet ranking, the two middle stages,
which leverages the intuition that relevant documents have good snippets and
good snippets come from relevant documents. The architecture is general and can
be used with any neural text relevance ranker. We experiment with two main
instantiations of the architecture, based on POSIT-DRMM (PDRMM) and a
BERT-based ranker. Experiments on biomedical data from BIOASQ show that our
joint models vastly outperform the pipelines in snippet retrieval, the main
goal for QA, with fewer trainable parameters, also remaining competitive in
document retrieval. Furthermore, our joint PDRMM-based model is competitive
with BERT-based models, despite using orders of magnitude fewer parameters.
These claims are also supported by human evaluation on two test batches of
BIOASQ. To test our key findings on another dataset, we modified the Natural
Questions dataset so that it can also be used for document and snippet
retrieval. Our joint PDRMM-based model again outperforms the corresponding
pipeline in snippet retrieval on the modified Natural Questions dataset, even
though it performs worse than the pipeline in document retrieval. We make our
code and the modified Natural Questions dataset publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pappas_D/0/1/0/all/0/1"&gt;Dimitris Pappas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1"&gt;Ion Androutsopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covariance-based smoothed particle hydrodynamics. A machine-learning application to simulating disc fragmentation. (arXiv:2106.08870v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2106.08870</id>
        <link href="http://arxiv.org/abs/2106.08870"/>
        <updated>2021-06-17T01:58:44.144Z</updated>
        <summary type="html"><![CDATA[A PCA-based, machine learning version of the SPH method is proposed. In the
present scheme, the smoothing tensor is computed to have their eigenvalues
proportional to the covariance's principal components, using a modified octree
data structure, which allows the fast estimation of the anisotropic
self-regulating kNN. Each SPH particle is the center of such an optimal kNN
cluster, i.e., the one whose covariance tensor allows the find of the kNN
cluster itself according to the Mahalanobis metric. Such machine learning
constitutes a fixed point problem. The definitive (self-regulating) kNN cluster
defines the smoothing volume, or properly saying, the smoothing ellipsoid,
required to perform the anisotropic interpolation. Thus, the smoothing kernel
has an ellipsoidal profile, which changes how the kernel gradients are
computed. As an application, it was performed the simulation of collapse and
fragmentation of a non-magnetic, rotating gaseous sphere. An interesting
outcome was the formation of protostars in the disc fragmentation, shown to be
much more persistent and much more abundant in the anisotropic simulation than
in the isotropic case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Marinho_E/0/1/0/all/0/1"&gt;Eraldo Pereira Marinho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding Spatio-Temporal Language with Transformers. (arXiv:2106.08858v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.08858</id>
        <link href="http://arxiv.org/abs/2106.08858"/>
        <updated>2021-06-17T01:58:44.138Z</updated>
        <summary type="html"><![CDATA[Language is an interface to the outside world. In order for embodied agents
to use it, language must be grounded in other, sensorimotor modalities. While
there is an extended literature studying how machines can learn grounded
language, the topic of how to learn spatio-temporal linguistic concepts is
still largely uncharted. To make progress in this direction, we here introduce
a novel spatio-temporal language grounding task where the goal is to learn the
meaning of spatio-temporal descriptions of behavioral traces of an embodied
agent. This is achieved by training a truth function that predicts if a
description matches a given history of observations. The descriptions involve
time-extended predicates in past and present tense as well as spatio-temporal
references to objects in the scene. To study the role of architectural biases
in this task, we train several models including multimodal Transformer
architectures; the latter implement different attention computations between
words and objects across space and time. We test models on two classes of
generalization: 1) generalization to randomly held-out sentences; 2)
generalization to grammar primitives. We observe that maintaining object
identity in the attention computation of our Transformers is instrumental to
achieving good performance on generalization overall, and that summarizing
object traces in a single token has little influence on performance. We then
discuss how this opens new perspectives for language-guided autonomous embodied
agents. We also release our code under open-source license as well as
pretrained models and datasets to encourage the wider community to build upon
and extend our work in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1"&gt;Tristan Karch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1"&gt;Laetitia Teodorescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1"&gt;Katja Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Moulin-Frier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1"&gt;Pierre-Yves Oudeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scene Transformer: A unified multi-task model for behavior prediction and planning. (arXiv:2106.08417v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08417</id>
        <link href="http://arxiv.org/abs/2106.08417"/>
        <updated>2021-06-17T01:58:44.132Z</updated>
        <summary type="html"><![CDATA[Predicting the future motion of multiple agents is necessary for planning in
dynamic environments. This task is challenging for autonomous driving since
agents (e.g., vehicles and pedestrians) and their associated behaviors may be
diverse and influence each other. Most prior work has focused on first
predicting independent futures for each agent based on all past motion, and
then planning against these independent predictions. However, planning against
fixed predictions can suffer from the inability to represent the future
interaction possibilities between different agents, leading to sub-optimal
planning. In this work, we formulate a model for predicting the behavior of all
agents jointly in real-world driving environments in a unified manner. Inspired
by recent language modeling approaches, we use a masking strategy as the query
to our model, enabling one to invoke a single model to predict agent behavior
in many ways, such as potentially conditioned on the goal or full future
trajectory of the autonomous vehicle or the behavior of other agents in the
environment. Our model architecture fuses heterogeneous world state in a
unified Transformer architecture by employing attention across road elements,
agent interactions and time steps. We evaluate our approach on autonomous
driving datasets for behavior prediction, and achieve state-of-the-art
performance. Our work demonstrates that formulating the problem of behavior
prediction in a unified architecture with a masking strategy may allow us to
have a single model that can perform multiple motion prediction and planning
related tasks effectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngiam_J/0/1/0/all/0/1"&gt;Jiquan Ngiam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1"&gt;Benjamin Caine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1"&gt;Vijay Vasudevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1"&gt;Hao-Tien Lewis Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1"&gt;Jeffrey Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1"&gt;Rebecca Roelofs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1"&gt;Alex Bewley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chenxi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venugopal_A/0/1/0/all/0/1"&gt;Ashish Venugopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_D/0/1/0/all/0/1"&gt;David Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1"&gt;Ben Sapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhifeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1"&gt;Jonathon Shlens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithm to Compilation Codesign: An Integrated View of Neural Network Sparsity. (arXiv:2106.08846v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08846</id>
        <link href="http://arxiv.org/abs/2106.08846"/>
        <updated>2021-06-17T01:58:44.126Z</updated>
        <summary type="html"><![CDATA[Reducing computation cost, inference latency, and memory footprint of neural
networks are frequently cited as research motivations for pruning and sparsity.
However, operationalizing those benefits and understanding the end-to-end
effect of algorithm design and regularization on the runtime execution is not
often examined in depth.

Here we apply structured and unstructured pruning to attention weights of
transformer blocks of the BERT language model, while also expanding block
sparse representation (BSR) operations in the TVM compiler. Integration of BSR
operations enables the TVM runtime execution to leverage structured pattern
sparsity induced by model regularization.

This integrated view of pruning algorithms enables us to study relationships
between modeling decisions and their direct impact on sparsity-enhanced
execution. Our main findings are: 1) we validate that performance benefits of
structured sparsity block regularization must be enabled by the BSR
augmentations to TVM, with 4x speedup relative to vanilla PyTorch and 2.2x
speedup relative to standard TVM compilation (without expanded BSR support). 2)
for BERT attention weights, the end-to-end optimal block sparsity shape in this
CPU inference context is not a square block (as in \cite{gray2017gpu}) but
rather a linear 32x1 block 3) the relationship between performance and block
size / shape is is suggestive of how model regularization parameters interact
with task scheduler optimizations resulting in the observed end-to-end
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1"&gt;Fu-Ming Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1"&gt;Austin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering Mixture Models in Almost-Linear Time via List-Decodable Mean Estimation. (arXiv:2106.08537v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.08537</id>
        <link href="http://arxiv.org/abs/2106.08537"/>
        <updated>2021-06-17T01:58:44.118Z</updated>
        <summary type="html"><![CDATA[We study the problem of list-decodable mean estimation, where an adversary
can corrupt a majority of the dataset. Specifically, we are given a set $T$ of
$n$ points in $\mathbb{R}^d$ and a parameter $0< \alpha <\frac 1 2$ such that
an $\alpha$-fraction of the points in $T$ are i.i.d. samples from a
well-behaved distribution $\mathcal{D}$ and the remaining $(1-\alpha)$-fraction
of the points are arbitrary. The goal is to output a small list of vectors at
least one of which is close to the mean of $\mathcal{D}$. As our main
contribution, we develop new algorithms for list-decodable mean estimation,
achieving nearly-optimal statistical guarantees, with running time $n^{1 +
o(1)} d$. All prior algorithms for this problem had additional polynomial
factors in $\frac 1 \alpha$. As a corollary, we obtain the first almost-linear
time algorithms for clustering mixtures of $k$ separated well-behaved
distributions, nearly-matching the statistical guarantees of spectral methods.
Prior clustering algorithms inherently relied on an application of $k$-PCA,
thereby incurring runtimes of $\Omega(n d k)$. This marks the first runtime
improvement for this basic statistical problem in nearly two decades.

The starting point of our approach is a novel and simpler near-linear time
robust mean estimation algorithm in the $\alpha \to 1$ regime, based on a
one-shot matrix multiplicative weights-inspired potential decrease. We
crucially leverage this new algorithmic framework in the context of the
iterative multi-filtering technique of Diakonikolas et. al. '18, '20, providing
a method to simultaneously cluster and downsample points using one-dimensional
projections --- thus, bypassing the $k$-PCA subroutines required by prior
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1"&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1"&gt;Daniel M. Kane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kongsgaard_D/0/1/0/all/0/1"&gt;Daniel Kongsgaard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jerry Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1"&gt;Kevin Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating the Robustness of Public Transport Systems Using Machine Learning. (arXiv:2106.08967v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08967</id>
        <link href="http://arxiv.org/abs/2106.08967"/>
        <updated>2021-06-17T01:58:44.102Z</updated>
        <summary type="html"><![CDATA[The planning of attractive and cost efficient public transport systems is a
highly complex optimization process involving many steps. Integrating
robustness from a passenger's point of view makes the task even more
challenging. With numerous different definitions of robustness in literature, a
real-world acceptable evaluation of the robustness of a public transport system
is to simulate its performance under a large number of possible scenarios.
Unfortunately, this is computationally very expensive. In this paper, we
therefore explore a new way of such a scenario-based robustness approximation
by using methods from machine learning. We achieve a fast approach with a very
high accuracy by gathering a subset of key features of a public transport
system and its passenger demand and training an artificial neural network to
learn the outcome of a given set of robustness tests. The network is then able
to predict the robustness of untrained instances with high accuracy using only
its key features, allowing for a robustness oracle for transport planners that
approximates the robustness in constant time. Such an oracle can be used as
black box to increase the robustness within a local search framework for
integrated public transportation planning. In computational experiments with
different benchmark instances we demonstrate an excellent quality of our
predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Hannemann_M/0/1/0/all/0/1"&gt;Matthias M&amp;#xfc;ller-Hannemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruckert_R/0/1/0/all/0/1"&gt;Ralf R&amp;#xfc;ckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiewe_A/0/1/0/all/0/1"&gt;Alexander Schiewe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schobel_A/0/1/0/all/0/1"&gt;Anita Sch&amp;#xf6;bel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ParticleAugment: Sampling-Based Data Augmentation. (arXiv:2106.08693v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08693</id>
        <link href="http://arxiv.org/abs/2106.08693"/>
        <updated>2021-06-17T01:58:44.096Z</updated>
        <summary type="html"><![CDATA[We present an automated data augmentation approach for image classification.
We formulate the problem as Monte Carlo sampling where our goal is to
approximate the optimal augmentation policies. We propose a particle filtering
formulation to find optimal augmentation policies and their schedules during
model training. Our performance measurement procedure relies on a validation
subset of our training set, while the policy transition model depends on a
Gaussian prior and an optional augmentation velocity parameter. In our
experiments, we show that our formulation for automated augmentation reaches
promising results on CIFAR-10, CIFAR-100, and ImageNet datasets using the
standard network architectures for this problem. By comparing with the related
work, we also show that our method reaches a balance between the computational
cost of policy search and the model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsaregorodtsev_A/0/1/0/all/0/1"&gt;Alexander Tsaregorodtsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1"&gt;Vasileios Belagiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Probabilistic Circuits for Nonparametric Multi-Output Regression. (arXiv:2106.08687v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08687</id>
        <link href="http://arxiv.org/abs/2106.08687"/>
        <updated>2021-06-17T01:58:44.089Z</updated>
        <summary type="html"><![CDATA[Inspired by recent advances in the field of expert-based approximations of
Gaussian processes (GPs), we present an expert-based approach to large-scale
multi-output regression using single-output GP experts. Employing a deeply
structured mixture of single-output GPs encoded via a probabilistic circuit
allows us to capture correlations between multiple output dimensions
accurately. By recursively partitioning the covariate space and the output
space, posterior inference in our model reduces to inference on single-output
GP experts, which only need to be conditioned on a small subset of the
observations. We show that inference can be performed exactly and efficiently
in our model, that it can capture correlations between output dimensions and,
hence, often outperforms approaches that do not incorporate inter-output
correlations, as demonstrated on several data sets in terms of the negative log
predictive density.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhongjie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Mingye Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1"&gt;Martin Trapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skryagin_A/0/1/0/all/0/1"&gt;Arseny Skryagin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maxmin-Fair Ranking: Individual Fairness under Group-Fairness Constraints. (arXiv:2106.08652v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08652</id>
        <link href="http://arxiv.org/abs/2106.08652"/>
        <updated>2021-06-17T01:58:44.072Z</updated>
        <summary type="html"><![CDATA[We study a novel problem of fairness in ranking aimed at minimizing the
amount of individual unfairness introduced when enforcing group-fairness
constraints. Our proposal is rooted in the distributional maxmin fairness
theory, which uses randomization to maximize the expected satisfaction of the
worst-off individuals. We devise an exact polynomial-time algorithm to find
maxmin-fair distributions of general search problems (including, but not
limited to, ranking), and show that our algorithm can produce rankings which,
while satisfying the given group-fairness constraints, ensure that the maximum
possible value is brought to individuals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Soriano_D/0/1/0/all/0/1"&gt;David Garcia-Soriano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1"&gt;Francesco Bonchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Learning to Select High-Quality Measurements. (arXiv:2106.08891v1 [physics.data-an])]]></title>
        <id>http://arxiv.org/abs/2106.08891</id>
        <link href="http://arxiv.org/abs/2106.08891"/>
        <updated>2021-06-17T01:58:44.065Z</updated>
        <summary type="html"><![CDATA[We describe the use of machine learning algorithms to select high-quality
measurements for the Mu2e experiment. This technique is important for
experiments with backgrounds that arise due to measurement errors. The
algorithms use multiple pieces of ancillary information that are sensitive to
measurement quality to separate high-quality and low-quality measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Edmonds_A/0/1/0/all/0/1"&gt;Andrew Edmonds&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Brown_D/0/1/0/all/0/1"&gt;David Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Vinas_L/0/1/0/all/0/1"&gt;Luciano Vinas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pagan_S/0/1/0/all/0/1"&gt;Samantha Pagan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Silent Speech and Emotion Recognition from Vocal Tract Shape Dynamics in Real-Time MRI. (arXiv:2106.08706v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08706</id>
        <link href="http://arxiv.org/abs/2106.08706"/>
        <updated>2021-06-17T01:58:44.057Z</updated>
        <summary type="html"><![CDATA[Speech sounds of spoken language are obtained by varying configuration of the
articulators surrounding the vocal tract. They contain abundant information
that can be utilized to better understand the underlying mechanism of human
speech production. We propose a novel deep neural network-based learning
framework that understands acoustic information in the variable-length sequence
of vocal tract shaping during speech production, captured by real-time magnetic
resonance imaging (rtMRI), and translate it into text. The proposed framework
comprises of spatiotemporal convolutions, a recurrent network, and the
connectionist temporal classification loss, trained entirely end-to-end. On the
USC-TIMIT corpus, the model achieved a 40.6% PER at sentence-level, much better
compared to the existing models. To the best of our knowledge, this is the
first study that demonstrates the recognition of entire spoken sentence based
on an individual's articulatory motions captured by rtMRI video. We also
performed an analysis of variations in the geometry of articulation in each
sub-regions of the vocal tract (i.e., pharyngeal, velar and dorsal, hard
palate, labial constriction region) with respect to different emotions and
genders. Results suggest that each sub-regions distortion is affected by both
emotion and gender.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pandey_L/0/1/0/all/0/1"&gt;Laxmi Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arif_A/0/1/0/all/0/1"&gt;Ahmed Sabbir Arif&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Accounting of Differential Privacy via Characteristic Function. (arXiv:2106.08567v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08567</id>
        <link href="http://arxiv.org/abs/2106.08567"/>
        <updated>2021-06-17T01:58:44.035Z</updated>
        <summary type="html"><![CDATA[Characterizing the privacy degradation over compositions, i.e., privacy
accounting, is a fundamental topic in differential privacy (DP) with many
applications to differentially private machine learning and federated learning.

We propose a unification of recent advances (Renyi DP, privacy profiles,
$f$-DP and the PLD formalism) via the characteristic function ($\phi$-function)
of a certain ``worst-case'' privacy loss random variable.

We show that our approach allows natural adaptive composition like Renyi DP,

provides exactly tight privacy accounting like PLD, and can be (often
losslessly) converted to privacy profile and $f$-DP, thus providing
$(\epsilon,\delta)$-DP guarantees and interpretable tradeoff functions.
Algorithmically, we propose an analytical Fourier accountant that represents
the complex logarithm of $\phi$-functions symbolically and uses Gaussian
quadrature for numerical computation. On several popular DP mechanisms and
their subsampled counterparts, we demonstrate the flexibility and tightness of
our approach in theory and experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuqing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jinshuo Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence. (arXiv:2106.08710v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.08710</id>
        <link href="http://arxiv.org/abs/2106.08710"/>
        <updated>2021-06-17T01:58:44.026Z</updated>
        <summary type="html"><![CDATA[Mobile Augmented Reality (MAR) integrates computer-generated virtual objects
with physical environments for mobile devices. MAR systems enable users to
interact with MAR devices, such as smartphones and head-worn wearables, and
performs seamless transitions from the physical world to a mixed world with
digital entities. These MAR systems support user experiences by using MAR
devices to provide universal accessibility to digital contents. Over the past
20 years, a number of MAR systems have been developed, however, the studies and
design of MAR frameworks have not yet been systematically reviewed from the
perspective of user-centric design. This article presents the first effort of
surveying existing MAR frameworks (count: 37) and further discusses the latest
studies on MAR through a top-down approach: 1) MAR applications; 2) MAR
visualisation techniques adaptive to user mobility and contexts; 3) systematic
evaluation of MAR frameworks including supported platforms and corresponding
features such as tracking, feature extraction plus sensing capabilities; and 4)
underlying machine learning approaches supporting intelligent operations within
MAR systems. Finally, we summarise the development of emerging research fields,
current state-of-the-art, and discuss the important open challenges and
possible theoretical and technical directions. This survey aims to benefit both
researchers and MAR system developers alike.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jacky Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1"&gt;Kit-Yung Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1"&gt;Lik-Hang Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_P/0/1/0/all/0/1"&gt;Pan Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xiang Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation. (arXiv:2106.09017v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09017</id>
        <link href="http://arxiv.org/abs/2106.09017"/>
        <updated>2021-06-17T01:58:44.019Z</updated>
        <summary type="html"><![CDATA[Multi-task learning (MTL) aims to improve the generalization of several
related tasks by learning them jointly. As a comparison, in addition to the
joint training scheme, modern meta-learning allows unseen tasks with limited
labels during the test phase, in the hope of fast adaptation over them. Despite
the subtle difference between MTL and meta-learning in the problem formulation,
both learning paradigms share the same insight that the shared structure
between existing training tasks could lead to better generalization and
adaptation. In this paper, we take one important step further to understand the
close connection between these two learning paradigms, through both theoretical
analysis and empirical investigation. Theoretically, we first demonstrate that
MTL shares the same optimization formulation with a class of gradient-based
meta-learning (GBML) algorithms. We then prove that for over-parameterized
neural networks with sufficient depth, the learned predictive functions of MTL
and GBML are close. In particular, this result implies that the predictions
given by these two models are similar over the same unseen task. Empirically,
we corroborate our theoretical findings by showing that, with proper
implementation, MTL is competitive against state-of-the-art GBML algorithms on
a set of few-shot image classification benchmarks. Since existing GBML
algorithms often involve costly second-order bi-level optimization, our
first-order MTL method is an order of magnitude faster on large-scale datasets
such as mini-ImageNet. We believe this work could help bridge the gap between
these two learning paradigms, and provide a computationally efficient
alternative to GBML that also supports fast task adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How memory architecture affects performance and learning in simple POMDPs. (arXiv:2106.08849v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08849</id>
        <link href="http://arxiv.org/abs/2106.08849"/>
        <updated>2021-06-17T01:58:44.013Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning is made much more complex when the agent's observation
is partial or noisy. This case corresponds to a partially observable Markov
decision process (POMDP). One strategy to seek good performance in POMDPs is to
endow the agent with a finite memory, whose update is governed by the policy.
However, policy optimization is non-convex in that case and can lead to poor
training performance for random initialization. The performance can be
empirically improved by constraining the memory architecture, then sacrificing
optimality to facilitate training. Here we study this trade-off in the two-arm
bandit problem, and compare two extreme cases: (i) the random access memory
where any transitions between $M$ memory states are allowed and (ii) a fixed
memory where the agent can access its last $m$ actions and rewards. For (i),
the probability $q$ to play the worst arm is known to be exponentially small in
$M$ for the optimal policy. Our main result is to show that similar performance
can be reached for (ii) as well, despite the simplicity of the memory
architecture: using a conjecture on Gray-ordered binary necklaces, we find
policies for which $q$ is exponentially small in $2^m$ i.e. $q\sim\alpha^{2^m}$
for some $\alpha < 1$. Interestingly, we observe empirically that training from
random initialization leads to very poor results for (i), and significantly
better results for (ii).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1"&gt;Mario Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eloy_C/0/1/0/all/0/1"&gt;Christophe Eloy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1"&gt;Matthieu Wyart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Auto-regressive Variational Attention Models for Text Modeling. (arXiv:2106.08571v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08571</id>
        <link href="http://arxiv.org/abs/2106.08571"/>
        <updated>2021-06-17T01:58:44.008Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) have been widely applied for text modeling.
In practice, however, they are troubled by two challenges: information
underrepresentation and posterior collapse. The former arises as only the last
hidden state of LSTM encoder is transformed into the latent space, which is
generally insufficient to summarize the data. The latter is a long-standing
problem during the training of VAEs as the optimization is trapped to a
disastrous local optimum. In this paper, we propose Discrete Auto-regressive
Variational Attention Model (DAVAM) to address the challenges. Specifically, we
introduce an auto-regressive variational attention approach to enrich the
latent space by effectively capturing the semantic dependency from the input.
We further design discrete latent space for the variational attention and
mathematically show that our model is free from posterior collapse. Extensive
experiments on language modeling tasks demonstrate the superiority of DAVAM
against several VAE counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locality defeats the curse of dimensionality in convolutional teacher-student scenarios. (arXiv:2106.08619v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08619</id>
        <link href="http://arxiv.org/abs/2106.08619"/>
        <updated>2021-06-17T01:58:43.990Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks perform a local and translationally-invariant
treatment of the data: quantifying which of these two aspects is central to
their success remains a challenge. We study this problem within a
teacher-student framework for kernel regression, using `convolutional' kernels
inspired by the neural tangent kernel of simple convolutional architectures of
given filter size. Using heuristic methods from physics, we find in the
ridgeless case that locality is key in determining the learning curve exponent
$\beta$ (that relates the test error $\epsilon_t\sim P^{-\beta}$ to the size of
the training set $P$), whereas translational invariance is not. In particular,
if the filter size of the teacher $t$ is smaller than that of the student $s$,
$\beta$ is a function of $s$ only and does not depend on the input dimension.
We confirm our predictions on $\beta$ empirically. Theoretically, in some cases
(including when teacher and student are equal) it can be shown that this
prediction is an upper bound on performance. We conclude by proving, using a
natural universality assumption, that performing kernel regression with a ridge
that decreases with the size of the training set leads to similar learning
curve exponents to those we obtain in the ridgeless case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Favero_A/0/1/0/all/0/1"&gt;Alessandro Favero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cagnetta_F/0/1/0/all/0/1"&gt;Francesco Cagnetta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wyart_M/0/1/0/all/0/1"&gt;Matthieu Wyart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameter-free Locally Accelerated Conditional Gradients. (arXiv:2102.06806v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06806</id>
        <link href="http://arxiv.org/abs/2102.06806"/>
        <updated>2021-06-17T01:58:43.962Z</updated>
        <summary type="html"><![CDATA[Projection-free conditional gradient (CG) methods are the algorithms of
choice for constrained optimization setups in which projections are often
computationally prohibitive but linear optimization over the constraint set
remains computationally feasible. Unlike in projection-based methods, globally
accelerated convergence rates are in general unattainable for CG. However, a
very recent work on Locally accelerated CG (LaCG) has demonstrated that local
acceleration for CG is possible for many settings of interest. The main
downside of LaCG is that it requires knowledge of the smoothness and strong
convexity parameters of the objective function. We remove this limitation by
introducing a novel, Parameter-Free Locally accelerated CG (PF-LaCG) algorithm,
for which we provide rigorous convergence guarantees. Our theoretical results
are complemented by numerical experiments, which demonstrate local acceleration
and showcase the practical improvements of PF-LaCG over non-accelerated
algorithms, both in terms of iteration count and wall-clock time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Carderera_A/0/1/0/all/0/1"&gt;Alejandro Carderera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Diakonikolas_J/0/1/0/all/0/1"&gt;Jelena Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lin_C/0/1/0/all/0/1"&gt;Cheuk Yin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Pokutta_S/0/1/0/all/0/1"&gt;Sebastian Pokutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training. (arXiv:2106.08616v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08616</id>
        <link href="http://arxiv.org/abs/2106.08616"/>
        <updated>2021-06-17T01:58:43.945Z</updated>
        <summary type="html"><![CDATA[Out-of-scope intent detection is of practical importance in task-oriented
dialogue systems. Since the distribution of outlier utterances is arbitrary and
unknown in the training stage, existing methods commonly rely on strong
assumptions on data distribution such as mixture of Gaussians to make
inference, resulting in either complex multi-step training procedures or
hand-crafted rules such as confidence threshold selection for outlier
detection. In this paper, we propose a simple yet effective method to train an
out-of-scope intent classifier in a fully end-to-end manner by simulating the
test scenario in training, which requires no assumption on data distribution
and no additional post-processing or threshold setting. Specifically, we
construct a set of pseudo outliers in the training stage, by generating
synthetic outliers using inliner features via self-supervision and sampling
out-of-scope sentences from easily available open-domain datasets. The pseudo
outliers are used to train a discriminative classifier that can be directly
applied to and generalize well on the test task. We evaluate our method
extensively on four benchmark dialogue datasets and observe significant
improvements over state-of-the-art approaches. Our code has been released at
https://github.com/liam0949/DCLOOS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1"&gt;Li-Ming Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Haowen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lu Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiao-Ming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_A/0/1/0/all/0/1"&gt;Albert Y.S. Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Gender Bias in Hindi-English Machine Translation. (arXiv:2106.08680v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08680</id>
        <link href="http://arxiv.org/abs/2106.08680"/>
        <updated>2021-06-17T01:58:43.939Z</updated>
        <summary type="html"><![CDATA[With language models being deployed increasingly in the real world, it is
essential to address the issue of the fairness of their outputs. The word
embedding representations of these language models often implicitly draw
unwanted associations that form a social bias within the model. The nature of
gendered languages like Hindi, poses an additional problem to the
quantification and mitigation of bias, owing to the change in the form of the
words in the sentence, based on the gender of the subject. Additionally, there
is sparse work done in the realm of measuring and debiasing systems for Indic
languages. In our work, we attempt to evaluate and quantify the gender bias
within a Hindi-English machine translation system. We implement a modified
version of the existing TGBI metric based on the grammatical considerations for
Hindi. We also compare and contrast the resulting bias measurements across
multiple metrics for pre-trained embeddings and the ones learned by our machine
translation model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1"&gt;Gauri Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1"&gt;Krithika Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sanjay Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CODA: Constructivism Learning for Instance-Dependent Dropout Architecture Construction. (arXiv:2106.08444v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08444</id>
        <link href="http://arxiv.org/abs/2106.08444"/>
        <updated>2021-06-17T01:58:43.933Z</updated>
        <summary type="html"><![CDATA[Dropout is attracting intensive research interest in deep learning as an
efficient approach to prevent overfitting. Recently incorporating structural
information when deciding which units to drop out produced promising results
comparing to methods that ignore the structural information. However, a major
issue of the existing work is that it failed to differentiate among instances
when constructing the dropout architecture. This can be a significant
deficiency for many applications. To solve this issue, we propose
Constructivism learning for instance-dependent Dropout Architecture (CODA),
which is inspired from a philosophical theory, constructivism learning.
Specially, based on the theory we have designed a better drop out technique,
Uniform Process Mixture Models, using a Bayesian nonparametric method Uniform
process. We have evaluated our proposed method on 5 real-world datasets and
compared the performance with other state-of-the-art dropout techniques. The
experimental results demonstrated the effectiveness of CODA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoli Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Directed Graph Embeddings in Pseudo-Riemannian Manifolds. (arXiv:2106.08678v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08678</id>
        <link href="http://arxiv.org/abs/2106.08678"/>
        <updated>2021-06-17T01:58:43.880Z</updated>
        <summary type="html"><![CDATA[The inductive biases of graph representation learning algorithms are often
encoded in the background geometry of their embedding space. In this paper, we
show that general directed graphs can be effectively represented by an
embedding model that combines three components: a pseudo-Riemannian metric
structure, a non-trivial global topology, and a unique likelihood function that
explicitly incorporates a preferred direction in embedding space. We
demonstrate the representational capabilities of this method by applying it to
the task of link prediction on a series of synthetic and real directed graphs
from natural language applications and biology. In particular, we show that
low-dimensional cylindrical Minkowski and anti-de Sitter spacetimes can produce
equal or better graph representations than curved Riemannian manifolds of
higher dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sim_A/0/1/0/all/0/1"&gt;Aaron Sim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wiatrak_M/0/1/0/all/0/1"&gt;Maciej Wiatrak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Brayne_A/0/1/0/all/0/1"&gt;Angus Brayne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Creed_P/0/1/0/all/0/1"&gt;P&amp;#xe1;id&amp;#xed; Creed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Paliwal_S/0/1/0/all/0/1"&gt;Saee Paliwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spoofing Generalization: When Can't You Trust Proprietary Models?. (arXiv:2106.08393v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08393</id>
        <link href="http://arxiv.org/abs/2106.08393"/>
        <updated>2021-06-17T01:58:43.865Z</updated>
        <summary type="html"><![CDATA[In this work, we study the computational complexity of determining whether a
machine learning model that perfectly fits the training data will generalizes
to unseen data. In particular, we study the power of a malicious agent whose
goal is to construct a model g that fits its training data and nothing else,
but is indistinguishable from an accurate model f. We say that g strongly
spoofs f if no polynomial-time algorithm can tell them apart. If instead we
restrict to algorithms that run in $n^c$ time for some fixed $c$, we say that g
c-weakly spoofs f. Our main results are

1. Under cryptographic assumptions, strong spoofing is possible and 2. For
any c> 0, c-weak spoofing is possible unconditionally

While the assumption of a malicious agent is an extreme scenario (hopefully
companies training large models are not malicious), we believe that it sheds
light on the inherent difficulties of blindly trusting large proprietary models
or data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1"&gt;Ankur Moitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mossel_E/0/1/0/all/0/1"&gt;Elchanan Mossel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandon_C/0/1/0/all/0/1"&gt;Colin Sandon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Graphs for Explainable Classification of Brain Networks. (arXiv:2106.08640v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.08640</id>
        <link href="http://arxiv.org/abs/2106.08640"/>
        <updated>2021-06-17T01:58:43.845Z</updated>
        <summary type="html"><![CDATA[Training graph classifiers able to distinguish between healthy brains and
dysfunctional ones, can help identifying substructures associated to specific
cognitive phenotypes. However, the mere predictive power of the graph
classifier is of limited interest to the neuroscientists, which have plenty of
tools for the diagnosis of specific mental disorders. What matters is the
interpretation of the model, as it can provide novel insights and new
hypotheses.

In this paper we propose \emph{counterfactual graphs} as a way to produce
local post-hoc explanations of any black-box graph classifier. Given a graph
and a black-box, a counterfactual is a graph which, while having high
structural similarity with the original graph, is classified by the black-box
in a different class. We propose and empirically compare several strategies for
counterfactual graph search. Our experiments against a white-box classifier
with known optimal counterfactual, show that our methods, although heuristic,
can produce counterfactuals very close to the optimal one. Finally, we show how
to use counterfactual graphs to build global explanations correctly capturing
the behaviour of different black-box classifiers and providing interesting
insights for the neuroscientists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abrate_C/0/1/0/all/0/1"&gt;Carlo Abrate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1"&gt;Francesco Bonchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predictive Modeling of Hospital Readmission: Challenges and Solutions. (arXiv:2106.08488v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08488</id>
        <link href="http://arxiv.org/abs/2106.08488"/>
        <updated>2021-06-17T01:58:43.743Z</updated>
        <summary type="html"><![CDATA[Hospital readmission prediction is a study to learn models from historical
medical data to predict probability of a patient returning to hospital in a
certain period, 30 or 90 days, after the discharge. The motivation is to help
health providers deliver better treatment and post-discharge strategies, lower
the hospital readmission rate, and eventually reduce the medical costs. Due to
inherent complexity of diseases and healthcare ecosystems, modeling hospital
readmission is facing many challenges. By now, a variety of methods have been
developed, but existing literature fails to deliver a complete picture to
answer some fundamental questions, such as what are the main challenges and
solutions in modeling hospital readmission; what are typical features/models
used for readmission prediction; how to achieve meaningful and transparent
predictions for decision making; and what are possible conflicts when deploying
predictive approaches for real-world usages. In this paper, we systematically
review computational models for hospital readmission prediction, and propose a
taxonomy of challenges featuring four main categories: (1) data variety and
complexity; (2) data imbalance, locality and privacy; (3) model
interpretability; and (4) model implementation. The review summarizes methods
in each category, and highlights technical solutions proposed to address the
challenges. In addition, a review of datasets and resources available for
hospital readmission modeling also provides firsthand materials to support
researchers and practitioners to design new approaches for effective and
efficient hospital readmission prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuwen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xingquan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ada-BKB: Scalable Gaussian Process Optimization on Continuous Domain by Adaptive Discretization. (arXiv:2106.08598v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08598</id>
        <link href="http://arxiv.org/abs/2106.08598"/>
        <updated>2021-06-17T01:58:43.738Z</updated>
        <summary type="html"><![CDATA[Gaussian process optimization is a successful class of algorithms (e.g.
GP-UCB) to optimize a black-box function through sequential evaluations.
However, when the domain of the function is continuous, Gaussian process
optimization has to either rely on a fixed discretization of the space, or
solve a non-convex optimization subproblem at each evaluation. The first
approach can negatively affect performance, while the second one puts a heavy
computational burden on the algorithm. A third option, that only recently has
been theoretically studied, is to adaptively discretize the function domain.
Even though this approach avoids the extra non-convex optimization costs, the
overall computational complexity is still prohibitive. An algorithm such as
GP-UCB has a runtime of $O(T^4)$, where $T$ is the number of iterations. In
this paper, we introduce Ada-BKB (Adaptive Budgeted Kernelized Bandit), a
no-regret Gaussian process optimization algorithm for functions on continuous
domains, that provably runs in $O(T^2 d_\text{eff}^2)$, where $d_\text{eff}$ is
the effective dimension of the explored space, and which is typically much
smaller than $T$. We corroborate our findings with experiments on synthetic
non-convex functions and on the real-world problem of hyper-parameter
optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rando_M/0/1/0/all/0/1"&gt;Marco Rando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carratino_L/0/1/0/all/0/1"&gt;Luigi Carratino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villa_S/0/1/0/all/0/1"&gt;Silvia Villa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosasco_L/0/1/0/all/0/1"&gt;Lorenzo Rosasco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WaveNet-Based Deep Neural Networks for the Characterization of Anomalous Diffusion (WADNet). (arXiv:2106.08887v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08887</id>
        <link href="http://arxiv.org/abs/2106.08887"/>
        <updated>2021-06-17T01:58:43.631Z</updated>
        <summary type="html"><![CDATA[Anomalous diffusion, which shows a deviation of transport dynamics from the
framework of standard Brownian motion, is involved in the evolution of various
physical, chemical, biological, and economic systems. The study of such random
processes is of fundamental importance in unveiling the physical properties of
random walkers and complex systems. However, classical methods to characterize
anomalous diffusion are often disqualified for individual short trajectories,
leading to the launch of the Anomalous Diffusion (AnDi) Challenge. This
challenge aims at objectively assessing and comparing new approaches for single
trajectory characterization, with respect to three different aspects: the
inference of the anomalous diffusion exponent; the classification of the
diffusion model; and the segmentation of trajectories. In this article, to
address the inference and classification tasks in the challenge, we develop a
WaveNet-based deep neural network (WADNet) by combining a modified WaveNet
encoder with long short-term memory networks, without any prior knowledge of
anomalous diffusion. As the performance of our model has surpassed the current
1st places in the challenge leaderboard on both two tasks for all dimensions (6
subtasks), WADNet could be the part of state-of-the-art techniques to decode
the AnDi database. Our method presents a benchmark for future research, and
could accelerate the development of a versatile tool for the characterization
of anomalous diffusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dezhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1"&gt;Qiujin Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zihan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSO: Curriculum Generation using continuous optimization. (arXiv:2106.08569v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08569</id>
        <link href="http://arxiv.org/abs/2106.08569"/>
        <updated>2021-06-17T01:58:43.593Z</updated>
        <summary type="html"><![CDATA[The training of deep learning models poses vast challenges of including
parameter tuning and ordering of training data. Significant research has been
done in Curriculum learning for optimizing the sequence of training data.
Recent works have focused on using complex reinforcement learning techniques to
find the optimal data ordering strategy to maximize learning for a given
network. In this paper, we present a simple and efficient technique based on
continuous optimization. We call this new approach Training Sequence
Optimization (TSO). There are three critical components in our proposed
approach: (a) An encoder network maps/embeds training sequence into continuous
space. (b) A predictor network uses the continuous representation of a strategy
as input and predicts the accuracy for fixed network architecture. (c) A
decoder further maps a continuous representation of a strategy to the ordered
training dataset. The performance predictor and encoder enable us to perform
gradient-based optimization in the continuous space to find the embedding of
optimal training data ordering with potentially better accuracy. Experiments
show that we can gain 2AP with our generated optimal curriculum strategy over
the random strategy using the CIFAR-100 dataset and have better boosts than the
state of the art CL algorithms. We do an ablation study varying the
architecture, dataset and sample sizes showcasing our approach's robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1"&gt;Dipankar Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Mukur Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-PSD Matrix Sketching with Applications to Regression and Optimization. (arXiv:2106.08544v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08544</id>
        <link href="http://arxiv.org/abs/2106.08544"/>
        <updated>2021-06-17T01:58:43.570Z</updated>
        <summary type="html"><![CDATA[A variety of dimensionality reduction techniques have been applied for
computations involving large matrices. The underlying matrix is randomly
compressed into a smaller one, while approximately retaining many of its
original properties. As a result, much of the expensive computation can be
performed on the small matrix. The sketching of positive semidefinite (PSD)
matrices is well understood, but there are many applications where the related
matrices are not PSD, including Hessian matrices in non-convex optimization and
covariance matrices in regression applications involving complex numbers. In
this paper, we present novel dimensionality reduction methods for non-PSD
matrices, as well as their ``square-roots", which involve matrices with complex
entries. We show how these techniques can be used for multiple downstream
tasks. In particular, we show how to use the proposed matrix sketching
techniques for both convex and non-convex optimization, $\ell_p$-regression for
every $1 \leq p \leq \infty$, and vector-matrix-vector queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhili Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roosta_F/0/1/0/all/0/1"&gt;Fred Roosta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dataset Dynamics via Gradient Flows in Probability Space. (arXiv:2010.12760v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12760</id>
        <link href="http://arxiv.org/abs/2010.12760"/>
        <updated>2021-06-17T01:58:43.563Z</updated>
        <summary type="html"><![CDATA[Various machine learning tasks, from generative modeling to domain
adaptation, revolve around the concept of dataset transformation and
manipulation. While various methods exist for transforming unlabeled datasets,
principled methods to do so for labeled (e.g., classification) datasets are
missing. In this work, we propose a novel framework for dataset transformation,
which we cast as optimization over data-generating joint probability
distributions. We approach this class of problems through Wasserstein gradient
flows in probability space, and derive practical and efficient particle-based
methods for a flexible but well-behaved class of objective functions. Through
various experiments, we show that this framework can be used to impose
constraints on classification datasets, adapt them for transfer learning, or to
re-purpose fixed or black-box models to classify ---with high accuracy---
previously unseen datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_Melis_D/0/1/0/all/0/1"&gt;David Alvarez-Melis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fusi_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Fusi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic DAG Search. (arXiv:2106.08717v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08717</id>
        <link href="http://arxiv.org/abs/2106.08717"/>
        <updated>2021-06-17T01:58:43.543Z</updated>
        <summary type="html"><![CDATA[Exciting contemporary machine learning problems have recently been phrased in
the classic formalism of tree search -- most famously, the game of Go.
Interestingly, the state-space underlying these sequential decision-making
problems often posses a more general latent structure than can be captured by a
tree. In this work, we develop a probabilistic framework to exploit a search
space's latent structure and thereby share information across the search tree.
The method is based on a combination of approximate inference in jointly
Gaussian models for the explored part of the problem, and an abstraction for
the unexplored part that imposes a reduction of complexity ad hoc. We
empirically find our algorithm to compare favorably to existing
non-probabilistic alternatives in Tic-Tac-Toe and a feature selection
application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grosse_J/0/1/0/all/0/1"&gt;Julia Grosse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1"&gt;Philipp Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint detection and matching of feature points in multimodal images. (arXiv:1810.12941v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.12941</id>
        <link href="http://arxiv.org/abs/1810.12941"/>
        <updated>2021-06-17T01:58:43.503Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a novel Convolutional Neural Network (CNN)
architecture for the joint detection and matching of feature points in images
acquired by different sensors using a single forward pass. The resulting
feature detector is tightly coupled with the feature descriptor, in contrast to
classical approaches (SIFT, etc.), where the detection phase precedes and
differs from computing the descriptor. Our approach utilizes two CNN
subnetworks, the first being a Siamese CNN and the second, consisting of dual
non-weight-sharing CNNs. This allows simultaneous processing and fusion of the
joint and disjoint cues in the multimodal image patches. The proposed approach
is experimentally shown to outperform contemporary state-of-the-art schemes
when applied to multiple datasets of multimodal images. It is also shown to
provide repeatable feature points detections across multisensor images,
outperforming state-of-the-art detectors. To the best of our knowledge, it is
the first unified approach for the detection and matching of such images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baruch_E/0/1/0/all/0/1"&gt;Elad Ben Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1"&gt;Yosi Keller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To Raise or Not To Raise: The Autonomous Learning Rate Question. (arXiv:2106.08767v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08767</id>
        <link href="http://arxiv.org/abs/2106.08767"/>
        <updated>2021-06-17T01:58:43.471Z</updated>
        <summary type="html"><![CDATA[There is a parameter ubiquitous throughout the deep learning world: learning
rate. There is likewise a ubiquitous question: what should that learning rate
be? The true answer to this question is often tedious and time consuming to
obtain, and a great deal of arcane knowledge has accumulated in recent years
over how to pick and modify learning rates to achieve optimal training
performance. Moreover, the long hours spent carefully crafting the perfect
learning rate can come to nothing the moment your network architecture,
optimizer, dataset, or initial conditions change ever so slightly. But it need
not be this way. We propose a new answer to the great learning rate question:
the Autonomous Learning Rate Controller. Find it at
https://github.com/fastestimator/ARC]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaomeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1"&gt;Michael Potter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yun-Chan Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1"&gt;Gaurav Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saripalli_V/0/1/0/all/0/1"&gt;V. Ratna Saripalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ModelDiff: Testing-Based DNN Similarity Comparison for Model Reuse Detection. (arXiv:2106.08890v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08890</id>
        <link href="http://arxiv.org/abs/2106.08890"/>
        <updated>2021-06-17T01:58:43.444Z</updated>
        <summary type="html"><![CDATA[The knowledge of a deep learning model may be transferred to a student model,
leading to intellectual property infringement or vulnerability propagation.
Detecting such knowledge reuse is nontrivial because the suspect models may not
be white-box accessible and/or may serve different tasks. In this paper, we
propose ModelDiff, a testing-based approach to deep learning model similarity
comparison. Instead of directly comparing the weights, activations, or outputs
of two models, we compare their behavioral patterns on the same set of test
inputs. Specifically, the behavioral pattern of a model is represented as a
decision distance vector (DDV), in which each element is the distance between
the model's reactions to a pair of inputs. The knowledge similarity between two
models is measured with the cosine similarity between their DDVs. To evaluate
ModelDiff, we created a benchmark that contains 144 pairs of models that cover
most popular model reuse methods, including transfer learning, model
compression, and model stealing. Our method achieved 91.7% correctness on the
benchmark, which demonstrates the effectiveness of using ModelDiff for model
reuse detection. A study on mobile deep learning apps has shown the feasibility
of ModelDiff on real-world models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanchun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bingyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ziyue Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yunxin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SEEN: Sharpening Explanations for Graph Neural Networks using Explanations from Neighborhoods. (arXiv:2106.08532v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08532</id>
        <link href="http://arxiv.org/abs/2106.08532"/>
        <updated>2021-06-17T01:58:43.437Z</updated>
        <summary type="html"><![CDATA[Explaining the foundations for predictions obtained from graph neural
networks (GNNs) is critical for credible use of GNN models for real-world
problems. Owing to the rapid growth of GNN applications, recent progress in
explaining predictions from GNNs, such as sensitivity analysis, perturbation
methods, and attribution methods, showed great opportunities and possibilities
for explaining GNN predictions. In this study, we propose a method to improve
the explanation quality of node classification tasks that can be applied in a
post hoc manner through aggregation of auxiliary explanations from important
neighboring nodes, named SEEN. Applying SEEN does not require modification of a
graph and can be used with diverse explainability techniques due to its
independent mechanism. Experiments on matching motif-participating nodes from a
given graph show great improvement in explanation accuracy of up to 12.71% and
demonstrate the correlation between the auxiliary explanations and the enhanced
explanation accuracy through leveraging their contributions. SEEN provides a
simple but effective method to enhance the explanation quality of GNN model
outputs, and this method is applicable in combination with most explainability
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hyeoncheol Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1"&gt;Youngrock Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_E/0/1/0/all/0/1"&gt;Eunjoo Jeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Achieving Domain Robustness in Stereo Matching Networks by Removing Shortcut Learning. (arXiv:2106.08486v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08486</id>
        <link href="http://arxiv.org/abs/2106.08486"/>
        <updated>2021-06-17T01:58:43.414Z</updated>
        <summary type="html"><![CDATA[Learning-based stereo matching and depth estimation networks currently excel
on public benchmarks with impressive results. However, state-of-the-art
networks often fail to generalize from synthetic imagery to more challenging
real data domains. This paper is an attempt to uncover hidden secrets of
achieving domain robustness and in particular, discovering the important
ingredients of generalization success of stereo matching networks by analyzing
the effect of synthetic image learning on real data performance. We provide
evidence that demonstrates that learning of features in the synthetic domain by
a stereo matching network is heavily influenced by two "shortcuts" presented in
the synthetic data: (1) identical local statistics (RGB colour features)
between matching pixels in the synthetic stereo images and (2) lack of realism
in synthetic textures on 3D objects simulated in game engines. We will show
that by removing such shortcuts, we can achieve domain robustness in the
state-of-the-art stereo matching frameworks and produce a remarkable
performance on multiple realistic datasets, despite the fact that the networks
were trained on synthetic data, only. Our experimental results point to the
fact that eliminating shortcuts from the synthetic data is key to achieve
domain-invariant generalization between synthetic and real data domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chuah_W/0/1/0/all/0/1"&gt;WeiQin Chuah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tennakoon_R/0/1/0/all/0/1"&gt;Ruwan Tennakoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bab_Hadiashar_A/0/1/0/all/0/1"&gt;Alireza Bab-Hadiashar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1"&gt;David Suter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamically Grown Generative Adversarial Networks. (arXiv:2106.08505v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08505</id>
        <link href="http://arxiv.org/abs/2106.08505"/>
        <updated>2021-06-17T01:58:43.368Z</updated>
        <summary type="html"><![CDATA[Recent work introduced progressive network growing as a promising way to ease
the training for large GANs, but the model design and architecture-growing
strategy still remain under-explored and needs manual design for different
image data. In this paper, we propose a method to dynamically grow a GAN during
training, optimizing the network architecture and its parameters together with
automation. The method embeds architecture search techniques as an interleaving
step with gradient-based training to periodically seek the optimal
architecture-growing strategy for the generator and discriminator. It enjoys
the benefits of both eased training because of progressive growing and improved
performance because of broader architecture design space. Experimental results
demonstrate new state-of-the-art of image generation. Observations in the
search procedure also provide constructive insights into the GAN model design
such as generator-discriminator balance and convolutional layer choices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lanlan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuting Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jia Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memorization and Generalization in Neural Code Intelligence Models. (arXiv:2106.08704v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.08704</id>
        <link href="http://arxiv.org/abs/2106.08704"/>
        <updated>2021-06-17T01:58:43.361Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNN) are increasingly commonly used in software
engineering and code intelligence tasks. These are powerful tools that are
capable of learning highly generalizable patterns from large datasets through
millions of parameters. At the same time, training DNNs means walking a knife's
edges, because their large capacity also renders them prone to memorizing data
points. While traditionally thought of as an aspect of over-training, recent
work suggests that the memorization risk manifests especially strongly when the
training datasets are noisy and memorization is the only recourse.
Unfortunately, most code intelligence tasks rely on rather noise-prone and
repetitive data sources, such as GitHub, which, due to their sheer size, cannot
be manually inspected and evaluated. We evaluate the memorization and
generalization tendencies in neural code intelligence models through a case
study across several benchmarks and model families by leveraging established
approaches from other fields that use DNNs, such as introducing targeted noise
into the training dataset. In addition to reinforcing prior general findings
about the extent of memorization in DNNs, our results shed light on the impact
of noisy dataset in training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rabin_M/0/1/0/all/0/1"&gt;Md Rafiqul Islam Rabin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1"&gt;Aftab Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1"&gt;Vincent J. Hellendoorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alipour_M/0/1/0/all/0/1"&gt;Mohammad Amin Alipour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Synchronized Reprojection-based Model for 3D Human Pose Estimation. (arXiv:2106.04274v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04274</id>
        <link href="http://arxiv.org/abs/2106.04274"/>
        <updated>2021-06-17T01:58:43.315Z</updated>
        <summary type="html"><![CDATA[3D human pose estimation is still a challenging problem despite the large
amount of work that has been done in this field. Generally, most methods
directly use neural networks and ignore certain constraints (e.g., reprojection
constraints and joint angle and bone length constraints). This paper proposes a
weakly supervised GAN-based model for 3D human pose estimation that considers
3D information along with 2D information simultaneously, in which a
reprojection network is employed to learn the mapping of the distribution from
3D poses to 2D poses. In particular, we train the reprojection network and the
generative adversarial network synchronously. Furthermore, inspired by the
typical kinematic chain space (KCS) matrix, we propose a weighted KCS matrix,
which is added into the discriminator's input to impose joint angle and bone
length constraints. The experimental results on Human3.6M show that our method
outperforms state-of-the-art methods by approximately 5.1\%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yicheng Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Cheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yongqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiahui Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selection of Source Images Heavily Influences the Effectiveness of Adversarial Attacks. (arXiv:2106.07141v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07141</id>
        <link href="http://arxiv.org/abs/2106.07141"/>
        <updated>2021-06-17T01:58:43.309Z</updated>
        <summary type="html"><![CDATA[Although the adoption rate of deep neural networks (DNNs) has tremendously
increased in recent years, a solution for their vulnerability against
adversarial examples has not yet been found. As a result, substantial research
efforts are dedicated to fix this weakness, with many studies typically using a
subset of source images to generate adversarial examples, treating every image
in this subset as equal. We demonstrate that, in fact, not every source image
is equally suited for this kind of assessment. To do so, we devise a
large-scale model-to-model transferability scenario for which we meticulously
analyze the properties of adversarial examples, generated from every suitable
source image in ImageNet by making use of two of the most frequently deployed
attacks. In this transferability scenario, which involves seven distinct DNN
models, including the recently proposed vision transformers, we reveal that it
is possible to have a difference of up to $12.5\%$ in model-to-model
transferability success, $1.01$ in average $L_2$ perturbation, and $0.03$
($8/225$) in average $L_{\infty}$ perturbation when $1,000$ source images are
sampled randomly among all suitable candidates. We then take one of the first
steps in evaluating the robustness of images used to create adversarial
examples, proposing a number of simple but effective methods to identify
unsuitable source images, thus making it possible to mitigate extreme cases in
experimentation and support high-quality benchmarking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozbulak_U/0/1/0/all/0/1"&gt;Utku Ozbulak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anzaku_E/0/1/0/all/0/1"&gt;Esla Timothy Anzaku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neve_W/0/1/0/all/0/1"&gt;Wesley De Neve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Messem_A/0/1/0/all/0/1"&gt;Arnout Van Messem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation. (arXiv:2106.09016v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09016</id>
        <link href="http://arxiv.org/abs/2106.09016"/>
        <updated>2021-06-17T01:58:43.303Z</updated>
        <summary type="html"><![CDATA[Image-to-Image (I2I) multi-domain translation models are usually evaluated
also using the quality of their semantic interpolation results. However,
state-of-the-art models frequently show abrupt changes in the image appearance
during interpolation, and usually perform poorly in interpolations across
domains. In this paper, we propose a new training protocol based on three
specific losses which help a translation network to learn a smooth and
disentangled latent style space in which: 1) Both intra- and inter-domain
interpolations correspond to gradual changes in the generated images and 2) The
content of the source image is better preserved during the translation.
Moreover, we propose a novel evaluation metric to properly measure the
smoothness of latent style space of I2I translation models. The proposed method
can be plugged into existing translation approaches, and our extensive
experiments on different datasets show that it can significantly boost the
quality of the generated images and the graduality of the interpolations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yahui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1"&gt;Enver Sangineto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yajing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1"&gt;Linchao Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoxian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1"&gt;Bruno Lepri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadai_M/0/1/0/all/0/1"&gt;Marco De Nadai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TextStyleBrush: Transfer of Text Aesthetics from a Single Example. (arXiv:2106.08385v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08385</id>
        <link href="http://arxiv.org/abs/2106.08385"/>
        <updated>2021-06-17T01:58:43.297Z</updated>
        <summary type="html"><![CDATA[We present a novel approach for disentangling the content of a text image
from all aspects of its appearance. The appearance representation we derive can
then be applied to new content, for one-shot transfer of the source style to
new content. We learn this disentanglement in a self-supervised manner. Our
method processes entire word boxes, without requiring segmentation of text from
background, per-character processing, or making assumptions on string lengths.
We show results in different text domains which were previously handled by
specialized methods, e.g., scene text, handwritten text. To these ends, we make
a number of technical contributions: (1) We disentangle the style and content
of a textual image into a non-parametric, fixed-dimensional vector. (2) We
propose a novel approach inspired by StyleGAN but conditioned over the example
style at different resolution and content. (3) We present novel self-supervised
training criteria which preserve both source style and target content using a
pre-trained font classifier and text recognizer. Finally, (4) we also introduce
Imgur5K, a new challenging dataset for handwritten word images. We offer
numerous qualitative photo-realistic results of our method. We further show
that our method surpasses previous work in quantitative tests on scene text and
handwriting datasets, as well as in a user study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_P/0/1/0/all/0/1"&gt;Praveen Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovvuri_R/0/1/0/all/0/1"&gt;Rama Kovvuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1"&gt;Guan Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vassilev_B/0/1/0/all/0/1"&gt;Boris Vassilev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1"&gt;Tal Hassner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment Analysis Methods. (arXiv:2106.08829v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.08829</id>
        <link href="http://arxiv.org/abs/2106.08829"/>
        <updated>2021-06-17T01:58:43.289Z</updated>
        <summary type="html"><![CDATA[Opinion and sentiment analysis is a vital task to characterize subjective
information in social media posts. In this paper, we present a comprehensive
experimental evaluation and comparison with six state-of-the-art methods, from
which we have re-implemented one of them. In addition, we investigate different
textual and visual feature embeddings that cover different aspects of the
content, as well as the recently introduced multimodal CLIP embeddings.
Experimental results are presented for two different publicly available
benchmark datasets of tweets and corresponding images. In contrast to the
evaluation methodology of previous work, we introduce a reproducible and fair
evaluation scheme to make results comparable. Finally, we conduct an error
analysis to outline the limitations of the methods and possibilities for the
future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheema_G/0/1/0/all/0/1"&gt;Gullal S. Cheema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1"&gt;Sherzod Hakimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1"&gt;Eric M&amp;#xfc;ller-Budack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1"&gt;Ralph Ewerth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Evaluating and Training Verifiably Robust Neural Networks. (arXiv:2104.00447v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00447</id>
        <link href="http://arxiv.org/abs/2104.00447"/>
        <updated>2021-06-17T01:58:43.273Z</updated>
        <summary type="html"><![CDATA[Recent works have shown that interval bound propagation (IBP) can be used to
train verifiably robust neural networks. Reseachers observe an intriguing
phenomenon on these IBP trained networks: CROWN, a bounding method based on
tight linear relaxation, often gives very loose bounds on these networks. We
also observe that most neurons become dead during the IBP training process,
which could hurt the representation capability of the network. In this paper,
we study the relationship between IBP and CROWN, and prove that CROWN is always
tighter than IBP when choosing appropriate bounding lines. We further propose a
relaxed version of CROWN, linear bound propagation (LBP), that can be used to
verify large networks to obtain lower verified errors than IBP. We also design
a new activation function, parameterized ramp function (ParamRamp), which has
more diversity of neuron status than ReLU. We conduct extensive experiments on
MNIST, CIFAR-10 and Tiny-ImageNet with ParamRamp activation and achieve
state-of-the-art verified robustness. Code and the appendix are available at
https://github.com/ZhaoyangLyu/VerifiablyRobustNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1"&gt;Zhaoyang Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Minghao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guodong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kehuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dahua Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Neural Architecture Search. (arXiv:2006.06863v8 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06863</id>
        <link href="http://arxiv.org/abs/2006.06863"/>
        <updated>2021-06-17T01:58:43.267Z</updated>
        <summary type="html"><![CDATA[Efficient evaluation of a network architecture drawn from a large search
space remains a key challenge in Neural Architecture Search (NAS). Vanilla NAS
evaluates each architecture by training from scratch, which gives the true
performance but is extremely time-consuming. Recently, one-shot NAS
substantially reduces the computation cost by training only one supernetwork,
a.k.a. supernet, to approximate the performance of every architecture in the
search space via weight-sharing. However, the performance estimation can be
very inaccurate due to the co-adaption among operations. In this paper, we
propose few-shot NAS that uses multiple supernetworks, called sub-supernet,
each covering different regions of the search space to alleviate the undesired
co-adaption. Compared to one-shot NAS, few-shot NAS improves the accuracy of
architecture evaluation with a small increase of evaluation cost. With only up
to 7 sub-supernets, few-shot NAS establishes new SoTAs: on ImageNet, it finds
models that reach 80.5% top-1 accuracy at 600 MB FLOPS and 77.5% top-1 accuracy
at 238 MFLOPS; on CIFAR10, it reaches 98.72% top-1 accuracy without using extra
data or transfer learning. In Auto-GAN, few-shot NAS outperforms the previously
published results by up to 20%. Extensive experiments show that few-shot NAS
significantly improves various one-shot methods, including 4 gradient-based and
6 search-based methods on 3 different tasks in NasBench-201 and
NasBench1-shot-1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linnan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuandong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_R/0/1/0/all/0/1"&gt;Rodrigo Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1"&gt;Tian Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Device-Cloud Collaborative Learning for Recommendation. (arXiv:2104.06624v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06624</id>
        <link href="http://arxiv.org/abs/2104.06624"/>
        <updated>2021-06-17T01:58:43.240Z</updated>
        <summary type="html"><![CDATA[With the rapid development of storage and computing power on mobile devices,
it becomes critical and popular to deploy models on devices to save onerous
communication latencies and to capture real-time features. While quite a lot of
works have explored to facilitate on-device learning and inference, most of
them focus on dealing with response delay or privacy protection. Little has
been done to model the collaboration between the device and the cloud modeling
and benefit both sides jointly. To bridge this gap, we are among the first
attempts to study the Device-Cloud Collaborative Learning (DCCL) framework.
Specifically, we propose a novel MetaPatch learning approach on the device side
to efficiently achieve "thousands of people with thousands of models" given a
centralized cloud model. Then, with billions of updated personalized device
models, we propose a "model-over-models" distillation algorithm, namely
MoMoDistill, to update the centralized cloud model. Our extensive experiments
over a range of datasets with different settings demonstrate the effectiveness
of such collaboration on both cloud and devices, especially its superiority to
model long-tailed users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jiangchao Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Feng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;KunYang Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metamorphic image registration using a semi-Lagrangian scheme. (arXiv:2106.08817v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08817</id>
        <link href="http://arxiv.org/abs/2106.08817"/>
        <updated>2021-06-17T01:58:43.234Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an implementation of both Large Deformation
Diffeomorphic Metric Mapping (LDDMM) and Metamorphosis image registration using
a semi-Lagrangian scheme for geodesic shooting. We propose to solve both
problems as an inexact matching providing a single and unifying cost function.
We demonstrate that for image registration the use of a semi-Lagrangian scheme
is more stable than a standard Eulerian scheme. Our GPU implementation is based
on PyTorch, which greatly simplifies and accelerates the computations thanks to
its powerful automatic differentiation engine. It will be freely available at
https://github.com/antonfrancois/Demeter_metamorphosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Francois_A/0/1/0/all/0/1"&gt;Anton Fran&amp;#xe7;ois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1"&gt;Pietro Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaunes_J/0/1/0/all/0/1"&gt;Joan Glaun&amp;#xe8;s&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Inference in medicine and in health policy, a summary. (arXiv:2105.04655v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04655</id>
        <link href="http://arxiv.org/abs/2105.04655"/>
        <updated>2021-06-17T01:58:43.226Z</updated>
        <summary type="html"><![CDATA[A data science task can be deemed as making sense of the data or testing a
hypothesis about it. The conclusions inferred from data can greatly guide us to
make informative decisions. Big data has enabled us to carry out countless
prediction tasks in conjunction with machine learning, such as identifying high
risk patients suffering from a certain disease and taking preventable measures.
However, healthcare practitioners are not content with mere predictions - they
are also interested in the cause-effect relation between input features and
clinical outcomes. Understanding such relations will help doctors treat
patients and reduce the risk effectively. Causality is typically identified by
randomized controlled trials. Often such trials are not feasible when
scientists and researchers turn to observational studies and attempt to draw
inferences. However, observational studies may also be affected by selection
and/or confounding biases that can result in wrong causal conclusions. In this
chapter, we will try to highlight some of the drawbacks that may arise in
traditional machine learning and statistical approaches to analyze the
observational data, particularly in the healthcare data analytics domain. We
will discuss causal inference and ways to discover the cause-effect from
observational studies in healthcare domain. Moreover, we will demonstrate the
applications of causal inference in tackling some common machine learning
issues such as missing data and model transportability. Finally, we will
discuss the possibility of integrating reinforcement learning with causality as
a way to counter confounding bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramezani_R/0/1/0/all/0/1"&gt;Ramin Ramezani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeim_A/0/1/0/all/0/1"&gt;Arash Naeim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Morphset:Augmenting categorical emotion datasets with dimensional affect labels using face morphing. (arXiv:2103.02854v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02854</id>
        <link href="http://arxiv.org/abs/2103.02854"/>
        <updated>2021-06-17T01:58:43.209Z</updated>
        <summary type="html"><![CDATA[Emotion recognition and understanding is a vital component in human-machine
interaction. Dimensional models of affect such as those using valence and
arousal have advantages over traditional categorical ones due to the complexity
of emotional states in humans. However, dimensional emotion annotations are
difficult and expensive to collect, therefore they are not as prevalent in the
affective computing community. To address these issues, we propose a method to
generate synthetic images from existing categorical emotion datasets using face
morphing as well as dimensional labels in the circumplex space with full
control over the resulting sample distribution, while achieving augmentation
factors of at least 20x or more.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vonikakis_V/0/1/0/all/0/1"&gt;Vassilios Vonikakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neo_D/0/1/0/all/0/1"&gt;Dexter Neo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1"&gt;Stefan Winkler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds. (arXiv:2007.07978v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.07978</id>
        <link href="http://arxiv.org/abs/2007.07978"/>
        <updated>2021-06-17T01:58:43.191Z</updated>
        <summary type="html"><![CDATA[Forecasting the formation and development of clouds is a central element of
modern weather forecasting systems. Incorrect clouds forecasts can lead to
major uncertainty in the overall accuracy of weather forecasts due to their
intrinsic role in the Earth's climate system. Few studies have tackled this
challenging problem from a machine learning point-of-view due to a shortage of
high-resolution datasets with many historical observations globally. In this
paper, we present a novel satellite-based dataset called ``CloudCast''. It
consists of 70,080 images with 10 different cloud types for multiple layers of
the atmosphere annotated on a pixel level. The spatial resolution of the
dataset is 928 x 1530 pixels (3x3 km per pixel) with 15-min intervals between
frames for the period 2017-01-01 to 2018-12-31. All frames are centered and
projected over Europe. To supplement the dataset, we conduct an evaluation
study with current state-of-the-art video prediction methods such as
convolutional long short-term memory networks, generative adversarial networks,
and optical flow-based extrapolation methods. As the evaluation of video
prediction is difficult in practice, we aim for a thorough evaluation in the
spatial and temporal domain. Our benchmark models show promising results but
with ample room for improvement. This is the first publicly available
global-scale dataset with high-resolution cloud types on a high temporal
granularity to the authors' best knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_A/0/1/0/all/0/1"&gt;A. H. Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;A. Iosifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karstoft_H/0/1/0/all/0/1"&gt;H. Karstoft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amortized Synthesis of Constrained Configurations Using a Differentiable Surrogate. (arXiv:2106.09019v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09019</id>
        <link href="http://arxiv.org/abs/2106.09019"/>
        <updated>2021-06-17T01:58:43.178Z</updated>
        <summary type="html"><![CDATA[In design, fabrication, and control problems, we are often faced with the
task of synthesis, in which we must generate an object or configuration that
satisfies a set of constraints while maximizing one or more objective
functions. The synthesis problem is typically characterized by a physical
process in which many different realizations may achieve the goal. This
many-to-one map presents challenges to the supervised learning of feed-forward
synthesis, as the set of viable designs may have a complex structure. In
addition, the non-differentiable nature of many physical simulations prevents
direct optimization. We address both of these problems with a two-stage neural
network architecture that we may consider to be an autoencoder. We first learn
the decoder: a differentiable surrogate that approximates the many-to-one
physical realization process. We then learn the encoder, which maps from goal
to design, while using the fixed decoder to evaluate the quality of the
realization. We evaluate the approach on two case studies: extruder path
planning in additive manufacturing and constrained soft robot inverse
kinematics. We compare our approach to direct optimization of design using the
learned surrogate, and to supervised learning of the synthesis problem. We find
that our approach produces higher quality solutions than supervised learning,
while being competitive in quality with direct optimization, at a greatly
reduced computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xingyuan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1"&gt;Tianju Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rusinkiewicz_S/0/1/0/all/0/1"&gt;Szymon M. Rusinkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adams_R/0/1/0/all/0/1"&gt;Ryan P. Adams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data. (arXiv:2106.03096v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03096</id>
        <link href="http://arxiv.org/abs/2106.03096"/>
        <updated>2021-06-17T01:58:43.170Z</updated>
        <summary type="html"><![CDATA[Tabular data are ubiquitous for the widespread applications of tables and
hence have attracted the attention of researchers to extract underlying
information. One of the critical problems in mining tabular data is how to
understand their inherent semantic structures automatically. Existing studies
typically adopt Convolutional Neural Network (CNN) to model the spatial
information of tabular structures yet ignore more diverse relational
information between cells, such as the hierarchical and paratactic
relationships. To simultaneously extract spatial and relational information
from tables, we propose a novel neural network architecture, TabularNet. The
spatial encoder of TabularNet utilizes the row/column-level Pooling and the
Bidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information
and local positional correlation, respectively. For relational information, we
design a new graph construction method based on the WordNet tree and adopt a
Graph Convolutional Network (GCN) based encoder that focuses on the
hierarchical and paratactic relationships between cells. Our neural network
architecture can be a unified neural backbone for different understanding tasks
and utilized in a multitask scenario. We conduct extensive experiments on three
classification tasks with two real-world spreadsheet data sets, and the results
demonstrate the effectiveness of our proposed TabularNet over state-of-the-art
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1"&gt;Lun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1"&gt;Fei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ran Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junshan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Learning and Personalization in Multi-Agent Stochastic Linear Bandits. (arXiv:2106.08902v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08902</id>
        <link href="http://arxiv.org/abs/2106.08902"/>
        <updated>2021-06-17T01:58:43.142Z</updated>
        <summary type="html"><![CDATA[We consider the problem of minimizing regret in an $N$ agent heterogeneous
stochastic linear bandits framework, where the agents (users) are similar but
not all identical. We model user heterogeneity using two popularly used ideas
in practice; (i) A clustering framework where users are partitioned into groups
with users in the same group being identical to each other, but different
across groups, and (ii) a personalization framework where no two users are
necessarily identical, but a user's parameters are close to that of the
population average. In the clustered users' setup, we propose a novel
algorithm, based on successive refinement of cluster identities and regret
minimization. We show that, for any agent, the regret scales as
$\mathcal{O}(\sqrt{T/N})$, if the agent is in a `well separated' cluster, or
scales as $\mathcal{O}(T^{\frac{1}{2} + \varepsilon}/(N)^{\frac{1}{2}
-\varepsilon})$ if its cluster is not well separated, where $\varepsilon$ is
positive and arbitrarily close to $0$. Our algorithm is adaptive to the cluster
separation, and is parameter free -- it does not need to know the number of
clusters, separation and cluster size, yet the regret guarantee adapts to the
inherent complexity. In the personalization framework, we introduce a natural
algorithm where, the personal bandit instances are initialized with the
estimates of the global average model. We show that, an agent $i$ whose
parameter deviates from the population average by $\epsilon_i$, attains a
regret scaling of $\widetilde{O}(\epsilon_i\sqrt{T})$. This demonstrates that
if the user representations are close (small $\epsilon_i)$, the resulting
regret is low, and vice-versa. The results are empirically validated and we
observe superior performance of our adaptive algorithms over non-adaptive
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Avishek Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sankararaman_A/0/1/0/all/0/1"&gt;Abishek Sankararaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1"&gt;Kannan Ramchandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation. (arXiv:2106.08823v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08823</id>
        <link href="http://arxiv.org/abs/2106.08823"/>
        <updated>2021-06-17T01:58:43.128Z</updated>
        <summary type="html"><![CDATA[State-of-the-art transformer models use pairwise dot-product based
self-attention, which comes at a computational cost quadratic in the input
sequence length. In this paper, we investigate the global structure of
attention scores computed using this dot product mechanism on a typical
distribution of inputs, and study the principal components of their variation.
Through eigen analysis of full attention score matrices, as well as of their
individual rows, we find that most of the variation among attention scores lie
in a low-dimensional eigenspace. Moreover, we find significant overlap between
these eigenspaces for different layers and even different transformer models.
Based on this, we propose to compute scores only for a partial subset of token
pairs, and use them to estimate scores for the remaining pairs. Beyond
investigating the accuracy of reconstructing attention scores themselves, we
investigate training transformer models that employ these approximations, and
analyze the effect on overall accuracy. Our analysis and the proposed method
provide insights into how to balance the benefits of exact pair-wise attention
and its significant computational expense.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1"&gt;Srinadh Bhojanapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1"&gt;Ayan Chakrabarti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1"&gt;Himanshu Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1"&gt;Michal Lukasik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1"&gt;Andreas Veit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaze Preserving CycleGANs for Eyeglass Removal & Persistent Gaze Estimation. (arXiv:2002.02077v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02077</id>
        <link href="http://arxiv.org/abs/2002.02077"/>
        <updated>2021-06-17T01:58:43.122Z</updated>
        <summary type="html"><![CDATA[A driver's gaze is critical for determining their attention, state,
situational awareness, and readiness to take over control from partially
automated vehicles. Estimating the gaze direction is the most obvious way to
gauge a driver's state under ideal conditions when limited to using
non-intrusive imaging sensors. Unfortunately, the vehicular environment
introduces a variety of challenges that are usually unaccounted for - harsh
illumination, nighttime conditions, and reflective eyeglasses. Relying on head
pose alone under such conditions can prove to be unreliable and erroneous. In
this study, we offer solutions to address these problems encountered in the
real world. To solve issues with lighting, we demonstrate that using an
infrared camera with suitable equalization and normalization suffices. To
handle eyeglasses and their corresponding artifacts, we adopt image-to-image
translation using generative adversarial networks to pre-process images prior
to gaze estimation. Our proposed Gaze Preserving CycleGAN (GPCycleGAN) is
trained to preserve the driver's gaze while removing potential eyeglasses from
face images. GPCycleGAN is based on the well-known CycleGAN approach - with the
addition of a gaze classifier and a gaze consistency loss for additional
supervision. Our approach exhibits improved performance, interpretability,
robustness and superior qualitative results on challenging real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1"&gt;Akshay Rangesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bowen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1"&gt;Mohan M. Trivedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing the Impact: Does an Improvement to a Revenue Management System Lead to an Improved Revenue?. (arXiv:2101.10249v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10249</id>
        <link href="http://arxiv.org/abs/2101.10249"/>
        <updated>2021-06-17T01:58:43.115Z</updated>
        <summary type="html"><![CDATA[Airlines and other industries have been making use of sophisticated Revenue
Management Systems to maximize revenue for decades. While improving the
different components of these systems has been the focus of numerous studies,
estimating the impact of such improvements on the revenue has been overlooked
in the literature despite its practical importance. Indeed, quantifying the
benefit of a change in a system serves as support for investment decisions.
This is a challenging problem as it corresponds to the difference between the
generated value and the value that would have been generated keeping the system
as before. The latter is not observable. Moreover, the expected impact can be
small in relative value. In this paper, we cast the problem as counterfactual
prediction of unobserved revenue. The impact on revenue is then the difference
between the observed and the estimated revenue. The originality of this work
lies in the innovative application of econometric methods proposed for
macroeconomic applications to a new problem setting. Broadly applicable, the
approach benefits from only requiring revenue data observed for
origin-destination pairs in the network of the airline at each day, before and
after a change in the system is applied. We report results using real
large-scale data from Air Canada. We compare a deep neural network
counterfactual predictions model with econometric models. They achieve
respectively 1% and 1.1% of error on the counterfactual revenue predictions,
and allow to accurately estimate small impacts (in the order of 2%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laage_G/0/1/0/all/0/1"&gt;Greta Laage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frejinger_E/0/1/0/all/0/1"&gt;Emma Frejinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lodi_A/0/1/0/all/0/1"&gt;Andrea Lodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1"&gt;Guillaume Rabusseau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline RL Without Off-Policy Evaluation. (arXiv:2106.08909v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08909</id>
        <link href="http://arxiv.org/abs/2106.08909"/>
        <updated>2021-06-17T01:58:43.108Z</updated>
        <summary type="html"><![CDATA[Most prior approaches to offline reinforcement learning (RL) have taken an
iterative actor-critic approach involving off-policy evaluation. In this paper
we show that simply doing one step of constrained/regularized policy
improvement using an on-policy Q estimate of the behavior policy performs
surprisingly well. This one-step algorithm beats the previously reported
results of iterative algorithms on a large portion of the D4RL benchmark. The
simple one-step baseline achieves this strong performance without many of the
tricks used by previously proposed iterative algorithms and is more robust to
hyperparameters. We argue that the relatively poor performance of iterative
approaches is a result of the high variance inherent in doing off-policy
evaluation and magnified by the repeated optimization of policies against those
high-variance estimates. In addition, we hypothesize that the strong
performance of the one-step algorithm is due to a combination of favorable
structure in the environment and behavior policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandfonbrener_D/0/1/0/all/0/1"&gt;David Brandfonbrener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1"&gt;William F. Whitney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1"&gt;Joan Bruna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep-learning based Tools for Automated Protocol Definition of Advanced Diagnostic Imaging Exams. (arXiv:2106.08963v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08963</id>
        <link href="http://arxiv.org/abs/2106.08963"/>
        <updated>2021-06-17T01:58:43.094Z</updated>
        <summary type="html"><![CDATA[Purpose: This study evaluates the effectiveness and impact of automated
order-based protocol assignment for magnetic resonance imaging (MRI) exams
using natural language processing (NLP) and deep learning (DL).

Methods: NLP tools were applied to retrospectively process orders from over
116,000 MRI exams with 200 unique sub-specialized protocols ("Local" protocol
class). Separate DL models were trained on 70\% of the processed data for
"Local" protocols as well as 93 American College of Radiology ("ACR") protocols
and 48 "General" protocols. The DL Models were assessed in an "auto-protocoling
(AP)" inference mode which returns the top recommendation and in a "clinical
decision support (CDS)" inference mode which returns up to 10 protocols for
radiologist review. The accuracy of each protocol recommendation was computed
and analyzed based on the difference between the normalized output score of the
corresponding neural net for the top two recommendations.

Results: The top predicted protocol in AP mode was correct for 82.8%, 73.8%,
and 69.3% of the test cases for "General", "ACR", and "Local" protocol classes,
respectively. Higher levels of accuracy over 96% were obtained for all protocol
classes in CDS mode. However, at current validation performance levels, the
proposed models offer modest, positive, financial impact on large-scale imaging
networks.

Conclusions: DL-based protocol automation is feasible and can be tuned to
route substantial fractions of exams for auto-protocoling, with higher accuracy
with more general protocols. Economic analyses of the tested algorithms
indicate that improved algorithm performance is required to yield a practical
exam auto-protocoling tool for sub-specialized imaging exams.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nencka_A/0/1/0/all/0/1"&gt;Andrew S. Nencka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sherafati_M/0/1/0/all/0/1"&gt;Mohammad Sherafati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_T/0/1/0/all/0/1"&gt;Timothy Goebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolat_P/0/1/0/all/0/1"&gt;Parag Tolat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_K/0/1/0/all/0/1"&gt;Kevin M. Koch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LemgoRL: An open-source Benchmark Tool to Train Reinforcement Learning Agents for Traffic Signal Control in a real-world simulation scenario. (arXiv:2103.16223v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16223</id>
        <link href="http://arxiv.org/abs/2103.16223"/>
        <updated>2021-06-17T01:58:43.086Z</updated>
        <summary type="html"><![CDATA[Sub-optimal control policies in intersection traffic signal controllers (TSC)
contribute to congestion and lead to negative effects on human health and the
environment. Reinforcement learning (RL) for traffic signal control is a
promising approach to design better control policies and has attracted
considerable research interest in recent years. However, most work done in this
area used simplified simulation environments of traffic scenarios to train
RL-based TSC. To deploy RL in real-world traffic systems, the gap between
simplified simulation environments and real-world applications has to be
closed. Therefore, we propose LemgoRL, a benchmark tool to train RL agents as
TSC in a realistic simulation environment of Lemgo, a medium-sized town in
Germany. In addition to the realistic simulation model, LemgoRL encompasses a
traffic signal logic unit that ensures compliance with all regulatory and
safety requirements. LemgoRL offers the same interface as the well-known OpenAI
gym toolkit to enable easy deployment in existing research work. Our benchmark
tool drives the development of RL algorithms towards real-world applications.
We provide LemgoRL as an open-source tool at https://github.com/rl-ina/lemgorl.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_A/0/1/0/all/0/1"&gt;Arthur M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangras_V/0/1/0/all/0/1"&gt;Vishal Rangras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnittker_G/0/1/0/all/0/1"&gt;Georg Schnittker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waldmann_M/0/1/0/all/0/1"&gt;Michael Waldmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friesen_M/0/1/0/all/0/1"&gt;Maxim Friesen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferfers_T/0/1/0/all/0/1"&gt;Tobias Ferfers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schreckenberg_L/0/1/0/all/0/1"&gt;Lukas Schreckenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hufen_F/0/1/0/all/0/1"&gt;Florian Hufen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jasperneite_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Jasperneite&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiering_M/0/1/0/all/0/1"&gt;Marco Wiering&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of Outlier Detection Techniques for Structured Data. (arXiv:2106.08779v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08779</id>
        <link href="http://arxiv.org/abs/2106.08779"/>
        <updated>2021-06-17T01:58:43.074Z</updated>
        <summary type="html"><![CDATA[An outlier is an observation or a data point that is far from rest of the
data points in a given dataset or we can be said that an outlier is away from
the center of mass of observations. Presence of outliers can skew statistical
measures and data distributions which can lead to misleading representation of
the underlying data and relationships. It is seen that the removal of outliers
from the training dataset before modeling can give better predictions. With the
advancement of machine learning, the outlier detection models are also
advancing at a good pace. The goal of this work is to highlight and compare
some of the existing outlier detection techniques for the data scientists to
use that information for outlier algorithm selection while building a machine
learning model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Amulya Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nitin Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilinear Dirichlet Processes. (arXiv:2106.08852v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08852</id>
        <link href="http://arxiv.org/abs/2106.08852"/>
        <updated>2021-06-17T01:58:43.069Z</updated>
        <summary type="html"><![CDATA[Dependent Dirichlet processes (DDP) have been widely applied to model data
from distributions over collections of measures which are correlated in some
way. On the other hand, in recent years, increasing research efforts in machine
learning and data mining have been dedicated to dealing with data involving
interactions from two or more factors. However, few researchers have addressed
the heterogeneous relationship in data brought by modulation of multiple
factors using techniques of DDP. In this paper, we propose a novel technique,
MultiLinear Dirichlet Processes (MLDP), to constructing DDPs by combining DP
with a state-of-the-art factor analysis technique, multilinear factor analyzers
(MLFA). We have evaluated MLDP on real-word data sets for different
applications and have achieved state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoli Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early fault detection with multi-target neural networks. (arXiv:2106.08957v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08957</id>
        <link href="http://arxiv.org/abs/2106.08957"/>
        <updated>2021-06-17T01:58:43.063Z</updated>
        <summary type="html"><![CDATA[Wind power is seeing a strong growth around the world. At the same time,
shrinking profit margins in the energy markets let wind farm managers explore
options for cost reductions in the turbine operation and maintenance.
Sensor-based condition monitoring facilitates remote diagnostics of turbine
subsystems, enabling faster responses when unforeseen maintenance is required.
Condition monitoring with data from the turbines' supervisory control and data
acquisition (SCADA) systems was proposed and SCADA-based fault detection and
diagnosis approaches introduced based on single-task normal operation models of
turbine state variables. As the number of SCADA channels has grown strongly,
thousands of independent single-target models are in place today for monitoring
a single turbine. Multi-target learning was recently proposed to limit the
number of models. This study applied multi-target neural networks to the task
of early fault detection in drive-train components. The accuracy and delay of
detecting gear bearing faults were compared to state-of-the-art single-target
approaches. We found that multi-target multi-layer perceptrons (MLPs) detected
faults at least as early and in many cases earlier than single-target MLPs. The
multi-target MLPs could detect faults up to several days earlier than the
single-target models. This can deliver a significant advantage in the planning
and performance of maintenance work. At the same time, the multi-target MLPs
achieved the same level of prediction stability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_A/0/1/0/all/0/1"&gt;Angela Meyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[mSHAP: SHAP Values for Two-Part Models. (arXiv:2106.08990v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08990</id>
        <link href="http://arxiv.org/abs/2106.08990"/>
        <updated>2021-06-17T01:58:43.043Z</updated>
        <summary type="html"><![CDATA[Two-part models are important to and used throughout insurance and actuarial
science. Since insurance is required for registering a car, obtaining a
mortgage, and participating in certain businesses, it is especially important
that the models which price insurance policies are fair and non-discriminatory.
Black box models can make it very difficult to know which covariates are
influencing the results. SHAP values enable interpretation of various black box
models, but little progress has been made in two-part models. In this paper, we
propose mSHAP (or multiplicative SHAP), a method for computing SHAP values of
two-part models using the SHAP values of the individual models. This method
will allow for the predictions of two-part models to be explained at an
individual observation level. After developing mSHAP, we perform an in-depth
simulation study. Although the kernelSHAP algorithm is also capable of
computing approximate SHAP values for a two-part model, a comparison with our
method demonstrates that mSHAP is exponentially faster. Ultimately, we apply
mSHAP to a two-part ratemaking model for personal auto property damage
insurance coverage. Additionally, an R package (mshap) is available to easily
implement the method in a wide variety of applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Matthews_S/0/1/0/all/0/1"&gt;Spencer Matthews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hartman_B/0/1/0/all/0/1"&gt;Brian Hartman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolving Image Compositions for Feature Representation Learning. (arXiv:2106.09011v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09011</id>
        <link href="http://arxiv.org/abs/2106.09011"/>
        <updated>2021-06-17T01:58:43.037Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks for visual recognition require large amounts of
training samples and usually benefit from data augmentation. This paper
proposes PatchMix, a data augmentation method that creates new samples by
composing patches from pairs of images in a grid-like pattern. These new
samples' ground truth labels are set as proportional to the number of patches
from each image. We then add a set of additional losses at the patch-level to
regularize and to encourage good representations at both the patch and image
levels. A ResNet-50 model trained on ImageNet using PatchMix exhibits superior
transfer learning capabilities across a wide array of benchmarks. Although
PatchMix can rely on random pairings and random grid-like patterns for mixing,
we explore evolutionary search as a guiding strategy to discover optimal
grid-like patterns and image pairing jointly. For this purpose, we conceive a
fitness function that bypasses the need to re-train a model to evaluate each
choice. In this way, PatchMix outperforms a base model on CIFAR-10 (+1.91),
CIFAR-100 (+5.31), Tiny Imagenet (+3.52), and ImageNet (+1.16) by significant
margins, also outperforming previous state-of-the-art pairwise augmentation
strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cascante_Bonilla_P/0/1/0/all/0/1"&gt;Paola Cascante-Bonilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1"&gt;Arshdeep Sekhon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yanjun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1"&gt;Vicente Ordonez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Voice and Biofeedback to Predict User Engagement during Requirements Interviews. (arXiv:2104.02410v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02410</id>
        <link href="http://arxiv.org/abs/2104.02410"/>
        <updated>2021-06-17T01:58:43.031Z</updated>
        <summary type="html"><![CDATA[Capturing users engagement is crucial for gathering feedback about the
features of a software product. In a market-driven context, current approaches
to collect and analyze users feedback are based on techniques leveraging
information extracted from product reviews and social media. These approaches
are hardly applicable in bespoke software development, or in contexts in which
one needs to gather information from specific users. In such cases, companies
need to resort to face-to-face interviews to get feedback on their products. In
this paper, we propose to utilize biometric data, in terms of physiological and
voice features, to complement interviews with information about the engagement
of the user on the discussed product-relevant topics. We evaluate our approach
by interviewing users while gathering their physiological data (i.e.,
biofeedback) using an Empatica E4 wristband, and capturing their voice through
the default audio-recorder of a common laptop. Our results show that we can
predict users' engagement by training supervised machine learning algorithms on
biometric data, and that voice features alone can be sufficiently effective.
The performance of the prediction algorithms is maximised when pre-processing
the training data with the synthetic minority oversampling technique (SMOTE).
The results of our work suggest that biofeedback and voice analysis can be used
to facilitate prioritization of requirements oriented to product improvement,
and to steer the interview based on users' engagement. Furthermore, the usage
of voice features can be particularly helpful for emotion-aware requirements
elicitation in remote communication, either performed by human analysts or
voice-based chatbots.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferrari_A/0/1/0/all/0/1"&gt;Alessio Ferrari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huichapa_T/0/1/0/all/0/1"&gt;Thaide Huichapa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spoletini_P/0/1/0/all/0/1"&gt;Paola Spoletini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novielli_N/0/1/0/all/0/1"&gt;Nicole Novielli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fucci_D/0/1/0/all/0/1"&gt;Davide Fucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girardi_D/0/1/0/all/0/1"&gt;Daniela Girardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voicy: Zero-Shot Non-Parallel Voice Conversion in Noisy Reverberant Environments. (arXiv:2106.08873v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08873</id>
        <link href="http://arxiv.org/abs/2106.08873"/>
        <updated>2021-06-17T01:58:43.023Z</updated>
        <summary type="html"><![CDATA[Voice Conversion (VC) is a technique that aims to transform the
non-linguistic information of a source utterance to change the perceived
identity of the speaker. While there is a rich literature on VC, most proposed
methods are trained and evaluated on clean speech recordings. However, many
acoustic environments are noisy and reverberant, severely restricting the
applicability of popular VC methods to such scenarios. To address this
limitation, we propose Voicy, a new VC framework particularly tailored for
noisy speech. Our method, which is inspired by the de-noising auto-encoders
framework, is comprised of four encoders (speaker, content, phonetic and
acoustic-ASR) and one decoder. Importantly, Voicy is capable of performing
non-parallel zero-shot VC, an important requirement for any VC system that
needs to work on speakers not seen during training. We have validated our
approach using a noisy reverberant version of the LibriSpeech dataset.
Experimental results show that Voicy outperforms other tested VC techniques in
terms of naturalness and target speaker similarity in noisy reverberant
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mottini_A/0/1/0/all/0/1"&gt;Alejandro Mottini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1"&gt;Jaime Lorenzo-Trueba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlapati_S/0/1/0/all/0/1"&gt;Sri Vishnu Kumar Karlapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drugman_T/0/1/0/all/0/1"&gt;Thomas Drugman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PRASEMap: A Probabilistic Reasoning and Semantic Embedding based Knowledge Graph Alignment System. (arXiv:2106.08801v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08801</id>
        <link href="http://arxiv.org/abs/2106.08801"/>
        <updated>2021-06-17T01:58:43.005Z</updated>
        <summary type="html"><![CDATA[Knowledge Graph (KG) alignment aims at finding equivalent entities and
relations (i.e., mappings) between two KGs. The existing approaches utilize
either reasoning-based or semantic embedding-based techniques, but few studies
explore their combination. In this demonstration, we present PRASEMap, an
unsupervised KG alignment system that iteratively computes the Mappings with
both Probabilistic Reasoning (PR) And Semantic Embedding (SE) techniques.
PRASEMap can support various embedding-based KG alignment approaches as the SE
module, and enables easy human computer interaction that additionally provides
an option for users to feed the mapping annotations back to the system for
better results. The demonstration showcases these features via a stand-alone
Web application with user friendly interfaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhiyuan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Vaccines: Characterizing Misinformation Campaigns and Vaccine Hesitancy on Twitter. (arXiv:2106.08423v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.08423</id>
        <link href="http://arxiv.org/abs/2106.08423"/>
        <updated>2021-06-17T01:58:42.999Z</updated>
        <summary type="html"><![CDATA[Vaccine hesitancy and misinformation on social media has increased concerns
about COVID-19 vaccine uptake required to achieve herd immunity and overcome
the pandemic. However anti-science and political misinformation and
conspiracies have been rampant throughout the pandemic. For COVID-19 vaccines,
we investigate misinformation and conspiracy campaigns and their characteristic
behaviours. We identify whether coordinated efforts are used to promote
misinformation in vaccine related discussions, and find accounts coordinately
promoting a `Great Reset' conspiracy group promoting vaccine related
misinformation and strong anti-vaccine and anti-social messages such as boycott
vaccine passports, no lock-downs and masks. We characterize other
misinformation communities from the information diffusion structure, and study
the large anti-vaccine misinformation community and smaller anti-vaccine
communities, including a far-right anti-vaccine conspiracy group. In comparison
with the mainstream and health news, left-leaning group, which are more
pro-vaccine, the right-leaning group is influenced more by the anti-vaccine and
far-right misinformation/conspiracy communities. The misinformation communities
are more vocal either specific to the vaccine discussion or political
discussion, and we find other differences in the characteristic behaviours of
different communities. Lastly, we investigate misinformation narratives and
tactics of information distortion that can increase vaccine hesitancy, using
topic modeling and comparison with reported vaccine side-effects (VAERS)
finding rarer side-effects are more frequently discussed on social media.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1"&gt;Karishma Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yizhou Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge-Adaptation Priors. (arXiv:2106.08769v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08769</id>
        <link href="http://arxiv.org/abs/2106.08769"/>
        <updated>2021-06-17T01:58:42.993Z</updated>
        <summary type="html"><![CDATA[Humans and animals have a natural ability to quickly adapt to their
surroundings, but machine-learning models, when subjected to changes, often
require a complete retraining from scratch. We present Knowledge-adaptation
priors (K-priors) to reduce the cost of retraining by enabling quick and
accurate adaptation for a wide-variety of tasks and models. This is made
possible by a combination of weight and function-space priors to reconstruct
the gradients of the past, which recovers and generalizes many existing, but
seemingly-unrelated, adaptation strategies. Training with simple first-order
gradient methods can often recover the exact retrained model to an arbitrary
accuracy by choosing a sufficiently large memory of the past data. Empirical
results confirm that the adaptation can be cheap and accurate, and a promising
alternative to retraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swaroop_S/0/1/0/all/0/1"&gt;Siddharth Swaroop&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Breaking The Dimension Dependence in Sparse Distribution Estimation under Communication Constraints. (arXiv:2106.08597v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08597</id>
        <link href="http://arxiv.org/abs/2106.08597"/>
        <updated>2021-06-17T01:58:42.986Z</updated>
        <summary type="html"><![CDATA[We consider the problem of estimating a $d$-dimensional $s$-sparse discrete
distribution from its samples observed under a $b$-bit communication
constraint. The best-known previous result on $\ell_2$ estimation error for
this problem is $O\left( \frac{s\log\left( {d}/{s}\right)}{n2^b}\right)$.
Surprisingly, we show that when sample size $n$ exceeds a minimum threshold
$n^*(s, d, b)$, we can achieve an $\ell_2$ estimation error of $O\left(
\frac{s}{n2^b}\right)$. This implies that when $n>n^*(s, d, b)$ the convergence
rate does not depend on the ambient dimension $d$ and is the same as knowing
the support of the distribution beforehand.

We next ask the question: ``what is the minimum $n^*(s, d, b)$ that allows
dimension-free convergence?''. To upper bound $n^*(s, d, b)$, we develop novel
localization schemes to accurately and efficiently localize the unknown
support. For the non-interactive setting, we show that $n^*(s, d, b) = O\left(
\min \left( {d^2\log^2 d}/{2^b}, {s^4\log^2 d}/{2^b}\right) \right)$. Moreover,
we connect the problem with non-adaptive group testing and obtain a
polynomial-time estimation scheme when $n = \tilde{\Omega}\left({s^4\log^4
d}/{2^b}\right)$. This group testing based scheme is adaptive to the sparsity
parameter $s$, and hence can be applied without knowing it. For the interactive
setting, we propose a novel tree-based estimation scheme and show that the
minimum sample-size needed to achieve dimension-free convergence can be further
reduced to $n^*(s, d, b) = \tilde{O}\left( {s^2\log^2 d}/{2^b} \right)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei-Ning Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kairouz_P/0/1/0/all/0/1"&gt;Peter Kairouz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ozgur_A/0/1/0/all/0/1"&gt;Ayfer &amp;#xd6;zg&amp;#xfc;r&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Unreliable Predictions by Shattering a Neural Network. (arXiv:2106.08365v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08365</id>
        <link href="http://arxiv.org/abs/2106.08365"/>
        <updated>2021-06-17T01:58:42.980Z</updated>
        <summary type="html"><![CDATA[Piecewise linear neural networks can be split into subfunctions, each with
its own activation pattern, domain, and empirical error. Empirical error for
the full network can be written as an expectation over empirical error of
subfunctions. Constructing a generalization bound on subfunction empirical
error indicates that the more densely a subfunction is surrounded by training
samples in representation space, the more reliable its predictions are.
Further, it suggests that models with fewer activation regions generalize
better, and models that abstract knowledge to a greater degree generalize
better, all else equal. We propose not only a theoretical framework to reason
about subfunction error bounds but also a pragmatic way of approximately
evaluating it, which we apply to predicting which samples the network will not
successfully generalize to. We test our method on detection of
misclassification and out-of-distribution samples, finding that it performs
competitively in both cases. In short, some network activation patterns are
associated with higher reliability than others, and these can be identified
using subfunction error bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xu Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1"&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hjelm_D/0/1/0/all/0/1"&gt;Devon Hjelm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1"&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Spiking Neural Network for Image Segmentation. (arXiv:2106.08921v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.08921</id>
        <link href="http://arxiv.org/abs/2106.08921"/>
        <updated>2021-06-17T01:58:42.973Z</updated>
        <summary type="html"><![CDATA[We seek to investigate the scalability of neuromorphic computing for computer
vision, with the objective of replicating non-neuromorphic performance on
computer vision tasks while reducing power consumption. We convert the deep
Artificial Neural Network (ANN) architecture U-Net to a Spiking Neural Network
(SNN) architecture using the Nengo framework. Both rate-based and spike-based
models are trained and optimized for benchmarking performance and power, using
a modified version of the ISBI 2D EM Segmentation dataset consisting of
microscope images of cells. We propose a partitioning method to optimize
inter-chip communication to improve speed and energy efficiency when deploying
multi-chip networks on the Loihi neuromorphic chip. We explore the advantages
of regularizing firing rates of Loihi neurons for converting ANN to SNN with
minimum accuracy loss and optimized energy consumption. We propose a percentile
based regularization loss function to limit the spiking rate of the neuron
between a desired range. The SNN is converted directly from the corresponding
ANN, and demonstrates similar semantic segmentation as the ANN using the same
number of neurons and weights. However, the neuromorphic implementation on the
Intel Loihi neuromorphic chip is over 2x more energy-efficient than
conventional hardware (CPU, GPU) when running online (one image at a time).
These power improvements are achieved without sacrificing the task performance
accuracy of the network, and when all weights (Loihi, CPU, and GPU networks)
are quantized to 8 bits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1"&gt;Kinjal Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hunsberger_E/0/1/0/all/0/1"&gt;Eric Hunsberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batir_S/0/1/0/all/0/1"&gt;Sean Batir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eliasmith_C/0/1/0/all/0/1"&gt;Chris Eliasmith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lorenz System State Stability Identification using Neural Networks. (arXiv:2106.08489v1 [math.DS])]]></title>
        <id>http://arxiv.org/abs/2106.08489</id>
        <link href="http://arxiv.org/abs/2106.08489"/>
        <updated>2021-06-17T01:58:42.956Z</updated>
        <summary type="html"><![CDATA[Nonlinear dynamical systems such as Lorenz63 equations are known to be
chaotic in nature and sensitive to initial conditions. As a result, a small
perturbation in the initial conditions results in deviation in state trajectory
after a few time steps. The algorithms and computational resources needed to
accurately identify the system states vary depending on whether the solution is
in transition region or not. We refer to the transition and non-transition
regions as unstable and stable regions respectively. We label a system state to
be stable if it's immediate past and future states reside in the same regime.
However, at a given time step we don't have the prior knowledge about whether
system is in stable or unstable region. In this paper, we develop and train a
feed forward (multi-layer perceptron) Neural Network to classify the system
states of a Lorenz system as stable and unstable. We pose this task as a
supervised learning problem where we train the neural network on Lorenz system
which have states labeled as stable or unstable. We then test the ability of
the neural network models to identify the stable and unstable states on a
different Lorenz system that is generated using different initial conditions.
We also evaluate the classification performance in the mismatched case i.e.,
when the initial conditions for training and validation data are sampled from
different intervals. We show that certain normalization schemes can greatly
improve the performance of neural networks in especially these mismatched
scenarios. The classification framework developed in the paper can be a
preprocessor for a larger context of sequential decision making framework where
the decision making is performed based on observed stable or unstable states.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Subramanian_M/0/1/0/all/0/1"&gt;Megha Subramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tipireddy_R/0/1/0/all/0/1"&gt;Ramakrishna Tipireddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chatterjee_S/0/1/0/all/0/1"&gt;Samrat Chatterjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask 3D CBCT-to-CT Translation and Organs-at-Risk Segmentation Using Physics-Based Data Augmentation. (arXiv:2103.05690v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05690</id>
        <link href="http://arxiv.org/abs/2103.05690"/>
        <updated>2021-06-17T01:58:42.949Z</updated>
        <summary type="html"><![CDATA[Purpose: In current clinical practice, noisy and artifact-ridden weekly
cone-beam computed tomography (CBCT) images are only used for patient setup
during radiotherapy. Treatment planning is done once at the beginning of the
treatment using high-quality planning CT (pCT) images and manual contours for
organs-at-risk (OARs) structures. If the quality of the weekly CBCT images can
be improved while simultaneously segmenting OAR structures, this can provide
critical information for adapting radiotherapy mid-treatment as well as for
deriving biomarkers for treatment response. Methods: Using a novel
physics-based data augmentation strategy, we synthesize a large dataset of
perfectly/inherently registered planning CT and synthetic-CBCT pairs for
locally advanced lung cancer patient cohort, which are then used in a multitask
3D deep learning framework to simultaneously segment and translate real weekly
CBCT images to high-quality planning CT-like images. Results: We compared the
synthetic CT and OAR segmentations generated by the model to real planning CT
and manual OAR segmentations and showed promising results. The real week 1
(baseline) CBCT images which had an average MAE of 162.77 HU compared to pCT
images are translated to synthetic CT images that exhibit a drastically
improved average MAE of 29.31 HU and average structural similarity of 92% with
the pCT images. The average DICE scores of the 3D organs-at-risk segmentations
are: lungs 0.96, heart 0.88, spinal cord 0.83 and esophagus 0.66. Conclusions:
We demonstrate an approach to translate artifact-ridden CBCT images to high
quality synthetic CT images while simultaneously generating good quality
segmentation masks for different organs-at-risk. This approach could allow
clinicians to adjust treatment plans using only the routine low-quality CBCT
images, potentially improving patient outcomes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dahiya_N/0/1/0/all/0/1"&gt;Navdeep Dahiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1"&gt;Sadegh R Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengpeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Si-Yuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yezzi_A/0/1/0/all/0/1"&gt;Anthony Yezzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1"&gt;Saad Nadeem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning with Uncertain Feedback Graphs. (arXiv:2106.08441v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08441</id>
        <link href="http://arxiv.org/abs/2106.08441"/>
        <updated>2021-06-17T01:58:42.942Z</updated>
        <summary type="html"><![CDATA[Online learning with expert advice is widely used in various machine learning
tasks. It considers the problem where a learner chooses one from a set of
experts to take advice and make a decision. In many learning problems, experts
may be related, henceforth the learner can observe the losses associated with a
subset of experts that are related to the chosen one. In this context, the
relationship among experts can be captured by a feedback graph, which can be
used to assist the learner's decision making. However, in practice, the nominal
feedback graph often entails uncertainties, which renders it impossible to
reveal the actual relationship among experts. To cope with this challenge, the
present work studies various cases of potential uncertainties, and develops
novel online learning algorithms to deal with uncertainties while making use of
the uncertain feedback graph. The proposed algorithms are proved to enjoy
sublinear regret under mild conditions. Experiments on real datasets are
presented to demonstrate the effectiveness of the novel algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghari_P/0/1/0/all/0/1"&gt;Pouya M Ghari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yanning Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Attacks on Deep Models for Financial Transaction Records. (arXiv:2106.08361v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08361</id>
        <link href="http://arxiv.org/abs/2106.08361"/>
        <updated>2021-06-17T01:58:42.936Z</updated>
        <summary type="html"><![CDATA[Machine learning models using transaction records as inputs are popular among
financial institutions. The most efficient models use deep-learning
architectures similar to those in the NLP community, posing a challenge due to
their tremendous number of parameters and limited robustness. In particular,
deep-learning models are vulnerable to adversarial attacks: a little change in
the input harms the model's output.

In this work, we examine adversarial attacks on transaction records data and
defences from these attacks. The transaction records data have a different
structure than the canonical NLP or time series data, as neighbouring records
are less connected than words in sentences, and each record consists of both
discrete merchant code and continuous transaction amount. We consider a
black-box attack scenario, where the attack doesn't know the true decision
model, and pay special attention to adding transaction tokens to the end of a
sequence. These limitations provide more realistic scenario, previously
unexplored in NLP world.

The proposed adversarial attacks and the respective defences demonstrate
remarkable performance using relevant datasets from the financial industry. Our
results show that a couple of generated transactions are sufficient to fool a
deep-learning model. Further, we improve model robustness via adversarial
training or separate adversarial examples detection. This work shows that
embedding protection from adversarial attacks improves model robustness,
allowing a wider adoption of deep models for transaction records in banking and
finance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fursov_I/0/1/0/all/0/1"&gt;Ivan Fursov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morozov_M/0/1/0/all/0/1"&gt;Matvey Morozov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaploukhaya_N/0/1/0/all/0/1"&gt;Nina Kaploukhaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovtun_E/0/1/0/all/0/1"&gt;Elizaveta Kovtun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rivera_Castro_R/0/1/0/all/0/1"&gt;Rodrigo Rivera-Castro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gusev_G/0/1/0/all/0/1"&gt;Gleb Gusev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babaev_D/0/1/0/all/0/1"&gt;Dmitry Babaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kireev_I/0/1/0/all/0/1"&gt;Ivan Kireev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1"&gt;Alexey Zaytsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainers in the Wild: Making Surrogate Explainers Robust to Distortions through Perception. (arXiv:2102.10951v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10951</id>
        <link href="http://arxiv.org/abs/2102.10951"/>
        <updated>2021-06-17T01:58:42.901Z</updated>
        <summary type="html"><![CDATA[Explaining the decisions of models is becoming pervasive in the image
processing domain, whether it is by using post-hoc methods or by creating
inherently interpretable models. While the widespread use of surrogate
explainers is a welcome addition to inspect and understand black-box models,
assessing the robustness and reliability of the explanations is key for their
success. Additionally, whilst existing work in the explainability field
proposes various strategies to address this problem, the challenges of working
with data in the wild is often overlooked. For instance, in image
classification, distortions to images can not only affect the predictions
assigned by the model, but also the explanation. Given a clean and a distorted
version of an image, even if the prediction probabilities are similar, the
explanation may still be different. In this paper we propose a methodology to
evaluate the effect of distortions in explanations by embedding perceptual
distances that tailor the neighbourhoods used to training surrogate explainers.
We also show that by operating in this way, we can make the explanations more
robust to distortions. We generate explanations for images in the Imagenet-C
dataset and demonstrate how using a perceptual distances in the surrogate
explainer creates more coherent explanations for the distorted and reference
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1"&gt;Alexander Hepburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1"&gt;Raul Santos-Rodriguez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of Automated Machine Learning Tools for SMS Spam Message Filtering. (arXiv:2106.08671v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08671</id>
        <link href="http://arxiv.org/abs/2106.08671"/>
        <updated>2021-06-17T01:58:42.886Z</updated>
        <summary type="html"><![CDATA[Short Message Service (SMS) is a very popular service used for communication
by mobile users. However, this popular service can be abused by executing
illegal activities and influencing security risks. Nowadays, many automatic
machine learning (AutoML) tools exist which can help domain experts and lay
users to build high-quality ML models with little or no machine learning
knowledge. In this work, a classification performance comparison was conducted
between three automatic ML tools for SMS spam message filtering. These tools
are mljar-supervised AutoML, H2O AutoML, and Tree-based Pipeline Optimization
Tool (TPOT) AutoML. Experimental results showed that ensemble models achieved
the best classification performance. The Stacked Ensemble model, which was
built using H2O AutoML, achieved the best performance in terms of Log Loss
(0.8370), true positive (1088/1116), and true negative (281/287) metrics. There
is a 19.05\% improvement in Log Loss with respect to TPOT AutoML and 10.53\%
improvement with respect to mljar-supervised AutoML. The satisfactory filtering
performance achieved with AutoML tools provides a potential application for
AutoML tools to automatically determine the best ML model that can perform best
for SMS spam message filtering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saeed_W/0/1/0/all/0/1"&gt;Waddah Saeed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imperfect ImaGANation: Implications of GANs Exacerbating Biases on Facial Data Augmentation and Snapchat Selfie Lenses. (arXiv:2001.09528v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09528</id>
        <link href="http://arxiv.org/abs/2001.09528"/>
        <updated>2021-06-17T01:58:42.880Z</updated>
        <summary type="html"><![CDATA[In this paper, we show that popular Generative Adversarial Networks (GANs)
exacerbate biases along the axes of gender and skin tone when given a skewed
distribution of face-shots. While practitioners celebrate synthetic data
generation using GANs as an economical way to augment data for training
data-hungry machine learning models, it is unclear whether they recognize the
perils of such techniques when applied to real world datasets biased along
latent dimensions. Specifically, we show that (1) traditional GANs further skew
the distribution of a dataset consisting of engineering faculty headshots,
generating minority modes less often and of worse quality and (2)
image-to-image translation (conditional) GANs also exacerbate biases by
lightening skin color of non-white faces and transforming female facial
features to be masculine when generating faces of engineering professors. Thus,
our study is meant to serve as a cautionary tale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1"&gt;Niharika Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olmo_A/0/1/0/all/0/1"&gt;Alberto Olmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1"&gt;Sailik Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manikonda_L/0/1/0/all/0/1"&gt;Lydia Manikonda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1"&gt;Subbarao Kambhampati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Developing a Fidelity Evaluation Approach for Interpretable Machine Learning. (arXiv:2106.08492v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08492</id>
        <link href="http://arxiv.org/abs/2106.08492"/>
        <updated>2021-06-17T01:58:42.871Z</updated>
        <summary type="html"><![CDATA[Although modern machine learning and deep learning methods allow for complex
and in-depth data analytics, the predictive models generated by these methods
are often highly complex, and lack transparency. Explainable AI (XAI) methods
are used to improve the interpretability of these complex models, and in doing
so improve transparency. However, the inherent fitness of these explainable
methods can be hard to evaluate. In particular, methods to evaluate the
fidelity of the explanation to the underlying black box require further
development, especially for tabular data. In this paper, we (a) propose a three
phase approach to developing an evaluation method; (b) adapt an existing
evaluation method primarily for image and text data to evaluate models trained
on tabular data; and (c) evaluate two popular explainable methods using this
evaluation method. Our evaluations suggest that the internal mechanism of the
underlying predictive model, the internal mechanism of the explainable method
used and model and data complexity all affect explanation fidelity. Given that
explanation fidelity is so sensitive to context and tools and data used, we
could not clearly identify any specific explainable method as being superior to
another.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Velmurugan_M/0/1/0/all/0/1"&gt;Mythreyi Velmurugan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1"&gt;Chun Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreira_C/0/1/0/all/0/1"&gt;Catarina Moreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sindhgatta_R/0/1/0/all/0/1"&gt;Renuka Sindhgatta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Evaluating Racial Biases in Image Captioning. (arXiv:2106.08503v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08503</id>
        <link href="http://arxiv.org/abs/2106.08503"/>
        <updated>2021-06-17T01:58:42.865Z</updated>
        <summary type="html"><![CDATA[Image captioning is an important task for benchmarking visual reasoning and
for enabling accessibility for people with vision impairments. However, as in
many machine learning settings, social biases can influence image captioning in
undesirable ways. In this work, we study bias propagation pathways within image
captioning, focusing specifically on the COCO dataset. Prior work has analyzed
gender bias in captions using automatically-derived gender labels; here we
examine racial and intersectional biases using manual annotations. Our first
contribution is in annotating the perceived gender and skin color of 28,315 of
the depicted people after obtaining IRB approval. Using these annotations, we
compare racial biases present in both manual and automatically-generated image
captions. We demonstrate differences in caption performance, sentiment, and
word choice between images of lighter versus darker-skinned people. Further, we
find the magnitude of these differences to be greater in modern captioning
systems compared to older ones, thus leading to concerns that without proper
consideration and mitigation these differences will only become increasingly
prevalent. Code and data is available at
https://princetonvisualai.github.io/imagecaptioning-bias .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dora Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Angelina Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1"&gt;Olga Russakovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evidence-based Factual Error Correction. (arXiv:2012.15788v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15788</id>
        <link href="http://arxiv.org/abs/2012.15788"/>
        <updated>2021-06-17T01:58:42.859Z</updated>
        <summary type="html"><![CDATA[This paper introduces the task of factual error correction: performing edits
to a claim so that the generated rewrite is better supported by evidence. This
extends the well-studied task of fact verification by providing a mechanism to
correct written texts that are refuted or only partially supported by evidence.
We demonstrate that it is feasible to train factual error correction systems
from existing fact checking datasets which only contain labeled claims
accompanied by evidence, but not the correction. We achieve this by employing a
two-stage distant supervision approach that incorporates evidence into masked
claims when generating corrections. Our approach, based on the T5 transformer
and using retrieved evidence, achieved better results than existing work which
used a pointer copy network and gold evidence, producing accurate factual error
corrections for 5x more instances in human evaluation and a .125 increase in
SARI score. The evaluation is conducted on a dataset of 65,000 instances based
on a recent fact verification shared task and we release it to enable further
work on the task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1"&gt;James Thorne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1"&gt;Andreas Vlachos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LaneAF: Robust Multi-Lane Detection with Affinity Fields. (arXiv:2103.12040v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12040</id>
        <link href="http://arxiv.org/abs/2103.12040"/>
        <updated>2021-06-17T01:58:42.840Z</updated>
        <summary type="html"><![CDATA[This study presents an approach to lane detection involving the prediction of
binary segmentation masks and per-pixel affinity fields. These affinity fields,
along with the binary masks, can then be used to cluster lane pixels
horizontally and vertically into corresponding lane instances in a
post-processing step. This clustering is achieved through a simple row-by-row
decoding process with little overhead; such an approach allows LaneAF to detect
a variable number of lanes without assuming a fixed or maximum number of lanes.
Moreover, this form of clustering is more interpretable in comparison to
previous visual clustering approaches, and can be analyzed to identify and
correct sources of error. Qualitative and quantitative results obtained on
popular lane detection datasets demonstrate the model's ability to detect and
cluster lanes effectively and robustly. Our proposed approach sets a new
state-of-the-art on the challenging CULane dataset and the recently introduced
Unsupervised LLAMAS dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abualsaud_H/0/1/0/all/0/1"&gt;Hala Abualsaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sean Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1"&gt;David Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Situ_K/0/1/0/all/0/1"&gt;Kenny Situ&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1"&gt;Akshay Rangesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1"&gt;Mohan M. Trivedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Silhouettes and quasi residual plots for neural nets and tree-based classifiers. (arXiv:2106.08814v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08814</id>
        <link href="http://arxiv.org/abs/2106.08814"/>
        <updated>2021-06-17T01:58:42.820Z</updated>
        <summary type="html"><![CDATA[Classification by neural nets and by tree-based methods are powerful tools of
machine learning. There exist interesting visualizations of the inner workings
of these and other classifiers. Here we pursue a different goal, which is to
visualize the cases being classified, either in training data or in test data.
An important aspect is whether a case has been classified to its given class
(label) or whether the classifier wants to assign it to different class. This
is reflected in the (conditional and posterior) probability of the alternative
class (PAC). A high PAC indicates label bias, i.e. the possibility that the
case was mislabeled. The PAC is used to construct a silhouette plot which is
similar in spirit to the silhouette plot for cluster analysis (Rousseeuw,
1987). The average silhouette width can be used to compare different
classifications of the same dataset. We will also draw quasi residual plots of
the PAC versus a data feature, which may lead to more insight in the data. One
of these data features is how far each case lies from its given class. The
graphical displays are illustrated and interpreted on benchmark data sets
containing images, mixed features, and tweets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Raymaekers_J/0/1/0/all/0/1"&gt;Jakob Raymaekers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rousseeuw_P/0/1/0/all/0/1"&gt;Peter J. Rousseeuw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion Dynamics in Movie Dialogues. (arXiv:2103.01345v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01345</id>
        <link href="http://arxiv.org/abs/2103.01345"/>
        <updated>2021-06-17T01:58:42.811Z</updated>
        <summary type="html"><![CDATA[Emotion dynamics is a framework for measuring how an individual's emotions
change over time. It is a powerful tool for understanding how we behave and
interact with the world. In this paper, we introduce a framework to track
emotion dynamics through one's utterances. Specifically we introduce a number
of utterance emotion dynamics (UED) metrics inspired by work in Psychology. We
use this approach to trace emotional arcs of movie characters. We analyze
thousands of such character arcs to test hypotheses that inform our broader
understanding of stories. Notably, we show that there is a tendency for
characters to use increasingly more negative words and become increasingly
emotionally discordant with each other until about 90 percent of the narrative
length. UED also has applications in behavior studies, social sciences, and
public health.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hipson_W/0/1/0/all/0/1"&gt;Will E. Hipson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1"&gt;Saif M. Mohammad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the long-term learning ability of LSTM LMs. (arXiv:2106.08927v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08927</id>
        <link href="http://arxiv.org/abs/2106.08927"/>
        <updated>2021-06-17T01:58:42.805Z</updated>
        <summary type="html"><![CDATA[We inspect the long-term learning ability of Long Short-Term Memory language
models (LSTM LMs) by evaluating a contextual extension based on the Continuous
Bag-of-Words (CBOW) model for both sentence- and discourse-level LSTM LMs and
by analyzing its performance. We evaluate on text and speech. Sentence-level
models using the long-term contextual module perform comparably to vanilla
discourse-level LSTM LMs. On the other hand, the extension does not provide
gains for discourse-level models. These findings indicate that discourse-level
LSTM LMs already rely on contextual information to perform long-term learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boes_W/0/1/0/all/0/1"&gt;Wim Boes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rompaey_R/0/1/0/all/0/1"&gt;Robbe Van Rompaey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verwimp_L/0/1/0/all/0/1"&gt;Lyan Verwimp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelemans_J/0/1/0/all/0/1"&gt;Joris Pelemans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+hamme_H/0/1/0/all/0/1"&gt;Hugo Van hamme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wambacq_P/0/1/0/all/0/1"&gt;Patrick Wambacq&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Black-Box Dissector: Towards Erasing-based Hard-Label Model Stealing Attack. (arXiv:2105.00623v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00623</id>
        <link href="http://arxiv.org/abs/2105.00623"/>
        <updated>2021-06-17T01:58:42.779Z</updated>
        <summary type="html"><![CDATA[Previous studies have verified that the functionality of black-box models can
be stolen with full probability outputs. However, under the more practical
hard-label setting, we observe that existing methods suffer from catastrophic
performance degradation. We argue this is due to the lack of rich information
in the probability prediction and the overfitting caused by hard labels. To
this end, we propose a novel hard-label model stealing method termed
\emph{black-box dissector}, which consists of two erasing-based modules. One is
a CAM-driven erasing strategy that is designed to increase the information
capacity hidden in hard labels from the victim model. The other is a
random-erasing-based self-knowledge distillation module that utilizes soft
labels from the substitute model to mitigate overfitting. Extensive experiments
on four widely-used datasets consistently demonstrate that our method
outperforms state-of-the-art methods, with an improvement of at most $8.27\%$.
We also validate the effectiveness and practical potential of our method on
real-world APIs and defense methods. Furthermore, our method promotes other
downstream tasks, \emph{i.e.}, transfer adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongjian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-trained Weights in Wide Neural Networks Align Layerwise to Error-scaled Input Correlations. (arXiv:2106.08453v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08453</id>
        <link href="http://arxiv.org/abs/2106.08453"/>
        <updated>2021-06-17T01:58:42.765Z</updated>
        <summary type="html"><![CDATA[Recent works have examined how deep neural networks, which can solve a
variety of difficult problems, incorporate the statistics of training data to
achieve their success. However, existing results have been established only in
limited settings. In this work, we derive the layerwise weight dynamics of
infinite-width neural networks with nonlinear activations trained by gradient
descent. We show theoretically that weight updates are aligned with input
correlations from intermediate layers weighted by error, and demonstrate
empirically that the result also holds in finite-width wide networks. The
alignment result allows us to formulate backpropagation-free learning rules,
named Align-zero and Align-ada, that theoretically achieve the same alignment
as backpropagation. Finally, we test these learning rules on benchmark problems
in feedforward and recurrent neural networks and demonstrate, in wide networks,
comparable performance to backpropagation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boopathy_A/0/1/0/all/0/1"&gt;Akhilan Boopathy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiete_I/0/1/0/all/0/1"&gt;Ila Fiete&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaze Preserving CycleGANs for Eyeglass Removal & Persistent Gaze Estimation. (arXiv:2002.02077v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02077</id>
        <link href="http://arxiv.org/abs/2002.02077"/>
        <updated>2021-06-17T01:58:42.757Z</updated>
        <summary type="html"><![CDATA[A driver's gaze is critical for determining their attention, state,
situational awareness, and readiness to take over control from partially
automated vehicles. Estimating the gaze direction is the most obvious way to
gauge a driver's state under ideal conditions when limited to using
non-intrusive imaging sensors. Unfortunately, the vehicular environment
introduces a variety of challenges that are usually unaccounted for - harsh
illumination, nighttime conditions, and reflective eyeglasses. Relying on head
pose alone under such conditions can prove to be unreliable and erroneous. In
this study, we offer solutions to address these problems encountered in the
real world. To solve issues with lighting, we demonstrate that using an
infrared camera with suitable equalization and normalization suffices. To
handle eyeglasses and their corresponding artifacts, we adopt image-to-image
translation using generative adversarial networks to pre-process images prior
to gaze estimation. Our proposed Gaze Preserving CycleGAN (GPCycleGAN) is
trained to preserve the driver's gaze while removing potential eyeglasses from
face images. GPCycleGAN is based on the well-known CycleGAN approach - with the
addition of a gaze classifier and a gaze consistency loss for additional
supervision. Our approach exhibits improved performance, interpretability,
robustness and superior qualitative results on challenging real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1"&gt;Akshay Rangesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bowen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1"&gt;Mohan M. Trivedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LARNet: Lie Algebra Residual Network for Face Recognition. (arXiv:2103.08147v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08147</id>
        <link href="http://arxiv.org/abs/2103.08147"/>
        <updated>2021-06-17T01:58:42.751Z</updated>
        <summary type="html"><![CDATA[Face recognition is an important yet challenging problem in computer vision.
A major challenge in practical face recognition applications lies in
significant variations between profile and frontal faces. Traditional
techniques address this challenge either by synthesizing frontal faces or by
pose invariant learning. In this paper, we propose a novel method with Lie
algebra theory to explore how face rotation in the 3D space affects the deep
feature generation process of convolutional neural networks (CNNs). We prove
that face rotation in the image space is equivalent to an additive residual
component in the feature space of CNNs, which is determined solely by the
rotation. Based on this theoretical finding, we further design a Lie Algebraic
Residual Network (LARNet) for tackling pose robust face recognition. Our LARNet
consists of a residual subnet for decoding rotation information from input face
images, and a gating subnet to learn rotation magnitude for controlling the
strength of the residual component contributing to the feature learning
process. Comprehensive experimental evaluations on both frontal-profile face
datasets and general face recognition datasets convincingly demonstrate that
our method consistently outperforms the state-of-the-art ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaolong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaohong Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1"&gt;Dihong Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1"&gt;Dong-Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Feature Alignment for Adversarial Training. (arXiv:2105.15157v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15157</id>
        <link href="http://arxiv.org/abs/2105.15157"/>
        <updated>2021-06-17T01:58:42.744Z</updated>
        <summary type="html"><![CDATA[Recent studies reveal that Convolutional Neural Networks (CNNs) are typically
vulnerable to adversarial attacks, which pose a threat to security-sensitive
applications. Many adversarial defense methods improve robustness at the cost
of accuracy, raising the contradiction between standard and adversarial
accuracies. In this paper, we observe an interesting phenomenon that feature
statistics change monotonically and smoothly w.r.t the rising of attacking
strength. Based on this observation, we propose the adaptive feature alignment
(AFA) to generate features of arbitrary attacking strengths. Our method is
trained to automatically align features of arbitrary attacking strength. This
is done by predicting a fusing weight in a dual-BN architecture. Unlike
previous works that need to either retrain the model or manually tune a
hyper-parameters for different attacking strengths, our method can deal with
arbitrary attacking strengths with a single model without introducing any
hyper-parameter. Importantly, our method improves the model robustness against
adversarial samples without incurring much loss in standard accuracy.
Experiments on CIFAR-10, SVHN, and tiny-ImageNet datasets demonstrate that our
method outperforms the state-of-the-art under a wide range of attacking
strengths.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruixin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xingyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuge Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaoxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Social Distance Estimation From Images: Performance Evaluation, Test Benchmark, and Algorithm. (arXiv:2103.06759v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06759</id>
        <link href="http://arxiv.org/abs/2103.06759"/>
        <updated>2021-06-17T01:58:42.733Z</updated>
        <summary type="html"><![CDATA[The COVID-19 virus has caused a global pandemic since March 2020. The World
Health Organization (WHO) has provided guidelines on how to reduce the spread
of the virus and one of the most important measures is social distancing.
Maintaining a minimum of one meter distance from other people is strongly
suggested to reduce the risk of infection. This has created a strong interest
in monitoring the social distances either as a safety measure or to study how
the measures have affected human behavior and country-wise differences in this.
The need for automatic social distance estimation algorithms is evident, but
there is no suitable test benchmark for such algorithms. Collecting images with
measured ground-truth pair-wise distances between all the people using
different camera settings is cumbersome. Furthermore, performance evaluation
for social distance estimation algorithms is not straightforward and there is
no widely accepted evaluation protocol. In this paper, we provide a dataset of
varying images with measured pair-wise social distances under different camera
positionings and focal length values. We suggest a performance evaluation
protocol and provide a benchmark to easily evaluate social distance estimation
algorithms. We also propose a method for automatic social distance estimation.
Our method takes advantage of object detection and human pose estimation. It
can be applied on any single image as long as focal length and sensor size
information are known. The results on our benchmark are encouraging with 92%
human detection rate and only 28.9% average error in distance estimation among
the detected people.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seker_M/0/1/0/all/0/1"&gt;Mert Seker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannisto_A/0/1/0/all/0/1"&gt;Anssi M&amp;#xe4;nnist&amp;#xf6;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1"&gt;Jenni Raitoharju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-MAN: Explaining multiple sources of anomalies in video. (arXiv:2106.08856v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08856</id>
        <link href="http://arxiv.org/abs/2106.08856"/>
        <updated>2021-06-17T01:58:42.690Z</updated>
        <summary type="html"><![CDATA[Our objective is to detect anomalies in video while also automatically
explaining the reason behind the detector's response. In a practical sense,
explainability is crucial for this task as the required response to an anomaly
depends on its nature and severity. However, most leading methods (based on
deep neural networks) are not interpretable and hide the decision making
process in uninterpretable feature representations. In an effort to tackle this
problem we make the following contributions: (1) we show how to build
interpretable feature representations suitable for detecting anomalies with
state of the art performance, (2) we propose an interpretable probabilistic
anomaly detector which can describe the reason behind it's response using high
level concepts, (3) we are the first to directly consider object interactions
for anomaly detection and (4) we propose a new task of explaining anomalies and
release a large dataset for evaluating methods on this task. Our method
competes well with the state of the art on public datasets while also providing
anomaly explanation based on objects and their interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Szymanowicz_S/0/1/0/all/0/1"&gt;Stanislaw Szymanowicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charles_J/0/1/0/all/0/1"&gt;James Charles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1"&gt;Roberto Cipolla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Person Re-identification via Multi-Label Prediction and Classification based on Graph-Structural Insight. (arXiv:2106.08798v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08798</id>
        <link href="http://arxiv.org/abs/2106.08798"/>
        <updated>2021-06-17T01:58:42.664Z</updated>
        <summary type="html"><![CDATA[This paper addresses unsupervised person re-identification (Re-ID) using
multi-label prediction and classification based on graph-structural insight.
Our method extracts features from person images and produces a graph that
consists of the features and a pairwise similarity of them as nodes and edges,
respectively. Based on the graph, the proposed graph structure based
multi-label prediction (GSMLP) method predicts multi-labels by considering the
pairwise similarity and the adjacency node distribution of each node. The
multi-labels created by GSMLP are applied to the proposed selective multi-label
classification (SMLC) loss. SMLC integrates a hard-sample mining scheme and a
multi-label classification. The proposed GSMLP and SMLC boost the performance
of unsupervised person Re-ID without any pre-labelled dataset. Experimental
results justify the superiority of the proposed method in unsupervised person
Re-ID by producing state-of-the-art performance. The source code for this paper
is publicly available on 'https://github.com/uknownpioneer/GSMLP-SMLC.git'.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jongmin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1"&gt;Hyeontaek Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Disentangle GAN Fingerprint for Fake Image Attribution. (arXiv:2106.08749v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08749</id>
        <link href="http://arxiv.org/abs/2106.08749"/>
        <updated>2021-06-17T01:58:42.651Z</updated>
        <summary type="html"><![CDATA[Rapid pace of generative models has brought about new threats to visual
forensics such as malicious personation and digital copyright infringement,
which promotes works on fake image attribution. Existing works on fake image
attribution mainly rely on a direct classification framework. Without
additional supervision, the extracted features could include many
content-relevant components and generalize poorly. Meanwhile, how to obtain an
interpretable GAN fingerprint to explain the decision remains an open question.
Adopting a multi-task framework, we propose a GAN Fingerprint Disentangling
Network (GFD-Net) to simultaneously disentangle the fingerprint from
GAN-generated images and produce a content-irrelevant representation for fake
image attribution. A series of constraints are provided to guarantee the
stability and discriminability of the fingerprint, which in turn helps
content-irrelevant feature extraction. Further, we perform comprehensive
analysis on GAN fingerprint, providing some clues about the properties of GAN
fingerprint and which factors dominate the fingerprint in GAN architecture.
Experiments show that our GFD-Net achieves superior fake image attribution
performance in both closed-world and open-world testing. We also apply our
method in binary fake image detection and exhibit a significant generalization
ability on unseen generators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianyun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Juan Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1"&gt;Qiang Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1"&gt;Jiaqi Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xirong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Sheng Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PGMAN: An Unsupervised Generative Multi-adversarial Network for Pan-sharpening. (arXiv:2012.09054v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09054</id>
        <link href="http://arxiv.org/abs/2012.09054"/>
        <updated>2021-06-17T01:58:42.627Z</updated>
        <summary type="html"><![CDATA[Pan-sharpening aims at fusing a low-resolution (LR) multi-spectral (MS) image
and a high-resolution (HR) panchromatic (PAN) image acquired by a satellite to
generate an HR MS image. Many deep learning based methods have been developed
in the past few years. However, since there are no intended HR MS images as
references for learning, almost all of the existing methods down-sample the MS
and PAN images and regard the original MS images as targets to form a
supervised setting for training. These methods may perform well on the
down-scaled images, however, they generalize poorly to the full-resolution
images. To conquer this problem, we design an unsupervised framework that is
able to learn directly from the full-resolution images without any
preprocessing. The model is built based on a novel generative multi-adversarial
network. We use a two-stream generator to extract the modality-specific
features from the PAN and MS images, respectively, and develop a
dual-discriminator to preserve the spectral and spatial information of the
inputs when performing fusion. Furthermore, a novel loss function is introduced
to facilitate training under the unsupervised setting. Experiments and
comparisons with other state-of-the-art methods on GaoFen-2 and QuickBird
images demonstrate that the proposed method can obtain much better fusion
results on the full-resolution images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huanyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Category- and Instance-Aware Pixel Embedding for Fast Panoptic Segmentation. (arXiv:2009.13342v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13342</id>
        <link href="http://arxiv.org/abs/2009.13342"/>
        <updated>2021-06-17T01:58:42.617Z</updated>
        <summary type="html"><![CDATA[Panoptic segmentation (PS) is a complex scene understanding task that
requires providing high-quality segmentation for both thing objects and stuff
regions. Previous methods handle these two classes with semantic and instance
segmentation modules separately, following with heuristic fusion or additional
modules to resolve the conflicts between the two outputs. This work simplifies
this pipeline of PS by consistently modeling the two classes with a novel PS
framework, which extends a detection model with an extra module to predict
category- and instance-aware pixel embedding (CIAE). CIAE is a novel pixel-wise
embedding feature that encodes both semantic-classification and
instance-distinction information. At the inference process, PS results are
simply derived by assigning each pixel to a detected instance or a stuff class
according to the learned embedding. Our method not only demonstrates fast
inference speed but also the first one-stage method to achieve comparable
performance to two-stage methods on the challenging COCO benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1"&gt;Naiyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Yanhu Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaiqi Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment Analysis Methods. (arXiv:2106.08829v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.08829</id>
        <link href="http://arxiv.org/abs/2106.08829"/>
        <updated>2021-06-17T01:58:42.610Z</updated>
        <summary type="html"><![CDATA[Opinion and sentiment analysis is a vital task to characterize subjective
information in social media posts. In this paper, we present a comprehensive
experimental evaluation and comparison with six state-of-the-art methods, from
which we have re-implemented one of them. In addition, we investigate different
textual and visual feature embeddings that cover different aspects of the
content, as well as the recently introduced multimodal CLIP embeddings.
Experimental results are presented for two different publicly available
benchmark datasets of tweets and corresponding images. In contrast to the
evaluation methodology of previous work, we introduce a reproducible and fair
evaluation scheme to make results comparable. Finally, we conduct an error
analysis to outline the limitations of the methods and possibilities for the
future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheema_G/0/1/0/all/0/1"&gt;Gullal S. Cheema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1"&gt;Sherzod Hakimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1"&gt;Eric M&amp;#xfc;ller-Budack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1"&gt;Ralph Ewerth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions. (arXiv:2012.04293v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04293</id>
        <link href="http://arxiv.org/abs/2012.04293"/>
        <updated>2021-06-17T01:58:42.604Z</updated>
        <summary type="html"><![CDATA[Humans are able to perceive, understand and reason about physical events.
Developing models with similar physical understanding capabilities is a
long-standing goal of artificial intelligence. As a step towards this goal, in
this work, we introduce CRAFT, a new visual question answering dataset that
requires causal reasoning about physical forces and object interactions. It
contains 58K video and question pairs that are generated from 10K videos from
20 different virtual environments, containing various objects in motion that
interact with each other and the scene. Two question categories from CRAFT
include previously studied descriptive and counterfactual questions. Besides,
inspired by the theories of force dynamics in cognitive linguistics, we
introduce new question categories that involve understanding the interactions
of objects through the notions of cause, enable, and prevent. Our results
demonstrate that even though these tasks seem to be simple and intuitive for
humans, the evaluated baseline models, including existing state-of-the-art
methods, do not yet deal with the challenges posed in our benchmark dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ates_T/0/1/0/all/0/1"&gt;Tayfun Ates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atesoglu_M/0/1/0/all/0/1"&gt;Muhammed Samil Atesoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yigit_C/0/1/0/all/0/1"&gt;Cagatay Yigit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kesen_I/0/1/0/all/0/1"&gt;Ilker Kesen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobas_M/0/1/0/all/0/1"&gt;Mert Kobas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1"&gt;Erkut Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1"&gt;Aykut Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goksun_T/0/1/0/all/0/1"&gt;Tilbe Goksun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1"&gt;Deniz Yuret&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured DropConnect for Uncertainty Inference in Image Classification. (arXiv:2106.08624v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08624</id>
        <link href="http://arxiv.org/abs/2106.08624"/>
        <updated>2021-06-17T01:58:42.598Z</updated>
        <summary type="html"><![CDATA[With the complexity of the network structure, uncertainty inference has
become an important task to improve the classification accuracy for artificial
intelligence systems. For image classification tasks, we propose a structured
DropConnect (SDC) framework to model the output of a deep neural network by a
Dirichlet distribution. We introduce a DropConnect strategy on weights in the
fully connected layers during training. In test, we split the network into
several sub-networks, and then model the Dirichlet distribution by match its
moments with the mean and variance of the outputs of these sub-networks. The
entropy of the estimated Dirichlet distribution is finally utilized for
uncertainty inference. In this paper, this framework is implemented on LeNet$5$
and VGG$16$ models for misclassification detection and out-of-distribution
detection on MNIST and CIFAR-$10$ datasets. Experimental results show that the
performance of the proposed SDC can be comparable to other uncertainty
inference methods. Furthermore, the SDC is adapted well to different network
structures with certain generalization capabilities and research prospects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wenqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weidong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhanyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking. (arXiv:2106.08816v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08816</id>
        <link href="http://arxiv.org/abs/2106.08816"/>
        <updated>2021-06-17T01:58:42.591Z</updated>
        <summary type="html"><![CDATA[Recently, the Siamese-based method has stood out from multitudinous tracking
methods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to
various special challenges in UAV tracking, \textit{e.g.}, severe occlusion,
and fast motion, most existing Siamese-based trackers hardly combine superior
performance with high efficiency. To this concern, in this paper, a novel
attentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking.
By virtue of the attention mechanism, the attentional aggregation network (AAN)
is conducted with self-AAN and cross-AAN, raising the expression ability of
features eventually. The former AAN aggregates and models the self-semantic
interdependencies of the single feature map via spatial and channel dimensions.
The latter aims to aggregate the cross-interdependencies of different semantic
features including the location information of anchors. In addition, the dual
features version of the anchor proposal network is proposed to raise the
robustness of proposing anchors, increasing the perception ability to objects
with various scales. Experiments on two well-known authoritative benchmarks are
conducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA
trackers. Besides, real-world tests onboard a typical embedded platform
demonstrate that SiamAPN++ achieves promising tracking results with real-time
speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Ziang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Junjie Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiming Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LCDNet: Deep Loop Closure Detection and Point Cloud Registration for LiDAR SLAM. (arXiv:2103.05056v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05056</id>
        <link href="http://arxiv.org/abs/2103.05056"/>
        <updated>2021-06-17T01:58:42.584Z</updated>
        <summary type="html"><![CDATA[Loop closure detection is an essential component of Simultaneous Localization
and Mapping (SLAM) systems, which reduces the drift accumulated over time. Over
the years, several deep learning approaches have been proposed to address this
task, however their performance has been subpar compared to handcrafted
techniques, especially while dealing with reverse loops. In this paper, we
introduce the novel LCDNet that effectively detects loop closures in LiDAR
point clouds by simultaneously identifying previously visited places and
estimating the 6-DoF relative transformation between the current scan and the
map. LCDNet is composed of a shared encoder, a place recognition head that
extracts global descriptors, and a relative pose head that estimates the
transformation between two point clouds. We introduce a novel relative pose
head based on the unbalanced optimal transport theory that we implement in a
differentiable manner to allow for end-to-end training. Extensive evaluations
of LCDNet on multiple real-world autonomous driving datasets show that our
approach outperforms state-of-the-art loop closure detection and point cloud
registration techniques by a large margin, especially while dealing with
reverse loops. Moreover, we integrate our proposed loop closure detection
approach into a LiDAR SLAM library to provide a complete mapping system and
demonstrate the generalization ability using different sensor setup in an
unseen city.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1"&gt;Daniele Cattaneo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1"&gt;Matteo Vaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1"&gt;Abhinav Valada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSRN: an Efficient Deep Network for Image Relighting. (arXiv:2102.09242v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09242</id>
        <link href="http://arxiv.org/abs/2102.09242"/>
        <updated>2021-06-17T01:58:42.566Z</updated>
        <summary type="html"><![CDATA[Custom and natural lighting conditions can be emulated in images of the scene
during post-editing. Extraordinary capabilities of the deep learning framework
can be utilized for such purpose. Deep image relighting allows automatic photo
enhancement by illumination-specific retouching. Most of the state-of-the-art
methods for relighting are run-time intensive and memory inefficient. In this
paper, we propose an efficient, real-time framework Deep Stacked Relighting
Network (DSRN) for image relighting by utilizing the aggregated features from
input image at different scales. Our model is very lightweight with total size
of about 42 MB and has an average inference time of about 0.0116s for image of
resolution $1024 \times 1024$ which is faster as compared to other multi-scale
models. Our solution is quite robust for translating image color temperature
from input image to target image and also performs moderately for light
gradient generation with respect to the target image. Additionally, we show
that if images illuminated from opposite directions are used as input, the
qualitative results improve over using a single input image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Sourya Dipta Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1"&gt;Nisarg A. Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Saikat Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_H/0/1/0/all/0/1"&gt;Himanshu Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLAWS: Clustering Assisted Weakly Supervised Learning with Normalcy Suppression for Anomalous Event Detection. (arXiv:2011.12077v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12077</id>
        <link href="http://arxiv.org/abs/2011.12077"/>
        <updated>2021-06-17T01:58:42.537Z</updated>
        <summary type="html"><![CDATA[Learning to detect real-world anomalous events through video-level labels is
a challenging task due to the rare occurrence of anomalies as well as noise in
the labels. In this work, we propose a weakly supervised anomaly detection
method which has manifold contributions including1) a random batch based
training procedure to reduce inter-batch correlation, 2) a normalcy suppression
mechanism to minimize anomaly scores of the normal regions of a video by taking
into account the overall information available in one training batch, and 3) a
clustering distance based loss to contribute towards mitigating the label noise
and to produce better anomaly representations by encouraging our model to
generate distinct normal and anomalous clusters. The proposed method
obtains83.03% and 89.67% frame-level AUC performance on the UCF Crime and
ShanghaiTech datasets respectively, demonstrating its superiority over the
existing state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Muhammad Zaigham Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1"&gt;Arif Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1"&gt;Marcella Astrid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seung-Ik Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Diffusion for Dense Depth Estimation from Multi-view Images. (arXiv:2106.08917v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08917</id>
        <link href="http://arxiv.org/abs/2106.08917"/>
        <updated>2021-06-17T01:58:42.529Z</updated>
        <summary type="html"><![CDATA[We present a method to estimate dense depth by optimizing a sparse set of
points such that their diffusion into a depth map minimizes a multi-view
reprojection error from RGB supervision. We optimize point positions, depths,
and weights with respect to the loss by differential splatting that models
points as Gaussians with analytic transmittance. Further, we develop an
efficient optimization routine that can simultaneously optimize the 50k+ points
required for complex scene reconstruction. We validate our routine using ground
truth data and show high reconstruction quality. Then, we apply this to light
field and wider baseline images via self supervision, and show improvements in
both average and outlier error for depth maps diffused from inaccurate sparse
points. Finally, we compare qualitative and quantitative results to image
processing and deep learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Numair Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Min H. Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1"&gt;James Tompkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point and Ask: Incorporating Pointing into Visual Question Answering. (arXiv:2011.13681v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13681</id>
        <link href="http://arxiv.org/abs/2011.13681"/>
        <updated>2021-06-17T01:58:42.514Z</updated>
        <summary type="html"><![CDATA[Visual Question Answering (VQA) has become one of the key benchmarks of
visual recognition progress. Multiple VQA extensions have been explored to
better simulate real-world settings: different question formulations, changing
training and test distributions, conversational consistency in dialogues, and
explanation-based answering. In this work, we further expand this space by
considering visual questions that include a spatial point of reference.
Pointing is a nearly universal gesture among humans, and real-world VQA is
likely to involve a gesture towards the target region.

Concretely, we (1) introduce and motivate point-input questions as an
extension of VQA, (2) define three novel classes of questions within this
space, and (3) for each class, introduce both a benchmark dataset and a series
of baseline models to handle its unique challenges. There are two key
distinctions from prior work. First, we explicitly design the benchmarks to
require the point input, i.e., we ensure that the visual question cannot be
answered accurately without the spatial reference. Second, we explicitly
explore the more realistic point spatial input rather than the standard but
unnatural bounding box input. Through our exploration we uncover and address
several visual recognition challenges, including the ability to infer human
intent, reason both locally and globally about the image, and effectively
combine visual, language and spatial inputs. Code is available at:
https://github.com/princetonvisualai/pointingqa .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mani_A/0/1/0/all/0/1"&gt;Arjun Mani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hinthorn_W/0/1/0/all/0/1"&gt;Will Hinthorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_N/0/1/0/all/0/1"&gt;Nobline Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1"&gt;Olga Russakovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptation with Variational Approximation for Cardiac Segmentation. (arXiv:2106.08752v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08752</id>
        <link href="http://arxiv.org/abs/2106.08752"/>
        <updated>2021-06-17T01:58:42.492Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation is useful in medical image segmentation.
Particularly, when ground truths of the target images are not available, domain
adaptation can train a target-specific model by utilizing the existing labeled
images from other modalities. Most of the reported works mapped images of both
the source and target domains into a common latent feature space, and then
reduced their discrepancy either implicitly with adversarial training or
explicitly by directly minimizing a discrepancy metric. In this work, we
propose a new framework, where the latent features of both domains are driven
towards a common and parameterized variational form, whose conditional
distribution given the image is Gaussian. This is achieved by two networks
based on variational auto-encoders (VAEs) and a regularization for this
variational approximation. Both of the VAEs, each for one domain, contain a
segmentation module, where the source segmentation is trained in a supervised
manner, while the target one is trained unsupervisedly. We validated the
proposed domain adaptation method using two cardiac segmentation tasks, i.e.,
the cross-modality (CT and MR) whole heart segmentation and the cross-sequence
cardiac MR segmentation. Results show that the proposed method achieved better
accuracies compared to two state-of-the-art approaches and demonstrated good
potential for cardiac segmentation. Furthermore, the proposed explicit
regularization was shown to be effective and efficient in narrowing down the
distribution gap between domains, which is useful for unsupervised domain
adaptation. Our code and data has been released via
https://zmiclab.github.io/projects.html.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fuping Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiahai Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Semi-Supervised Object Detection with Soft Teacher. (arXiv:2106.09018v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09018</id>
        <link href="http://arxiv.org/abs/2106.09018"/>
        <updated>2021-06-17T01:58:42.478Z</updated>
        <summary type="html"><![CDATA[This paper presents an end-to-end semi-supervised object detection approach,
in contrast to previous more complex multi-stage methods. The end-to-end
training gradually improves pseudo label qualities during the curriculum, and
the more and more accurate pseudo labels in turn benefit object detection
training. We also propose two simple yet effective techniques within this
framework: a soft teacher mechanism where the classification loss of each
unlabeled bounding box is weighed by the classification score produced by the
teacher network; a box jittering approach to select reliable pseudo boxes for
the learning of box regression. On COCO benchmark, the proposed approach
outperforms previous methods by a large margin under various labeling ratios,
i.e. 1\%, 5\% and 10\%. Moreover, our approach proves to perform also well when
the amount of labeled data is relatively large. For example, it can improve a
40.9 mAP baseline detector trained using the full COCO training set by +3.6
mAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the
state-of-the-art Swin Transformer-based object detector (58.9 mAP on test-dev),
it can still significantly improve the detection accuracy by +1.5 mAP, reaching
60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching
52.4 mAP, pushing the new state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengde Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Fangyun Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiang Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AtrialGeneral: Domain Generalization for Left Atrial Segmentation of Multi-Center LGE MRIs. (arXiv:2106.08727v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08727</id>
        <link href="http://arxiv.org/abs/2106.08727"/>
        <updated>2021-06-17T01:58:42.460Z</updated>
        <summary type="html"><![CDATA[Left atrial (LA) segmentation from late gadolinium enhanced magnetic
resonance imaging (LGE MRI) is a crucial step needed for planning the treatment
of atrial fibrillation. However, automatic LA segmentation from LGE MRI is
still challenging, due to the poor image quality, high variability in LA
shapes, and unclear LA boundary. Though deep learning-based methods can provide
promising LA segmentation results, they often generalize poorly to unseen
domains, such as data from different scanners and/or sites. In this work, we
collect 210 LGE MRIs from different centers with different levels of image
quality. To evaluate the domain generalization ability of models on the LA
segmentation task, we employ four commonly used semantic segmentation networks
for the LA segmentation from multi-center LGE MRIs. Besides, we investigate
three domain generalization strategies, i.e., histogram matching, mutual
information based disentangled representation, and random style transfer, where
a simple histogram matching is proved to be most effective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_V/0/1/0/all/0/1"&gt;Veronika A. Zimmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1"&gt;Julia A. Schnabel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiahai Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$C^3$: Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08914</id>
        <link href="http://arxiv.org/abs/2106.08914"/>
        <updated>2021-06-17T01:58:42.454Z</updated>
        <summary type="html"><![CDATA[Video-grounded dialogue systems aim to integrate video understanding and
dialogue understanding to generate responses that are relevant to both the
dialogue and video context. Most existing approaches employ deep learning
models and have achieved remarkable performance, given the relatively small
datasets available. However, the results are partly accomplished by exploiting
biases in the datasets rather than developing multimodal reasoning, resulting
in limited generalization. In this paper, we propose a novel approach of
Compositional Counterfactual Contrastive Learning ($C^3$) to develop
contrastive training between factual and counterfactual samples in
video-grounded dialogues. Specifically, we design factual/counterfactual
sampling based on the temporal steps in videos and tokens in dialogues and
propose contrastive loss functions that exploit object-level or action-level
variance. Different from prior approaches, we focus on contrastive hidden state
representations among compositional output tokens to optimize the
representation space in a generation setting. We achieved promising performance
gains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the
benefits of our approach in grounding video and dialogue context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hung Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nancy F. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1"&gt;Steven C.H. Hoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds. (arXiv:2007.07978v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.07978</id>
        <link href="http://arxiv.org/abs/2007.07978"/>
        <updated>2021-06-17T01:58:42.448Z</updated>
        <summary type="html"><![CDATA[Forecasting the formation and development of clouds is a central element of
modern weather forecasting systems. Incorrect clouds forecasts can lead to
major uncertainty in the overall accuracy of weather forecasts due to their
intrinsic role in the Earth's climate system. Few studies have tackled this
challenging problem from a machine learning point-of-view due to a shortage of
high-resolution datasets with many historical observations globally. In this
paper, we present a novel satellite-based dataset called ``CloudCast''. It
consists of 70,080 images with 10 different cloud types for multiple layers of
the atmosphere annotated on a pixel level. The spatial resolution of the
dataset is 928 x 1530 pixels (3x3 km per pixel) with 15-min intervals between
frames for the period 2017-01-01 to 2018-12-31. All frames are centered and
projected over Europe. To supplement the dataset, we conduct an evaluation
study with current state-of-the-art video prediction methods such as
convolutional long short-term memory networks, generative adversarial networks,
and optical flow-based extrapolation methods. As the evaluation of video
prediction is difficult in practice, we aim for a thorough evaluation in the
spatial and temporal domain. Our benchmark models show promising results but
with ample room for improvement. This is the first publicly available
global-scale dataset with high-resolution cloud types on a high temporal
granularity to the authors' best knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_A/0/1/0/all/0/1"&gt;A. H. Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;A. Iosifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karstoft_H/0/1/0/all/0/1"&gt;H. Karstoft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Generative Adversarial Networks in One Stage. (arXiv:2103.00430v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00430</id>
        <link href="http://arxiv.org/abs/2103.00430"/>
        <updated>2021-06-17T01:58:42.440Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have demonstrated unprecedented
success in various image generation tasks. The encouraging results, however,
come at the price of a cumbersome training process, during which the generator
and discriminator are alternately updated in two stages. In this paper, we
investigate a general training scheme that enables training GANs efficiently in
only one stage. Based on the adversarial losses of the generator and
discriminator, we categorize GANs into two classes, Symmetric GANs and
Asymmetric GANs, and introduce a novel gradient decomposition method to unify
the two, allowing us to train both classes in one stage and hence alleviate the
training effort. We also computationally analyze the efficiency of the proposed
method, and empirically demonstrate that, the proposed method yields a solid
$1.5\times$ acceleration across various datasets and network architectures.
Furthermore, we show that the proposed method is readily applicable to other
adversarial-training scenarios, such as data-free knowledge distillation. The
code is available at https://github.com/zju-vipa/OSGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chengchao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Youtan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xubin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jie Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Mingli Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Naturalness Evaluation Database for Video Prediction Models. (arXiv:2005.00356v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00356</id>
        <link href="http://arxiv.org/abs/2005.00356"/>
        <updated>2021-06-17T01:58:42.434Z</updated>
        <summary type="html"><![CDATA[The study of video prediction models is believed to be a fundamental approach
to representation learning for videos. While a plethora of generative models
for predicting the future frame pixel values given the past few frames exist,
the quantitative evaluation of the predicted frames has been found to be
extremely challenging. In this context, we introduce the problem of naturalness
evaluation, which refers to how natural or realistic a predicted video looks.
We create the Indian Institute of Science VIdeo Naturalness Evaluation (IISc
VINE) Database consisting of 300 videos, obtained by applying different
prediction models on different datasets, and accompanying human opinion scores.
We collected subjective ratings of naturalness from 50 human participants for
these videos. Our subjective study reveals that human observers were highly
consistent in their judgments of naturalness. We benchmark several popularly
used measures for evaluating video prediction and show that they do not
adequately correlate with these subjective scores. We introduce two new
features to effectively capture naturalness, motion-compensated cosine
similarities of deep features of predicted frames with past frames, and deep
features extracted from rescaled frame differences. We show that our feature
design leads to state of the art naturalness prediction in accordance with
human judgments on our IISc VINE Database. The database and code are publicly
available on our project website:
https://nagabhushansn95.github.io/publications/2020/vine]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Somraj_N/0/1/0/all/0/1"&gt;Nagabhushan Somraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kashi_M/0/1/0/all/0/1"&gt;Manoj Surya Kashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arun_S/0/1/0/all/0/1"&gt;S. P. Arun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soundararajan_R/0/1/0/all/0/1"&gt;Rajiv Soundararajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JRDB-Act: A Large-scale Multi-modal Dataset for Spatio-temporal Action, Social Group and Activity Detection. (arXiv:2106.08827v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08827</id>
        <link href="http://arxiv.org/abs/2106.08827"/>
        <updated>2021-06-17T01:58:42.411Z</updated>
        <summary type="html"><![CDATA[The availability of large-scale video action understanding datasets has
facilitated advances in the interpretation of visual scenes containing people.
However, learning to recognize human activities in an unconstrained real-world
environment, with potentially highly unbalanced and long-tailed distributed
data remains a significant challenge, not least owing to the lack of a
reflective large-scale dataset. Most existing large-scale datasets are either
collected from a specific or constrained environment, e.g. kitchens or rooms,
or video sharing platforms such as YouTube. In this paper, we introduce
JRDB-Act, a multi-modal dataset, as an extension of the existing JRDB, which is
captured by asocial mobile manipulator and reflects a real distribution of
human daily life actions in a university campus environment. JRDB-Act has been
densely annotated with atomic actions, comprises over 2.8M action labels,
constituting a large-scale spatio-temporal action detection dataset. Each human
bounding box is labelled with one pose-based action label and multiple
(optional) interaction-based action labels. Moreover JRDB-Act comes with social
group identification annotations conducive to the task of grouping individuals
based on their interactions in the scene to infer their social activities
(common activities in each social group).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ehsanpour_M/0/1/0/all/0/1"&gt;Mahsa Ehsanpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saleh_F/0/1/0/all/0/1"&gt;Fatemeh Saleh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1"&gt;Ian Reid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1"&gt;Hamid Rezatofighi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SEOVER: Sentence-level Emotion Orientation Vector based Conversation Emotion Recognition Model. (arXiv:2106.08785v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08785</id>
        <link href="http://arxiv.org/abs/2106.08785"/>
        <updated>2021-06-17T01:58:42.405Z</updated>
        <summary type="html"><![CDATA[For the task of conversation emotion recognition, recent works focus on
speaker relationship modeling but ignore the role of utterance's emotional
tendency.In this paper, we propose a new expression paradigm of sentence-level
emotion orientation vector to model the potential correlation of emotions
between sentence vectors. Based on it, we design an emotion recognition model,
which extracts the sentence-level emotion orientation vectors from the language
model and jointly learns from the dialogue sentiment analysis model and
extracted sentence-level emotion orientation vectors to identify the speaker's
emotional orientation during the conversation. We conduct experiments on two
benchmark datasets and compare them with the five baseline models.The
experimental results show that our model has better performance on all data
sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zaijing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1"&gt;Fengxiao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1"&gt;Tieyu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yusen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Ming Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predictive coding feedback results in perceived illusory contours in a recurrent neural network. (arXiv:2102.01955v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01955</id>
        <link href="http://arxiv.org/abs/2102.01955"/>
        <updated>2021-06-17T01:58:42.399Z</updated>
        <summary type="html"><![CDATA[Modern feedforward convolutional neural networks (CNNs) can now solve some
computer vision tasks at super-human levels. However, these networks only
roughly mimic human visual perception. One difference from human vision is that
they do not appear to perceive illusory contours (e.g. Kanizsa squares) in the
same way humans do. Physiological evidence from visual cortex suggests that the
perception of illusory contours could involve feedback connections. Would
recurrent feedback neural networks perceive illusory contours like humans? In
this work we equip a deep feedforward convolutional network with brain-inspired
recurrent dynamics. The network was first pretrained with an unsupervised
reconstruction objective on a natural image dataset, to expose it to natural
object contour statistics. Then, a classification decision layer was added and
the model was finetuned on a form discrimination task: squares vs. randomly
oriented inducer shapes (no illusory contour). Finally, the model was tested
with the unfamiliar ''illusory contour'' configuration: inducer shapes oriented
to form an illusory square. Compared with feedforward baselines, the iterative
''predictive coding'' feedback resulted in more illusory contours being
classified as physical squares. The perception of the illusory contour was
measurable in the luminance profile of the image reconstructions produced by
the model, demonstrating that the model really ''sees'' the illusion. Ablation
studies revealed that natural image pretraining and feedback error correction
are both critical to the perception of the illusion. Finally we validated our
conclusions in a deeper network (VGG): adding the same predictive coding
feedback dynamics again leads to the perception of illusory contours.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_Z/0/1/0/all/0/1"&gt;Zhaoyang Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OMay_C/0/1/0/all/0/1"&gt;Callum Biggs O&amp;#x27;May&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choksi_B/0/1/0/all/0/1"&gt;Bhavin Choksi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1"&gt;Rufin VanRullen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathological voice adaptation with autoencoder-based voice conversion. (arXiv:2106.08427v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08427</id>
        <link href="http://arxiv.org/abs/2106.08427"/>
        <updated>2021-06-17T01:58:42.393Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a new approach to pathological speech synthesis.
Instead of using healthy speech as a source, we customise an existing
pathological speech sample to a new speaker's voice characteristics. This
approach alleviates the evaluation problem one normally has when converting
typical speech to pathological speech, as in our approach, the voice conversion
(VC) model does not need to be optimised for speech degradation but only for
the speaker change. This change in the optimisation ensures that any
degradation found in naturalness is due to the conversion process and not due
to the model exaggerating characteristics of a speech pathology. To show a
proof of concept of this method, we convert dysarthric speech using the
UASpeech database and an autoencoder-based VC technique. Subjective evaluation
results show reasonable naturalness for high intelligibility dysarthric
speakers, though lower intelligibility seems to introduce a marginal
degradation in naturalness scores for mid and low intelligibility speakers
compared to ground truth. Conversion of speaker characteristics for low and
high intelligibility speakers is successful, but not for mid. Whether the
differences in the results for the different intelligibility levels is due to
the intelligibility levels or due to the speakers needs to be further
investigated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Illa_M/0/1/0/all/0/1"&gt;Marc Illa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halpern_B/0/1/0/all/0/1"&gt;Bence Mark Halpern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Son_R/0/1/0/all/0/1"&gt;Rob van Son&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moro_Velazquez_L/0/1/0/all/0/1"&gt;Laureano Moro-Velazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scharenborg_O/0/1/0/all/0/1"&gt;Odette Scharenborg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved CNN-based Learning of Interpolation Filters for Low-Complexity Inter Prediction in Video Coding. (arXiv:2106.08936v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08936</id>
        <link href="http://arxiv.org/abs/2106.08936"/>
        <updated>2021-06-17T01:58:42.374Z</updated>
        <summary type="html"><![CDATA[The versatility of recent machine learning approaches makes them ideal for
improvement of next generation video compression solutions. Unfortunately,
these approaches typically bring significant increases in computational
complexity and are difficult to interpret into explainable models, affecting
their potential for implementation within practical video coding applications.
This paper introduces a novel explainable neural network-based inter-prediction
scheme, to improve the interpolation of reference samples needed for fractional
precision motion compensation. The approach requires a single neural network to
be trained from which a full quarter-pixel interpolation filter set is derived,
as the network is easily interpretable due to its linear structure. A novel
training framework enables each network branch to resemble a specific
fractional shift. This practical solution makes it very efficient to use
alongside conventional video coding schemes. When implemented in the context of
the state-of-the-art Versatile Video Coding (VVC) test model, 0.77%, 1.27% and
2.25% BD-rate savings can be achieved on average for lower resolution sequences
under the random access, low-delay B and low-delay P configurations,
respectively, while the complexity of the learned interpolation schemes is
significantly reduced compared to the interpolation with full CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1"&gt;Luka Murn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blasi_S/0/1/0/all/0/1"&gt;Saverio Blasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smeaton_A/0/1/0/all/0/1"&gt;Alan F. Smeaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1"&gt;Marta Mrak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-and-Under Complete Convolutional RNN for MRI Reconstruction. (arXiv:2106.08886v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08886</id>
        <link href="http://arxiv.org/abs/2106.08886"/>
        <updated>2021-06-17T01:58:42.368Z</updated>
        <summary type="html"><![CDATA[Reconstructing magnetic resonance (MR) images from undersampled data is a
challenging problem due to various artifacts introduced by the under-sampling
operation. Recent deep learning-based methods for MR image reconstruction
usually leverage a generic auto-encoder architecture which captures low-level
features at the initial layers and high?level features at the deeper layers.
Such networks focus much on global features which may not be optimal to
reconstruct the fully-sampled image. In this paper, we propose an
Over-and-Under Complete Convolu?tional Recurrent Neural Network (OUCR), which
consists of an overcomplete and an undercomplete Convolutional Recurrent Neural
Network(CRNN). The overcomplete branch gives special attention in learning
local structures by restraining the receptive field of the network. Combining
it with the undercomplete branch leads to a network which focuses more on
low-level features without losing out on the global structures. Extensive
experiments on two datasets demonstrate that the proposed method achieves
significant improvements over the compressed sensing and popular deep
learning-based methods with less number of trainable parameters. Our code is
available at https://github.com/guopengf/OUCR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1"&gt;Pengfei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1"&gt;Jeya Maria Jose Valanarasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Puyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shanshan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided interactive image segmentation using machine learning and color based data set clustering. (arXiv:2005.07662v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07662</id>
        <link href="http://arxiv.org/abs/2005.07662"/>
        <updated>2021-06-17T01:58:42.361Z</updated>
        <summary type="html"><![CDATA[We present a novel approach that combines machine learning based interactive
image segmentation with a two-stage clustering method to identify similarly
colored images for efficient batch image segmentation by guided reuse of
classifiers. The segmentation task is formulated as a supervised machine
learning problem working on homogeneous groups of voxels termed supervoxels.
Classifiers are interactively trained from sparse annotations in an iterative
process of annotation refinement. Resulting models can be used for batch
processing of previously unseen images. By clustering images into subsets of
similar colorization, we identify a minimal set of prototype images and
demonstrate that using only classifiers trained on these prototype images for
their color-cluster significantly improves the average segmentation performance
of batch processing. The presented methods are applicable for almost any image
type and therefore represent a useful tool for image analysis tasks in general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Friebel_A/0/1/0/all/0/1"&gt;Adrian Friebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johann_T/0/1/0/all/0/1"&gt;Tim Johann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drasdo_D/0/1/0/all/0/1"&gt;Dirk Drasdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoehme_S/0/1/0/all/0/1"&gt;Stefan Hoehme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch. (arXiv:2106.08970v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08970</id>
        <link href="http://arxiv.org/abs/2106.08970"/>
        <updated>2021-06-17T01:58:42.347Z</updated>
        <summary type="html"><![CDATA[As the curation of data for machine learning becomes increasingly automated,
dataset tampering is a mounting threat. Backdoor attackers tamper with training
data to embed a vulnerability in models that are trained on that data. This
vulnerability is then activated at inference time by placing a "trigger" into
the model's input. Typical backdoor attacks insert the trigger directly into
the training data, although the presence of such an attack may be visible upon
inspection. In contrast, the Hidden Trigger Backdoor Attack achieves poisoning
without placing a trigger into the training data at all. However, this hidden
trigger attack is ineffective at poisoning neural networks trained from
scratch. We develop a new hidden trigger attack, Sleeper Agent, which employs
gradient matching, data selection, and target model re-training during the
crafting process. Sleeper Agent is the first hidden trigger backdoor attack to
be effective against neural networks trained from scratch. We demonstrate its
effectiveness on ImageNet and in black-box settings. Our implementation code
can be found at https://github.com/hsouri/Sleeper-Agent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Souri_H/0/1/0/all/0/1"&gt;Hossein Souri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1"&gt;Liam Fowl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1"&gt;Rama Chellappa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-independent User Simulation with Transformers for Task-oriented Dialogue Systems. (arXiv:2106.08838v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08838</id>
        <link href="http://arxiv.org/abs/2106.08838"/>
        <updated>2021-06-17T01:58:42.340Z</updated>
        <summary type="html"><![CDATA[Dialogue policy optimisation via reinforcement learning requires a large
number of training interactions, which makes learning with real users time
consuming and expensive. Many set-ups therefore rely on a user simulator
instead of humans. These user simulators have their own problems. While
hand-coded, rule-based user simulators have been shown to be sufficient in
small, simple domains, for complex domains the number of rules quickly becomes
intractable. State-of-the-art data-driven user simulators, on the other hand,
are still domain-dependent. This means that adaptation to each new domain
requires redesigning and retraining. In this work, we propose a
domain-independent transformer-based user simulator (TUS). The structure of our
TUS is not tied to a specific domain, enabling domain generalisation and
learning of cross-domain user behaviour from data. We compare TUS with the
state of the art using automatic as well as human evaluations. TUS can compete
with rule-based user simulators on pre-defined domains and is able to
generalise to unseen domains in a zero-shot fashion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hsien-chin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1"&gt;Nurul Lubis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Songbo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1"&gt;Carel van Niekerk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1"&gt;Christian Geishauser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1"&gt;Michael Heck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shutong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1"&gt;Milica Ga&amp;#x161;i&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving filling level classification with adversarial training. (arXiv:2102.04057v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04057</id>
        <link href="http://arxiv.org/abs/2102.04057"/>
        <updated>2021-06-17T01:58:42.334Z</updated>
        <summary type="html"><![CDATA[We investigate the problem of classifying - from a single image - the level
of content in a cup or a drinking glass. This problem is made challenging by
several ambiguities caused by transparencies, shape variations and partial
occlusions, and by the availability of only small training datasets. In this
paper, we tackle this problem with an appropriate strategy for transfer
learning. Specifically, we use adversarial training in a generic source dataset
and then refine the training with a task-specific dataset. We also discuss and
experimentally evaluate several training strategies and their combination on a
range of container types of the CORSMAL Containers Manipulation dataset. We
show that transfer learning with adversarial training in the source domain
consistently improves the classification accuracy on the test set and limits
the overfitting of the classifier to specific features of the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Modas_A/0/1/0/all/0/1"&gt;Apostolos Modas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1"&gt;Alessio Xompero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Matilla_R/0/1/0/all/0/1"&gt;Ricardo Sanchez-Matilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1"&gt;Pascal Frossard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1"&gt;Andrea Cavallaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GelSight Wedge: Measuring High-Resolution 3D Contact Geometry with a Compact Robot Finger. (arXiv:2106.08851v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.08851</id>
        <link href="http://arxiv.org/abs/2106.08851"/>
        <updated>2021-06-17T01:58:42.317Z</updated>
        <summary type="html"><![CDATA[Vision-based tactile sensors have the potential to provide important contact
geometry to localize the objective with visual occlusion. However, it is
challenging to measure high-resolution 3D contact geometry for a compact robot
finger, to simultaneously meet optical and mechanical constraints. In this
work, we present the GelSight Wedge sensor, which is optimized to have a
compact shape for robot fingers, while achieving high-resolution 3D
reconstruction. We evaluate the 3D reconstruction under different lighting
configurations, and extend the method from 3 lights to 1 or 2 lights. We
demonstrate the flexibility of the design by shrinking the sensor to the size
of a human finger for fine manipulation tasks. We also show the effectiveness
and potential of the reconstructed 3D geometry for pose tracking in the 3D
space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shaoxiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+She_Y/0/1/0/all/0/1"&gt;Yu She&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romero_B/0/1/0/all/0/1"&gt;Branden Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adelson_E/0/1/0/all/0/1"&gt;Edward Adelson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent. (arXiv:2009.04709v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04709</id>
        <link href="http://arxiv.org/abs/2009.04709"/>
        <updated>2021-06-17T01:58:42.311Z</updated>
        <summary type="html"><![CDATA[Adversarial training, especially projected gradient descent (PGD), has been a
successful approach for improving robustness against adversarial attacks. After
adversarial training, gradients of models with respect to their inputs have a
preferential direction. However, the direction of alignment is not
mathematically well established, making it difficult to evaluate
quantitatively. We propose a novel definition of this direction as the
direction of the vector pointing toward the closest point of the support of the
closest inaccurate class in decision space. To evaluate the alignment with this
direction after adversarial training, we apply a metric that uses generative
adversarial networks to produce the smallest residual needed to change the
class present in the image. We show that PGD-trained models have a higher
alignment than the baseline according to our definition, that our metric
presents higher alignment values than a competing metric formulation, and that
enforcing this alignment increases the robustness of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lanfredi_R/0/1/0/all/0/1"&gt;Ricardo Bigolin Lanfredi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schroeder_J/0/1/0/all/0/1"&gt;Joyce D. Schroeder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tasdizen_T/0/1/0/all/0/1"&gt;Tolga Tasdizen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning with Continuous Proxy Meta-Data for 3D MRI Classification. (arXiv:2106.08808v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08808</id>
        <link href="http://arxiv.org/abs/2106.08808"/>
        <updated>2021-06-17T01:58:42.304Z</updated>
        <summary type="html"><![CDATA[Traditional supervised learning with deep neural networks requires a
tremendous amount of labelled data to converge to a good solution. For 3D
medical images, it is often impractical to build a large homogeneous annotated
dataset for a specific pathology. Self-supervised methods offer a new way to
learn a representation of the images in an unsupervised manner with a neural
network. In particular, contrastive learning has shown great promises by
(almost) matching the performance of fully-supervised CNN on vision tasks.
Nonetheless, this method does not take advantage of available meta-data, such
as participant's age, viewed as prior knowledge. Here, we propose to leverage
continuous proxy metadata, in the contrastive learning framework, by
introducing a new loss called y-Aware InfoNCE loss. Specifically, we improve
the positive sampling during pre-training by adding more positive examples with
similar proxy meta-data with the anchor, assuming they share similar
discriminative semantic features.With our method, a 3D CNN model pre-trained on
$10^4$ multi-site healthy brain MRI scans can extract relevant features for
three classification tasks: schizophrenia, bipolar diagnosis and Alzheimer's
detection. When fine-tuned, it also outperforms 3D CNN trained from scratch on
these tasks, as well as state-of-the-art self-supervised methods. Our code is
made publicly available here.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dufumier_B/0/1/0/all/0/1"&gt;Benoit Dufumier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1"&gt;Pietro Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Victor_J/0/1/0/all/0/1"&gt;Julie Victor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grigis_A/0/1/0/all/0/1"&gt;Antoine Grigis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wessa_M/0/1/0/all/0/1"&gt;Michel Wessa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brambilla_P/0/1/0/all/0/1"&gt;Paolo Brambilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favre_P/0/1/0/all/0/1"&gt;Pauline Favre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polosan_M/0/1/0/all/0/1"&gt;Mircea Polosan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_C/0/1/0/all/0/1"&gt;Colm McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piguet_C/0/1/0/all/0/1"&gt;Camille Marie Piguet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duchesnay_E/0/1/0/all/0/1"&gt;Edouard Duchesnay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An unifying point of view on expressive power of GNNs. (arXiv:2106.08992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08992</id>
        <link href="http://arxiv.org/abs/2106.08992"/>
        <updated>2021-06-17T01:58:42.297Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are a wide class of connectionist models for
graph processing. They perform an iterative message passing operation on each
node and its neighbors, to solve classification/ clustering tasks --- on some
nodes or on the whole graph --- collecting all such messages, regardless of
their order. Despite the differences among the various models belonging to this
class, most of them adopt the same computation scheme, based on a local
aggregation mechanism and, intuitively, the local computation framework is
mainly responsible for the expressive power of GNNs. In this paper, we prove
that the Weisfeiler--Lehman test induces an equivalence relationship on the
graph nodes that exactly corresponds to the unfolding equivalence, defined on
the original GNN model. Therefore, the results on the expressive power of the
original GNNs can be extended to general GNNs which, under mild conditions, can
be proved capable of approximating, in probability and up to any precision, any
function on graphs that respects the unfolding equivalence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DInverno_G/0/1/0/all/0/1"&gt;Giuseppe Alessio D&amp;#x27;Inverno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bianchini_M/0/1/0/all/0/1"&gt;Monica Bianchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sampoli_M/0/1/0/all/0/1"&gt;Maria Lucia Sampoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1"&gt;Franco Scarselli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing. (arXiv:2011.09899v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09899</id>
        <link href="http://arxiv.org/abs/2011.09899"/>
        <updated>2021-06-17T01:58:42.280Z</updated>
        <summary type="html"><![CDATA[User data confidentiality protection is becoming a rising challenge in the
present deep learning research. Without access to data, conventional
data-driven model compression faces a higher risk of performance degradation.
Recently, some works propose to generate images from a specific pretrained
model to serve as training data. However, the inversion process only utilizes
biased feature statistics stored in one model and is from low-dimension to
high-dimension. As a consequence, it inevitably encounters the difficulties of
generalizability and inexact inversion, which leads to unsatisfactory
performance. To address these problems, we propose MixMix based on two simple
yet effective techniques: (1) Feature Mixing: utilizes various models to
construct a universal feature space for generalized inversion; (2) Data Mixing:
mixes the synthesized images and labels to generate exact label information. We
prove the effectiveness of MixMix from both theoretical and empirical
perspectives. Extensive experiments show that MixMix outperforms existing
methods on the mainstream compression tasks, including quantization, knowledge
distillation, and pruning. Specifically, MixMix achieves up to 4% and 20%
accuracy uplift on quantization and pruning, respectively, compared to existing
data-free compression work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Feng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Ruihao Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1"&gt;Mingzhu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fengwei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shaoqing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shi Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local plasticity rules can learn deep representations using self-supervised contrastive predictions. (arXiv:2010.08262v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08262</id>
        <link href="http://arxiv.org/abs/2010.08262"/>
        <updated>2021-06-17T01:58:42.274Z</updated>
        <summary type="html"><![CDATA[Learning in the brain is poorly understood and learning rules that respect
biological constraints, yet yield deep hierarchical representations, are still
unknown. Here, we propose a learning rule that takes inspiration from
neuroscience and recent advances in self-supervised deep learning. Learning
minimizes a simple layer-specific loss function and does not need to
back-propagate error signals within or between layers. Instead, weight updates
follow a local, Hebbian, learning rule that only depends on pre- and
post-synaptic neuronal activity, predictive dendritic input and widely
broadcasted modulation factors which are identical for large groups of neurons.
The learning rule applies contrastive predictive learning to a causal,
biological setting using saccades (i.e. rapid shifts in gaze direction). We
find that networks trained with this self-supervised and local rule build deep
hierarchical representations of images, speech and video.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Illing_B/0/1/0/all/0/1"&gt;Bernd Illing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventura_J/0/1/0/all/0/1"&gt;Jean Ventura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellec_G/0/1/0/all/0/1"&gt;Guillaume Bellec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1"&gt;Wulfram Gerstner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolving Image Compositions for Feature Representation Learning. (arXiv:2106.09011v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09011</id>
        <link href="http://arxiv.org/abs/2106.09011"/>
        <updated>2021-06-17T01:58:42.253Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks for visual recognition require large amounts of
training samples and usually benefit from data augmentation. This paper
proposes PatchMix, a data augmentation method that creates new samples by
composing patches from pairs of images in a grid-like pattern. These new
samples' ground truth labels are set as proportional to the number of patches
from each image. We then add a set of additional losses at the patch-level to
regularize and to encourage good representations at both the patch and image
levels. A ResNet-50 model trained on ImageNet using PatchMix exhibits superior
transfer learning capabilities across a wide array of benchmarks. Although
PatchMix can rely on random pairings and random grid-like patterns for mixing,
we explore evolutionary search as a guiding strategy to discover optimal
grid-like patterns and image pairing jointly. For this purpose, we conceive a
fitness function that bypasses the need to re-train a model to evaluate each
choice. In this way, PatchMix outperforms a base model on CIFAR-10 (+1.91),
CIFAR-100 (+5.31), Tiny Imagenet (+3.52), and ImageNet (+1.16) by significant
margins, also outperforming previous state-of-the-art pairwise augmentation
strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cascante_Bonilla_P/0/1/0/all/0/1"&gt;Paola Cascante-Bonilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1"&gt;Arshdeep Sekhon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yanjun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1"&gt;Vicente Ordonez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape from Blur: Recovering Textured 3D Shape and Motion of Fast Moving Objects. (arXiv:2106.08762v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08762</id>
        <link href="http://arxiv.org/abs/2106.08762"/>
        <updated>2021-06-17T01:58:42.247Z</updated>
        <summary type="html"><![CDATA[We address the novel task of jointly reconstructing the 3D shape, texture,
and motion of an object from a single motion-blurred image. While previous
approaches address the deblurring problem only in the 2D image domain, our
proposed rigorous modeling of all object properties in the 3D domain enables
the correct description of arbitrary object motion. This leads to significantly
better image decomposition and sharper deblurring results. We model the
observed appearance of a motion-blurred object as a combination of the
background and a 3D object with constant translation and rotation. Our method
minimizes a loss on reconstructing the input image via differentiable rendering
with suitable regularizers. This enables estimating the textured 3D mesh of the
blurred object with high fidelity. Our method substantially outperforms
competing approaches on several benchmarks for fast moving objects deblurring.
Qualitative results show that the reconstructed 3D mesh generates high-quality
temporal super-resolution and novel views of the deblurred object.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rozumnyi_D/0/1/0/all/0/1"&gt;Denys Rozumnyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1"&gt;Martin R. Oswald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1"&gt;Vittorio Ferrari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1"&gt;Marc Pollefeys&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Robotic Weed Control: Detection of Nutsedge Weed in Bermudagrass Turf Using Inaccurate and Insufficient Training Data. (arXiv:2106.08897v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08897</id>
        <link href="http://arxiv.org/abs/2106.08897"/>
        <updated>2021-06-17T01:58:42.230Z</updated>
        <summary type="html"><![CDATA[To enable robotic weed control, we develop algorithms to detect nutsedge weed
from bermudagrass turf. Due to the similarity between the weed and the
background turf, manual data labeling is expensive and error-prone.
Consequently, directly applying deep learning methods for object detection
cannot generate satisfactory results. Building on an instance detection
approach (i.e. Mask R-CNN), we combine synthetic data with raw data to train
the network. We propose an algorithm to generate high fidelity synthetic data,
adopting different levels of annotations to reduce labeling cost. Moreover, we
construct a nutsedge skeleton-based probabilistic map (NSPM) as the neural
network input to reduce the reliance on pixel-wise precise labeling. We also
modify loss function from cross entropy to Kullback-Leibler divergence which
accommodates uncertainty in the labeling process. We implement the proposed
algorithm and compare it with both Faster R-CNN and Mask R-CNN. The results
show that our design can effectively overcome the impact of imprecise and
insufficient training sample issues and significantly outperform the Faster
R-CNN counterpart with a false negative rate of only 0.4%. In particular, our
approach also reduces labeling time by 95% while achieving better performance
if comparing with the original Mask R-CNN approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Shuangyu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Chengsong Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagavathiannan_M/0/1/0/all/0/1"&gt;Muthukumar Bagavathiannan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dezhen Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence. (arXiv:2106.08710v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.08710</id>
        <link href="http://arxiv.org/abs/2106.08710"/>
        <updated>2021-06-17T01:58:42.224Z</updated>
        <summary type="html"><![CDATA[Mobile Augmented Reality (MAR) integrates computer-generated virtual objects
with physical environments for mobile devices. MAR systems enable users to
interact with MAR devices, such as smartphones and head-worn wearables, and
performs seamless transitions from the physical world to a mixed world with
digital entities. These MAR systems support user experiences by using MAR
devices to provide universal accessibility to digital contents. Over the past
20 years, a number of MAR systems have been developed, however, the studies and
design of MAR frameworks have not yet been systematically reviewed from the
perspective of user-centric design. This article presents the first effort of
surveying existing MAR frameworks (count: 37) and further discusses the latest
studies on MAR through a top-down approach: 1) MAR applications; 2) MAR
visualisation techniques adaptive to user mobility and contexts; 3) systematic
evaluation of MAR frameworks including supported platforms and corresponding
features such as tracking, feature extraction plus sensing capabilities; and 4)
underlying machine learning approaches supporting intelligent operations within
MAR systems. Finally, we summarise the development of emerging research fields,
current state-of-the-art, and discuss the important open challenges and
possible theoretical and technical directions. This survey aims to benefit both
researchers and MAR system developers alike.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jacky Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1"&gt;Kit-Yung Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1"&gt;Lik-Hang Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_P/0/1/0/all/0/1"&gt;Pan Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xiang Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimality of short-term synaptic plasticity in modelling certain dynamic environments. (arXiv:2009.06808v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06808</id>
        <link href="http://arxiv.org/abs/2009.06808"/>
        <updated>2021-06-17T01:58:42.206Z</updated>
        <summary type="html"><![CDATA[Biological neurons and their in-silico emulations for neuromorphic artificial
intelligence (AI) use extraordinarily energy-efficient mechanisms, such as
spike-based communication and local synaptic plasticity. It remains unclear
whether these neuronal mechanisms only offer efficiency or also underlie the
superiority of biological intelligence. Here, we prove rigorously that, indeed,
the Bayes-optimal prediction and inference of randomly but continuously
transforming environments, a common natural setting, relies on short-term
spike-timing-dependent plasticity, a hallmark of biological synapses. Further,
this dynamic Bayesian inference through plasticity enables circuits of the
cerebral cortex in simulations to recognize previously unseen, highly distorted
dynamic stimuli. Strikingly, this also introduces a biologically-modelled AI,
the first to overcome multiple limitations of deep learning and outperform
artificial neural networks in a visual task. The cortical-like network is
spiking and event-based, trained only with unsupervised and local plasticity,
on a small, narrow, and static training dataset, but achieves recognition of
unseen, transformed, and dynamic data better than deep neural networks with
continuous activations, trained with supervised backpropagation on the
transforming data. These results link short-term plasticity to high-level
cortical function, suggest optimality of natural intelligence for natural
environments, and repurpose neuromorphic AI from mere efficiency to
computational supremacy altogether.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1"&gt;Timoleon Moraitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1"&gt;Abu Sebastian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eleftheriou_E/0/1/0/all/0/1"&gt;Evangelos Eleftheriou&lt;/a&gt; (IBM Research - Zurich)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Oxford Road Boundaries Dataset. (arXiv:2106.08983v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08983</id>
        <link href="http://arxiv.org/abs/2106.08983"/>
        <updated>2021-06-17T01:58:42.191Z</updated>
        <summary type="html"><![CDATA[In this paper we present the Oxford Road Boundaries Dataset, designed for
training and testing machine-learning-based road-boundary detection and
inference approaches. We have hand-annotated two of the 10 km-long forays from
the Oxford Robotcar Dataset and generated from other forays several thousand
further examples with semi-annotated road-boundary masks. To boost the number
of training samples in this way, we used a vision-based localiser to project
labels from the annotated datasets to other traversals at different times and
weather conditions. As a result, we release 62605 labelled samples, of which
47639 samples are curated. Each of these samples contains both raw and
classified masks for left and right lenses. Our data contains images from a
diverse set of scenarios such as straight roads, parked cars, junctions, etc.
Files for download and tools for manipulating the labelled data are available
at: oxford-robotics-institute.github.io/road-boundaries-dataset]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suleymanov_T/0/1/0/all/0/1"&gt;Tarlan Suleymanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadd_M/0/1/0/all/0/1"&gt;Matthew Gadd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martini_D/0/1/0/all/0/1"&gt;Daniele De Martini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newman_P/0/1/0/all/0/1"&gt;Paul Newman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The shape and simplicity biases of adversarially robust ImageNet-trained CNNs. (arXiv:2006.09373v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09373</id>
        <link href="http://arxiv.org/abs/2006.09373"/>
        <updated>2021-06-17T01:58:42.186Z</updated>
        <summary type="html"><![CDATA[Adversarial training has been the topic of dozens of studies and a leading
method for defending against adversarial attacks. Yet, it remains largely
unknown (a) how adversarially-robust ImageNet classifiers (R classifiers)
generalize to out-of-distribution examples; and (b) how their generalization
capability relates to their hidden representations. In this paper, we perform a
thorough, systematic study to answer these two questions across AlexNet,
GoogLeNet, and ResNet-50 architectures. We found that while standard ImageNet
classifiers have a strong texture bias, their R counterparts rely heavily on
shapes. Remarkably, adversarial training induces three simplicity biases into
hidden neurons in the process of 'robustifying' the network. That is, each
convolutional neuron in R networks often changes to detecting (1) pixel-wise
smoother patterns i.e. a mechanism that blocks high-frequency noise from
passing through the network; (2) more lower-level features i.e. textures and
colors (instead of objects); and (3) fewer types of inputs. Our findings reveal
the interesting mechanisms that made networks more adversarially robust and
also explain some recent findings. Our findings reveal the interesting
mechanisms that made networks more adversarially robust and also explain some
recent findings e.g. why R networks benefit from much larger capacity (Xie and
Yuille, 2020) and can act as a strong image prior in image synthesis (Santurkar
et al., 2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Peijie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1"&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invertible Attention. (arXiv:2106.09003v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09003</id>
        <link href="http://arxiv.org/abs/2106.09003"/>
        <updated>2021-06-17T01:58:42.143Z</updated>
        <summary type="html"><![CDATA[Attention has been proved to be an efficient mechanism to capture long-range
dependencies. However, so far it has not been deployed in invertible networks.
This is due to the fact that in order to make a network invertible, every
component within the network needs to be a bijective transformation, but a
normal attention block is not. In this paper, we propose invertible attention
that can be plugged into existing invertible models. We mathematically and
experimentally prove that the invertibility of an attention model can be
achieved by carefully constraining its Lipschitz constant. We validate the
invertibility of our invertible attention on image reconstruction task with 3
popular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible
attention achieves similar performance in comparison with normal non-invertible
attention on dense prediction tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zha_J/0/1/0/all/0/1"&gt;Jiajun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yiran Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1"&gt;Richard Hartley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Split and Expand: An inference-time improvement for Weakly Supervised Cell Instance Segmentation. (arXiv:2007.10817v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10817</id>
        <link href="http://arxiv.org/abs/2007.10817"/>
        <updated>2021-06-17T01:58:42.082Z</updated>
        <summary type="html"><![CDATA[We consider the problem of segmenting cell nuclei instances from Hematoxylin
and Eosin (H&E) stains with dot annotations only. While most recent works focus
on improving the segmentation quality, this is usually insufficient for
instance segmentation of cell instances clustered together or with a small
size. In this work, we propose a simple two-step post-processing procedure,
Split and Expand, that directly improves the conversion of segmentation maps to
instances. In the splitting step, we generate fine-grained cell instances from
the segmentation map with the guidance of cell-center predictions. For the
expansion step, we utilize Layer-wise Relevance Propagation (LRP) explanation
results to add small cells that are not captured in the segmentation map.
Although we additionally train an output head to predict cell-centers, the
post-processing procedure itself is not explicitly trained and is executed at
inference-time only. A feature re-weighting loss based on LRP is proposed to
improve our method even further. We test our procedure on the MoNuSeg and TNBC
datasets and show quantitatively and qualitatively that our proposed method
improves object-level metrics substantially.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Foo_L/0/1/0/all/0/1"&gt;Lin Geng Foo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiamei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1"&gt;Alexander Binder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ParticleAugment: Sampling-Based Data Augmentation. (arXiv:2106.08693v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08693</id>
        <link href="http://arxiv.org/abs/2106.08693"/>
        <updated>2021-06-17T01:58:42.075Z</updated>
        <summary type="html"><![CDATA[We present an automated data augmentation approach for image classification.
We formulate the problem as Monte Carlo sampling where our goal is to
approximate the optimal augmentation policies. We propose a particle filtering
formulation to find optimal augmentation policies and their schedules during
model training. Our performance measurement procedure relies on a validation
subset of our training set, while the policy transition model depends on a
Gaussian prior and an optional augmentation velocity parameter. In our
experiments, we show that our formulation for automated augmentation reaches
promising results on CIFAR-10, CIFAR-100, and ImageNet datasets using the
standard network architectures for this problem. By comparing with the related
work, we also show that our method reaches a balance between the computational
cost of policy search and the model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsaregorodtsev_A/0/1/0/all/0/1"&gt;Alexander Tsaregorodtsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1"&gt;Vasileios Belagiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polynomial Trajectory Predictions for Improved Learning Performance. (arXiv:2101.12616v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12616</id>
        <link href="http://arxiv.org/abs/2101.12616"/>
        <updated>2021-06-17T01:58:42.068Z</updated>
        <summary type="html"><![CDATA[The rising demand for Active Safety systems in automotive applications
stresses the need for a reliable short to mid-term trajectory prediction.
Anticipating the unfolding path of road users, one can act to increase the
overall safety. In this work, we propose to train artificial neural networks
for movement understanding by predicting trajectories in their natural form, as
a function of time. Predicting polynomial coefficients allows us to increased
accuracy and improve generalisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_I/0/1/0/all/0/1"&gt;Ido Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kummert_A/0/1/0/all/0/1"&gt;Anton Kummert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization. (arXiv:2102.01670v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01670</id>
        <link href="http://arxiv.org/abs/2102.01670"/>
        <updated>2021-06-17T01:58:42.062Z</updated>
        <summary type="html"><![CDATA[Training sparse networks to converge to the same performance as dense neural
architectures has proven to be elusive. Recent work suggests that
initialization is the key. However, while this direction of research has had
some success, focusing on initialization alone appears to be inadequate. In
this paper, we take a broader view of training sparse networks and consider the
role of regularization, optimization, and architecture choices on sparse
models. We propose a simple experimental framework, Same Capacity Sparse vs
Dense Comparison (SC-SDC), that allows for a fair comparison of sparse and
dense networks. Furthermore, we propose a new measure of gradient flow,
Effective Gradient Flow (EGF), that better correlates to performance in sparse
networks. Using top-line metrics, SC-SDC and EGF, we show that default choices
of optimizers, activation functions and regularizers used for dense networks
can disadvantage sparse networks. Based upon these findings, we show that
gradient flow in sparse networks can be improved by reconsidering aspects of
the architecture design and the training regime. Our work suggests that
initialization is only one piece of the puzzle and taking a wider view of
tailoring optimization to sparse networks yields promising results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1"&gt;Kale-ab Tessera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1"&gt;Sara Hooker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosman_B/0/1/0/all/0/1"&gt;Benjamin Rosman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Semantic-to-visual Confusion for Zero-shot Learning. (arXiv:2106.08605v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08605</id>
        <link href="http://arxiv.org/abs/2106.08605"/>
        <updated>2021-06-17T01:58:42.047Z</updated>
        <summary type="html"><![CDATA[Using generative models to synthesize visual features from semantic
distribution is one of the most popular solutions to ZSL image classification
in recent years. The triplet loss (TL) is popularly used to generate realistic
visual distributions from semantics by automatically searching discriminative
representations. However, the traditional TL cannot search reliable unseen
disentangled representations due to the unavailability of unseen classes in
ZSL. To alleviate this drawback, we propose in this work a multi-modal triplet
loss (MMTL) which utilizes multimodal information to search a disentangled
representation space. As such, all classes can interplay which can benefit
learning disentangled class representations in the searched space. Furthermore,
we develop a novel model called Disentangling Class Representation Generative
Adversarial Network (DCR-GAN) focusing on exploiting the disentangled
representations in training, feature synthesis, and final recognition stages.
Benefiting from the disentangled representations, DCR-GAN could fit a more
realistic distribution over both seen and unseen features. Extensive
experiments show that our proposed model can lead to superior performance to
the state-of-the-arts on four benchmark datasets. Our code is available at
https://github.com/FouriYe/DCRGAN-TMM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1"&gt;Zihan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1"&gt;Fuyuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_F/0/1/0/all/0/1"&gt;Fan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the proper role of linguistically-oriented deep net analysis in linguistic theorizing. (arXiv:2106.08694v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08694</id>
        <link href="http://arxiv.org/abs/2106.08694"/>
        <updated>2021-06-17T01:58:42.035Z</updated>
        <summary type="html"><![CDATA[A lively research field has recently emerged that uses experimental methods
to probe the linguistic behavior of modern deep networks. While work in this
tradition often reports intriguing results about the grammatical skills of deep
nets, it is not clear what their implications for linguistic theorizing should
be. As a consequence, linguistically-oriented deep net analysis has had very
little impact on linguistics at large. In this chapter, I suggest that deep
networks should be treated as theories making explicit predictions about the
acceptability of linguistic utterances. I argue that, if we overcome some
obstacles standing in the way of seriously pursuing this idea, we will gain a
powerful new theoretical tool, complementary to mainstream algebraic
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1"&gt;Marco Baroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Silent Speech and Emotion Recognition from Vocal Tract Shape Dynamics in Real-Time MRI. (arXiv:2106.08706v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08706</id>
        <link href="http://arxiv.org/abs/2106.08706"/>
        <updated>2021-06-17T01:58:42.010Z</updated>
        <summary type="html"><![CDATA[Speech sounds of spoken language are obtained by varying configuration of the
articulators surrounding the vocal tract. They contain abundant information
that can be utilized to better understand the underlying mechanism of human
speech production. We propose a novel deep neural network-based learning
framework that understands acoustic information in the variable-length sequence
of vocal tract shaping during speech production, captured by real-time magnetic
resonance imaging (rtMRI), and translate it into text. The proposed framework
comprises of spatiotemporal convolutions, a recurrent network, and the
connectionist temporal classification loss, trained entirely end-to-end. On the
USC-TIMIT corpus, the model achieved a 40.6% PER at sentence-level, much better
compared to the existing models. To the best of our knowledge, this is the
first study that demonstrates the recognition of entire spoken sentence based
on an individual's articulatory motions captured by rtMRI video. We also
performed an analysis of variations in the geometry of articulation in each
sub-regions of the vocal tract (i.e., pharyngeal, velar and dorsal, hard
palate, labial constriction region) with respect to different emotions and
genders. Results suggest that each sub-regions distortion is affected by both
emotion and gender.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pandey_L/0/1/0/all/0/1"&gt;Laxmi Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arif_A/0/1/0/all/0/1"&gt;Ahmed Sabbir Arif&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shuffle Transformer with Feature Alignment for Video Face Parsing. (arXiv:2106.08650v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08650</id>
        <link href="http://arxiv.org/abs/2106.08650"/>
        <updated>2021-06-17T01:58:41.996Z</updated>
        <summary type="html"><![CDATA[This is a short technical report introducing the solution of the Team
TCParser for Short-video Face Parsing Track of The 3rd Person in Context (PIC)
Workshop and Challenge at CVPR 2021. In this paper, we introduce a strong
backbone which is cross-window based Shuffle Transformer for presenting
accurate face parsing representation. To further obtain the finer segmentation
results, especially on the edges, we introduce a Feature Alignment Aggregation
(FAA) module. It can effectively relieve the feature misalignment issue caused
by multi-resolution feature aggregation. Benefiting from the stronger backbone
and better feature aggregation, the proposed method achieves 86.9519% score in
the Short-video Face Parsing track of the 3rd Person in Context (PIC) Workshop
and Challenge, ranked the first place.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zilong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Pei Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1"&gt;Guozhong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1"&gt;Gang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1"&gt;Bin Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CMF: Cascaded Multi-model Fusion for Referring Image Segmentation. (arXiv:2106.08617v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08617</id>
        <link href="http://arxiv.org/abs/2106.08617"/>
        <updated>2021-06-17T01:58:41.990Z</updated>
        <summary type="html"><![CDATA[In this work, we address the task of referring image segmentation (RIS),
which aims at predicting a segmentation mask for the object described by a
natural language expression. Most existing methods focus on establishing
unidirectional or directional relationships between visual and linguistic
features to associate two modalities together, while the multi-scale context is
ignored or insufficiently modeled. Multi-scale context is crucial to localize
and segment those objects that have large scale variations during the
multi-modal fusion process. To solve this problem, we propose a simple yet
effective Cascaded Multi-modal Fusion (CMF) module, which stacks multiple
atrous convolutional layers in parallel and further introduces a cascaded
branch to fuse visual and linguistic features. The cascaded branch can
progressively integrate multi-scale contextual information and facilitate the
alignment of two modalities during the multi-modal fusion process. Experimental
results on four benchmark datasets demonstrate that our method outperforms most
state-of-the-art methods. Code is available at
https://github.com/jianhua2022/CMF-Refseg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianhua Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhanyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[2nd Place Solution for Waymo Open Dataset Challenge - Real-time 2D Object Detection. (arXiv:2106.08713v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08713</id>
        <link href="http://arxiv.org/abs/2106.08713"/>
        <updated>2021-06-17T01:58:41.983Z</updated>
        <summary type="html"><![CDATA[In an autonomous driving system, it is essential to recognize vehicles,
pedestrians and cyclists from images. Besides the high accuracy of the
prediction, the requirement of real-time running brings new challenges for
convolutional network models. In this report, we introduce a real-time method
to detect the 2D objects from images. We aggregate several popular one-stage
object detectors and train the models of variety input strategies
independently, to yield better performance for accurate multi-scale detection
of each category, especially for small objects. For model acceleration, we
leverage TensorRT to optimize the inference time of our detection pipeline. As
shown in the leaderboard, our proposed detection framework ranks the 2nd place
with 75.00% L1 mAP and 69.72% L2 mAP in the real-time 2D detection track of the
Waymo Open Dataset Challenges, while our framework achieves the latency of
45.8ms/frame on an Nvidia Tesla V100 GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yueming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xiaolin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_B/0/1/0/all/0/1"&gt;Bing Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_T/0/1/0/all/0/1"&gt;Tengfei Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhihui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yawei Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1"&gt;Haojin Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guoshan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Pengfei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Semi-supervised Medical Image Classification via Inter-client Relation Matching. (arXiv:2106.08600v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08600</id>
        <link href="http://arxiv.org/abs/2106.08600"/>
        <updated>2021-06-17T01:58:41.968Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has emerged with increasing popularity to collaborate
distributed medical institutions for training deep networks. However, despite
existing FL algorithms only allow the supervised training setting, most
hospitals in realistic usually cannot afford the intricate data labeling due to
absence of budget or expertise. This paper studies a practical yet challenging
FL problem, named \textit{Federated Semi-supervised Learning} (FSSL), which
aims to learn a federated model by jointly utilizing the data from both labeled
and unlabeled clients (i.e., hospitals). We present a novel approach for this
problem, which improves over traditional consistency regularization mechanism
with a new inter-client relation matching scheme. The proposed learning scheme
explicitly connects the learning across labeled and unlabeled clients by
aligning their extracted disease relationships, thereby mitigating the
deficiency of task knowledge at unlabeled clients and promoting discriminative
information from unlabeled samples. We validate our method on two large-scale
medical image classification datasets. The effectiveness of our method has been
demonstrated with the clear improvements over state-of-the-arts as well as the
thorough ablation analysis on both tasks\footnote{Code will be made available
at \url{https://github.com/liuquande/FedIRM}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quande Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongzheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1"&gt;Qi Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng-Ann Heng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection in Video Sequences: A Benchmark and Computational Model. (arXiv:2106.08570v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08570</id>
        <link href="http://arxiv.org/abs/2106.08570"/>
        <updated>2021-06-17T01:58:41.938Z</updated>
        <summary type="html"><![CDATA[Anomaly detection has attracted considerable search attention. However,
existing anomaly detection databases encounter two major problems. Firstly,
they are limited in scale. Secondly, training sets contain only video-level
labels indicating the existence of an abnormal event during the full video
while lacking annotations of precise time durations. To tackle these problems,
we contribute a new Large-scale Anomaly Detection (LAD) database as the
benchmark for anomaly detection in video sequences, which is featured in two
aspects. 1) It contains 2000 video sequences including normal and abnormal
video clips with 14 anomaly categories including crash, fire, violence, etc.
with large scene varieties, making it the largest anomaly analysis database to
date. 2) It provides the annotation data, including video-level labels
(abnormal/normal video, anomaly type) and frame-level labels (abnormal/normal
video frame) to facilitate anomaly detection. Leveraging the above benefits
from the LAD database, we further formulate anomaly detection as a
fully-supervised learning problem and propose a multi-task deep neural network
to solve it. We first obtain the local spatiotemporal contextual feature by
using an Inflated 3D convolutional (I3D) network. Then we construct a recurrent
convolutional neural network fed the local spatiotemporal contextual feature to
extract the spatiotemporal contextual feature. With the global spatiotemporal
contextual feature, the anomaly type and score can be computed simultaneously
by a multi-task neural network. Experimental results show that the proposed
method outperforms the state-of-the-art anomaly detection methods on our
database and other public databases of anomaly detection. Codes are available
at https://github.com/wanboyang/anomaly_detection_LAD2000.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_B/0/1/0/all/0/1"&gt;Boyang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wenhui Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuming Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhiyuan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1"&gt;Guanqun Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compound Frechet Inception Distance for Quality Assessment of GAN Created Images. (arXiv:2106.08575v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08575</id>
        <link href="http://arxiv.org/abs/2106.08575"/>
        <updated>2021-06-17T01:58:41.929Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks or GANs are a type of generative modeling
framework. GANs involve a pair of neural networks engaged in a competition in
iteratively creating fake data, indistinguishable from the real data. One
notable application of GANs is developing fake human faces, also known as "deep
fakes," due to the deep learning algorithms at the core of the GAN framework.
Measuring the quality of the generated images is inherently subjective but
attempts to objectify quality using standardized metrics have been made. One
example of objective metrics is the Frechet Inception Distance (FID), which
measures the difference between distributions of feature vectors for two
separate datasets of images. There are situations that images with low
perceptual qualities are not assigned appropriate FID scores. We propose to
improve the robustness of the evaluation process by integrating lower-level
features to cover a wider array of visual defects. Our proposed method
integrates three levels of feature abstractions to evaluate the quality of
generated images. Experimental evaluations show better performance of the
proposed method for distorted images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nunn_E/0/1/0/all/0/1"&gt;Eric J. Nunn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khadivi_P/0/1/0/all/0/1"&gt;Pejman Khadivi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samavi_S/0/1/0/all/0/1"&gt;Shadrokh Samavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PatchNet: Unsupervised Object Discovery based on Patch Embedding. (arXiv:2106.08599v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08599</id>
        <link href="http://arxiv.org/abs/2106.08599"/>
        <updated>2021-06-17T01:58:41.922Z</updated>
        <summary type="html"><![CDATA[We demonstrate that frequently appearing objects can be discovered by
training randomly sampled patches from a small number of images (100 to 200) by
self-supervision. Key to this approach is the pattern space, a latent space of
patterns that represents all possible sub-images of the given image data. The
distance structure in the pattern space captures the co-occurrence of patterns
due to the frequent objects. The pattern space embedding is learned by
minimizing the contrastive loss between randomly generated adjacent patches. To
prevent the embedding from learning the background, we modulate the contrastive
loss by color-based object saliency and background dissimilarity. The learned
distance structure serves as object memory, and the frequent objects are simply
discovered by clustering the pattern vectors from the random patches sampled
for inference. Our image representation based on image patches naturally
handles the position and scale invariance property that is crucial to
multi-object discovery. The method has been proven surprisingly effective, and
successfully applied to finding multiple human faces and bodies from natural
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1"&gt;Hankyu Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1"&gt;Heng Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Didari_S/0/1/0/all/0/1"&gt;Sima Didari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jae Oh Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bangert_P/0/1/0/all/0/1"&gt;Patrick Bangert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastAno: Fast Anomaly Detection via Spatio-temporal Patch Transformation. (arXiv:2106.08613v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08613</id>
        <link href="http://arxiv.org/abs/2106.08613"/>
        <updated>2021-06-17T01:58:41.911Z</updated>
        <summary type="html"><![CDATA[Video anomaly detection has gained significant attention due to the
increasing requirements of automatic monitoring for surveillance videos.
Especially, the prediction based approach is one of the most studied methods to
detect anomalies by predicting frames that include abnormal events in the test
set after learning with the normal frames of the training set. However, a lot
of prediction networks are computationally expensive owing to the use of
pre-trained optical flow networks, or fail to detect abnormal situations
because of their strong generative ability to predict even the anomalies. To
address these shortcomings, we propose spatial rotation transformation (SRT)
and temporal mixing transformation (TMT) to generate irregular patch cuboids
within normal frame cuboids in order to enhance the learning of normal
features. Additionally, the proposed patch transformation is used only during
the training phase, allowing our model to detect abnormal frames at fast speed
during inference. Our model is evaluated on three anomaly detection benchmarks,
achieving competitive accuracy and surpassing all the previous works in terms
of speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chaewon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1"&gt;MyeongAh Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Minhyeok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangyoun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Consistency Regularization for Unsupervised Multi-source Domain Adaptive Classification. (arXiv:2106.08590v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08590</id>
        <link href="http://arxiv.org/abs/2106.08590"/>
        <updated>2021-06-17T01:58:41.895Z</updated>
        <summary type="html"><![CDATA[Deep learning-based multi-source unsupervised domain adaptation (MUDA) has
been actively studied in recent years. Compared with single-source unsupervised
domain adaptation (SUDA), domain shift in MUDA exists not only between the
source and target domains but also among multiple source domains. Most existing
MUDA algorithms focus on extracting domain-invariant representations among all
domains whereas the task-specific decision boundaries among classes are largely
neglected. In this paper, we propose an end-to-end trainable network that
exploits domain Consistency Regularization for unsupervised Multi-source domain
Adaptive classification (CRMA). CRMA aligns not only the distributions of each
pair of source and target domains but also that of all domains. For each pair
of source and target domains, we employ an intra-domain consistency to
regularize a pair of domain-specific classifiers to achieve intra-domain
alignment. In addition, we design an inter-domain consistency that targets
joint inter-domain alignment among all domains. To address different
similarities between multiple source domains and the target domain, we design
an authorization strategy that assigns different authorities to domain-specific
classifiers adaptively for optimal pseudo label prediction and self-training.
Extensive experiments show that CRMA tackles unsupervised domain adaptation
effectively under a multi-source setup and achieves superior adaptation
consistently across multiple MUDA datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhipeng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaobing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1"&gt;Shuai Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisit Visual Representation in Analytics Taxonomy: A Compression Perspective. (arXiv:2106.08512v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08512</id>
        <link href="http://arxiv.org/abs/2106.08512"/>
        <updated>2021-06-17T01:58:41.871Z</updated>
        <summary type="html"><![CDATA[Visual analytics have played an increasingly critical role in the Internet of
Things, where massive visual signals have to be compressed and fed into
machines. But facing such big data and constrained bandwidth capacity, existing
image/video compression methods lead to very low-quality representations, while
existing feature compression techniques fail to support diversified visual
analytics applications/tasks with low-bit-rate representations. In this paper,
we raise and study the novel problem of supporting multiple machine vision
analytics tasks with the compressed visual representation, namely, the
information compression problem in analytics taxonomy. By utilizing the
intrinsic transferability among different tasks, our framework successfully
constructs compact and expressive representations at low bit-rates to support a
diversified set of machine vision tasks, including both high-level
semantic-related tasks and mid-level geometry analytic tasks. In order to
impose compactness in the representations, we propose a codebook-based
hyperprior, which helps map the representation into a low-dimensional manifold.
As it well fits the signal structure of the deep visual feature, it facilitates
more accurate entropy estimation, and results in higher compression efficiency.
With the proposed framework and the codebook-based hyperprior, we further
investigate the relationship of different task features owning different levels
of abstraction granularity. Experimental results demonstrate that with the
proposed scheme, a set of diversified tasks can be supported at a significantly
lower bit-rate, compared with existing compression schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yueyu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenhan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haofeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised-learning-based method for chest MRI-CT transformation using structure constrained unsupervised generative attention networks. (arXiv:2106.08557v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08557</id>
        <link href="http://arxiv.org/abs/2106.08557"/>
        <updated>2021-06-17T01:58:41.862Z</updated>
        <summary type="html"><![CDATA[The integrated positron emission tomography/magnetic resonance imaging
(PET/MRI) scanner facilitates the simultaneous acquisition of metabolic
information via PET and morphological information with high soft-tissue
contrast using MRI. Although PET/MRI facilitates the capture of high-accuracy
fusion images, its major drawback can be attributed to the difficulty
encountered when performing attenuation correction, which is necessary for
quantitative PET evaluation. The combined PET/MRI scanning requires the
generation of attenuation-correction maps from MRI owing to no direct
relationship between the gamma-ray attenuation information and MRIs. While
MRI-based bone-tissue segmentation can be readily performed for the head and
pelvis regions, the realization of accurate bone segmentation via chest CT
generation remains a challenging task. This can be attributed to the
respiratory and cardiac motions occurring in the chest as well as its
anatomically complicated structure and relatively thin bone cortex. This paper
presents a means to minimise the anatomical structural changes without human
annotation by adding structural constraints using a modality-independent
neighbourhood descriptor (MIND) to a generative adversarial network (GAN) that
can transform unpaired images. The results obtained in this study revealed the
proposed U-GAT-IT + MIND approach to outperform all other competing approaches.
The findings of this study hint towards possibility of synthesising clinically
acceptable CT images from chest MRI without human annotation, thereby
minimising the changes in the anatomical structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matsuo_H/0/1/0/all/0/1"&gt;Hidetoshi Matsuo&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Nishio_M/0/1/0/all/0/1"&gt;Mizuho Nishio&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Nogami_M/0/1/0/all/0/1"&gt;Munenobu Nogami&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1"&gt;Feibi Zeng&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Kurimoto_T/0/1/0/all/0/1"&gt;Takako Kurimoto&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1"&gt;Sandeep Kaushik&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Wiesinger_F/0/1/0/all/0/1"&gt;Florian Wiesinger&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Kono_A/0/1/0/all/0/1"&gt;Atsushi K Kono&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1"&gt;Takamichi Murakami&lt;/a&gt; (1) ((1) Department of Radiology, Kobe University Graduate School of Medicine, Kobe, Japan, (2) GE Healthcare, Hino, Japan and (3) GE Healthcare, Munich, Germany)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Watching Too Much Television is Good: Self-Supervised Audio-Visual Representation Learning from Movies and TV Shows. (arXiv:2106.08513v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08513</id>
        <link href="http://arxiv.org/abs/2106.08513"/>
        <updated>2021-06-17T01:58:41.855Z</updated>
        <summary type="html"><![CDATA[The abundance and ease of utilizing sound, along with the fact that auditory
clues reveal so much about what happens in the scene, make the audio-visual
space a perfectly intuitive choice for self-supervised representation learning.
However, the current literature suggests that training on \textit{uncurated}
data yields considerably poorer representations compared to the
\textit{curated} alternatives collected in supervised manner, and the gap only
narrows when the volume of data significantly increases. Furthermore, the
quality of learned representations is known to be heavily influenced by the
size and taxonomy of the curated datasets used for self-supervised training.
This begs the question of whether we are celebrating too early on catching up
with supervised learning when our self-supervised efforts still rely almost
exclusively on curated data. In this paper, we study the efficacy of learning
from Movies and TV Shows as forms of uncurated data for audio-visual
self-supervised learning. We demonstrate that a simple model based on
contrastive learning, trained on a collection of movies and TV shows, not only
dramatically outperforms more complex methods which are trained on orders of
magnitude larger uncurated datasets, but also performs very competitively with
the state-of-the-art that learns from large-scale curated data. We identify
that audiovisual patterns like the appearance of the main character or
prominent scenes and mise-en-sc\`ene which frequently occur through the whole
duration of a movie, lead to an overabundance of easy negative instances in the
contrastive learning formulation. Capitalizing on such observation, we propose
a hierarchical sampling policy, which despite its simplicity, effectively
improves the performance, particularly when learning from TV shows which
naturally face less semantic diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalayeh_M/0/1/0/all/0/1"&gt;Mahdi M. Kalayeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamath_N/0/1/0/all/0/1"&gt;Nagendra Kamath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrashekar_A/0/1/0/all/0/1"&gt;Ashok Chandrashekar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning-based analysis of hyperspectral images for automated sepsis diagnosis. (arXiv:2106.08445v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08445</id>
        <link href="http://arxiv.org/abs/2106.08445"/>
        <updated>2021-06-17T01:58:41.843Z</updated>
        <summary type="html"><![CDATA[Sepsis is a leading cause of mortality and critical illness worldwide. While
robust biomarkers for early diagnosis are still missing, recent work indicates
that hyperspectral imaging (HSI) has the potential to overcome this bottleneck
by monitoring microcirculatory alterations. Automated machine learning-based
diagnosis of sepsis based on HSI data, however, has not been explored to date.
Given this gap in the literature, we leveraged an existing data set to (1)
investigate whether HSI-based automated diagnosis of sepsis is possible and (2)
put forth a list of possible confounders relevant for HSI-based tissue
classification. While we were able to classify sepsis with an accuracy of over
$98\,\%$ using the existing data, our research also revealed several subject-,
therapy- and imaging-related confounders that may lead to an overestimation of
algorithm performance when not balanced across the patient groups. We conclude
that further prospective studies, carefully designed with respect to these
confounders, are necessary to confirm the preliminary results obtained in this
study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dietrich_M/0/1/0/all/0/1"&gt;Maximilian Dietrich&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Seidlitz_S/0/1/0/all/0/1"&gt;Silvia Seidlitz&lt;/a&gt; (2, 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Schreck_N/0/1/0/all/0/1"&gt;Nicholas Schreck&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Wiesenfarth_M/0/1/0/all/0/1"&gt;Manuel Wiesenfarth&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Godau_P/0/1/0/all/0/1"&gt;Patrick Godau&lt;/a&gt; (2, 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1"&gt;Minu Tizabi&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Sellner_J/0/1/0/all/0/1"&gt;Jan Sellner&lt;/a&gt; (2, 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Marx_S/0/1/0/all/0/1"&gt;Sebastian Marx&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Knodler_S/0/1/0/all/0/1"&gt;Samuel Kn&amp;#xf6;dler&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Allers_M/0/1/0/all/0/1"&gt;Michael M. Allers&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Ayala_L/0/1/0/all/0/1"&gt;Leonardo Ayala&lt;/a&gt; (2, 7), &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_K/0/1/0/all/0/1"&gt;Karsten Schmidt&lt;/a&gt; (8), &lt;a href="http://arxiv.org/find/cs/1/au:+Brenner_T/0/1/0/all/0/1"&gt;Thorsten Brenner&lt;/a&gt; (8), &lt;a href="http://arxiv.org/find/cs/1/au:+Studier_Fischer_A/0/1/0/all/0/1"&gt;Alexander Studier-Fischer&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Nickel_F/0/1/0/all/0/1"&gt;Felix Nickel&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1"&gt;Beat P. M&amp;#xfc;ller-Stich&lt;/a&gt; (5), &lt;a href="http://arxiv.org/find/cs/1/au:+Kopp_Schneider_A/0/1/0/all/0/1"&gt;Annette Kopp-Schneider&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Weigand_M/0/1/0/all/0/1"&gt;Markus A. Weigand&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt; (2, 6, 7) ((1) Department of Anesthesiology, Heidelberg University Hospital, Heidelberg, Germany, (2) Division of Computer Assisted Medical Interventions, German Cancer Research Center (DKFZ), Heidelberg, Germany, (3) HIDSS4Health - Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany (4) Division of Biostatistics, German Cancer Research Center (DKFZ), Heidelberg, Germany, (5) Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany, (6) Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany, (7) Medical Faculty, Heidelberg University, Heidelberg, Germany, (8) Department of Anesthesiology and Intensive Care Medicine, University Hospital Essen, University Duisburg-Essen, Essen, Germany)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Achieving Domain Robustness in Stereo Matching Networks by Removing Shortcut Learning. (arXiv:2106.08486v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08486</id>
        <link href="http://arxiv.org/abs/2106.08486"/>
        <updated>2021-06-17T01:58:41.823Z</updated>
        <summary type="html"><![CDATA[Learning-based stereo matching and depth estimation networks currently excel
on public benchmarks with impressive results. However, state-of-the-art
networks often fail to generalize from synthetic imagery to more challenging
real data domains. This paper is an attempt to uncover hidden secrets of
achieving domain robustness and in particular, discovering the important
ingredients of generalization success of stereo matching networks by analyzing
the effect of synthetic image learning on real data performance. We provide
evidence that demonstrates that learning of features in the synthetic domain by
a stereo matching network is heavily influenced by two "shortcuts" presented in
the synthetic data: (1) identical local statistics (RGB colour features)
between matching pixels in the synthetic stereo images and (2) lack of realism
in synthetic textures on 3D objects simulated in game engines. We will show
that by removing such shortcuts, we can achieve domain robustness in the
state-of-the-art stereo matching frameworks and produce a remarkable
performance on multiple realistic datasets, despite the fact that the networks
were trained on synthetic data, only. Our experimental results point to the
fact that eliminating shortcuts from the synthetic data is key to achieve
domain-invariant generalization between synthetic and real data domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chuah_W/0/1/0/all/0/1"&gt;WeiQin Chuah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tennakoon_R/0/1/0/all/0/1"&gt;Ruwan Tennakoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bab_Hadiashar_A/0/1/0/all/0/1"&gt;Alireza Bab-Hadiashar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1"&gt;David Suter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Implicit Glyph Shape Representation. (arXiv:2106.08573v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08573</id>
        <link href="http://arxiv.org/abs/2106.08573"/>
        <updated>2021-06-17T01:58:41.817Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel implicit glyph shape representation, which
models glyphs as shape primitives enclosed by quadratic curves, and naturally
enables generating glyph images at arbitrary high resolutions. Experiments on
font reconstruction and interpolation tasks verified that this structured
implicit representation is suitable for describing both structure and style
features of glyphs. Furthermore, based on the proposed representation, we
design a simple yet effective disentangled network for the challenging one-shot
font style transfer problem, and achieve the best results comparing to
state-of-the-art alternatives in both quantitative and qualitative comparisons.
Benefit from this representation, our generated glyphs have the potential to be
converted to vector fonts through post-processing, reducing the gap between
rasterized images and vector graphics. We hope this work can provide a powerful
tool for 2D shape analysis and synthesis, and inspire further exploitation in
implicit representations for 2D shape modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Ying-Tian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuan-Chen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yi-Xiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Song-Hai Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Convolution Networks with Positional Encoding for Evoked Expression Estimation. (arXiv:2106.08596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08596</id>
        <link href="http://arxiv.org/abs/2106.08596"/>
        <updated>2021-06-17T01:58:41.801Z</updated>
        <summary type="html"><![CDATA[This paper presents an approach for Evoked Expressions from Videos (EEV)
challenge, which aims to predict evoked facial expressions from video. We take
advantage of pre-trained models on large-scale datasets in computer vision and
audio signals to extract the deep representation of timestamps in the video. A
temporal convolution network, rather than an RNN like architecture, is used to
explore temporal relationships due to its advantage in memory consumption and
parallelism. Furthermore, to address the missing annotations of some
timestamps, positional encoding is employed to ensure continuity of input data
when discarding these timestamps during training. We achieved state-of-the-art
results on the EEV challenge with a Pearson correlation coefficient of 0.05477,
the first ranked performance in the EEV 2021 challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_V/0/1/0/all/0/1"&gt;VanThong Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Guee-Sang Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hyung-Jeong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Soo-Huyng Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Resolution Continuous Normalizing Flows. (arXiv:2106.08462v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08462</id>
        <link href="http://arxiv.org/abs/2106.08462"/>
        <updated>2021-06-17T01:58:41.795Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that Neural Ordinary Differential Equations (ODEs) can
serve as generative models of images using the perspective of Continuous
Normalizing Flows (CNFs). Such models offer exact likelihood calculation, and
invertible generation/density estimation. In this work we introduce a
Multi-Resolution variant of such models (MRCNF), by characterizing the
conditional distribution over the additional information required to generate a
fine image that is consistent with the coarse image. We introduce a
transformation between resolutions that allows for no change in the log
likelihood. We show that this approach yields comparable likelihood values for
various image datasets, with improved performance at higher resolutions, with
fewer parameters, using only 1 GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1"&gt;Vikram Voleti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finlay_C/0/1/0/all/0/1"&gt;Chris Finlay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1"&gt;Adam Oberman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling the Challenges in Scene Graph Generation with Local-to-Global Interactions. (arXiv:2106.08543v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08543</id>
        <link href="http://arxiv.org/abs/2106.08543"/>
        <updated>2021-06-17T01:58:41.788Z</updated>
        <summary type="html"><![CDATA[In this work, we seek new insights into the underlying challenges of the
Scene Graph Generation (SGG) task. Quantitative and qualitative analysis of the
Visual Genome dataset implies -- 1) Ambiguity: even if inter-object
relationship contains the same object (or predicate), they may not be visually
or semantically similar, 2) Asymmetry: despite the nature of the relationship
that embodied the direction, it was not well addressed in previous studies, and
3) Higher-order contexts: leveraging the identities of certain graph elements
can help to generate accurate scene graphs. Motivated by the analysis, we
design a novel SGG framework, Local-to-Global Interaction Networks (LOGIN).
Locally, interactions extract the essence between three instances - subject,
object, and background - while baking direction awareness into the network by
constraining the input order. Globally, interactions encode the contexts
between every graph components -- nodes and edges. Also we introduce Attract &
Repel loss which finely adjusts predicate embeddings. Our framework enables
predicting the scene graph in a local-to-global manner by design, leveraging
the possible complementariness. To quantify how much LOGIN is aware of
relational direction, we propose a new diagnostic task called Bidirectional
Relationship Classification (BRC). We see that LOGIN can successfully
distinguish relational direction than existing methods (in BRC task) while
showing state-of-the-art results on the Visual Genome benchmark (in SGG task).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1"&gt;Sangmin Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1"&gt;Junhyug Noh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kangil Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICDAR 2021 Competition on Components Segmentation Task of Document Photos. (arXiv:2106.08499v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08499</id>
        <link href="http://arxiv.org/abs/2106.08499"/>
        <updated>2021-06-17T01:58:41.771Z</updated>
        <summary type="html"><![CDATA[This paper describes the short-term competition on Components Segmentation
Task of Document Photos that was prepared in the context of the 16th
International Conference on Document Analysis and Recognition (ICDAR 2021).
This competition aims to bring together researchers working on the filed of
identification document image processing and provides them a suitable benchmark
to compare their techniques on the component segmentation task of document
images. Three challenge tasks were proposed entailing different segmentation
assignments to be performed on a provided dataset. The collected data are from
several types of Brazilian ID documents, whose personal information was
conveniently replaced. There were 16 participants whose results obtained for
some or all the three tasks show different rates for the adopted metrics, like
Dice Similarity Coefficient ranging from 0.06 to 0.99. Different Deep Learning
models were applied by the entrants with diverse strategies to achieve the best
results in each of the tasks. Obtained results show that the current applied
methods for solving one of the proposed tasks (document boundary detection) are
already well stablished. However, for the other two challenge tasks (text zone
and handwritten sign detection) research and development of more robust
approaches are still required to achieve acceptable results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Junior_C/0/1/0/all/0/1"&gt;Celso A. M. Lopes Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junior_R/0/1/0/all/0/1"&gt;Ricardo B. das Neves Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bezerra_B/0/1/0/all/0/1"&gt;Byron L. D. Bezerra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toselli_A/0/1/0/all/0/1"&gt;Alejandro H. Toselli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Impedovo_D/0/1/0/all/0/1"&gt;Donato Impedovo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECKPN: Explicit Class Knowledge Propagation Network for Transductive Few-shot Learning. (arXiv:2106.08523v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08523</id>
        <link href="http://arxiv.org/abs/2106.08523"/>
        <updated>2021-06-17T01:58:41.765Z</updated>
        <summary type="html"><![CDATA[Recently, the transductive graph-based methods have achieved great success in
the few-shot classification task. However, most existing methods ignore
exploring the class-level knowledge that can be easily learned by humans from
just a handful of samples. In this paper, we propose an Explicit Class
Knowledge Propagation Network (ECKPN), which is composed of the comparison,
squeeze and calibration modules, to address this problem. Specifically, we
first employ the comparison module to explore the pairwise sample relations to
learn rich sample representations in the instance-level graph. Then, we squeeze
the instance-level graph to generate the class-level graph, which can help
obtain the class-level visual knowledge and facilitate modeling the relations
of different classes. Next, the calibration module is adopted to characterize
the relations of the classes explicitly to obtain the more discriminative
class-level knowledge representations. Finally, we combine the class-level
knowledge with the instance-level sample representations to guide the inference
of the query samples. We conduct extensive experiments on four few-shot
classification benchmarks, and the experimental results show that the proposed
ECKPN significantly outperforms the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaofan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoshan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Changsheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuhui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhe Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scene Transformer: A unified multi-task model for behavior prediction and planning. (arXiv:2106.08417v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08417</id>
        <link href="http://arxiv.org/abs/2106.08417"/>
        <updated>2021-06-17T01:58:41.759Z</updated>
        <summary type="html"><![CDATA[Predicting the future motion of multiple agents is necessary for planning in
dynamic environments. This task is challenging for autonomous driving since
agents (e.g., vehicles and pedestrians) and their associated behaviors may be
diverse and influence each other. Most prior work has focused on first
predicting independent futures for each agent based on all past motion, and
then planning against these independent predictions. However, planning against
fixed predictions can suffer from the inability to represent the future
interaction possibilities between different agents, leading to sub-optimal
planning. In this work, we formulate a model for predicting the behavior of all
agents jointly in real-world driving environments in a unified manner. Inspired
by recent language modeling approaches, we use a masking strategy as the query
to our model, enabling one to invoke a single model to predict agent behavior
in many ways, such as potentially conditioned on the goal or full future
trajectory of the autonomous vehicle or the behavior of other agents in the
environment. Our model architecture fuses heterogeneous world state in a
unified Transformer architecture by employing attention across road elements,
agent interactions and time steps. We evaluate our approach on autonomous
driving datasets for behavior prediction, and achieve state-of-the-art
performance. Our work demonstrates that formulating the problem of behavior
prediction in a unified architecture with a masking strategy may allow us to
have a single model that can perform multiple motion prediction and planning
related tasks effectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngiam_J/0/1/0/all/0/1"&gt;Jiquan Ngiam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1"&gt;Benjamin Caine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1"&gt;Vijay Vasudevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1"&gt;Hao-Tien Lewis Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1"&gt;Jeffrey Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1"&gt;Rebecca Roelofs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1"&gt;Alex Bewley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chenxi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venugopal_A/0/1/0/all/0/1"&gt;Ashish Venugopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_D/0/1/0/all/0/1"&gt;David Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1"&gt;Ben Sapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhifeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1"&gt;Jonathon Shlens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing Through Clouds in Satellite Images. (arXiv:2106.08408v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08408</id>
        <link href="http://arxiv.org/abs/2106.08408"/>
        <updated>2021-06-17T01:58:41.752Z</updated>
        <summary type="html"><![CDATA[This paper presents a neural-network-based solution to recover pixels
occluded by clouds in satellite images. We leverage radio frequency (RF)
signals in the ultra/super-high frequency band that penetrate clouds to help
reconstruct the occluded regions in multispectral images. We introduce the
first multi-modal multi-temporal cloud removal model. Our model uses publicly
available satellite observations and produces daily cloud-free images.
Experimental results show that our system significantly outperforms baselines
by 8dB in PSNR. We also demonstrate use cases of our system in digital
agriculture, flood monitoring, and wildfire detection. We will release the
processed dataset to facilitate future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mingmin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olsen_P/0/1/0/all/0/1"&gt;Peder A. Olsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Ranveer Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmenting Part-of-speech Tagging with Syntactic Information for Vietnamese and Chinese. (arXiv:2102.12136v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12136</id>
        <link href="http://arxiv.org/abs/2102.12136"/>
        <updated>2021-06-17T01:58:41.744Z</updated>
        <summary type="html"><![CDATA[Word segmentation and part-of-speech tagging are two critical preliminary
steps for downstream tasks in Vietnamese natural language processing. In
reality, people tend to consider also the phrase boundary when performing word
segmentation and part of speech tagging rather than solely process word by word
from left to right. In this paper, we implement this idea to improve word
segmentation and part of speech tagging the Vietnamese language by employing a
simplified constituency parser. Our neural model for joint word segmentation
and part-of-speech tagging has the architecture of the syllable-based CRF
constituency parser. To reduce the complexity of parsing, we replace all
constituent labels with a single label indicating for phrases. This model can
be augmented with predicted word boundary and part-of-speech tags by other
tools. Because Vietnamese and Chinese have some similar linguistic phenomena,
we evaluated the proposed model and its augmented versions on three Vietnamese
benchmark datasets and six Chinese benchmark datasets. Our experimental results
show that the proposed model achieves higher performances than previous works
for both languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Duc-Vu Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Kiet Van Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Ngan Luu-Thuy Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-scale Neural ODEs for 3D Medical Image Registration. (arXiv:2106.08493v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08493</id>
        <link href="http://arxiv.org/abs/2106.08493"/>
        <updated>2021-06-17T01:58:41.726Z</updated>
        <summary type="html"><![CDATA[Image registration plays an important role in medical image analysis.
Conventional optimization based methods provide an accurate estimation due to
the iterative process at the cost of expensive computation. Deep learning
methods such as learn-to-map are much faster but either iterative or
coarse-to-fine approach is required to improve accuracy for handling large
motions. In this work, we proposed to learn a registration optimizer via a
multi-scale neural ODE model. The inference consists of iterative gradient
updates similar to a conventional gradient descent optimizer but in a much
faster way, because the neural ODE learns from the training data to adapt the
gradient efficiently at each iteration. Furthermore, we proposed to learn a
modal-independent similarity metric to address image appearance variations
across different image contrasts. We performed evaluations through extensive
experiments in the context of multi-contrast 3D MR images from both public and
private data sources and demonstrate the superior performance of our proposed
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Junshen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Eric Z. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Terrence Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shanhui Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GKNet: grasp keypoint network for grasp candidates detection. (arXiv:2106.08497v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.08497</id>
        <link href="http://arxiv.org/abs/2106.08497"/>
        <updated>2021-06-17T01:58:41.720Z</updated>
        <summary type="html"><![CDATA[Contemporary grasp detection approaches employ deep learning to achieve
robustness to sensor and object model uncertainty. The two dominant approaches
design either grasp-quality scoring or anchor-based grasp recognition networks.
This paper presents a different approach to grasp detection by treating it as
keypoint detection. The deep network detects each grasp candidate as a pair of
keypoints, convertible to the grasp representation g = {x, y, w, {\theta}}^T,
rather than a triplet or quartet of corner points. Decreasing the detection
difficulty by grouping keypoints into pairs boosts performance. To further
promote dependencies between keypoints, the general non-local module is
incorporated into the proposed learning framework. A final filtering strategy
based on discrete and continuous orientation prediction removes false
correspondences and further improves grasp detection performance. GKNet, the
approach presented here, achieves the best balance of accuracy and speed on the
Cornell and the abridged Jacquard dataset (96.9% and 98.39% at 41.67 and 23.26
fps). Follow-up experiments on a manipulator evaluate GKNet using 4 types of
grasping experiments reflecting different nuisance sources: static grasping,
dynamic grasping, grasping at varied camera angles, and bin picking. GKNet
outperforms reference baselines in static and dynamic grasping experiments
while showing robustness to varied camera viewpoints and bin picking
experiments. The results confirm the hypothesis that grasp keypoints are an
effective output representation for deep grasp networks that provide robustness
to expected nuisance factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruinian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1"&gt;Fu-Jen Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vela_P/0/1/0/all/0/1"&gt;Patricio A. Vela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-Layered Approach for Measuring the Simulation-to-Reality Gap of Radar Perception for Autonomous Driving. (arXiv:2106.08372v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.08372</id>
        <link href="http://arxiv.org/abs/2106.08372"/>
        <updated>2021-06-17T01:58:41.714Z</updated>
        <summary type="html"><![CDATA[With the increasing safety validation requirements for the release of a
self-driving car, alternative approaches, such as simulation-based testing, are
emerging in addition to conventional real-world testing. In order to rely on
virtual tests the employed sensor models have to be validated. For this reason,
it is necessary to quantify the discrepancy between simulation and reality in
order to determine whether a certain fidelity is sufficient for a desired
intended use. There exists no sound method to measure this
simulation-to-reality gap of radar perception for autonomous driving. We
address this problem by introducing a multi-layered evaluation approach, which
consists of a combination of an explicit and an implicit sensor model
evaluation. The former directly evaluates the realism of the synthetically
generated sensor data, while the latter refers to an evaluation of a downstream
target application. In order to demonstrate the method, we evaluated the
fidelity of three typical radar model types (ideal, data-driven, ray
tracing-based) and their applicability for virtually testing radar-based
multi-object tracking. We have shown the effectiveness of the proposed approach
in terms of providing an in-depth sensor model assessment that renders existing
disparities visible and enables a realistic estimation of the overall model
fidelity across different scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_A/0/1/0/all/0/1"&gt;Anthony Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1"&gt;Max Paul Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Resch_M/0/1/0/all/0/1"&gt;Michael Resch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining. (arXiv:2012.15525v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15525</id>
        <link href="http://arxiv.org/abs/2012.15525"/>
        <updated>2021-06-17T01:58:41.703Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose BANG, a new pretraining model to Bridge the gap
between Autoregressive (AR) and Non-autoregressive (NAR) Generation. AR and NAR
generation can be uniformly regarded as to what extent previous tokens can be
attended, and BANG bridges AR and NAR generation by designing a novel model
structure for large-scale pretraining. The pretrained BANG model can
simultaneously support AR, NAR and semi-NAR generation to meet different
requirements. Experiments on question generation (SQuAD 1.1), summarization
(XSum) and dialogue generation (PersonaChat) show that BANG improves NAR and
semi-NAR performance significantly as well as attaining comparable performance
with strong AR pretrained models. Compared with the semi-NAR strong baselines,
BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of
SQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute
improvements of 10.73, 6.39 and 5.90 in the overall scores of SQuAD, XSUM and
PersonaChat respectively compared with the strong NAR baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1"&gt;Weizhen Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yeyun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1"&gt;Jian Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dayiheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Kewen Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiusheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruofei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Ming Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining decision of model from its prediction. (arXiv:2106.08366v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08366</id>
        <link href="http://arxiv.org/abs/2106.08366"/>
        <updated>2021-06-17T01:58:41.677Z</updated>
        <summary type="html"><![CDATA[This document summarizes different visual explanations methods such as CAM,
Grad-CAM, Localization using Multiple Instance Learning - Saliency-based
methods, Saliency-driven Class-Impressions, Muting pixels in input image -
Adversarial methods and Activation visualization, Convolution filter
visualization - Feature-based methods. We have also shown the results produced
by different methods and a comparison between CAM, GradCAM, and Guided
Backpropagation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tamboli_D/0/1/0/all/0/1"&gt;Dipesh Tamboli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DMSANet: Dual Multi Scale Attention Network. (arXiv:2106.08382v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08382</id>
        <link href="http://arxiv.org/abs/2106.08382"/>
        <updated>2021-06-17T01:58:41.661Z</updated>
        <summary type="html"><![CDATA[Attention mechanism of late has been quite popular in the computer vision
community. A lot of work has been done to improve the performance of the
network, although almost always it results in increased computational
complexity. In this paper, we propose a new attention module that not only
achieves the best performance but also has lesser parameters compared to most
existing models. Our attention module can easily be integrated with other
convolutional neural networks because of its lightweight nature. The proposed
network named Dual Multi Scale Attention Network (DMSANet) is comprised of two
parts: the first part is used to extract features at various scales and
aggregate them, the second part uses spatial and channel attention modules in
parallel to adaptively integrate local features with their global dependencies.
We benchmark our network performance for Image Classification on ImageNet
dataset, Object Detection and Instance Segmentation both on MS COCO dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DialogSum: A Real-Life Scenario Dialogue Summarization Dataset. (arXiv:2105.06762v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06762</id>
        <link href="http://arxiv.org/abs/2105.06762"/>
        <updated>2021-06-17T01:58:41.617Z</updated>
        <summary type="html"><![CDATA[Proposal of large-scale datasets has facilitated research on deep neural
models for news summarization. Deep learning can also be potentially useful for
spoken dialogue summarization, which can benefit a range of real-life scenarios
including customer service management and medication tracking. To this end, we
propose DialogSum, a large-scale labeled dialogue summarization dataset. We
conduct empirical analysis on DialogSum using state-of-the-art neural
summarizers. Experimental results show unique challenges in dialogue
summarization, such as spoken terms, special discourse structures, coreferences
and ellipsis, pragmatics and social common sense, which require specific
representation learning technologies to better deal with.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yulong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QuatDE: Dynamic Quaternion Embedding for Knowledge Graph Completion. (arXiv:2105.09002v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09002</id>
        <link href="http://arxiv.org/abs/2105.09002"/>
        <updated>2021-06-17T01:58:41.610Z</updated>
        <summary type="html"><![CDATA[Knowledge graph embedding has been an active research topic for knowledge
base completion (KGC), with progressive improvement from the initial TransE,
TransH, RotatE et al to the current state-of-the-art QuatE. However, QuatE
ignores the multi-faceted nature of the entity and the complexity of the
relation, only using rigorous operation on quaternion space to capture the
interaction between entitiy pair and relation, leaving opportunities for better
knowledge representation which will finally help KGC. In this paper, we propose
a novel model, QuatDE, with a dynamic mapping strategy to explicitly capture
the variety of relational patterns and separate different semantic information
of the entity, using transition vectors to adjust the point position of the
entity embedding vectors in the quaternion space via Hamilton product,
enhancing the feature interaction capability between elements of the triplet.
Experiment results show QuatDE achieves state-of-the-art performance on three
well-established knowledge graph completion benchmarks. In particular, the MR
evaluation has relatively increased by 26% on WN18 and 15% on WN18RR, which
proves the generalization of QuatDE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1"&gt;Haipeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuxue Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zakari_R/0/1/0/all/0/1"&gt;Rufai Yusuf Zakari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Owusu_J/0/1/0/all/0/1"&gt;Jim Wilson Owusu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1"&gt;Ke Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset. (arXiv:2104.08459v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08459</id>
        <link href="http://arxiv.org/abs/2104.08459"/>
        <updated>2021-06-17T01:58:41.604Z</updated>
        <summary type="html"><![CDATA[This paper introduces a high-quality open-source speech synthesis dataset for
Kazakh, a low-resource language spoken by over 13 million people worldwide. The
dataset consists of about 93 hours of transcribed audio recordings spoken by
two professional speakers (female and male). It is the first publicly available
large-scale dataset developed to promote Kazakh text-to-speech (TTS)
applications in both academia and industry. In this paper, we share our
experience by describing the dataset development procedures and faced
challenges, and discuss important future directions. To demonstrate the
reliability of our dataset, we built baseline end-to-end TTS models and
evaluated them using the subjective mean opinion score (MOS) measure.
Evaluation results show that the best TTS models trained on our dataset achieve
MOS above 4 for both speakers, which makes them applicable for practical use.
The dataset, training recipe, and pretrained TTS models are freely available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mussakhojayeva_S/0/1/0/all/0/1"&gt;Saida Mussakhojayeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Janaliyeva_A/0/1/0/all/0/1"&gt;Aigerim Janaliyeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mirzakhmetov_A/0/1/0/all/0/1"&gt;Almas Mirzakhmetov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khassanov_Y/0/1/0/all/0/1"&gt;Yerbolat Khassanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1"&gt;Huseyin Atakan Varol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Information Divergence Measure Between Neural Text and Human Text. (arXiv:2102.01454v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01454</id>
        <link href="http://arxiv.org/abs/2102.01454"/>
        <updated>2021-06-17T01:58:41.552Z</updated>
        <summary type="html"><![CDATA[As major progress is made in open-ended text generation, measuring how close
machine-generated text is to human language remains a critical open problem. We
propose Mauve, a comparison measure for open-ended text generation, which
directly compares a generation model's distribution to that of human-written
text. Mauve measures the mean area under a divergence curve for the two
distributions, exploring the trade-off between two types of errors: those
arising from parts of the human distribution that the model distribution
approximates well, and those it does not. Mauve extends a family of information
divergence metrics, introducing a tractable approximation based on computing
the KL divergence in a quantized embedding space. This yields an efficient
implementation that scales up to modern text generation models. Through an
extensive empirical study on three open-ended generation tasks, we find that
Mauve identifies known properties of generated text, scales naturally with
model size, and correlates with human judgments, with fewer restrictions than
existing distributional evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pillutla_K/0/1/0/all/0/1"&gt;Krishna Pillutla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1"&gt;Swabha Swayamdipta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1"&gt;Rowan Zellers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1"&gt;John Thickstun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1"&gt;Sean Welleck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harchaoui_Z/0/1/0/all/0/1"&gt;Zaid Harchaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Topic Coverage Approach to Evaluation of Topic Models. (arXiv:2012.06274v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06274</id>
        <link href="http://arxiv.org/abs/2012.06274"/>
        <updated>2021-06-17T01:58:41.545Z</updated>
        <summary type="html"><![CDATA[Topic models are widely used unsupervised models of text capable of learning
topics - weighted lists of words and documents - from large collections of text
documents. When topic models are used for discovery of topics in text
collections, a question that arises naturally is how well the model-induced
topics correspond to topics of interest to the analyst. In this paper we
revisit and extend a so far neglected approach to topic model evaluation based
on measuring topic coverage - computationally matching model topics with a set
of reference topics that models are expected to uncover. The approach is well
suited for analyzing models' performance in topic discovery and for large-scale
analysis of both topic models and measures of model quality. We propose new
measures of coverage and evaluate, in a series of experiments, different types
of topic models on two distinct text domains for which interest for topic
discovery exists. The experiments include evaluation of model quality, analysis
of coverage of distinct topic categories, and the analysis of the relationship
between coverage and other methods of topic model evaluation. The contributions
of the paper include new measures of coverage, insights into both topic models
and other methods of model evaluation, and the datasets and code for
facilitating future research of both topic coverage and other approaches to
topic model evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korencic_D/0/1/0/all/0/1"&gt;Damir Koren&amp;#x10d;i&amp;#x107;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ristov_S/0/1/0/all/0/1"&gt;Strahil Ristov&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Repar_J/0/1/0/all/0/1"&gt;Jelena Repar&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1"&gt;Jan &amp;#x160;najder&lt;/a&gt; (2) ((1) Rudjer Bo&amp;#x161;kovi&amp;#x107; Institute, Croatia, (2) University of Zagreb, Faculty of Electrical Engineering and Computing, Croatia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Impact of ASR on the Automatic Analysis of Linguistic Complexity and Sophistication in Spontaneous L2 Speech. (arXiv:2104.08529v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08529</id>
        <link href="http://arxiv.org/abs/2104.08529"/>
        <updated>2021-06-17T01:58:41.539Z</updated>
        <summary type="html"><![CDATA[In recent years, automated approaches to assessing linguistic complexity in
second language (L2) writing have made significant progress in gauging learner
performance, predicting human ratings of the quality of learner productions,
and benchmarking L2 development. In contrast, there is comparatively little
work in the area of speaking, particularly with respect to fully automated
approaches to assessing L2 spontaneous speech. While the importance of a
well-performing ASR system is widely recognized, little research has been
conducted to investigate the impact of its performance on subsequent automatic
text analysis. In this paper, we focus on this issue and examine the impact of
using a state-of-the-art ASR system for subsequent automatic analysis of
linguistic complexity in spontaneously produced L2 speech. A set of 30 selected
measures were considered, falling into four categories: syntactic, lexical,
n-gram frequency, and information-theoretic measures. The agreement between the
scores for these measures obtained on the basis of ASR-generated vs. manual
transcriptions was determined through correlation analysis. A more differential
effect of ASR performance on specific types of complexity measures when
controlling for task type effects is also presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerz_E/0/1/0/all/0/1"&gt;Elma Kerz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Refining Language Models with Compositional Explanations. (arXiv:2103.10415v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10415</id>
        <link href="http://arxiv.org/abs/2103.10415"/>
        <updated>2021-06-17T01:58:41.531Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models have been successful on text classification
tasks, but are prone to learning spurious correlations from biased datasets,
and are thus vulnerable when making inferences in a new domain. Prior works
reveal such spurious patterns via post-hoc explanation algorithms which compute
the importance of input features. Further, the model is regularized to align
the importance scores with human knowledge, so that the unintended model
behaviors are eliminated. However, such a regularization technique lacks
flexibility and coverage, since only importance scores towards a pre-defined
list of features are adjusted, while more complex human knowledge such as
feature interaction and pattern generalization can hardly be incorporated. In
this work, we propose to refine a learned language model for a target domain by
collecting human-provided compositional explanations regarding observed biases.
By parsing these explanations into executable logic rules, the human-specified
refinement advice from a small set of explanations can be generalized to more
training examples. We additionally introduce a regularization term allowing
adjustments for both importance and interaction of features to better rectify
model behavior. We demonstrate the effectiveness of the proposed approach on
two text classification tasks by showing improved performance in target domain
as well as improved model fairness after refinement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Huihan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qinyuan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xisen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Earnings-21: A Practical Benchmark for ASR in the Wild. (arXiv:2104.11348v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11348</id>
        <link href="http://arxiv.org/abs/2104.11348"/>
        <updated>2021-06-17T01:58:41.524Z</updated>
        <summary type="html"><![CDATA[Commonly used speech corpora inadequately challenge academic and commercial
ASR systems. In particular, speech corpora lack metadata needed for detailed
analysis and WER measurement. In response, we present Earnings-21, a 39-hour
corpus of earnings calls containing entity-dense speech from nine different
financial sectors. This corpus is intended to benchmark ASR systems in the wild
with special attention towards named entity recognition. We benchmark four
commercial ASR models, two internal models built with open-source tools, and an
open-source LibriSpeech model and discuss their differences in performance on
Earnings-21. Using our recently released fstalign tool, we provide a candid
analysis of each model's recognition capabilities under different partitions.
Our analysis finds that ASR accuracy for certain NER categories is poor,
presenting a significant impediment to transcript comprehension and usage.
Earnings-21 bridges academic and commercial ASR system evaluation and enables
further research on entity modeling and WER on real world audio.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rio_M/0/1/0/all/0/1"&gt;Miguel Del Rio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delworth_N/0/1/0/all/0/1"&gt;Natalie Delworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Westerman_R/0/1/0/all/0/1"&gt;Ryan Westerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Michelle Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1"&gt;Nishchal Bhandari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palakapilly_J/0/1/0/all/0/1"&gt;Joseph Palakapilly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McNamara_Q/0/1/0/all/0/1"&gt;Quinten McNamara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Joshua Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelasko_P/0/1/0/all/0/1"&gt;Piotr Zelasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jette_M/0/1/0/all/0/1"&gt;Miguel Jette&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Variational Attention Models for Language Generation. (arXiv:2004.09764v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09764</id>
        <link href="http://arxiv.org/abs/2004.09764"/>
        <updated>2021-06-17T01:58:41.517Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders have been widely applied for natural language
generation, however, there are two long-standing problems: information
under-representation and posterior collapse. The former arises from the fact
that only the last hidden state from the encoder is transformed to the latent
space, which is insufficient to summarize data. The latter comes as a result of
the imbalanced scale between the reconstruction loss and the KL divergence in
the objective function. To tackle these issues, in this paper we propose the
discrete variational attention model with categorical distribution over the
attention mechanism owing to the discrete nature in languages. Our approach is
combined with an auto-regressive prior to capture the sequential dependency
from observations, which can enhance the latent space for language generation.
Moreover, thanks to the property of discreteness, the training of our proposed
approach does not suffer from posterior collapse. Furthermore, we carefully
analyze the superiority of discrete latent space over the continuous space with
the common Gaussian distribution. Extensive experiments on language generation
demonstrate superior advantages of our proposed approach in comparison with the
state-of-the-art counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subword Sampling for Low Resource Word Alignment. (arXiv:2012.11657v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11657</id>
        <link href="http://arxiv.org/abs/2012.11657"/>
        <updated>2021-06-17T01:58:41.489Z</updated>
        <summary type="html"><![CDATA[Annotation projection is an important area in NLP that can greatly contribute
to creating language resources for low-resource languages. Word alignment plays
a key role in this setting. However, most of the existing word alignment
methods are designed for a high resource setting in machine translation where
millions of parallel sentences are available. This amount reduces to a few
thousands of sentences when dealing with low-resource languages failing the
existing established IBM models. In this paper, we propose subword
sampling-based alignment of text units. This method's hypothesis is that the
aggregation of different granularities of text for certain language pairs can
help word-level alignment. For certain languages for which gold-standard
alignments exist, we propose an iterative Bayesian optimization framework to
optimize selecting possible subwords from the space of possible subword
representations of the source and target sentences. We show that the subword
sampling method consistently outperforms word-level alignment on six language
pairs: English-German, English-French, English-Romanian, English-Persian,
English-Hindi, and English-Inuktitut. In addition, we show that the
hyperparameters learned for certain language pairs can be applied to other
languages at no supervision and consistently improve the alignment results. We
observe that using $5K$ parallel sentences together with our proposed subword
sampling approach, we obtain similar F1 scores to the use of $100K$'s of
parallel sentences in existing word-level fast-align/eflomal alignment methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asgari_E/0/1/0/all/0/1"&gt;Ehsaneddin Asgari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1"&gt;Masoud Jalili Sabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1"&gt;Philipp Dufter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ringlstetter_C/0/1/0/all/0/1"&gt;Christopher Ringlstetter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1"&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phoneme-BERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript. (arXiv:2102.00804v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00804</id>
        <link href="http://arxiv.org/abs/2102.00804"/>
        <updated>2021-06-17T01:58:41.472Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed significant improvement in ASR systems to
recognize spoken utterances. However, it is still a challenging task for noisy
and out-of-domain data, where substitution and deletion errors are prevalent in
the transcribed text. These errors significantly degrade the performance of
downstream tasks. In this work, we propose a BERT-style language model,
referred to as PhonemeBERT, that learns a joint language model with phoneme
sequence and ASR transcript to learn phonetic-aware representations that are
robust to ASR errors. We show that PhonemeBERT can be used on downstream tasks
using phoneme sequences as additional features, and also in low-resource setup
where we only have ASR-transcripts for the downstream tasks with no phoneme
information available. We evaluate our approach extensively by generating noisy
data for three benchmark datasets - Stanford Sentiment Treebank, TREC and ATIS
for sentiment, question and intent classification tasks respectively. The
results of the proposed approach beats the state-of-the-art baselines
comprehensively on each dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sundararaman_M/0/1/0/all/0/1"&gt;Mukuntha Narayanan Sundararaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Ayush Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vepa_J/0/1/0/all/0/1"&gt;Jithendra Vepa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models. (arXiv:2010.08566v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08566</id>
        <link href="http://arxiv.org/abs/2010.08566"/>
        <updated>2021-06-17T01:58:41.430Z</updated>
        <summary type="html"><![CDATA[Publicly available, large pretrained LanguageModels (LMs) generate text with
remarkable quality, but only sequentially from left to right. As a result, they
are not immediately applicable to generation tasks that break the
unidirectional assumption, such as paraphrasing or text-infilling,
necessitating task-specific supervision.

In this paper, we present Reflective Decoding, a novel unsupervised algorithm
that allows for direct application of unidirectional LMs to non-sequential
tasks. Our 2-step approach requires no supervision or even parallel corpora,
only two off-the-shelf pretrained LMs in opposite directions: forward and
backward. First, in the contextualization step, we use LMs to generate
ensembles of past and future contexts which collectively capture the input
(e.g. the source sentence for paraphrasing). Second, in the reflection step, we
condition on these "context ensembles", generating outputs that are compatible
with them. Comprehensive empirical results demonstrate that Reflective Decoding
outperforms strong unsupervised baselines on both paraphrasing and abductive
text infilling, significantly narrowing the gap between unsupervised and
supervised methods. Reflective Decoding surpasses multiple supervised baselines
on various metrics including human evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1"&gt;Peter West&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Ximing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1"&gt;Ari Holtzman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1"&gt;Chandra Bhagavatula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Jena Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions. (arXiv:2012.04293v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04293</id>
        <link href="http://arxiv.org/abs/2012.04293"/>
        <updated>2021-06-17T01:58:41.390Z</updated>
        <summary type="html"><![CDATA[Humans are able to perceive, understand and reason about physical events.
Developing models with similar physical understanding capabilities is a
long-standing goal of artificial intelligence. As a step towards this goal, in
this work, we introduce CRAFT, a new visual question answering dataset that
requires causal reasoning about physical forces and object interactions. It
contains 58K video and question pairs that are generated from 10K videos from
20 different virtual environments, containing various objects in motion that
interact with each other and the scene. Two question categories from CRAFT
include previously studied descriptive and counterfactual questions. Besides,
inspired by the theories of force dynamics in cognitive linguistics, we
introduce new question categories that involve understanding the interactions
of objects through the notions of cause, enable, and prevent. Our results
demonstrate that even though these tasks seem to be simple and intuitive for
humans, the evaluated baseline models, including existing state-of-the-art
methods, do not yet deal with the challenges posed in our benchmark dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ates_T/0/1/0/all/0/1"&gt;Tayfun Ates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atesoglu_M/0/1/0/all/0/1"&gt;Muhammed Samil Atesoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yigit_C/0/1/0/all/0/1"&gt;Cagatay Yigit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kesen_I/0/1/0/all/0/1"&gt;Ilker Kesen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobas_M/0/1/0/all/0/1"&gt;Mert Kobas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1"&gt;Erkut Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1"&gt;Aykut Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goksun_T/0/1/0/all/0/1"&gt;Tilbe Goksun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1"&gt;Deniz Yuret&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automated Website Classification by Deep Learning. (arXiv:1910.09991v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.09991</id>
        <link href="http://arxiv.org/abs/1910.09991"/>
        <updated>2021-06-17T01:58:41.379Z</updated>
        <summary type="html"><![CDATA[In recent years, the interest in Big Data sources has been steadily growing
within the Official Statistic community. The Italian National Institute of
Statistics (Istat) is currently carrying out several Big Data pilot studies.
One of these studies, the ICT Big Data pilot, aims at exploiting massive
amounts of textual data automatically scraped from the websites of Italian
enterprises in order to predict a set of target variables (e.g. e-commerce)
that are routinely observed by the traditional ICT Survey. In this paper, we
show that Deep Learning techniques can successfully address this problem.
Essentially, we tackle a text classification task: an algorithm must learn to
infer whether an Italian enterprise performs e-commerce from the textual
content of its website. To reach this goal, we developed a sophisticated
processing pipeline and evaluated its performance through extensive
experiments. Our pipeline uses Convolutional Neural Networks and relies on Word
Embeddings to encode raw texts into grayscale images (i.e. normalized numeric
matrices). Web-scraped texts are huge and have very low signal to noise ratio:
to overcome these issues, we adopted a framework known as False Positive
Reduction, which has seldom (if ever) been applied before to text
classification tasks. Several original contributions enable our processing
pipeline to reach good classification results. Empirical evidence shows that
our proposal outperforms all the alternative Machine Learning solutions already
tested in Istat for the same task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fausti_F/0/1/0/all/0/1"&gt;Fabrizio De Fausti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pugliese_F/0/1/0/all/0/1"&gt;Francesco Pugliese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zardetto_D/0/1/0/all/0/1"&gt;Diego Zardetto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$C^3$: Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08914</id>
        <link href="http://arxiv.org/abs/2106.08914"/>
        <updated>2021-06-17T01:58:41.308Z</updated>
        <summary type="html"><![CDATA[Video-grounded dialogue systems aim to integrate video understanding and
dialogue understanding to generate responses that are relevant to both the
dialogue and video context. Most existing approaches employ deep learning
models and have achieved remarkable performance, given the relatively small
datasets available. However, the results are partly accomplished by exploiting
biases in the datasets rather than developing multimodal reasoning, resulting
in limited generalization. In this paper, we propose a novel approach of
Compositional Counterfactual Contrastive Learning ($C^3$) to develop
contrastive training between factual and counterfactual samples in
video-grounded dialogues. Specifically, we design factual/counterfactual
sampling based on the temporal steps in videos and tokens in dialogues and
propose contrastive loss functions that exploit object-level or action-level
variance. Different from prior approaches, we focus on contrastive hidden state
representations among compositional output tokens to optimize the
representation space in a generation setting. We achieved promising performance
gains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the
benefits of our approach in grounding video and dialogue context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hung Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nancy F. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1"&gt;Steven C.H. Hoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Training of Acoustic Encoders for Speech Recognition. (arXiv:2106.08960v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08960</id>
        <link href="http://arxiv.org/abs/2106.08960"/>
        <updated>2021-06-17T01:58:41.287Z</updated>
        <summary type="html"><![CDATA[On-device speech recognition requires training models of different sizes for
deploying on devices with various computational budgets. When building such
different models, we can benefit from training them jointly to take advantage
of the knowledge shared between them. Joint training is also efficient since it
reduces the redundancy in the training procedure's data handling operations. We
propose a method for collaboratively training acoustic encoders of different
sizes for speech recognition. We use a sequence transducer setup where
different acoustic encoders share a common predictor and joiner modules. The
acoustic encoders are also trained using co-distillation through an auxiliary
task for frame level chenone prediction, along with the transducer loss. We
perform experiments using the LibriSpeech corpus and demonstrate that the
collaboratively trained acoustic encoders can provide up to a 11% relative
improvement in the word error rate on both the test partitions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagaraja_V/0/1/0/all/0/1"&gt;Varun Nagaraja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yangyang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1"&gt;Ganesh Venkatesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1"&gt;Ozlem Kalinli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1"&gt;Michael L. Seltzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1"&gt;Vikas Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RefBERT: Compressing BERT by Referencing to Pre-computed Representations. (arXiv:2106.08898v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08898</id>
        <link href="http://arxiv.org/abs/2106.08898"/>
        <updated>2021-06-17T01:58:41.281Z</updated>
        <summary type="html"><![CDATA[Recently developed large pre-trained language models, e.g., BERT, have
achieved remarkable performance in many downstream natural language processing
applications. These pre-trained language models often contain hundreds of
millions of parameters and suffer from high computation and latency in
real-world applications. It is desirable to reduce the computation overhead of
the models for fast training and inference while keeping the model performance
in downstream applications. Several lines of work utilize knowledge
distillation to compress the teacher model to a smaller student model. However,
they usually discard the teacher's knowledge when in inference. Differently, in
this paper, we propose RefBERT to leverage the knowledge learned from the
teacher, i.e., facilitating the pre-computed BERT representation on the
reference sample and compressing BERT into a smaller student model. To
guarantee our proposal, we provide theoretical justification on the loss
function and the usage of reference samples. Significantly, the theoretical
result shows that including the pre-computed teacher's representations on the
reference samples indeed increases the mutual information in learning the
student model. Finally, we conduct the empirical evaluation and show that our
RefBERT can beat the vanilla TinyBERT over 8.1\% and achieves more than 94\% of
the performance of $\BERTBASE$ on the GLUE benchmark. Meanwhile, RefBERT is
7.4x smaller and 9.5x faster on inference than BERT$_{\rm BASE}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haiqin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1"&gt;Yang Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jianping Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Spoken Language Understanding for Generalized Voice Assistants. (arXiv:2106.09009v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09009</id>
        <link href="http://arxiv.org/abs/2106.09009"/>
        <updated>2021-06-17T01:58:41.271Z</updated>
        <summary type="html"><![CDATA[End-to-end (E2E) spoken language understanding (SLU) systems predict
utterance semantics directly from speech using a single model. Previous work in
this area has focused on targeted tasks in fixed domains, where the output
semantic structure is assumed a priori and the input speech is of limited
complexity. In this work we present our approach to developing an E2E model for
generalized SLU in commercial voice assistants (VAs). We propose a fully
differentiable, transformer-based, hierarchical system that can be pretrained
at both the ASR and NLU levels. This is then fine-tuned on both transcription
and semantic classification losses to handle a diverse set of intent and
argument combinations. This leads to an SLU system that achieves significant
improvements over baselines on a complex internal generalized VA dataset with a
43% improvement in accuracy, while still meeting the 99% accuracy benchmark on
the popular Fluent Speech Commands dataset. We further evaluate our model on a
hard test set, exclusively containing slot arguments unseen in training, and
demonstrate a nearly 20% improvement, showing the efficacy of our approach in
truly demanding VA scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1"&gt;Michael Saxon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1"&gt;Samridhi Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKenna_J/0/1/0/all/0/1"&gt;Joseph P. McKenna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1"&gt;Athanasios Mouchtaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08977</id>
        <link href="http://arxiv.org/abs/2106.08977"/>
        <updated>2021-06-17T01:58:41.262Z</updated>
        <summary type="html"><![CDATA[Weak supervision has shown promising results in many natural language
processing tasks, such as Named Entity Recognition (NER). Existing work mainly
focuses on learning deep NER models only with weak supervision, i.e., without
any human annotation, and shows that by merely using weakly labeled data, one
can achieve good performance, though still underperforms fully supervised NER
with manually/strongly labeled data. In this paper, we consider a more
practical scenario, where we have both a small amount of strongly labeled data
and a large amount of weakly labeled data. Unfortunately, we observe that
weakly labeled data does not necessarily improve, or even deteriorate the model
performance (due to the extensive noise in the weak labels) when we train deep
NER models over a simple or weighted combination of the strongly labeled and
weakly labeled data. To address this issue, we propose a new multi-stage
computational framework -- NEEDLE with three essential ingredients: (1) weak
label completion, (2) noise-aware loss function, and (3) final fine-tuning over
the strongly labeled data. Through experiments on E-commerce query NER and
Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise
of the weak labels and outperforms existing methods. In particular, we achieve
new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,
BC5CDR-disease 90.69, NCBI-disease 92.28.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Danqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tianyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Bing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Discourse to Narrative: Knowledge Projection for Event Relation Extraction. (arXiv:2106.08629v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08629</id>
        <link href="http://arxiv.org/abs/2106.08629"/>
        <updated>2021-06-17T01:58:41.255Z</updated>
        <summary type="html"><![CDATA[Current event-centric knowledge graphs highly rely on explicit connectives to
mine relations between events. Unfortunately, due to the sparsity of
connectives, these methods severely undermine the coverage of EventKGs. The
lack of high-quality labelled corpora further exacerbates that problem. In this
paper, we propose a knowledge projection paradigm for event relation
extraction: projecting discourse knowledge to narratives by exploiting the
commonalities between them. Specifically, we propose Multi-tier Knowledge
Projection Network (MKPNet), which can leverage multi-tier discourse knowledge
effectively for event relation extraction. In this way, the labelled data
requirement is significantly reduced, and implicit event relations can be
effectively extracted. Intrinsic experimental results show that MKPNet achieves
the new state-of-the-art performance, and extrinsic experimental results verify
the value of the extracted event relations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jialong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hongyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1"&gt;Meng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yaojie Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xianpei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Le Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Weijian Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding Spatio-Temporal Language with Transformers. (arXiv:2106.08858v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.08858</id>
        <link href="http://arxiv.org/abs/2106.08858"/>
        <updated>2021-06-17T01:58:41.228Z</updated>
        <summary type="html"><![CDATA[Language is an interface to the outside world. In order for embodied agents
to use it, language must be grounded in other, sensorimotor modalities. While
there is an extended literature studying how machines can learn grounded
language, the topic of how to learn spatio-temporal linguistic concepts is
still largely uncharted. To make progress in this direction, we here introduce
a novel spatio-temporal language grounding task where the goal is to learn the
meaning of spatio-temporal descriptions of behavioral traces of an embodied
agent. This is achieved by training a truth function that predicts if a
description matches a given history of observations. The descriptions involve
time-extended predicates in past and present tense as well as spatio-temporal
references to objects in the scene. To study the role of architectural biases
in this task, we train several models including multimodal Transformer
architectures; the latter implement different attention computations between
words and objects across space and time. We test models on two classes of
generalization: 1) generalization to randomly held-out sentences; 2)
generalization to grammar primitives. We observe that maintaining object
identity in the attention computation of our Transformers is instrumental to
achieving good performance on generalization overall, and that summarizing
object traces in a single token has little influence on performance. We then
discuss how this opens new perspectives for language-guided autonomous embodied
agents. We also release our code under open-source license as well as
pretrained models and datasets to encourage the wider community to build upon
and extend our work in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1"&gt;Tristan Karch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1"&gt;Laetitia Teodorescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1"&gt;Katja Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Moulin-Frier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1"&gt;Pierre-Yves Oudeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alzheimer's Disease Detection from Spontaneous Speech through Combining Linguistic Complexity and (Dis)Fluency Features with Pretrained Language Models. (arXiv:2106.08689v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08689</id>
        <link href="http://arxiv.org/abs/2106.08689"/>
        <updated>2021-06-17T01:58:41.215Z</updated>
        <summary type="html"><![CDATA[In this paper, we combined linguistic complexity and (dis)fluency features
with pretrained language models for the task of Alzheimer's disease detection
of the 2021 ADReSSo (Alzheimer's Dementia Recognition through Spontaneous
Speech) challenge. An accuracy of 83.1% was achieved on the test set, which
amounts to an improvement of 4.23% over the baseline model. Our best-performing
model that integrated component models using a stacking ensemble technique
performed equally well on cross-validation and test data, indicating that it is
robust against overfitting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xuefeng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiechmann_D/0/1/0/all/0/1"&gt;Daniel Wiechmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerz_E/0/1/0/all/0/1"&gt;Elma Kerz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coreference Augmentation for Multi-Domain Task-Oriented Dialogue State Tracking. (arXiv:2106.08723v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08723</id>
        <link href="http://arxiv.org/abs/2106.08723"/>
        <updated>2021-06-17T01:58:41.207Z</updated>
        <summary type="html"><![CDATA[Dialogue State Tracking (DST), which is the process of inferring user goals
by estimating belief states given the dialogue history, plays a critical role
in task-oriented dialogue systems. A coreference phenomenon observed in
multi-turn conversations is not addressed by existing DST models, leading to
sub-optimal performances. In this paper, we propose Coreference Dialogue State
Tracker (CDST) that explicitly models the coreference feature. In particular,
at each turn, the proposed model jointly predicts the coreferred domain-slot
pair and extracts the coreference values from the dialogue context.
Experimental results on MultiWOZ 2.1 dataset show that the proposed model
achieves the state-of-the-art joint goal accuracy of 56.47%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1"&gt;Ting Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chongxuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-Based Keyword Localisation in Speech using Visual Grounding. (arXiv:2106.08859v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08859</id>
        <link href="http://arxiv.org/abs/2106.08859"/>
        <updated>2021-06-17T01:58:41.200Z</updated>
        <summary type="html"><![CDATA[Visually grounded speech models learn from images paired with spoken
captions. By tagging images with soft text labels using a trained visual
classifier with a fixed vocabulary, previous work has shown that it is possible
to train a model that can detect whether a particular text keyword occurs in
speech utterances or not. Here we investigate whether visually grounded speech
models can also do keyword localisation: predicting where, within an utterance,
a given textual keyword occurs without any explicit text-based or alignment
supervision. We specifically consider whether incorporating attention into a
convolutional model is beneficial for localisation. Although absolute
localisation performance with visually supervised models is still modest
(compared to using unordered bag-of-word text labels for supervision), we show
that attention provides a large gain in performance over previous visually
grounded models. As in many other speech-image studies, we find that many of
the incorrect localisations are due to semantic confusions, e.g. locating the
word 'backstroke' for the query keyword 'swimming'.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olaleye_K/0/1/0/all/0/1"&gt;Kayode Olaleye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1"&gt;Herman Kamper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation. (arXiv:2106.08942v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08942</id>
        <link href="http://arxiv.org/abs/2106.08942"/>
        <updated>2021-06-17T01:58:41.183Z</updated>
        <summary type="html"><![CDATA[Policy gradient algorithms have found wide adoption in NLP, but have recently
become subject to criticism, doubting their suitability for NMT. Choshen et al.
(2020) identify multiple weaknesses and suspect that their success is
determined by the shape of output distributions rather than the reward. In this
paper, we revisit these claims and study them under a wider range of
configurations. Our experiments on in-domain and cross-domain adaptation reveal
the importance of exploration and reward scaling, and provide empirical
counter-evidence to these claims.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kiegeland_S/0/1/0/all/0/1"&gt;Samuel Kiegeland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1"&gt;Julia Kreutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the expressiveness of neural vocoding with non-affine Normalizing Flows. (arXiv:2106.08649v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.08649</id>
        <link href="http://arxiv.org/abs/2106.08649"/>
        <updated>2021-06-17T01:58:41.166Z</updated>
        <summary type="html"><![CDATA[This paper proposes a general enhancement to the Normalizing Flows (NF) used
in neural vocoding. As a case study, we improve expressive speech vocoding with
a revamped Parallel Wavenet (PW). Specifically, we propose to extend the affine
transformation of PW to the more expressive invertible non-affine function. The
greater expressiveness of the improved PW leads to better-perceived signal
quality and naturalness in the waveform reconstruction and text-to-speech (TTS)
tasks. We evaluate the model across different speaking styles on a
multi-speaker, multi-lingual dataset. In the waveform reconstruction task, the
proposed model closes the naturalness and signal quality gap from the original
PW to recordings by $10\%$, and from other state-of-the-art neural vocoding
systems by more than $60\%$. We also demonstrate improvements in objective
metrics on the evaluation test set with L2 Spectral Distance and Cross-Entropy
reduced by $3\%$ and $6\unicode{x2030}$ comparing to the affine PW.
Furthermore, we extend the probability density distillation procedure proposed
by the original PW paper, so that it works with any non-affine invertible and
differentiable function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gabrys_A/0/1/0/all/0/1"&gt;Adam Gabry&amp;#x15b;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yunlong Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klimkov_V/0/1/0/all/0/1"&gt;Viacheslav Klimkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korzekwa_D/0/1/0/all/0/1"&gt;Daniel Korzekwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barra_Chicote_R/0/1/0/all/0/1"&gt;Roberto Barra-Chicote&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithm to Compilation Codesign: An Integrated View of Neural Network Sparsity. (arXiv:2106.08846v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08846</id>
        <link href="http://arxiv.org/abs/2106.08846"/>
        <updated>2021-06-17T01:58:41.071Z</updated>
        <summary type="html"><![CDATA[Reducing computation cost, inference latency, and memory footprint of neural
networks are frequently cited as research motivations for pruning and sparsity.
However, operationalizing those benefits and understanding the end-to-end
effect of algorithm design and regularization on the runtime execution is not
often examined in depth.

Here we apply structured and unstructured pruning to attention weights of
transformer blocks of the BERT language model, while also expanding block
sparse representation (BSR) operations in the TVM compiler. Integration of BSR
operations enables the TVM runtime execution to leverage structured pattern
sparsity induced by model regularization.

This integrated view of pruning algorithms enables us to study relationships
between modeling decisions and their direct impact on sparsity-enhanced
execution. Our main findings are: 1) we validate that performance benefits of
structured sparsity block regularization must be enabled by the BSR
augmentations to TVM, with 4x speedup relative to vanilla PyTorch and 2.2x
speedup relative to standard TVM compilation (without expanded BSR support). 2)
for BERT attention weights, the end-to-end optimal block sparsity shape in this
CPU inference context is not a square block (as in \cite{gray2017gpu}) but
rather a linear 32x1 block 3) the relationship between performance and block
size / shape is is suggestive of how model regularization parameters interact
with task scheduler optimizations resulting in the observed end-to-end
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1"&gt;Fu-Ming Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1"&gt;Austin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic sentence similarity: size does not always matter. (arXiv:2106.08648v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08648</id>
        <link href="http://arxiv.org/abs/2106.08648"/>
        <updated>2021-06-17T01:58:41.037Z</updated>
        <summary type="html"><![CDATA[This study addresses the question whether visually grounded speech
recognition (VGS) models learn to capture sentence semantics without access to
any prior linguistic knowledge. We produce synthetic and natural spoken
versions of a well known semantic textual similarity database and show that our
VGS model produces embeddings that correlate well with human semantic
similarity judgements. Our results show that a model trained on a small
image-caption database outperforms two models trained on much larger databases,
indicating that database size is not all that matters. We also investigate the
importance of having multiple captions per image and find that this is indeed
helpful even if the total number of images is lower, suggesting that
paraphrasing is a valuable learning signal. While the general trend in the
field is to create ever larger datasets to train models on, our findings
indicate other characteristics of the database can just as important important.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Merkx_D/0/1/0/all/0/1"&gt;Danny Merkx&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_S/0/1/0/all/0/1"&gt;Stefan L. Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ernestus_M/0/1/0/all/0/1"&gt;Mirjam Ernestus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Gender Bias in Hindi-English Machine Translation. (arXiv:2106.08680v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08680</id>
        <link href="http://arxiv.org/abs/2106.08680"/>
        <updated>2021-06-17T01:58:41.028Z</updated>
        <summary type="html"><![CDATA[With language models being deployed increasingly in the real world, it is
essential to address the issue of the fairness of their outputs. The word
embedding representations of these language models often implicitly draw
unwanted associations that form a social bias within the model. The nature of
gendered languages like Hindi, poses an additional problem to the
quantification and mitigation of bias, owing to the change in the form of the
words in the sentence, based on the gender of the subject. Additionally, there
is sparse work done in the realm of measuring and debiasing systems for Indic
languages. In our work, we attempt to evaluate and quantify the gender bias
within a Hindi-English machine translation system. We implement a modified
version of the existing TGBI metric based on the grammatical considerations for
Hindi. We also compare and contrast the resulting bias measurements across
multiple metrics for pre-trained embeddings and the ones learned by our machine
translation model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1"&gt;Gauri Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1"&gt;Krithika Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sanjay Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alternated Training with Synthetic and Authentic Data for Neural Machine Translation. (arXiv:2106.08582v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08582</id>
        <link href="http://arxiv.org/abs/2106.08582"/>
        <updated>2021-06-17T01:58:41.007Z</updated>
        <summary type="html"><![CDATA[While synthetic bilingual corpora have demonstrated their effectiveness in
low-resource neural machine translation (NMT), adding more synthetic data often
deteriorates translation performance. In this work, we propose alternated
training with synthetic and authentic data for NMT. The basic idea is to
alternate synthetic and authentic corpora iteratively during training. Compared
with previous work, we introduce authentic data as guidance to prevent the
training of NMT models from being disturbed by noisy synthetic data.
Experiments on Chinese-English and German-English translation tasks show that
our approach improves the performance over several strong baselines. We
visualize the BLEU landscape to further investigate the role of authentic and
synthetic data during alternated training. From the visualization, we find that
authentic data helps to direct the NMT model parameters towards points with
higher BLEU scores and leads to consistent translation performance improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_R/0/1/0/all/0/1"&gt;Rui Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zonghan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coreference-Aware Dialogue Summarization. (arXiv:2106.08556v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08556</id>
        <link href="http://arxiv.org/abs/2106.08556"/>
        <updated>2021-06-17T01:58:41.001Z</updated>
        <summary type="html"><![CDATA[Summarizing conversations via neural approaches has been gaining research
traction lately, yet it is still challenging to obtain practical solutions.
Examples of such challenges include unstructured information exchange in
dialogues, informal interactions between speakers, and dynamic role changes of
speakers as the dialogue evolves. Many of such challenges result in complex
coreference links. Therefore, in this work, we investigate different approaches
to explicitly incorporate coreference information in neural abstractive
dialogue summarization models to tackle the aforementioned challenges.
Experimental results show that the proposed approaches achieve state-of-the-art
performance, implying it is useful to utilize coreference information in
dialogue summarization. Evaluation results on factual correctness suggest such
coreference-aware models are better at tracing the information flow among
interlocutors and associating accurate status/actions with the corresponding
interlocutors and person mentions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1"&gt;Ke Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nancy F. Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PRASEMap: A Probabilistic Reasoning and Semantic Embedding based Knowledge Graph Alignment System. (arXiv:2106.08801v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08801</id>
        <link href="http://arxiv.org/abs/2106.08801"/>
        <updated>2021-06-17T01:58:40.968Z</updated>
        <summary type="html"><![CDATA[Knowledge Graph (KG) alignment aims at finding equivalent entities and
relations (i.e., mappings) between two KGs. The existing approaches utilize
either reasoning-based or semantic embedding-based techniques, but few studies
explore their combination. In this demonstration, we present PRASEMap, an
unsupervised KG alignment system that iteratively computes the Mappings with
both Probabilistic Reasoning (PR) And Semantic Embedding (SE) techniques.
PRASEMap can support various embedding-based KG alignment approaches as the SE
module, and enables easy human computer interaction that additionally provides
an option for users to feed the mapping annotations back to the system for
better results. The demonstration showcases these features via a stand-alone
Web application with user friendly interfaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhiyuan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Context Features Can Transformer Language Models Use?. (arXiv:2106.08367v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08367</id>
        <link href="http://arxiv.org/abs/2106.08367"/>
        <updated>2021-06-17T01:58:40.892Z</updated>
        <summary type="html"><![CDATA[Transformer-based language models benefit from conditioning on contexts of
hundreds to thousands of previous tokens. What aspects of these contexts
contribute to accurate model prediction? We describe a series of experiments
that measure usable information by selectively ablating lexical and structural
information in transformer language models trained on English Wikipedia. In
both mid- and long-range contexts, we find that several extremely destructive
context manipulations -- including shuffling word order within sentences and
deleting all words other than nouns -- remove less than 15% of the usable
information. Our results suggest that long contexts, but not their detailed
syntactic and propositional content, are important for the low perplexity of
current transformer language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+OConnor_J/0/1/0/all/0/1"&gt;Joe O&amp;#x27;Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Enrichment of Persona-grounded Dialog with Background Stories. (arXiv:2106.08364v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08364</id>
        <link href="http://arxiv.org/abs/2106.08364"/>
        <updated>2021-06-17T01:58:40.885Z</updated>
        <summary type="html"><![CDATA[Humans often refer to personal narratives, life experiences, and events to
make a conversation more engaging and rich. While persona-grounded dialog
models are able to generate responses that follow a given persona, they often
miss out on stating detailed experiences or events related to a persona, often
leaving conversations shallow and dull. In this work, we equip dialog models
with 'background stories' related to a persona by leveraging fictional
narratives from existing story datasets (e.g. ROCStories). Since current dialog
datasets do not contain such narratives as responses, we perform an
unsupervised adaptation of a retrieved story for generating a dialog response
using a gradient-based rewriting technique. Our proposed method encourages the
generated response to be fluent (i.e., highly likely) with the dialog history,
minimally different from the retrieved story to preserve event ordering and
consistent with the original persona. We demonstrate that our method can
generate responses that are more diverse, and are rated more engaging and
human-like by human evaluators, compared to outputs from existing dialog
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1"&gt;Bodhisattwa Prasad Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1"&gt;Harsh Jhamtani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Entity Linking through Semantic Reinforced Entity Embeddings. (arXiv:2106.08495v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08495</id>
        <link href="http://arxiv.org/abs/2106.08495"/>
        <updated>2021-06-17T01:58:40.825Z</updated>
        <summary type="html"><![CDATA[Entity embeddings, which represent different aspects of each entity with a
single vector like word embeddings, are a key component of neural entity
linking models. Existing entity embeddings are learned from canonical Wikipedia
articles and local contexts surrounding target entities. Such entity embeddings
are effective, but too distinctive for linking models to learn contextual
commonality. We propose a simple yet effective method, FGS2EE, to inject
fine-grained semantic information into entity embeddings to reduce the
distinctiveness and facilitate the learning of contextual commonality. FGS2EE
first uses the embeddings of semantic type words to generate semantic
embeddings, and then combines them with existing entity embeddings through
linear aggregation. Extensive experiments show the effectiveness of such
embeddings. Based on our entity embeddings, we achieved new sate-of-the-art
performance on entity linking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1"&gt;Feng Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruili Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jun He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yi Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Auto-regressive Variational Attention Models for Text Modeling. (arXiv:2106.08571v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08571</id>
        <link href="http://arxiv.org/abs/2106.08571"/>
        <updated>2021-06-17T01:58:40.807Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) have been widely applied for text modeling.
In practice, however, they are troubled by two challenges: information
underrepresentation and posterior collapse. The former arises as only the last
hidden state of LSTM encoder is transformed into the latent space, which is
generally insufficient to summarize the data. The latter is a long-standing
problem during the training of VAEs as the optimization is trapped to a
disastrous local optimum. In this paper, we propose Discrete Auto-regressive
Variational Attention Model (DAVAM) to address the challenges. Specifically, we
introduce an auto-regressive variational attention approach to enrich the
latent space by effectively capturing the semantic dependency from the input.
We further design discrete latent space for the variational attention and
mathematically show that our model is free from posterior collapse. Extensive
experiments on language modeling tasks demonstrate the superiority of DAVAM
against several VAE counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code to Comment Translation: A Comparative Study on Model Effectiveness & Errors. (arXiv:2106.08415v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.08415</id>
        <link href="http://arxiv.org/abs/2106.08415"/>
        <updated>2021-06-17T01:58:40.800Z</updated>
        <summary type="html"><![CDATA[Automated source code summarization is a popular software engineering
research topic wherein machine translation models are employed to "translate"
code snippets into relevant natural language descriptions. Most evaluations of
such models are conducted using automatic reference-based metrics. However,
given the relatively large semantic gap between programming languages and
natural language, we argue that this line of research would benefit from a
qualitative investigation into the various error modes of current
state-of-the-art models. Therefore, in this work, we perform both a
quantitative and qualitative comparison of three recently proposed source code
summarization models. In our quantitative evaluation, we compare the models
based on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics,
and in our qualitative evaluation, we perform a manual open-coding of the most
common errors committed by the models when compared to ground truth captions.
Our investigation reveals new insights into the relationship between
metric-based performance and model prediction errors grounded in an empirically
derived error taxonomy that can be used to drive future research efforts]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmud_J/0/1/0/all/0/1"&gt;Junayed Mahmud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1"&gt;Fahim Faisal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnob_R/0/1/0/all/0/1"&gt;Raihan Islam Arnob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1"&gt;Antonios Anastasopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moran_K/0/1/0/all/0/1"&gt;Kevin Moran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Eider: Evidence-enhanced Document-level Relation Extraction. (arXiv:2106.08657v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08657</id>
        <link href="http://arxiv.org/abs/2106.08657"/>
        <updated>2021-06-17T01:58:40.790Z</updated>
        <summary type="html"><![CDATA[Document-level relation extraction (DocRE) aims at extracting the semantic
relations among entity pairs in a document. In DocRE, a subset of the sentences
in a document, called the evidence sentences, might be sufficient for
predicting the relation between a specific entity pair. To make better use of
the evidence sentences, in this paper, we propose a three-stage
evidence-enhanced DocRE framework consisting of joint relation and evidence
extraction, evidence-centered relation extraction (RE), and fusion of
extraction results. We first jointly train an RE model with a simple and
memory-efficient evidence extraction model. Then, we construct pseudo documents
based on the extracted evidence sentences and run the RE model again. Finally,
we fuse the extraction results of the first two stages using a blending layer
and make a final prediction. Extensive experiments show that our proposed
framework achieves state-of-the-art performance on the DocRED dataset,
outperforming the second-best method by 0.76/0.82 Ign F1/F1. In particular, our
method significantly improves the performance on inter-sentence relations by
1.23 Inter F1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yiqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jiaming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sha Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Yuning Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Conversational Networks. (arXiv:2106.08484v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08484</id>
        <link href="http://arxiv.org/abs/2106.08484"/>
        <updated>2021-06-17T01:58:40.780Z</updated>
        <summary type="html"><![CDATA[Inspired by recent work in meta-learning and generative teaching networks, we
propose a framework called Generative Conversational Networks, in which
conversational agents learn to generate their own labelled training data (given
some seed data) and then train themselves from that data to perform a given
task. We use reinforcement learning to optimize the data generation process
where the reward signal is the agent's performance on the task. The task can be
any language-related task, from intent detection to full task-oriented
conversations. In this work, we show that our approach is able to generalise
from seed data and performs well in limited data and limited computation
settings, with significant gains for intent detection and slot tagging across
multiple datasets: ATIS, TOD, SNIPS, and Restaurants8k. We show an average
improvement of 35% in intent detection and 21% in slot tagging over a baseline
model trained from the seed data. We also conduct an analysis of the novelty of
the generated data and provide generated examples for intent detection, slot
tagging, and non-goal oriented conversations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1"&gt;Alexandros Papangelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1"&gt;Karthik Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1"&gt;Aishwarya Padmakumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seokhwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1"&gt;Gokhan Tur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1"&gt;Dilek Hakkani-Tur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RyanSpeech: A Corpus for Conversational Text-to-Speech Synthesis. (arXiv:2106.08468v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08468</id>
        <link href="http://arxiv.org/abs/2106.08468"/>
        <updated>2021-06-17T01:58:40.772Z</updated>
        <summary type="html"><![CDATA[This paper introduces RyanSpeech, a new speech corpus for research on
automated text-to-speech (TTS) systems. Publicly available TTS corpora are
often noisy, recorded with multiple speakers, or lack quality male speech data.
In order to meet the need for a high quality, publicly available male speech
corpus within the field of speech recognition, we have designed and created
RyanSpeech which contains textual materials from real-world conversational
settings. These materials contain over 10 hours of a professional male voice
actor's speech recorded at 44.1 kHz. This corpus's design and pipeline make
RyanSpeech ideal for developing TTS systems in real-world applications. To
provide a baseline for future research, protocols, and benchmarks, we trained 4
state-of-the-art speech models and a vocoder on RyanSpeech. The results show
3.36 in mean opinion scores (MOS) in our best model. We have made both the
corpus and trained models for public use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zandie_R/0/1/0/all/0/1"&gt;Rohola Zandie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoor_M/0/1/0/all/0/1"&gt;Mohammad H. Mahoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madsen_J/0/1/0/all/0/1"&gt;Julia Madsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emamian_E/0/1/0/all/0/1"&gt;Eshrat S. Emamian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training. (arXiv:2106.08616v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08616</id>
        <link href="http://arxiv.org/abs/2106.08616"/>
        <updated>2021-06-17T01:58:40.754Z</updated>
        <summary type="html"><![CDATA[Out-of-scope intent detection is of practical importance in task-oriented
dialogue systems. Since the distribution of outlier utterances is arbitrary and
unknown in the training stage, existing methods commonly rely on strong
assumptions on data distribution such as mixture of Gaussians to make
inference, resulting in either complex multi-step training procedures or
hand-crafted rules such as confidence threshold selection for outlier
detection. In this paper, we propose a simple yet effective method to train an
out-of-scope intent classifier in a fully end-to-end manner by simulating the
test scenario in training, which requires no assumption on data distribution
and no additional post-processing or threshold setting. Specifically, we
construct a set of pseudo outliers in the training stage, by generating
synthetic outliers using inliner features via self-supervision and sampling
out-of-scope sentences from easily available open-domain datasets. The pseudo
outliers are used to train a discriminative classifier that can be directly
applied to and generalize well on the test task. We evaluate our method
extensively on four benchmark dialogue datasets and observe significant
improvements over state-of-the-art approaches. Our code has been released at
https://github.com/liam0949/DCLOOS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1"&gt;Li-Ming Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Haowen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lu Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiao-Ming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_A/0/1/0/all/0/1"&gt;Albert Y.S. Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study. (arXiv:2106.08686v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08686</id>
        <link href="http://arxiv.org/abs/2106.08686"/>
        <updated>2021-06-17T01:58:40.737Z</updated>
        <summary type="html"><![CDATA[Several variants of deep neural networks have been successfully employed for
building parametric models that project variable-duration spoken word segments
onto fixed-size vector representations, or acoustic word embeddings (AWEs).
However, it remains unclear to what degree we can rely on the distance in the
emerging AWE space as an estimate of word-form similarity. In this paper, we
ask: does the distance in the acoustic embedding space correlate with
phonological dissimilarity? To answer this question, we empirically investigate
the performance of supervised approaches for AWEs with different neural
architectures and learning objectives. We train AWE models in controlled
settings for two languages (German and Czech) and evaluate the embeddings on
two tasks: word discrimination and phonological similarity. Our experiments
show that (1) the distance in the embedding space in the best cases only
moderately correlates with phonological distance, and (2) improving the
performance on the word discrimination task does not necessarily yield models
that better reflect word phonological similarity. Our findings highlight the
necessity to rethink the current intrinsic evaluations for AWEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdullah_B/0/1/0/all/0/1"&gt;Badr M. Abdullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1"&gt;Marius Mosbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaitova_I/0/1/0/all/0/1"&gt;Iuliia Zaitova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mobius_B/0/1/0/all/0/1"&gt;Bernd M&amp;#xf6;bius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1"&gt;Dietrich Klakow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Better Understanding of Linear Models for Recommendation. (arXiv:2105.12937v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12937</id>
        <link href="http://arxiv.org/abs/2105.12937"/>
        <updated>2021-06-17T01:58:40.720Z</updated>
        <summary type="html"><![CDATA[Recently, linear regression models, such as EASE and SLIM, have shown to
often produce rather competitive results against more sophisticated deep
learning models. On the other side, the (weighted) matrix factorization
approaches have been popular choices for recommendation in the past and widely
adopted in the industry. In this work, we aim to theoretically understand the
relationship between these two approaches, which are the cornerstones of
model-based recommendations. Through the derivation and analysis of the
closed-form solutions for two basic regression and matrix factorization
approaches, we found these two approaches are indeed inherently related but
also diverge in how they "scale-down" the singular values of the original
user-item interaction matrix. This analysis also helps resolve the questions
related to the regularization parameter range and model complexities. We
further introduce a new learning algorithm in searching (hyper)parameters for
the closed-form solution and utilize it to discover the nearby models of the
existing solutions. The experimental results demonstrate that the basic models
and their closed-form solutions are indeed quite competitive against the
state-of-the-art models, thus, confirming the validity of studying the basic
models. The effectiveness of exploring the nearby models are also
experimentally validated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Ruoming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jing Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yang Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Topic Coverage Approach to Evaluation of Topic Models. (arXiv:2012.06274v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06274</id>
        <link href="http://arxiv.org/abs/2012.06274"/>
        <updated>2021-06-17T01:58:40.701Z</updated>
        <summary type="html"><![CDATA[Topic models are widely used unsupervised models of text capable of learning
topics - weighted lists of words and documents - from large collections of text
documents. When topic models are used for discovery of topics in text
collections, a question that arises naturally is how well the model-induced
topics correspond to topics of interest to the analyst. In this paper we
revisit and extend a so far neglected approach to topic model evaluation based
on measuring topic coverage - computationally matching model topics with a set
of reference topics that models are expected to uncover. The approach is well
suited for analyzing models' performance in topic discovery and for large-scale
analysis of both topic models and measures of model quality. We propose new
measures of coverage and evaluate, in a series of experiments, different types
of topic models on two distinct text domains for which interest for topic
discovery exists. The experiments include evaluation of model quality, analysis
of coverage of distinct topic categories, and the analysis of the relationship
between coverage and other methods of topic model evaluation. The contributions
of the paper include new measures of coverage, insights into both topic models
and other methods of model evaluation, and the datasets and code for
facilitating future research of both topic coverage and other approaches to
topic model evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korencic_D/0/1/0/all/0/1"&gt;Damir Koren&amp;#x10d;i&amp;#x107;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ristov_S/0/1/0/all/0/1"&gt;Strahil Ristov&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Repar_J/0/1/0/all/0/1"&gt;Jelena Repar&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1"&gt;Jan &amp;#x160;najder&lt;/a&gt; (2) ((1) Rudjer Bo&amp;#x161;kovi&amp;#x107; Institute, Croatia, (2) University of Zagreb, Faculty of Electrical Engineering and Computing, Croatia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal and specific features of Ukrainian economic research: publication analysis based on Crossref data. (arXiv:2106.08701v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.08701</id>
        <link href="http://arxiv.org/abs/2106.08701"/>
        <updated>2021-06-17T01:58:40.608Z</updated>
        <summary type="html"><![CDATA[Our study is one of the first examples of multidimensional and longitudinal
disciplinary analysis at the national level based on Crossref data. We present
a large-scale quantitative analysis of Ukrainian economics. This study is not
yet another example of research aimed at ranking of local journals, authors or
institutions, but rather exploring general tendencies that can be compared to
other countries or regions. We study different aspects of Ukrainian economics
output. In particular, the collaborative nature, geographic landscape and some
peculiarities of citation statistics are investigated. We have found that
Ukrainian economics is characterized by a comparably small share of co-authored
publications, however, it demonstrates the tendency towards more collaborative
output. Based on our analysis, we discuss specific and universal features of
Ukrainian economic research. The importance of supporting various initiatives
aimed at enriching open scholarly metadata is considered. A comprehensive and
high-quality meta description of publications is probably the shortest path to
a better understanding of national trends, especially for non-English speaking
countries. The results of our analysis can be used to better understand
Ukrainian economic research and support research policy decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mryglod_O/0/1/0/all/0/1"&gt;O. Mryglod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nazarovets_S/0/1/0/all/0/1"&gt;S. Nazarovets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kozmenko_S/0/1/0/all/0/1"&gt;S. Kozmenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSSuBERT: Tweet Stream Summarization Using BERT. (arXiv:2106.08770v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08770</id>
        <link href="http://arxiv.org/abs/2106.08770"/>
        <updated>2021-06-17T01:58:40.264Z</updated>
        <summary type="html"><![CDATA[The development of deep neural networks and the emergence of pre-trained
language models such as BERT allow to increase performance on many NLP tasks.
However, these models do not meet the same popularity for tweet summarization,
which can probably be explained by the lack of existing collections for
training and evaluation. Our contribution in this paper is twofold : (1) we
introduce a large dataset for Twitter event summarization, and (2) we propose a
neural model to automatically summarize huge tweet streams. This extractive
model combines in an original way pre-trained language models and vocabulary
frequency-based representations to predict tweet salience. An additional
advantage of the model is that it automatically adapts the size of the output
summary according to the input tweet stream. We conducted experiments using two
different Twitter collections, and promising results are observed in comparison
with state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dusart_A/0/1/0/all/0/1"&gt;Alexis Dusart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinel_Sauvagnat_K/0/1/0/all/0/1"&gt;Karen Pinel-Sauvagnat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubert_G/0/1/0/all/0/1"&gt;Gilles Hubert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized News Recommendation: A Survey. (arXiv:2106.08934v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08934</id>
        <link href="http://arxiv.org/abs/2106.08934"/>
        <updated>2021-06-17T01:58:40.220Z</updated>
        <summary type="html"><![CDATA[Personalized news recommendation is an important technique to help users find
their interested news information and alleviate their information overload. It
has been extensively studied over decades and has achieved notable success in
improving users' news reading experience. However, there are still many
unsolved problems and challenges that need to be further studied. To help
researchers master the advances in personalized news recommendation over the
past years, in this paper we present a comprehensive overview of personalized
news recommendation. Instead of following the conventional taxonomy of news
recommendation methods, in this paper we propose a novel perspective to
understand personalized news recommendation based on its core problems and the
associated techniques and challenges. We first review the techniques for
tackling each core problem in a personalized news recommender system and the
challenges they face. Next, we introduce the public datasets and evaluation
metrics used for personalized news recommendation. We then discuss the key
points on improving the responsibility of personalized news recommender
systems. Finally, we raise several research directions that are worth
investigating in future. This paper can provide up-to-date and comprehensive
views to help readers understand the personalized news recommendation field. We
hope this paper can facilitate research on personalized news recommendation and
as well as related fields in natural language processing and data mining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TUTA: Tree-based Transformers for Generally Structured Table Pre-training. (arXiv:2010.12537v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12537</id>
        <link href="http://arxiv.org/abs/2010.12537"/>
        <updated>2021-06-17T01:58:40.200Z</updated>
        <summary type="html"><![CDATA[Tables are widely used with various structures to organize and present data.
Recent attempts on table understanding mainly focus on relational tables, yet
overlook to other common table structures. In this paper, we propose TUTA, a
unified pre-training architecture for understanding generally structured
tables. Noticing that understanding a table requires spatial, hierarchical, and
semantic information, we enhance transformers with three novel structure-aware
mechanisms. First, we devise a unified tree-based structure, called a
bi-dimensional coordinate tree, to describe both the spatial and hierarchical
information of generally structured tables. Upon this, we propose tree-based
attention and position embedding to better capture the spatial and hierarchical
information. Moreover, we devise three progressive pre-training objectives to
enable representations at the token, cell, and table levels. We pre-train TUTA
on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two
critical tasks in the field of table structure understanding: cell type
classification and table type classification. Experiments show that TUTA is
highly effective, achieving state-of-the-art on five widely-studied datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiruo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Haoyu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ran Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhiyi Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Mappings: Generating Open-Ended Expressive Mappings Using Variational Autoencoders. (arXiv:2106.08867v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.08867</id>
        <link href="http://arxiv.org/abs/2106.08867"/>
        <updated>2021-06-17T01:58:40.177Z</updated>
        <summary type="html"><![CDATA[In many contexts, creating mappings for gestural interactions can form part
of an artistic process. Creators seeking a mapping that is expressive, novel,
and affords them a sense of authorship may not know how to program it up in a
signal processing patch. Tools like Wekinator and MIMIC allow creators to use
supervised machine learning to learn mappings from example input/output
pairings. However, a creator may know a good mapping when they encounter it yet
start with little sense of what the inputs or outputs should be. We call this
an open-ended mapping process. Addressing this need, we introduce the latent
mapping, which leverages the latent space of an unsupervised machine learning
algorithm such as a Variational Autoencoder trained on a corpus of unlabelled
gestural data from the creator. We illustrate it with Sonified Body, a system
mapping full-body movement to sound which we explore in a residency with three
dancers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murray_Browne_T/0/1/0/all/0/1"&gt;Tim Murray-Browne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tigas_P/0/1/0/all/0/1"&gt;Panagiotis Tigas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved CNN-based Learning of Interpolation Filters for Low-Complexity Inter Prediction in Video Coding. (arXiv:2106.08936v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08936</id>
        <link href="http://arxiv.org/abs/2106.08936"/>
        <updated>2021-06-17T01:58:40.151Z</updated>
        <summary type="html"><![CDATA[The versatility of recent machine learning approaches makes them ideal for
improvement of next generation video compression solutions. Unfortunately,
these approaches typically bring significant increases in computational
complexity and are difficult to interpret into explainable models, affecting
their potential for implementation within practical video coding applications.
This paper introduces a novel explainable neural network-based inter-prediction
scheme, to improve the interpolation of reference samples needed for fractional
precision motion compensation. The approach requires a single neural network to
be trained from which a full quarter-pixel interpolation filter set is derived,
as the network is easily interpretable due to its linear structure. A novel
training framework enables each network branch to resemble a specific
fractional shift. This practical solution makes it very efficient to use
alongside conventional video coding schemes. When implemented in the context of
the state-of-the-art Versatile Video Coding (VVC) test model, 0.77%, 1.27% and
2.25% BD-rate savings can be achieved on average for lower resolution sequences
under the random access, low-delay B and low-delay P configurations,
respectively, while the complexity of the learned interpolation schemes is
significantly reduced compared to the interpolation with full CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1"&gt;Luka Murn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blasi_S/0/1/0/all/0/1"&gt;Saverio Blasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smeaton_A/0/1/0/all/0/1"&gt;Alan F. Smeaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1"&gt;Marta Mrak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topology Distillation for Recommender System. (arXiv:2106.08700v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08700</id>
        <link href="http://arxiv.org/abs/2106.08700"/>
        <updated>2021-06-17T01:58:40.114Z</updated>
        <summary type="html"><![CDATA[Recommender Systems (RS) have employed knowledge distillation which is a
model compression technique training a compact student model with the knowledge
transferred from a pre-trained large teacher model. Recent work has shown that
transferring knowledge from the teacher's intermediate layer significantly
improves the recommendation quality of the student. However, they transfer the
knowledge of individual representation point-wise and thus have a limitation in
that primary information of RS lies in the relations in the representation
space. This paper proposes a new topology distillation approach that guides the
student by transferring the topological structure built upon the relations in
the teacher space. We first observe that simply making the student learn the
whole topological structure is not always effective and even degrades the
student's performance. We demonstrate that because the capacity of the student
is highly limited compared to that of the teacher, learning the whole
topological structure is daunting for the student. To address this issue, we
propose a novel method named Hierarchical Topology Distillation (HTD) which
distills the topology hierarchically to cope with the large capacity gap. Our
extensive experiments on real-world datasets show that the proposed method
significantly outperforms the state-of-the-art competitors. We also provide
in-depth analyses to ascertain the benefit of distilling the topology for RS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;SeongKu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Junyoung Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kweon_W/0/1/0/all/0/1"&gt;Wonbin Kweon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hwanjo Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FAIR: Fairness-Aware Information Retrieval Evaluation. (arXiv:2106.08527v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08527</id>
        <link href="http://arxiv.org/abs/2106.08527"/>
        <updated>2021-06-17T01:58:40.089Z</updated>
        <summary type="html"><![CDATA[With the emerging needs of creating fairness-aware solutions for search and
recommendation systems, a daunting challenge exists of evaluating such
solutions. While many of the traditional information retrieval (IR) metrics can
capture the relevance, diversity and novelty for the utility with respect to
users, they are not suitable for inferring whether the presented results are
fair from the perspective of responsible information exposure. On the other
hand, various fairness metrics have been proposed but they do not account for
the user utility or do not measure it adequately. To address this problem, we
propose a new metric called Fairness-Aware IR (FAIR). By unifying standard IR
metrics and fairness measures into an integrated metric, this metric offers a
new perspective for evaluating fairness-aware ranking results. Based on this
metric, we developed an effective ranking algorithm that jointly optimized user
utility and fairness. The experimental results showed that our FAIR metric
could highlight results with good user utility and fair information exposure.
We showed how FAIR related to existing metrics and demonstrated the
effectiveness of our FAIR-based algorithm. We believe our work opens up a new
direction of pursuing a computationally feasible metric for evaluating and
implementing the fairness-aware IR systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruoyuan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1"&gt;Chirag Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysing Dense Passage Retrieval for Multi-hop Question Answering. (arXiv:2106.08433v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08433</id>
        <link href="http://arxiv.org/abs/2106.08433"/>
        <updated>2021-06-17T01:58:40.067Z</updated>
        <summary type="html"><![CDATA[We analyse the performance of passage retrieval models in the presence of
complex (multi-hop) questions to provide a better understanding of how
retrieval systems behave when multiple hops of reasoning are needed. In simple
open-domain question answering (QA), dense passage retrieval has become one of
the standard approaches for retrieving the relevant passages to infer an
answer. Recently, dense passage retrieval also achieved state-of-the-art
results in multi-hop QA, where aggregating information from multiple documents
and reasoning over them is required. However, so far, the dense retrieval
models are not evaluated properly concerning the multi-hop nature of the
problem: models are typically evaluated by the end result of the retrieval
pipeline, which leaves unclear where their success lies. In this work, we
provide an in-depth evaluation of such models not only unveiling the reasons
behind their success but also their limitations. Moreover, we introduce a
hybrid (lexical and dense) retrieval approach that is highly competitive with
the state-of-the-art dense retrieval model, while requiring substantially less
computational resources. Furthermore, we also perform qualitative analysis to
better understand the challenges behind passage retrieval for multi-hop QA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sidiropoulos_G/0/1/0/all/0/1"&gt;Georgios Sidiropoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voskarides_N/0/1/0/all/0/1"&gt;Nikos Voskarides&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakulenko_S/0/1/0/all/0/1"&gt;Svitlana Vakulenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1"&gt;Evangelos Kanoulas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections. (arXiv:2106.08908v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08908</id>
        <link href="http://arxiv.org/abs/2106.08908"/>
        <updated>2021-06-17T01:58:40.029Z</updated>
        <summary type="html"><![CDATA[Question answering (QA) systems for large document collections typically use
pipelines that (i) retrieve possibly relevant documents, (ii) re-rank them,
(iii) rank paragraphs or other snippets of the top-ranked documents, and (iv)
select spans of the top-ranked snippets as exact answers. Pipelines are
conceptually simple, but errors propagate from one component to the next,
without later components being able to revise earlier decisions. We present an
architecture for joint document and snippet ranking, the two middle stages,
which leverages the intuition that relevant documents have good snippets and
good snippets come from relevant documents. The architecture is general and can
be used with any neural text relevance ranker. We experiment with two main
instantiations of the architecture, based on POSIT-DRMM (PDRMM) and a
BERT-based ranker. Experiments on biomedical data from BIOASQ show that our
joint models vastly outperform the pipelines in snippet retrieval, the main
goal for QA, with fewer trainable parameters, also remaining competitive in
document retrieval. Furthermore, our joint PDRMM-based model is competitive
with BERT-based models, despite using orders of magnitude fewer parameters.
These claims are also supported by human evaluation on two test batches of
BIOASQ. To test our key findings on another dataset, we modified the Natural
Questions dataset so that it can also be used for document and snippet
retrieval. Our joint PDRMM-based model again outperforms the corresponding
pipeline in snippet retrieval on the modified Natural Questions dataset, even
though it performs worse than the pipeline in document retrieval. We make our
code and the modified Natural Questions dataset publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pappas_D/0/1/0/all/0/1"&gt;Dimitris Pappas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1"&gt;Ion Androutsopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Latent Vector Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation. (arXiv:2106.08188v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08188</id>
        <link href="http://arxiv.org/abs/2106.08188"/>
        <updated>2021-06-16T01:21:12.922Z</updated>
        <summary type="html"><![CDATA[This paper addresses the domain shift problem for segmentation. As a
solution, we propose OLVA, a novel and lightweight unsupervised domain
adaptation method based on a Variational Auto-Encoder (VAE) and Optimal
Transport (OT) theory. Thanks to the VAE, our model learns a shared
cross-domain latent space that follows a normal distribution, which reduces the
domain shift. To guarantee valid segmentations, our shared latent space is
designed to model the shape rather than the intensity variations. We further
rely on an OT loss to match and align the remaining discrepancy between the two
domains in the latent space. We demonstrate OLVA's effectiveness for the
segmentation of multiple cardiac structures on the public Multi-Modality Whole
Heart Segmentation (MM-WHS) dataset, where the source domain consists of
annotated 3D MR images and the unlabelled target domain of 3D CTs. Our results
show remarkable improvements with an additional margin of 12.5\% dice score
over concurrent generative training approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chanti_D/0/1/0/all/0/1"&gt;Dawood Al Chanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mateus_D/0/1/0/all/0/1"&gt;Diana Mateus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-16T01:21:12.915Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-the-Air Decentralized Federated Learning. (arXiv:2106.08011v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.08011</id>
        <link href="http://arxiv.org/abs/2106.08011"/>
        <updated>2021-06-16T01:21:12.908Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider decentralized federated learning (FL) over
wireless networks, where over-the-air computation (AirComp) is adopted to
facilitate the local model consensus in a device-to-device (D2D) communication
manner. However, the AirComp-based consensus phase brings the additive noise in
each algorithm iterate and the consensus needs to be robust to wireless network
topology changes, which introduce a coupled and novel challenge of establishing
the convergence for wireless decentralized FL algorithm. To facilitate
consensus phase, we propose an AirComp-based DSGD with gradient tracking and
variance reduction (DSGT-VR) algorithm, where both precoding and decoding
strategies are developed for D2D communication. Furthermore, we prove that the
proposed algorithm converges linearly and establish the optimality gap for
strongly convex and smooth loss functions, taking into account the channel
fading and noise. The theoretical result shows that the additional error bound
in the optimality gap depends on the number of devices. Extensive simulations
verify the theoretical results and show that the proposed algorithm outperforms
other benchmark decentralized FL algorithms over wireless networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yandong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuanming Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis. (arXiv:2106.07049v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07049</id>
        <link href="http://arxiv.org/abs/2106.07049"/>
        <updated>2021-06-16T01:21:12.902Z</updated>
        <summary type="html"><![CDATA[In the last few years, deep learning classifiers have shown promising results
in image-based medical diagnosis. However, interpreting the outputs of these
models remains a challenge. In cancer diagnosis, interpretability can be
achieved by localizing the region of the input image responsible for the
output, i.e. the location of a lesion. Alternatively, segmentation or detection
models can be trained with pixel-wise annotations indicating the locations of
malignant lesions. Unfortunately, acquiring such labels is labor-intensive and
requires medical expertise. To overcome this difficulty, weakly-supervised
localization can be utilized. These methods allow neural network classifiers to
output saliency maps highlighting the regions of the input most relevant to the
classification task (e.g. malignant lesions in mammograms) using only
image-level labels (e.g. whether the patient has cancer or not) during
training. When applied to high-resolution images, existing methods produce
low-resolution saliency maps. This is problematic in applications in which
suspicious lesions are small in relation to the image size. In this work, we
introduce a novel neural network architecture to perform weakly-supervised
segmentation of high-resolution images. The proposed model selects regions of
interest via coarse-level localization, and then performs fine-grained
segmentation of those regions. We apply this model to breast cancer diagnosis
with screening mammography, and validate it on a large clinically-realistic
dataset. Measured by Dice similarity score, our approach outperforms existing
methods by a large margin in terms of localization performance of benign and
malignant lesions, relatively improving the performance by 39.6% and 20.0%,
respectively. Code and the weights of some of the models are available at
https://github.com/nyukat/GLAM]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kangning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yiqiu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1"&gt;Nan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1"&gt;Jakub Ch&amp;#x142;&amp;#x119;dowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1"&gt;Carlos Fernandez-Granda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1"&gt;Krzysztof J. Geras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Reinforcement Learning from Demonstrations. (arXiv:2106.08050v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08050</id>
        <link href="http://arxiv.org/abs/2106.08050"/>
        <updated>2021-06-16T01:21:12.895Z</updated>
        <summary type="html"><![CDATA[Residual reinforcement learning (RL) has been proposed as a way to solve
challenging robotic tasks by adapting control actions from a conventional
feedback controller to maximize a reward signal. We extend the residual
formulation to learn from visual inputs and sparse rewards using
demonstrations. Learning from images, proprioceptive inputs and a sparse
task-completion reward relaxes the requirement of accessing full state
features, such as object and target positions. In addition, replacing the base
controller with a policy learned from demonstrations removes the dependency on
a hand-engineered controller in favour of a dataset of demonstrations, which
can be provided by non-experts. Our experimental evaluation on simulated
manipulation tasks on a 6-DoF UR5 arm and a 28-DoF dexterous hand demonstrates
that residual RL from demonstrations is able to generalize to unseen
environment conditions more flexibly than either behavioral cloning or RL
fine-tuning, and is capable of solving high-dimensional, sparse-reward tasks
out of reach for RL from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alakuijala_M/0/1/0/all/0/1"&gt;Minttu Alakuijala&lt;/a&gt; (WILLOW, Thoth), &lt;a href="http://arxiv.org/find/cs/1/au:+Dulac_Arnold_G/0/1/0/all/0/1"&gt;Gabriel Dulac-Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1"&gt;Julien Mairal&lt;/a&gt; (Thoth), &lt;a href="http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1"&gt;Jean Ponce&lt;/a&gt; (WILLOW), &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1"&gt;Cordelia Schmid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascading Convolutional Temporal Colour Constancy. (arXiv:2106.07955v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07955</id>
        <link href="http://arxiv.org/abs/2106.07955"/>
        <updated>2021-06-16T01:21:12.878Z</updated>
        <summary type="html"><![CDATA[Computational Colour Constancy (CCC) consists of estimating the colour of one
or more illuminants in a scene and using them to remove unwanted chromatic
distortions. Much research has focused on illuminant estimation for CCC on
single images, with few attempts of leveraging the temporal information
intrinsic in sequences of correlated images (e.g., the frames in a video), a
task known as Temporal Colour Constancy (TCC). The state-of-the-art for TCC is
TCCNet, a deep-learning architecture that uses a ConvLSTM for aggregating the
encodings produced by CNN submodules for each image in a sequence. We extend
this architecture with different models obtained by (i) substituting the TCCNet
submodules with C4, the state-of-the-art method for CCC targeting images; (ii)
adding a cascading strategy to perform an iterative improvement of the estimate
of the illuminant. We tested our models on the recently released TCC benchmark
and achieved results that surpass the state-of-the-art. Analyzing the impact of
the number of frames involved in illuminant estimation on performance, we show
that it is possible to reduce inference time by training the models on few
selected frames from the sequences while retaining comparable accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rizzo_M/0/1/0/all/0/1"&gt;Matteo Rizzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conati_C/0/1/0/all/0/1"&gt;Cristina Conati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1"&gt;Daesik Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hui Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Combinatorial Node Labeling Algorithms. (arXiv:2106.03594v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03594</id>
        <link href="http://arxiv.org/abs/2106.03594"/>
        <updated>2021-06-16T01:21:12.871Z</updated>
        <summary type="html"><![CDATA[We present a graph neural network to learn graph coloring heuristics using
reinforcement learning. Our learned deterministic heuristics give better
solutions than classical degree-based greedy heuristics and only take seconds
to evaluate on graphs with tens of thousands of vertices. As our approach is
based on policy-gradients, it also learns a probabilistic policy as well. These
probabilistic policies outperform all greedy coloring baselines and a machine
learning baseline. Our approach generalizes several previous machine-learning
frameworks, which applied to problems like minimum vertex cover. We also
demonstrate that our approach outperforms two greedy heuristics on minimum
vertex cover.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gianinazzi_L/0/1/0/all/0/1"&gt;Lukas Gianinazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fries_M/0/1/0/all/0/1"&gt;Maximilian Fries&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dryden_N/0/1/0/all/0/1"&gt;Nikoli Dryden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Nun_T/0/1/0/all/0/1"&gt;Tal Ben-Nun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besta_M/0/1/0/all/0/1"&gt;Maciej Besta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1"&gt;Torsten Hoefler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Fair Averaging. (arXiv:2104.14937v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14937</id>
        <link href="http://arxiv.org/abs/2104.14937"/>
        <updated>2021-06-16T01:21:12.865Z</updated>
        <summary type="html"><![CDATA[Fairness has emerged as a critical problem in federated learning (FL). In
this work, we identify a cause of unfairness in FL -- conflicting gradients
with large differences in the magnitudes. To address this issue, we propose the
federated fair averaging (FedFV) algorithm to mitigate potential conflicts
among clients before averaging their gradients. We first use the cosine
similarity to detect gradient conflicts, and then iteratively eliminate such
conflicts by modifying both the direction and the magnitude of the gradients.
We further show the theoretical foundation of FedFV to mitigate the issue
conflicting gradients and converge to Pareto stationary solutions. Extensive
experiments on a suite of federated datasets confirm that FedFV compares
favorably against state-of-the-art methods in terms of fairness, accuracy and
efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiaoliang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jianzhong Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1"&gt;Chenglu Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Rongshan Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Assessment of Federated Learning using Private Personalized Layers. (arXiv:2106.08060v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.08060</id>
        <link href="http://arxiv.org/abs/2106.08060"/>
        <updated>2021-06-16T01:21:12.859Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a collaborative scheme to train a learning model
across multiple participants without sharing data. While FL is a clear step
forward towards enforcing users' privacy, different inference attacks have been
developed. In this paper, we quantify the utility and privacy trade-off of a FL
scheme using private personalized layers. While this scheme has been proposed
as local adaptation to improve the accuracy of the model through local
personalization, it has also the advantage to minimize the information about
the model exchanged with the server. However, the privacy of such a scheme has
never been quantified. Our evaluations using motion sensor dataset show that
personalized layers speed up the convergence of the model and slightly improve
the accuracy for all users compared to a standard FL scheme while better
preventing both attribute and membership inferences compared to a FL scheme
using local differential privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jourdan_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Jourdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boutet_A/0/1/0/all/0/1"&gt;Antoine Boutet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frindel_C/0/1/0/all/0/1"&gt;Carole Frindel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Transformers. (arXiv:2106.04554v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04554</id>
        <link href="http://arxiv.org/abs/2106.04554"/>
        <updated>2021-06-16T01:21:12.833Z</updated>
        <summary type="html"><![CDATA[Transformers have achieved great success in many artificial intelligence
fields, such as natural language processing, computer vision, and audio
processing. Therefore, it is natural to attract lots of interest from academic
and industry researchers. Up to the present, a great variety of Transformer
variants (a.k.a. X-formers) have been proposed, however, a systematic and
comprehensive literature review on these Transformer variants is still missing.
In this survey, we provide a comprehensive review of various X-formers. We
first briefly introduce the vanilla Transformer and then propose a new taxonomy
of X-formers. Next, we introduce the various X-formers from three perspectives:
architectural modification, pre-training, and applications. Finally, we outline
some potential directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Characterization of Fair Machine Learning For Clinical Risk Prediction. (arXiv:2007.10306v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10306</id>
        <link href="http://arxiv.org/abs/2007.10306"/>
        <updated>2021-06-16T01:21:12.815Z</updated>
        <summary type="html"><![CDATA[The use of machine learning to guide clinical decision making has the
potential to worsen existing health disparities. Several recent works frame the
problem as that of algorithmic fairness, a framework that has attracted
considerable attention and criticism. However, the appropriateness of this
framework is unclear due to both ethical as well as technical considerations,
the latter of which include trade-offs between measures of fairness and model
performance that are not well-understood for predictive models of clinical
outcomes. To inform the ongoing debate, we conduct an empirical study to
characterize the impact of penalizing group fairness violations on an array of
measures of model performance and group fairness. We repeat the analyses across
multiple observational healthcare databases, clinical outcomes, and sensitive
attributes. We find that procedures that penalize differences between the
distributions of predictions across groups induce nearly-universal degradation
of multiple performance metrics within groups. On examining the secondary
impact of these procedures, we observe heterogeneity of the effect of these
procedures on measures of fairness in calibration and ranking across
experimental conditions. Beyond the reported trade-offs, we emphasize that
analyses of algorithmic fairness in healthcare lack the contextual grounding
and causal awareness necessary to reason about the mechanisms that lead to
health disparities, as well as about the potential of algorithmic fairness
methods to counteract those mechanisms. In light of these limitations, we
encourage researchers building predictive models for clinical use to step
outside the algorithmic fairness frame and engage critically with the broader
sociotechnical context surrounding the use of machine learning in healthcare.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pfohl_S/0/1/0/all/0/1"&gt;Stephen R. Pfohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Foryciarz_A/0/1/0/all/0/1"&gt;Agata Foryciarz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shah_N/0/1/0/all/0/1"&gt;Nigam H. Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Mixture of Posteriors for Counterfactual Inference, Data Integration and Fairness. (arXiv:2106.08161v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08161</id>
        <link href="http://arxiv.org/abs/2106.08161"/>
        <updated>2021-06-16T01:21:12.807Z</updated>
        <summary type="html"><![CDATA[Learning meaningful representations of data that can address challenges such
as batch effect correction, data integration and counterfactual inference is a
central problem in many domains including computational biology. Adopting a
Conditional VAE framework, we identify the mathematical principle that unites
these challenges: learning a representation that is marginally independent of a
condition variable. We therefore propose the Contrastive Mixture of Posteriors
(CoMP) method that uses a novel misalignment penalty to enforce this
independence. This penalty is defined in terms of mixtures of the variational
posteriors themselves, unlike prior work which uses external discrepancy
measures such as MMD to ensure independence in latent space. We show that CoMP
has attractive theoretical properties compared to previous approaches,
especially when there is complex global structure in latent space. We further
demonstrate state of the art performance on a number of real-world problems,
including the challenging tasks of aligning human tumour samples with cancer
cell-lines and performing counterfactual inference on single-cell RNA
sequencing data. Incidentally, we find parallels with the fair representation
learning literature, and demonstrate CoMP has competitive performance in
learning fair yet expressive latent representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1"&gt;Adam Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vezer_A/0/1/0/all/0/1"&gt;&amp;#xc1;rpi Vez&amp;#xe9;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Glastonbury_C/0/1/0/all/0/1"&gt;Craig A Glastonbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Creed_P/0/1/0/all/0/1"&gt;P&amp;#xe1;id&amp;#xed; Creed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Abujudeh_S/0/1/0/all/0/1"&gt;Sam Abujudeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sim_A/0/1/0/all/0/1"&gt;Aaron Sim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07306</id>
        <link href="http://arxiv.org/abs/2106.07306"/>
        <updated>2021-06-16T01:21:12.800Z</updated>
        <summary type="html"><![CDATA[In structured prediction, a major challenge for models is to represent the
interdependencies within their output structures. For the common case where
outputs are structured as a sequence, linear-chain conditional random fields
(CRFs) are a widely used model class which can learn local dependencies in
output sequences. However, the CRF's Markov assumption makes it impossible for
these models to capture nonlocal dependencies, and standard CRFs are unable to
respect nonlocal constraints of the data (such as global arity constraints on
output labels). We present a generalization of CRFs that can enforce a broad
class of constraints, including nonlocal ones, by specifying the space of
possible output structures as a regular language $\mathcal{L}$. The resulting
regular-constrained CRF (RegCCRF) has the same formal properties as a standard
CRF, but assigns zero probability to all label sequences not in $\mathcal{L}$.
Notably, RegCCRFs can incorporate their constraints during training, while
related models only enforce constraints during decoding. We prove that
constrained training is never worse than constrained decoding, and show using
synthetic data that it can be substantially better in practice. Additionally,
we demonstrate a practical benefit on downstream tasks by incorporating a
RegCCRF into a deep neural model for semantic role labeling, exceeding
state-of-the-art results on a standard dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papay_S/0/1/0/all/0/1"&gt;Sean Papay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1"&gt;Roman Klinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1"&gt;Sebastian Pad&amp;#xf3;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal consistency of Wasserstein $k$-NN classifier. (arXiv:2009.04651v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04651</id>
        <link href="http://arxiv.org/abs/2009.04651"/>
        <updated>2021-06-16T01:21:12.794Z</updated>
        <summary type="html"><![CDATA[The Wasserstein distance provides a notion of dissimilarities between
probability measures, which has recent applications in learning of structured
data with varying size such as images and text documents. In this work, we
analyze the $k$-nearest neighbor classifier ($k$-NN) under the Wasserstein
distance and establish the universal consistency on families of distributions.
Using previous known results on the consistency of the $k$-NN classifier on
infinite dimensional metric spaces, it suffices to show that the families is a
countable union of finite dimension sets. As a result, we show that the $k$-NN
classifier is universally consistent on spaces of finitely supported measures,
the space of Gaussian measures, and the space of measures with finite wavelet
densities. In addition, we give a counterexample to show that the universal
consistency does not hold on $\mathcal{W}_p((0,1))$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ponnoprat_D/0/1/0/all/0/1"&gt;Donlapark Ponnoprat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A baseline for semi-supervised learning of efficient semantic segmentation models. (arXiv:2106.07075v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07075</id>
        <link href="http://arxiv.org/abs/2106.07075"/>
        <updated>2021-06-16T01:21:12.771Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning is especially interesting in the dense prediction
context due to high cost of pixel-level ground truth. Unfortunately, most such
approaches are evaluated on outdated architectures which hamper research due to
very slow training and high requirements on GPU RAM. We address this concern by
presenting a simple and effective baseline which works very well both on
standard and efficient architectures. Our baseline is based on one-way
consistency and non-linear geometric and photometric perturbations. We show
advantage of perturbing only the student branch and present a plausible
explanation of such behaviour. Experiments on Cityscapes and CIFAR-10
demonstrate competitive performance with respect to prior work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1"&gt;Ivan Grubi&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orsic_M/0/1/0/all/0/1"&gt;Marin Or&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1"&gt;Sini&amp;#x161;a &amp;#x160;egvi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Epidemic modelling of multiple virus strains:a case study of SARS-CoV-2 B.1.1.7 in Moscow. (arXiv:2106.08048v1 [q-bio.PE])]]></title>
        <id>http://arxiv.org/abs/2106.08048</id>
        <link href="http://arxiv.org/abs/2106.08048"/>
        <updated>2021-06-16T01:21:12.744Z</updated>
        <summary type="html"><![CDATA[During a long-running pandemic a pathogen can mutate, producing new strains
with different epidemiological parameters. Existing approaches to epidemic
modelling only consider one virus strain. We have developed a modified SEIR
model to simulate multiple virus strains within the same population. As a case
study, we investigate the potential effects of SARS-CoV-2 strain B.1.1.7 on the
city of Moscow. Our analysis indicates a high risk of a new wave of infections
in September-October 2021 with up to 35 000 daily infections at peak. We
open-source our code and data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Tseytlin_B/0/1/0/all/0/1"&gt;Boris Tseytlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Makarov_I/0/1/0/all/0/1"&gt;Ilya Makarov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PRANK: motion Prediction based on RANKing. (arXiv:2010.12007v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12007</id>
        <link href="http://arxiv.org/abs/2010.12007"/>
        <updated>2021-06-16T01:21:12.602Z</updated>
        <summary type="html"><![CDATA[Predicting the motion of agents such as pedestrians or human-driven vehicles
is one of the most critical problems in the autonomous driving domain. The
overall safety of driving and the comfort of a passenger directly depend on its
successful solution. The motion prediction problem also remains one of the most
challenging problems in autonomous driving engineering, mainly due to high
variance of the possible agent's future behavior given a situation. The two
phenomena responsible for the said variance are the multimodality caused by the
uncertainty of the agent's intent (e.g., turn right or move forward) and
uncertainty in the realization of a given intent (e.g., which lane to turn
into). To be useful within a real-time autonomous driving pipeline, a motion
prediction system must provide efficient ways to describe and quantify this
uncertainty, such as computing posterior modes and their probabilities or
estimating density at the point corresponding to a given trajectory. It also
should not put substantial density on physically impossible trajectories, as
they can confuse the system processing the predictions. In this paper, we
introduce the PRANK method, which satisfies these requirements. PRANK takes
rasterized bird-eye images of agent's surroundings as an input and extracts
features of the scene with a convolutional neural network. It then produces the
conditional distribution of agent's trajectories plausible in the given scene.
The key contribution of PRANK is a way to represent that distribution using
nearest-neighbor methods in latent trajectory space, which allows for efficient
inference in real time. We evaluate PRANK on the in-house and Argoverse
datasets, where it shows competitive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biktairov_Y/0/1/0/all/0/1"&gt;Yuriy Biktairov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stebelev_M/0/1/0/all/0/1"&gt;Maxim Stebelev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudenko_I/0/1/0/all/0/1"&gt;Irina Rudenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shliazhko_O/0/1/0/all/0/1"&gt;Oleh Shliazhko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yangel_B/0/1/0/all/0/1"&gt;Boris Yangel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Beyond Classification Accuracy Metrics in Model Compression. (arXiv:2012.01604v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01604</id>
        <link href="http://arxiv.org/abs/2012.01604"/>
        <updated>2021-06-16T01:21:12.595Z</updated>
        <summary type="html"><![CDATA[With the rise in edge-computing devices, there has been an increasing demand
to deploy energy and resource-efficient models. A large body of research has
been devoted to developing methods that can reduce the size of the model
considerably without affecting the standard metrics such as top-1 accuracy.
However, these pruning approaches tend to result in a significant mismatch in
other metrics such as fairness across classes and explainability. To combat
such misalignment, we propose a novel multi-part loss function inspired by the
knowledge-distillation literature. Through extensive experiments, we
demonstrate the effectiveness of our approach across different compression
algorithms, architectures, tasks as well as datasets. In particular, we obtain
up to $4.1\times$ reduction in the number of prediction mismatches between the
compressed and reference models, and up to $5.7\times$ in cases where the
reference model makes the correct prediction; all while making no changes to
the compression algorithm, and minor modifications to the loss function.
Furthermore, we demonstrate how inducing simple alignment between the
predictions of the models naturally improves the alignment on other metrics
including fairness and attributions. Our framework can thus serve as a simple
plug-and-play component for compression algorithms in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_V/0/1/0/all/0/1"&gt;Vinu Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1"&gt;Shoaib Ahmed Siddiqui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhaskara_A/0/1/0/all/0/1"&gt;Aditya Bhaskara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muralidharan_S/0/1/0/all/0/1"&gt;Saurav Muralidharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garland_M/0/1/0/all/0/1"&gt;Michael Garland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sheraz Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1"&gt;Andreas Dengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploration in Online Advertising Systems with Deep Uncertainty-Aware Learning. (arXiv:2012.02298v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02298</id>
        <link href="http://arxiv.org/abs/2012.02298"/>
        <updated>2021-06-16T01:21:12.587Z</updated>
        <summary type="html"><![CDATA[Modern online advertising systems inevitably rely on personalization methods,
such as click-through rate (CTR) prediction. Recent progress in CTR prediction
enjoys the rich representation capabilities of deep learning and achieves great
success in large-scale industrial applications. However, these methods can
suffer from lack of exploration. Another line of prior work addresses the
exploration-exploitation trade-off problem with contextual bandit methods,
which are recently less studied in the industry due to the difficulty in
extending their flexibility with deep models. In this paper, we propose a novel
Deep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on
Gaussian processes, which can provide predictive uncertainty estimations while
maintaining the flexibility of deep neural networks. DUAL can be easily
implemented on existing models and deployed in real-time systems with minimal
extra computational overhead. By linking the predictive uncertainty estimation
ability of DUAL to well-known bandit algorithms, we further present DUAL-based
Ad-ranking strategies to boost up long-term utilities such as the social
welfare in advertising systems. Experimental results on several public datasets
demonstrate the effectiveness of our methods. Remarkably, an online A/B test
deployed in the Alibaba display advertising platform shows an 8.2% social
welfare improvement and an 8.0% revenue lift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1"&gt;Chao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhifeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1"&gt;Shuo Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lining Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1"&gt;Yifan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1"&gt;Kun Gai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kuang-chih Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Optimization Methods for Extreme Similarity Learning with Nonlinear Embeddings. (arXiv:2010.13511v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13511</id>
        <link href="http://arxiv.org/abs/2010.13511"/>
        <updated>2021-06-16T01:21:12.532Z</updated>
        <summary type="html"><![CDATA[We study the problem of learning similarity by using nonlinear embedding
models (e.g., neural networks) from all possible pairs. This problem is
well-known for its difficulty of training with the extreme number of pairs. For
the special case of using linear embeddings, many studies have addressed this
issue of handling all pairs by considering certain loss functions and
developing efficient optimization algorithms. This paper aims to extend results
for general nonlinear embeddings. First, we finish detailed derivations and
provide clean formulations for efficiently calculating some building blocks of
optimization algorithms such as function, gradient evaluation, and
Hessian-vector product. The result enables the use of many optimization methods
for extreme similarity learning with nonlinear embeddings. Second, we study
some optimization methods in detail. Due to the use of nonlinear embeddings,
implementation issues different from linear cases are addressed. In the end,
some methods are shown to be highly efficient for extreme similarity learning
with nonlinear embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yuan_B/0/1/0/all/0/1"&gt;Bowen Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu-Sheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Quan_P/0/1/0/all/0/1"&gt;Pengrui Quan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chih-Jen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Regret Bounds for Online Submodular Maximization. (arXiv:2106.07836v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07836</id>
        <link href="http://arxiv.org/abs/2106.07836"/>
        <updated>2021-06-16T01:21:11.497Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider an online optimization problem over $T$ rounds
where at each step $t\in[T]$, the algorithm chooses an action $x_t$ from the
fixed convex and compact domain set $\mathcal{K}$. A utility function
$f_t(\cdot)$ is then revealed and the algorithm receives the payoff $f_t(x_t)$.
This problem has been previously studied under the assumption that the
utilities are adversarially chosen monotone DR-submodular functions and
$\mathcal{O}(\sqrt{T})$ regret bounds have been derived. We first characterize
the class of strongly DR-submodular functions and then, we derive regret bounds
for the following new online settings: $(1)$ $\{f_t\}_{t=1}^T$ are monotone
strongly DR-submodular and chosen adversarially, $(2)$ $\{f_t\}_{t=1}^T$ are
monotone submodular (while the average $\frac{1}{T}\sum_{t=1}^T f_t$ is
strongly DR-submodular) and chosen by an adversary but they arrive in a
uniformly random order, $(3)$ $\{f_t\}_{t=1}^T$ are drawn i.i.d. from some
unknown distribution $f_t\sim \mathcal{D}$ where the expected function
$f(\cdot)=\mathbb{E}_{f_t\sim\mathcal{D}}[f_t(\cdot)]$ is monotone
DR-submodular. For $(1)$, we obtain the first logarithmic regret bounds. In
terms of the second framework, we show that it is possible to obtain similar
logarithmic bounds with high probability. Finally, for the i.i.d. model, we
provide algorithms with $\tilde{\mathcal{O}}(\sqrt{T})$ stochastic regret
bound, both in expectation and with high probability. Experimental results
demonstrate that our algorithms outperform the previous techniques in the
aforementioned three settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadeghi_O/0/1/0/all/0/1"&gt;Omid Sadeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raut_P/0/1/0/all/0/1"&gt;Prasanna Raut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazel_M/0/1/0/all/0/1"&gt;Maryam Fazel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsuitability of NOTEARS for Causal Graph Discovery. (arXiv:2104.05441v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05441</id>
        <link href="http://arxiv.org/abs/2104.05441"/>
        <updated>2021-06-16T01:21:11.489Z</updated>
        <summary type="html"><![CDATA[Causal Discovery methods aim to identify a DAG structure that represents
causal relationships from observational data. In this article, we stress that
it is important to test such methods for robustness in practical settings. As
our main example, we analyze the NOTEARS method, for which we demonstrate a
lack of scale-invariance. We show that NOTEARS is a method that aims to
identify a parsimonious DAG from the data that explains the residual variance.
We conclude that NOTEARS is not suitable for identifying truly causal
relationships from the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kaiser_M/0/1/0/all/0/1"&gt;Marcus Kaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sipos_M/0/1/0/all/0/1"&gt;Maksim Sipos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exponential Reduction in Sample Complexity with Learning of Ising Model Dynamics. (arXiv:2104.00995v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00995</id>
        <link href="http://arxiv.org/abs/2104.00995"/>
        <updated>2021-06-16T01:21:11.483Z</updated>
        <summary type="html"><![CDATA[The usual setting for learning the structure and parameters of a graphical
model assumes the availability of independent samples produced from the
corresponding multivariate probability distribution. However, for many models
the mixing time of the respective Markov chain can be very large and i.i.d.
samples may not be obtained. We study the problem of reconstructing binary
graphical models from correlated samples produced by a dynamical process, which
is natural in many applications. We analyze the sample complexity of two
estimators that are based on the interaction screening objective and the
conditional likelihood loss. We observe that for samples coming from a
dynamical process far from equilibrium, the sample complexity reduces
exponentially compared to a dynamical process that mixes quickly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutt_A/0/1/0/all/0/1"&gt;Arkopal Dutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lokhov_A/0/1/0/all/0/1"&gt;Andrey Y. Lokhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vuffray_M/0/1/0/all/0/1"&gt;Marc Vuffray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_S/0/1/0/all/0/1"&gt;Sidhant Misra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Program Synthesis for Images By Sampling Without Replacement. (arXiv:2001.10119v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10119</id>
        <link href="http://arxiv.org/abs/2001.10119"/>
        <updated>2021-06-16T01:21:11.476Z</updated>
        <summary type="html"><![CDATA[Program synthesis has emerged as a successful approach to the image parsing
task. Most prior works rely on a two-step scheme involving supervised
pretraining of a Seq2Seq model with synthetic programs followed by
reinforcement learning (RL) for fine-tuning with real reference images. Fully
unsupervised approaches promise to train the model directly on the target
images without requiring curated pretraining datasets. However, they struggle
with the inherent sparsity of meaningful programs in the search space. In this
paper, we present the first unsupervised algorithm capable of parsing
constructive solid geometry (CSG) images into context-free grammar (CFG)
without pretraining via non-differentiable renderer. To tackle the
\emph{non-Markovian} sparse reward problem, we combine three key ingredients --
(i) a grammar-encoded tree LSTM ensuring program validity (ii) entropy
regularization and (iii) sampling without replacement from the CFG syntax tree.
Empirically, our algorithm recovers meaningful programs in large search spaces
(up to $3.8 \times 10^{28}$). Further, even though our approach is fully
unsupervised, it generalizes better than supervised methods on the synthetic 2D
CSG dataset. On the 2D computer aided design (CAD) dataset, our approach
significantly outperforms the supervised pretrained model and is competitive to
the refined model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chenghui Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1"&gt;Barnabas Poczos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-efficient Hindsight Off-policy Option Learning. (arXiv:2007.15588v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15588</id>
        <link href="http://arxiv.org/abs/2007.15588"/>
        <updated>2021-06-16T01:21:11.448Z</updated>
        <summary type="html"><![CDATA[We introduce Hindsight Off-policy Options (HO2), a data-efficient option
learning algorithm. Given any trajectory, HO2 infers likely option choices and
backpropagates through the dynamic programming inference procedure to robustly
train all policy components off-policy and end-to-end. The approach outperforms
existing option learning methods on common benchmarks. To better understand the
option framework and disentangle benefits from both temporal and action
abstraction, we evaluate ablations with flat policies and mixture policies with
comparable optimization. The results highlight the importance of both types of
abstraction as well as off-policy training and trust-region constraints,
particularly in challenging, simulated 3D robot manipulation tasks from raw
pixel inputs. Finally, we intuitively adapt the inference step to investigate
the effect of increased temporal abstraction on training with pre-trained
options and from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wulfmeier_M/0/1/0/all/0/1"&gt;Markus Wulfmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1"&gt;Dushyant Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hafner_R/0/1/0/all/0/1"&gt;Roland Hafner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampe_T/0/1/0/all/0/1"&gt;Thomas Lampe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdolmaleki_A/0/1/0/all/0/1"&gt;Abbas Abdolmaleki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hertweck_T/0/1/0/all/0/1"&gt;Tim Hertweck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neunert_M/0/1/0/all/0/1"&gt;Michael Neunert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tirumala_D/0/1/0/all/0/1"&gt;Dhruva Tirumala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siegel_N/0/1/0/all/0/1"&gt;Noah Siegel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1"&gt;Nicolas Heess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1"&gt;Martin Riedmiller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcomplete Representations Against Adversarial Videos. (arXiv:2012.04262v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04262</id>
        <link href="http://arxiv.org/abs/2012.04262"/>
        <updated>2021-06-16T01:21:11.441Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness of deep neural networks is an extensively studied
problem in the literature and various methods have been proposed to defend
against adversarial images. However, only a handful of defense methods have
been developed for defending against attacked videos. In this paper, we propose
a novel Over-and-Under complete restoration network for Defending against
adversarial videos (OUDefend). Most restoration networks adopt an
encoder-decoder architecture that first shrinks spatial dimension then expands
it back. This approach learns undercomplete representations, which have large
receptive fields to collect global information but overlooks local details. On
the other hand, overcomplete representations have opposite properties. Hence,
OUDefend is designed to balance local and global features by learning those two
representations. We attach OUDefend to target video recognition models as a
feature restoration block and train the entire network end-to-end. Experimental
results show that the defenses focusing on images may be ineffective to videos,
while OUDefend enhances robustness against different types of adversarial
videos, ranging from additive attacks, multiplicative attacks to physically
realizable attacks. Code: https://github.com/shaoyuanlo/OUDefend]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shao-Yuan Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1"&gt;Jeya Maria Jose Valanarasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modern Hopfield Networks for Few- and Zero-Shot Reaction Template Prediction. (arXiv:2104.03279v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03279</id>
        <link href="http://arxiv.org/abs/2104.03279"/>
        <updated>2021-06-16T01:21:11.434Z</updated>
        <summary type="html"><![CDATA[Finding synthesis routes for molecules of interest is an essential step in
the discovery of new drugs and materials. To find such routes,
computer-assisted synthesis planning (CASP) methods are employed which rely on
a model of chemical reactivity. In this study, we model single-step
retrosynthesis in a template-based approach using modern Hopfield networks
(MHNs). We adapt MHNs to associate different modalities, reaction templates and
molecules, which allows the model to leverage structural information about
reaction templates. This approach significantly improves the performance of
template relevance prediction, especially for templates with few or zero
training examples. With inference speed several times faster than that of
baseline methods, we improve predictive performance for top-k exact match
accuracy for $\mathrm{k}\geq5$ in the retrosynthesis benchmark USPTO-50k.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seidl_P/0/1/0/all/0/1"&gt;Philipp Seidl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renz_P/0/1/0/all/0/1"&gt;Philipp Renz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyubankova_N/0/1/0/all/0/1"&gt;Natalia Dyubankova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neves_P/0/1/0/all/0/1"&gt;Paulo Neves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verhoeven_J/0/1/0/all/0/1"&gt;Jonas Verhoeven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segler_M/0/1/0/all/0/1"&gt;Marwin Segler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg K. Wegner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1"&gt;Sepp Hochreiter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klambauer_G/0/1/0/all/0/1"&gt;G&amp;#xfc;nter Klambauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothness Analysis of Adversarial Training. (arXiv:2103.01400v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01400</id>
        <link href="http://arxiv.org/abs/2103.01400"/>
        <updated>2021-06-16T01:21:11.427Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are vulnerable to adversarial attacks. Recent studies
about adversarial robustness focus on the loss landscape in the parameter space
since it is related to optimization and generalization performance. These
studies conclude that the difficulty of adversarial training is caused by the
non-smoothness of the loss function: i.e., its gradient is not Lipschitz
continuous. However, this analysis ignores the dependence of adversarial
attacks on model parameters. Since adversarial attacks are optimized for
models, they should depend on the parameters. Considering this dependence, we
analyze the smoothness of the loss function of adversarial training using the
optimal attacks for the model parameter in more detail. We reveal that the
constraint of adversarial attacks is one cause of the non-smoothness and that
the smoothness depends on the types of the constraints. Specifically, the
$L_\infty$ constraint can cause non-smoothness more than the $L_2$ constraint.
Moreover, our analysis implies that if we flatten the loss function with
respect to input data, the Lipschitz constant of the gradient of adversarial
loss tends to increase. To address the non-smoothness, we show that EntropySGD
smoothens the non-smooth loss and improves the performance of adversarial
training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1"&gt;Masanori Yamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_H/0/1/0/all/0/1"&gt;Hiroshi Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamanaka_Y/0/1/0/all/0/1"&gt;Yuki Yamanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ida_Y/0/1/0/all/0/1"&gt;Yasutoshi Ida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composing Normalizing Flows for Inverse Problems. (arXiv:2002.11743v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.11743</id>
        <link href="http://arxiv.org/abs/2002.11743"/>
        <updated>2021-06-16T01:21:11.421Z</updated>
        <summary type="html"><![CDATA[Given an inverse problem with a normalizing flow prior, we wish to estimate
the distribution of the underlying signal conditioned on the observations. We
approach this problem as a task of conditional inference on the pre-trained
unconditional flow model. We first establish that this is computationally hard
for a large class of flow models. Motivated by this, we propose a framework for
approximate inference that estimates the target conditional as a composition of
two flow models. This formulation leads to a stable variational inference
training procedure that avoids adversarial training. Our method is evaluated on
a variety of inverse problems and is shown to produce high-quality samples with
uncertainty quantification. We further demonstrate that our approach can be
amortized for zero-shot inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Whang_J/0/1/0/all/0/1"&gt;Jay Whang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lindgren_E/0/1/0/all/0/1"&gt;Erik M. Lindgren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amortized Probabilistic Detection of Communities in Graphs. (arXiv:2010.15727v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15727</id>
        <link href="http://arxiv.org/abs/2010.15727"/>
        <updated>2021-06-16T01:21:11.383Z</updated>
        <summary type="html"><![CDATA[Learning community structures in graphs has broad applications across
scientific domains. While graph neural networks (GNNs) have been successful in
encoding graph structures, existing GNN-based methods for community detection
are limited by requiring knowledge of the number of communities in advance, in
addition to lacking a proper probabilistic formulation to handle uncertainty.
We propose a simple framework for amortized community detection, which
addresses both of these issues by combining the expressive power of GNNs with
recent methods for amortized clustering. Our models consist of a graph
representation backbone that extracts structural information and an amortized
clustering network that naturally handles variable numbers of clusters. Both
components combine into well-defined models of the posterior distribution of
graph communities and are jointly optimized given labeled graphs. At inference
time, the models yield parallel samples from the posterior of community labels,
quantifying uncertainty in a principled way. We evaluate several models from
our framework on synthetic and real datasets and demonstrate superior
performance to previous methods. As a separate contribution, we extend recent
amortized probabilistic clustering architectures by adding attention modules,
which yield further improvements on community detection tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yueqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yoonho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Basu_P/0/1/0/all/0/1"&gt;Pallab Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1"&gt;Juho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Paninski_L/0/1/0/all/0/1"&gt;Liam Paninski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pakman_A/0/1/0/all/0/1"&gt;Ari Pakman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bringing Differential Private SGD to Practice: On the Independence of Gaussian Noise and the Number of Training Rounds. (arXiv:2102.09030v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09030</id>
        <link href="http://arxiv.org/abs/2102.09030"/>
        <updated>2021-06-16T01:21:11.364Z</updated>
        <summary type="html"><![CDATA[In the context of DP-SGD each round communicates a local SGD update which
leaks some new information about the underlying local data set to the outside
world. In order to provide privacy, Gaussian noise is added to local SGD
updates. However, privacy leakage still aggregates over multiple training
rounds. Therefore, in order to control privacy leakage over an increasing
number of training rounds, we need to increase the added Gaussian noise per
local SGD update. This dependence of the amount of Gaussian noise $\sigma$ on
the number of training rounds $T$ may impose an impractical upper bound on $T$
(because $\sigma$ cannot be too large) leading to a low accuracy global model
(because the global model receives too few local SGD updates). DP-SGD much less
competitive compared to other existing privacy techniques.

We show for the first time that for $(\epsilon,\delta)$-differential privacy
$\sigma$ can be chosen equal to $\sqrt{2(\epsilon +\ln(1/\delta))/\epsilon}$
regardless the total number of training rounds $T$. In other words, $\sigma$
does not depend on $T$ anymore (and aggregation of privacy leakage increases to
a limit). This important discovery brings DP-SGD to practice because $\sigma$
can remain small to make the trained model have high accuracy even for large
$T$ as usually happens in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1"&gt;Marten van Dijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Nhuong V. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Toan N. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Lam M. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong Ha Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Stochastic Gradient Langevin Dynamics. (arXiv:2004.11231v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.11231</id>
        <link href="http://arxiv.org/abs/2004.11231"/>
        <updated>2021-06-16T01:21:11.356Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient MCMC methods, such as stochastic gradient Langevin
dynamics (SGLD), employ fast but noisy gradient estimates to enable large-scale
posterior sampling. Although we can easily extend SGLD to distributed settings,
it suffers from two issues when applied to federated non-IID data. First, the
variance of these estimates increases significantly. Second, delaying
communication causes the Markov chains to diverge from the true posterior even
for very simple models. To alleviate both these problems, we propose conducive
gradients, a simple mechanism that combines local likelihood approximations to
correct gradient updates. Notably, conducive gradients are easy to compute, and
since we only calculate the approximations once, they incur negligible
overhead. We apply conducive gradients to distributed stochastic gradient
Langevin dynamics (DSGLD) and call the resulting method federated stochastic
gradient Langevin dynamics (FSGLD). We demonstrate that our approach can handle
delayed communication rounds, converging to the target posterior in cases where
DSGLD fails. We also show that FSGLD outperforms DSGLD for non-IID federated
data with experiments on metric learning and neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mekkaoui_K/0/1/0/all/0/1"&gt;Khaoula El Mekkaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mesquita_D/0/1/0/all/0/1"&gt;Diego Mesquita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Blomstedt_P/0/1/0/all/0/1"&gt;Paul Blomstedt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10005</id>
        <link href="http://arxiv.org/abs/2105.10005"/>
        <updated>2021-06-16T01:21:11.349Z</updated>
        <summary type="html"><![CDATA[Physical processes, camera movement, and unpredictable environmental
conditions like the presence of dust can induce noise and artifacts in video
feeds. We observe that popular unsupervised MOT methods are dependent on
noise-free inputs. We show that the addition of a small amount of artificial
random noise causes a sharp degradation in model performance on benchmark
metrics. We resolve this problem by introducing a robust unsupervised
multi-object tracking (MOT) model: AttU-Net. The proposed single-head attention
model helps limit the negative impact of noise by learning visual
representations at different segment scales. AttU-Net shows better unsupervised
MOT tracking performance over variational inference-based state-of-the-art
baselines. We evaluate our method in the MNIST-MOT and the Atari game video
benchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''
which consists of moving Japanese characters and ``Fashion-MNIST MOT'' to
validate the effectiveness of the MOT models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;C.-H. Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1"&gt;Mohit Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Y.-C. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quan Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1"&gt;Tomoaki Yoshinaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1"&gt;Tomokazu Murakami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Based Domain Generalization. (arXiv:2102.11436v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11436</id>
        <link href="http://arxiv.org/abs/2102.11436"/>
        <updated>2021-06-16T01:21:11.342Z</updated>
        <summary type="html"><![CDATA[Despite remarkable success in a variety of applications, it is well-known
that deep learning can fail catastrophically when presented with
out-of-distribution data. Toward addressing this challenge, we consider the
domain generalization problem, wherein predictors are trained using data drawn
from a family of related training domains and then evaluated on a distinct and
unseen test domain. We show that under a natural model of data generation and a
concomitant invariance condition, the domain generalization problem is
equivalent to an infinite-dimensional constrained statistical learning problem;
this problem forms the basis of our approach, which we call Model-Based Domain
Generalization. Due to the inherent challenges in solving constrained
optimization problems in deep learning, we exploit nonconvex duality theory to
develop unconstrained relaxations of this statistical problem with tight bounds
on the duality gap. Based on this theoretical motivation, we propose a novel
domain generalization algorithm with convergence guarantees. In our
experiments, we report improvements of up to 30 percentage points over
state-of-the-art domain generalization baselines on several benchmarks
including ColoredMNIST, Camelyon17-WILDS, FMoW-WILDS, and PACS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Robey_A/0/1/0/all/0/1"&gt;Alexander Robey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pappas_G/0/1/0/all/0/1"&gt;George J. Pappas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hassani_H/0/1/0/all/0/1"&gt;Hamed Hassani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantized Adam with Error Feedback. (arXiv:2004.14180v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.14180</id>
        <link href="http://arxiv.org/abs/2004.14180"/>
        <updated>2021-06-16T01:21:11.321Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a distributed variant of adaptive stochastic
gradient method for training deep neural networks in the parameter-server
model. To reduce the communication cost among the workers and server, we
incorporate two types of quantization schemes, i.e., gradient quantization and
weight quantization, into the proposed distributed Adam. Besides, to reduce the
bias introduced by quantization operations, we propose an error-feedback
technique to compensate for the quantized gradient. Theoretically, in the
stochastic nonconvex setting, we show that the distributed adaptive gradient
method with gradient quantization and error-feedback converges to the
first-order stationary point, and that the distributed adaptive gradient method
with weight quantization and error-feedback converges to the point related to
the quantized level under both the single-worker and multi-worker modes. At
last, we apply the proposed distributed adaptive gradient methods to train deep
neural networks. Experimental results demonstrate the efficacy of our methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Congliang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haozhi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Lossless Compression Rates via Monte Carlo Bits-Back Coding. (arXiv:2102.11086v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11086</id>
        <link href="http://arxiv.org/abs/2102.11086"/>
        <updated>2021-06-16T01:21:11.305Z</updated>
        <summary type="html"><![CDATA[Latent variable models have been successfully applied in lossless compression
with the bits-back coding algorithm. However, bits-back suffers from an
increase in the bitrate equal to the KL divergence between the approximate
posterior and the true posterior. In this paper, we show how to remove this gap
asymptotically by deriving bits-back coding algorithms from tighter variational
bounds. The key idea is to exploit extended space representations of Monte
Carlo estimators of the marginal likelihood. Naively applied, our schemes would
require more initial bits than the standard bits-back coder, but we show how to
drastically reduce this additional cost with couplings in the latent space.
When parallel architectures can be exploited, our coders can achieve better
rates than bits-back with little additional cost. We demonstrate improved
lossless compression rates in a variety of settings, especially in
out-of-distribution or sequential data compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1"&gt;Yangjun Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1"&gt;Karen Ullrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Severo_D/0/1/0/all/0/1"&gt;Daniel Severo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Townsend_J/0/1/0/all/0/1"&gt;James Townsend&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khisti_A/0/1/0/all/0/1"&gt;Ashish Khisti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makhzani_A/0/1/0/all/0/1"&gt;Alireza Makhzani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1"&gt;Chris J. Maddison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multivariate Uncertainty in Deep Learning. (arXiv:1910.14215v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.14215</id>
        <link href="http://arxiv.org/abs/1910.14215"/>
        <updated>2021-06-16T01:21:11.298Z</updated>
        <summary type="html"><![CDATA[Deep learning has the potential to dramatically impact navigation and
tracking state estimation problems critical to autonomous vehicles and
robotics. Measurement uncertainties in state estimation systems based on Kalman
and other Bayes filters are typically assumed to be a fixed covariance matrix.
This assumption is risky, particularly for "black box" deep learning models, in
which uncertainty can vary dramatically and unexpectedly. Accurate
quantification of multivariate uncertainty will allow for the full potential of
deep learning to be used more safely and reliably in these applications. We
show how to model multivariate uncertainty for regression problems with neural
networks, incorporating both aleatoric and epistemic sources of heteroscedastic
uncertainty. We train a deep uncertainty covariance matrix model in two ways:
directly using a multivariate Gaussian density loss function, and indirectly
using end-to-end training through a Kalman filter. We experimentally show in a
visual tracking problem the large impact that accurate multivariate uncertainty
quantification can have on Kalman filter performance for both in-domain and
out-of-domain evaluation data. We additionally show in a challenging visual
odometry problem how end-to-end filter training can allow uncertainty
predictions to compensate for filter weaknesses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Russell_R/0/1/0/all/0/1"&gt;Rebecca L. Russell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reale_C/0/1/0/all/0/1"&gt;Christopher Reale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Quantiles. (arXiv:2102.08244v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08244</id>
        <link href="http://arxiv.org/abs/2102.08244"/>
        <updated>2021-06-16T01:21:11.291Z</updated>
        <summary type="html"><![CDATA[Quantiles are often used for summarizing and understanding data. If that data
is sensitive, it may be necessary to compute quantiles in a way that is
differentially private, providing theoretical guarantees that the result does
not reveal private information. However, when multiple quantiles are needed,
existing differentially private algorithms fare poorly: they either compute
quantiles individually, splitting the privacy budget, or summarize the entire
distribution, wasting effort. In either case the result is reduced accuracy. In
this work we propose an instance of the exponential mechanism that
simultaneously estimates exactly $m$ quantiles from $n$ data points while
guaranteeing differential privacy. The utility function is carefully structured
to allow for an efficient implementation that returns estimates of all $m$
quantiles in time $O(mn\log(n) + m^2n)$. Experiments show that our method
significantly outperforms the current state of the art on both real and
synthetic data while remaining efficient enough to be practical.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gillenwater_J/0/1/0/all/0/1"&gt;Jennifer Gillenwater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_M/0/1/0/all/0/1"&gt;Matthew Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulesza_A/0/1/0/all/0/1"&gt;Alex Kulesza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning for Unknown Partially Observable MDPs. (arXiv:2102.12661v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12661</id>
        <link href="http://arxiv.org/abs/2102.12661"/>
        <updated>2021-06-16T01:21:11.275Z</updated>
        <summary type="html"><![CDATA[Solving Partially Observable Markov Decision Processes (POMDPs) is hard.
Learning optimal controllers for POMDPs when the model is unknown is harder.
Online learning of optimal controllers for unknown POMDPs, which requires
efficient learning using regret-minimizing algorithms that effectively tradeoff
exploration and exploitation, is even harder, and no solution exists currently.
In this paper, we consider infinite-horizon average-cost POMDPs with unknown
transition model, though a known observation model. We propose a natural
posterior sampling-based reinforcement learning algorithm (PSRL-POMDP) and show
that it achieves a regret bound of $O(\log T)$, where $T$ is the time horizon,
when the parameter set is finite. In the general case (continuous parameter
set), we show that the algorithm achieves $O (T^{2/3})$ regret under two
technical assumptions. To the best of our knowledge, this is the first online
RL algorithm for POMDPs and has sub-linear regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jafarnia_Jahromi_M/0/1/0/all/0/1"&gt;Mehdi Jafarnia-Jahromi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1"&gt;Rahul Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nayyar_A/0/1/0/all/0/1"&gt;Ashutosh Nayyar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Field-Embedded Factorization Machines for Click-through rate prediction. (arXiv:2009.09931v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09931</id>
        <link href="http://arxiv.org/abs/2009.09931"/>
        <updated>2021-06-16T01:21:11.264Z</updated>
        <summary type="html"><![CDATA[Click-through rate (CTR) prediction models are common in many online
applications such as digital advertising and recommender systems. Field-Aware
Factorization Machine (FFM) and Field-weighted Factorization Machine (FwFM) are
state-of-the-art among the shallow models for CTR prediction. Recently, many
deep learning-based models have also been proposed. Among deeper models,
DeepFM, xDeepFM, AutoInt+, and FiBiNet are state-of-the-art models. The deeper
models combine a core architectural component, which learns explicit feature
interactions, with a deep neural network (DNN) component. We propose a novel
shallow Field-Embedded Factorization Machine (FEFM) and its deep counterpart
Deep Field-Embedded Factorization Machine (DeepFEFM). FEFM learns symmetric
matrix embeddings for each field pair along with the usual single vector
embeddings for each feature. FEFM has significantly lower model complexity than
FFM and roughly the same complexity as FwFM. FEFM also has insightful
mathematical properties about important fields and field interactions. DeepFEFM
combines the FEFM interaction vectors learned by the FEFM component with a DNN
and is thus able to learn higher order interactions. We conducted comprehensive
experiments over a wide range of hyperparameters on two large publicly
available real-world datasets. When comparing test AUC and log loss, the
results show that FEFM and DeepFEFM outperform the existing state-of-the-art
shallow and deep models for CTR prediction tasks. We have made the code of FEFM
and DeepFEFM available in the DeepCTR library
(https://github.com/shenweichen/DeepCTR).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pande_H/0/1/0/all/0/1"&gt;Harshit Pande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled Gradient Descent. (arXiv:2005.08898v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.08898</id>
        <link href="http://arxiv.org/abs/2005.08898"/>
        <updated>2021-06-16T01:21:11.246Z</updated>
        <summary type="html"><![CDATA[Low-rank matrix estimation is a canonical problem that finds numerous
applications in signal processing, machine learning and imaging science. A
popular approach in practice is to factorize the matrix into two compact
low-rank factors, and then optimize these factors directly via simple iterative
methods such as gradient descent and alternating minimization. Despite
nonconvexity, recent literatures have shown that these simple heuristics in
fact achieve linear convergence when initialized properly for a growing number
of problems of interest. However, upon closer examination, existing approaches
can still be computationally expensive especially for ill-conditioned matrices:
the convergence rate of gradient descent depends linearly on the condition
number of the low-rank matrix, while the per-iteration cost of alternating
minimization is often prohibitive for large matrices. The goal of this paper is
to set forth a competitive algorithmic approach dubbed Scaled Gradient Descent
(ScaledGD) which can be viewed as pre-conditioned or diagonally-scaled gradient
descent, where the pre-conditioners are adaptive and iteration-varying with a
minimal computational overhead. With tailored variants for low-rank matrix
sensing, robust principal component analysis and matrix completion, we
theoretically show that ScaledGD achieves the best of both worlds: it converges
linearly at a rate independent of the condition number of the low-rank matrix
similar as alternating minimization, while maintaining the low per-iteration
cost of gradient descent. Our analysis is also applicable to general loss
functions that are restricted strongly convex and smooth over low-rank
matrices. To the best of our knowledge, ScaledGD is the first algorithm that
provably has such properties over a wide range of low-rank matrix estimation
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tong_T/0/1/0/all/0/1"&gt;Tian Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Cong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1"&gt;Yuejie Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top-Related Meta-Learning Method for Few-Shot Object Detection. (arXiv:2007.06837v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06837</id>
        <link href="http://arxiv.org/abs/2007.06837"/>
        <updated>2021-06-16T01:21:11.214Z</updated>
        <summary type="html"><![CDATA[Many meta-learning methods are proposed for few-shot detection. However,
previous most methods have two main problems, poor detection APs, and strong
bias because of imbalance and insufficient datasets. Previous works mainly
alleviate these issues by additional datasets, multi-relation attention
mechanisms and sub-modules. However, they require more cost. In this work, for
meta-learning, we find that the main challenges focus on related or irrelevant
semantic features between categories. Therefore, based on semantic features, we
propose a Top-C classification loss (i.e., TCL-C) for classification task and a
category-based grouping mechanism for category-based meta-features obtained by
the meta-model. The TCL-C exploits the true-label prediction and the most
likely C-1 false classification predictions to improve detection performance on
few-shot classes. According to similar appearance (i.e., visual appearance,
shape, and limbs etc.) and environment in which objects often appear, the
category-based grouping mechanism splits categories into disjoint groups to
make similar semantic features more compact between categories within a group
and obtain more significant difference between groups, alleviating the strong
bias problem and further improving detection APs. The whole training consists
of the base model and the fine-tuning phases. According to grouping mechanism,
we group the meta-features vectors obtained by meta-model, so that the
distribution difference between groups is obvious, and the one within each
group is less. Extensive experiments on Pascal VOC dataset demonstrate that
ours which combines the TCL-C with category-based grouping significantly
outperforms previous state-of-the-art methods for few-shot detection. Compared
with previous competitive baseline, ours improves detection APs by almost 4%
for few-shot detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1"&gt;Nan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaochun Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Duo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Dongrui Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhimin Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-Associated Differential Learning. (arXiv:2102.05246v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05246</id>
        <link href="http://arxiv.org/abs/2102.05246"/>
        <updated>2021-06-16T01:21:11.201Z</updated>
        <summary type="html"><![CDATA[Conventional Supervised Learning approaches focus on the mapping from input
features to output labels. After training, the learnt models alone are adapted
onto testing features to predict testing labels in isolation, with training
data wasted and their associations ignored. To take full advantage of the vast
number of training data and their associations, we propose a novel learning
paradigm called Memory-Associated Differential (MAD) Learning. We first
introduce an additional component called Memory to memorize all the training
data. Then we learn the differences of labels as well as the associations of
features in the combination of a differential equation and some sampling
methods. Finally, in the evaluating phase, we predict unknown labels by
inferencing from the memorized facts plus the learnt differences and
associations in a geometrically meaningful manner. We gently build this theory
in unary situations and apply it on Image Recognition, then extend it into Link
Prediction as a binary situation, in which our method outperforms strong
state-of-the-art baselines on ogbl-ddi dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Aiguo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1"&gt;Bei Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1"&gt;Ke Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Recurrent Neural Tangent Kernel. (arXiv:2006.10246v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10246</id>
        <link href="http://arxiv.org/abs/2006.10246"/>
        <updated>2021-06-16T01:21:11.176Z</updated>
        <summary type="html"><![CDATA[The study of deep neural networks (DNNs) in the infinite-width limit, via the
so-called neural tangent kernel (NTK) approach, has provided new insights into
the dynamics of learning, generalization, and the impact of initialization. One
key DNN architecture remains to be kernelized, namely, the recurrent neural
network (RNN). In this paper we introduce and study the Recurrent Neural
Tangent Kernel (RNTK), which provides new insights into the behavior of
overparametrized RNNs. A key property of the RNTK should greatly benefit
practitioners is its ability to compare inputs of different length. To this
end, we characterize how the RNTK weights different time steps to form its
output under different initialization parameters and nonlinearity choices. A
synthetic and 56 real-world data experiments demonstrate that the RNTK offers
significant performance gains over other kernels, including standard NTKs,
across a wide array of data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alemohammad_S/0/1/0/all/0/1"&gt;Sina Alemohammad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zichao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1"&gt;Randall Balestriero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1"&gt;Richard Baraniuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Sparse Regression with Clustering: An Invex Relaxation for a Combinatorial Problem. (arXiv:2102.09704v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09704</id>
        <link href="http://arxiv.org/abs/2102.09704"/>
        <updated>2021-06-16T01:21:11.157Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of fair sparse regression on a biased
dataset where bias depends upon a hidden binary attribute. The presence of a
hidden attribute adds an extra layer of complexity to the problem by combining
sparse regression and clustering with unknown binary labels. The corresponding
optimization problem is combinatorial, but we propose a novel relaxation of it
as an \emph{invex} optimization problem. To the best of our knowledge, this is
the first invex relaxation for a combinatorial problem. We show that the
inclusion of the debiasing/fairness constraint in our model has no adverse
effect on the performance. Rather, it enables the recovery of the hidden
attribute. The support of our recovered regression parameter vector matches
exactly with the true parameter vector. Moreover, we simultaneously solve the
clustering problem by recovering the exact value of the hidden attribute for
each sample. Our method uses carefully constructed primal dual witnesses to
provide theoretical guarantees for the combinatorial problem. To that end, we
show that the sample complexity of our method is logarithmic in terms of the
dimension of the regression parameter vector.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barik_A/0/1/0/all/0/1"&gt;Adarsh Barik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honorio_J/0/1/0/all/0/1"&gt;Jean Honorio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reasoning Over Virtual Knowledge Bases With Open Predicate Relations. (arXiv:2102.07043v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07043</id>
        <link href="http://arxiv.org/abs/2102.07043"/>
        <updated>2021-06-16T01:21:11.151Z</updated>
        <summary type="html"><![CDATA[We present the Open Predicate Query Language (OPQL); a method for
constructing a virtual KB (VKB) trained entirely from text. Large Knowledge
Bases (KBs) are indispensable for a wide-range of industry applications such as
question answering and recommendation. Typically, KBs encode world knowledge in
a structured, readily accessible form derived from laborious human annotation
efforts. Unfortunately, while they are extremely high precision, KBs are
inevitably highly incomplete and automated methods for enriching them are far
too inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set
of relation mentions in a way that naturally enables reasoning and can be
trained without any structured supervision. We demonstrate that OPQL
outperforms prior VKB methods on two different KB reasoning tasks and,
additionally, can be used as an external memory integrated into a language
model (OPQL-LM) leading to improvements on two open-domain question answering
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haitian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1"&gt;Pat Verga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1"&gt;Bhuwan Dhingra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"&gt;William W. Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Semantic Guidance and Deep Reinforcement Learning For Generating Human Level Paintings. (arXiv:2011.12589v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12589</id>
        <link href="http://arxiv.org/abs/2011.12589"/>
        <updated>2021-06-16T01:21:11.144Z</updated>
        <summary type="html"><![CDATA[Generation of stroke-based non-photorealistic imagery, is an important
problem in the computer vision community. As an endeavor in this direction,
substantial recent research efforts have been focused on teaching machines "how
to paint", in a manner similar to a human painter. However, the applicability
of previous methods has been limited to datasets with little variation in
position, scale and saliency of the foreground object. As a consequence, we
find that these methods struggle to cover the granularity and diversity
possessed by real world images. To this end, we propose a Semantic Guidance
pipeline with 1) a bi-level painting procedure for learning the distinction
between foreground and background brush strokes at training time. 2) We also
introduce invariance to the position and scale of the foreground object through
a neural alignment model, which combines object localization and spatial
transformer networks in an end to end manner, to zoom into a particular
semantic instance. 3) The distinguishing features of the in-focus object are
then amplified by maximizing a novel guided backpropagation based focus reward.
The proposed agent does not require any supervision on human stroke-data and
successfully handles variations in foreground object attributes, thus,
producing much higher quality canvases for the CUB-200 Birds and Stanford
Cars-196 datasets. Finally, we demonstrate the further efficacy of our method
on complex datasets with multiple foreground object instances by evaluating an
extension of our method on the challenging Virtual-KITTI dataset. Source code
and models are available at https://github.com/1jsingh/semantic-guidance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Jaskirat Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Navigation by Continuous-time Neural Networks. (arXiv:2106.08314v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08314</id>
        <link href="http://arxiv.org/abs/2106.08314"/>
        <updated>2021-06-16T01:21:11.123Z</updated>
        <summary type="html"><![CDATA[Imitation learning enables high-fidelity, vision-based learning of policies
within rich, photorealistic environments. However, such techniques often rely
on traditional discrete-time neural models and face difficulties in
generalizing to domain shifts by failing to account for the causal
relationships between the agent and the environment. In this paper, we propose
a theoretical and experimental framework for learning causal representations
using continuous-time neural networks, specifically over their discrete-time
counterparts. We evaluate our method in the context of visual-control learning
of drones over a series of complex tasks, ranging from short- and long-term
navigation, to chasing static and dynamic objects through photorealistic
environments. Our results demonstrate that causal continuous-time deep models
can perform robust navigation tasks, where advanced recurrent models fail.
These models learn complex causal control representations directly from raw
visual inputs and scale to solve a variety of tasks using imitation learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vorbach_C/0/1/0/all/0/1"&gt;Charles Vorbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1"&gt;Ramin Hasani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1"&gt;Alexander Amini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1"&gt;Mathias Lechner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Keyphrase Detection using Speaker and Environment Information. (arXiv:2104.13970v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13970</id>
        <link href="http://arxiv.org/abs/2104.13970"/>
        <updated>2021-06-16T01:21:11.104Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a streaming keyphrase detection system that can
be easily customized to accurately detect any phrase composed of words from a
large vocabulary. The system is implemented with an end-to-end trained
automatic speech recognition (ASR) model and a text-independent speaker
verification model. To address the challenge of detecting these keyphrases
under various noisy conditions, a speaker separation model is added to the
feature frontend of the speaker verification model, and an adaptive noise
cancellation (ANC) algorithm is included to exploit cross-microphone noise
coherence. Our experiments show that the text-independent speaker verification
model largely reduces the false triggering rate of the keyphrase detection,
while the speaker separation model and adaptive noise cancellation largely
reduce false rejections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rikhye_R/0/1/0/all/0/1"&gt;Rajeev Rikhye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Quan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_Q/0/1/0/all/0/1"&gt;Qiao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1"&gt;Yanzhang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yiteng/0/1/0/all/0/1"&gt;Yiteng&lt;/a&gt; (Arden) &lt;a href="http://arxiv.org/find/eess/1/au:+Huang/0/1/0/all/0/1"&gt;Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Narayanan_A/0/1/0/all/0/1"&gt;Arun Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+McGraw_I/0/1/0/all/0/1"&gt;Ian McGraw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analytic function approximation by path norm regularized deep networks. (arXiv:2104.02095v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02095</id>
        <link href="http://arxiv.org/abs/2104.02095"/>
        <updated>2021-06-16T01:21:11.045Z</updated>
        <summary type="html"><![CDATA[We provide an entropy bound for the spaces of path norm regularized neural
networks with piecewise linear activation functions, such as the ReLU and the
absolute value functions. This bound generalizes the known entropy bound for
the spaces of linear functions on $\mathbb{R}^d$. Keeping the path norm
together with the depth, width and the weights of networks to have logarithmic
dependence on $1/\varepsilon$, we $\varepsilon$-approximate functions that are
analytic on certain regions of $\mathbb{C}^d$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1"&gt;Aleksandr Beknazaryan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust and Sample Optimal Algorithms for PSD Low-Rank Approximation. (arXiv:1912.04177v5 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.04177</id>
        <link href="http://arxiv.org/abs/1912.04177"/>
        <updated>2021-06-16T01:21:11.035Z</updated>
        <summary type="html"><![CDATA[Recently, Musco and Woodruff (FOCS, 2017) showed that given an $n \times n$
positive semidefinite (PSD) matrix $A$, it is possible to compute a
$(1+\epsilon)$-approximate relative-error low-rank approximation to $A$ by
querying $O(nk/\epsilon^{2.5})$ entries of $A$ in time $O(nk/\epsilon^{2.5} +n
k^{\omega-1}/\epsilon^{2(\omega-1)})$. They also showed that any relative-error
low-rank approximation algorithm must query $\Omega(nk/\epsilon)$ entries of
$A$, this gap has since remained open. Our main result is to resolve this
question by obtaining an optimal algorithm that queries $O(nk/\epsilon)$
entries of $A$ and outputs a relative-error low-rank approximation in
$O(n(k/\epsilon)^{\omega-1})$ time. Note, our running time improves that of
Musco and Woodruff, and matches the information-theoretic lower bound if the
matrix-multiplication exponent $\omega$ is $2$.

We then extend our techniques to negative-type distance matrices. Bakshi and
Woodruff (NeurIPS, 2018) showed a bi-criteria, relative-error low-rank
approximation which queries $O(nk/\epsilon^{2.5})$ entries and outputs a
rank-$(k+4)$ matrix. We show that the bi-criteria guarantee is not necessary
and obtain an $O(nk/\epsilon)$ query algorithm, which is optimal. Our algorithm
applies to all distance matrices that arise from metrics satisfying
negative-type inequalities, including $\ell_1, \ell_2,$ spherical metrics and
hypermetrics.

Next, we introduce a new robust low-rank approximation model which captures
PSD matrices that have been corrupted with noise. While a sample complexity
lower bound precludes sublinear algorithms for arbitrary PSD matrices, we
provide the first sublinear time and query algorithms when the corruption on
the diagonal entries is bounded. As a special case, we show sample-optimal
sublinear time algorithms for low-rank approximation of correlation matrices
corrupted by noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bakshi_A/0/1/0/all/0/1"&gt;Ainesh Bakshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chepurko_N/0/1/0/all/0/1"&gt;Nadiia Chepurko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mind Your Weight(s): A Large-scale Study on Insufficient Machine Learning Model Protection in Mobile Apps. (arXiv:2002.07687v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.07687</id>
        <link href="http://arxiv.org/abs/2002.07687"/>
        <updated>2021-06-16T01:21:11.018Z</updated>
        <summary type="html"><![CDATA[On-device machine learning (ML) is quickly gaining popularity among mobile
apps. It allows offline model inference while preserving user privacy. However,
ML models, considered as core intellectual properties of model owners, are now
stored on billions of untrusted devices and subject to potential thefts. Leaked
models can cause both severe financial loss and security consequences. This
paper presents the first empirical study of ML model protection on mobile
devices. Our study aims to answer three open questions with quantitative
evidence: How widely is model protection used in apps? How robust are existing
model protection techniques? What impacts can (stolen) models incur? To that
end, we built a simple app analysis pipeline and analyzed 46,753 popular apps
collected from the US and Chinese app markets. We identified 1,468 ML apps
spanning all popular app categories. We found that, alarmingly, 41% of ML apps
do not protect their models at all, which can be trivially stolen from app
packages. Even for those apps that use model protection or encryption, we were
able to extract the models from 66% of them via unsophisticated dynamic
analysis techniques. The extracted models are mostly commercial products and
used for face recognition, liveness detection, ID/bank card recognition, and
malware detection. We quantitatively estimated the potential financial and
security impact of a leaked model, which can amount to millions of dollars for
different stakeholders. Our study reveals that on-device models are currently
at high risk of being leaked; attackers are highly motivated to steal such
models. Drawn from our large-scale study, we report our insights into this
emerging security problem and discuss the technical challenges, hoping to
inspire future research on robust and practical model protection for mobile
devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhichuang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1"&gt;Ruimin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Long Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mislove_A/0/1/0/all/0/1"&gt;Alan Mislove&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11066</id>
        <link href="http://arxiv.org/abs/2010.11066"/>
        <updated>2021-06-16T01:21:11.011Z</updated>
        <summary type="html"><![CDATA[Spoken conversational question answering (SCQA) requires machines to model
complex dialogue flow given the speech utterances and text corpora. Different
from traditional text question answering (QA) tasks, SCQA involves audio signal
processing, passage comprehension, and contextual understanding. However, ASR
systems introduce unexpected noisy signals to the transcriptions, which result
in performance degradation on SCQA. To overcome the problem, we propose CADNet,
a novel contextualized attention-based distillation approach, which applies
both cross-attention and self-attention to obtain ASR-robust contextualized
embedding representations of the passage and dialogue history for performance
improvements. We also introduce the spoken conventional knowledge distillation
framework to distill the ASR-robust knowledge from the estimated probabilities
of the teacher model to the student. We conduct extensive experiments on the
Spoken-CoQA dataset and demonstrate that our approach achieves remarkable
performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ST-UNet: A Spatio-Temporal U-Network for Graph-structured Time Series Modeling. (arXiv:1903.05631v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.05631</id>
        <link href="http://arxiv.org/abs/1903.05631"/>
        <updated>2021-06-16T01:21:11.004Z</updated>
        <summary type="html"><![CDATA[The spatio-temporal graph learning is becoming an increasingly important
object of graph study. Many application domains involve highly dynamic graphs
where temporal information is crucial, e.g. traffic networks and financial
transaction graphs. Despite the constant progress made on learning structured
data, there is still a lack of effective means to extract dynamic complex
features from spatio-temporal structures. Particularly, conventional models
such as convolutional networks or recurrent neural networks are incapable of
revealing the temporal patterns in short or long terms and exploring the
spatial properties in local or global scope from spatio-temporal graphs
simultaneously. To tackle this problem, we design a novel multi-scale
architecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series
modeling. In this U-shaped network, a paired sampling operation is proposed in
spacetime domain accordingly: the pooling (ST-Pool) coarsens the input graph in
spatial from its deterministic partition while abstracts multi-resolution
temporal dependencies through dilated recurrent skip connections; based on
previous settings in the downsampling, the unpooling (ST-Unpool) restores the
original structure of spatio-temporal graphs and resumes regular intervals
within graph sequences. Experiments on spatio-temporal prediction tasks
demonstrate that our model effectively captures comprehensive features in
multiple scales and achieves substantial improvements over mainstream methods
on several real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Haoteng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhanxing Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation of Teacher Ensembles for Spoken Command Classification. (arXiv:2104.01271v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01271</id>
        <link href="http://arxiv.org/abs/2104.01271"/>
        <updated>2021-06-16T01:21:10.997Z</updated>
        <summary type="html"><![CDATA[We propose using an adversarial autoencoder (AAE) to replace generative
adversarial network (GAN) in the private aggregation of teacher ensembles
(PATE), a solution for ensuring differential privacy in speech applications.
The AAE architecture allows us to obtain good synthetic speech leveraging upon
a discriminative training of latent vectors. Such synthetic speech is used to
build a privacy-preserving classifier when non-sensitive data is not
sufficiently available in the public domain. This classifier follows the PATE
scheme that uses an ensemble of noisy outputs to label the synthetic samples
and guarantee $\varepsilon$-differential privacy (DP) on its derived
classifiers. Our proposed framework thus consists of an AAE-based generator and
a PATE-based classifier (PATE-AAE). Evaluated on the Google Speech Commands
Dataset Version II, the proposed PATE-AAE improves the average classification
accuracy by +$2.11\%$ and +$6.60\%$, respectively, when compared with
alternative privacy-preserving solutions, namely PATE-GAN and DP-GAN, while
maintaining a strong level of privacy target at $\varepsilon$=0.01 with a fixed
$\delta$=10$^{-5}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao-Han Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1"&gt;Sabato Marco Siniscalchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chin-Hui Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double Double Descent: On Generalization Errors in Transfer Learning between Linear Regression Tasks. (arXiv:2006.07002v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07002</id>
        <link href="http://arxiv.org/abs/2006.07002"/>
        <updated>2021-06-16T01:21:10.971Z</updated>
        <summary type="html"><![CDATA[We study the transfer learning process between two linear regression
problems. An important and timely special case is when the regressors are
overparameterized and perfectly interpolate their training data. We examine a
parameter transfer mechanism whereby a subset of the parameters of the target
task solution are constrained to the values learned for a related source task.
We analytically characterize the generalization error of the target task in
terms of the salient factors in the transfer learning architecture, i.e., the
number of examples available, the number of (free) parameters in each of the
tasks, the number of parameters transferred from the source to target task, and
the correlation between the two tasks. Our non-asymptotic analysis shows that
the generalization error of the target task follows a two-dimensional double
descent trend (with respect to the number of free parameters in each of the
tasks) that is controlled by the transfer learning factors. Our analysis points
to specific cases where the transfer of parameters is beneficial. Specifically,
we show that transferring a specific set of parameters that generalizes well on
the respective part of the source task can soften the demand on the task
correlation level that is required for successful transfer learning. Moreover,
we show that the usefulness of a transfer learning setting is fragile and
depends on a delicate interplay among the set of transferred parameters, the
relation between the tasks, and the true solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dar_Y/0/1/0/all/0/1"&gt;Yehuda Dar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1"&gt;Richard G. Baraniuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning. (arXiv:2104.04975v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04975</id>
        <link href="http://arxiv.org/abs/2104.04975"/>
        <updated>2021-06-16T01:21:10.956Z</updated>
        <summary type="html"><![CDATA[Marginal-likelihood based model-selection, even though promising, is rarely
used in deep learning due to estimation difficulties. Instead, most approaches
rely on validation data, which may not be readily available. In this work, we
present a scalable marginal-likelihood estimation method to select both
hyperparameters and network architectures, based on the training data alone.
Some hyperparameters can be estimated online during training, simplifying the
procedure. Our marginal-likelihood estimate is based on Laplace's method and
Gauss-Newton approximations to the Hessian, and it outperforms cross-validation
and manual-tuning on standard regression and image classification datasets,
especially in terms of calibration and out-of-distribution detection. Our work
shows that marginal likelihoods can improve generalization and be useful when
validation data is unavailable (e.g., in nonstationary settings).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Immer_A/0/1/0/all/0/1"&gt;Alexander Immer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bauer_M/0/1/0/all/0/1"&gt;Matthias Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1"&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGENT: A Benchmark for Core Psychological Reasoning. (arXiv:2102.12321v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12321</id>
        <link href="http://arxiv.org/abs/2102.12321"/>
        <updated>2021-06-16T01:21:10.940Z</updated>
        <summary type="html"><![CDATA[For machine agents to successfully interact with humans in real-world
settings, they will need to develop an understanding of human mental life.
Intuitive psychology, the ability to reason about hidden mental variables that
drive observable actions, comes naturally to people: even pre-verbal infants
can tell agents from objects, expecting agents to act efficiently to achieve
goals given constraints. Despite recent interest in machine agents that reason
about other agents, it is not clear if such agents learn or hold the core
psychology principles that drive human reasoning. Inspired by cognitive
development studies on intuitive psychology, we present a benchmark consisting
of a large dataset of procedurally generated 3D animations, AGENT (Action,
Goal, Efficiency, coNstraint, uTility), structured around four scenarios (goal
preferences, action efficiency, unobserved constraints, and cost-reward
trade-offs) that probe key concepts of core intuitive psychology. We validate
AGENT with human-ratings, propose an evaluation protocol emphasizing
generalization, and compare two strong baselines built on Bayesian inverse
planning and a Theory of Mind neural network. Our results suggest that to pass
the designed tests of core intuitive psychology at human levels, a model must
acquire or have built-in representations of how agents plan, combining utility
computations and core knowledge of objects and physics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1"&gt;Tianmin Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1"&gt;Abhishek Bhandwaldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kevin A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shari Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1"&gt;Dan Gutfreund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spelke_E/0/1/0/all/0/1"&gt;Elizabeth Spelke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1"&gt;Tomer D. Ullman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Contextual Bandit Learning for Adaptive Radar Waveform Selection. (arXiv:2103.05541v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05541</id>
        <link href="http://arxiv.org/abs/2103.05541"/>
        <updated>2021-06-16T01:21:10.922Z</updated>
        <summary type="html"><![CDATA[A sequential decision process in which an adaptive radar system repeatedly
interacts with a finite-state target channel is studied. The radar is capable
of passively sensing the spectrum at regular intervals, which provides side
information for the waveform selection process. The radar transmitter uses the
sequence of spectrum observations as well as feedback from a collocated
receiver to select waveforms which accurately estimate target parameters. It is
shown that the waveform selection problem can be effectively addressed using a
linear contextual bandit formulation in a manner that is both computationally
feasible and sample efficient. Stochastic and adversarial linear contextual
bandit models are introduced, allowing the radar to achieve effective
performance in broad classes of physical environments. Simulations in a
radar-communication coexistence scenario, as well as in an adversarial
radar-jammer scenario, demonstrate that the proposed formulation provides a
substantial improvement in target detection performance when Thompson Sampling
and EXP3 algorithms are used to drive the waveform selection process. Further,
it is shown that the harmful impacts of pulse-agile behavior on coherently
processed radar data can be mitigated by adopting a time-varying constraint on
the radar's waveform catalog.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thornton_C/0/1/0/all/0/1"&gt;Charles E. Thornton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buehrer_R/0/1/0/all/0/1"&gt;R. Michael Buehrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martone_A/0/1/0/all/0/1"&gt;Anthony F. Martone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Functionals on the Space of Probabilities with Input Convex Neural Networks. (arXiv:2106.00774v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00774</id>
        <link href="http://arxiv.org/abs/2106.00774"/>
        <updated>2021-06-16T01:21:10.876Z</updated>
        <summary type="html"><![CDATA[Gradient flows are a powerful tool for optimizing functionals in general
metric spaces, including the space of probabilities endowed with the
Wasserstein metric. A typical approach to solving this optimization problem
relies on its connection to the dynamic formulation of optimal transport and
the celebrated Jordan-Kinderlehrer-Otto (JKO) scheme. However, this formulation
involves optimization over convex functions, which is challenging, especially
in high dimensions. In this work, we propose an approach that relies on the
recently introduced input-convex neural networks (ICNN) to parameterize the
space of convex functions in order to approximate the JKO scheme, as well as in
designing functionals over measures that enjoy convergence guarantees. We
derive a computationally efficient implementation of this JKO-ICNN framework
and use various experiments to demonstrate its feasibility and validity in
approximating solutions of low-dimensional partial differential equations with
known solutions. We also explore the use of our JKO-ICNN approach in high
dimensions with an experiment in controlled generation for molecular discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Alvarez_Melis_D/0/1/0/all/0/1"&gt;David Alvarez-Melis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schiff_Y/0/1/0/all/0/1"&gt;Yair Schiff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mroueh_Y/0/1/0/all/0/1"&gt;Youssef Mroueh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks Inspired by Classical Iterative Algorithms. (arXiv:2103.06064v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06064</id>
        <link href="http://arxiv.org/abs/2103.06064"/>
        <updated>2021-06-16T01:21:10.870Z</updated>
        <summary type="html"><![CDATA[Despite the recent success of graph neural networks (GNN), common
architectures often exhibit significant limitations, including sensitivity to
oversmoothing, long-range dependencies, and spurious edges, e.g., as can occur
as a result of graph heterophily or adversarial attacks. To at least partially
address these issues within a simple transparent framework, we consider a new
family of GNN layers designed to mimic and integrate the update rules of two
classical iterative algorithms, namely, proximal gradient descent and iterative
reweighted least squares (IRLS). The former defines an extensible base GNN
architecture that is immune to oversmoothing while nonetheless capturing
long-range dependencies by allowing arbitrary propagation steps. In contrast,
the latter produces a novel attention mechanism that is explicitly anchored to
an underlying end-to-end energy function, contributing stability with respect
to edge uncertainty. When combined we obtain an extremely simple yet robust
model that we evaluate across disparate scenarios including standardized
benchmarks, adversarially-perturbated graphs, graphs with heterophily, and
graphs involving long-range dependencies. In doing so, we compare against SOTA
GNN approaches that have been explicitly designed for the respective task,
achieving competitive or superior node classification accuracy. Our code is
available at https://github.com/FFTYYY/TWIRLS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yangkun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinjing Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1"&gt;Quan Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhewei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zengfeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1"&gt;David Wipf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Language Adversarial Defense through Synonym Encoding. (arXiv:1909.06723v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.06723</id>
        <link href="http://arxiv.org/abs/1909.06723"/>
        <updated>2021-06-16T01:21:10.863Z</updated>
        <summary type="html"><![CDATA[In the area of natural language processing, deep learning models are recently
known to be vulnerable to various types of adversarial perturbations, but
relatively few works are done on the defense side. Especially, there exists few
effective defense method against the successful synonym substitution based
attacks that preserve the syntactic structure and semantic information of the
original text while fooling the deep learning models. We contribute in this
direction and propose a novel adversarial defense method called Synonym
Encoding Method (SEM). Specifically, SEM inserts an encoder before the input
layer of the target model to map each cluster of synonyms to a unique encoding
and trains the model to eliminate possible adversarial perturbations without
modifying the network architecture or adding extra data. Extensive experiments
demonstrate that SEM can effectively defend the current synonym substitution
based attacks and block the transferability of adversarial examples. SEM is
also easy and efficient to scale to large models and big datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaosen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yichen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1"&gt;Kun He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum. (arXiv:2102.07367v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07367</id>
        <link href="http://arxiv.org/abs/2102.07367"/>
        <updated>2021-06-16T01:21:10.857Z</updated>
        <summary type="html"><![CDATA[This paper proposes a new algorithm -- the \underline{S}ingle-timescale
Do\underline{u}ble-momentum \underline{St}ochastic
\underline{A}pprox\underline{i}matio\underline{n} (SUSTAIN) -- for tackling
stochastic unconstrained bilevel optimization problems. We focus on bilevel
problems where the lower level subproblem is strongly-convex and the upper
level objective function is smooth. Unlike prior works which rely on
\emph{two-timescale} or \emph{double loop} techniques, we design a stochastic
momentum-assisted gradient estimator for both the upper and lower level
updates. The latter allows us to control the error in the stochastic gradient
updates due to inaccurate solution to both subproblems. If the upper objective
function is smooth but possibly non-convex, we show that {\aname}~requires
$\mathcal{O}(\epsilon^{-3/2})$ iterations (each using ${\cal O}(1)$ samples) to
find an $\epsilon$-stationary solution. The $\epsilon$-stationary solution is
defined as the point whose squared norm of the gradient of the outer function
is less than or equal to $\epsilon$. The total number of stochastic gradient
samples required for the upper and lower level objective functions matches the
best-known complexity for single-level stochastic gradient algorithms. We also
analyze the case when the upper level objective function is strongly-convex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Khanduri_P/0/1/0/all/0/1"&gt;Prashant Khanduri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Siliang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hong_M/0/1/0/all/0/1"&gt;Mingyi Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wai_H/0/1/0/all/0/1"&gt;Hoi-To Wai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuoran Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-16T01:21:10.832Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08318</id>
        <link href="http://arxiv.org/abs/2106.08318"/>
        <updated>2021-06-16T01:21:10.797Z</updated>
        <summary type="html"><![CDATA[How can neural networks be trained on large-volume temporal data efficiently?
To compute the gradients required to update parameters, backpropagation blocks
computations until the forward and backward passes are completed. For temporal
signals, this introduces high latency and hinders real-time learning. It also
creates a coupling between consecutive layers, which limits model parallelism
and increases memory consumption. In this paper, we build upon Sideways, which
avoids blocking by propagating approximate gradients forward in time, and we
propose mechanisms for temporal integration of information based on different
variants of skip connections. We also show how to decouple computation and
delegate individual neural modules to different devices, allowing distributed
and parallel training. The proposed Skip-Sideways achieves low latency
training, model parallelism, and, importantly, is capable of extracting
temporal features, leading to more stable training and improved performance on
real-world action recognition video datasets such as HMDB51, UCF101, and the
large-scale Kinetics-600. Finally, we also show that models trained with
Skip-Sideways generate better future frames than Sideways models, and hence
they can better utilize motion cues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1"&gt;Dimitrios Vytiniotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1"&gt;Grzegorz Swirszcz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1"&gt;Viorica Patraucean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Joao Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encouraging Intra-Class Diversity Through a Reverse Contrastive Loss for Better Single-Source Domain Generalization. (arXiv:2106.07916v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07916</id>
        <link href="http://arxiv.org/abs/2106.07916"/>
        <updated>2021-06-16T01:21:10.753Z</updated>
        <summary type="html"><![CDATA[Traditional deep learning algorithms often fail to generalize when they are
tested outside of the domain of training data. Because data distributions can
change dynamically in real-life applications once a learned model is deployed,
in this paper we are interested in single-source domain generalization (SDG)
which aims to develop deep learning algorithms able to generalize from a single
training domain where no information about the test domain is available at
training time. Firstly, we design two simple MNISTbased SDG benchmarks, namely
MNIST Color SDG-MP and MNIST Color SDG-UP, which highlight the two different
fundamental SDG issues of increasing difficulties: 1) a class-correlated
pattern in the training domain is missing (SDG-MP), or 2) uncorrelated with the
class (SDG-UP), in the testing data domain. This is in sharp contrast with the
current domain generalization (DG) benchmarks which mix up different
correlation and variation factors and thereby make hard to disentangle success
or failure factors when benchmarking DG algorithms. We further evaluate several
state-of-the-art SDG algorithms through our simple benchmark, namely MNIST
Color SDG-MP, and show that the issue SDG-MP is largely unsolved despite of a
decade of efforts in developing DG algorithms. Finally, we also propose a
partially reversed contrastive loss to encourage intra-class diversity and find
less strongly correlated patterns, to deal with SDG-MP and show that the
proposed approach is very effective on our MNIST Color SDG-MP benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duboudin_T/0/1/0/all/0/1"&gt;Thomas Duboudin&lt;/a&gt; (imagine), &lt;a href="http://arxiv.org/find/cs/1/au:+Dellandrea_E/0/1/0/all/0/1"&gt;Emmanuel Dellandr&amp;#xe9;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abgrall_C/0/1/0/all/0/1"&gt;Corentin Abgrall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henaff_G/0/1/0/all/0/1"&gt;Gilles H&amp;#xe9;naff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProsoBeast Prosody Annotation Tool. (arXiv:2104.02397v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02397</id>
        <link href="http://arxiv.org/abs/2104.02397"/>
        <updated>2021-06-16T01:21:10.731Z</updated>
        <summary type="html"><![CDATA[The labelling of speech corpora is a laborious and time-consuming process.
The ProsoBeast Annotation Tool seeks to ease and accelerate this process by
providing an interactive 2D representation of the prosodic landscape of the
data, in which contours are distributed based on their similarity. This
interactive map allows the user to inspect and label the utterances. The tool
integrates several state-of-the-art methods for dimensionality reduction and
feature embedding, including variational autoencoders. The user can use these
to find a good representation for their data. In addition, as most of these
methods are stochastic, each can be used to generate an unlimited number of
different prosodic maps. The web app then allows the user to seamlessly switch
between these alternative representations in the annotation process.
Experiments with a sample prosodically rich dataset have shown that the tool
manages to find good representations of varied data and is helpful both for
annotation and label correction. The tool is released as free software for use
by the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gerazov_B/0/1/0/all/0/1"&gt;Branislav Gerazov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wagner_M/0/1/0/all/0/1"&gt;Michael Wagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speaker Diarization using Two-pass Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings. (arXiv:2104.02469v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02469</id>
        <link href="http://arxiv.org/abs/2104.02469"/>
        <updated>2021-06-16T01:21:10.709Z</updated>
        <summary type="html"><![CDATA[Many modern systems for speaker diarization, such as the recently-developed
VBx approach, rely on clustering of DNN speaker embeddings followed by
resegmentation. Two problems with this approach are that the DNN is not
directly optimized for this task, and the parameters need significant retuning
for different applications. We have recently presented progress in this
direction with a Leave-One-Out Gaussian PLDA (LGP) clustering algorithm and an
approach to training the DNN such that embeddings directly optimize performance
of this scoring method. This paper presents a new two-pass version of this
system, where the second pass uses finer time resolution to significantly
improve overall performance. For the Callhome corpus, we achieve the first
published error rate below 4% without any task-dependent parameter tuning. We
also show significant progress towards a robust single solution for multiple
diarization tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Karra_K/0/1/0/all/0/1"&gt;Kiran Karra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+McCree_A/0/1/0/all/0/1"&gt;Alan McCree&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NNrepair: Constraint-based Repair of Neural Network Classifiers. (arXiv:2103.12535v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12535</id>
        <link href="http://arxiv.org/abs/2103.12535"/>
        <updated>2021-06-16T01:21:10.700Z</updated>
        <summary type="html"><![CDATA[We present NNrepair, a constraint-based technique for repairing neural
network classifiers. The technique aims to fix the logic of the network at an
intermediate layer or at the last layer. NNrepair first uses fault localization
to find potentially faulty network parameters (such as the weights) and then
performs repair using constraint solving to apply small modifications to the
parameters to remedy the defects. We present novel strategies to enable precise
yet efficient repair such as inferring correctness specifications to act as
oracles for intermediate layer repair, and generation of experts for each
class. We demonstrate the technique in the context of three different
scenarios: (1) Improving the overall accuracy of a model, (2) Fixing security
vulnerabilities caused by poisoning of training data and (3) Improving the
robustness of the network against adversarial attacks. Our evaluation on MNIST
and CIFAR-10 models shows that NNrepair can improve the accuracy by 45.56
percentage points on poisoned data and 10.40 percentage points on adversarial
data. NNrepair also provides small improvement in the overall accuracy of
models, without requiring new data or re-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1"&gt;Muhammad Usman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopinath_D/0/1/0/all/0/1"&gt;Divya Gopinath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Youcheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noller_Y/0/1/0/all/0/1"&gt;Yannic Noller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasareanu_C/0/1/0/all/0/1"&gt;Corina Pasareanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inexact-ADMM Based Federated Meta-Learning for Fast and Continual Edge Learning. (arXiv:2012.08677v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08677</id>
        <link href="http://arxiv.org/abs/2012.08677"/>
        <updated>2021-06-16T01:21:10.617Z</updated>
        <summary type="html"><![CDATA[In order to meet the requirements for performance, safety, and latency in
many IoT applications, intelligent decisions must be made right here right now
at the network edge. However, the constrained resources and limited local data
amount pose significant challenges to the development of edge AI. To overcome
these challenges, we explore continual edge learning capable of leveraging the
knowledge transfer from previous tasks. Aiming to achieve fast and continual
edge learning, we propose a platform-aided federated meta-learning architecture
where edge nodes collaboratively learn a meta-model, aided by the knowledge
transfer from prior tasks. The edge learning problem is cast as a regularized
optimization problem, where the valuable knowledge learned from previous tasks
is extracted as regularization. Then, we devise an ADMM based federated
meta-learning algorithm, namely ADMM-FedMeta, where ADMM offers a natural
mechanism to decompose the original problem into many subproblems which can be
solved in parallel across edge nodes and the platform. Further, a variant of
inexact-ADMM method is employed where the subproblems are `solved' via linear
approximation as well as Hessian estimation to reduce the computational cost
per round to $\mathcal{O}(n)$. We provide a comprehensive analysis of
ADMM-FedMeta, in terms of the convergence properties, the rapid adaptation
performance, and the forgetting effect of prior knowledge transfer, for the
general non-convex case. Extensive experimental studies demonstrate the
effectiveness and efficiency of ADMM-FedMeta, and showcase that it
substantially outperforms the existing baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1"&gt;Sheng Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Ju Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Jiang Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Sen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junshan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoupling Value and Policy for Generalization in Reinforcement Learning. (arXiv:2102.10330v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10330</id>
        <link href="http://arxiv.org/abs/2102.10330"/>
        <updated>2021-06-16T01:21:10.616Z</updated>
        <summary type="html"><![CDATA[Standard deep reinforcement learning algorithms use a shared representation
for the policy and value function, especially when training directly from
images. However, we argue that more information is needed to accurately
estimate the value function than to learn the optimal policy. Consequently, the
use of a shared representation for the policy and value function can lead to
overfitting. To alleviate this problem, we propose two approaches which are
combined to create IDAAC: Invariant Decoupled Advantage Actor-Critic. First,
IDAAC decouples the optimization of the policy and value function, using
separate networks to model them. Second, it introduces an auxiliary loss which
encourages the representation to be invariant to task-irrelevant properties of
the environment. IDAAC shows good generalization to unseen environments,
achieving a new state-of-the-art on the Procgen benchmark and outperforming
popular methods on DeepMind Control tasks with distractors. Our implementation
is available at https://github.com/rraileanu/idaac.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1"&gt;Roberta Raileanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1"&gt;Rob Fergus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust learning under clean-label attack. (arXiv:2103.00671v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00671</id>
        <link href="http://arxiv.org/abs/2103.00671"/>
        <updated>2021-06-16T01:21:10.608Z</updated>
        <summary type="html"><![CDATA[We study the problem of robust learning under clean-label data-poisoning
attacks, where the attacker injects (an arbitrary set of) correctly-labeled
examples to the training set to fool the algorithm into making mistakes on
specific test instances at test time. The learning goal is to minimize the
attackable rate (the probability mass of attackable test instances), which is
more difficult than optimal PAC learning. As we show, any robust algorithm with
diminishing attackable rate can achieve the optimal dependence on $\epsilon$ in
its PAC sample complexity, i.e., $O(1/\epsilon)$. On the other hand, the
attackable rate might be large even for some optimal PAC learners, e.g., SVM
for linear classifiers. Furthermore, we show that the class of linear
hypotheses is not robustly learnable when the data distribution has zero margin
and is robustly learnable in the case of positive margin but requires sample
complexity exponential in the dimension. For a general hypothesis class with
bounded VC dimension, if the attacker is limited to add at most $t>0$ poison
examples, the optimal robust learning sample complexity grows almost linearly
with $t$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blum_A/0/1/0/all/0/1"&gt;Avrim Blum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1"&gt;Steve Hanneke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jian Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1"&gt;Han Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keyword Transformer: A Self-Attention Model for Keyword Spotting. (arXiv:2104.00769v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00769</id>
        <link href="http://arxiv.org/abs/2104.00769"/>
        <updated>2021-06-16T01:21:10.607Z</updated>
        <summary type="html"><![CDATA[The Transformer architecture has been successful across many domains,
including natural language processing, computer vision and speech recognition.
In keyword spotting, self-attention has primarily been used on top of
convolutional or recurrent encoders. We investigate a range of ways to adapt
the Transformer architecture to keyword spotting and introduce the Keyword
Transformer (KWT), a fully self-attentional architecture that exceeds
state-of-the-art performance across multiple tasks without any pre-training or
additional data. Surprisingly, this simple architecture outperforms more
complex models that mix convolutional, recurrent and attentive layers. KWT can
be used as a drop-in replacement for these models, setting two new benchmark
records on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on
the 12 and 35-command tasks respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Berg_A/0/1/0/all/0/1"&gt;Axel Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+OConnor_M/0/1/0/all/0/1"&gt;Mark O&amp;#x27;Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cruz_M/0/1/0/all/0/1"&gt;Miguel Tairum Cruz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. (arXiv:1901.10002v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1901.10002</id>
        <link href="http://arxiv.org/abs/1901.10002"/>
        <updated>2021-06-16T01:21:10.491Z</updated>
        <summary type="html"><![CDATA[As machine learning (ML) increasingly affects people and society, awareness
of its potential unwanted consequences has also grown. To anticipate, prevent,
and mitigate undesirable downstream consequences, it is critical that we
understand when and how harm might be introduced throughout the ML life cycle.
In this paper, we provide a framework that identifies seven distinct potential
sources of downstream harm in machine learning, spanning data collection,
development, and deployment. In doing so, we aim to facilitate more productive
and precise communication around these issues, as well as more direct,
application-grounded ways to mitigate them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_H/0/1/0/all/0/1"&gt;Harini Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1"&gt;John V. Guttag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Accuracy-Efficiency Trade-Offs as a Means for Holding Distributed ML Systems Accountable. (arXiv:2007.02203v5 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02203</id>
        <link href="http://arxiv.org/abs/2007.02203"/>
        <updated>2021-06-16T01:21:10.464Z</updated>
        <summary type="html"><![CDATA[Trade-offs between accuracy and efficiency are found in multiple
non-computing domains, such as law and public health, which have developed
rules and heuristics to guide how to balance the two in conditions of
uncertainty. While accuracy-efficiency trade-offs are also commonly
acknowledged in some areas of computer science, their policy implications
remain poorly examined. Drawing on risk assessment practices in the US, we
argue that, since examining accuracy-efficiency trade-offs has been useful for
guiding governance in other domains, explicitly framing such trade-offs in
computing is similarly useful for the governance of computer systems. Our
discussion focuses on real-time distributed ML systems; understanding the
policy implications in this area is particularly urgent because such systems,
which include autonomous vehicles, tend to be high-stakes and safety-critical.
We describe how the trade-off takes shape for these systems, highlight gaps
between existing US risk assessment standards and what these systems require in
order to be properly assessed, and make specific calls to action to facilitate
accountability when hypothetical risks become realized as accidents in the real
world. We close by discussing how such accountability mechanisms encourage more
just, transparent governance aligned with public values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1"&gt;A. Feder Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1"&gt;Karen Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Label Propagation for Semi-Supervised Speaker Identification. (arXiv:2106.08207v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08207</id>
        <link href="http://arxiv.org/abs/2106.08207"/>
        <updated>2021-06-16T01:21:10.431Z</updated>
        <summary type="html"><![CDATA[Speaker identification in the household scenario (e.g., for smart speakers)
is typically based on only a few enrollment utterances but a much larger set of
unlabeled data, suggesting semisupervised learning to improve speaker profiles.
We propose a graph-based semi-supervised learning approach for speaker
identification in the household scenario, to leverage the unlabeled speech
samples. In contrast to most of the works in speaker recognition that focus on
speaker-discriminative embeddings, this work focuses on speaker label inference
(scoring). Given a pre-trained embedding extractor, graph-based learning allows
us to integrate information about both labeled and unlabeled utterances.
Considering each utterance as a graph node, we represent pairwise utterance
similarity scores as edge weights. Graphs are constructed per household, and
speaker identities are propagated to unlabeled nodes to optimize a global
consistency criterion. We show in experiments on the VoxCeleb dataset that this
approach makes effective use of unlabeled data and improves speaker
identification accuracy compared to two state-of-the-art scoring methods as
well as their semi-supervised variants based on pseudo-labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_V/0/1/0/all/0/1"&gt;Venkatesh Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1"&gt;Andreas Stolcke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Modular Should Neural Module Networks Be for Systematic Generalization?. (arXiv:2106.08170v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08170</id>
        <link href="http://arxiv.org/abs/2106.08170"/>
        <updated>2021-06-16T01:21:10.424Z</updated>
        <summary type="html"><![CDATA[Neural Module Networks (NMNs) aim at Visual Question Answering (VQA) via
composition of modules that tackle a sub-task. NMNs are a promising strategy to
achieve systematic generalization, i.e. overcoming biasing factors in the
training distribution. However, the aspects of NMNs that facilitate systematic
generalization are not fully understood. In this paper, we demonstrate that the
stage and the degree at which modularity is defined has large influence on
systematic generalization. In a series of experiments on three VQA datasets
(MNIST with multiple attributes, SQOOP, and CLEVR-CoGenT), our results reveal
that tuning the degree of modularity in the network, especially at the image
encoder stage, reaches substantially higher systematic generalization. These
findings lead to new NMN architectures that outperform previous ones in terms
of systematic generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1"&gt;Vanessa D&amp;#x27;Amario&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1"&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Identification Through Transformers. (arXiv:2106.08185v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08185</id>
        <link href="http://arxiv.org/abs/2106.08185"/>
        <updated>2021-06-16T01:21:10.417Z</updated>
        <summary type="html"><![CDATA[Kernel selection plays a central role in determining the performance of
Gaussian Process (GP) models, as the chosen kernel determines both the
inductive biases and prior support of functions under the GP prior. This work
addresses the challenge of constructing custom kernel functions for
high-dimensional GP regression models. Drawing inspiration from recent progress
in deep learning, we introduce a novel approach named KITT: Kernel
Identification Through Transformers. KITT exploits a transformer-based
architecture to generate kernel recommendations in under 0.1 seconds, which is
several orders of magnitude faster than conventional kernel search algorithms.
We train our model using synthetic data generated from priors over a vocabulary
of known kernels. By exploiting the nature of the self-attention mechanism,
KITT is able to process datasets with inputs of arbitrary dimension. We
demonstrate that kernels chosen by KITT yield strong performance over a diverse
collection of regression benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Simpson_F/0/1/0/all/0/1"&gt;Fergus Simpson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Davies_I/0/1/0/all/0/1"&gt;Ian Davies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lalchand_V/0/1/0/all/0/1"&gt;Vidhi Lalchand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vullo_A/0/1/0/all/0/1"&gt;Alessandro Vullo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Durrande_N/0/1/0/all/0/1"&gt;Nicolas Durrande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rasmussen_C/0/1/0/all/0/1"&gt;Carl Rasmussen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning with Kernel Dependence Maximization. (arXiv:2106.08320v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08320</id>
        <link href="http://arxiv.org/abs/2106.08320"/>
        <updated>2021-06-16T01:21:10.380Z</updated>
        <summary type="html"><![CDATA[We approach self-supervised learning of image representations from a
statistical dependence perspective, proposing Self-Supervised Learning with the
Hilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes
dependence between representations of transformed versions of an image and the
image identity, while minimizing the kernelized variance of those features.
This self-supervised learning framework yields a new understanding of InfoNCE,
a variational lower bound on the mutual information (MI) between different
transformations. While the MI itself is known to have pathologies which can
result in meaningless representations being learned, its bound is much better
behaved: we show that it implicitly approximates SSL-HSIC (with a slightly
different regularizer). Our approach also gives us insight into BYOL, since
SSL-HSIC similarly learns local neighborhoods of samples. SSL-HSIC allows us to
directly optimize statistical dependence in time linear in the batch size,
without restrictive data assumptions or indirect mutual information estimators.
Trained with or without a target network, SSL-HSIC matches the current
state-of-the-art for standard linear evaluation on ImageNet, semi-supervised
learning and transfer to other classification and vision tasks such as semantic
segmentation, depth estimation and object recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yazhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pogodin_R/0/1/0/all/0/1"&gt;Roman Pogodin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1"&gt;Danica J. Sutherland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evading Malware Classifiers via Monte Carlo Mutant Feature Discovery. (arXiv:2106.07860v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.07860</id>
        <link href="http://arxiv.org/abs/2106.07860"/>
        <updated>2021-06-16T01:21:10.347Z</updated>
        <summary type="html"><![CDATA[The use of Machine Learning has become a significant part of malware
detection efforts due to the influx of new malware, an ever changing threat
landscape, and the ability of Machine Learning methods to discover meaningful
distinctions between malicious and benign software. Antivirus vendors have also
begun to widely utilize malware classifiers based on dynamic and static malware
analysis features. Therefore, a malware author might make evasive binary
modifications against Machine Learning models as part of the malware
development life cycle to execute an attack successfully. This makes the
studying of possible classifier evasion strategies an essential part of cyber
defense against malice. To this extent, we stage a grey box setup to analyze a
scenario where the malware author does not know the target classifier
algorithm, and does not have access to decisions made by the classifier, but
knows the features used in training. In this experiment, a malicious actor
trains a surrogate model using the EMBER-2018 dataset to discover binary
mutations that cause an instance to be misclassified via a Monte Carlo tree
search. Then, mutated malware is sent to the victim model that takes the place
of an antivirus API to test whether it can evade detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boutsikas_J/0/1/0/all/0/1"&gt;John Boutsikas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eren_M/0/1/0/all/0/1"&gt;Maksim E. Eren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varga_C/0/1/0/all/0/1"&gt;Charles Varga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1"&gt;Edward Raff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matuszek_C/0/1/0/all/0/1"&gt;Cynthia Matuszek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nicholas_C/0/1/0/all/0/1"&gt;Charles Nicholas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved SVRG for quadratic functions. (arXiv:2006.01017v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01017</id>
        <link href="http://arxiv.org/abs/2006.01017"/>
        <updated>2021-06-16T01:21:10.341Z</updated>
        <summary type="html"><![CDATA[We analyse an iterative algorithm to minimize quadratic functions whose
Hessian matrix $H$ is the expectation of a random symmetric $d\times d$ matrix.
The algorithm is a variant of the stochastic variance reduced gradient (SVRG).
In several applications, including least-squares regressions, ridge
regressions, linear discriminant analysis and regularized linear discriminant
analysis, the running time of each iteration is proportional to $d$. Under
smoothness and convexity conditions, the algorithm has linear convergence. When
applied to quadratic functions, our analysis improves the state-of-the-art
performance of SVRG up to a logarithmic factor. Furthermore, for
well-conditioned quadratic problems, our analysis improves the state-of-the-art
running times of accelerated SVRG, and is better than the known matching lower
bound, by a logarithmic factor. Our theoretical results are backed with
numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kahale_N/0/1/0/all/0/1"&gt;Nabil Kahale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Multi-objective Policy Optimization as a Tool for Reinforcement Learning. (arXiv:2106.08199v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08199</id>
        <link href="http://arxiv.org/abs/2106.08199"/>
        <updated>2021-06-16T01:21:10.334Z</updated>
        <summary type="html"><![CDATA[Many advances that have improved the robustness and efficiency of deep
reinforcement learning (RL) algorithms can, in one way or another, be
understood as introducing additional objectives, or constraints, in the policy
optimization step. This includes ideas as far ranging as exploration bonuses,
entropy regularization, and regularization toward teachers or data priors when
learning from experts or in offline RL. Often, task reward and auxiliary
objectives are in conflict with each other and it is therefore natural to treat
these examples as instances of multi-objective (MO) optimization problems. We
study the principles underlying MORL and introduce a new algorithm,
Distillation of a Mixture of Experts (DiME), that is intuitive and
scale-invariant under some conditions. We highlight its strengths on standard
MO benchmark problems and consider case studies in which we recast offline RL
and learning from experts as MO problems. This leads to a natural algorithmic
formulation that sheds light on the connection between existing approaches. For
offline RL, we use the MO perspective to derive a simple algorithm, that
optimizes for the standard RL objective plus a behavioral cloning term. This
outperforms state-of-the-art on two established offline RL benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdolmaleki_A/0/1/0/all/0/1"&gt;Abbas Abdolmaleki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Sandy H. Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vezzani_G/0/1/0/all/0/1"&gt;Giulia Vezzani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahriari_B/0/1/0/all/0/1"&gt;Bobak Shahriari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Springenberg_J/0/1/0/all/0/1"&gt;Jost Tobias Springenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Shruti Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+TB_D/0/1/0/all/0/1"&gt;Dhruva TB&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byravan_A/0/1/0/all/0/1"&gt;Arunkumar Byravan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bousmalis_K/0/1/0/all/0/1"&gt;Konstantinos Bousmalis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1"&gt;Andras Gyorgy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesvari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1"&gt;Raia Hadsell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1"&gt;Nicolas Heess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1"&gt;Martin Riedmiller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simon Says: Evaluating and Mitigating Bias in Pruned Neural Networks with Knowledge Distillation. (arXiv:2106.07849v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07849</id>
        <link href="http://arxiv.org/abs/2106.07849"/>
        <updated>2021-06-16T01:21:10.326Z</updated>
        <summary type="html"><![CDATA[In recent years the ubiquitous deployment of AI has posed great concerns in
regards to algorithmic bias, discrimination, and fairness. Compared to
traditional forms of bias or discrimination caused by humans, algorithmic bias
generated by AI is more abstract and unintuitive therefore more difficult to
explain and mitigate. A clear gap exists in the current literature on
evaluating and mitigating bias in pruned neural networks. In this work, we
strive to tackle the challenging issues of evaluating, mitigating, and
explaining induced bias in pruned neural networks. Our paper makes three
contributions. First, we propose two simple yet effective metrics, Combined
Error Variance (CEV) and Symmetric Distance Error (SDE), to quantitatively
evaluate the induced bias prevention quality of pruned models. Second, we
demonstrate that knowledge distillation can mitigate induced bias in pruned
neural networks, even with unbalanced datasets. Third, we reveal that model
similarity has strong correlations with pruning induced bias, which provides a
powerful method to explain why bias occurs in pruned neural networks. Our code
is available at https://github.com/codestar12/pruning-distilation-bias]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blakeney_C/0/1/0/all/0/1"&gt;Cody Blakeney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huish_N/0/1/0/all/0/1"&gt;Nathaniel Huish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yan Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1"&gt;Ziliang Zong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active feature selection discovers minimal gene-sets for classifying cell-types and disease states in single-cell mRNA-seq data. (arXiv:2106.08317v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2106.08317</id>
        <link href="http://arxiv.org/abs/2106.08317"/>
        <updated>2021-06-16T01:21:10.312Z</updated>
        <summary type="html"><![CDATA[Sequencing costs currently prohibit the application of single cell mRNA-seq
for many biological and clinical tasks of interest. Here, we introduce an
active learning framework that constructs compressed gene sets that enable high
accuracy classification of cell-types and physiological states while analyzing
a minimal number of gene transcripts. Our active feature selection procedure
constructs gene sets through an iterative cell-type classification task where
misclassified cells are examined at each round to identify maximally
informative genes through an `active' support vector machine (SVM) classifier.
Our active SVM procedure automatically identifies gene sets that enables
$>90\%$ cell-type classification accuracy in the Tabula Muris mouse tissue
survey as well as a $\sim 40$ gene set that enables classification of multiple
myeloma patient samples with $>95\%$ accuracy. Broadly, the discovery of
compact but highly informative gene sets might enable drastic reductions in
sequencing requirements for applications of single-cell mRNA-seq.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoqiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sisi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Thomson_M/0/1/0/all/0/1"&gt;Matt Thomson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sliced Iterative Normalizing Flows. (arXiv:2007.00674v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00674</id>
        <link href="http://arxiv.org/abs/2007.00674"/>
        <updated>2021-06-16T01:21:10.292Z</updated>
        <summary type="html"><![CDATA[We develop an iterative (greedy) deep learning (DL) algorithm which is able
to transform an arbitrary probability distribution function (PDF) into the
target PDF. The model is based on iterative Optimal Transport of a series of 1D
slices, matching on each slice the marginal PDF to the target. The axes of the
orthogonal slices are chosen to maximize the PDF difference using Wasserstein
distance at each iteration, which enables the algorithm to scale well to high
dimensions. As special cases of this algorithm, we introduce two sliced
iterative Normalizing Flow (SINF) models, which map from the data to the latent
space (GIS) and vice versa (SIG). We show that SIG is able to generate high
quality samples of image datasets, which match the GAN benchmarks, while GIS
obtains competitive results on density estimation tasks compared to the density
trained NFs, and is more stable, faster, and achieves higher $p(x)$ when
trained on small training sets. SINF approach deviates significantly from the
current DL paradigm, as it is greedy and does not use concepts such as
mini-batching, stochastic gradient descent and gradient back-propagation
through deep layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Biwei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seljak_U/0/1/0/all/0/1"&gt;Uros Seljak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08043</id>
        <link href="http://arxiv.org/abs/2106.08043"/>
        <updated>2021-06-16T01:21:10.274Z</updated>
        <summary type="html"><![CDATA[The vast majority of existing methods and systems for causal inference assume
that all variables under consideration are categorical or numerical (e.g.,
gender, price, blood pressure, enrollment). In this paper, we present
CausalNLP, a toolkit for inferring causality from observational data that
includes text in addition to traditional numerical and categorical variables.
CausalNLP employs the use of meta-learners for treatment effect estimation and
supports using raw text and its linguistic properties as both a treatment and a
"controlled-for" variable (e.g., confounder). The library is open-source and
available at: https://github.com/amaiya/causalnlp.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1"&gt;Arun S. Maiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation. (arXiv:1911.01529v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.01529</id>
        <link href="http://arxiv.org/abs/1911.01529"/>
        <updated>2021-06-16T01:21:10.268Z</updated>
        <summary type="html"><![CDATA[Deep learning approaches have become the standard solution to many problems
in computer vision and robotics, but obtaining sufficient training data in high
enough quality is challenging, as human labor is error prone, time consuming,
and expensive. Solutions based on simulation have become more popular in recent
years, but the gap between simulation and reality is still a major issue. In
this paper, we introduce a novel method for augmenting synthetic image data
through unsupervised image-to-image translation by applying the style of real
world images to simulated images with open source frameworks. The generated
dataset is combined with conventional augmentation methods and is then applied
to a neural network model running in real-time on autonomous soccer robots. Our
evaluation shows a significant improvement compared to models trained on images
generated entirely in simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blumenkamp_J/0/1/0/all/0/1"&gt;Jan Blumenkamp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baude_A/0/1/0/all/0/1"&gt;Andreas Baude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laue_T/0/1/0/all/0/1"&gt;Tim Laue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PairConnect: A Compute-Efficient MLP Alternative to Attention. (arXiv:2106.08235v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08235</id>
        <link href="http://arxiv.org/abs/2106.08235"/>
        <updated>2021-06-16T01:21:10.262Z</updated>
        <summary type="html"><![CDATA[Transformer models have demonstrated superior performance in natural language
processing. The dot product self-attention in Transformer allows us to model
interactions between words. However, this modeling comes with significant
computational overhead. In this work, we revisit the memory-compute trade-off
associated with Transformer, particularly multi-head attention, and show a
memory-heavy but significantly more compute-efficient alternative to
Transformer. Our proposal, denoted as PairConnect, a multilayer perceptron
(MLP), models the pairwise interaction between words by explicit pairwise word
embeddings. As a result, PairConnect substitutes self dot product with a simple
embedding lookup. We show mathematically that despite being an MLP, our
compute-efficient PairConnect is strictly more expressive than Transformer. Our
experiment on language modeling tasks suggests that PairConnect could achieve
comparable results with Transformer while reducing the computational cost
associated with inference significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhaozhuo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Minghao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RFpredInterval: An R Package for Prediction Intervals with Random Forests and Boosted Forests. (arXiv:2106.08217v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08217</id>
        <link href="http://arxiv.org/abs/2106.08217"/>
        <updated>2021-06-16T01:21:10.255Z</updated>
        <summary type="html"><![CDATA[Like many predictive models, random forests provide a point prediction for a
new observation. Besides the point prediction, it is important to quantify the
uncertainty in the prediction. Prediction intervals provide information about
the reliability of the point predictions. We have developed a comprehensive R
package, RFpredInterval, that integrates 16 methods to build prediction
intervals with random forests and boosted forests. The methods implemented in
the package are a new method to build prediction intervals with boosted forests
(PIBF) and 15 different variants to produce prediction intervals with random
forests proposed by Roy and Larocque (2020). We perform an extensive simulation
study and apply real data analyses to compare the performance of the proposed
method to ten existing methods to build prediction intervals with random
forests. The results show that the proposed method is very competitive and,
globally, it outperforms the competing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Alakus_C/0/1/0/all/0/1"&gt;Cansu Alakus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Larocque_D/0/1/0/all/0/1"&gt;Denis Larocque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Labbe_A/0/1/0/all/0/1"&gt;Aurelie Labbe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CRFL: Certifiably Robust Federated Learning against Backdoor Attacks. (arXiv:2106.08283v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08283</id>
        <link href="http://arxiv.org/abs/2106.08283"/>
        <updated>2021-06-16T01:21:10.205Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) as a distributed learning paradigm that aggregates
information from diverse clients to train a shared global model, has
demonstrated great success. However, malicious clients can perform poisoning
attacks and model replacement to introduce backdoors into the trained global
model. Although there have been intensive studies designing robust aggregation
methods and empirical robust federated training protocols against backdoors,
existing approaches lack robustness certification. This paper provides the
first general framework, Certifiably Robust Federated Learning (CRFL), to train
certifiably robust FL models against backdoors. Our method exploits clipping
and smoothing on model parameters to control the global model smoothness, which
yields a sample-wise robustness certification on backdoors with limited
magnitude. Our certification also specifies the relation to federated learning
parameters, such as poisoning ratio on instance level, number of attackers, and
training iterations. Practically, we conduct comprehensive experiments across a
range of federated datasets, and provide the first benchmark for certified
robustness against backdoor attacks in federated learning. Our code is
available at https://github.com/AI-secure/CRFL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chulin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minghao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Autoregressive Electron Redistribution Modeling for Reaction Prediction. (arXiv:2106.07801v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.07801</id>
        <link href="http://arxiv.org/abs/2106.07801"/>
        <updated>2021-06-16T01:21:10.185Z</updated>
        <summary type="html"><![CDATA[Reliably predicting the products of chemical reactions presents a fundamental
challenge in synthetic chemistry. Existing machine learning approaches
typically produce a reaction product by sequentially forming its subparts or
intermediate molecules. Such autoregressive methods, however, not only require
a pre-defined order for the incremental construction but preclude the use of
parallel decoding for efficient computation. To address these issues, we devise
a non-autoregressive learning paradigm that predicts reaction in one shot.
Leveraging the fact that chemical reactions can be described as a
redistribution of electrons in molecules, we formulate a reaction as an
arbitrary electron flow and predict it with a novel multi-pointer decoding
network. Experiments on the USPTO-MIT dataset show that our approach has
established a new state-of-the-art top-1 accuracy and achieves at least 27
times inference speedup over the state-of-the-art methods. Also, our
predictions are easier for chemists to interpret owing to predicting the
electron flows.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Bi_H/0/1/0/all/0/1"&gt;Hangrui Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hengyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chence Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Coley_C/0/1/0/all/0/1"&gt;Connor Coley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text Generation with Efficient (Soft) Q-Learning. (arXiv:2106.07704v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07704</id>
        <link href="http://arxiv.org/abs/2106.07704"/>
        <updated>2021-06-16T01:21:10.177Z</updated>
        <summary type="html"><![CDATA[Maximum likelihood estimation (MLE) is the predominant algorithm for training
text generation models. This paradigm relies on direct supervision examples,
which is not applicable to many applications, such as generating adversarial
attacks or generating prompts to control language models. Reinforcement
learning (RL) on the other hand offers a more flexible solution by allowing
users to plug in arbitrary task metrics as reward. Yet previous RL algorithms
for text generation, such as policy gradient (on-policy RL) and Q-learning
(off-policy RL), are often notoriously inefficient or unstable to train due to
the large sequence space and the sparse reward received only at the end of
sequences. In this paper, we introduce a new RL formulation for text generation
from the soft Q-learning perspective. It further enables us to draw from the
latest RL advances, such as path consistency learning, to combine the best of
on-/off-policy updates, and learn effectively from sparse reward. We apply the
approach to a wide range of tasks, including learning from noisy/negative
examples, adversarial attacks, and prompt generation. Experiments show our
approach consistently outperforms both task-specialized algorithms and the
previous RL methods. On standard supervised tasks where MLE prevails, our
approach also achieves competitive performance and stability by training text
generation from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Han Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1"&gt;Bowen Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengzhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"&gt;Eric P. Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhiting Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Prevent Leakage: Privacy-Preserving Inference in the Mobile Cloud. (arXiv:1912.08421v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.08421</id>
        <link href="http://arxiv.org/abs/1912.08421"/>
        <updated>2021-06-16T01:21:10.170Z</updated>
        <summary type="html"><![CDATA[Powered by machine learning services in the cloud, numerous learning-driven
mobile applications are gaining popularity in the market. As deep learning
tasks are mostly computation-intensive, it has become a trend to process raw
data on devices and send the deep neural network (DNN) features to the cloud,
where the features are further processed to return final results. However,
there is always unexpected leakage with the release of features, with which an
adversary could infer a significant amount of information about the original
data. We propose a privacy-preserving reinforcement learning framework on top
of the mobile cloud infrastructure from the perspective of DNN structures. The
framework aims to learn a policy to modify the base DNNs to prevent information
leakage while maintaining high inference accuracy. The policy can also be
readily transferred to large-size DNNs to speed up learning. Extensive
evaluations on a variety of DNNs have shown that our framework can successfully
find privacy-preserving DNN structures to defend different privacy attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1"&gt;Liyao Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Congcong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanshi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence of Deep Learning with Differential Privacy. (arXiv:2106.07830v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07830</id>
        <link href="http://arxiv.org/abs/2106.07830"/>
        <updated>2021-06-16T01:21:10.161Z</updated>
        <summary type="html"><![CDATA[In deep learning with differential privacy (DP), the neural network achieves
the privacy usually at the cost of slower convergence (and thus lower
performance) than its non-private counterpart. This work gives the first
convergence analysis of the DP deep learning, through the lens of training
dynamics and the neural tangent kernel (NTK). Our convergence theory
successfully characterizes the effects of two key components in the DP
training: the per-sample clipping (flat or layerwise) and the noise addition.
Our analysis not only initiates a general principled framework to understand
the DP deep learning with any network architecture and loss function, but also
motivates a new clipping method -- the global clipping, that significantly
improves the convergence while preserving the same privacy guarantee as the
existing local clipping.

In terms of theoretical results, we establish the precise connection between
the per-sample clipping and NTK matrix. We show that in the gradient flow,
i.e., with infinitesimal learning rate, the noise level of DP optimizers does
not affect the convergence. We prove that DP gradient descent (GD) with global
clipping guarantees the monotone convergence to zero loss, which can be
violated by the existing DP-GD with local clipping. Notably, our analysis
framework easily extends to other optimizers, e.g., DP-Adam. Empirically
speaking, DP optimizers equipped with global clipping perform strongly on a
wide range of classification and regression tasks. In particular, our global
clipping is surprisingly effective at learning calibrated classifiers, in
contrast to the existing DP classifiers which are oftentimes over-confident and
unreliable. Implementation-wise, the new clipping can be realized by adding one
line of code into the Opacus library.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1"&gt;Zhiqi Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1"&gt;Qi Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weijie J. Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAN-LOC: Spoofing Detection and Physical Intrusion Localization on an In-Vehicle CAN Bus Based on Deep Features of Voltage Signals. (arXiv:2106.07895v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.07895</id>
        <link href="http://arxiv.org/abs/2106.07895"/>
        <updated>2021-06-16T01:21:10.152Z</updated>
        <summary type="html"><![CDATA[The Controller Area Network (CAN) is used for communication between
in-vehicle devices. The CAN bus has been shown to be vulnerable to remote
attacks. To harden vehicles against such attacks, vehicle manufacturers have
divided in-vehicle networks into sub-networks, logically isolating critical
devices. However, attackers may still have physical access to various
sub-networks where they can connect a malicious device. This threat has not
been adequately addressed, as methods proposed to determine physical intrusion
points have shown weak results, emphasizing the need to develop more advanced
techniques. To address this type of threat, we propose a security hardening
system for in-vehicle networks. The proposed system includes two mechanisms
that process deep features extracted from voltage signals measured on the CAN
bus. The first mechanism uses data augmentation and deep learning to detect and
locate physical intrusions when the vehicle starts; this mechanism can detect
and locate intrusions, even when the connected malicious devices are silent.
This mechanism's effectiveness (100% accuracy) is demonstrated in a wide
variety of insertion scenarios on a CAN bus prototype. The second mechanism is
a continuous device authentication mechanism, which is also based on deep
learning; this mechanism's robustness (99.8% accuracy) is demonstrated on a
real moving vehicle.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levy_E/0/1/0/all/0/1"&gt;Efrat Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1"&gt;Asaf Shabtai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Groza_B/0/1/0/all/0/1"&gt;Bogdan Groza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murvay_P/0/1/0/all/0/1"&gt;Pal-Stefan Murvay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1"&gt;Yuval Elovici&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Robustness of Graph Neural Networks with Heterophily-Inspired Designs. (arXiv:2106.07767v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07767</id>
        <link href="http://arxiv.org/abs/2106.07767"/>
        <updated>2021-06-16T01:21:10.112Z</updated>
        <summary type="html"><![CDATA[Recent studies have exposed that many graph neural networks (GNNs) are
sensitive to adversarial attacks, and can suffer from performance loss if the
graph structure is intentionally perturbed. A different line of research has
shown that many GNN architectures implicitly assume that the underlying graph
displays homophily, i.e., connected nodes are more likely to have similar
features and class labels, and perform poorly if this assumption is not
fulfilled. In this work, we formalize the relation between these two seemingly
different issues. We theoretically show that in the standard scenario in which
node features exhibit homophily, impactful structural attacks always lead to
increased levels of heterophily. Then, inspired by GNN architectures that
target heterophily, we present two designs -- (i) separate aggregators for ego-
and neighbor-embeddings, and (ii) a reduced scope of aggregation -- that can
significantly improve the robustness of GNNs. Our extensive empirical
evaluations show that GNNs featuring merely these two designs can achieve
significantly improved robustness compared to the best-performing unvaccinated
model with 24.99% gain in average performance under targeted attacks, while
having smaller computational overhead than existing defense mechanisms.
Furthermore, these designs can be readily combined with explicit defense
mechanisms to yield state-of-the-art robustness with up to 18.33% increase in
performance under attacks compared to the best-performing vaccinated model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Junchen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaub_M/0/1/0/all/0/1"&gt;Michael T. Schaub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1"&gt;Danai Koutra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Clinically Inspired Approach for Melanoma classification. (arXiv:2106.08021v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08021</id>
        <link href="http://arxiv.org/abs/2106.08021"/>
        <updated>2021-06-16T01:21:10.098Z</updated>
        <summary type="html"><![CDATA[Melanoma is a leading cause of deaths due to skin cancer deaths and hence,
early and effective diagnosis of melanoma is of interest. Current approaches
for automated diagnosis of melanoma either use pattern recognition or
analytical recognition like ABCDE (asymmetry, border, color, diameter and
evolving) criterion. In practice however, a differential approach wherein
outliers (ugly duckling) are detected and used to evaluate nevi/lesions.
Incorporation of differential recognition in Computer Aided Diagnosis (CAD)
systems has not been explored but can be beneficial as it can provide a
clinical justification for the derived decision. We present a method for
identifying and quantifying ugly ducklings by performing Intra-Patient
Comparative Analysis (IPCA) of neighboring nevi. This is then incorporated in a
CAD system design for melanoma detection. This design ensures flexibility to
handle cases where IPCA is not possible. Our experiments on a public dataset
show that the outlier information helps boost the sensitivity of detection by
at least 4.1 % and specificity by 4.0 % to 8.9 %, depending on the use of a
strong (EfficientNet) or moderately strong (VGG or ResNet) classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akundi_P/0/1/0/all/0/1"&gt;Prathyusha Akundi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gun_S/0/1/0/all/0/1"&gt;Soumyasis Gun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivaswamy_J/0/1/0/all/0/1"&gt;Jayanthi Sivaswamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KL Guided Domain Adaptation. (arXiv:2106.07780v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07780</id>
        <link href="http://arxiv.org/abs/2106.07780"/>
        <updated>2021-06-16T01:21:10.092Z</updated>
        <summary type="html"><![CDATA[Domain adaptation is an important problem and often needed for real-world
applications. In this problem, instead of i.i.d. datapoints, we assume that the
source (training) data and the target (testing) data have different
distributions. With that setting, the empirical risk minimization training
procedure often does not perform well, since it does not account for the change
in the distribution. A common approach in the domain adaptation literature is
to learn a representation of the input that has the same distributions over the
source and the target domain. However, these approaches often require
additional networks and/or optimizing an adversarial (minimax) objective, which
can be very expensive or unstable in practice. To tackle this problem, we first
derive a generalization bound for the target loss based on the training loss
and the reverse Kullback-Leibler (KL) divergence between the source and the
target representation distributions. Based on this bound, we derive an
algorithm that minimizes the KL term to obtain a better generalization to the
target domain. We show that with a probabilistic representation network, the KL
term can be estimated efficiently via minibatch samples without any additional
network or a minimax objective. This leads to a theoretically sound alignment
method which is also very efficient and stable in practice. Experimental
results also suggest that our method outperforms other representation-alignment
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;A. Tuan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Toan Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H. S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baydin_A/0/1/0/all/0/1"&gt;At&amp;#x131;l&amp;#x131;m G&amp;#xfc;ne&amp;#x15f; Baydin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.08208</id>
        <link href="http://arxiv.org/abs/2106.08208"/>
        <updated>2021-06-16T01:21:10.083Z</updated>
        <summary type="html"><![CDATA[Adaptive gradient methods have shown excellent performance for solving many
machine learning problems. Although multiple adaptive methods were recently
studied, they mainly focus on either empirical or theoretical aspects and also
only work for specific problems by using specific adaptive learning rates. It
is desired to design a universal framework for practical algorithms of adaptive
gradients with theoretical guarantee to solve general problems. To fill this
gap, we propose a faster and universal framework of adaptive gradients (i.e.,
SUPER-ADAM) by introducing a universal adaptive matrix that includes most
existing adaptive gradient forms. Moreover, our framework can flexibly
integrates the momentum and variance reduced techniques. In particular, our
novel framework provides the convergence analysis support for adaptive gradient
methods under the nonconvex setting. In theoretical analysis, we prove that our
new algorithm can achieve the best known complexity of
$\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point of
nonconvex optimization, which matches the lower bound for stochastic smooth
nonconvex optimization. In numerical experiments, we employ various deep
learning tasks to validate that our algorithm consistently outperforms the
existing adaptive algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Large-Cohort Training for Federated Learning. (arXiv:2106.07820v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07820</id>
        <link href="http://arxiv.org/abs/2106.07820"/>
        <updated>2021-06-16T01:21:10.074Z</updated>
        <summary type="html"><![CDATA[Federated learning methods typically learn a model by iteratively sampling
updates from a population of clients. In this work, we explore how the number
of clients sampled at each round (the cohort size) impacts the quality of the
learned model and the training dynamics of federated learning algorithms. Our
work poses three fundamental questions. First, what challenges arise when
trying to scale federated learning to larger cohorts? Second, what parallels
exist between cohort sizes in federated learning and batch sizes in centralized
learning? Last, how can we design federated learning methods that effectively
utilize larger cohort sizes? We give partial answers to these questions based
on extensive empirical evaluation. Our work highlights a number of challenges
stemming from the use of larger cohorts. While some of these (such as
generalization issues and diminishing returns) are analogs of large-batch
training challenges, others (including training failures and fairness concerns)
are unique to federated learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1"&gt;Zachary Charles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1"&gt;Zachary Garrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1"&gt;Zhouyuan Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmulyian_S/0/1/0/all/0/1"&gt;Sergei Shmulyian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1"&gt;Virginia Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Out-of-Distribution Detection on Deep Probabilistic Generative Models. (arXiv:2106.07903v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07903</id>
        <link href="http://arxiv.org/abs/2106.07903"/>
        <updated>2021-06-16T01:21:10.049Z</updated>
        <summary type="html"><![CDATA[Out-of-distribution (OOD) detection is an important task in machine learning
systems for ensuring their reliability and safety. Deep probabilistic
generative models facilitate OOD detection by estimating the likelihood of a
data sample. However, such models frequently assign a suspiciously high
likelihood to a specific outlier. Several recent works have addressed this
issue by training a neural network with auxiliary outliers, which are generated
by perturbing the input data. In this paper, we discover that these approaches
fail for certain OOD datasets. Thus, we suggest a new detection metric that
operates without outlier exposure. We observe that our metric is robust to
diverse variations of an image compared to the previous outlier-exposing
methods. Furthermore, our proposed score requires neither auxiliary models nor
additional training. Instead, this paper utilizes the likelihood ratio
statistic in a new perspective to extract genuine properties from the given
single deep probabilistic generative model. We also apply a novel numerical
approximation to enable fast implementation. Finally, we demonstrate
comprehensive experiments on various probabilistic generative models and show
that our method achieves state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jaemoo Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1"&gt;Changyeon Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1"&gt;Jeongwoo Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Myungjoo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Micro-Structured Weight Unification for Neural Network Compression. (arXiv:2106.08301v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08301</id>
        <link href="http://arxiv.org/abs/2106.08301"/>
        <updated>2021-06-16T01:21:10.042Z</updated>
        <summary type="html"><![CDATA[Compressing Deep Neural Network (DNN) models to alleviate the storage and
computation requirements is essential for practical applications, especially
for resource limited devices. Although capable of reducing a reasonable amount
of model parameters, previous unstructured or structured weight pruning methods
can hardly truly accelerate inference, either due to the poor hardware
compatibility of the unstructured sparsity or due to the low sparse rate of the
structurally pruned network. Aiming at reducing both storage and computation,
as well as preserving the original task performance, we propose a generalized
weight unification framework at a hardware compatible micro-structured level to
achieve high amount of compression and acceleration. Weight coefficients of a
selected micro-structured block are unified to reduce the storage and
computation of the block without changing the neuron connections, which turns
to a micro-structured pruning special case when all unified coefficients are
set to zero, where neuron connections (hence storage and computation) are
completely removed. In addition, we developed an effective training framework
based on the alternating direction method of multipliers (ADMM), which converts
our complex constrained optimization into separately solvable subproblems.
Through iteratively optimizing the subproblems, the desired micro-structure can
be ensured with high compression ratio and low performance degradation. We
extensively evaluated our method using a variety of benchmark models and
datasets for different applications. Experimental results demonstrate
state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Sheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Songnan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08038</id>
        <link href="http://arxiv.org/abs/2106.08038"/>
        <updated>2021-06-16T01:21:10.030Z</updated>
        <summary type="html"><![CDATA[Averaging predictions over a set of models -- an ensemble -- is widely used
to improve predictive performance and uncertainty estimation of deep learning
models. At the same time, many machine learning systems, such as search,
matching, and recommendation systems, heavily rely on embeddings.
Unfortunately, due to misalignment of features of independently trained models,
embeddings, cannot be improved with a naive deep ensemble like approach. In
this work, we look at the ensembling of representations and propose mean
embeddings with test-time augmentation (MeTTA) simple yet well-performing
recipe for ensembling representations. Empirically we demonstrate that MeTTA
significantly boosts the quality of linear evaluation on ImageNet for both
supervised and self-supervised models. Even more exciting, we draw connections
between MeTTA, image retrieval, and transformation invariant models. We believe
that spreading the success of ensembles to inference higher-quality
representations is the important step that will open many new applications of
ensembling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1"&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1"&gt;Andrei Atanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Next Generation Reservoir Computing. (arXiv:2106.07688v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07688</id>
        <link href="http://arxiv.org/abs/2106.07688"/>
        <updated>2021-06-16T01:21:10.023Z</updated>
        <summary type="html"><![CDATA[Reservoir computing is a best-in-class machine learning algorithm for
processing information generated by dynamical systems using observed
time-series data. Importantly, it requires very small training data sets, uses
linear optimization, and thus requires minimal computing resources. However,
the algorithm uses randomly sampled matrices to define the underlying recurrent
neural network and has a multitude of metaparameters that must be optimized.
Recent results demonstrate the equivalence of reservoir computing to nonlinear
vector autoregression, which requires no random matrices, fewer metaparameters,
and provides interpretable results. Here, we demonstrate that nonlinear vector
autoregression excels at reservoir computing benchmark tasks and requires even
shorter training data sets and training time, heralding the next generation of
reservoir computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1"&gt;Daniel J. Gauthier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bollt_E/0/1/0/all/0/1"&gt;Erik Bollt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffith_A/0/1/0/all/0/1"&gt;Aaron Griffith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbosa_W/0/1/0/all/0/1"&gt;Wendson A.S. Barbosa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Gradient Manifold Neural Network. (arXiv:2106.07905v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07905</id>
        <link href="http://arxiv.org/abs/2106.07905"/>
        <updated>2021-06-16T01:21:10.014Z</updated>
        <summary type="html"><![CDATA[Deep neural network (DNN) generally takes thousands of iterations to optimize
via gradient descent and thus has a slow convergence. In addition, softmax, as
a decision layer, may ignore the distribution information of the data during
classification. Aiming to tackle the referred problems, we propose a novel
manifold neural network based on non-gradient optimization, i.e., the
closed-form solutions. Considering that the activation function is generally
invertible, we reconstruct the network via forward ridge regression and low
rank backward approximation, which achieve the rapid convergence. Moreover, by
unifying the flexible Stiefel manifold and adaptive support vector machine, we
devise the novel decision layer which efficiently fits the manifold structure
of the data and label information. Consequently, a jointly non-gradient
optimization method is designed to generate the network with closed-form
results. Eventually, extensive experiments validate the superior performance of
the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Z/0/1/0/all/0/1"&gt;Ziheng Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuelong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voting for the right answer: Adversarial defense for speaker verification. (arXiv:2106.07868v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07868</id>
        <link href="http://arxiv.org/abs/2106.07868"/>
        <updated>2021-06-16T01:21:10.001Z</updated>
        <summary type="html"><![CDATA[Automatic speaker verification (ASV) is a well developed technology for
biometric identification, and has been ubiquitous implemented in
security-critic applications, such as banking and access control. However,
previous works have shown that ASV is under the radar of adversarial attacks,
which are very similar to their original counterparts from human's perception,
yet will manipulate the ASV render wrong prediction. Due to the very late
emergence of adversarial attacks for ASV, effective countermeasures against
them are limited. Given that the security of ASV is of high priority, in this
work, we propose the idea of "voting for the right answer" to prevent risky
decisions of ASV in blind spot areas, by employing random sampling and voting.
Experimental results show that our proposed method improves the robustness
against both the limited-knowledge attackers by pulling the adversarial samples
out of the blind spots, and the perfect-knowledge attackers by introducing
randomness and increasing the attackers' budgets. The code for reproducing main
results is available at https://github.com/thuhcsi/adsv_voting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haibin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling Neural Networks with Rule Representations. (arXiv:2106.07804v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07804</id>
        <link href="http://arxiv.org/abs/2106.07804"/>
        <updated>2021-06-16T01:21:09.977Z</updated>
        <summary type="html"><![CDATA[We propose a novel training method to integrate rules into deep learning, in
a way their strengths are controllable at inference. Deep Neural Networks with
Controllable Rule Representations (DeepCTRL) incorporates a rule encoder into
the model coupled with a rule-based objective, enabling a shared representation
for decision making. DeepCTRL is agnostic to data type and model architecture.
It can be applied to any kind of rule defined for inputs and outputs. The key
aspect of DeepCTRL is that it does not require retraining to adapt the rule
strength -- at inference, the user can adjust it based on the desired operation
point on accuracy vs. rule verification ratio. In real-world domains where
incorporating rules is critical -- such as Physics, Retail and Healthcare -- we
show the effectiveness of DeepCTRL in teaching rules for deep learning.
DeepCTRL improves the trust and reliability of the trained models by
significantly increasing their rule verification ratio, while also providing
accuracy gains at downstream tasks. Additionally, DeepCTRL enables novel use
cases such as hypothesis testing of the rules on data samples, and unsupervised
adaptation based on shared rules between datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1"&gt;Sungyong Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1"&gt;Sercan O. Arik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jinsung Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kihyuk Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1"&gt;Tomas Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exponential Improvement on the Memorization Capacity of Deep Threshold Networks. (arXiv:2106.07724v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07724</id>
        <link href="http://arxiv.org/abs/2106.07724"/>
        <updated>2021-06-16T01:21:09.963Z</updated>
        <summary type="html"><![CDATA[It is well known that modern deep neural networks are powerful enough to
memorize datasets even when the labels have been randomized. Recently,
Vershynin (2020) settled a long standing question by Baum (1988), proving that
\emph{deep threshold} networks can memorize $n$ points in $d$ dimensions using
$\widetilde{\mathcal{O}}(e^{1/\delta^2}+\sqrt{n})$ neurons and
$\widetilde{\mathcal{O}}(e^{1/\delta^2}(d+\sqrt{n})+n)$ weights, where $\delta$
is the minimum distance between the points. In this work, we improve the
dependence on $\delta$ from exponential to almost linear, proving that
$\widetilde{\mathcal{O}}(\frac{1}{\delta}+\sqrt{n})$ neurons and
$\widetilde{\mathcal{O}}(\frac{d}{\delta}+n)$ weights are sufficient. Our
construction uses Gaussian random weights only in the first layer, while all
the subsequent layers use binary or integer weights. We also prove new lower
bounds by connecting memorization in neural networks to the purely geometric
problem of separating $n$ points on a sphere using hyperplanes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajput_S/0/1/0/all/0/1"&gt;Shashank Rajput&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivasan_K/0/1/0/all/0/1"&gt;Kartik Sreenivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1"&gt;Dimitris Papailiopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1"&gt;Amin Karbasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEiT: BERT Pre-Training of Image Transformers. (arXiv:2106.08254v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08254</id>
        <link href="http://arxiv.org/abs/2106.08254"/>
        <updated>2021-06-16T01:21:09.955Z</updated>
        <summary type="html"><![CDATA[We introduce a self-supervised vision representation model BEiT, which stands
for Bidirectional Encoder representation from Image Transformers. Following
BERT developed in the natural language processing area, we propose a masked
image modeling task to pretrain vision Transformers. Specifically, each image
has two views in our pre-training, i.e, image patches (such as 16x16 pixels),
and visual tokens (i.e., discrete tokens). We first "tokenize" the original
image into visual tokens. Then we randomly mask some image patches and fed them
into the backbone Transformer. The pre-training objective is to recover the
original visual tokens based on the corrupted image patches. After pre-training
BEiT, we directly fine-tune the model parameters on downstream tasks by
appending task layers upon the pretrained encoder. Experimental results on
image classification and semantic segmentation show that our model achieves
competitive results with previous pre-training methods. For example, base-size
BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming
from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size
BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with
supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models
are available at https://aka.ms/beit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hangbo Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Canonical-Correlation-Based Fast Feature Selection. (arXiv:2106.08247v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08247</id>
        <link href="http://arxiv.org/abs/2106.08247"/>
        <updated>2021-06-16T01:21:09.944Z</updated>
        <summary type="html"><![CDATA[This paper proposes a canonical-correlation-based filter method for feature
selection. The sum of squared canonical correlation coefficients is adopted as
the feature ranking criterion. The proposed method boosts the computational
speed of the ranking criterion in greedy search. The supporting theorems
developed for the feature selection method are fundamental to the understanding
of the canonical correlation analysis. In empirical studies, a synthetic
dataset is used to demonstrate the speed advantage of the proposed method, and
eight real datasets are applied to show the effectiveness of the proposed
feature ranking criterion in both classification and regression. The results
show that the proposed method is considerably faster than the definition-based
method, and the proposed ranking criterion is competitive compared with the
seven mutual-information-based criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sikai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tingna Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Worden_K/0/1/0/all/0/1"&gt;Keith Worden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cross_E/0/1/0/all/0/1"&gt;Elizabeth J. Cross&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Efficient Reinforcement Learning In Continuous State Spaces: A Perspective Beyond Linearity. (arXiv:2106.07814v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07814</id>
        <link href="http://arxiv.org/abs/2106.07814"/>
        <updated>2021-06-16T01:21:09.938Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) is empirically successful in complex nonlinear
Markov decision processes (MDPs) with continuous state spaces. By contrast, the
majority of theoretical RL literature requires the MDP to satisfy some form of
linear structure, in order to guarantee sample efficient RL. Such efforts
typically assume the transition dynamics or value function of the MDP are
described by linear functions of the state features. To resolve this
discrepancy between theory and practice, we introduce the Effective Planning
Window (EPW) condition, a structural condition on MDPs that makes no linearity
assumptions. We demonstrate that the EPW condition permits sample efficient RL,
by providing an algorithm which provably solves MDPs satisfying this condition.
Our algorithm requires minimal assumptions on the policy class, which can
include multi-layer neural networks with nonlinear activation functions.
Notably, the EPW condition is directly motivated by popular gaming benchmarks,
and we show that many classic Atari games satisfy this condition. We
additionally show the necessity of conditions like EPW, by demonstrating that
simple MDPs with slight nonlinearities cannot be solved sample efficiently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malik_D/0/1/0/all/0/1"&gt;Dhruv Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_V/0/1/0/all/0/1"&gt;Vishwak Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S-LIME: Stabilized-LIME for Model Explanation. (arXiv:2106.07875v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.07875</id>
        <link href="http://arxiv.org/abs/2106.07875"/>
        <updated>2021-06-16T01:21:09.919Z</updated>
        <summary type="html"><![CDATA[An increasing number of machine learning models have been deployed in domains
with high stakes such as finance and healthcare. Despite their superior
performances, many models are black boxes in nature which are hard to explain.
There are growing efforts for researchers to develop methods to interpret these
black-box models. Post hoc explanations based on perturbations, such as LIME,
are widely used approaches to interpret a machine learning model after it has
been built. This class of methods has been shown to exhibit large instability,
posing serious challenges to the effectiveness of the method itself and harming
user trust. In this paper, we propose S-LIME, which utilizes a hypothesis
testing framework based on central limit theorem for determining the number of
perturbation points needed to guarantee stability of the resulting explanation.
Experiments on both simulated and real world data sets are provided to
demonstrate the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhengze Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hooker_G/0/1/0/all/0/1"&gt;Giles Hooker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tracing Back Music Emotion Predictions to Sound Sources and Intuitive Perceptual Qualities. (arXiv:2106.07787v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.07787</id>
        <link href="http://arxiv.org/abs/2106.07787"/>
        <updated>2021-06-16T01:21:09.912Z</updated>
        <summary type="html"><![CDATA[Music emotion recognition is an important task in MIR (Music Information
Retrieval) research. Owing to factors like the subjective nature of the task
and the variation of emotional cues between musical genres, there are still
significant challenges in developing reliable and generalizable models. One
important step towards better models would be to understand what a model is
actually learning from the data and how the prediction for a particular input
is made. In previous work, we have shown how to derive explanations of model
predictions in terms of spectrogram image segments that connect to the
high-level emotion prediction via a layer of easily interpretable perceptual
features. However, that scheme lacks intuitive musical comprehensibility at the
spectrogram level. In the present work, we bridge this gap by merging audioLIME
-- a source-separation based explainer -- with mid-level perceptual features,
thus forming an intuitive connection chain between the input audio and the
output emotion predictions. We demonstrate the usefulness of this method by
applying it to debug a biased emotion prediction model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Shreyan Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Praher_V/0/1/0/all/0/1"&gt;Verena Praher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1"&gt;Gerhard Widmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Continuized View on Nesterov Acceleration for Stochastic Gradient Descent and Randomized Gossip. (arXiv:2106.07644v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.07644</id>
        <link href="http://arxiv.org/abs/2106.07644"/>
        <updated>2021-06-16T01:21:09.905Z</updated>
        <summary type="html"><![CDATA[We introduce the continuized Nesterov acceleration, a close variant of
Nesterov acceleration whose variables are indexed by a continuous time
parameter. The two variables continuously mix following a linear ordinary
differential equation and take gradient steps at random times. This continuized
variant benefits from the best of the continuous and the discrete frameworks:
as a continuous process, one can use differential calculus to analyze
convergence and obtain analytical expressions for the parameters; and a
discretization of the continuized process can be computed exactly with
convergence rates similar to those of Nesterov original acceleration. We show
that the discretization has the same structure as Nesterov acceleration, but
with random parameters. We provide continuized Nesterov acceleration under
deterministic as well as stochastic gradients, with either additive or
multiplicative noise. Finally, using our continuized framework and expressing
the gossip averaging problem as the stochastic minimization of a certain energy
function, we provide the first rigorous acceleration of asynchronous gossip
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Even_M/0/1/0/all/0/1"&gt;Mathieu Even&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Berthier_R/0/1/0/all/0/1"&gt;Rapha&amp;#xeb;l Berthier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Flammarion_N/0/1/0/all/0/1"&gt;Nicolas Flammarion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gaillard_P/0/1/0/all/0/1"&gt;Pierre Gaillard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hendrikx_H/0/1/0/all/0/1"&gt;Hadrien Hendrikx&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Massoulie_L/0/1/0/all/0/1"&gt;Laurent Massouli&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Taylor_A/0/1/0/all/0/1"&gt;Adrien Taylor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented Tensor Decomposition with Stochastic Optimization. (arXiv:2106.07900v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2106.07900</id>
        <link href="http://arxiv.org/abs/2106.07900"/>
        <updated>2021-06-16T01:21:09.888Z</updated>
        <summary type="html"><![CDATA[Tensor decompositions are powerful tools for dimensionality reduction and
feature interpretation of multidimensional data such as signals. Existing
tensor decomposition objectives (e.g., Frobenius norm) are designed for fitting
raw data under statistical assumptions, which may not align with downstream
classification tasks. Also, real-world tensor data are usually high-ordered and
have large dimensions with millions or billions of entries. Thus, it is
expensive to decompose the whole tensor with traditional algorithms. In
practice, raw tensor data also contains redundant information while data
augmentation techniques may be used to smooth out noise in samples. This paper
addresses the above challenges by proposing augmented tensor decomposition
(ATD), which effectively incorporates data augmentations to boost downstream
classification. To reduce the memory footprint of the decomposition, we propose
a stochastic algorithm that updates the factor matrices in a batch fashion. We
evaluate ATD on multiple signal datasets. It shows comparable or better
performance (e.g., up to 15% in accuracy) over self-supervised and autoencoder
baselines with less than 5% of model parameters, achieves 0.6% ~ 1.3% accuracy
gain over other tensor-based baselines, and reduces the memory footprint by 9X
when compared to standard tensor decomposition algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chaoqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Qian_C/0/1/0/all/0/1"&gt;Cheng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Singh_N/0/1/0/all/0/1"&gt;Navjot Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Cao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Westover_M/0/1/0/all/0/1"&gt;M Brandon Westover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Solomonik_E/0/1/0/all/0/1"&gt;Edgar Solomonik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jimeng Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reverse Engineering of Generative Models: Inferring Model Hyperparameters from Generated Images. (arXiv:2106.07873v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07873</id>
        <link href="http://arxiv.org/abs/2106.07873"/>
        <updated>2021-06-16T01:21:09.877Z</updated>
        <summary type="html"><![CDATA[State-of-the-art (SOTA) Generative Models (GMs) can synthesize
photo-realistic images that are hard for humans to distinguish from genuine
photos. We propose to perform reverse engineering of GMs to infer the model
hyperparameters from the images generated by these models. We define a novel
problem, "model parsing", as estimating GM network architectures and training
loss functions by examining their generated images -- a task seemingly
impossible for human beings. To tackle this problem, we propose a framework
with two components: a Fingerprint Estimation Network (FEN), which estimates a
GM fingerprint from a generated image by training with four constraints to
encourage the fingerprint to have desired properties, and a Parsing Network
(PN), which predicts network architecture and loss functions from the estimated
fingerprints. To evaluate our approach, we collect a fake image dataset with
$100$K images generated by $100$ GMs. Extensive experiments show encouraging
results in parsing the hyperparameters of the unseen models. Finally, our
fingerprint estimation can be leveraged for deepfake detection and image
attribution, as we show by reporting SOTA results on both the recent Celeb-DF
and image attribution benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asnani_V/0/1/0/all/0/1"&gt;Vishal Asnani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xi Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1"&gt;Tal Hassner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoDERT: Distilling Encoder Representations with Co-learning for Transducer-based Speech Recognition. (arXiv:2106.07734v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07734</id>
        <link href="http://arxiv.org/abs/2106.07734"/>
        <updated>2021-06-16T01:21:09.858Z</updated>
        <summary type="html"><![CDATA[We propose a simple yet effective method to compress an RNN-Transducer
(RNN-T) through the well-known knowledge distillation paradigm. We show that
the transducer's encoder outputs naturally have a high entropy and contain rich
information about acoustically similar word-piece confusions. This rich
information is suppressed when combined with the lower entropy decoder outputs
to produce the joint network logits. Consequently, we introduce an auxiliary
loss to distill the encoder logits from a teacher transducer's encoder, and
explore training strategies where this encoder distillation works effectively.
We find that tandem training of teacher and student encoders with an inplace
encoder distillation outperforms the use of a pre-trained and static teacher
transducer. We also report an interesting phenomenon we refer to as implicit
distillation, that occurs when the teacher and student encoders share the same
decoder. Our experiments show 5.37-8.4% relative word error rate reductions
(WERR) on in-house test sets, and 5.05-6.18% relative WERRs on LibriSpeech test
sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Swaminathan_R/0/1/0/all/0/1"&gt;Rupak Vignesh Swaminathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_B/0/1/0/all/0/1"&gt;Brian King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1"&gt;Grant P. Strimel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1"&gt;Jasha Droppo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1"&gt;Athanasios Mouchtaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Revenue-Maximizing Auctions With Differentiable Matching. (arXiv:2106.07877v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.07877</id>
        <link href="http://arxiv.org/abs/2106.07877"/>
        <updated>2021-06-16T01:21:09.842Z</updated>
        <summary type="html"><![CDATA[We propose a new architecture to approximately learn incentive compatible,
revenue-maximizing auctions from sampled valuations. Our architecture uses the
Sinkhorn algorithm to perform a differentiable bipartite matching which allows
the network to learn strategyproof revenue-maximizing mechanisms in settings
not learnable by the previous RegretNet architecture. In particular, our
architecture is able to learn mechanisms in settings without free disposal
where each bidder must be allocated exactly some number of items. In
experiments, we show our approach successfully recovers multiple known optimal
mechanisms and high-revenue, low-regret mechanisms in larger settings where the
optimal mechanism is unknown.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Curry_M/0/1/0/all/0/1"&gt;Michael J. Curry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyi_U/0/1/0/all/0/1"&gt;Uro Lyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1"&gt;John Dickerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CathAI: Fully Automated Interpretation of Coronary Angiograms Using Neural Networks. (arXiv:2106.07708v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07708</id>
        <link href="http://arxiv.org/abs/2106.07708"/>
        <updated>2021-06-16T01:21:09.834Z</updated>
        <summary type="html"><![CDATA[Coronary heart disease (CHD) is the leading cause of adult death in the
United States and worldwide, and for which the coronary angiography procedure
is the primary gateway for diagnosis and clinical management decisions. The
standard-of-care for interpretation of coronary angiograms depends upon ad-hoc
visual assessment by the physician operator. However, ad-hoc visual
interpretation of angiograms is poorly reproducible, highly variable and bias
prone. Here we show for the first time that fully-automated angiogram
interpretation to estimate coronary artery stenosis is possible using a
sequence of deep neural network algorithms. The algorithmic pipeline we
developed--called CathAI--achieves state-of-the art performance across the
sequence of tasks required to accomplish automated interpretation of
unselected, real-world angiograms. CathAI (Algorithms 1-2) demonstrated
positive predictive value, sensitivity and F1 score of >=90% to identify the
projection angle overall and >=93% for left or right coronary artery angiogram
detection, the primary anatomic structures of interest. To predict obstructive
coronary artery stenosis (>=70% stenosis), CathAI (Algorithm 4) exhibited an
area under the receiver operating characteristic curve (AUC) of 0.862 (95% CI:
0.843-0.880). When externally validated in a healthcare system in another
country, CathAI AUC was 0.869 (95% CI: 0.830-0.907) to predict obstructive
coronary artery stenosis. Our results demonstrate that multiple purpose-built
neural networks can function in sequence to accomplish the complex series of
tasks required for automated analysis of real-world angiograms. Deployment of
CathAI may serve to increase standardization and reproducibility in coronary
stenosis assessment, while providing a robust foundation to accomplish future
tasks for algorithmic angiographic interpretation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avram_R/0/1/0/all/0/1"&gt;Robert Avram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olgin_J/0/1/0/all/0/1"&gt;Jeffrey E. Olgin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_A/0/1/0/all/0/1"&gt;Alvin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1"&gt;Zeeshan Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verreault_Julien_L/0/1/0/all/0/1"&gt;Louis Verreault-Julien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abreau_S/0/1/0/all/0/1"&gt;Sean Abreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1"&gt;Derek Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1"&gt;Derek Y. So&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soni_K/0/1/0/all/0/1"&gt;Krishan Soni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tison_G/0/1/0/all/0/1"&gt;Geoffrey H. Tison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GeoMol: Torsional Geometric Generation of Molecular 3D Conformer Ensembles. (arXiv:2106.07802v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.07802</id>
        <link href="http://arxiv.org/abs/2106.07802"/>
        <updated>2021-06-16T01:21:09.826Z</updated>
        <summary type="html"><![CDATA[Prediction of a molecule's 3D conformer ensemble from the molecular graph
holds a key role in areas of cheminformatics and drug discovery. Existing
generative models have several drawbacks including lack of modeling important
molecular geometry elements (e.g. torsion angles), separate optimization stages
prone to error accumulation, and the need for structure fine-tuning based on
approximate classical force-fields or computationally expensive methods such as
metadynamics with approximate quantum mechanics calculations at each geometry.
We propose GeoMol--an end-to-end, non-autoregressive and SE(3)-invariant
machine learning approach to generate distributions of low-energy molecular 3D
conformers. Leveraging the power of message passing neural networks (MPNNs) to
capture local and global graph information, we predict local atomic 3D
structures and torsion angles, avoiding unnecessary over-parameterization of
the geometric degrees of freedom (e.g. one angle per non-terminal bond). Such
local predictions suffice both for the training loss computation, as well as
for the full deterministic conformer assembly (at test time). We devise a
non-adversarial optimal transport based loss function to promote diverse
conformer generation. GeoMol predominantly outperforms popular open-source,
commercial, or state-of-the-art machine learning (ML) models, while achieving
significant speed-ups. We expect such differentiable 3D structure generators to
significantly impact molecular modeling and related applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Ganea_O/0/1/0/all/0/1"&gt;Octavian-Eugen Ganea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pattanaik_L/0/1/0/all/0/1"&gt;Lagnajit Pattanaik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Coley_C/0/1/0/all/0/1"&gt;Connor W. Coley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Jensen_K/0/1/0/all/0/1"&gt;Klavs F. Jensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Green_W/0/1/0/all/0/1"&gt;William H. Green&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Jaakkola_T/0/1/0/all/0/1"&gt;Tommi S. Jaakkola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Credit Assignment in Neural Networks through Deep Feedback Control. (arXiv:2106.07887v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07887</id>
        <link href="http://arxiv.org/abs/2106.07887"/>
        <updated>2021-06-16T01:21:09.818Z</updated>
        <summary type="html"><![CDATA[The success of deep learning sparked interest in whether the brain learns by
using similar techniques for assigning credit to each synaptic weight for its
contribution to the network output. However, the majority of current attempts
at biologically-plausible learning methods are either non-local in time,
require highly specific connectivity motives, or have no clear link to any
known mathematical optimization method. Here, we introduce Deep Feedback
Control (DFC), a new learning method that uses a feedback controller to drive a
deep neural network to match a desired output target and whose control signal
can be used for credit assignment. The resulting learning rule is fully local
in space and time and approximates Gauss-Newton optimization for a wide range
of feedback connectivity patterns. To further underline its biological
plausibility, we relate DFC to a multi-compartment model of cortical pyramidal
neurons with a local voltage-dependent synaptic plasticity rule, consistent
with recent theories of dendritic processing. By combining dynamical system
theory with mathematical optimization theory, we provide a strong theoretical
foundation for DFC that we corroborate with detailed results on toy experiments
and standard computer-vision benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meulemans_A/0/1/0/all/0/1"&gt;Alexander Meulemans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farinha_M/0/1/0/all/0/1"&gt;Matilde Tristany Farinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ordonez_J/0/1/0/all/0/1"&gt;Javier Garc&amp;#xed;a Ord&amp;#xf3;&amp;#xf1;ez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aceituno_P/0/1/0/all/0/1"&gt;Pau Vilimelis Aceituno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sacramento_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Sacramento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grewe_B/0/1/0/all/0/1"&gt;Benjamin F. Grewe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers. (arXiv:2106.07798v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07798</id>
        <link href="http://arxiv.org/abs/2106.07798"/>
        <updated>2021-06-16T01:21:09.796Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a new data poisoning attack and apply it to deep
reinforcement learning agents. Our attack centers on what we call
in-distribution triggers, which are triggers native to the data distributions
the model will be trained on and deployed in. We outline a simple procedure for
embedding these, and other, triggers in deep reinforcement learning agents
following a multi-task learning paradigm, and demonstrate in three common
reinforcement learning environments. We believe that this work has important
implications for the security of deep learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashcraft_C/0/1/0/all/0/1"&gt;Chace Ashcraft&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karra_K/0/1/0/all/0/1"&gt;Kiran Karra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Randomized Exploration for Reinforcement Learning with General Value Function Approximation. (arXiv:2106.07841v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07841</id>
        <link href="http://arxiv.org/abs/2106.07841"/>
        <updated>2021-06-16T01:21:09.777Z</updated>
        <summary type="html"><![CDATA[We propose a model-free reinforcement learning algorithm inspired by the
popular randomized least squares value iteration (RLSVI) algorithm as well as
the optimism principle. Unlike existing upper-confidence-bound (UCB) based
approaches, which are often computationally intractable, our algorithm drives
exploration by simply perturbing the training data with judiciously chosen
i.i.d. scalar noises. To attain optimistic value function estimation without
resorting to a UCB-style bonus, we introduce an optimistic reward sampling
procedure. When the value functions can be represented by a function class
$\mathcal{F}$, our algorithm achieves a worst-case regret bound of
$\widetilde{O}(\mathrm{poly}(d_EH)\sqrt{T})$ where $T$ is the time elapsed, $H$
is the planning horizon and $d_E$ is the $\textit{eluder dimension}$ of
$\mathcal{F}$. In the linear setting, our algorithm reduces to LSVI-PHE, a
variant of RLSVI, that enjoys an $\widetilde{\mathcal{O}}(\sqrt{d^3H^3T})$
regret. We complement the theory with an empirical evaluation across known
difficult exploration tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ishfaq_H/0/1/0/all/0/1"&gt;Haque Ishfaq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1"&gt;Qiwen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Viet Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayoub_A/0/1/0/all/0/1"&gt;Alex Ayoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lin F. Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RETRIEVE: Coreset Selection for Efficient and Robust Semi-Supervised Learning. (arXiv:2106.07760v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07760</id>
        <link href="http://arxiv.org/abs/2106.07760"/>
        <updated>2021-06-16T01:21:09.767Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning (SSL) algorithms have had great success in recent
years in limited labeled data regimes. However, the current state-of-the-art
SSL algorithms are computationally expensive and entail significant compute
time and energy requirements. This can prove to be a huge limitation for many
smaller companies and academic groups. Our main insight is that training on a
subset of unlabeled data instead of entire unlabeled data enables the current
SSL algorithms to converge faster, thereby reducing the computational costs
significantly. In this work, we propose RETRIEVE, a coreset selection framework
for efficient and robust semi-supervised learning. RETRIEVE selects the coreset
by solving a mixed discrete-continuous bi-level optimization problem such that
the selected coreset minimizes the labeled set loss. We use a one-step gradient
approximation and show that the discrete optimization problem is approximately
submodular, thereby enabling simple greedy algorithms to obtain the coreset. We
empirically demonstrate on several real-world datasets that existing SSL
algorithms like VAT, Mean-Teacher, FixMatch, when used with RETRIEVE, achieve
a) faster training times, b) better performance when unlabeled data consists of
Out-of-Distribution(OOD) data and imbalance. More specifically, we show that
with minimal accuracy degradation, RETRIEVE achieves a speedup of around 3X in
the traditional SSL setting and achieves a speedup of 5X compared to
state-of-the-art (SOTA) robust SSL algorithms in the case of imbalance and OOD
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1"&gt;Krishnateja Killamsetty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xujiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Feng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embarrassingly parallel MCMC using deep invertible transformations. (arXiv:1903.04556v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.04556</id>
        <link href="http://arxiv.org/abs/1903.04556"/>
        <updated>2021-06-16T01:21:09.684Z</updated>
        <summary type="html"><![CDATA[While MCMC methods have become a main work-horse for Bayesian inference,
scaling them to large distributed datasets is still a challenge. Embarrassingly
parallel MCMC strategies take a divide-and-conquer stance to achieve this by
writing the target posterior as a product of subposteriors, running MCMC for
each of them in parallel and subsequently combining the results. The challenge
then lies in devising efficient aggregation strategies. Current strategies
trade-off between approximation quality, and costs of communication and
computation. In this work, we introduce a novel method that addresses these
issues simultaneously. Our key insight is to introduce a deep invertible
transformation to approximate each of the subposteriors. These approximations
can be made accurate even for complex distributions and serve as intermediate
representations, keeping the total communication cost limited. Moreover, they
enable us to sample from the product of the subposteriors using an efficient
and stable importance sampling scheme. We demonstrate the approach outperforms
available state-of-the-art methods in a range of challenging scenarios,
including high-dimensional and heterogeneous subposteriors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mesquita_D/0/1/0/all/0/1"&gt;Diego Mesquita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blomstedt_P/0/1/0/all/0/1"&gt;Paul Blomstedt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Flip Side of the Reweighted Coin: Duality of Adaptive Dropout and Regularization. (arXiv:2106.07769v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07769</id>
        <link href="http://arxiv.org/abs/2106.07769"/>
        <updated>2021-06-16T01:21:09.666Z</updated>
        <summary type="html"><![CDATA[Among the most successful methods for sparsifying deep (neural) networks are
those that adaptively mask the network weights throughout training. By
examining this masking, or dropout, in the linear case, we uncover a duality
between such adaptive methods and regularization through the so-called
"$\eta$-trick" that casts both as iteratively reweighted optimizations. We show
that any dropout strategy that adapts to the weights in a monotonic way
corresponds to an effective subquadratic regularization penalty, and therefore
leads to sparse solutions. We obtain the effective penalties for several
popular sparsification strategies, which are remarkably similar to classical
penalties commonly used in sparse optimization. Considering variational dropout
as a case study, we demonstrate similar empirical behavior between the adaptive
dropout method and classical methods on the task of deep network
sparsification, validating our theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+LeJeune_D/0/1/0/all/0/1"&gt;Daniel LeJeune&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javadi_H/0/1/0/all/0/1"&gt;Hamid Javadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1"&gt;Richard G. Baraniuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Q-Rank: New Data Dependent Definition of Tensor Rank. (arXiv:1910.12016v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.12016</id>
        <link href="http://arxiv.org/abs/1910.12016"/>
        <updated>2021-06-16T01:21:09.654Z</updated>
        <summary type="html"><![CDATA[Recently, the \textit{Tensor Nuclear Norm~(TNN)} regularization based on
t-SVD has been widely used in various low tubal-rank tensor recovery tasks.
However, these models usually require smooth change of data along the third
dimension to ensure their low rank structures. In this paper, we propose a new
definition of data dependent tensor rank named \textit{tensor Q-rank} by a
learnable orthogonal matrix $\mathbf{Q}$, and further introduce a unified data
dependent low rank tensor recovery model. According to the low rank hypothesis,
we introduce two explainable selection method of $\mathbf{Q}$, under which the
data tensor may have a more significant low tensor Q-rank structure than that
of low tubal-rank structure. Specifically, maximizing the variance of singular
value distribution leads to Variance Maximization Tensor Q-Nuclear
norm~(VMTQN), while minimizing the value of nuclear norm through manifold
optimization leads to Manifold Optimization Tensor Q-Nuclear norm~(MOTQN).
Moreover, we apply these two models to the low rank tensor completion problem,
and then give an effective algorithm and briefly analyze why our method works
better than TNN based methods in the case of complex data with low sampling
rate. Finally, experimental results on real-world datasets demonstrate the
superiority of our proposed model in the tensor completion problem with respect
to other tensor rank regularization models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1"&gt;Hao Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Canyi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Divergence Frontiers for Generative Models: Sample Complexity, Quantization Level, and Frontier Integral. (arXiv:2106.07898v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.07898</id>
        <link href="http://arxiv.org/abs/2106.07898"/>
        <updated>2021-06-16T01:21:09.647Z</updated>
        <summary type="html"><![CDATA[The spectacular success of deep generative models calls for quantitative
tools to measure their statistical performance. Divergence frontiers have
recently been proposed as an evaluation framework for generative models, due to
their ability to measure the quality-diversity trade-off inherent to deep
generative modeling. However, the statistical behavior of divergence frontiers
estimated from data remains unknown to this day. In this paper, we establish
non-asymptotic bounds on the sample complexity of the plug-in estimator of
divergence frontiers. Along the way, we introduce a novel integral summary of
divergence frontiers. We derive the corresponding non-asymptotic bounds and
discuss the choice of the quantization level by balancing the two types of
approximation errors arisen from its computation. We also augment the
divergence frontier framework by investigating the statistical performance of
smoothed distribution estimators such as the Good-Turing estimator. We
illustrate the theoretical results with numerical examples from natural
language processing and computer vision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pillutla_K/0/1/0/all/0/1"&gt;Krishna Pillutla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Welleck_S/0/1/0/all/0/1"&gt;Sean Welleck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Oh_S/0/1/0/all/0/1"&gt;Sewoong Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1"&gt;Zaid Harchaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning. (arXiv:1910.10897v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.10897</id>
        <link href="http://arxiv.org/abs/1910.10897"/>
        <updated>2021-06-16T01:21:09.640Z</updated>
        <summary type="html"><![CDATA[Meta-reinforcement learning algorithms can enable robots to acquire new
skills much more quickly, by leveraging prior experience to learn how to learn.
However, much of the current research on meta-reinforcement learning focuses on
task distributions that are very narrow. For example, a commonly used
meta-reinforcement learning benchmark uses different running velocities for a
simulated robot as different tasks. When policies are meta-trained on such
narrow task distributions, they cannot possibly generalize to more quickly
acquire entirely new tasks. Therefore, if the aim of these methods is to enable
faster acquisition of entirely new behaviors, we must evaluate them on task
distributions that are sufficiently broad to enable generalization to new
behaviors. In this paper, we propose an open-source simulated benchmark for
meta-reinforcement learning and multi-task learning consisting of 50 distinct
robotic manipulation tasks. Our aim is to make it possible to develop
algorithms that generalize to accelerate the acquisition of entirely new,
held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and
multi-task learning algorithms on these tasks. Surprisingly, while each task
and its variations (e.g., with different object positions) can be learned with
reasonable success, these algorithms struggle to learn with multiple tasks at
the same time, even with as few as ten distinct training tasks. Our analysis
and open-source environments pave the way for future research in multi-task
learning and meta-learning that can enable meaningful generalization, thereby
unlocking the full potential of these methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tianhe Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quillen_D/0/1/0/all/0/1"&gt;Deirdre Quillen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhanpeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1"&gt;Ryan Julian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1"&gt;Avnish Narayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shively_H/0/1/0/all/0/1"&gt;Hayden Shively&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellathur_A/0/1/0/all/0/1"&gt;Adithya Bellathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1"&gt;Karol Hausman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Deep Morphological Networks with Neural Architecture Search. (arXiv:2106.07714v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07714</id>
        <link href="http://arxiv.org/abs/2106.07714"/>
        <updated>2021-06-16T01:21:09.633Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) are generated by sequentially performing linear
and non-linear processes. Using a combination of linear and non-linear
procedures is critical for generating a sufficiently deep feature space. The
majority of non-linear operators are derivations of activation functions or
pooling functions. Mathematical morphology is a branch of mathematics that
provides non-linear operators for a variety of image processing problems. We
investigate the utility of integrating these operations in an end-to-end deep
learning framework in this paper. DNNs are designed to acquire a realistic
representation for a particular job. Morphological operators give topological
descriptors that convey salient information about the shapes of objects
depicted in images. We propose a method based on meta-learning to incorporate
morphological operators into DNNs. The learned architecture demonstrates how
our novel morphological operations significantly increase DNN performance on
various tasks, including picture classification and edge detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yufei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belkhir_N/0/1/0/all/0/1"&gt;Nacim Belkhir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angulo_J/0/1/0/all/0/1"&gt;Jesus Angulo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1"&gt;Angela Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1"&gt;Gianni Franchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Equivariant Energy Based Models with Equivariant Stein Variational Gradient Descent. (arXiv:2106.07832v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07832</id>
        <link href="http://arxiv.org/abs/2106.07832"/>
        <updated>2021-06-16T01:21:09.623Z</updated>
        <summary type="html"><![CDATA[We focus on the problem of efficient sampling and learning of probability
densities by incorporating symmetries in probabilistic models. We first
introduce Equivariant Stein Variational Gradient Descent algorithm -- an
equivariant sampling method based on Stein's identity for sampling from
densities with symmetries. Equivariant SVGD explicitly incorporates symmetry
information in a density through equivariant kernels which makes the resultant
sampler efficient both in terms of sample complexity and the quality of
generated samples. Subsequently, we define equivariant energy based models to
model invariant densities that are learned using contrastive divergence. By
utilizing our equivariant SVGD for training equivariant EBMs, we propose new
ways of improving and scaling up training of energy based models. We apply
these equivariant energy models for modelling joint densities in regression and
classification tasks for image datasets, many-body particle systems and
molecular structure generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaini_P/0/1/0/all/0/1"&gt;Priyank Jaini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holdijk_L/0/1/0/all/0/1"&gt;Lars Holdijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A White Paper on Neural Network Quantization. (arXiv:2106.08295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08295</id>
        <link href="http://arxiv.org/abs/2106.08295"/>
        <updated>2021-06-16T01:21:09.583Z</updated>
        <summary type="html"><![CDATA[While neural networks have advanced the frontiers in many applications, they
often come at a high computational cost. Reducing the power and latency of
neural network inference is key if we want to integrate modern networks into
edge devices with strict power and compute requirements. Neural network
quantization is one of the most effective ways of achieving these savings but
the additional noise it induces can lead to accuracy degradation. In this white
paper, we introduce state-of-the-art algorithms for mitigating the impact of
quantization noise on the network's performance while maintaining low-bit
weights and activations. We start with a hardware motivated introduction to
quantization and then consider two main classes of algorithms: Post-Training
Quantization (PTQ) and Quantization-Aware-Training (QAT). PTQ requires no
re-training or labelled data and is thus a lightweight push-button approach to
quantization. In most cases, PTQ is sufficient for achieving 8-bit quantization
with close to floating-point accuracy. QAT requires fine-tuning and access to
labeled training data but enables lower bit quantization with competitive
results. For both solutions, we provide tested pipelines based on existing
literature and extensive experimentation that lead to state-of-the-art
performance for common deep learning models and tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagel_M/0/1/0/all/0/1"&gt;Markus Nagel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fournarakis_M/0/1/0/all/0/1"&gt;Marios Fournarakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amjad_R/0/1/0/all/0/1"&gt;Rana Ali Amjad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bondarenko_Y/0/1/0/all/0/1"&gt;Yelysei Bondarenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baalen_M/0/1/0/all/0/1"&gt;Mart van Baalen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1"&gt;Tijmen Blankevoort&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.07806</id>
        <link href="http://arxiv.org/abs/2106.07806"/>
        <updated>2021-06-16T01:21:09.475Z</updated>
        <summary type="html"><![CDATA[Machine learning is revolutionizing image-based diagnostics in pathology and
radiology. ML models have shown promising results in research settings, but
their lack of interoperability has been a major barrier for clinical
integration and evaluation. The DICOM a standard specifies Information Object
Definitions and Services for the representation and communication of digital
images and related information, including image-derived annotations and
analysis results. However, the complexity of the standard represents an
obstacle for its adoption in the ML community and creates a need for software
libraries and tools that simplify working with data sets in DICOM format. Here
we present the highdicom library, which provides a high-level application
programming interface for the Python programming language that abstracts
low-level details of the standard and enables encoding and decoding of
image-derived information in DICOM format in a few lines of Python code. The
highdicom library ties into the extensive Python ecosystem for image processing
and machine learning. Simultaneously, by simplifying creation and parsing of
DICOM-compliant files, highdicom achieves interoperability with the medical
imaging systems that hold the data used to train and run ML models, and
ultimately communicate and store model outputs for clinical use. We demonstrate
through experiments with slide microscopy and computed tomography imaging,
that, by bridging these two ecosystems, highdicom enables developers to train
and evaluate state-of-the-art ML models in pathology and radiology while
remaining compliant with the DICOM standard and interoperable with clinical
systems at all stages. To promote standardization of ML research and streamline
the ML model development and deployment process, we made the library available
free and open-source.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bridge_C/0/1/0/all/0/1"&gt;Christopher P. Bridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gorman_C/0/1/0/all/0/1"&gt;Chris Gorman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pieper_S/0/1/0/all/0/1"&gt;Steven Pieper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Doyle_S/0/1/0/all/0/1"&gt;Sean W. Doyle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lennerz_J/0/1/0/all/0/1"&gt;Jochen K. Lennerz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1"&gt;Jayashree Kalpathy-Cramer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Clunie_D/0/1/0/all/0/1"&gt;David A. Clunie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fedorov_A/0/1/0/all/0/1"&gt;Andriy Y. Fedorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Herrmann_M/0/1/0/all/0/1"&gt;Markus D. Herrmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Site-Agnostic 3D Dose Distribution Prediction with Deep Learning Neural Networks. (arXiv:2106.07825v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07825</id>
        <link href="http://arxiv.org/abs/2106.07825"/>
        <updated>2021-06-16T01:21:09.468Z</updated>
        <summary type="html"><![CDATA[Typically, the current dose prediction models are limited to small amounts of
data and require re-training for a specific site, often leading to suboptimal
performance. We propose a site-agnostic, 3D dose distribution prediction model
using deep learning that can leverage data from any treatment site, thus
increasing the total data available to train the model. Applying our proposed
model to a new target treatment site requires only a brief fine-tuning of the
model to the new data and involves no modifications to the model input channels
or its parameters. Thus, it can be efficiently adapted to a different treatment
site, even with a small training dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mashayekhi_M/0/1/0/all/0/1"&gt;Maryam Mashayekhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tapia_I/0/1/0/all/0/1"&gt;Itzel Ramirez Tapia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balagopal_A/0/1/0/all/0/1"&gt;Anjali Balagopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1"&gt;Xinran Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barkousaraie_A/0/1/0/all/0/1"&gt;Azar Sadeghnejad Barkousaraie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McBeth_R/0/1/0/all/0/1"&gt;Rafe McBeth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mu-Han Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Steve Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dan Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HUMAP: Hierarchical Uniform Manifold Approximation and Projection. (arXiv:2106.07718v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07718</id>
        <link href="http://arxiv.org/abs/2106.07718"/>
        <updated>2021-06-16T01:21:09.401Z</updated>
        <summary type="html"><![CDATA[Dimensionality reduction (DR) techniques help analysts to understand patterns
in high-dimensional spaces. These techniques, often represented by scatter
plots, are employed in diverse science domains and facilitate similarity
analysis among clusters and data samples. For datasets containing many
granularities or when analysis follows the information visualization mantra,
hierarchical DR techniques are the most suitable approach since they present
major structures beforehand and details on demand. However, current
hierarchical DR techniques are not fully capable of addressing literature
problems because they do not preserve the projection mental map across
hierarchical levels or are not suitable for most data types. This work presents
HUMAP, a novel hierarchical dimensionality reduction technique designed to be
flexible on preserving local and global structures and preserve the mental map
throughout hierarchical exploration. We provide empirical evidence of our
technique's superiority compared with current hierarchical approaches and show
two case studies to demonstrate its strengths.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+E%2E_W/0/1/0/all/0/1"&gt;Wilson E. Marc&amp;#xed;lio-Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eler_D/0/1/0/all/0/1"&gt;Danilo M. Eler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paulovich_F/0/1/0/all/0/1"&gt;Fernando V. Paulovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_R/0/1/0/all/0/1"&gt;Rafael M. Martins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to find a unicorn: a novel model-free, unsupervised anomaly detection method for time series. (arXiv:2004.11468v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.11468</id>
        <link href="http://arxiv.org/abs/2004.11468"/>
        <updated>2021-06-16T01:21:09.222Z</updated>
        <summary type="html"><![CDATA[Recognition of anomalous events is a challenging but critical task in many
scientific and industrial fields, especially when the properties of anomalies
are unknown. In this paper, we introduce a new anomaly concept called "unicorn"
or unique event and present a new, model-free, unsupervised detection algorithm
to detect unicorns. The key component of the new algorithm is the Temporal
Outlier Factor (TOF) to measure the uniqueness of events in continuous data
sets from dynamic systems. The concept of unique events differs significantly
from traditional outliers in many aspects: while repetitive outliers are no
longer unique events, a unique event is not necessarily an outlier; it does not
necessarily fall out from the distribution of normal activity. The performance
of our algorithm was examined in recognizing unique events on different types
of simulated data sets with anomalies and it was compared with the Local
Outlier Factor (LOF) and discord discovery algorithms. TOF had superior
performance compared to LOF and discord algorithms even in recognizing
traditional outliers and it also recognized unique events that those did not.
The benefits of the unicorn concept and the new detection method were
illustrated by example data sets from very different scientific fields. Our
algorithm successfully recognized unique events in those cases where they were
already known such as the gravitational waves of a binary black hole merger on
LIGO detector data and the signs of respiratory failure on ECG data series.
Furthermore, unique events were found on the LIBOR data set of the last 30
years.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benko_Z/0/1/0/all/0/1"&gt;Zsigmond Benk&amp;#x151;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babel_T/0/1/0/all/0/1"&gt;Tam&amp;#xe1;s B&amp;#xe1;bel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Somogyvari_Z/0/1/0/all/0/1"&gt;Zolt&amp;#xe1;n Somogyv&amp;#xe1;ri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spot the Difference: Topological Anomaly Detection via Geometric Alignment. (arXiv:2106.08233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08233</id>
        <link href="http://arxiv.org/abs/2106.08233"/>
        <updated>2021-06-16T01:21:09.202Z</updated>
        <summary type="html"><![CDATA[Geometric alignment appears in a variety of applications, ranging from domain
adaptation, optimal transport, and normalizing flows in machine learning;
optical flow and learned augmentation in computer vision and deformable
registration within biomedical imaging. A recurring challenge is the alignment
of domains whose topology is not the same; a problem that is routinely ignored,
potentially introducing bias in downstream analysis. As a first step towards
solving such alignment problems, we propose an unsupervised topological
difference detection algorithm. The model is based on a conditional variational
auto-encoder and detects topological anomalies with regards to a reference
alongside the registration step. We consider both a) topological changes in the
image under spatial variation and b) unexpected transformations. Our approach
is validated on a proxy task of unsupervised anomaly detection in images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Czolbe_S/0/1/0/all/0/1"&gt;Steffen Czolbe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1"&gt;Aasa Feragen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1"&gt;Oswin Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08285</id>
        <link href="http://arxiv.org/abs/2106.08285"/>
        <updated>2021-06-16T01:21:09.193Z</updated>
        <summary type="html"><![CDATA[Time-lapse fluorescent microscopy (TLFM) combined with predictive
mathematical modelling is a powerful tool to study the inherently dynamic
processes of life on the single-cell level. Such experiments are costly,
complex and labour intensive. A complimentary approach and a step towards
completely in silico experiments, is to synthesise the imagery itself. Here, we
propose Multi-StyleGAN as a descriptive approach to simulate time-lapse
fluorescence microscopy imagery of living cells, based on a past experiment.
This novel generative adversarial network synthesises a multi-domain sequence
of consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple
live yeast cells in microstructured environments and train on a dataset
recorded in our laboratory. The simulation captures underlying biophysical
factors and time dependencies, such as cell morphology, growth, physical
interactions, as well as the intensity of a fluorescent reporter protein. An
immediate application is to generate additional training and validation data
for feature extraction algorithms or to aid and expedite development of
advanced experimental techniques such as online monitoring or control of cells.

Code and dataset is available at
https://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1"&gt;Tim Prangemeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1"&gt;Christoph Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1"&gt;Christian Wildner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1"&gt;Heinz Koeppl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Data Augmentation samples for Semantic Segmentation of Salt Bodies in a Synthetic Seismic Image Dataset. (arXiv:2106.08269v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08269</id>
        <link href="http://arxiv.org/abs/2106.08269"/>
        <updated>2021-06-16T01:21:09.187Z</updated>
        <summary type="html"><![CDATA[Nowadays, subsurface salt body localization and delineation, also called
semantic segmentation of salt bodies, are among the most challenging
geophysicist tasks. Thus, identifying large salt bodies is notoriously tricky
and is crucial for identifying hydrocarbon reservoirs and drill path planning.
This work proposes a Data Augmentation method based on training two generative
models to augment the number of samples in a seismic image dataset for the
semantic segmentation of salt bodies. Our method uses deep learning models to
generate pairs of seismic image patches and their respective salt masks for the
Data Augmentation. The first model is a Variational Autoencoder and is
responsible for generating patches of salt body masks. The second is a
Conditional Normalizing Flow model, which receives the generated masks as
inputs and generates the associated seismic image patches. We evaluate the
proposed method by comparing the performance of ten distinct state-of-the-art
models for semantic segmentation, trained with and without the generated
augmentations, in a dataset from two synthetic seismic images. The proposed
methodology yields an average improvement of 8.57% in the IoU metric across all
compared models. The best result is achieved by a DeeplabV3+ model variant,
which presents an IoU score of 95.17% when trained with our augmentations.
Additionally, our proposal outperformed six selected data augmentation methods,
and the most significant improvement in the comparison, of 9.77%, is achieved
by composing our DA with augmentations from an elastic transformation. At last,
we show that the proposed method is adaptable for a larger context size by
achieving results comparable to the obtained on the smaller context size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1"&gt;Luis Felipe Henriques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1"&gt;S&amp;#xe9;rgio Colcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1"&gt;Ruy Luiz Milidi&amp;#xfa;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulcao_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Bulc&amp;#xe3;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barros_P/0/1/0/all/0/1"&gt;Pablo Barros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multivariate Business Process Representation Learning utilizing Gramian Angular Fields and Convolutional Neural Networks. (arXiv:2106.08027v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08027</id>
        <link href="http://arxiv.org/abs/2106.08027"/>
        <updated>2021-06-16T01:21:09.180Z</updated>
        <summary type="html"><![CDATA[Learning meaningful representations of data is an important aspect of machine
learning and has recently been successfully applied to many domains like
language understanding or computer vision. Instead of training a model for one
specific task, representation learning is about training a model to capture all
useful information in the underlying data and make it accessible for a
predictor. For predictive process analytics, it is essential to have all
explanatory characteristics of a process instance available when making
predictions about the future, as well as for clustering and anomaly detection.
Due to the large variety of perspectives and types within business process
data, generating a good representation is a challenging task. In this paper, we
propose a novel approach for representation learning of business process
instances which can process and combine most perspectives in an event log. In
conjunction with a self-supervised pre-training method, we show the
capabilities of the approach through a visualization of the representation
space and case retrieval. Furthermore, the pre-trained model is fine-tuned to
multiple process prediction tasks and demonstrates its effectiveness in
comparison with existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_P/0/1/0/all/0/1"&gt;Peter Pfeiffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahann_J/0/1/0/all/0/1"&gt;Johannes Lahann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fettke_P/0/1/0/all/0/1"&gt;Peter Fettke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypergraph Dissimilarity Measures. (arXiv:2106.08206v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08206</id>
        <link href="http://arxiv.org/abs/2106.08206"/>
        <updated>2021-06-16T01:21:09.173Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose two novel approaches for hypergraph comparison. The
first approach transforms the hypergraph into a graph representation for use of
standard graph dissimilarity measures. The second approach exploits the
mathematics of tensors to intrinsically capture multi-way relations. For each
approach, we present measures that assess hypergraph dissimilarity at a
specific scale or provide a more holistic multi-scale comparison. We test these
measures on synthetic hypergraphs and apply them to biological datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Surana_A/0/1/0/all/0/1"&gt;Amit Surana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Can Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajapakse_I/0/1/0/all/0/1"&gt;Indika Rajapakse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EuroCrops: A Pan-European Dataset for Time Series Crop Type Classification. (arXiv:2106.08151v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08151</id>
        <link href="http://arxiv.org/abs/2106.08151"/>
        <updated>2021-06-16T01:21:09.149Z</updated>
        <summary type="html"><![CDATA[We present EuroCrops, a dataset based on self-declared field annotations for
training and evaluating methods for crop type classification and mapping,
together with its process of acquisition and harmonisation. By this, we aim to
enrich the research efforts and discussion for data-driven land cover
classification via Earth observation and remote sensing. Additionally, through
inclusion of self-declarations gathered in the scope of subsidy control from
all countries of the European Union (EU), this dataset highlights the
difficulties and pitfalls one comes across when operating on a transnational
level. We, therefore, also introduce a new taxonomy scheme, HCAT-ID, that
aspires to capture all the aspects of reference data originating from
administrative and agency databases. To address researchers from both the
remote sensing and the computer vision and machine learning communities, we
publish the dataset in different formats and processing levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schneider_M/0/1/0/all/0/1"&gt;Maja Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Broszeit_A/0/1/0/all/0/1"&gt;Amelie Broszeit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korner_M/0/1/0/all/0/1"&gt;Marco K&amp;#xf6;rner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graphical Gaussian Process Regression Model for Aqueous Solvation Free Energy Prediction of Organic Molecules in Redox Flow Battery. (arXiv:2106.08146v1 [cs.CE])]]></title>
        <id>http://arxiv.org/abs/2106.08146</id>
        <link href="http://arxiv.org/abs/2106.08146"/>
        <updated>2021-06-16T01:21:09.140Z</updated>
        <summary type="html"><![CDATA[The solvation free energy of organic molecules is a critical parameter in
determining emergent properties such as solubility, liquid-phase equilibrium
constants, and pKa and redox potentials in an organic redox flow battery. In
this work, we present a machine learning (ML) model that can learn and predict
the aqueous solvation free energy of an organic molecule using Gaussian process
regression method based on a new molecular graph kernel. To investigate the
performance of the ML model on electrostatic interaction, the nonpolar
interaction contribution of solvent and the conformational entropy of solute in
solvation free energy, three data sets with implicit or explicit water solvent
models, and contribution of conformational entropy of solute are tested. We
demonstrate that our ML model can predict the solvation free energy of
molecules at chemical accuracy with a mean absolute error of less than 1
kcal/mol for subsets of the QM9 dataset and the Freesolv database. To solve the
general data scarcity problem for a graph-based ML model, we propose a
dimension reduction algorithm based on the distance between molecular graphs,
which can be used to examine the diversity of the molecular data set. It
provides a promising way to build a minimum training set to improve prediction
for certain test sets where the space of molecular structures is predetermined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peiyuan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yu-Hang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Muqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1"&gt;Amity Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murugesan_V/0/1/0/all/0/1"&gt;Vijayakumar Murugesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hollas_A/0/1/0/all/0/1"&gt;Aaron Hollas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Now You See It, Now You Dont: Adversarial Vulnerabilities in Computational Pathology. (arXiv:2106.08153v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08153</id>
        <link href="http://arxiv.org/abs/2106.08153"/>
        <updated>2021-06-16T01:21:09.112Z</updated>
        <summary type="html"><![CDATA[Deep learning models are routinely employed in computational pathology
(CPath) for solving problems of diagnostic and prognostic significance.
Typically, the generalization performance of CPath models is analyzed using
evaluation protocols such as cross-validation and testing on multi-centric
cohorts. However, to ensure that such CPath solutions are robust and safe for
use in a clinical setting, a critical analysis of their predictive performance
and vulnerability to adversarial attacks is required, which is the focus of
this paper. Specifically, we show that a highly accurate model for
classification of tumour patches in pathology images (AUC > 0.95) can easily be
attacked with minimal perturbations which are imperceptible to lay humans and
trained pathologists alike. Our analytical results show that it is possible to
generate single-instance white-box attacks on specific input images with high
success rate and low perturbation energy. Furthermore, we have also generated a
single universal perturbation matrix using the training dataset only which,
when added to unseen test images, results in forcing the trained neural network
to flip its prediction labels with high confidence at a success rate of > 84%.
We systematically analyze the relationship between perturbation energy of an
adversarial attack, its impact on morphological constructs of clinical
significance, their perceptibility by a trained pathologist and saliency maps
obtained using deep learning models. Based on our analysis, we strongly
recommend that computational pathology models be critically analyzed using the
proposed adversarial validation strategy prior to clinical adoption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Foote_A/0/1/0/all/0/1"&gt;Alex Foote&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asif_A/0/1/0/all/0/1"&gt;Amina Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Azam_A/0/1/0/all/0/1"&gt;Ayesha Azam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1"&gt;Nasir Rajpoot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1"&gt;Fayyaz Minhas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Long-term Non-invasive Monitoring for Epilepsy via Wearable EEG Devices. (arXiv:2106.08008v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.08008</id>
        <link href="http://arxiv.org/abs/2106.08008"/>
        <updated>2021-06-16T01:21:09.103Z</updated>
        <summary type="html"><![CDATA[We present the implementation of seizure detection algorithms based on a
minimal number of EEG channels on a parallel ultra-low-power embedded platform.
The analyses are based on the CHB-MIT dataset, and include explorations of
different classification approaches (Support Vector Machines, Random Forest,
Extra Trees, AdaBoost) and different pre/post-processing techniques to maximize
sensitivity while guaranteeing no false alarms. We analyze global and
subject-specific approaches, considering all 23-electrodes or only 4 temporal
channels. For 8s window size and subject-specific approach, we report zero
false positives and 100% sensitivity. These algorithms are parallelized and
optimized for a parallel ultra-low power (PULP) platform, enabling 300h of
continuous monitoring on a 300 mAh battery, in a wearable form factor and power
budget. These results pave the way for the implementation of affordable,
wearable, long-term epilepsy monitoring solutions with low false-positive rates
and high sensitivity, meeting both patient and caregiver requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ingolfsson_T/0/1/0/all/0/1"&gt;Thorir Mar Ingolfsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cossettini_A/0/1/0/all/0/1"&gt;Andrea Cossettini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tabanelli_E/0/1/0/all/0/1"&gt;Enrico Tabanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tagliavini_G/0/1/0/all/0/1"&gt;Guiseppe Tagliavini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ryvlin_P/0/1/0/all/0/1"&gt;Philippe Ryvlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Benini_L/0/1/0/all/0/1"&gt;Luca Benini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MICo: Learning improved representations via sampling-based state similarity for Markov decision processes. (arXiv:2106.08229v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08229</id>
        <link href="http://arxiv.org/abs/2106.08229"/>
        <updated>2021-06-16T01:21:09.083Z</updated>
        <summary type="html"><![CDATA[We present a new behavioural distance over the state space of a Markov
decision process, and demonstrate the use of this distance as an effective
means of shaping the learnt representations of deep reinforcement learning
agents. While existing notions of state similarity are typically difficult to
learn at scale due to high computational cost and lack of sample-based
algorithms, our newly-proposed distance addresses both of these issues. In
addition to providing detailed theoretical analysis, we provide empirical
evidence that learning this distance alongside the value function yields
structured and informative representations, including strong results on the
Arcade Learning Environment benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1"&gt;Pablo Samuel Castro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kastner_T/0/1/0/all/0/1"&gt;Tyler Kastner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panangaden_P/0/1/0/all/0/1"&gt;Prakash Panangaden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rowland_M/0/1/0/all/0/1"&gt;Mark Rowland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Application of the Quantum Potential Neural Network to multi-electronic atoms. (arXiv:2106.08138v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.08138</id>
        <link href="http://arxiv.org/abs/2106.08138"/>
        <updated>2021-06-16T01:21:09.067Z</updated>
        <summary type="html"><![CDATA[In this report, the application of the Quantum Potential Neural Network
(QPNN) framework to many electron atomic systems is presented. For this study,
full configuration interaction (FCI) one--electron density functions within
predefined limits of accuracy were used to train the QPNN. The obtained results
suggest that this new neural network is capable of learning the effective
potential functions of many electron atoms in a completely unsupervised manner,
and using only limited information from the probability density. Using the
effective potential functions learned for each of the studied systems the QPNN
was able to estimate the total energies of each of the systems (with a maximum
of 10 trials) with a remarkable accuracy when compared to the FCI energies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Corzo_H/0/1/0/all/0/1"&gt;Hector H. Corzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sehanobish_A/0/1/0/all/0/1"&gt;Arijit Sehanobish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kara_O/0/1/0/all/0/1"&gt;Onur Kara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualizing Multiple Tasks via Learning to Decompose. (arXiv:2106.08112v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08112</id>
        <link href="http://arxiv.org/abs/2106.08112"/>
        <updated>2021-06-16T01:21:09.050Z</updated>
        <summary type="html"><![CDATA[One single instance could possess multiple portraits and reveal diverse
relationships with others according to different contexts. Those ambiguities
increase the difficulty of learning a generalizable model when there exists one
concept or mixed concepts in a task. We propose a general approach Learning to
Decompose Network (LeadNet) for both two cases, which contextualizes a model
through meta-learning multiple maps for concepts discovery -- the
representations of instances are decomposed and adapted conditioned on the
contexts. Through taking a holistic view over multiple latent components over
instances in a sampled pseudo task, LeadNet learns to automatically select the
right concept via incorporating those rich semantics inside and between
objects. LeadNet demonstrates its superiority in various applications,
including exploring multiple views of confusing tasks, out-of-distribution
recognition, and few-shot image classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Da-Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lanqing Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiu-Shen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Awardee Solution of KDD Cup 2021 OGB Large-Scale Challenge Graph-Level Track. (arXiv:2106.08279v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08279</id>
        <link href="http://arxiv.org/abs/2106.08279"/>
        <updated>2021-06-16T01:21:09.031Z</updated>
        <summary type="html"><![CDATA[In this technical report, we present our solution of KDD Cup 2021 OGB
Large-Scale Challenge - PCQM4M-LSC Track. We adopt Graphormer and ExpC as our
basic models. We train each model by 8-fold cross-validation, and additionally
train two Graphormer models on the union of training and validation sets with
different random seeds. For final submission, we use a naive ensemble for these
18 models by taking average of their outputs. Using our method, our team
MachineLearning achieved 0.1200 MAE on test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1"&gt;Chengxuan Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mingqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuxin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1"&gt;Guolin Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shengjie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenglin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yanming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the List Decoding Version of the Cyclically Equivariant Neural Decoder. (arXiv:2106.07964v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.07964</id>
        <link href="http://arxiv.org/abs/2106.07964"/>
        <updated>2021-06-16T01:21:09.025Z</updated>
        <summary type="html"><![CDATA[The cyclically equivariant neural decoder was recently proposed in [Chen-Ye,
International Conference on Machine Learning, 2021] to decode cyclic codes. In
the same paper, a list decoding procedure was also introduced for two widely
used classes of cyclic codes -- BCH codes and punctured Reed-Muller (RM) codes.
While the list decoding procedure significantly improves the Frame Error Rate
(FER) of the cyclically equivariant neural decoder, the Bit Error Rate (BER) of
the list decoding procedure is even worse than the unique decoding algorithm
when the list size is small. In this paper, we propose an improved version of
the list decoding algorithm for BCH codes and punctured RM codes. Our new
proposal significantly reduces the BER while maintaining the same (in some
cases even smaller) FER. More specifically, our new decoder provides up to
$2$dB gain over the previous list decoder when measured by BER, and the running
time of our new decoder is $15\%$ smaller. Code available at
https://github.com/improvedlistdecoder/code]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1"&gt;Min Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning for Conservation Decisions. (arXiv:2106.08272v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08272</id>
        <link href="http://arxiv.org/abs/2106.08272"/>
        <updated>2021-06-16T01:21:09.019Z</updated>
        <summary type="html"><![CDATA[Can machine learning help us make better decisions about a changing planet?
In this paper, we illustrate and discuss the potential of a promising corner of
machine learning known as _reinforcement learning_ (RL) to help tackle the most
challenging conservation decision problems. RL is uniquely well suited to
conservation and global change challenges for three reasons: (1) RL explicitly
focuses on designing an agent who _interacts_ with an environment which is
dynamic and uncertain, (2) RL approaches do not require massive amounts of
data, (3) RL approaches would utilize rather than replace existing models,
simulations, and the knowledge they contain. We provide a conceptual and
technical introduction to RL and its relevance to ecological and conservation
challenges, including examples of a problem in setting fisheries quotas and in
managing ecological tipping points. Four appendices with annotated code provide
a tangible introduction to researchers looking to adopt, evaluate, or extend
these approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lapeyrolerie_M/0/1/0/all/0/1"&gt;Marcus Lapeyrolerie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chapman_M/0/1/0/all/0/1"&gt;Melissa S. Chapman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norman_K/0/1/0/all/0/1"&gt;Kari E. A. Norman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boettiger_C/0/1/0/all/0/1"&gt;Carl Boettiger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Representation and Inference for NLP. (arXiv:2106.08117v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08117</id>
        <link href="http://arxiv.org/abs/2106.08117"/>
        <updated>2021-06-16T01:21:09.012Z</updated>
        <summary type="html"><![CDATA[Semantic representation and inference is essential for Natural Language
Processing (NLP). The state of the art for semantic representation and
inference is deep learning, and particularly Recurrent Neural Networks (RNNs),
Convolutional Neural Networks (CNNs), and transformer Self-Attention models.
This thesis investigates the use of deep learning for novel semantic
representation and inference, and makes contributions in the following three
areas: creating training data, improving semantic representations and extending
inference learning. In terms of creating training data, we contribute the
largest publicly available dataset of real-life factual claims for the purpose
of automatic claim verification (MultiFC), and we present a novel inference
model composed of multi-scale CNNs with different kernel sizes that learn from
external sources to infer fact checking labels. In terms of improving semantic
representations, we contribute a novel model that captures non-compositional
semantic indicators. By definition, the meaning of a non-compositional phrase
cannot be inferred from the individual meanings of its composing words (e.g.,
hot dog). Motivated by this, we operationalize the compositionality of a phrase
contextually by enriching the phrase representation with external word
embeddings and knowledge graphs. Finally, in terms of inference learning, we
propose a series of novel deep learning architectures that improve inference by
using syntactic dependencies, by ensembling role guided attention heads,
incorporating gating layers, and concatenating multiple heads in novel and
effective ways. This thesis consists of seven publications (five published and
two under review).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dongsheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness as Equality of Opportunity: Normative Guidance from Political Philosophy. (arXiv:2106.08259v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2106.08259</id>
        <link href="http://arxiv.org/abs/2106.08259"/>
        <updated>2021-06-16T01:21:09.006Z</updated>
        <summary type="html"><![CDATA[Recent interest in codifying fairness in Automated Decision Systems (ADS) has
resulted in a wide range of formulations of what it means for an algorithmic
system to be fair. Most of these propositions are inspired by, but inadequately
grounded in, political philosophy scholarship. This paper aims to correct that
deficit. We introduce a taxonomy of fairness ideals using doctrines of Equality
of Opportunity (EOP) from political philosophy, clarifying their conceptions in
philosophy and the proposed codification in fair machine learning. We arrange
these fairness ideals onto an EOP spectrum, which serves as a useful frame to
guide the design of a fair ADS in a given context.

We use our fairness-as-EOP framework to re-interpret the impossibility
results from a philosophical perspective, as the in-compatibility between
different value systems, and demonstrate the utility of the framework with
several real-world and hypothetical examples. Through our EOP-framework we hope
to answer what it means for an ADS to be fair from a moral and political
philosophy standpoint, and to pave the way for similar scholarship from ethics
and legal experts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Falaah Arif Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manis_E/0/1/0/all/0/1"&gt;Eleni Manis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanovich_J/0/1/0/all/0/1"&gt;Julia Stoyanovich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialectal Speech Recognition and Translation of Swiss German Speech to Standard German Text: Microsoft's Submission to SwissText 2021. (arXiv:2106.08126v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.08126</id>
        <link href="http://arxiv.org/abs/2106.08126"/>
        <updated>2021-06-16T01:21:08.999Z</updated>
        <summary type="html"><![CDATA[This paper describes the winning approach in the public SwissText 2021
competition on dialect recognition and translation of Swiss German speech to
standard German text. Swiss German refers to the multitude of Alemannic
dialects spoken in the German-speaking parts of Switzerland. Swiss German
differs significantly from standard German in pronunciation, word inventory and
grammar. It is mostly incomprehensible to native German speakers. Moreover, it
lacks a standardized written script. To solve the challenging task, we propose
a hybrid automatic speech recognition system with a lexicon that incorporates
translations, a 1st pass language model that deals with Swiss German
particularities, a transfer-learned acoustic model and a strong neural language
model for 2nd pass rescoring. Our submission reaches 46.04% BLEU on a blind
conversational test set and outperforms the second best competitor by a 12%
relative margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Arabskyy_Y/0/1/0/all/0/1"&gt;Yuriy Arabskyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Aashish Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dey_S/0/1/0/all/0/1"&gt;Subhadeep Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Koller_O/0/1/0/all/0/1"&gt;Oscar Koller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Incident Prediction Models Over Large Geographical Areas for Emergency Response Systems. (arXiv:2106.08307v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08307</id>
        <link href="http://arxiv.org/abs/2106.08307"/>
        <updated>2021-06-16T01:21:08.980Z</updated>
        <summary type="html"><![CDATA[Principled decision making in emergency response management necessitates the
use of statistical models that predict the spatial-temporal likelihood of
incident occurrence. These statistical models are then used for proactive
stationing which allocates first responders across the spatial area in order to
reduce overall response time. Traditional methods that simply aggregate past
incidents over space and time fail to make useful short-term predictions when
the spatial region is large and focused on fine-grained spatial entities like
interstate highway networks. This is partially due to the sparsity of incidents
with respect to the area in consideration. Further, accidents are affected by
several covariates, and collecting, cleaning, and managing multiple streams of
data from various sources is challenging for large spatial areas. In this
paper, we highlight how this problem is being solved for the state of
Tennessee, a state in the USA with a total area of over 100,000 sq. km. Our
pipeline, based on a combination of synthetic resampling, non-spatial
clustering, and learning from data can efficiently forecast the spatial and
temporal dynamics of accident occurrence, even under sparse conditions. In the
paper, we describe our pipeline that uses data related to roadway geometry,
weather, historical accidents, and real-time traffic congestion to aid accident
forecasting. To understand how our forecasting model can affect allocation and
dispatch, we improve upon a classical resource allocation approach.
Experimental results show that our approach can significantly reduce response
times in the field in comparison with current approaches followed by first
responders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vazirizade_S/0/1/0/all/0/1"&gt;Sayyed Mohsen Vazirizade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1"&gt;Ayan Mukhopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pettet_G/0/1/0/all/0/1"&gt;Geoffrey Pettet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Said_S/0/1/0/all/0/1"&gt;Said El Said&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baroud_H/0/1/0/all/0/1"&gt;Hiba Baroud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1"&gt;Abhishek Dubey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detect and remove watermark in deep neural networks via generative adversarial networks. (arXiv:2106.08104v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.08104</id>
        <link href="http://arxiv.org/abs/2106.08104"/>
        <updated>2021-06-16T01:21:08.973Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNN) have achieved remarkable performance in various
fields. However, training a DNN model from scratch requires a lot of computing
resources and training data. It is difficult for most individual users to
obtain such computing resources and training data. Model copyright infringement
is an emerging problem in recent years. For instance, pre-trained models may be
stolen or abuse by illegal users without the authorization of the model owner.
Recently, many works on protecting the intellectual property of DNN models have
been proposed. In these works, embedding watermarks into DNN based on backdoor
is one of the widely used methods. However, when the DNN model is stolen, the
backdoor-based watermark may face the risk of being detected and removed by an
adversary. In this paper, we propose a scheme to detect and remove watermark in
deep neural networks via generative adversarial networks (GAN). We demonstrate
that the backdoor-based DNN watermarks are vulnerable to the proposed GAN-based
watermark removal attack. The proposed attack method includes two phases. In
the first phase, we use the GAN and few clean images to detect and reverse the
watermark in the DNN model. In the second phase, we fine-tune the watermarked
DNN based on the reversed backdoor images. Experimental evaluations on the
MNIST and CIFAR10 datasets demonstrate that, the proposed method can
effectively remove about 98% of the watermark in DNN models, as the watermark
retention rate reduces from 100% to less than 2% after applying the proposed
attack. In the meantime, the proposed attack hardly affects the model's
performance. The test accuracy of the watermarked DNN on the MNIST and the
CIFAR10 datasets drops by less than 1% and 3%, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1"&gt;Mingfu Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shichang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yushu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiqiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Employing an Adjusted Stability Measure for Multi-Criteria Model Fitting on Data Sets with Similar Features. (arXiv:2106.08105v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08105</id>
        <link href="http://arxiv.org/abs/2106.08105"/>
        <updated>2021-06-16T01:21:08.964Z</updated>
        <summary type="html"><![CDATA[Fitting models with high predictive accuracy that include all relevant but no
irrelevant or redundant features is a challenging task on data sets with
similar (e.g. highly correlated) features. We propose the approach of tuning
the hyperparameters of a predictive model in a multi-criteria fashion with
respect to predictive accuracy and feature selection stability. We evaluate
this approach based on both simulated and real data sets and we compare it to
the standard approach of single-criteria tuning of the hyperparameters as well
as to the state-of-the-art technique "stability selection". We conclude that
our approach achieves the same or better predictive performance compared to the
two established approaches. Considering the stability during tuning does not
decrease the predictive accuracy of the resulting models. Our approach succeeds
at selecting the relevant features while avoiding irrelevant or redundant
features. The single-criteria approach fails at avoiding irrelevant or
redundant features and the stability selection approach fails at selecting
enough relevant features for achieving acceptable predictive accuracy. For our
approach, for data sets with many similar features, the feature selection
stability must be evaluated with an adjusted stability measure, that is, a
measure that considers similarities between features. For data sets with only
few similar features, an unadjusted stability measure suffices and is faster to
compute.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bommert_A/0/1/0/all/0/1"&gt;Andrea Bommert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rahnenfuhrer_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg Rahnenf&amp;#xfc;hrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lang_M/0/1/0/all/0/1"&gt;Michel Lang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Extraction and Adversarial Attacks on Neural Networks using Switching Power Information. (arXiv:2106.08299v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08299</id>
        <link href="http://arxiv.org/abs/2106.08299"/>
        <updated>2021-06-16T01:21:08.957Z</updated>
        <summary type="html"><![CDATA[Artificial neural networks (ANNs) have gained significant popularity in the
last decade for solving narrow AI problems in domains such as healthcare,
transportation, and defense. As ANNs become more ubiquitous, it is imperative
to understand their associated safety, security, and privacy vulnerabilities.
Recently, it has been shown that ANNs are susceptible to a number of
adversarial evasion attacks--inputs that cause the ANN to make high-confidence
misclassifications despite being almost indistinguishable from the data used to
train and test the network. This work explores to what degree finding these
examples maybe aided by using side-channel information, specifically switching
power consumption, of hardware implementations of ANNs. A black-box threat
scenario is assumed, where an attacker has access to the ANN hardware's input,
outputs, and topology, but the trained model parameters are unknown. Then, a
surrogate model is trained to have similar functional (i.e. input-output
mapping) and switching power characteristics as the oracle (black-box) model.
Our results indicate that the inclusion of power consumption data increases the
fidelity of the model extraction by up to 30 percent based on a mean square
error comparison of the oracle and surrogate weights. However, transferability
of adversarial examples from the surrogate to the oracle model was not
significantly affected.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tommy Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merkel_C/0/1/0/all/0/1"&gt;Cory Merkel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compression Implies Generalization. (arXiv:2106.07989v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07989</id>
        <link href="http://arxiv.org/abs/2106.07989"/>
        <updated>2021-06-16T01:21:08.950Z</updated>
        <summary type="html"><![CDATA[Explaining the surprising generalization performance of deep neural networks
is an active and important line of research in theoretical machine learning.
Influential work by Arora et al. (ICML'18) showed that, noise stability
properties of deep nets occurring in practice can be used to provably compress
model representations. They then argued that the small representations of
compressed networks imply good generalization performance albeit only of the
compressed nets. Extending their compression framework to yield generalization
bounds for the original uncompressed networks remains elusive.

Our main contribution is the establishment of a compression-based framework
for proving generalization bounds. The framework is simple and powerful enough
to extend the generalization bounds by Arora et al. to also hold for the
original network. To demonstrate the flexibility of the framework, we also show
that it allows us to give simple proofs of the strongest known generalization
bounds for other popular machine learning models, namely Support Vector
Machines and Boosting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gronlund_A/0/1/0/all/0/1"&gt;Allan Gr&amp;#xf8;nlund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hogsgaard_M/0/1/0/all/0/1"&gt;Mikael H&amp;#xf8;gsgaard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamma_L/0/1/0/all/0/1"&gt;Lior Kamma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1"&gt;Kasper Green Larsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSMix: Saliency-Based Span Mixup for Text Classification. (arXiv:2106.08062v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08062</id>
        <link href="http://arxiv.org/abs/2106.08062"/>
        <updated>2021-06-16T01:21:08.928Z</updated>
        <summary type="html"><![CDATA[Data augmentation with mixup has shown to be effective on various computer
vision tasks. Despite its great success, there has been a hurdle to apply mixup
to NLP tasks since text consists of discrete tokens with variable length. In
this work, we propose SSMix, a novel mixup method where the operation is
performed on input text rather than on hidden vectors like previous approaches.
SSMix synthesizes a sentence while preserving the locality of two original
texts by span-based mixing and keeping more tokens related to the prediction
relying on saliency information. With extensive experiments, we empirically
validate that our method outperforms hidden-level mixup methods on a wide range
of text classification benchmarks, including textual entailment, sentiment
classification, and question-type classification. Our code is available at
https://github.com/clovaai/ssmix.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Soyoung Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gyuwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyumin Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Very Deep Graph Neural Networks Via Noise Regularisation. (arXiv:2106.07971v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07971</id>
        <link href="http://arxiv.org/abs/2106.07971"/>
        <updated>2021-06-16T01:21:08.921Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) perform learned message passing over an input
graph, but conventional wisdom says performing more than handful of steps makes
training difficult and does not yield improved performance. Here we show the
contrary. We train a deep GNN with up to 100 message passing steps and achieve
several state-of-the-art results on two challenging molecular property
prediction benchmarks, Open Catalyst 2020 IS2RE and QM9. Our approach depends
crucially on a novel but simple regularisation method, which we call ``Noisy
Nodes'', in which we corrupt the input graph with noise and add an auxiliary
node autoencoder loss if the task is graph property prediction. Our results
show this regularisation method allows the model to monotonically improve in
performance with increased message passing steps. Our work opens new
opportunities for reaping the benefits of deep neural networks in the space of
graph and other structured prediction problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Godwin_J/0/1/0/all/0/1"&gt;Jonathan Godwin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaarschmidt_M/0/1/0/all/0/1"&gt;Michael Schaarschmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaunt_A/0/1/0/all/0/1"&gt;Alexander Gaunt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1"&gt;Alvaro Sanchez-Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubanova_Y/0/1/0/all/0/1"&gt;Yulia Rubanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1"&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirkpatrick_J/0/1/0/all/0/1"&gt;James Kirkpatrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1"&gt;Peter Battaglia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural continual learning: success is a journey, not (just) a destination. (arXiv:2106.08085v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08085</id>
        <link href="http://arxiv.org/abs/2106.08085"/>
        <updated>2021-06-16T01:21:08.915Z</updated>
        <summary type="html"><![CDATA[Biological agents are known to learn many different tasks over the course of
their lives, and to be able to revisit previous tasks and behaviors with little
to no loss in performance. In contrast, artificial agents are prone to
'catastrophic forgetting' whereby performance on previous tasks deteriorates
rapidly as new ones are acquired. This shortcoming has recently been addressed
using methods that encourage parameters to stay close to those used for
previous tasks. This can be done by (i) using specific parameter regularizers
that map out suitable destinations in parameter space, or (ii) guiding the
optimization journey by projecting gradients into subspaces that do not
interfere with previous tasks. However, parameter regularization has been shown
to be relatively ineffective in recurrent neural networks (RNNs), a setting
relevant to the study of neural dynamics supporting biological continual
learning. Similarly, projection based methods can reach capacity and fail to
learn any further as the number of tasks increases. To address these
limitations, we propose Natural Continual Learning (NCL), a new method that
unifies weight regularization and projected gradient descent. NCL uses Bayesian
weight regularization to encourage good performance on all tasks at convergence
and combines this with gradient projections designed to prevent catastrophic
forgetting during optimization. NCL formalizes gradient projection as a trust
region algorithm based on the Fisher information metric, and achieves
scalability via a novel Kronecker-factored approximation strategy. Our method
outperforms both standard weight regularization techniques and projection based
approaches when applied to continual learning problems in RNNs. The trained
networks evolve task-specific dynamics that are strongly preserved as new tasks
are learned, similar to experimental findings in biological circuits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kao_T/0/1/0/all/0/1"&gt;Ta-Chu Kao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jensen_K/0/1/0/all/0/1"&gt;Kristopher T. Jensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernacchia_A/0/1/0/all/0/1"&gt;Alberto Bernacchia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennequin_G/0/1/0/all/0/1"&gt;Guillaume Hennequin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S2Engine: A Novel Systolic Architecture for Sparse Convolutional Neural Networks. (arXiv:2106.07894v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2106.07894</id>
        <link href="http://arxiv.org/abs/2106.07894"/>
        <updated>2021-06-16T01:21:08.908Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have achieved great success in
performing cognitive tasks. However, execution of CNNs requires a large amount
of computing resources and generates heavy memory traffic, which imposes a
severe challenge on computing system design. Through optimizing parallel
executions and data reuse in convolution, systolic architecture demonstrates
great advantages in accelerating CNN computations. However, regular internal
data transmission path in traditional systolic architecture prevents the
systolic architecture from completely leveraging the benefits introduced by
neural network sparsity. Deployment of fine-grained sparsity on the existing
systolic architectures is greatly hindered by the incurred computational
overheads. In this work, we propose S2Engine $-$ a novel systolic architecture
that can fully exploit the sparsity in CNNs with maximized data reuse. S2Engine
transmits compressed data internally and allows each processing element to
dynamically select an aligned data from the compressed dataflow in convolution.
Compared to the naive systolic array, S2Engine achieves about $3.2\times$ and
about $3.0\times$ improvements on speed and energy efficiency, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianlei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1"&gt;Wenzhi Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xingzhou Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xucheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1"&gt;Pengcheng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Weisheng Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hotel Recognition via Latent Image Embedding. (arXiv:2106.08042v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08042</id>
        <link href="http://arxiv.org/abs/2106.08042"/>
        <updated>2021-06-16T01:21:08.901Z</updated>
        <summary type="html"><![CDATA[We approach the problem of hotel recognition with deep metric learning. We
overview the existing approaches and propose a modification to Contrastive loss
called Contrastive-Triplet loss. We construct a robust pipeline for
benchmarking metric learning models and perform experiments on Hotels-50K and
CUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval
on Hotels-50k. We open-source our code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tseytlin_B/0/1/0/all/0/1"&gt;Boris Tseytlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1"&gt;Ilya Makarov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Learning of Keypoint Representations for Continuous Control from Images. (arXiv:2106.07995v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07995</id>
        <link href="http://arxiv.org/abs/2106.07995"/>
        <updated>2021-06-16T01:21:08.881Z</updated>
        <summary type="html"><![CDATA[In many control problems that include vision, optimal controls can be
inferred from the location of the objects in the scene. This information can be
represented using keypoints, which is a list of spatial locations in the input
image. Previous works show that keypoint representations learned during
unsupervised pre-training using encoder-decoder architectures can provide good
features for control tasks. In this paper, we show that it is possible to learn
efficient keypoint representations end-to-end, without the need for
unsupervised pre-training, decoders, or additional losses. Our proposed
architecture consists of a differentiable keypoint extractor that feeds the
coordinates of the estimated keypoints directly to a soft actor-critic agent.
The proposed algorithm yields performance competitive to the state-of-the art
on DeepMind Control Suite tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boney_R/0/1/0/all/0/1"&gt;Rinu Boney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1"&gt;Alexander Ilin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1"&gt;Juho Kannala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Contrastive Explanations for Inductive Logic Programming Based on a Near Miss Approach. (arXiv:2106.08064v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08064</id>
        <link href="http://arxiv.org/abs/2106.08064"/>
        <updated>2021-06-16T01:21:08.874Z</updated>
        <summary type="html"><![CDATA[In recent research, human-understandable explanations of machine learning
models have received a lot of attention. Often explanations are given in form
of model simplifications or visualizations. However, as shown in cognitive
science as well as in early AI research, concept understanding can also be
improved by the alignment of a given instance for a concept with a similar
counterexample. Contrasting a given instance with a structurally similar
example which does not belong to the concept highlights what characteristics
are necessary for concept membership. Such near misses have been proposed by
Winston (1970) as efficient guidance for learning in relational domains. We
introduce an explanation generation algorithm for relational concepts learned
with Inductive Logic Programming (\textsc{GeNME}). The algorithm identifies
near miss examples from a given set of instances and ranks these examples by
their degree of closeness to a specific positive instance. A modified rule
which covers the near miss but not the original instance is given as an
explanation. We illustrate \textsc{GeNME} with the well known family domain
consisting of kinship relations, the visual relational Winston arches domain
and a real-world domain dealing with file management. We also present a
psychological experiment comparing human preferences of rule-based,
example-based, and near miss explanations in the family and the arches domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rabold_J/0/1/0/all/0/1"&gt;Johannes Rabold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siebers_M/0/1/0/all/0/1"&gt;Michael Siebers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1"&gt;Ute Schmid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning with Electronic Health Records is vulnerable to Backdoor Trigger Attacks. (arXiv:2106.07925v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07925</id>
        <link href="http://arxiv.org/abs/2106.07925"/>
        <updated>2021-06-16T01:21:08.868Z</updated>
        <summary type="html"><![CDATA[Electronic Health Records (EHRs) provide a wealth of information for machine
learning algorithms to predict the patient outcome from the data including
diagnostic information, vital signals, lab tests, drug administration, and
demographic information. Machine learning models can be built, for example, to
evaluate patients based on their predicted mortality or morbidity and to
predict required resources for efficient resource management in hospitals. In
this paper, we demonstrate that an attacker can manipulate the machine learning
predictions with EHRs easily and selectively at test time by backdoor attacks
with the poisoned training data. Furthermore, the poison we create has
statistically similar features to the original data making it hard to detect,
and can also attack multiple machine learning models without any knowledge of
the models. With less than 5% of the raw EHR data poisoned, we achieve average
attack success rates of 97% on mortality prediction tasks with MIMIC-III
database against Logistic Regression, Multilayer Perceptron, and Long
Short-term Memory models simultaneously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joe_B/0/1/0/all/0/1"&gt;Byunggill Joe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_A/0/1/0/all/0/1"&gt;Akshay Mehra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1"&gt;Insik Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamm_J/0/1/0/all/0/1"&gt;Jihun Hamm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Compensate: A Deep Neural Network Framework for 5G Power Amplifier Compensation. (arXiv:2106.07953v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.07953</id>
        <link href="http://arxiv.org/abs/2106.07953"/>
        <updated>2021-06-16T01:21:08.861Z</updated>
        <summary type="html"><![CDATA[Owing to the complicated characteristics of 5G communication system,
designing RF components through mathematical modeling becomes a challenging
obstacle. Moreover, such mathematical models need numerous manual adjustments
for various specification requirements. In this paper, we present a
learning-based framework to model and compensate Power Amplifiers (PAs) in 5G
communication. In the proposed framework, Deep Neural Networks (DNNs) are used
to learn the characteristics of the PAs, while, correspondent Digital
Pre-Distortions (DPDs) are also learned to compensate for the nonlinear and
memory effects of PAs. On top of the framework, we further propose two
frequency domain losses to guide the learning process to better optimize the
target, compared to naive time domain Mean Square Error (MSE). The proposed
framework serves as a drop-in replacement for the conventional approach. The
proposed approach achieves an average of 56.7% reduction of nonlinear and
memory effects, which converts to an average of 16.3% improvement over a
carefully-designed mathematical model, and even reaches 34% enhancement in
severe distortion scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1"&gt;Po-Yu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yi-Min Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuo_H/0/1/0/all/0/1"&gt;Hsien-Kai Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hantao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hsin-Hung Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_S/0/1/0/all/0/1"&gt;Sheng-Hong Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ou_W/0/1/0/all/0/1"&gt;Wei-Lun Ou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Chia-Ming Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning for Internet of Things: A Federated Learning Framework for On-device Anomaly Data Detection. (arXiv:2106.07976v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07976</id>
        <link href="http://arxiv.org/abs/2106.07976"/>
        <updated>2021-06-16T01:21:08.854Z</updated>
        <summary type="html"><![CDATA[Federated learning can be a promising solution for enabling IoT cybersecurity
(i.e., anomaly detection in the IoT environment) while preserving data privacy
and mitigating the high communication/storage overhead (e.g., high-frequency
data from time-series sensors) of centralized over-the-cloud approaches. In
this paper, to further push forward this direction with a comprehensive study
in both algorithm and system design, we build FedIoT platform that contains a
synthesized dataset using N-BaIoT, FedDetect algorithm, and a system design for
IoT devices. Furthermore, the proposed FedDetect learning framework improves
the performance by utilizing an adaptive optimizer (e.g., Adam) and a
cross-round learning rate scheduler. In a network of realistic IoT devices
(Raspberry PI), we evaluate FedIoT platform and FedDetect algorithm in both
model and system performance. Our results demonstrate the efficacy of federated
learning in detecting a large range of attack types. The system efficiency
analysis indicates that both end-to-end training time and memory cost are
affordable and promising for resource-constrained IoT devices. The source code
is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tuo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1"&gt;Chaoyang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tianhao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Mark Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1"&gt;Salman Avestimehr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequence-Level Training for Non-Autoregressive Neural Machine Translation. (arXiv:2106.08122v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08122</id>
        <link href="http://arxiv.org/abs/2106.08122"/>
        <updated>2021-06-16T01:21:08.846Z</updated>
        <summary type="html"><![CDATA[In recent years, Neural Machine Translation (NMT) has achieved notable
results in various translation tasks. However, the word-by-word generation
manner determined by the autoregressive mechanism leads to high translation
latency of the NMT and restricts its low-latency applications.
Non-Autoregressive Neural Machine Translation (NAT) removes the autoregressive
mechanism and achieves significant decoding speedup through generating target
words independently and simultaneously. Nevertheless, NAT still takes the
word-level cross-entropy loss as the training objective, which is not optimal
because the output of NAT cannot be properly evaluated due to the multimodality
problem. In this paper, we propose using sequence-level training objectives to
train NAT models, which evaluate the NAT outputs as a whole and correlates well
with the real translation quality. Firstly, we propose training NAT models to
optimize sequence-level evaluation metrics (e.g., BLEU) based on several novel
reinforcement algorithms customized for NAT, which outperforms the conventional
method by reducing the variance of gradient estimation. Secondly, we introduce
a novel training objective for NAT models, which aims to minimize the
Bag-of-Ngrams (BoN) difference between the model output and the reference
sentence. The BoN training objective is differentiable and can be calculated
efficiently without doing any approximations. Finally, we apply a three-stage
training strategy to combine these two methods to train the NAT model. We
validate our approach on four translation tasks (WMT14 En$\leftrightarrow$De,
WMT16 En$\leftrightarrow$Ro), which shows that our approach largely outperforms
NAT baselines and achieves remarkable performance on all translation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1"&gt;Chenze Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceptually-inspired super-resolution of compressed videos. (arXiv:2106.08147v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08147</id>
        <link href="http://arxiv.org/abs/2106.08147"/>
        <updated>2021-06-16T01:21:08.826Z</updated>
        <summary type="html"><![CDATA[Spatial resolution adaptation is a technique which has often been employed in
video compression to enhance coding efficiency. This approach encodes a lower
resolution version of the input video and reconstructs the original resolution
during decoding. Instead of using conventional up-sampling filters, recent work
has employed advanced super-resolution methods based on convolutional neural
networks (CNNs) to further improve reconstruction quality. These approaches are
usually trained to minimise pixel-based losses such as Mean-Squared Error
(MSE), despite the fact that this type of loss metric does not correlate well
with subjective opinions. In this paper, a perceptually-inspired
super-resolution approach (M-SRGAN) is proposed for spatial up-sampling of
compressed video using a modified CNN model, which has been trained using a
generative adversarial network (GAN) on compressed content with perceptual loss
functions. The proposed method was integrated with HEVC HM 16.20, and has been
evaluated on the JVET Common Test Conditions (UHD test sequences) using the
Random Access configuration. The results show evident perceptual quality
improvement over the original HM 16.20, with an average bitrate saving of 35.6%
(Bj{\o}ntegaard Delta measurement) based on a perceptual quality metric, VMAF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ma_D/0/1/0/all/0/1"&gt;Di Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Afonso_M/0/1/0/all/0/1"&gt;Mariana Afonso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1"&gt;David R. Bull&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Abstractive Opinion Summarization by Generating Sentences with Tree-Structured Topic Guidance. (arXiv:2106.08007v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08007</id>
        <link href="http://arxiv.org/abs/2106.08007"/>
        <updated>2021-06-16T01:21:08.819Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel unsupervised abstractive summarization method for
opinionated texts. While the basic variational autoencoder-based models assume
a unimodal Gaussian prior for the latent code of sentences, we alternate it
with a recursive Gaussian mixture, where each mixture component corresponds to
the latent code of a topic sentence and is mixed by a tree-structured topic
distribution. By decoding each Gaussian component, we generate sentences with
tree-structured topic guidance, where the root sentence conveys generic
content, and the leaf sentences describe specific topics. Experimental results
demonstrate that the generated topic sentences are appropriate as a summary of
opinionated texts, which are more informative and cover more input contents
than those generated by the recent unsupervised summarization model
(Bra\v{z}inskas et al., 2020). Furthermore, we demonstrate that the variance of
latent Gaussians represents the granularity of sentences, analogous to Gaussian
word embedding (Vilnis and McCallum, 2015).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Isonuma_M/0/1/0/all/0/1"&gt;Masaru Isonuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_J/0/1/0/all/0/1"&gt;Junichiro Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1"&gt;Danushka Bollegala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakata_I/0/1/0/all/0/1"&gt;Ichiro Sakata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coupled Gradient Estimators for Discrete Latent Variables. (arXiv:2106.08056v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08056</id>
        <link href="http://arxiv.org/abs/2106.08056"/>
        <updated>2021-06-16T01:21:08.813Z</updated>
        <summary type="html"><![CDATA[Training models with discrete latent variables is challenging due to the high
variance of unbiased gradient estimators. While low-variance reparameterization
gradients of a continuous relaxation can provide an effective solution, a
continuous relaxation is not always available or tractable. Dong et al. (2020)
and Yin et al. (2020) introduced a performant estimator that does not rely on
continuous relaxations; however, it is limited to binary random variables. We
introduce a novel derivation of their estimator based on importance sampling
and statistical couplings, which we extend to the categorical setting.
Motivated by the construction of a stick-breaking coupling, we introduce
gradient estimators based on reparameterizing categorical variables as
sequences of binary variables and Rao-Blackwellization. In systematic
experiments, we show that our proposed categorical gradient estimators provide
state-of-the-art performance, whereas even with additional
Rao-Blackwellization, previous estimators (Yin et al., 2019) underperform a
simpler REINFORCE with a leave-one-out-baseline estimator (Kool et al., 2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhe Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mnih_A/0/1/0/all/0/1"&gt;Andriy Mnih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tucker_G/0/1/0/all/0/1"&gt;George Tucker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization. (arXiv:2106.07991v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.07991</id>
        <link href="http://arxiv.org/abs/2106.07991"/>
        <updated>2021-06-16T01:21:08.806Z</updated>
        <summary type="html"><![CDATA[Bi-level optimization model is able to capture a wide range of complex
learning tasks with practical interest. Due to the witnessed efficiency in
solving bi-level programs, gradient-based methods have gained popularity in the
machine learning community. In this work, we propose a new gradient-based
solution scheme, namely, the Bi-level Value-Function-based Interior-point
Method (BVFIM). Following the main idea of the log-barrier interior-point
scheme, we penalize the regularized value function of the lower level problem
into the upper level objective. By further solving a sequence of differentiable
unconstrained approximation problems, we consequently derive a sequential
programming scheme. The numerical advantage of our scheme relies on the fact
that, when gradient methods are applied to solve the approximation problem, we
successfully avoid computing any expensive Hessian-vector or Jacobian-vector
product. We prove the convergence without requiring any convexity assumption on
either the upper level or the lower level objective. Experiments demonstrate
the efficiency of the proposed BVFIM on non-convex bi-level problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Liu_R/0/1/0/all/0/1"&gt;Risheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiaoming Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shangzhi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Evaluation of Sequential Machine Learning for Network Intrusion Detection. (arXiv:2106.07961v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.07961</id>
        <link href="http://arxiv.org/abs/2106.07961"/>
        <updated>2021-06-16T01:21:08.800Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning renewed the research interests in machine
learning for Network Intrusion Detection Systems (NIDS). Specifically,
attention has been given to sequential learning models, due to their ability to
extract the temporal characteristics of Network traffic Flows (NetFlows), and
use them for NIDS tasks. However, the applications of these sequential models
often consist of transferring and adapting methodologies directly from other
fields, without an in-depth investigation on how to leverage the specific
circumstances of cybersecurity scenarios; moreover, there is a lack of
comprehensive studies on sequential models that rely on NetFlow data, which
presents significant advantages over traditional full packet captures. We
tackle this problem in this paper. We propose a detailed methodology to extract
temporal sequences of NetFlows that denote patterns of malicious activities.
Then, we apply this methodology to compare the efficacy of sequential learning
models against traditional static learning models. In particular, we perform a
fair comparison of a `sequential' Long Short-Term Memory (LSTM) against a
`static' Feedforward Neural Networks (FNN) in distinct environments represented
by two well-known datasets for NIDS: the CICIDS2017 and the CTU13. Our results
highlight that LSTM achieves comparable performance to FNN in the CICIDS2017
with over 99.5\% F1-score; while obtaining superior performance in the CTU13,
with 95.7\% F1-score against 91.5\%. This paper thus paves the way to future
applications of sequential learning models for NIDS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Corsini_A/0/1/0/all/0/1"&gt;Andrea Corsini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shanchieh Jay Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Apruzzese_G/0/1/0/all/0/1"&gt;Giovanni Apruzzese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting the Calibration of Modern Neural Networks. (arXiv:2106.07998v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07998</id>
        <link href="http://arxiv.org/abs/2106.07998"/>
        <updated>2021-06-16T01:21:08.781Z</updated>
        <summary type="html"><![CDATA[Accurate estimation of predictive uncertainty (model calibration) is
essential for the safe application of neural networks. Many instances of
miscalibration in modern neural networks have been reported, suggesting a trend
that newer, more accurate models produce poorly calibrated predictions. Here,
we revisit this question for recent state-of-the-art image classification
models. We systematically relate model calibration and accuracy, and find that
the most recent models, notably those not using convolutions, are among the
best calibrated. Trends observed in prior model generations, such as decay of
calibration with distribution shift or model size, are less pronounced in
recent architectures. We also show that model size and amount of pretraining do
not fully explain these differences, suggesting that architecture is a major
determinant of calibration properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1"&gt;Matthias Minderer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1"&gt;Josip Djolonga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1"&gt;Rob Romijnders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubis_F/0/1/0/all/0/1"&gt;Frances Hubis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Dustin Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1"&gt;Mario Lucic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Query Embedding on Hyper-relational Knowledge Graphs. (arXiv:2106.08166v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.08166</id>
        <link href="http://arxiv.org/abs/2106.08166"/>
        <updated>2021-06-16T01:21:08.775Z</updated>
        <summary type="html"><![CDATA[Multi-hop logical reasoning is an established problem in the field of
representation learning on knowledge graphs (KGs). It subsumes both one-hop
link prediction as well as other more complex types of logical queries.
Existing algorithms operate only on classical, triple-based graphs, whereas
modern KGs often employ a hyper-relational modeling paradigm. In this paradigm,
typed edges may have several key-value pairs known as qualifiers that provide
fine-grained context for facts. In queries, this context modifies the meaning
of relations, and usually reduces the answer set. Hyper-relational queries are
often observed in real-world KG applications, and existing approaches for
approximate query answering cannot make use of qualifier pairs. In this work,
we bridge this gap and extend the multi-hop reasoning problem to
hyper-relational KGs allowing to tackle this new type of complex queries.
Building upon recent advancements in Graph Neural Networks and query embedding
techniques, we study how to embed and answer hyper-relational conjunctive
queries. Besides that, we propose a method to answer such queries and
demonstrate in our experiments that qualifiers improve query answering on a
diverse set of query patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alivanistos_D/0/1/0/all/0/1"&gt;Dimitrios Alivanistos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1"&gt;Max Berrendorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1"&gt;Michael Cochez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1"&gt;Mikhail Galkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thompson Sampling for Unimodal Bandits. (arXiv:2106.08187v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08187</id>
        <link href="http://arxiv.org/abs/2106.08187"/>
        <updated>2021-06-16T01:21:08.768Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a Thompson Sampling algorithm for \emph{unimodal}
bandits, where the expected reward is unimodal over the partially ordered arms.
To exploit the unimodal structure better, at each step, instead of exploration
from the entire decision space, our algorithm makes decision according to
posterior distribution only in the neighborhood of the arm that has the highest
empirical mean estimate. We theoretically prove that, for Bernoulli rewards,
the regret of our algorithm reaches the lower bound of unimodal bandits, thus
it is asymptotically optimal. For Gaussian rewards, the regret of our algorithm
is $\mathcal{O}(\log T)$, which is far better than standard Thompson Sampling
algorithms. Extensive experiments demonstrate the effectiveness of the proposed
algorithm on both synthetic data sets and the real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Long Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zehong Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1"&gt;Shasha Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shijian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1"&gt;Gang Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hongyang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Autonomy in Management of Wireless Random Networks. (arXiv:2106.07984v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.07984</id>
        <link href="http://arxiv.org/abs/2106.07984"/>
        <updated>2021-06-16T01:21:08.762Z</updated>
        <summary type="html"><![CDATA[This paper presents a machine learning strategy that tackles a distributed
optimization task in a wireless network with an arbitrary number of randomly
interconnected nodes. Individual nodes decide their optimal states with
distributed coordination among other nodes through randomly varying backhaul
links. This poses a technical challenge in distributed universal optimization
policy robust to a random topology of the wireless network, which has not been
properly addressed by conventional deep neural networks (DNNs) with rigid
structural configurations. We develop a flexible DNN formalism termed
distributed message-passing neural network (DMPNN) with forward and backward
computations independent of the network topology. A key enabler of this
approach is an iterative message-sharing strategy through arbitrarily connected
backhaul links. The DMPNN provides a convergent solution for iterative
coordination by learning numerous random backhaul interactions. The DMPNN is
investigated for various configurations of the power control in wireless
networks, and intensive numerical results prove its universality and viability
over conventional optimization and DNN approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sang Hyun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1"&gt;Tony Q. S. Quek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decomposition of Global Feature Importance into Direct and Associative Components (DEDACT). (arXiv:2106.08086v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08086</id>
        <link href="http://arxiv.org/abs/2106.08086"/>
        <updated>2021-06-16T01:21:08.731Z</updated>
        <summary type="html"><![CDATA[Global model-agnostic feature importance measures either quantify whether
features are directly used for a model's predictions (direct importance) or
whether they contain prediction-relevant information (associative importance).
Direct importance provides causal insight into the model's mechanism, yet it
fails to expose the leakage of information from associated but not directly
used variables. In contrast, associative importance exposes information leakage
but does not provide causal insight into the model's mechanism. We introduce
DEDACT - a framework to decompose well-established direct and associative
importance measures into their respective associative and direct components.
DEDACT provides insight into both the sources of prediction-relevant
information in the data and the direct and indirect feature pathways by which
the information enters the model. We demonstrate the method's usefulness on
simulated examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Konig_G/0/1/0/all/0/1"&gt;Gunnar K&amp;#xf6;nig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Freiesleben_T/0/1/0/all/0/1"&gt;Timo Freiesleben&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1"&gt;Bernd Bischl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Casalicchio_G/0/1/0/all/0/1"&gt;Giuseppe Casalicchio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Grosse_Wentrup_M/0/1/0/all/0/1"&gt;Moritz Grosse-Wentrup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Capabilities of Deep Learning Models on Learning Physical Relationships: Case of Rainfall-Runoff Modeling with LSTM. (arXiv:2106.07963v1 [physics.ao-ph])]]></title>
        <id>http://arxiv.org/abs/2106.07963</id>
        <link href="http://arxiv.org/abs/2106.07963"/>
        <updated>2021-06-16T01:21:08.651Z</updated>
        <summary type="html"><![CDATA[This study investigates the relationships which deep learning methods can
identify between the input and output data. As a case study, rainfall-runoff
modeling in a snow-dominated watershed by means of a long- and short-term
memory (LSTM) network is selected. Daily precipitation and mean air temperature
were used as model input to estimate daily flow discharge. After model training
and verification, two experimental simulations were conducted with hypothetical
inputs instead of observed meteorological data to clarify the response of the
trained model to the inputs. The first numerical experiment showed that even
without input precipitation, the trained model generated flow discharge,
particularly winter low flow and high flow during the snow-melting period. The
effects of warmer and colder conditions on the flow discharge were also
replicated by the trained model without precipitation. Additionally, the model
reflected only 17-39% of the total precipitation mass during the snow
accumulation period in the total annual flow discharge, revealing a strong lack
of water mass conservation. The results of this study indicated that a deep
learning method may not properly learn the explicit physical relationships
between input and target variables, although they are still capable of
maintaining strong goodness-of-fit results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Yokoo_K/0/1/0/all/0/1"&gt;Kazuki Yokoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ishida_K/0/1/0/all/0/1"&gt;Kei Ishida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ercan_A/0/1/0/all/0/1"&gt;Ali Ercan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tu_T/0/1/0/all/0/1"&gt;Tongbi Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Nagasato_T/0/1/0/all/0/1"&gt;Takeyoshi Nagasato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kiyama_M/0/1/0/all/0/1"&gt;Masato Kiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Amagasaki_M/0/1/0/all/0/1"&gt;Motoki Amagasaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization-friendly generic mechanisms without money. (arXiv:2106.07752v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.07752</id>
        <link href="http://arxiv.org/abs/2106.07752"/>
        <updated>2021-06-16T01:21:08.578Z</updated>
        <summary type="html"><![CDATA[The goal of this paper is to develop a generic framework for converting
modern optimization algorithms into mechanisms where inputs come from
self-interested agents. We focus on aggregating preferences from $n$ players in
a context without money. Special cases of this setting include voting,
allocation of items by lottery, and matching. Our key technical contribution is
a new meta-algorithm we call \apex (Adaptive Pricing Equalizing Externalities).
The framework is sufficiently general to be combined with any optimization
algorithm that is based on local search. We outline an agenda for studying the
algorithm's properties and its applications. As a special case of applying the
framework to the problem of one-sided assignment with lotteries, we obtain a
strengthening of the 1979 result by Hylland and Zeckhauser on allocation via a
competitive equilibrium from equal incomes (CEEI). The [HZ79] result posits
that there is a (fractional) allocation and a set of item prices such that the
allocation is a competitive equilibrium given prices. We further show that
there is always a reweighing of the players' utility values such that running
unit-demand VCG with reweighed utilities leads to a HZ-equilibrium prices.
Interestingly, not all HZ competitive equilibria come from VCG prices. As part
of our proof, we re-prove the [HZ79] result using only Brouwer's fixed point
theorem (and not the more general Kakutani's theorem). This may be of
independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Braverman_M/0/1/0/all/0/1"&gt;Mark Braverman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time Series Anomaly Detection for Cyber-physical Systems via Neural System Identification and Bayesian Filtering. (arXiv:2106.07992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07992</id>
        <link href="http://arxiv.org/abs/2106.07992"/>
        <updated>2021-06-16T01:21:08.560Z</updated>
        <summary type="html"><![CDATA[Recent advances in AIoT technologies have led to an increasing popularity of
utilizing machine learning algorithms to detect operational failures for
cyber-physical systems (CPS). In its basic form, an anomaly detection module
monitors the sensor measurements and actuator states from the physical plant,
and detects anomalies in these measurements to identify abnormal operation
status. Nevertheless, building effective anomaly detection models for CPS is
rather challenging as the model has to accurately detect anomalies in presence
of highly complicated system dynamics and unknown amount of sensor noise. In
this work, we propose a novel time series anomaly detection method called
Neural System Identification and Bayesian Filtering (NSIBF) in which a
specially crafted neural network architecture is posed for system
identification, i.e., capturing the dynamics of CPS in a dynamical state-space
model; then a Bayesian filtering algorithm is naturally applied on top of the
"identified" state-space model for robust anomaly detection by tracking the
uncertainty of the hidden state of the system recursively over time. We provide
qualitative as well as quantitative experiments with the proposed method on a
synthetic and three real-world CPS datasets, showing that NSIBF compares
favorably to the state-of-the-art methods with considerable improvements on
anomaly detection in CPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Cheng Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1"&gt;Pengwei Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges and Considerations with Code-Mixed NLP for Multilingual Societies. (arXiv:2106.07823v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07823</id>
        <link href="http://arxiv.org/abs/2106.07823"/>
        <updated>2021-06-16T01:21:08.554Z</updated>
        <summary type="html"><![CDATA[Multilingualism refers to the high degree of proficiency in two or more
languages in the written and oral communication modes. It often results in
language mixing, a.k.a. code-mixing, when a multilingual speaker switches
between multiple languages in a single utterance of a text or speech. This
paper discusses the current state of the NLP research, limitations, and
foreseeable pitfalls in addressing five real-world applications for social good
crisis management, healthcare, political campaigning, fake news, and hate
speech for multilingual societies. We also propose futuristic datasets, models,
and tools that can significantly advance the current research in multilingual
NLP applications for the societal good. As a representative example, we
consider English-Hindi code-mixing but draw similar inferences for other
language pairs]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1"&gt;Vivek Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the compromise between accuracy, interpretability and personalization of rule-based machine learning in medical problems. (arXiv:2106.07827v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07827</id>
        <link href="http://arxiv.org/abs/2106.07827"/>
        <updated>2021-06-16T01:21:08.548Z</updated>
        <summary type="html"><![CDATA[One of the key challenges when developing a predictive model is the
capability to describe the domain knowledge and the cause-effect relationships
in a simple way. Decision rules are a useful and important methodology in this
context, justifying their application in several areas, in particular in
clinical practice. Several machine-learning classifiers have exploited the
advantageous properties of decision rules to build intelligent prediction
models, namely decision trees and ensembles of trees (ETs). However, such
methodologies usually suffer from a trade-off between interpretability and
predictive performance. Some procedures consider a simplification of ETs, using
heuristic approaches to select an optimal reduced set of decision rules. In
this paper, we introduce a novel step to those methodologies. We create a new
component to predict if a given rule will be correct or not for a particular
patient, which introduces personalization into the procedure. Furthermore, the
validation results using three public clinical datasets show that it also
allows to increase the predictive performance of the selected set of rules,
improving the mentioned trade-off.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valente_F/0/1/0/all/0/1"&gt;Francisco Valente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paredes_S/0/1/0/all/0/1"&gt;Simao Paredes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1"&gt;Jorge Henriques&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code Integrity Attestation for PLCs using Black Box Neural Network Predictions. (arXiv:2106.07851v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.07851</id>
        <link href="http://arxiv.org/abs/2106.07851"/>
        <updated>2021-06-16T01:21:08.498Z</updated>
        <summary type="html"><![CDATA[Cyber-physical systems (CPSs) are widespread in critical domains, and
significant damage can be caused if an attacker is able to modify the code of
their programmable logic controllers (PLCs). Unfortunately, traditional
techniques for attesting code integrity (i.e. verifying that it has not been
modified) rely on firmware access or roots-of-trust, neither of which
proprietary or legacy PLCs are likely to provide. In this paper, we propose a
practical code integrity checking solution based on privacy-preserving black
box models that instead attest the input/output behaviour of PLC programs.
Using faithful offline copies of the PLC programs, we identify their most
important inputs through an information flow analysis, execute them on multiple
combinations to collect data, then train neural networks able to predict PLC
outputs (i.e. actuator commands) from their inputs. By exploiting the black box
nature of the model, our solution maintains the privacy of the original PLC
code and does not assume that attackers are unaware of its presence. The trust
instead comes from the fact that it is extremely hard to attack the PLC code
and neural networks at the same time and with consistent outcomes. We evaluated
our approach on a modern six-stage water treatment plant testbed, finding that
it could predict actuator states from PLC inputs with near-100% accuracy, and
thus could detect all 120 effective code mutations that we subjected the PLCs
to. Finally, we found that it is not practically possible to simultaneously
modify the PLC code and apply discreet adversarial noise to our attesters in a
way that leads to consistent (mis-)predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poskitt_C/0/1/0/all/0/1"&gt;Christopher M. Poskitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jun Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MobILE: Model-Based Imitation Learning From Observation Alone. (arXiv:2102.10769v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10769</id>
        <link href="http://arxiv.org/abs/2102.10769"/>
        <updated>2021-06-16T01:21:08.466Z</updated>
        <summary type="html"><![CDATA[This paper studies Imitation Learning from Observations alone (ILFO) where
the learner is presented with expert demonstrations that consist only of states
visited by an expert (without access to actions taken by the expert). We
present a provably efficient model-based framework MobILE to solve the ILFO
problem. MobILE involves carefully trading off strategic exploration against
imitation - this is achieved by integrating the idea of optimism in the face of
uncertainty into the distribution matching imitation learning (IL) framework.
We provide a unified analysis for MobILE, and demonstrate that MobILE enjoys
strong performance guarantees for classes of MDP dynamics that satisfy certain
well studied notions of structural complexity. We also show that the ILFO
problem is strictly harder than the standard IL problem by presenting an
exponential sample complexity separation between IL and ILFO. We complement
these theoretical results with experimental simulations on benchmark OpenAI Gym
tasks that indicate the efficacy of MobILE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kidambi_R/0/1/0/all/0/1"&gt;Rahul Kidambi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;Jonathan Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CatBoost model with synthetic features in application to loan risk assessment of small businesses. (arXiv:2106.07954v1 [cs.CE])]]></title>
        <id>http://arxiv.org/abs/2106.07954</id>
        <link href="http://arxiv.org/abs/2106.07954"/>
        <updated>2021-06-16T01:21:08.426Z</updated>
        <summary type="html"><![CDATA[Loan risk for small business has long been a complex problem worthy of
exploring. Predicting the loan risk approximately can benefit entrepreneurship
by developing more jobs for the society. CatBoost (Categorical Boosting) is a
powerful machine learning algorithm that is suitable for dataset with many
categorical variables like the dataset for forecasting loan risk. In this
paper, we identify the important risk factors that contribute to loan status
classification problem. Then we compare the the performance between
boosting-type algorithms(especially CatBoost) with other traditional yet
popular ones. The dataset we adopt in the research comes from the U.S. Small
Business Administration (SBA) and holds a very large sample size (899,164
observations and 27 features). We obtain a high accuracy of 95.74% and
well-performed AUC of 98.59% compared with the existent literature of related
research. In order to make best use of the important features in the dataset,
we propose a technique named "synthetic generation" to develop more combined
features based on arithmetic operation, which ends up improving the accuracy
and AUC of original CatBoost model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Liexing Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoxue Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval. (arXiv:2104.01894v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01894</id>
        <link href="http://arxiv.org/abs/2104.01894"/>
        <updated>2021-06-16T01:21:08.419Z</updated>
        <summary type="html"><![CDATA[Speech-based image retrieval has been studied as a proxy for joint
representation learning, usually without emphasis on retrieval itself. As such,
it is unclear how well speech-based retrieval can work in practice -- both in
an absolute sense and versus alternative strategies that combine automatic
speech recognition (ASR) with strong text encoders. In this work, we
extensively study and expand choices of encoder architectures, training
methodology (including unimodal and multimodal pretraining), and other factors.
Our experiments cover different types of speech in three datasets: Flickr
Audio, Places Audio, and Localized Narratives. Our best model configuration
achieves large gains over state of the art, e.g., pushing recall-at-one from
21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also
show our best speech-based models can match or exceed cascaded ASR-to-text
encoding when speech is spontaneous, accented, or otherwise hard to
automatically transcribe.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1"&gt;Ramon Sanabria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1"&gt;Austin Waters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1"&gt;Jason Baldridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07847</id>
        <link href="http://arxiv.org/abs/2106.07847"/>
        <updated>2021-06-16T01:21:08.412Z</updated>
        <summary type="html"><![CDATA[We study transfer learning in the presence of spurious correlations. We
experimentally demonstrate that directly transferring the stable feature
extractor learned on the source task may not eliminate these biases for the
target task. However, we hypothesize that the unstable features in the source
task and those in the target task are directly related. By explicitly informing
the target classifier of the source task's unstable features, we can regularize
the biases in the target task. Specifically, we derive a representation that
encodes the unstable features by contrasting different data environments in the
source task. On the target task, we cluster data from this representation, and
achieve robustness by minimizing the worst-case risk across all clusters. We
evaluate our method on both text and image classifications. Empirical results
demonstrate that our algorithm is able to maintain robustness on the target
task, outperforming the best baseline by 22.9% in absolute accuracy across 12
transfer settings. Our code is available at https://github.com/YujiaBao/Tofu.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1"&gt;Yujia Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepKoCo: Efficient latent planning with a robust Koopman representation. (arXiv:2011.12690v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12690</id>
        <link href="http://arxiv.org/abs/2011.12690"/>
        <updated>2021-06-16T01:21:08.391Z</updated>
        <summary type="html"><![CDATA[This paper presents DeepKoCo, a novel model-based agent that learns a latent
Koopman representation from images. This representation allows DeepKoCo to plan
efficiently using linear control methods, such as linear model predictive
control. Compared to traditional agents, DeepKoCo is robust to task-irrelevant
dynamics, thanks to the use of a tailored lossy autoencoder network that allows
DeepKoCo to learn latent dynamics that reconstruct and predict only observed
costs, rather than all observed dynamics. As our results show, DeepKoCo
achieves a similar final performance as traditional model-free methods on
complex control tasks, while being considerably more robust to distractor
dynamics, making the proposed agent more amenable for real-life applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heijden_B/0/1/0/all/0/1"&gt;Bas van der Heijden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferranti_L/0/1/0/all/0/1"&gt;Laura Ferranti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1"&gt;Jens Kober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1"&gt;Robert Babuska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation. (arXiv:2011.09757v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09757</id>
        <link href="http://arxiv.org/abs/2011.09757"/>
        <updated>2021-06-16T01:21:08.385Z</updated>
        <summary type="html"><![CDATA[Conventional unsupervised multi-source domain adaptation (UMDA) methods
assume all source domains can be accessed directly. This neglects the
privacy-preserving policy, that is, all the data and computations must be kept
decentralized. There exists three problems in this scenario: (1) Minimizing the
domain distance requires the pairwise calculation of the data from source and
target domains, which is not accessible. (2) The communication cost and privacy
security limit the application of UMDA methods (e.g., the domain adversarial
training). (3) Since users have no authority to check the data quality, the
irrelevant or malicious source domains are more likely to appear, which causes
negative transfer. In this study, we propose a privacy-preserving UMDA paradigm
named Knowledge Distillation based Decentralized Domain Adaptation (KD3A),
which performs domain adaptation through the knowledge distillation on models
from different source domains. KD3A solves the above problems with three
components: (1) A multi-source knowledge distillation method named Knowledge
Vote to learn high-quality domain consensus knowledge. (2) A dynamic weighting
strategy named Consensus Focus to identify both the malicious and irrelevant
domains. (3) A decentralized optimization strategy for domain distance named
BatchNorm MMD. The extensive experiments on DomainNet demonstrate that KD3A is
robust to the negative transfer and brings a 100x reduction of communication
cost compared with other decentralized UMDA methods. Moreover, our KD3A
significantly outperforms state-of-the-art UMDA approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1"&gt;Hao-Zhe Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1"&gt;Zhaoyang You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minghao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianye Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Minfeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Control Variates for Slate Off-Policy Evaluation. (arXiv:2106.07914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07914</id>
        <link href="http://arxiv.org/abs/2106.07914"/>
        <updated>2021-06-16T01:21:08.377Z</updated>
        <summary type="html"><![CDATA[We study the problem of off-policy evaluation from batched contextual bandit
data with multidimensional actions, often termed slates. The problem is common
to recommender systems and user-interface optimization, and it is particularly
challenging because of the combinatorially-sized action space. Swaminathan et
al. (2017) have proposed the pseudoinverse (PI) estimator under the assumption
that the conditional mean rewards are additive in actions. Using control
variates, we consider a large class of unbiased estimators that includes as
specific cases the PI estimator and (asymptotically) its self-normalized
variant. By optimizing over this class, we obtain new estimators with risk
improvement guarantees over both the PI and self-normalized PI estimators.
Experiments with real-world recommender data as well as synthetic data validate
these improvements in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vlassis_N/0/1/0/all/0/1"&gt;Nikos Vlassis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrashekar_A/0/1/0/all/0/1"&gt;Ashok Chandrashekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gil_F/0/1/0/all/0/1"&gt;Fernando Amat Gil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kallus_N/0/1/0/all/0/1"&gt;Nathan Kallus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ShadowNet: A Secure and Efficient System for On-device Model Inference. (arXiv:2011.05905v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05905</id>
        <link href="http://arxiv.org/abs/2011.05905"/>
        <updated>2021-06-16T01:21:08.371Z</updated>
        <summary type="html"><![CDATA[With the increased usage of AI accelerators on mobile and edge devices,
on-device machine learning (ML) is gaining popularity. Consequently, thousands
of proprietary ML models are being deployed on billions of untrusted devices.
This raises serious security concerns about model privacy. However, protecting
the model privacy without losing access to the AI accelerators is a challenging
problem. In this paper, we present a novel on-device model inference system,
ShadowNet. ShadowNet protects the model privacy with Trusted Execution
Environment (TEE) while securely outsourcing the heavy linear layers of the
model to the untrusted hardware accelerators. ShadowNet achieves this by
transforming the weights of the linear layers before outsourcing them and
restoring the results inside the TEE. The nonlinear layers are also kept secure
inside the TEE. The transformation of the weights and the restoration of the
results are designed in a way that can be implemented efficiently. We have
built a ShadowNet prototype based on TensorFlow Lite and applied it on four
popular CNNs, namely, MobileNets, ResNet-44, AlexNet and MiniVGG. Our
evaluation shows that ShadowNet achieves strong security guarantees with
reasonable performance, offering a practical solution for secure on-device
model inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhichuang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1"&gt;Ruimin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Changming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1"&gt;Amrita Roy Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Long Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Explanations as Interventions in Latent Space. (arXiv:2106.07754v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.07754</id>
        <link href="http://arxiv.org/abs/2106.07754"/>
        <updated>2021-06-16T01:21:08.364Z</updated>
        <summary type="html"><![CDATA[Explainable Artificial Intelligence (XAI) is a set of techniques that allows
the understanding of both technical and non-technical aspects of Artificial
Intelligence (AI) systems. XAI is crucial to help satisfying the increasingly
important demand of \emph{trustworthy} Artificial Intelligence, characterized
by fundamental characteristics such as respect of human autonomy, prevention of
harm, transparency, accountability, etc. Within XAI techniques, counterfactual
explanations aim to provide to end users a set of features (and their
corresponding values) that need to be changed in order to achieve a desired
outcome. Current approaches rarely take into account the feasibility of actions
needed to achieve the proposed explanations, and in particular they fall short
of considering the causal impact of such actions. In this paper, we present
Counterfactual Explanations as Interventions in Latent Space (CEILS), a
methodology to generate counterfactual explanations capturing by design the
underlying causal relations from the data, and at the same time to provide
feasible recommendations to reach the proposed profile. Moreover, our
methodology has the advantage that it can be set on top of existing
counterfactuals generator algorithms, thus minimising the complexity of
imposing additional causal constrains. We demonstrate the effectiveness of our
approach with a set of different experiments using synthetic and real datasets
(including a proprietary dataset of the financial domain).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Crupi_R/0/1/0/all/0/1"&gt;Riccardo Crupi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castelnovo_A/0/1/0/all/0/1"&gt;Alessandro Castelnovo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Regoli_D/0/1/0/all/0/1"&gt;Daniele Regoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_B/0/1/0/all/0/1"&gt;Beatriz San Miguel Gonzalez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased Sentence Encoder For Large-Scale Multi-lingual Search Engines. (arXiv:2106.07719v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07719</id>
        <link href="http://arxiv.org/abs/2106.07719"/>
        <updated>2021-06-16T01:21:08.343Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a multi-lingual sentence encoder that can be used
in search engines as a query and document encoder. This embedding enables a
semantic similarity score between queries and documents that can be an
important feature in document ranking and relevancy. To train such a customized
sentence encoder, it is beneficial to leverage users search data in the form of
query-document clicked pairs however, we must avoid relying too much on search
click data as it is biased and does not cover many unseen cases. The search
data is heavily skewed towards short queries and for long queries is small and
often noisy. The goal is to design a universal multi-lingual encoder that works
for all cases and covers both short and long queries. We select a number of
public NLI datasets in different languages and translation data and together
with user search data we train a language model using a multi-task approach. A
challenge is that these datasets are not homogeneous in terms of content, size
and the balance ratio. While the public NLI datasets are usually two-sentence
based with the same portion of positive and negative pairs, the user search
data can contain multi-sentence documents and only positive pairs. We show how
multi-task training enables us to leverage all these datasets and exploit
knowledge sharing across these tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1"&gt;Mahdi Hajiaghayi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1"&gt;Monir Hajiaghayi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolin_M/0/1/0/all/0/1"&gt;Mark Bolin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Local Stochastic Extra-Gradient for Variational Inequalities. (arXiv:2106.08315v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.08315</id>
        <link href="http://arxiv.org/abs/2106.08315"/>
        <updated>2021-06-16T01:21:08.337Z</updated>
        <summary type="html"><![CDATA[We consider decentralized stochastic variational inequalities where the
problem data is distributed across many participating devices (heterogeneous,
or non-IID data setting). We propose a novel method - based on stochastic
extra-gradient - where participating devices can communicate over arbitrary,
possibly time-varying network topologies. This covers both the fully
decentralized optimization setting and the centralized topologies commonly used
in Federated Learning. Our method further supports multiple local updates on
the workers for reducing the communication frequency between workers. We
theoretically analyze the proposed scheme in the strongly monotone, monotone
and non-monotone setting. As a special case, our method and analysis apply in
particular to decentralized stochastic min-max problems which are being studied
with increased interest in Deep Learning. For example, the training objective
of Generative Adversarial Networks (GANs) are typically saddle point problems
and the decentralized training of GANs has been reported to be extremely
challenging. While SOTA techniques rely on either repeated gossip rounds or
proximal updates, we alleviate both of these requirements. Experimental results
for decentralized GAN demonstrate the effectiveness of our proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Beznosikov_A/0/1/0/all/0/1"&gt;Aleksandr Beznosikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Dvurechensky_P/0/1/0/all/0/1"&gt;Pavel Dvurechensky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Koloskova_A/0/1/0/all/0/1"&gt;Anastasia Koloskova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Samokhin_V/0/1/0/all/0/1"&gt;Valentin Samokhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1"&gt;Alexander Gasnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topics to Avoid: Demoting Latent Confounds in Text Classification. (arXiv:1909.00453v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.00453</id>
        <link href="http://arxiv.org/abs/1909.00453"/>
        <updated>2021-06-16T01:21:08.330Z</updated>
        <summary type="html"><![CDATA[Despite impressive performance on many text classification tasks, deep neural
networks tend to learn frequent superficial patterns that are specific to the
training data and do not always generalize well. In this work, we observe this
limitation with respect to the task of native language identification. We find
that standard text classifiers which perform well on the test set end up
learning topical features which are confounds of the prediction task (e.g., if
the input text mentions Sweden, the classifier predicts that the author's
native language is Swedish). We propose a method that represents the latent
topical confounds and a model which "unlearns" confounding features by
predicting both the label of the input text and the confound; but we train the
two predictors adversarially in an alternating fashion to learn a text
representation that predicts the correct label but is less prone to using
information about the confound. We show that this model generalizes better and
learns features that are indicative of the writing style rather than the
content.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sachin Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wintner_S/0/1/0/all/0/1"&gt;Shuly Wintner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aggregating From Multiple Target-Shifted Sources. (arXiv:2105.04051v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04051</id>
        <link href="http://arxiv.org/abs/2105.04051"/>
        <updated>2021-06-16T01:21:08.323Z</updated>
        <summary type="html"><![CDATA[Multi-source domain adaptation aims at leveraging the knowledge from multiple
tasks for predicting a related target domain. Hence, a crucial aspect is to
properly combine different sources based on their relations. In this paper, we
analyzed the problem for aggregating source domains with different label
distributions, where most recent source selection approaches fail. Our proposed
algorithm differs from previous approaches in two key ways: the model
aggregates multiple sources mainly through the similarity of semantic
conditional distribution rather than marginal distribution; the model proposes
a \emph{unified} framework to select relevant sources for three popular
scenarios, i.e., domain adaptation with limited label on target domain,
unsupervised domain adaptation and label partial unsupervised domain adaption.
We evaluate the proposed method through extensive experiments. The empirical
results significantly outperform the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shui_C/0/1/0/all/0/1"&gt;Changjian Shui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zijian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiaqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1"&gt;Christian Gagn&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1"&gt;Charles Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Boyu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coping with Label Shift via Distributionally Robust Optimisation. (arXiv:2010.12230v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12230</id>
        <link href="http://arxiv.org/abs/2010.12230"/>
        <updated>2021-06-16T01:21:08.316Z</updated>
        <summary type="html"><![CDATA[The label shift problem refers to the supervised learning setting where the
train and test label distributions do not match. Existing work addressing label
shift usually assumes access to an \emph{unlabelled} test sample. This sample
may be used to estimate the test label distribution, and to then train a
suitably re-weighted classifier. While approaches using this idea have proven
effective, their scope is limited as it is not always feasible to access the
target domain; further, they require repeated retraining if the model is to be
deployed in \emph{multiple} test environments. Can one instead learn a
\emph{single} classifier that is robust to arbitrary label shifts from a broad
family? In this paper, we answer this question by proposing a model that
minimises an objective based on distributionally robust optimisation (DRO). We
then design and analyse a gradient descent-proximal mirror ascent algorithm
tailored for large-scale problems to optimise the proposed objective. %, and
establish its convergence. Finally, through experiments on CIFAR-100 and
ImageNet, we show that our technique can significantly improve performance over
a number of baselines in settings where label shift is present.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingzhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1"&gt;Andreas Veit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1"&gt;Srinadh Bhojanapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1"&gt;Suvrit Sra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dont Just Divide; Polarize and Conquer!. (arXiv:2102.11872v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11872</id>
        <link href="http://arxiv.org/abs/2102.11872"/>
        <updated>2021-06-16T01:21:08.297Z</updated>
        <summary type="html"><![CDATA[In data containing heterogeneous subpopulations, classification performance
benefits from incorporating the knowledge of cluster structure in the
classifier. Previous methods for such combined clustering and classification
are either 1) classifier-specific and not generic, or 2) independently perform
clustering and classifier training, which may not form clusters that can
potentially benefit classifier performance. The question of how to perform
clustering to improve the performance of classifiers trained on the clusters
has received scant attention in previous literature, despite its importance in
several real-world applications. In this paper, we design a simple and
efficient classification algorithm called Clustering Aware Classification
(CAC), to find clusters that are well suited for being used as training
datasets by classifiers for each underlying subpopulation. Our experiments on
synthetic and real benchmark datasets demonstrate the efficacy of CAC over
previous methods for combined clustering and classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Shivin Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1"&gt;Siddharth Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lingxiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_L/0/1/0/all/0/1"&gt;Lim Jun Heng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_V/0/1/0/all/0/1"&gt;Vaibhav Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting in the Presence of Massart Noise. (arXiv:2106.07779v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07779</id>
        <link href="http://arxiv.org/abs/2106.07779"/>
        <updated>2021-06-16T01:21:08.291Z</updated>
        <summary type="html"><![CDATA[We study the problem of boosting the accuracy of a weak learner in the
(distribution-independent) PAC model with Massart noise. In the Massart noise
model, the label of each example $x$ is independently misclassified with
probability $\eta(x) \leq \eta$, where $\eta<1/2$. The Massart model lies
between the random classification noise model and the agnostic model. Our main
positive result is the first computationally efficient boosting algorithm in
the presence of Massart noise that achieves misclassification error arbitrarily
close to $\eta$. Prior to our work, no non-trivial booster was known in this
setting. Moreover, we show that this error upper bound is best possible for
polynomial-time black-box boosters, under standard cryptographic assumptions.
Our upper and lower bounds characterize the complexity of boosting in the
distribution-independent PAC model with Massart noise. As a simple application
of our positive result, we give the first efficient Massart learner for unions
of high-dimensional rectangles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1"&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Impagliazzo_R/0/1/0/all/0/1"&gt;Russell Impagliazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1"&gt;Daniel Kane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_R/0/1/0/all/0/1"&gt;Rex Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorrell_J/0/1/0/all/0/1"&gt;Jessica Sorrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1"&gt;Christos Tzamos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When does gradient descent with logistic loss find interpolating two-layer networks?. (arXiv:2012.02409v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02409</id>
        <link href="http://arxiv.org/abs/2012.02409"/>
        <updated>2021-06-16T01:21:08.285Z</updated>
        <summary type="html"><![CDATA[We study the training of finite-width two-layer smoothed ReLU networks for
binary classification using the logistic loss. We show that gradient descent
drives the training loss to zero if the initial loss is small enough. When the
data satisfies certain cluster and separation conditions and the network is
wide enough, we show that one step of gradient descent reduces the loss
sufficiently that the first result applies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1"&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Long_P/0/1/0/all/0/1"&gt;Philip M. Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter L. Bartlett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph cuts always find a global optimum for Potts models (with a catch). (arXiv:2011.03639v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03639</id>
        <link href="http://arxiv.org/abs/2011.03639"/>
        <updated>2021-06-16T01:21:08.278Z</updated>
        <summary type="html"><![CDATA[We prove that the $\alpha$-expansion algorithm for MAP inference always
returns a globally optimal assignment for Markov Random Fields with Potts
pairwise potentials, with a catch: the returned assignment is only guaranteed
to be optimal for an instance within a small perturbation of the original
problem instance. In other words, all local minima with respect to expansion
moves are global minima to slightly perturbed versions of the problem. On
"real-world" instances, MAP assignments of small perturbations of the problem
should be very similar to the MAP assignment(s) of the original problem
instance. We design an algorithm that can certify whether this is the case in
practice. On several MAP inference problem instances from computer vision, this
algorithm certifies that MAP solutions to all of these perturbations are very
close to solutions of the original instance. These results taken together give
a cohesive explanation for the good performance of "graph cuts" algorithms in
practice. Every local expansion minimum is a global minimum in a small
perturbation of the problem, and all of these global minima are close to the
original solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lang_H/0/1/0/all/0/1"&gt;Hunter Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sontag_D/0/1/0/all/0/1"&gt;David Sontag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vijayaraghavan_A/0/1/0/all/0/1"&gt;Aravindan Vijayaraghavan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Video Captioning Through Latent Variable Expansion. (arXiv:1910.12019v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.12019</id>
        <link href="http://arxiv.org/abs/1910.12019"/>
        <updated>2021-06-16T01:21:08.269Z</updated>
        <summary type="html"><![CDATA[Automatically describing video content with text description is challenging
but important task, which has been attracting a lot of attention in computer
vision community. Previous works mainly strive for the accuracy of the
generated sentences, while ignoring the sentences diversity, which is
inconsistent with human behavior. In this paper, we aim to caption each video
with multiple descriptions and propose a novel framework. Concretely, for a
given video, the intermediate latent variables of conventional encode-decode
process are utilized as input to the conditional generative adversarial network
(CGAN) with the purpose of generating diverse sentences. We adopt different
Convolutional Neural Networks (CNNs) as our generator that produces
descriptions conditioned on latent variables and discriminator that assesses
the quality of generated sentences. Simultaneously, a novel DCE metric is
designed to assess the diverse captions. We evaluate our method on the
benchmark datasets, where it demonstrates its ability to generate diverse
descriptions and achieves superior results against other state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Huanhou Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jinglun Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning-based conditional mean filter: a generalization of the ensemble Kalman filter for nonlinear data assimilation. (arXiv:2106.07908v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07908</id>
        <link href="http://arxiv.org/abs/2106.07908"/>
        <updated>2021-06-16T01:21:08.252Z</updated>
        <summary type="html"><![CDATA[Filtering is a data assimilation technique that performs the sequential
inference of dynamical systems states from noisy observations. Herein, we
propose a machine learning-based ensemble conditional mean filter (ML-EnCMF)
for tracking possibly high-dimensional non-Gaussian state models with nonlinear
dynamics based on sparse observations. The proposed filtering method is
developed based on the conditional expectation and numerically implemented
using machine learning (ML) techniques combined with the ensemble method. The
contribution of this work is twofold. First, we demonstrate that the ensembles
assimilated using the ensemble conditional mean filter (EnCMF) provide an
unbiased estimator of the Bayesian posterior mean, and their variance matches
the expected conditional variance. Second, we implement the EnCMF using
artificial neural networks, which have a significant advantage in representing
nonlinear functions over high-dimensional domains such as the conditional mean.
Finally, we demonstrate the effectiveness of the ML-EnCMF for tracking the
states of Lorenz-63 and Lorenz-96 systems under the chaotic regime. Numerical
results show that the ML-EnCMF outperforms the ensemble Kalman filter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1"&gt;Truong-Vinh Hoang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Krumscheid_S/0/1/0/all/0/1"&gt;Sebastian Krumscheid&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Matthies_H/0/1/0/all/0/1"&gt;Hermann G. Matthies&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Tempone_R/0/1/0/all/0/1"&gt;Ra&amp;#xfa;l Tempone&lt;/a&gt; (1 and 3) ((1) Chair of Mathematics for Uncertainty Quantification, RWTH Aachen University, (2) Technische Universit&amp;#xe4;t Braunschweig (3) Computer, Electrical and Mathematical Sciences and Engineering, KAUST, and Alexander von Humboldt professor in Mathematics of Uncertainty Quantification, RWTH Aachen University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Neural Tangent Kernels via Sketching and Random Features. (arXiv:2106.07880v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07880</id>
        <link href="http://arxiv.org/abs/2106.07880"/>
        <updated>2021-06-16T01:21:08.245Z</updated>
        <summary type="html"><![CDATA[The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely-wide
neural networks trained under least squares loss by gradient descent. Recent
works also report that NTK regression can outperform finitely-wide neural
networks trained on small-scale datasets. However, the computational complexity
of kernel methods has limited its use in large-scale learning tasks. To
accelerate learning with NTK, we design a near input-sparsity time
approximation algorithm for NTK, by sketching the polynomial expansions of
arc-cosine kernels: our sketch for the convolutional counterpart of NTK (CNTK)
can transform any image using a linear runtime in the number of pixels.
Furthermore, we prove a spectral approximation guarantee for the NTK matrix, by
combining random features (based on leverage score sampling) of the arc-cosine
kernels with a sketching algorithm. We benchmark our methods on various
large-scale regression and classification tasks and show that a linear
regressor trained on our CNTK features matches the accuracy of exact CNTK on
CIFAR-10 dataset while achieving 150x speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zandieh_A/0/1/0/all/0/1"&gt;Amir Zandieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_I/0/1/0/all/0/1"&gt;Insu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avron_H/0/1/0/all/0/1"&gt;Haim Avron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoham_N/0/1/0/all/0/1"&gt;Neta Shoham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1"&gt;Chaewon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Domain Knowledge into Health Recommender Systems using Hyperbolic Embeddings. (arXiv:2106.07720v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07720</id>
        <link href="http://arxiv.org/abs/2106.07720"/>
        <updated>2021-06-16T01:21:08.228Z</updated>
        <summary type="html"><![CDATA[In contrast to many other domains, recommender systems in health services may
benefit particularly from the incorporation of health domain knowledge, as it
helps to provide meaningful and personalised recommendations catering to the
individual's health needs. With recent advances in representation learning
enabling the hierarchical embedding of health knowledge into the hyperbolic
Poincare space, this work proposes a content-based recommender system for
patient-doctor matchmaking in primary care based on patients' health profiles,
enriched by pre-trained Poincare embeddings of the ICD-9 codes through transfer
learning. The proposed model outperforms its conventional counterpart in terms
of recommendation accuracy and has several important business implications for
improving the patient-doctor relationship.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peito_J/0/1/0/all/0/1"&gt;Joel Peito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1"&gt;Qiwei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Analytical Theory of Curriculum Learning in Teacher-Student Networks. (arXiv:2106.08068v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08068</id>
        <link href="http://arxiv.org/abs/2106.08068"/>
        <updated>2021-06-16T01:21:08.207Z</updated>
        <summary type="html"><![CDATA[In humans and animals, curriculum learning -- presenting data in a curated
order - is critical to rapid learning and effective pedagogy. Yet in machine
learning, curricula are not widely used and empirically often yield only
moderate benefits. This stark difference in the importance of curriculum raises
a fundamental theoretical question: when and why does curriculum learning help?

In this work, we analyse a prototypical neural network model of curriculum
learning in the high-dimensional limit, employing statistical physics methods.
Curricula could in principle change both the learning speed and asymptotic
performance of a model. To study the former, we provide an exact description of
the online learning setting, confirming the long-standing experimental
observation that curricula can modestly speed up learning. To study the latter,
we derive performance in a batch learning setting, in which a network trains to
convergence in successive phases of learning on dataset slices of varying
difficulty. With standard training losses, curriculum does not provide
generalisation benefit, in line with empirical observations. However, we show
that by connecting different learning phases through simple Gaussian priors,
curriculum can yield a large improvement in test performance. Taken together,
our reduced analytical descriptions help reconcile apparently conflicting
empirical results and trace regimes where curriculum learning yields the
largest gains. More broadly, our results suggest that fully exploiting a
curriculum may require explicit changes to the loss function at curriculum
boundaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saglietti_L/0/1/0/all/0/1"&gt;Luca Saglietti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannelli_S/0/1/0/all/0/1"&gt;Stefano Sarao Mannelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1"&gt;Andrew Saxe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unique sparse decomposition of low rank matrices. (arXiv:2106.07736v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.07736</id>
        <link href="http://arxiv.org/abs/2106.07736"/>
        <updated>2021-06-16T01:21:08.196Z</updated>
        <summary type="html"><![CDATA[The problem of finding the unique low dimensional decomposition of a given
matrix has been a fundamental and recurrent problem in many areas. In this
paper, we study the problem of seeking a unique decomposition of a low rank
matrix $Y\in \mathbb{R}^{p\times n}$ that admits a sparse representation.
Specifically, we consider $Y = A X\in \mathbb{R}^{p\times n}$ where the matrix
$A\in \mathbb{R}^{p\times r}$ has full column rank, with $r < \min\{n,p\}$, and
the matrix $X\in \mathbb{R}^{r\times n}$ is element-wise sparse. We prove that
this sparse decomposition of $Y$ can be uniquely identified, up to some
intrinsic signed permutation. Our approach relies on solving a nonconvex
optimization problem constrained over the unit sphere. Our geometric analysis
for the nonconvex optimization landscape shows that any {\em strict} local
solution is close to the ground truth solution, and can be recovered by a
simple data-driven initialization followed with any second order descent
algorithm. At last, we corroborate these theoretical results with numerical
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Jin_D/0/1/0/all/0/1"&gt;Dian Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bing_X/0/1/0/all/0/1"&gt;Xin Bing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuqian Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Margins for Instance Reweighting in Adversarial Training. (arXiv:2106.07904v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07904</id>
        <link href="http://arxiv.org/abs/2106.07904"/>
        <updated>2021-06-16T01:21:08.190Z</updated>
        <summary type="html"><![CDATA[Reweighting adversarial data during training has been recently shown to
improve adversarial robustness, where data closer to the current decision
boundaries are regarded as more critical and given larger weights. However,
existing methods measuring the closeness are not very reliable: they are
discrete and can take only a few values, and they are path-dependent, i.e.,
they may change given the same start and end points with different attack
paths. In this paper, we propose three types of probabilistic margin (PM),
which are continuous and path-independent, for measuring the aforementioned
closeness and reweighting adversarial data. Specifically, a PM is defined as
the difference between two estimated class-posterior probabilities, e.g., such
the probability of the true label minus the probability of the most confusing
label given some natural data. Though different PMs capture different geometric
properties, all three PMs share a negative correlation with the vulnerability
of data: data with larger/smaller PMs are safer/riskier and should have
smaller/larger weights. Experiments demonstrate that PMs are reliable
measurements and PM-based reweighting methods outperform state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qizhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chen Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pitfalls of Explainable ML: An Industry Perspective. (arXiv:2106.07758v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07758</id>
        <link href="http://arxiv.org/abs/2106.07758"/>
        <updated>2021-06-16T01:21:08.116Z</updated>
        <summary type="html"><![CDATA[As machine learning (ML) systems take a more prominent and central role in
contributing to life-impacting decisions, ensuring their trustworthiness and
accountability is of utmost importance. Explanations sit at the core of these
desirable attributes of a ML system. The emerging field is frequently called
``Explainable AI (XAI)'' or ``Explainable ML.'' The goal of explainable ML is
to intuitively explain the predictions of a ML system, while adhering to the
needs to various stakeholders. Many explanation techniques were developed with
contributions from both academia and industry. However, there are several
existing challenges that have not garnered enough interest and serve as
roadblocks to widespread adoption of explainable ML. In this short paper, we
enumerate challenges in explainable ML from an industry perspective. We hope
these challenges will serve as promising future research directions, and would
contribute to democratizing explainable ML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1"&gt;Sahil Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahiri_A/0/1/0/all/0/1"&gt;Aditya Lahiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1"&gt;John P. Dickerson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Su-In Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting. (arXiv:2106.07677v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07677</id>
        <link href="http://arxiv.org/abs/2106.07677"/>
        <updated>2021-06-16T01:21:08.108Z</updated>
        <summary type="html"><![CDATA[Restless and collapsing bandits are commonly used to model constrained
resource allocation in settings featuring arms with action-dependent transition
probabilities, such as allocating health interventions among patients [Whittle,
1988; Mate et al., 2020]. However, state-of-the-art Whittle-index-based
approaches to this planning problem either do not consider fairness among arms,
or incentivize fairness without guaranteeing it [Mate et al., 2021].
Additionally, their optimality guarantees only apply when arms are indexable
and threshold-optimal. We demonstrate that the incorporation of hard fairness
constraints necessitates the coupling of arms, which undermines the
tractability, and by extension, indexability of the problem. We then introduce
ProbFair, a probabilistically fair stationary policy that maximizes total
expected reward and satisfies the budget constraint, while ensuring a strictly
positive lower bound on the probability of being pulled at each timestep. We
evaluate our algorithm on a real-world application, where interventions support
continuous positive airway pressure (CPAP) therapy adherence among obstructive
sleep apnea (OSA) patients, as well as simulations on a broader class of
synthetic transition matrices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Herlihy_C/0/1/0/all/0/1"&gt;Christine Herlihy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prins_A/0/1/0/all/0/1"&gt;Aviva Prins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1"&gt;Aravind Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1"&gt;John Dickerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing Structural Regularities of Labeled Data in Overparameterized Models. (arXiv:2002.03206v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.03206</id>
        <link href="http://arxiv.org/abs/2002.03206"/>
        <updated>2021-06-16T01:21:08.101Z</updated>
        <summary type="html"><![CDATA[Humans are accustomed to environments that contain both regularities and
exceptions. For example, at most gas stations, one pays prior to pumping, but
the occasional rural station does not accept payment in advance. Likewise, deep
neural networks can generalize across instances that share common patterns or
structures, yet have the capacity to memorize rare or irregular forms. We
analyze how individual instances are treated by a model via a consistency
score. The score characterizes the expected accuracy for a held-out instance
given training sets of varying size sampled from the data distribution. We
obtain empirical estimates of this score for individual instances in multiple
data sets, and we show that the score identifies out-of-distribution and
mislabeled examples at one end of the continuum and strongly regular examples
at the other end. We identify computationally inexpensive proxies to the
consistency score using statistics collected during training. We show examples
of potential applications to the analysis of deep-learning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Ziheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chiyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1"&gt;Kunal Talwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1"&gt;Michael C. Mozer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition. (arXiv:2104.09106v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09106</id>
        <link href="http://arxiv.org/abs/2104.09106"/>
        <updated>2021-06-16T01:21:08.065Z</updated>
        <summary type="html"><![CDATA[Subword units are commonly used for end-to-end automatic speech recognition
(ASR), while a fully acoustic-oriented subword modeling approach is somewhat
missing. We propose an acoustic data-driven subword modeling (ADSM) approach
that adapts the advantages of several text-based and acoustic-based subword
methods into one pipeline. With a fully acoustic-oriented label design and
learning process, ADSM produces acoustic-structured subword units and
acoustic-matched target sequence for further ASR training. The obtained ADSM
labels are evaluated with different end-to-end ASR approaches including CTC,
RNN-Transducer and attention models. Experiments on the LibriSpeech corpus show
that ADSM clearly outperforms both byte pair encoding (BPE) and
pronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis
shows that ADSM achieves acoustically more logical word segmentation and more
balanced sequence length, and thus, is suitable for both time-synchronous and
label-synchronous models. We also briefly describe how to apply acoustic-based
subword regularization and unseen text segmentation using ADSM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1"&gt;Mohammad Zeineldeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zuoyun Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear-Time Probabilistic Solutions of Boundary Value Problems. (arXiv:2106.07761v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.07761</id>
        <link href="http://arxiv.org/abs/2106.07761"/>
        <updated>2021-06-16T01:21:08.028Z</updated>
        <summary type="html"><![CDATA[We propose a fast algorithm for the probabilistic solution of boundary value
problems (BVPs), which are ordinary differential equations subject to boundary
conditions. In contrast to previous work, we introduce a Gauss--Markov prior
and tailor it specifically to BVPs, which allows computing a posterior
distribution over the solution in linear time, at a quality and cost comparable
to that of well-established, non-probabilistic methods. Our model further
delivers uncertainty quantification, mesh refinement, and hyperparameter
adaptation. We demonstrate how these practical considerations positively impact
the efficiency of the scheme. Altogether, this results in a practically usable
probabilistic BVP solver that is (in contrast to non-probabilistic algorithms)
natively compatible with other parts of the statistical modelling tool-chain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kramer_N/0/1/0/all/0/1"&gt;Nicholas Kr&amp;#xe4;mer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hennig_P/0/1/0/all/0/1"&gt;Philipp Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question Answering Infused Pre-training of General-Purpose Contextualized Representations. (arXiv:2106.08190v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08190</id>
        <link href="http://arxiv.org/abs/2106.08190"/>
        <updated>2021-06-16T01:21:08.021Z</updated>
        <summary type="html"><![CDATA[This paper proposes a pre-training objective based on question answering (QA)
for learning general-purpose contextual representations, motivated by the
intuition that the representation of a phrase in a passage should encode all
questions that the phrase can answer in context. We accomplish this goal by
training a bi-encoder QA model, which independently encodes passages and
questions, to match the predictions of a more accurate cross-encoder model on
80 million synthesized QA pairs. By encoding QA-relevant information, the
bi-encoder's token-level representations are useful for non-QA downstream tasks
without extensive (or in some cases, any) fine-tuning. We show large
improvements over both RoBERTa-large and previous state-of-the-art results on
zero-shot and few-shot paraphrase detection on four datasets, few-shot named
entity recognition on two datasets, and zero-shot sentiment analysis on three
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Robin Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Mike Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OCHADAI-KYOTO at SemEval-2021 Task 1: Enhancing Model Generalization and Robustness for Lexical Complexity Prediction. (arXiv:2105.05535v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05535</id>
        <link href="http://arxiv.org/abs/2105.05535"/>
        <updated>2021-06-16T01:21:08.014Z</updated>
        <summary type="html"><![CDATA[We propose an ensemble model for predicting the lexical complexity of words
and multiword expressions (MWEs). The model receives as input a sentence with a
target word or MWEand outputs its complexity score. Given that a key challenge
with this task is the limited size of annotated data, our model relies on
pretrained contextual representations from different state-of-the-art
transformer-based language models (i.e., BERT and RoBERTa), and on a variety of
training methods for further enhancing model generalization and
robustness:multi-step fine-tuning and multi-task learning, and adversarial
training. Additionally, we propose to enrich contextual representations by
adding hand-crafted features during training. Our model achieved competitive
results and ranked among the top-10 systems in both sub-tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taya_Y/0/1/0/all/0/1"&gt;Yuki Taya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_L/0/1/0/all/0/1"&gt;Lis Kanashiro Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1"&gt;Fei Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_I/0/1/0/all/0/1"&gt;Ichiro Kobayashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistency Regularization for Cross-Lingual Fine-Tuning. (arXiv:2106.08226v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08226</id>
        <link href="http://arxiv.org/abs/2106.08226"/>
        <updated>2021-06-16T01:21:08.008Z</updated>
        <summary type="html"><![CDATA[Fine-tuning pre-trained cross-lingual language models can transfer
task-specific supervision from one language to the others. In this work, we
propose to improve cross-lingual fine-tuning with consistency regularization.
Specifically, we use example consistency regularization to penalize the
prediction sensitivity to four types of data augmentations, i.e., subword
sampling, Gaussian noise, code-switch substitution, and machine translation. In
addition, we employ model consistency to regularize the models trained with two
augmented versions of the same training set. Experimental results on the XTREME
benchmark show that our method significantly improves cross-lingual fine-tuning
across various tasks, including text classification, question answering, and
sequence labeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Bo Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shaohan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1"&gt;Zewen Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1"&gt;Saksham Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1"&gt;Wanxiang Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xia Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynthASR: Unlocking Synthetic Data for Speech Recognition. (arXiv:2106.07803v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07803</id>
        <link href="http://arxiv.org/abs/2106.07803"/>
        <updated>2021-06-16T01:21:08.001Z</updated>
        <summary type="html"><![CDATA[End-to-end (E2E) automatic speech recognition (ASR) models have recently
demonstrated superior performance over the traditional hybrid ASR models.
Training an E2E ASR model requires a large amount of data which is not only
expensive but may also raise dependency on production data. At the same time,
synthetic speech generated by the state-of-the-art text-to-speech (TTS) engines
has advanced to near-human naturalness. In this work, we propose to utilize
synthetic speech for ASR training (SynthASR) in applications where data is
sparse or hard to get for ASR model training. In addition, we apply continual
learning with a novel multi-stage training strategy to address catastrophic
forgetting, achieved by a mix of weighted multi-style training, data
augmentation, encoder freezing, and parameter regularization. In our
experiments conducted on in-house datasets for a new application of recognizing
medication names, training ASR RNN-T models with synthetic audio via the
proposed multi-stage training improved the recognition performance on new
application by more than 65% relative, without degradation on existing general
applications. Our observations show that SynthASR holds great promise in
training the state-of-the-art large-scale E2E ASR models for new applications
while reducing the costs and dependency on production data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fazel_A/0/1/0/all/0/1"&gt;Amin Fazel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yulan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barra_Chicote_R/0/1/0/all/0/1"&gt;Roberto Barra-Chicote&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1"&gt;Yixiong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maas_R/0/1/0/all/0/1"&gt;Roland Maas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1"&gt;Jasha Droppo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Abstractive Opinion Summarization by Generating Sentences with Tree-Structured Topic Guidance. (arXiv:2106.08007v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08007</id>
        <link href="http://arxiv.org/abs/2106.08007"/>
        <updated>2021-06-16T01:21:07.994Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel unsupervised abstractive summarization method for
opinionated texts. While the basic variational autoencoder-based models assume
a unimodal Gaussian prior for the latent code of sentences, we alternate it
with a recursive Gaussian mixture, where each mixture component corresponds to
the latent code of a topic sentence and is mixed by a tree-structured topic
distribution. By decoding each Gaussian component, we generate sentences with
tree-structured topic guidance, where the root sentence conveys generic
content, and the leaf sentences describe specific topics. Experimental results
demonstrate that the generated topic sentences are appropriate as a summary of
opinionated texts, which are more informative and cover more input contents
than those generated by the recent unsupervised summarization model
(Bra\v{z}inskas et al., 2020). Furthermore, we demonstrate that the variance of
latent Gaussians represents the granularity of sentences, analogous to Gaussian
word embedding (Vilnis and McCallum, 2015).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Isonuma_M/0/1/0/all/0/1"&gt;Masaru Isonuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_J/0/1/0/all/0/1"&gt;Junichiro Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1"&gt;Danushka Bollegala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakata_I/0/1/0/all/0/1"&gt;Ichiro Sakata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Learning from Activity Description. (arXiv:2102.07024v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07024</id>
        <link href="http://arxiv.org/abs/2102.07024"/>
        <updated>2021-06-16T01:21:07.976Z</updated>
        <summary type="html"><![CDATA[We present a novel interactive learning protocol that enables training
request-fulfilling agents by verbally describing their activities. Unlike
imitation learning (IL), our protocol allows the teaching agent to provide
feedback in a language that is most appropriate for them. Compared with reward
in reinforcement learning (RL), the description feedback is richer and allows
for improved sample complexity. We develop a probabilistic framework and an
algorithm that practically implements our protocol. Empirical results in two
challenging request-fulfilling problems demonstrate the strengths of our
approach: compared with RL baselines, it is more sample-efficient; compared
with IL baselines, it achieves competitive success rates without requiring the
teaching agent to be able to demonstrate the desired behavior using the
learning agent's actions. Apart from empirical evaluation, we also provide
theoretical guarantees for our algorithm under certain assumptions about the
teacher and the environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khanh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schapire_R/0/1/0/all/0/1"&gt;Robert Schapire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1"&gt;Miro Dud&amp;#xed;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafto_P/0/1/0/all/0/1"&gt;Patrick Shafto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11066</id>
        <link href="http://arxiv.org/abs/2010.11066"/>
        <updated>2021-06-16T01:21:07.968Z</updated>
        <summary type="html"><![CDATA[Spoken conversational question answering (SCQA) requires machines to model
complex dialogue flow given the speech utterances and text corpora. Different
from traditional text question answering (QA) tasks, SCQA involves audio signal
processing, passage comprehension, and contextual understanding. However, ASR
systems introduce unexpected noisy signals to the transcriptions, which result
in performance degradation on SCQA. To overcome the problem, we propose CADNet,
a novel contextualized attention-based distillation approach, which applies
both cross-attention and self-attention to obtain ASR-robust contextualized
embedding representations of the passage and dialogue history for performance
improvements. We also introduce the spoken conventional knowledge distillation
framework to distill the ASR-robust knowledge from the estimated probabilities
of the teacher model to the student. We conduct extensive experiments on the
Spoken-CoQA dataset and demonstrate that our approach achieves remarkable
performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Label Propagation for Semi-Supervised Speaker Identification. (arXiv:2106.08207v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08207</id>
        <link href="http://arxiv.org/abs/2106.08207"/>
        <updated>2021-06-16T01:21:07.961Z</updated>
        <summary type="html"><![CDATA[Speaker identification in the household scenario (e.g., for smart speakers)
is typically based on only a few enrollment utterances but a much larger set of
unlabeled data, suggesting semisupervised learning to improve speaker profiles.
We propose a graph-based semi-supervised learning approach for speaker
identification in the household scenario, to leverage the unlabeled speech
samples. In contrast to most of the works in speaker recognition that focus on
speaker-discriminative embeddings, this work focuses on speaker label inference
(scoring). Given a pre-trained embedding extractor, graph-based learning allows
us to integrate information about both labeled and unlabeled utterances.
Considering each utterance as a graph node, we represent pairwise utterance
similarity scores as edge weights. Graphs are constructed per household, and
speaker identities are propagated to unlabeled nodes to optimize a global
consistency criterion. We show in experiments on the VoxCeleb dataset that this
approach makes effective use of unlabeled data and improves speaker
identification accuracy compared to two state-of-the-art scoring methods as
well as their semi-supervised variants based on pseudo-labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_V/0/1/0/all/0/1"&gt;Venkatesh Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1"&gt;Andreas Stolcke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Commonsense Cues in BERT for Solving Commonsense Tasks. (arXiv:2008.03945v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03945</id>
        <link href="http://arxiv.org/abs/2008.03945"/>
        <updated>2021-06-16T01:21:07.944Z</updated>
        <summary type="html"><![CDATA[BERT has been used for solving commonsense tasks such as CommonsenseQA. While
prior research has found that BERT does contain commonsense information to some
extent, there has been work showing that pre-trained models can rely on
spurious associations (e.g., data bias) rather than key cues in solving
sentiment classification and other problems. We quantitatively investigate the
presence of structural commonsense cues in BERT when solving commonsense tasks,
and the importance of such cues for the model prediction. Using two different
measures, we find that BERT does use relevant knowledge for solving the task,
and the presence of commonsense knowledge is positively correlated to the model
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Leyang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Sijie Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Word Sense Disambiguation in Neural Language Models. (arXiv:2106.07967v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07967</id>
        <link href="http://arxiv.org/abs/2106.07967"/>
        <updated>2021-06-16T01:21:07.936Z</updated>
        <summary type="html"><![CDATA[We present two supervised (pre-)training methods to incorporate gloss
definitions from lexical resources into neural language models (LMs). The
training improves our models' performance for Word Sense Disambiguation (WSD)
but also benefits general language understanding tasks while adding almost no
parameters. We evaluate our techniques with seven different neural LMs and find
that XLNet is more suitable for WSD than BERT. Our best-performing methods
exceeds state-of-the-art WSD techniques on the SemCor 3.0 dataset by 0.5% F1
and increase BERT's performance on the GLUE benchmark by 1.1% on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1"&gt;Jan Philip Wahle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1"&gt;Terry Ruas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1"&gt;Norman Meuschke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1"&gt;Bela Gipp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges and Considerations with Code-Mixed NLP for Multilingual Societies. (arXiv:2106.07823v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07823</id>
        <link href="http://arxiv.org/abs/2106.07823"/>
        <updated>2021-06-16T01:21:07.925Z</updated>
        <summary type="html"><![CDATA[Multilingualism refers to the high degree of proficiency in two or more
languages in the written and oral communication modes. It often results in
language mixing, a.k.a. code-mixing, when a multilingual speaker switches
between multiple languages in a single utterance of a text or speech. This
paper discusses the current state of the NLP research, limitations, and
foreseeable pitfalls in addressing five real-world applications for social good
crisis management, healthcare, political campaigning, fake news, and hate
speech for multilingual societies. We also propose futuristic datasets, models,
and tools that can significantly advance the current research in multilingual
NLP applications for the societal good. As a representative example, we
consider English-Hindi code-mixing but draw similar inferences for other
language pairs]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1"&gt;Vivek Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Representation and Inference for NLP. (arXiv:2106.08117v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08117</id>
        <link href="http://arxiv.org/abs/2106.08117"/>
        <updated>2021-06-16T01:21:07.918Z</updated>
        <summary type="html"><![CDATA[Semantic representation and inference is essential for Natural Language
Processing (NLP). The state of the art for semantic representation and
inference is deep learning, and particularly Recurrent Neural Networks (RNNs),
Convolutional Neural Networks (CNNs), and transformer Self-Attention models.
This thesis investigates the use of deep learning for novel semantic
representation and inference, and makes contributions in the following three
areas: creating training data, improving semantic representations and extending
inference learning. In terms of creating training data, we contribute the
largest publicly available dataset of real-life factual claims for the purpose
of automatic claim verification (MultiFC), and we present a novel inference
model composed of multi-scale CNNs with different kernel sizes that learn from
external sources to infer fact checking labels. In terms of improving semantic
representations, we contribute a novel model that captures non-compositional
semantic indicators. By definition, the meaning of a non-compositional phrase
cannot be inferred from the individual meanings of its composing words (e.g.,
hot dog). Motivated by this, we operationalize the compositionality of a phrase
contextually by enriching the phrase representation with external word
embeddings and knowledge graphs. Finally, in terms of inference learning, we
propose a series of novel deep learning architectures that improve inference by
using syntactic dependencies, by ensembling role guided attention heads,
incorporating gating layers, and concatenating multiple heads in novel and
effective ways. This thesis consists of seven publications (five published and
two under review).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dongsheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARTA: Collection and Classification of Ambiguous Requests and Thoughtful Actions. (arXiv:2106.07999v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07999</id>
        <link href="http://arxiv.org/abs/2106.07999"/>
        <updated>2021-06-16T01:21:07.911Z</updated>
        <summary type="html"><![CDATA[Human-assisting systems such as dialogue systems must take thoughtful,
appropriate actions not only for clear and unambiguous user requests, but also
for ambiguous user requests, even if the users themselves are not aware of
their potential requirements. To construct such a dialogue agent, we collected
a corpus and developed a model that classifies ambiguous user requests into
corresponding system actions. In order to collect a high-quality corpus, we
asked workers to input antecedent user requests whose pre-defined actions could
be regarded as thoughtful. Although multiple actions could be identified as
thoughtful for a single user request, annotating all combinations of user
requests and system actions is impractical. For this reason, we fully annotated
only the test data and left the annotation of the training data incomplete. In
order to train the classification model on such training data, we applied the
positive/unlabeled (PU) learning method, which assumes that only a part of the
data is labeled with positive examples. The experimental results show that the
PU learning method achieved better performance than the general
positive/negative (PN) learning method to classify thoughtful actions given an
ambiguous user request.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_S/0/1/0/all/0/1"&gt;Shohei Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshino_K/0/1/0/all/0/1"&gt;Koichiro Yoshino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1"&gt;Katsuhito Sudoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1"&gt;Satoshi Nakamura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Explanations for Machine Learning: Challenges Revisited. (arXiv:2106.07756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07756</id>
        <link href="http://arxiv.org/abs/2106.07756"/>
        <updated>2021-06-16T01:21:07.904Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations (CFEs) are an emerging technique under the
umbrella of interpretability of machine learning (ML) models. They provide
``what if'' feedback of the form ``if an input datapoint were $x'$ instead of
$x$, then an ML model's output would be $y'$ instead of $y$.'' Counterfactual
explainability for ML models has yet to see widespread adoption in industry. In
this short paper, we posit reasons for this slow uptake. Leveraging recent work
outlining desirable properties of CFEs and our experience running the ML wing
of a model monitoring startup, we identify outstanding obstacles hindering CFE
deployment in industry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1"&gt;Sahil Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1"&gt;John Dickerson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hines_K/0/1/0/all/0/1"&gt;Keegan Hines&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissecting User-Perceived Latency of On-Device E2E Speech Recognition. (arXiv:2104.02207v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02207</id>
        <link href="http://arxiv.org/abs/2104.02207"/>
        <updated>2021-06-16T01:21:07.898Z</updated>
        <summary type="html"><![CDATA[As speech-enabled devices such as smartphones and smart speakers become
increasingly ubiquitous, there is growing interest in building automatic speech
recognition (ASR) systems that can run directly on-device; end-to-end (E2E)
speech recognition models such as recurrent neural network transducers and
their variants have recently emerged as prime candidates for this task. Apart
from being accurate and compact, such systems need to decode speech with low
user-perceived latency (UPL), producing words as soon as they are spoken. This
work examines the impact of various techniques - model architectures, training
criteria, decoding hyperparameters, and endpointer parameters - on UPL. Our
analyses suggest that measures of model size (parameters, input chunk sizes),
or measures of computation (e.g., FLOPS, RTF) that reflect the model's ability
to process input frames are not always strongly correlated with observed UPL.
Thus, conventional algorithmic latency measurements might be inadequate in
accurately capturing latency observed when models are deployed on embedded
devices. Instead, we find that factors affecting token emission latency, and
endpointing behavior have a larger impact on UPL. We achieve the best trade-off
between latency and word error rate when performing ASR jointly with
endpointing, while utilizing the recently proposed alignment regularization
mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1"&gt;Yuan Shangguan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1"&gt;Rohit Prabhavalkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahadeokar_J/0/1/0/all/0/1"&gt;Jay Mahadeokar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yangyang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiatong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chunyang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1"&gt;Duc Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1"&gt;Ozlem Kalinli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuegen_C/0/1/0/all/0/1"&gt;Christian Fuegen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1"&gt;Michael L. Seltzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Transfer Learning for Brain Magnetic Resonance Image Multi-class Classification. (arXiv:2106.07333v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07333</id>
        <link href="http://arxiv.org/abs/2106.07333"/>
        <updated>2021-06-16T01:21:07.891Z</updated>
        <summary type="html"><![CDATA[Magnetic Resonance Imaging (MRI) is a principal diagnostic approach used in
the field of radiology to create images of the anatomical and physiological
structure of patients. MRI is the prevalent medical imaging practice to find
abnormalities in soft tissues. Traditionally they are analyzed by a radiologist
to detect abnormalities in soft tissues, especially the brain. The process of
interpreting a massive volume of patient's MRI is laborious. Hence, the use of
Machine Learning methodologies can aid in detecting abnormalities in soft
tissues with considerable accuracy. In this research, we have curated a novel
dataset and developed a framework that uses Deep Transfer Learning to perform a
multi-classification of tumors in the brain MRI images. In this paper, we
adopted the Deep Residual Convolutional Neural Network (ResNet50) architecture
for the experiments along with discriminative learning techniques to train the
model. Using the novel dataset and two publicly available MRI brain datasets,
this proposed approach attained a classification accuracy of 86.40% on the
curated dataset, 93.80% on the Harvard Whole Brain Atlas dataset, and 97.05%
accuracy on the School of Biomedical Engineering dataset. Results of our
experiments significantly demonstrate our proposed framework for transfer
learning is a potential and effective method for brain tumor
multi-classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brima_Y/0/1/0/all/0/1"&gt;Yusuf Brima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tushar_M/0/1/0/all/0/1"&gt;Mossadek Hossain Kamal Tushar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabir_U/0/1/0/all/0/1"&gt;Upama Kabir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1"&gt;Tariqul Islam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequence-Level Training for Non-Autoregressive Neural Machine Translation. (arXiv:2106.08122v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08122</id>
        <link href="http://arxiv.org/abs/2106.08122"/>
        <updated>2021-06-16T01:21:07.859Z</updated>
        <summary type="html"><![CDATA[In recent years, Neural Machine Translation (NMT) has achieved notable
results in various translation tasks. However, the word-by-word generation
manner determined by the autoregressive mechanism leads to high translation
latency of the NMT and restricts its low-latency applications.
Non-Autoregressive Neural Machine Translation (NAT) removes the autoregressive
mechanism and achieves significant decoding speedup through generating target
words independently and simultaneously. Nevertheless, NAT still takes the
word-level cross-entropy loss as the training objective, which is not optimal
because the output of NAT cannot be properly evaluated due to the multimodality
problem. In this paper, we propose using sequence-level training objectives to
train NAT models, which evaluate the NAT outputs as a whole and correlates well
with the real translation quality. Firstly, we propose training NAT models to
optimize sequence-level evaluation metrics (e.g., BLEU) based on several novel
reinforcement algorithms customized for NAT, which outperforms the conventional
method by reducing the variance of gradient estimation. Secondly, we introduce
a novel training objective for NAT models, which aims to minimize the
Bag-of-Ngrams (BoN) difference between the model output and the reference
sentence. The BoN training objective is differentiable and can be calculated
efficiently without doing any approximations. Finally, we apply a three-stage
training strategy to combine these two methods to train the NAT model. We
validate our approach on four translation tasks (WMT14 En$\leftrightarrow$De,
WMT16 En$\leftrightarrow$Ro), which shows that our approach largely outperforms
NAT baselines and achieves remarkable performance on all translation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1"&gt;Chenze Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedNILM: Applying Federated Learning to NILM Applications at the Edge. (arXiv:2106.07751v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07751</id>
        <link href="http://arxiv.org/abs/2106.07751"/>
        <updated>2021-06-16T01:21:07.847Z</updated>
        <summary type="html"><![CDATA[Non-intrusive load monitoring (NILM) helps disaggregate the household's main
electricity consumption to energy usages of individual appliances, thus greatly
cutting down the cost in fine-grained household load monitoring. To address the
arisen privacy concern in NILM applications, federated learning (FL) could be
leveraged for NILM model training and sharing. When applying the FL paradigm in
real-world NILM applications, however, we are faced with the challenges of edge
resource restriction, edge model personalization and edge training data
scarcity.

In this paper we present FedNILM, a practical FL paradigm for NILM
applications at the edge client. Specifically, FedNILM is designed to deliver
privacy-preserving and personalized NILM services to large-scale edge clients,
by leveraging i) secure data aggregation through federated learning, ii)
efficient cloud model compression via filter pruning and multi-task learning,
and iii) personalized edge model building with unsupervised transfer learning.
Our experiments on real-world energy data show that, FedNILM is able to achieve
personalized energy disaggregation with the state-of-the-art accuracy, while
ensuring privacy preserving at the edge client.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1"&gt;Guoming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qianyi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xudong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1"&gt;Jiadong Lou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teacher-Student MixIT for Unsupervised and Semi-supervised Speech Separation. (arXiv:2106.07843v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.07843</id>
        <link href="http://arxiv.org/abs/2106.07843"/>
        <updated>2021-06-16T01:21:07.836Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a novel semi-supervised learning framework for
end-to-end speech separation. The proposed method first uses mixtures of
unseparated sources and the mixture invariant training (MixIT) criterion to
train a teacher model. The teacher model then estimates separated sources that
are used to train a student model with standard permutation invariant training
(PIT). The student model can be fine-tuned with supervised data, i.e., paired
artificial mixtures and clean speech sources, and further improved via model
distillation. Experiments with single and multi channel mixtures show that the
teacher-student training resolves the over-separation problem observed in the
original MixIT method. Further, the semisupervised performance is comparable to
a fully-supervised separation system trained using ten times the amount of
supervised data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jisi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zorila_C/0/1/0/all/0/1"&gt;Catalin Zorila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1"&gt;Rama Doddipatla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barker_J/0/1/0/all/0/1"&gt;Jon Barker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Generate Task-Specific Adapters from Task Description. (arXiv:2101.00420v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00420</id>
        <link href="http://arxiv.org/abs/2101.00420"/>
        <updated>2021-06-16T01:21:07.822Z</updated>
        <summary type="html"><![CDATA[Pre-trained text-to-text transformers such as BART have achieved impressive
performance across a range of NLP tasks. Recent study further shows that they
can learn to generalize to novel tasks, by including task descriptions as part
of the source sequence and training the model with (source, target) examples.
At test time, these fine-tuned models can make inferences on new tasks using
the new task descriptions as part of the input. However, this approach has
potential limitations, as the model learns to solve individual (source, target)
examples (i.e., at the instance level), instead of learning to solve tasks by
taking all examples within a task as a whole (i.e., at the task level). To this
end, we introduce Hypter, a framework that improves text-to-text transformer's
generalization ability to unseen tasks by training a hypernetwork to generate
task-specific, light-weight adapters from task descriptions. Experiments on
ZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves
upon fine-tuning baselines. Notably, when using BART-Large as the main network,
Hypter brings 11.3% comparative improvement on ZEST dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qinyuan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Launching into clinical space with medspaCy: a new clinical text processing toolkit in Python. (arXiv:2106.07799v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07799</id>
        <link href="http://arxiv.org/abs/2106.07799"/>
        <updated>2021-06-16T01:21:07.797Z</updated>
        <summary type="html"><![CDATA[Despite impressive success of machine learning algorithms in clinical natural
language processing (cNLP), rule-based approaches still have a prominent role.
In this paper, we introduce medspaCy, an extensible, open-source cNLP library
based on spaCy framework that allows flexible integration of rule-based and
machine learning-based algorithms adapted to clinical text. MedspaCy includes a
variety of components that meet common cNLP needs such as context analysis and
mapping to standard terminologies. By utilizing spaCy's clear and easy-to-use
conventions, medspaCy enables development of custom pipelines that integrate
easily with other spaCy-based modules. Our toolkit includes several core
components and facilitates rapid development of pipelines for clinical text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eyre_H/0/1/0/all/0/1"&gt;Hannah Eyre&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Chapman_A/0/1/0/all/0/1"&gt;Alec B Chapman&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Peterson_K/0/1/0/all/0/1"&gt;Kelly S Peterson&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jianlin Shi&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Alba_P/0/1/0/all/0/1"&gt;Patrick R Alba&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1"&gt;Makoto M Jones&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Box_T/0/1/0/all/0/1"&gt;Tamara L Box&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+DuVall_S/0/1/0/all/0/1"&gt;Scott L DuVall&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Patterson_O/0/1/0/all/0/1"&gt;Olga V Patterson&lt;/a&gt; (1 and 2) ((1) VA Salt Lake City Health Care System, (2) University of Utah, Salt Lake City, UT, USA, (3) Veterans Health Administration Office of Analytics and Performance Integration)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using heterogeneity in semi-supervised transcription hypotheses to improve code-switched speech recognition. (arXiv:2106.07699v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07699</id>
        <link href="http://arxiv.org/abs/2106.07699"/>
        <updated>2021-06-16T01:21:07.767Z</updated>
        <summary type="html"><![CDATA[Modeling code-switched speech is an important problem in automatic speech
recognition (ASR). Labeled code-switched data are rare, so monolingual data are
often used to model code-switched speech. These monolingual data may be more
closely matched to one of the languages in the code-switch pair. We show that
such asymmetry can bias prediction toward the better-matched language and
degrade overall model performance. To address this issue, we propose a
semi-supervised approach for code-switched ASR. We consider the case of
English-Mandarin code-switching, and the problem of using monolingual data to
build bilingual "transcription models'' for annotation of unlabeled
code-switched data. We first build multiple transcription models so that their
individual predictions are variously biased toward either English or Mandarin.
We then combine these biased transcriptions using confidence-based selection.
This strategy generates a superior transcript for semi-supervised training, and
obtains a 19% relative improvement compared to a semi-supervised system that
relies on a transcription model built with only the best-matched monolingual
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slottje_A/0/1/0/all/0/1"&gt;Andrew Slottje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wotherspoon_S/0/1/0/all/0/1"&gt;Shannon Wotherspoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartmann_W/0/1/0/all/0/1"&gt;William Hartmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snover_M/0/1/0/all/0/1"&gt;Matthew Snover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kimball_O/0/1/0/all/0/1"&gt;Owen Kimball&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07306</id>
        <link href="http://arxiv.org/abs/2106.07306"/>
        <updated>2021-06-16T01:21:07.743Z</updated>
        <summary type="html"><![CDATA[In structured prediction, a major challenge for models is to represent the
interdependencies within their output structures. For the common case where
outputs are structured as a sequence, linear-chain conditional random fields
(CRFs) are a widely used model class which can learn local dependencies in
output sequences. However, the CRF's Markov assumption makes it impossible for
these models to capture nonlocal dependencies, and standard CRFs are unable to
respect nonlocal constraints of the data (such as global arity constraints on
output labels). We present a generalization of CRFs that can enforce a broad
class of constraints, including nonlocal ones, by specifying the space of
possible output structures as a regular language $\mathcal{L}$. The resulting
regular-constrained CRF (RegCCRF) has the same formal properties as a standard
CRF, but assigns zero probability to all label sequences not in $\mathcal{L}$.
Notably, RegCCRFs can incorporate their constraints during training, while
related models only enforce constraints during decoding. We prove that
constrained training is never worse than constrained decoding, and show using
synthetic data that it can be substantially better in practice. Additionally,
we demonstrate a practical benefit on downstream tasks by incorporating a
RegCCRF into a deep neural model for semantic role labeling, exceeding
state-of-the-art results on a standard dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papay_S/0/1/0/all/0/1"&gt;Sean Papay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1"&gt;Roman Klinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1"&gt;Sebastian Pad&amp;#xf3;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased Sentence Encoder For Large-Scale Multi-lingual Search Engines. (arXiv:2106.07719v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07719</id>
        <link href="http://arxiv.org/abs/2106.07719"/>
        <updated>2021-06-16T01:21:07.736Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a multi-lingual sentence encoder that can be used
in search engines as a query and document encoder. This embedding enables a
semantic similarity score between queries and documents that can be an
important feature in document ranking and relevancy. To train such a customized
sentence encoder, it is beneficial to leverage users search data in the form of
query-document clicked pairs however, we must avoid relying too much on search
click data as it is biased and does not cover many unseen cases. The search
data is heavily skewed towards short queries and for long queries is small and
often noisy. The goal is to design a universal multi-lingual encoder that works
for all cases and covers both short and long queries. We select a number of
public NLI datasets in different languages and translation data and together
with user search data we train a language model using a multi-task approach. A
challenge is that these datasets are not homogeneous in terms of content, size
and the balance ratio. While the public NLI datasets are usually two-sentence
based with the same portion of positive and negative pairs, the user search
data can contain multi-sentence documents and only positive pairs. We show how
multi-task training enables us to leverage all these datasets and exploit
knowledge sharing across these tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1"&gt;Mahdi Hajiaghayi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1"&gt;Monir Hajiaghayi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolin_M/0/1/0/all/0/1"&gt;Mark Bolin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Diffusion Halftoning Against Adversarial Examples. (arXiv:2101.09451v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09451</id>
        <link href="http://arxiv.org/abs/2101.09451"/>
        <updated>2021-06-16T01:21:07.729Z</updated>
        <summary type="html"><![CDATA[Adversarial examples contain carefully crafted perturbations that can fool
deep neural networks (DNNs) into making wrong predictions. Enhancing the
adversarial robustness of DNNs has gained considerable interest in recent
years. Although image transformation-based defenses were widely considered at
an earlier time, most of them have been defeated by adaptive attacks. In this
paper, we propose a new image transformation defense based on error diffusion
halftoning, and combine it with adversarial training to defend against
adversarial examples. Error diffusion halftoning projects an image into a 1-bit
space and diffuses quantization error to neighboring pixels. This process can
remove adversarial perturbations from a given image while maintaining
acceptable image quality in the meantime in favor of recognition. Experimental
results demonstrate that the proposed method is able to improve adversarial
robustness even under advanced adaptive attacks, while most of the other image
transformation-based defenses do not. We show that a proper image
transformation can still be an effective defense approach. Code:
https://github.com/shaoyuanlo/Halftoning-Defense]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shao-Yuan Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks with Heterophily. (arXiv:2009.13566v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13566</id>
        <link href="http://arxiv.org/abs/2009.13566"/>
        <updated>2021-06-16T01:21:07.710Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have proven to be useful for many different
practical applications. However, many existing GNN models have implicitly
assumed homophily among the nodes connected in the graph, and therefore have
largely overlooked the important setting of heterophily, where most connected
nodes are from different classes. In this work, we propose a novel framework
called CPGNN that generalizes GNNs for graphs with either homophily or
heterophily. The proposed framework incorporates an interpretable compatibility
matrix for modeling the heterophily or homophily level in the graph, which can
be learned in an end-to-end fashion, enabling it to go beyond the assumption of
strong homophily. Theoretically, we show that replacing the compatibility
matrix in our framework with the identity (which represents pure homophily)
reduces to GCN. Our extensive experiments demonstrate the effectiveness of our
approach in more realistic and challenging experimental settings with
significantly less training data compared to previous works: CPGNN variants
achieve state-of-the-art results in heterophily settings with or without
contextual node features, while maintaining comparable performance in homophily
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1"&gt;Ryan A. Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1"&gt;Anup Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1"&gt;Tung Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1"&gt;Nedim Lipka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1"&gt;Nesreen K. Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1"&gt;Danai Koutra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient and Accurate Gradients for Neural SDEs. (arXiv:2105.13493v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13493</id>
        <link href="http://arxiv.org/abs/2105.13493"/>
        <updated>2021-06-16T01:21:07.703Z</updated>
        <summary type="html"><![CDATA[Neural SDEs combine many of the best qualities of both RNNs and SDEs: memory
efficient training, high-capacity function approximation, and strong priors on
model space. This makes them a natural choice for modelling many types of
temporal dynamics. Training a Neural SDE (either as a VAE or as a GAN) requires
backpropagating through an SDE solve. This may be done by solving a
backwards-in-time SDE whose solution is the desired parameter gradients.
However, this has previously suffered from severe speed and accuracy issues,
due to high computational cost and numerical truncation errors. Here, we
overcome these issues through several technical innovations. First, we
introduce the \textit{reversible Heun method}. This is a new SDE solver that is
\textit{algebraically reversible}: eliminating numerical gradient errors, and
the first such solver of which we are aware. Moreover it requires half as many
function evaluations as comparable solvers, giving up to a $1.98\times$
speedup. Second, we introduce the \textit{Brownian Interval}: a new, fast,
memory efficient, and exact way of sampling \textit{and reconstructing}
Brownian motion. With this we obtain up to a $10.6\times$ speed improvement
over previous techniques, which in contrast are both approximate and relatively
slow. Third, when specifically training Neural SDEs as GANs (Kidger et al.
2021), we demonstrate how SDE-GANs may be trained through careful weight
clipping and choice of activation function. This reduces computational cost
(giving up to a $1.87\times$ speedup) and removes the numerical truncation
errors associated with gradient penalty. Altogether, we outperform the
state-of-the-art by substantial margins, with respect to training speed, and
with respect to classification, prediction, and MMD test metrics. We have
contributed implementations of all of our techniques to the torchsde library to
help facilitate their adoption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kidger_P/0/1/0/all/0/1"&gt;Patrick Kidger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1"&gt;James Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuechen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyons_T/0/1/0/all/0/1"&gt;Terry Lyons&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selfish Sparse RNN Training. (arXiv:2101.09048v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09048</id>
        <link href="http://arxiv.org/abs/2101.09048"/>
        <updated>2021-06-16T01:21:07.696Z</updated>
        <summary type="html"><![CDATA[Sparse neural networks have been widely applied to reduce the computational
demands of training and deploying over-parameterized deep neural networks. For
inference acceleration, methods that discover a sparse network from a
pre-trained dense network (dense-to-sparse training) work effectively.
Recently, dynamic sparse training (DST) has been proposed to train sparse
neural networks without pre-training a dense model (sparse-to-sparse training),
so that the training process can also be accelerated. However, previous
sparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs)
and Convolutional Neural Networks (CNNs), failing to match the performance of
dense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting. In
this paper, we propose an approach to train intrinsically sparse RNNs with a
fixed parameter count in one single run, without compromising performance.
During training, we allow RNN layers to have a non-uniform redistribution
across cell gates for better regularization. Further, we propose SNT-ASGD, a
novel variant of the averaged stochastic gradient optimizer, which
significantly improves the performance of all sparse training methods for RNNs.
Using these strategies, we achieve state-of-the-art sparse training results,
better than the dense-to-sparse methods, with various types of RNNs on Penn
TreeBank and Wikitext-2 datasets. Our codes are available at
https://github.com/Shiweiliuiiiiiii/Selfish-RNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1"&gt;Decebal Constantin Mocanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1"&gt;Yulong Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate spectral clustering using both reference vectors and topology of the network generated by growing neural gas. (arXiv:2009.07101v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07101</id>
        <link href="http://arxiv.org/abs/2009.07101"/>
        <updated>2021-06-16T01:21:07.690Z</updated>
        <summary type="html"><![CDATA[Spectral clustering (SC) is one of the most popular clustering methods and
often outperforms traditional clustering methods. SC uses the eigenvectors of a
Laplacian matrix calculated from a similarity matrix of a dataset. SC has the
serious drawbacks that are the significant increases in the time complexity
derived from the computation of eigenvectors and the memory space complexity to
store the similarity matrix. To address the issues, I develop a new approximate
spectral clustering using the network generated by growing neural gas (GNG),
called ASC with GNG in this study. ASC with GNG uses not only reference vectors
for vector quantization but also the topology of the network for extraction of
the topological relationship between data points in a dataset. ASC with GNG
makes the similarity matrix from both the reference vectors and the topology of
the network generated by GNG. Using the network generated from a dataset by
GNG, ASC with GNG achieves to reduce the computational and space complexities
and improve clustering quality. In this study, I demonstrate that ASC with GNG
effectively reduces computational time. Moreover, this study shows that ASC
with GNG displays equal to or better clustering performance than SC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujita_K/0/1/0/all/0/1"&gt;Kazuhisa Fujita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coded Machine Unlearning. (arXiv:2012.15721v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15721</id>
        <link href="http://arxiv.org/abs/2012.15721"/>
        <updated>2021-06-16T01:21:07.683Z</updated>
        <summary type="html"><![CDATA[There are applications that may require removing the trace of a sample from
the system, e.g., a user requests their data to be deleted, or corrupted data
is discovered. Simply removing a sample from storage units does not necessarily
remove its entire trace since downstream machine learning models may store some
information about the samples used to train them. A sample can be perfectly
unlearned if we retrain all models that used it from scratch with that sample
removed from their training dataset. When multiple such unlearning requests are
expected to be served, unlearning by retraining becomes prohibitively
expensive. Ensemble learning enables the training data to be split into smaller
disjoint shards that are assigned to non-communicating weak learners. Each
shard is used to produce a weak model. These models are then aggregated to
produce the final central model. This setup introduces an inherent trade-off
between performance and unlearning cost, as reducing the shard size reduces the
unlearning cost but may cause degradation in performance. In this paper, we
propose a coded learning protocol where we utilize linear encoders to encode
the training data into shards prior to the learning phase. We also present the
corresponding unlearning protocol and show that it satisfies the perfect
unlearning criterion. Our experimental results show that the proposed coded
machine unlearning provides a better performance versus unlearning cost
trade-off compared to the uncoded baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aldaghri_N/0/1/0/all/0/1"&gt;Nasser Aldaghri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahdavifar_H/0/1/0/all/0/1"&gt;Hessam Mahdavifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1"&gt;Ahmad Beirami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can BERT Dig It? -- Named Entity Recognition for Information Retrieval in the Archaeology Domain. (arXiv:2106.07742v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07742</id>
        <link href="http://arxiv.org/abs/2106.07742"/>
        <updated>2021-06-16T01:21:07.661Z</updated>
        <summary type="html"><![CDATA[The amount of archaeological literature is growing rapidly. Until recently,
these data were only accessible through metadata search. We implemented a text
retrieval engine for a large archaeological text collection ($\sim 658$ Million
words). In archaeological IR, domain-specific entities such as locations, time
periods, and artefacts, play a central role. This motivated the development of
a named entity recognition (NER) model to annotate the full collection with
archaeological named entities. In this paper, we present ArcheoBERTje, a BERT
model pre-trained on Dutch archaeological texts. We compare the model's quality
and output on a Named Entity Recognition task to a generic multilingual model
and a generic Dutch model. We also investigate ensemble methods for combining
multiple BERT models, and combining the best BERT model with a domain thesaurus
using Conditional Random Fields (CRF). We find that ArcheoBERTje outperforms
both the multilingual and Dutch model significantly with a smaller standard
deviation between runs, reaching an average F1 score of 0.735. The model also
outperforms ensemble methods combining the three models. Combining ArcheoBERTje
predictions and explicit domain knowledge from the thesaurus did not increase
the F1 score. We quantitatively and qualitatively analyse the differences
between the vocabulary and output of the BERT models on the full collection and
provide some valuable insights in the effect of fine-tuning for specific
domains. Our results indicate that for a highly specific text domain such as
archaeology, further pre-training on domain-specific data increases the model's
quality on NER by a much larger margin than shown for other domains in the
literature, and that domain-specific pre-training makes the addition of domain
knowledge from a thesaurus unnecessary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandsen_A/0/1/0/all/0/1"&gt;Alex Brandsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1"&gt;Suzan Verberne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lambers_K/0/1/0/all/0/1"&gt;Karsten Lambers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wansleeben_M/0/1/0/all/0/1"&gt;Milco Wansleeben&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Modules in Graph Contrastive Learning. (arXiv:2106.08171v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08171</id>
        <link href="http://arxiv.org/abs/2106.08171"/>
        <updated>2021-06-16T01:21:07.652Z</updated>
        <summary type="html"><![CDATA[The recent emergence of contrastive learning approaches facilitates the
research on graph representation learning (GRL), introducing graph contrastive
learning (GCL) into the literature. These methods contrast semantically similar
and dissimilar sample pairs to encode the semantics into node or graph
embeddings. However, most existing works only performed model-level evaluation,
and did not explore the combination space of modules for more comprehensive and
systematic studies. For effective module-level evaluation, we propose a
framework that decomposes GCL models into four modules: (1) a sampler to
generate anchor, positive and negative data samples (nodes or graphs); (2) an
encoder and a readout function to get sample embeddings; (3) a discriminator to
score each sample pair (anchor-positive and anchor-negative); and (4) an
estimator to define the loss function. Based on this framework, we conduct
controlled experiments over a wide range of architectural designs and
hyperparameter settings on node and graph classification tasks. Specifically,
we manage to quantify the impact of a single module, investigate the
interaction between modules, and compare the overall performance with current
model architectures. Our key findings include a set of module-level guidelines
for GCL, e.g., simple samplers from LINE and DeepWalk are strong and robust; an
MLP encoder associated with Sum readout could achieve competitive performance
on graph classification. Finally, we release our implementations and results as
OpenGCL, a modularized toolkit that allows convenient reproduction, standard
model and module evaluation, and easy extension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1"&gt;Ganqu Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yufeng Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lifeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training. (arXiv:2102.02887v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02887</id>
        <link href="http://arxiv.org/abs/2102.02887"/>
        <updated>2021-06-16T01:21:07.644Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a new perspective on training deep neural
networks capable of state-of-the-art performance without the need for the
expensive over-parameterization by proposing the concept of In-Time
Over-Parameterization (ITOP) in sparse training. By starting from a random
sparse network and continuously exploring sparse connectivities during
training, we can perform an Over-Parameterization in the space-time manifold,
closing the gap in the expressibility between sparse training and dense
training. We further use ITOP to understand the underlying mechanism of Dynamic
Sparse Training (DST) and indicate that the benefits of DST come from its
ability to consider across time all possible parameters when searching for the
optimal sparse connectivity. As long as there are sufficient parameters that
have been reliably explored during training, DST can outperform the dense
neural network by a large margin. We present a series of experiments to support
our conjecture and achieve the state-of-the-art sparse training performance
with ResNet-50 on ImageNet. More impressively, our method achieves dominant
performance over the overparameterization-based sparse methods at extreme
sparsity levels. When trained on CIFAR-100, our method can match the
performance of the dense model even at an extreme sparsity (98%). Code can be
found https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1"&gt;Lu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1"&gt;Decebal Constantin Mocanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-script Handwritten Digit Recognition Using Multi-task Learning. (arXiv:2106.08267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08267</id>
        <link href="http://arxiv.org/abs/2106.08267"/>
        <updated>2021-06-16T01:21:07.638Z</updated>
        <summary type="html"><![CDATA[Handwritten digit recognition is one of the extensively studied area in
machine learning. Apart from the wider research on handwritten digit
recognition on MNIST dataset, there are many other research works on various
script recognition. However, it is not very common for multi-script digit
recognition which encourage the development of robust and multipurpose systems.
Additionally working on multi-script digit recognition enables multi-task
learning, considering the script classification as a related task for instance.
It is evident that multi-task learning improves model performance through
inductive transfer using the information contained in related tasks. Therefore,
in this study multi-script handwritten digit recognition using multi-task
learning will be investigated. As a specific case of demonstrating the solution
to the problem, Amharic handwritten character recognition will also be
experimented. The handwritten digits of three scripts including Latin, Arabic
and Kannada are studied to show that multi-task models with reformulation of
the individual tasks have shown promising results. In this study a novel way of
using the individual tasks predictions was proposed to help classification
performance and regularize the different loss for the purpose of the main task.
This finding has outperformed the baseline and the conventional multi-task
learning models. More importantly, it avoided the need for weighting the
different losses of the tasks, which is one of the challenges in multi-task
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gondere_M/0/1/0/all/0/1"&gt;Mesay Samuel Gondere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1"&gt;Lars Schmidt-Thieme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1"&gt;Durga Prasad Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholz_R/0/1/0/all/0/1"&gt;Randolf Scholz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Generate Task-Specific Adapters from Task Description. (arXiv:2101.00420v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00420</id>
        <link href="http://arxiv.org/abs/2101.00420"/>
        <updated>2021-06-16T01:21:07.630Z</updated>
        <summary type="html"><![CDATA[Pre-trained text-to-text transformers such as BART have achieved impressive
performance across a range of NLP tasks. Recent study further shows that they
can learn to generalize to novel tasks, by including task descriptions as part
of the source sequence and training the model with (source, target) examples.
At test time, these fine-tuned models can make inferences on new tasks using
the new task descriptions as part of the input. However, this approach has
potential limitations, as the model learns to solve individual (source, target)
examples (i.e., at the instance level), instead of learning to solve tasks by
taking all examples within a task as a whole (i.e., at the task level). To this
end, we introduce Hypter, a framework that improves text-to-text transformer's
generalization ability to unseen tasks by training a hypernetwork to generate
task-specific, light-weight adapters from task descriptions. Experiments on
ZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves
upon fine-tuning baselines. Notably, when using BART-Large as the main network,
Hypter brings 11.3% comparative improvement on ZEST dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qinyuan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keyword Transformer: A Self-Attention Model for Keyword Spotting. (arXiv:2104.00769v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00769</id>
        <link href="http://arxiv.org/abs/2104.00769"/>
        <updated>2021-06-16T01:21:07.623Z</updated>
        <summary type="html"><![CDATA[The Transformer architecture has been successful across many domains,
including natural language processing, computer vision and speech recognition.
In keyword spotting, self-attention has primarily been used on top of
convolutional or recurrent encoders. We investigate a range of ways to adapt
the Transformer architecture to keyword spotting and introduce the Keyword
Transformer (KWT), a fully self-attentional architecture that exceeds
state-of-the-art performance across multiple tasks without any pre-training or
additional data. Surprisingly, this simple architecture outperforms more
complex models that mix convolutional, recurrent and attentive layers. KWT can
be used as a drop-in replacement for these models, setting two new benchmark
records on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on
the 12 and 35-command tasks respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Berg_A/0/1/0/all/0/1"&gt;Axel Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+OConnor_M/0/1/0/all/0/1"&gt;Mark O&amp;#x27;Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cruz_M/0/1/0/all/0/1"&gt;Miguel Tairum Cruz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Syntax and Semantics in the Brain with Deep Networks. (arXiv:2103.01620v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01620</id>
        <link href="http://arxiv.org/abs/2103.01620"/>
        <updated>2021-06-16T01:21:07.603Z</updated>
        <summary type="html"><![CDATA[The activations of language transformers like GPT-2 have been shown to
linearly map onto brain activity during speech comprehension. However, the
nature of these activations remains largely unknown and presumably conflate
distinct linguistic classes. Here, we propose a taxonomy to factorize the
high-dimensional activations of language models into four combinatorial
classes: lexical, compositional, syntactic, and semantic representations. We
then introduce a statistical method to decompose, through the lens of GPT-2's
activations, the brain activity of 345 subjects recorded with functional
magnetic resonance imaging (fMRI) during the listening of ~4.6 hours of
narrated text. The results highlight two findings. First, compositional
representations recruit a more widespread cortical network than lexical ones,
and encompass the bilateral temporal, parietal and prefrontal cortices. Second,
contrary to previous claims, syntax and semantics are not associated with
separated modules, but, instead, appear to share a common and distributed
neural substrate. Overall, this study introduces a versatile framework to
isolate, in the brain activity, the distributed representations of linguistic
constructs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caucheteux_C/0/1/0/all/0/1"&gt;Charlotte Caucheteux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gramfort_A/0/1/0/all/0/1"&gt;Alexandre Gramfort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_J/0/1/0/all/0/1"&gt;Jean-Remi King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Power of Multitask Representation Learning in Linear MDP. (arXiv:2106.08053v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08053</id>
        <link href="http://arxiv.org/abs/2106.08053"/>
        <updated>2021-06-16T01:21:07.596Z</updated>
        <summary type="html"><![CDATA[While multitask representation learning has become a popular approach in
reinforcement learning (RL), theoretical understanding of why and when it works
remains limited. This paper presents analyses for the statistical benefit of
multitask representation learning in linear Markov Decision Process (MDP) under
a generative model. In this paper, we consider an agent to learn a
representation function $\phi$ out of a function class $\Phi$ from $T$ source
tasks with $N$ data per task, and then use the learned $\hat{\phi}$ to reduce
the required number of sample for a new task. We first discover a
\emph{Least-Activated-Feature-Abundance} (LAFA) criterion, denoted as $\kappa$,
with which we prove that a straightforward least-square algorithm learns a
policy which is $\tilde{O}(H^2\sqrt{\frac{\mathcal{C}(\Phi)^2 \kappa
d}{NT}+\frac{\kappa d}{n}})$ sub-optimal. Here $H$ is the planning horizon,
$\mathcal{C}(\Phi)$ is $\Phi$'s complexity measure, $d$ is the dimension of the
representation (usually $d\ll \mathcal{C}(\Phi)$) and $n$ is the number of
samples for the new task. Thus the required $n$ is $O(\kappa d H^4)$ for the
sub-optimality to be close to zero, which is much smaller than
$O(\mathcal{C}(\Phi)^2\kappa d H^4)$ in the setting without multitask
representation learning, whose sub-optimality gap is
$\tilde{O}(H^2\sqrt{\frac{\kappa \mathcal{C}(\Phi)^2d}{n}})$. This
theoretically explains the power of multitask representation learning in
reducing sample complexity. Further, we note that to ensure high sample
efficiency, the LAFA criterion $\kappa$ should be small. In fact, $\kappa$
varies widely in magnitude depending on the different sampling distribution for
new task. This indicates adaptive sampling technique is important to make
$\kappa$ solely depend on $d$. Finally, we provide empirical results of a noisy
grid-world environment to corroborate our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1"&gt;Rui Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Training Data from Large Language Models. (arXiv:2012.07805v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07805</id>
        <link href="http://arxiv.org/abs/2012.07805"/>
        <updated>2021-06-16T01:21:07.588Z</updated>
        <summary type="html"><![CDATA[It has become common to publish large (billion parameter) language models
that have been trained on private datasets. This paper demonstrates that in
such settings, an adversary can perform a training data extraction attack to
recover individual training examples by querying the language model.

We demonstrate our attack on GPT-2, a language model trained on scrapes of
the public Internet, and are able to extract hundreds of verbatim text
sequences from the model's training data. These extracted examples include
(public) personally identifiable information (names, phone numbers, and email
addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible
even though each of the above sequences are included in just one document in
the training data.

We comprehensively evaluate our extraction attack to understand the factors
that contribute to its success. Worryingly, we find that larger models are more
vulnerable than smaller models. We conclude by drawing lessons and discussing
possible safeguards for training large language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1"&gt;Florian Tramer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1"&gt;Eric Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jagielski_M/0/1/0/all/0/1"&gt;Matthew Jagielski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herbert_Voss_A/0/1/0/all/0/1"&gt;Ariel Herbert-Voss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Katherine Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1"&gt;Adam Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1"&gt;Tom Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erlingsson_U/0/1/0/all/0/1"&gt;Ulfar Erlingsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oprea_A/0/1/0/all/0/1"&gt;Alina Oprea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1"&gt;Colin Raffel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Model Stitching to Compare Neural Representations. (arXiv:2106.07682v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07682</id>
        <link href="http://arxiv.org/abs/2106.07682"/>
        <updated>2021-06-16T01:21:07.579Z</updated>
        <summary type="html"><![CDATA[We revisit and extend model stitching (Lenc & Vedaldi 2015) as a methodology
to study the internal representations of neural networks. Given two trained and
frozen models $A$ and $B$, we consider a "stitched model'' formed by connecting
the bottom-layers of $A$ to the top-layers of $B$, with a simple trainable
layer between them. We argue that model stitching is a powerful and perhaps
under-appreciated tool, which reveals aspects of representations that measures
such as centered kernel alignment (CKA) cannot. Through extensive experiments,
we use model stitching to obtain quantitative verifications for intuitive
statements such as "good networks learn similar representations'', by
demonstrating that good networks of the same architecture, but trained in very
different ways (e.g.: supervised vs. self-supervised learning), can be stitched
to each other without drop in performance. We also give evidence for the
intuition that "more is better'' by showing that representations learnt with
(1) more data, (2) bigger width, or (3) more training time can be "plugged in''
to weaker models to improve performance. Finally, our experiments reveal a new
structural property of SGD which we call "stitching connectivity'', akin to
mode-connectivity: typical minima reached by SGD can all be stitched to each
other with minimal change in accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_Y/0/1/0/all/0/1"&gt;Yamini Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1"&gt;Preetum Nakkiran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1"&gt;Boaz Barak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Learning from Activity Description. (arXiv:2102.07024v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07024</id>
        <link href="http://arxiv.org/abs/2102.07024"/>
        <updated>2021-06-16T01:21:07.559Z</updated>
        <summary type="html"><![CDATA[We present a novel interactive learning protocol that enables training
request-fulfilling agents by verbally describing their activities. Unlike
imitation learning (IL), our protocol allows the teaching agent to provide
feedback in a language that is most appropriate for them. Compared with reward
in reinforcement learning (RL), the description feedback is richer and allows
for improved sample complexity. We develop a probabilistic framework and an
algorithm that practically implements our protocol. Empirical results in two
challenging request-fulfilling problems demonstrate the strengths of our
approach: compared with RL baselines, it is more sample-efficient; compared
with IL baselines, it achieves competitive success rates without requiring the
teaching agent to be able to demonstrate the desired behavior using the
learning agent's actions. Apart from empirical evaluation, we also provide
theoretical guarantees for our algorithm under certain assumptions about the
teacher and the environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khanh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schapire_R/0/1/0/all/0/1"&gt;Robert Schapire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1"&gt;Miro Dud&amp;#xed;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafto_P/0/1/0/all/0/1"&gt;Patrick Shafto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Global Dynamics of Loss Landscape in Deep Learning Models. (arXiv:2106.07683v1 [math.DS])]]></title>
        <id>http://arxiv.org/abs/2106.07683</id>
        <link href="http://arxiv.org/abs/2106.07683"/>
        <updated>2021-06-16T01:21:07.551Z</updated>
        <summary type="html"><![CDATA[Deep learning models evolve through training to learn the manifold in which
the data exists to satisfy an objective. It is well known that evolution leads
to different final states which produce inconsistent predictions of the same
test data points. This calls for techniques to be able to empirically quantify
the difference in the trajectories and highlight problematic regions. While
much focus is placed on discovering what models learn, the question of how a
model learns is less studied beyond theoretical landscape characterizations and
local geometric approximations near optimal conditions. Here, we present a
toolkit for the Dynamical Organization Of Deep Learning Loss Landscapes, or
DOODL3. DOODL3 formulates the training of neural networks as a dynamical
system, analyzes the learning process, and presents an interpretable global
view of trajectories in the loss landscape. Our approach uses the coarseness of
topology to capture the granularity of geometry to mitigate against states of
instability or elongated training. Overall, our analysis presents an empirical
framework to extract the global dynamics of a model and to use that information
to guide the training of neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Eslami_M/0/1/0/all/0/1"&gt;Mohammed Eslami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Eramian_H/0/1/0/all/0/1"&gt;Hamed Eramian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gameiro_M/0/1/0/all/0/1"&gt;Marcio Gameiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kalies_W/0/1/0/all/0/1"&gt;William Kalies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mischaikow_K/0/1/0/all/0/1"&gt;Konstantin Mischaikow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Inference with Continuously-Indexed Normalizing Flows. (arXiv:2007.05426v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05426</id>
        <link href="http://arxiv.org/abs/2007.05426"/>
        <updated>2021-06-16T01:21:07.543Z</updated>
        <summary type="html"><![CDATA[Continuously-indexed flows (CIFs) have recently achieved improvements over
baseline normalizing flows on a variety of density estimation tasks. CIFs do
not possess a closed-form marginal density, and so, unlike standard flows,
cannot be plugged in directly to a variational inference (VI) scheme in order
to produce a more expressive family of approximate posteriors. However, we show
here how CIFs can be used as part of an auxiliary VI scheme to formulate and
train expressive posterior approximations in a natural way. We exploit the
conditional independence structure of multi-layer CIFs to build the required
auxiliary inference models, which we show empirically yield low-variance
estimators of the model evidence. We then demonstrate the advantages of CIFs
over baseline flows in VI problems when the posterior distribution of interest
possesses a complicated topology, obtaining improved results in both the
Bayesian inference and surrogate maximum likelihood settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Caterini_A/0/1/0/all/0/1"&gt;Anthony Caterini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cornish_R/0/1/0/all/0/1"&gt;Rob Cornish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sejdinovic_D/0/1/0/all/0/1"&gt;Dino Sejdinovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallel Training of Deep Networks with Local Updates. (arXiv:2012.03837v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03837</id>
        <link href="http://arxiv.org/abs/2012.03837"/>
        <updated>2021-06-16T01:21:07.536Z</updated>
        <summary type="html"><![CDATA[Deep learning models trained on large data sets have been widely successful
in both vision and language domains. As state-of-the-art deep learning
architectures have continued to grow in parameter count so have the compute
budgets and times required to train them, increasing the need for
compute-efficient methods that parallelize training. Two common approaches to
parallelize the training of deep networks have been data and model parallelism.
While useful, data and model parallelism suffer from diminishing returns in
terms of compute efficiency for large batch sizes. In this paper, we
investigate how to continue scaling compute efficiently beyond the point of
diminishing returns for large batches through local parallelism, a framework
which parallelizes training of individual layers in deep networks by replacing
global backpropagation with truncated layer-wise backpropagation. Local
parallelism enables fully asynchronous layer-wise parallelism with a low memory
footprint, and requires little communication overhead compared with model
parallelism. We show results in both vision and language domains across a
diverse set of architectures, and find that local parallelism is particularly
effective in the high-compute regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laskin_M/0/1/0/all/0/1"&gt;Michael Laskin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metz_L/0/1/0/all/0/1"&gt;Luke Metz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nabarro_S/0/1/0/all/0/1"&gt;Seth Nabarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saroufim_M/0/1/0/all/0/1"&gt;Mark Saroufim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noune_B/0/1/0/all/0/1"&gt;Badreddine Noune&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1"&gt;Carlo Luschi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1"&gt;Jascha Sohl-Dickstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantics Altering Modifications for Evaluating Comprehension in Machine Reading. (arXiv:2012.04056v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04056</id>
        <link href="http://arxiv.org/abs/2012.04056"/>
        <updated>2021-06-16T01:21:07.529Z</updated>
        <summary type="html"><![CDATA[Advances in NLP have yielded impressive results for the task of machine
reading comprehension (MRC), with approaches having been reported to achieve
performance comparable to that of humans. In this paper, we investigate whether
state-of-the-art MRC models are able to correctly process Semantics Altering
Modifications (SAM): linguistically-motivated phenomena that alter the
semantics of a sentence while preserving most of its lexical surface form. We
present a method to automatically generate and align challenge sets featuring
original and altered examples. We further propose a novel evaluation
methodology to correctly assess the capability of MRC systems to process these
examples independent of the data they were optimised on, by discounting for
effects introduced by domain shift. In a large-scale empirical study, we apply
the methodology in order to evaluate extractive MRC models with regard to their
capability to correctly process SAM-enriched data. We comprehensively cover 12
different state-of-the-art neural architecture configurations and four training
datasets and find that -- despite their well-known remarkable performance --
optimised models consistently struggle to correctly process semantically
altered data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schlegel_V/0/1/0/all/0/1"&gt;Viktor Schlegel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1"&gt;Goran Nenadic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batista_Navarro_R/0/1/0/all/0/1"&gt;Riza Batista-Navarro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Principle of Least Action for the Training of Neural Networks. (arXiv:2009.08372v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08372</id>
        <link href="http://arxiv.org/abs/2009.08372"/>
        <updated>2021-06-16T01:21:07.521Z</updated>
        <summary type="html"><![CDATA[Neural networks have been achieving high generalization performance on many
tasks despite being highly over-parameterized. Since classical statistical
learning theory struggles to explain this behavior, much effort has recently
been focused on uncovering the mechanisms behind it, in the hope of developing
a more adequate theoretical framework and having a better control over the
trained models. In this work, we adopt an alternate perspective, viewing the
neural network as a dynamical system displacing input particles over time. We
conduct a series of experiments and, by analyzing the network's behavior
through its displacements, we show the presence of a low kinetic energy
displacement bias in the transport map of the network, and link this bias with
generalization performance. From this observation, we reformulate the learning
problem as follows: finding neural networks which solve the task while
transporting the data as efficiently as possible. This offers a novel
formulation of the learning problem which allows us to provide regularity
results for the solution network, based on Optimal Transport theory. From a
practical viewpoint, this allows us to propose a new learning algorithm, which
automatically adapts to the complexity of the given task, and leads to networks
with a high generalization ability even in low data regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Karkar_S/0/1/0/all/0/1"&gt;Skander Karkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ibrahim Ayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bezenac_E/0/1/0/all/0/1"&gt;Emmanuel de B&amp;#xe9;zenac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gallinari_P/0/1/0/all/0/1"&gt;Patrick Gallinari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phase Transitions, Distance Functions, and Implicit Neural Representations. (arXiv:2106.07689v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07689</id>
        <link href="http://arxiv.org/abs/2106.07689"/>
        <updated>2021-06-16T01:21:07.502Z</updated>
        <summary type="html"><![CDATA[Representing surfaces as zero level sets of neural networks recently emerged
as a powerful modeling paradigm, named Implicit Neural Representations (INRs),
serving numerous downstream applications in geometric deep learning and 3D
vision. Training INRs previously required choosing between occupancy and
distance function representation and different losses with unknown limit
behavior and/or bias. In this paper we draw inspiration from the theory of
phase transitions of fluids and suggest a loss for training INRs that learns a
density function that converges to a proper occupancy function, while its log
transform converges to a distance function. Furthermore, we analyze the limit
minimizer of this loss showing it satisfies the reconstruction constraints and
has minimal surface perimeter, a desirable inductive bias for surface
reconstruction. Training INRs with this new loss leads to state-of-the-art
reconstructions on a standard benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1"&gt;Yaron Lipman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast decentralized non-convex finite-sum optimization with recursive variance reduction. (arXiv:2008.07428v5 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07428</id>
        <link href="http://arxiv.org/abs/2008.07428"/>
        <updated>2021-06-16T01:21:07.488Z</updated>
        <summary type="html"><![CDATA[This paper considers decentralized minimization of $N:=nm$ smooth non-convex
cost functions equally divided over a directed network of $n$ nodes.
Specifically, we describe a stochastic first-order gradient method, called
GT-SARAH, that employs a SARAH-type variance reduction technique and gradient
tracking (GT) to address the stochastic and decentralized nature of the
problem. We show that GT-SARAH, with appropriate algorithmic parameters, finds
an $\epsilon$-accurate first-order stationary point with
$O\big(\max\big\{N^{\frac{1}{2}},n(1-\lambda)^{-2},n^{\frac{2}{3}}m^{\frac{1}{3}}(1-\lambda)^{-1}\big\}L\epsilon^{-2}\big)$
gradient complexity, where ${(1-\lambda)\in(0,1]}$ is the spectral gap of the
network weight matrix and $L$ is the smoothness parameter of the cost
functions. This gradient complexity outperforms that of the existing
decentralized stochastic gradient methods. In particular, in a big-data regime
such that ${n = O(N^{\frac{1}{2}}(1-\lambda)^{3})}$, this gradient complexity
furthers reduces to ${O(N^{\frac{1}{2}}L\epsilon^{-2})}$, independent of the
network topology, and matches that of the centralized near-optimal
variance-reduced methods. Moreover, in this regime GT-SARAH achieves a
non-asymptotic linear speedup, in that, the total number of gradient
computations at each node is reduced by a factor of $1/n$ compared to the
centralized near-optimal algorithms that perform all gradient computations at a
single node. To the best of our knowledge, GT-SARAH is the first algorithm that
achieves this property. In addition, we show that appropriate choices of local
minibatch size balance the trade-offs between the gradient and communication
complexity of GT-SARAH. Over infinite time horizon, we establish that all nodes
in GT-SARAH asymptotically achieve consensus and converge to a first-order
stationary point in the almost sure and mean-squared sense.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Xin_R/0/1/0/all/0/1"&gt;Ran Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Khan_U/0/1/0/all/0/1"&gt;Usman A. Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kar_S/0/1/0/all/0/1"&gt;Soummya Kar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Potato Crop Stress Identification in Aerial Images using Deep Learning-based Object Detection. (arXiv:2106.07770v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07770</id>
        <link href="http://arxiv.org/abs/2106.07770"/>
        <updated>2021-06-16T01:21:07.474Z</updated>
        <summary type="html"><![CDATA[Recent research on the application of remote sensing and deep learning-based
analysis in precision agriculture demonstrated a potential for improved crop
management and reduced environmental impacts of agricultural production.
Despite the promising results, the practical relevance of these technologies
for actual field deployment requires novel algorithms that are customized for
analysis of agricultural images and robust to implementation on natural field
imagery. The paper presents an approach for analyzing aerial images of a potato
crop using deep neural networks. The main objective is to demonstrate automated
spatial recognition of a healthy versus stressed crop at a plant level.
Specifically, we examine premature plant senescence resulting in drought stress
on Russet Burbank potato plants. The proposed deep learning model, named
Retina-UNet-Ag, is a variant of Retina-UNet (Jaeger et al., 2018) and includes
connections from low-level semantic dense representation maps to the feature
pyramid network. The paper also introduces a dataset of field images acquired
with a Parrot Sequoia camera carried by a Solo unmanned aerial vehicle.
Experimental validation demonstrated the ability for distinguishing healthy and
stressed plants in field images, achieving an average Dice score coefficient of
0.74. A comparison to related state-of-the-art deep learning models for object
detection revealed that the presented approach is effective for the task at
hand. The method applied here is conducive toward the assessment and
recognition of potato crop stress (early plant senescence resulting from
drought stress in this case) in natural aerial field images collected under
real conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Butte_S/0/1/0/all/0/1"&gt;Sujata Butte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakanski_A/0/1/0/all/0/1"&gt;Aleksandar Vakanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duellman_K/0/1/0/all/0/1"&gt;Kasia Duellman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirkouei_A/0/1/0/all/0/1"&gt;Amin Mirkouei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-Informed Neural Network for Modelling the Thermochemical Curing Process of Composite-Tool Systems During Manufacture. (arXiv:2011.13511v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13511</id>
        <link href="http://arxiv.org/abs/2011.13511"/>
        <updated>2021-06-16T01:21:07.467Z</updated>
        <summary type="html"><![CDATA[We present a Physics-Informed Neural Network (PINN) to simulate the
thermochemical evolution of a composite material on a tool undergoing cure in
an autoclave. In particular, we solve the governing coupled system of
differential equations -- including conductive heat transfer and resin cure
kinetics -- by optimizing the parameters of a deep neural network (DNN) using a
physics-based loss function. To account for the vastly different behaviour of
thermal conduction and resin cure, we design a PINN consisting of two
disconnected subnetworks, and develop a sequential training algorithm that
mitigates instability present in traditional training methods. Further, we
incorporate explicit discontinuities into the DNN at the composite-tool
interface and enforce known physical behaviour directly in the loss function to
improve the solution near the interface. We train the PINN with a technique
that automatically adapts the weights on the loss terms corresponding to PDE,
boundary, interface, and initial conditions. Finally, we demonstrate that one
can include problem parameters as an input to the model -- resulting in a
surrogate that provides real-time simulation for a range of problem settings --
and that one can use transfer learning to significantly reduce the training
time for problem settings similar to that of an initial trained model. The
performance of the proposed PINN is demonstrated in multiple scenarios with
different material thicknesses and thermal boundary conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niaki_S/0/1/0/all/0/1"&gt;Sina Amini Niaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haghighat_E/0/1/0/all/0/1"&gt;Ehsan Haghighat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campbell_T/0/1/0/all/0/1"&gt;Trevor Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poursartip_A/0/1/0/all/0/1"&gt;Anoush Poursartip&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaziri_R/0/1/0/all/0/1"&gt;Reza Vaziri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Audio-Visual Dereverberation. (arXiv:2106.07732v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.07732</id>
        <link href="http://arxiv.org/abs/2106.07732"/>
        <updated>2021-06-16T01:21:07.443Z</updated>
        <summary type="html"><![CDATA[Reverberation from audio reflecting off surfaces and objects in the
environment not only degrades the quality of speech for human perception, but
also severely impacts the accuracy of automatic speech recognition. Prior work
attempts to remove reverberation based on the audio modality only. Our idea is
to learn to dereverberate speech from audio-visual observations. The visual
environment surrounding a human speaker reveals important cues about the room
geometry, materials, and speaker location, all of which influence the precise
reverberation effects in the audio stream. We introduce Visually-Informed
Dereverberation of Audio (VIDA), an end-to-end approach that learns to remove
reverberation based on both the observed sounds and visual scene. In support of
this new task, we develop a large-scale dataset that uses realistic acoustic
renderings of speech in real-world 3D scans of homes offering a variety of room
acoustics. Demonstrating our approach on both simulated and real imagery for
speech enhancement, speech recognition, and speaker identification, we show it
achieves state-of-the-art performance and substantially improves over
traditional audio-only methods. Project page:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1"&gt;David Harwath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Training Data from Large Language Models. (arXiv:2012.07805v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07805</id>
        <link href="http://arxiv.org/abs/2012.07805"/>
        <updated>2021-06-16T01:21:07.435Z</updated>
        <summary type="html"><![CDATA[It has become common to publish large (billion parameter) language models
that have been trained on private datasets. This paper demonstrates that in
such settings, an adversary can perform a training data extraction attack to
recover individual training examples by querying the language model.

We demonstrate our attack on GPT-2, a language model trained on scrapes of
the public Internet, and are able to extract hundreds of verbatim text
sequences from the model's training data. These extracted examples include
(public) personally identifiable information (names, phone numbers, and email
addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible
even though each of the above sequences are included in just one document in
the training data.

We comprehensively evaluate our extraction attack to understand the factors
that contribute to its success. Worryingly, we find that larger models are more
vulnerable than smaller models. We conclude by drawing lessons and discussing
possible safeguards for training large language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1"&gt;Florian Tramer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1"&gt;Eric Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jagielski_M/0/1/0/all/0/1"&gt;Matthew Jagielski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herbert_Voss_A/0/1/0/all/0/1"&gt;Ariel Herbert-Voss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Katherine Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1"&gt;Adam Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1"&gt;Tom Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erlingsson_U/0/1/0/all/0/1"&gt;Ulfar Erlingsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oprea_A/0/1/0/all/0/1"&gt;Alina Oprea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1"&gt;Colin Raffel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Software Engineering Perspective on Engineering Machine Learning Systems: State of the Art and Challenges. (arXiv:2012.07919v3 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07919</id>
        <link href="http://arxiv.org/abs/2012.07919"/>
        <updated>2021-06-16T01:21:07.427Z</updated>
        <summary type="html"><![CDATA[Context: Advancements in machine learning (ML) lead to a shift from the
traditional view of software development, where algorithms are hard-coded by
humans, to ML systems materialized through learning from data. Therefore, we
need to revisit our ways of developing software systems and consider the
particularities required by these new types of systems. Objective: The purpose
of this study is to systematically identify, analyze, summarize, and synthesize
the current state of software engineering (SE) research for engineering ML
systems. Method: I performed a systematic literature review (SLR). I
systematically selected a pool of 141 studies from SE venues and then conducted
a quantitative and qualitative analysis using the data extracted from these
studies. Results: The non-deterministic nature of ML systems complicates all SE
aspects of engineering ML systems. Despite increasing interest from 2018
onwards, the results reveal that none of the SE aspects have a mature set of
tools and techniques. Testing is by far the most popular area among
researchers. Even for testing ML systems, engineers have only some tool
prototypes and solution proposals with weak experimental proof. Many of the
challenges of ML systems engineering were identified through surveys and
interviews. Researchers should conduct experiments and case studies, ideally in
industrial environments, to further understand these challenges and propose
solutions. Conclusion: The results may benefit (1) practitioners in foreseeing
the challenges of ML systems engineering; (2) researchers and academicians in
identifying potential research questions; and (3) educators in designing or
updating SE courses to cover ML systems engineering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giray_G/0/1/0/all/0/1"&gt;G&amp;#xf6;rkem Giray&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Biomedical Entity Linking with Contrastive Context Matching. (arXiv:2106.07583v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07583</id>
        <link href="http://arxiv.org/abs/2106.07583"/>
        <updated>2021-06-16T01:21:07.402Z</updated>
        <summary type="html"><![CDATA[We introduce BioCoM, a contrastive learning framework for biomedical entity
linking that uses only two resources: a small-sized dictionary and a large
number of raw biomedical articles. Specifically, we build the training
instances from raw PubMed articles by dictionary matching and use them to train
a context-aware entity linking model with contrastive learning. We predict the
normalized biomedical entity at inference time through a nearest-neighbor
search. Results found that BioCoM substantially outperforms state-of-the-art
models, especially in low-resource settings, by effectively using the context
of the entities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ujiie_S/0/1/0/all/0/1"&gt;Shogo Ujiie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1"&gt;Hayate Iso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aramaki_E/0/1/0/all/0/1"&gt;Eiji Aramaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-Trained Models: Past, Present and Future. (arXiv:2106.07139v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07139</id>
        <link href="http://arxiv.org/abs/2106.07139"/>
        <updated>2021-06-16T01:21:07.396Z</updated>
        <summary type="html"><![CDATA[Large-scale pre-trained models (PTMs) such as BERT and GPT have recently
achieved great success and become a milestone in the field of artificial
intelligence (AI). Owing to sophisticated pre-training objectives and huge
model parameters, large-scale PTMs can effectively capture knowledge from
massive labeled and unlabeled data. By storing knowledge into huge parameters
and fine-tuning on specific tasks, the rich knowledge implicitly encoded in
huge parameters can benefit a variety of downstream tasks, which has been
extensively demonstrated via experimental verification and empirical analysis.
It is now the consensus of the AI community to adopt PTMs as backbone for
downstream tasks rather than learning models from scratch. In this paper, we
take a deep look into the history of pre-training, especially its special
relation with transfer learning and self-supervised learning, to reveal the
crucial position of PTMs in the AI development spectrum. Further, we
comprehensively review the latest breakthroughs of PTMs. These breakthroughs
are driven by the surge of computational power and the increasing availability
of data, towards four important directions: designing effective architectures,
utilizing rich contexts, improving computational efficiency, and conducting
interpretation and theoretical analysis. Finally, we discuss a series of open
problems and research directions of PTMs, and hope our view can inspire and
advance the future study of PTMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1"&gt;Ning Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yuxian Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuqi Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1"&gt;Jiezhong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1"&gt;Wentao Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yanyan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhiwu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Ruihua Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jinhui Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Language Adversarial Defense through Synonym Encoding. (arXiv:1909.06723v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.06723</id>
        <link href="http://arxiv.org/abs/1909.06723"/>
        <updated>2021-06-16T01:21:07.353Z</updated>
        <summary type="html"><![CDATA[In the area of natural language processing, deep learning models are recently
known to be vulnerable to various types of adversarial perturbations, but
relatively few works are done on the defense side. Especially, there exists few
effective defense method against the successful synonym substitution based
attacks that preserve the syntactic structure and semantic information of the
original text while fooling the deep learning models. We contribute in this
direction and propose a novel adversarial defense method called Synonym
Encoding Method (SEM). Specifically, SEM inserts an encoder before the input
layer of the target model to map each cluster of synonyms to a unique encoding
and trains the model to eliminate possible adversarial perturbations without
modifying the network architecture or adding extra data. Extensive experiments
demonstrate that SEM can effectively defend the current synonym substitution
based attacks and block the transferability of adversarial examples. SEM is
also easy and efficient to scale to large models and big datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaosen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yichen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1"&gt;Kun He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders. (arXiv:2105.05752v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05752</id>
        <link href="http://arxiv.org/abs/2105.05752"/>
        <updated>2021-06-16T01:21:07.328Z</updated>
        <summary type="html"><![CDATA[Encoder pre-training is promising in end-to-end Speech Translation (ST),
given the fact that speech-to-translation data is scarce. But ST encoders are
not simple instances of Automatic Speech Recognition (ASR) or Machine
Translation (MT) encoders. For example, we find that ASR encoders lack the
global context representation, which is necessary for translation, whereas MT
encoders are not designed to deal with long but locally attentive acoustic
sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding
(SATE) method for speech translation. Our encoder begins with processing the
acoustic sequence as usual, but later behaves more like an MT encoder for a
global representation of the input sequence. In this way, it is straightforward
to incorporate the pre-trained models into the system. Also, we develop an
adaptor module to alleviate the representation inconsistency between the
pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge
distillation method to preserve the pre-training knowledge. Experimental
results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method
achieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we
are the first to develop an end-to-end ST system that achieves comparable or
even better BLEU performance than the cascaded ST counterpart when large-scale
ASR and MT data is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1"&gt;Bojie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+huang_s/0/1/0/all/0/1"&gt;shen huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_Q/0/1/0/all/0/1"&gt;Qi Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1"&gt;Tong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jingbo Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Determinantal Beam Search. (arXiv:2106.07400v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07400</id>
        <link href="http://arxiv.org/abs/2106.07400"/>
        <updated>2021-06-16T01:21:07.284Z</updated>
        <summary type="html"><![CDATA[Beam search is a go-to strategy for decoding neural sequence models. The
algorithm can naturally be viewed as a subset optimization problem, albeit one
where the corresponding set function does not reflect interactions between
candidates. Empirically, this leads to sets often exhibiting high overlap,
e.g., strings may differ by only a single word. Yet in use-cases that call for
multiple solutions, a diverse or representative set is often desired. To
address this issue, we propose a reformulation of beam search, which we call
determinantal beam search. Determinantal beam search has a natural relationship
to determinantal point processes (DPPs), models over sets that inherently
encode intra-set interactions. By posing iterations in beam search as a series
of subdeterminant maximization problems, we can turn the algorithm into a
diverse subset selection process. In a case study, we use the string
subsequence kernel to explicitly encourage n-gram coverage in text generated
from a sequence model. We observe that our algorithm offers competitive
performance against other diverse set generation strategies in the context of
language generation, while providing a more general approach to optimizing for
diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1"&gt;Clara Meister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forster_M/0/1/0/all/0/1"&gt;Martina Forster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alternate Endings: Improving Prosody for Incremental Neural TTS with Predicted Future Text Input. (arXiv:2102.09914v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09914</id>
        <link href="http://arxiv.org/abs/2102.09914"/>
        <updated>2021-06-16T01:21:07.267Z</updated>
        <summary type="html"><![CDATA[The prosody of a spoken word is determined by its surrounding context. In
incremental text-to-speech synthesis, where the synthesizer produces an output
before it has access to the complete input, the full context is often unknown
which can result in a loss of naturalness in the synthesized speech. In this
paper, we investigate whether the use of predicted future text can attenuate
this loss. We compare several test conditions of next future word: (a) unknown
(zero-word), (b) language model predicted, (c) randomly predicted and (d)
ground-truth. We measure the prosodic features (pitch, energy and duration) and
find that predicted text provides significant improvements over a zero-word
lookahead, but only slight gains over random-word lookahead. We confirm these
results with a perceptive test.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stephenson_B/0/1/0/all/0/1"&gt;Brooke Stephenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hueber_T/0/1/0/all/0/1"&gt;Thomas Hueber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girin_L/0/1/0/all/0/1"&gt;Laurent Girin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1"&gt;Laurent Besacier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deriving Word Vectors from Contextualized Language Models using Topic-Aware Mention Selection. (arXiv:2106.07947v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07947</id>
        <link href="http://arxiv.org/abs/2106.07947"/>
        <updated>2021-06-16T01:21:07.233Z</updated>
        <summary type="html"><![CDATA[One of the long-standing challenges in lexical semantics consists in learning
representations of words which reflect their semantic properties. The
remarkable success of word embeddings for this purpose suggests that
high-quality representations can be obtained by summarizing the sentence
contexts of word mentions. In this paper, we propose a method for learning word
representations that follows this basic strategy, but differs from standard
word embeddings in two important ways. First, we take advantage of
contextualized language models (CLMs) rather than bags of word vectors to
encode contexts. Second, rather than learning a word vector directly, we use a
topic model to partition the contexts in which words appear, and then learn
different topic-specific vectors for each word. Finally, we use a task-specific
supervision signal to make a soft selection of the resulting vectors. We show
that this simple strategy leads to high-quality word vectors, which are more
predictive of semantic properties than word embeddings and existing CLM-based
strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouraoui_Z/0/1/0/all/0/1"&gt;Zied Bouraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anke_L/0/1/0/all/0/1"&gt;Luis Espinosa Anke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1"&gt;Steven Schockaert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Syntax and Semantics in the Brain with Deep Networks. (arXiv:2103.01620v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01620</id>
        <link href="http://arxiv.org/abs/2103.01620"/>
        <updated>2021-06-16T01:21:07.206Z</updated>
        <summary type="html"><![CDATA[The activations of language transformers like GPT-2 have been shown to
linearly map onto brain activity during speech comprehension. However, the
nature of these activations remains largely unknown and presumably conflate
distinct linguistic classes. Here, we propose a taxonomy to factorize the
high-dimensional activations of language models into four combinatorial
classes: lexical, compositional, syntactic, and semantic representations. We
then introduce a statistical method to decompose, through the lens of GPT-2's
activations, the brain activity of 345 subjects recorded with functional
magnetic resonance imaging (fMRI) during the listening of ~4.6 hours of
narrated text. The results highlight two findings. First, compositional
representations recruit a more widespread cortical network than lexical ones,
and encompass the bilateral temporal, parietal and prefrontal cortices. Second,
contrary to previous claims, syntax and semantics are not associated with
separated modules, but, instead, appear to share a common and distributed
neural substrate. Overall, this study introduces a versatile framework to
isolate, in the brain activity, the distributed representations of linguistic
constructs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caucheteux_C/0/1/0/all/0/1"&gt;Charlotte Caucheteux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gramfort_A/0/1/0/all/0/1"&gt;Alexandre Gramfort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_J/0/1/0/all/0/1"&gt;Jean-Remi King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topics to Avoid: Demoting Latent Confounds in Text Classification. (arXiv:1909.00453v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.00453</id>
        <link href="http://arxiv.org/abs/1909.00453"/>
        <updated>2021-06-16T01:21:07.198Z</updated>
        <summary type="html"><![CDATA[Despite impressive performance on many text classification tasks, deep neural
networks tend to learn frequent superficial patterns that are specific to the
training data and do not always generalize well. In this work, we observe this
limitation with respect to the task of native language identification. We find
that standard text classifiers which perform well on the test set end up
learning topical features which are confounds of the prediction task (e.g., if
the input text mentions Sweden, the classifier predicts that the author's
native language is Swedish). We propose a method that represents the latent
topical confounds and a model which "unlearns" confounding features by
predicting both the label of the input text and the confound; but we train the
two predictors adversarially in an alternating fashion to learn a text
representation that predicts the correct label but is less prone to using
information about the confound. We show that this model generalizes better and
learns features that are indicative of the writing style rather than the
content.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sachin Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wintner_S/0/1/0/all/0/1"&gt;Shuly Wintner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Multilingual TEDx Corpus for Speech Recognition and Translation. (arXiv:2102.01757v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01757</id>
        <link href="http://arxiv.org/abs/2102.01757"/>
        <updated>2021-06-16T01:21:07.191Z</updated>
        <summary type="html"><![CDATA[We present the Multilingual TEDx corpus, built to support speech recognition
(ASR) and speech translation (ST) research across many non-English source
languages. The corpus is a collection of audio recordings from TEDx talks in 8
source languages. We segment transcripts into sentences and align them to the
source-language audio and target-language translations. The corpus is released
along with open-sourced code enabling extension to new talks and languages as
they become available. Our corpus creation methodology can be applied to more
languages than previous work, and creates multi-way parallel evaluation sets.
We provide baselines in multiple ASR and ST settings, including multilingual
models to improve translation performance for low-resource language pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1"&gt;Elizabeth Salesky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiesner_M/0/1/0/all/0/1"&gt;Matthew Wiesner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bremerman_J/0/1/0/all/0/1"&gt;Jacob Bremerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cattoni_R/0/1/0/all/0/1"&gt;Roldano Cattoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1"&gt;Matteo Negri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1"&gt;Marco Turchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1"&gt;Douglas W. Oard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1"&gt;Matt Post&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reasoning Over Virtual Knowledge Bases With Open Predicate Relations. (arXiv:2102.07043v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07043</id>
        <link href="http://arxiv.org/abs/2102.07043"/>
        <updated>2021-06-16T01:21:07.184Z</updated>
        <summary type="html"><![CDATA[We present the Open Predicate Query Language (OPQL); a method for
constructing a virtual KB (VKB) trained entirely from text. Large Knowledge
Bases (KBs) are indispensable for a wide-range of industry applications such as
question answering and recommendation. Typically, KBs encode world knowledge in
a structured, readily accessible form derived from laborious human annotation
efforts. Unfortunately, while they are extremely high precision, KBs are
inevitably highly incomplete and automated methods for enriching them are far
too inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set
of relation mentions in a way that naturally enables reasoning and can be
trained without any structured supervision. We demonstrate that OPQL
outperforms prior VKB methods on two different KB reasoning tasks and,
additionally, can be used as an external memory integrated into a language
model (OPQL-LM) leading to improvements on two open-domain question answering
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haitian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1"&gt;Pat Verga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1"&gt;Bhuwan Dhingra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"&gt;William W. Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling morphology with Linear Discriminative Learning: considerations and design choices. (arXiv:2106.07936v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07936</id>
        <link href="http://arxiv.org/abs/2106.07936"/>
        <updated>2021-06-16T01:21:07.178Z</updated>
        <summary type="html"><![CDATA[This study addresses a series of methodological questions that arise when
modeling inflectional morphology with Linear Discriminative Learning. Taking
the semi-productive German noun system as example, we illustrate how decisions
made about the representation of form and meaning influence model performance.
We clarify that for modeling frequency effects in learning, it is essential to
make use of incremental learning rather than the endstate of learning. We also
discuss how the model can be set up to approximate the learning of inflected
words in context. In addition, we illustrate how in this approach the wug task
can be modeled in considerable detail. In general, the model provides an
excellent memory for known words, but appropriately shows more limited
performance for unseen data, in line with the semi-productivity of German noun
inflection and generalization performance of native German speakers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heitmeier_M/0/1/0/all/0/1"&gt;Maria Heitmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1"&gt;Yu-Ying Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baayen_R/0/1/0/all/0/1"&gt;R. Harald Baayen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSMix: Saliency-Based Span Mixup for Text Classification. (arXiv:2106.08062v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08062</id>
        <link href="http://arxiv.org/abs/2106.08062"/>
        <updated>2021-06-16T01:21:07.158Z</updated>
        <summary type="html"><![CDATA[Data augmentation with mixup has shown to be effective on various computer
vision tasks. Despite its great success, there has been a hurdle to apply mixup
to NLP tasks since text consists of discrete tokens with variable length. In
this work, we propose SSMix, a novel mixup method where the operation is
performed on input text rather than on hidden vectors like previous approaches.
SSMix synthesizes a sentence while preserving the locality of two original
texts by span-based mixing and keeping more tokens related to the prediction
relying on saliency information. With extensive experiments, we empirically
validate that our method outperforms hidden-level mixup methods on a wide range
of text classification benchmarks, including textual entailment, sentiment
classification, and question-type classification. Our code is available at
https://github.com/clovaai/ssmix.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Soyoung Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gyuwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyumin Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialectal Speech Recognition and Translation of Swiss German Speech to Standard German Text: Microsoft's Submission to SwissText 2021. (arXiv:2106.08126v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.08126</id>
        <link href="http://arxiv.org/abs/2106.08126"/>
        <updated>2021-06-16T01:21:07.152Z</updated>
        <summary type="html"><![CDATA[This paper describes the winning approach in the public SwissText 2021
competition on dialect recognition and translation of Swiss German speech to
standard German text. Swiss German refers to the multitude of Alemannic
dialects spoken in the German-speaking parts of Switzerland. Swiss German
differs significantly from standard German in pronunciation, word inventory and
grammar. It is mostly incomprehensible to native German speakers. Moreover, it
lacks a standardized written script. To solve the challenging task, we propose
a hybrid automatic speech recognition system with a lexicon that incorporates
translations, a 1st pass language model that deals with Swiss German
particularities, a transfer-learned acoustic model and a strong neural language
model for 2nd pass rescoring. Our submission reaches 46.04% BLEU on a blind
conversational test set and outperforms the second best competitor by a 12%
relative margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Arabskyy_Y/0/1/0/all/0/1"&gt;Yuriy Arabskyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Aashish Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dey_S/0/1/0/all/0/1"&gt;Subhadeep Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Koller_O/0/1/0/all/0/1"&gt;Oscar Koller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10005</id>
        <link href="http://arxiv.org/abs/2105.10005"/>
        <updated>2021-06-16T01:21:07.143Z</updated>
        <summary type="html"><![CDATA[Physical processes, camera movement, and unpredictable environmental
conditions like the presence of dust can induce noise and artifacts in video
feeds. We observe that popular unsupervised MOT methods are dependent on
noise-free inputs. We show that the addition of a small amount of artificial
random noise causes a sharp degradation in model performance on benchmark
metrics. We resolve this problem by introducing a robust unsupervised
multi-object tracking (MOT) model: AttU-Net. The proposed single-head attention
model helps limit the negative impact of noise by learning visual
representations at different segment scales. AttU-Net shows better unsupervised
MOT tracking performance over variational inference-based state-of-the-art
baselines. We evaluate our method in the MNIST-MOT and the Atari game video
benchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''
which consists of moving Japanese characters and ``Fashion-MNIST MOT'' to
validate the effectiveness of the MOT models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;C.-H. Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1"&gt;Mohit Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Y.-C. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quan Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1"&gt;Tomoaki Yoshinaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1"&gt;Tomokazu Murakami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivalence of Segmental and Neural Transducer Modeling: A Proof of Concept. (arXiv:2104.06104v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06104</id>
        <link href="http://arxiv.org/abs/2104.06104"/>
        <updated>2021-06-16T01:21:07.137Z</updated>
        <summary type="html"><![CDATA[With the advent of direct models in automatic speech recognition (ASR), the
formerly prevalent frame-wise acoustic modeling based on hidden Markov models
(HMM) diversified into a number of modeling architectures like encoder-decoder
attention models, transducer models and segmental models (direct HMM). While
transducer models stay with a frame-level model definition, segmental models
are defined on the level of label segments directly. While
(soft-)attention-based models avoid explicit alignment, transducer and
segmental approach internally do model alignment, either by segment hypotheses
or, more implicitly, by emitting so-called blank symbols. In this work, we
prove that the widely used class of RNN-Transducer models and segmental models
(direct HMM) are equivalent and therefore show equal modeling power. It is
shown that blank probabilities translate into segment length probabilities and
vice versa. In addition, we provide initial experiments investigating decoding
and beam-pruning, comparing time-synchronous and label-/segment-synchronous
search strategies and their properties using the same underlying model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1"&gt;Albert Zeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merboldt_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Merboldt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Self-supervised Multi-task Learning for COVID-19 Information Retrieval and Extraction. (arXiv:2106.08252v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08252</id>
        <link href="http://arxiv.org/abs/2106.08252"/>
        <updated>2021-06-16T01:21:07.129Z</updated>
        <summary type="html"><![CDATA[The rapidly evolving literature of COVID-19 related articles makes it
challenging for NLP models to be effectively trained for information retrieval
and extraction with the corresponding labeled data that follows the current
distribution of the pandemic. On the other hand, due to the uncertainty of the
situation, human experts' supervision would always be required to double check
the decision making of these models highlighting the importance of
interpretability. In the light of these challenges, this study proposes an
interpretable self-supervised multi-task learning model to jointly and
effectively tackle the tasks of information retrieval (IR) and extraction (IE)
during the current emergency health crisis situation. Our results show that our
model effectively leverage the multi-task and self-supervised learning to
improve generalization, data efficiency and robustness to the ongoing dataset
shift problem. Our model outperforms baselines in IE and IR tasks, respectively
by micro-f score of 0.08 (LCA-F score of 0.05), and MAP of 0.05 on average. In
IE the zero- and few-shot learning performances are on average 0.32 and 0.19
micro-f score higher than those of the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ebadi_N/0/1/0/all/0/1"&gt;Nima Ebadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1"&gt;Peyman Najafirad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Margin Circle Loss for Speaker Verification. (arXiv:2106.08004v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.08004</id>
        <link href="http://arxiv.org/abs/2106.08004"/>
        <updated>2021-06-16T01:21:07.110Z</updated>
        <summary type="html"><![CDATA[Deep-Neural-Network (DNN) based speaker verification sys-tems use the angular
softmax loss with margin penalties toenhance the intra-class compactness of
speaker embeddings,which achieved remarkable performance. In this paper, we
pro-pose a novel angular loss function called adaptive margin cir-cle loss for
speaker verification. The stage-based margin andchunk-based margin are applied
to improve the angular discrim-ination of circle loss on the training set. The
analysis on gradi-ents shows that, compared with the previous angular loss
likeAdditive Margin Softmax(Am-Softmax), circle loss has flexi-ble optimization
and definite convergence status. Experimentsare carried out on the Voxceleb and
SITW. By applying adap-tive margin circle loss, our best system achieves
1.31%EER onVoxceleb1 and 2.13% on SITW core-core.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1"&gt;Runqiu Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Transformers. (arXiv:2106.04554v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04554</id>
        <link href="http://arxiv.org/abs/2106.04554"/>
        <updated>2021-06-16T01:21:07.104Z</updated>
        <summary type="html"><![CDATA[Transformers have achieved great success in many artificial intelligence
fields, such as natural language processing, computer vision, and audio
processing. Therefore, it is natural to attract lots of interest from academic
and industry researchers. Up to the present, a great variety of Transformer
variants (a.k.a. X-formers) have been proposed, however, a systematic and
comprehensive literature review on these Transformer variants is still missing.
In this survey, we provide a comprehensive review of various X-formers. We
first briefly introduce the vanilla Transformer and then propose a new taxonomy
of X-formers. Next, we introduce the various X-formers from three perspectives:
architectural modification, pre-training, and applications. Finally, we outline
some potential directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PairConnect: A Compute-Efficient MLP Alternative to Attention. (arXiv:2106.08235v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08235</id>
        <link href="http://arxiv.org/abs/2106.08235"/>
        <updated>2021-06-16T01:21:07.098Z</updated>
        <summary type="html"><![CDATA[Transformer models have demonstrated superior performance in natural language
processing. The dot product self-attention in Transformer allows us to model
interactions between words. However, this modeling comes with significant
computational overhead. In this work, we revisit the memory-compute trade-off
associated with Transformer, particularly multi-head attention, and show a
memory-heavy but significantly more compute-efficient alternative to
Transformer. Our proposal, denoted as PairConnect, a multilayer perceptron
(MLP), models the pairwise interaction between words by explicit pairwise word
embeddings. As a result, PairConnect substitutes self dot product with a simple
embedding lookup. We show mathematically that despite being an MLP, our
compute-efficient PairConnect is strictly more expressive than Transformer. Our
experiment on language modeling tasks suggests that PairConnect could achieve
comparable results with Transformer while reducing the computational cost
associated with inference significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhaozhuo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Minghao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Spanning Trees Are Invariant to Temperature Scaling in Graph-based Dependency Parsing. (arXiv:2106.08159v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08159</id>
        <link href="http://arxiv.org/abs/2106.08159"/>
        <updated>2021-06-16T01:21:07.091Z</updated>
        <summary type="html"><![CDATA[Modern graph-based syntactic dependency parsers operate by predicting, for
each token within a sentence, a probability distribution over its possible
syntactic heads (i.e., all other tokens) and then extracting a maximum spanning
tree from the resulting log-probabilities. Nowadays, virtually all such parsers
utilize deep neural networks and may thus be susceptible to miscalibration (in
particular, overconfident predictions). In this paper, we prove that
temperature scaling, a popular technique for post-hoc calibration of neural
networks, cannot change the output of the aforementioned procedure. We conclude
that other techniques are needed to tackle miscalibration in graph-based
dependency parsers in a way that improves parsing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grunewald_S/0/1/0/all/0/1"&gt;Stefan Gr&amp;#xfc;newald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Three-part diachronic semantic change dataset for Russian. (arXiv:2106.08294v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08294</id>
        <link href="http://arxiv.org/abs/2106.08294"/>
        <updated>2021-06-16T01:21:07.085Z</updated>
        <summary type="html"><![CDATA[We present a manually annotated lexical semantic change dataset for Russian:
RuShiftEval. Its novelty is ensured by a single set of target words annotated
for their diachronic semantic shifts across three time periods, while the
previous work either used only two time periods, or different sets of target
words. The paper describes the composition and annotation procedure for the
dataset. In addition, it is shown how the ternary nature of RuShiftEval allows
to trace specific diachronic trajectories: `changed at a particular time period
and stable afterwards' or `was changing throughout all time periods'. Based on
the analysis of the submissions to the recent shared task on semantic change
detection for Russian, we argue that correctly identifying such trajectories
can be an interesting sub-task itself.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kutuzov_A/0/1/0/all/0/1"&gt;Andrey Kutuzov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pivovarova_L/0/1/0/all/0/1"&gt;Lidia Pivovarova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark. (arXiv:2106.08087v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08087</id>
        <link href="http://arxiv.org/abs/2106.08087"/>
        <updated>2021-06-16T01:21:07.078Z</updated>
        <summary type="html"><![CDATA[Artificial Intelligence (AI), along with the recent progress in biomedical
language understanding, is gradually changing medical practice. With the
development of biomedical language understanding benchmarks, AI applications
are widely used in the medical field. However, most benchmarks are limited to
English, which makes it challenging to replicate many of the successes in
English for other languages. To facilitate research in this direction, we
collect real-world biomedical data and present the first Chinese Biomedical
Language Understanding Evaluation (CBLUE) benchmark: a collection of natural
language understanding tasks including named entity recognition, information
extraction, clinical diagnosis normalization, single-sentence/sentence-pair
classification, and an associated online platform for model evaluation,
comparison, and analysis. To establish evaluation on these tasks, we report
empirical results with the current 11 pre-trained Chinese models, and
experimental results show that state-of-the-art neural models perform by far
worse than the human ceiling. Our benchmark is released at
\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&lang=en-us}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1"&gt;Zhen Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaozhuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Luoqiu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Hongbin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1"&gt;Xin Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1"&gt;Kangping Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chuanqi Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mosha Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Luo Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1"&gt;Yuan Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1"&gt;Guotong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1"&gt;Zhifang Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zong_H/0/1/0/all/0/1"&gt;Hui Zong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1"&gt;Zheng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linfeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zan_H/0/1/0/all/0/1"&gt;Hongying Zan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kunli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huajun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1"&gt;Buzhou Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qingcai Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direction is what you need: Improving Word Embedding Compression in Large Language Models. (arXiv:2106.08181v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08181</id>
        <link href="http://arxiv.org/abs/2106.08181"/>
        <updated>2021-06-16T01:21:07.059Z</updated>
        <summary type="html"><![CDATA[The adoption of Transformer-based models in natural language processing (NLP)
has led to great success using a massive number of parameters. However, due to
deployment constraints in edge devices, there has been a rising interest in the
compression of these models to improve their inference time and memory
footprint. This paper presents a novel loss objective to compress token
embeddings in the Transformer-based models by leveraging an AutoEncoder
architecture. More specifically, we emphasize the importance of the direction
of compressed embeddings with respect to original uncompressed embeddings. The
proposed method is task-agnostic and does not require further language modeling
pre-training. Our method significantly outperforms the commonly used SVD-based
matrix-factorization approach in terms of initial language model Perplexity.
Moreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several
downstream tasks from the GLUE benchmark, where we also outperform the baseline
in most scenarios. Our code is public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balazy_K/0/1/0/all/0/1"&gt;Klaudia Ba&amp;#x142;azy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banaei_M/0/1/0/all/0/1"&gt;Mohammadreza Banaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lebret_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Lebret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1"&gt;Jacek Tabor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1"&gt;Karl Aberer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08043</id>
        <link href="http://arxiv.org/abs/2106.08043"/>
        <updated>2021-06-16T01:21:07.053Z</updated>
        <summary type="html"><![CDATA[The vast majority of existing methods and systems for causal inference assume
that all variables under consideration are categorical or numerical (e.g.,
gender, price, blood pressure, enrollment). In this paper, we present
CausalNLP, a toolkit for inferring causality from observational data that
includes text in addition to traditional numerical and categorical variables.
CausalNLP employs the use of meta-learners for treatment effect estimation and
supports using raw text and its linguistic properties as both a treatment and a
"controlled-for" variable (e.g., confounder). The library is open-source and
available at: https://github.com/amaiya/causalnlp.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1"&gt;Arun S. Maiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kaizen: Continuously improving teacher using Exponential Moving Average for semi-supervised speech recognition. (arXiv:2106.07759v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.07759</id>
        <link href="http://arxiv.org/abs/2106.07759"/>
        <updated>2021-06-16T01:21:07.045Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce the Kaizen framework that uses a continuously
improving teacher to generate pseudo-labels for semi-supervised training. The
proposed approach uses a teacher model which is updated as the exponential
moving average of the student model parameters. This can be seen as a
continuous version of the iterative pseudo-labeling approach for
semi-supervised training. It is applicable for different training criteria, and
in this paper we demonstrate it for frame-level hybrid hidden Markov model -
deep neural network (HMM-DNN) models and sequence-level connectionist temporal
classification (CTC) based models. The proposed approach shows more than 10%
word error rate (WER) reduction over standard teacher-student training and more
than 50\% relative WER reduction over 10 hour supervised baseline when using
large scale realistic unsupervised public videos in UK English and Italian
languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Manohar_V/0/1/0/all/0/1"&gt;Vimal Manohar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Likhomanenko_T/0/1/0/all/0/1"&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiantong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wei-Ning Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Collobert_R/0/1/0/all/0/1"&gt;Ronan Collobert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saraf_Y/0/1/0/all/0/1"&gt;Yatharth Saraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zweig_G/0/1/0/all/0/1"&gt;Geoffrey Zweig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Abdelrahman Mohamed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval. (arXiv:2104.01894v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01894</id>
        <link href="http://arxiv.org/abs/2104.01894"/>
        <updated>2021-06-16T01:21:07.030Z</updated>
        <summary type="html"><![CDATA[Speech-based image retrieval has been studied as a proxy for joint
representation learning, usually without emphasis on retrieval itself. As such,
it is unclear how well speech-based retrieval can work in practice -- both in
an absolute sense and versus alternative strategies that combine automatic
speech recognition (ASR) with strong text encoders. In this work, we
extensively study and expand choices of encoder architectures, training
methodology (including unimodal and multimodal pretraining), and other factors.
Our experiments cover different types of speech in three datasets: Flickr
Audio, Places Audio, and Localized Narratives. Our best model configuration
achieves large gains over state of the art, e.g., pushing recall-at-one from
21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also
show our best speech-based models can match or exceed cascaded ASR-to-text
encoding when speech is spontaneous, accented, or otherwise hard to
automatically transcribe.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1"&gt;Ramon Sanabria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1"&gt;Austin Waters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1"&gt;Jason Baldridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing. (arXiv:2106.08037v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.08037</id>
        <link href="http://arxiv.org/abs/2106.08037"/>
        <updated>2021-06-16T01:21:07.012Z</updated>
        <summary type="html"><![CDATA[Modality is the linguistic ability to describe events with added information
such as how desirable, plausible, or feasible they are. Modality is important
for many NLP downstream tasks such as the detection of hedging, uncertainty,
speculation, and more. Previous studies that address modality detection in NLP
often restrict modal expressions to a closed syntactic class, and the modal
sense labels are vastly different across different studies, lacking an accepted
standard. Furthermore, these senses are often analyzed independently of the
events that they modify. This work builds on the theoretical foundations of the
Georgetown Gradable Modal Expressions (GME) work by Rubinstein et al. (2013) to
propose an event-based modality detection task where modal expressions can be
words of any syntactic class and sense labels are drawn from a comprehensive
taxonomy which harmonizes the modal concepts contributed by the different
studies. We present experiments on the GME corpus aiming to detect and classify
fine-grained modal concepts and associate them with their modified events. We
show that detecting and classifying modal expressions is not only feasible, but
also improves the detection of modal events in their own right.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1"&gt;Valentina Pyatkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadde_S/0/1/0/all/0/1"&gt;Shoval Sadde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1"&gt;Aynat Rubinstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portner_P/0/1/0/all/0/1"&gt;Paul Portner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1"&gt;Reut Tsarfaty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07847</id>
        <link href="http://arxiv.org/abs/2106.07847"/>
        <updated>2021-06-16T01:21:07.005Z</updated>
        <summary type="html"><![CDATA[We study transfer learning in the presence of spurious correlations. We
experimentally demonstrate that directly transferring the stable feature
extractor learned on the source task may not eliminate these biases for the
target task. However, we hypothesize that the unstable features in the source
task and those in the target task are directly related. By explicitly informing
the target classifier of the source task's unstable features, we can regularize
the biases in the target task. Specifically, we derive a representation that
encodes the unstable features by contrasting different data environments in the
source task. On the target task, we cluster data from this representation, and
achieve robustness by minimizing the worst-case risk across all clusters. We
evaluate our method on both text and image classifications. Empirical results
demonstrate that our algorithm is able to maintain robustness on the target
task, outperforming the best baseline by 22.9% in absolute accuracy across 12
transfer settings. Our code is available at https://github.com/YujiaBao/Tofu.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1"&gt;Yujia Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Semantic Guidance and Deep Reinforcement Learning For Generating Human Level Paintings. (arXiv:2011.12589v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12589</id>
        <link href="http://arxiv.org/abs/2011.12589"/>
        <updated>2021-06-16T01:21:06.998Z</updated>
        <summary type="html"><![CDATA[Generation of stroke-based non-photorealistic imagery, is an important
problem in the computer vision community. As an endeavor in this direction,
substantial recent research efforts have been focused on teaching machines "how
to paint", in a manner similar to a human painter. However, the applicability
of previous methods has been limited to datasets with little variation in
position, scale and saliency of the foreground object. As a consequence, we
find that these methods struggle to cover the granularity and diversity
possessed by real world images. To this end, we propose a Semantic Guidance
pipeline with 1) a bi-level painting procedure for learning the distinction
between foreground and background brush strokes at training time. 2) We also
introduce invariance to the position and scale of the foreground object through
a neural alignment model, which combines object localization and spatial
transformer networks in an end to end manner, to zoom into a particular
semantic instance. 3) The distinguishing features of the in-focus object are
then amplified by maximizing a novel guided backpropagation based focus reward.
The proposed agent does not require any supervision on human stroke-data and
successfully handles variations in foreground object attributes, thus,
producing much higher quality canvases for the CUB-200 Birds and Stanford
Cars-196 datasets. Finally, we demonstrate the further efficacy of our method
on complex datasets with multiple foreground object instances by evaluating an
extension of our method on the challenging Virtual-KITTI dataset. Source code
and models are available at https://github.com/1jsingh/semantic-guidance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Jaskirat Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StockBabble: A Conversational Financial Agent to support Stock Market Investors. (arXiv:2106.08298v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.08298</id>
        <link href="http://arxiv.org/abs/2106.08298"/>
        <updated>2021-06-16T01:21:06.989Z</updated>
        <summary type="html"><![CDATA[We introduce StockBabble, a conversational agent designed to support
understanding and engagement with the stock market. StockBabble's value and
novelty is in its ability to empower retail investors -- many of which may be
new to investing -- and supplement their informational needs using a
user-friendly agent. Users have the ability to query information on companies
to retrieve a general and financial overview of a stock, including accessing
the latest news and trading recommendations. They can also request charts which
contain live prices and technical investment indicators, and add shares to a
personal portfolio to allow performance monitoring over time. To evaluate our
agent's potential, we conducted a user study with 15 participants. In total,
73% (11/15) of respondents said that they felt more confident in investing
after using StockBabble, and all 15 would consider recommending it to others.
These results are encouraging and suggest a wider appeal for such agents.
Moreover, we believe this research can help to inform the design and
development of future intelligent, financial personal assistants.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Suraj Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brennan_J/0/1/0/all/0/1"&gt;Joseph Brennan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nurse_J/0/1/0/all/0/1"&gt;Jason R. C. Nurse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Transfer Learning for Brain Magnetic Resonance Image Multi-class Classification. (arXiv:2106.07333v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07333</id>
        <link href="http://arxiv.org/abs/2106.07333"/>
        <updated>2021-06-16T01:21:06.943Z</updated>
        <summary type="html"><![CDATA[Magnetic Resonance Imaging (MRI) is a principal diagnostic approach used in
the field of radiology to create images of the anatomical and physiological
structure of patients. MRI is the prevalent medical imaging practice to find
abnormalities in soft tissues. Traditionally they are analyzed by a radiologist
to detect abnormalities in soft tissues, especially the brain. The process of
interpreting a massive volume of patient's MRI is laborious. Hence, the use of
Machine Learning methodologies can aid in detecting abnormalities in soft
tissues with considerable accuracy. In this research, we have curated a novel
dataset and developed a framework that uses Deep Transfer Learning to perform a
multi-classification of tumors in the brain MRI images. In this paper, we
adopted the Deep Residual Convolutional Neural Network (ResNet50) architecture
for the experiments along with discriminative learning techniques to train the
model. Using the novel dataset and two publicly available MRI brain datasets,
this proposed approach attained a classification accuracy of 86.40% on the
curated dataset, 93.80% on the Harvard Whole Brain Atlas dataset, and 97.05%
accuracy on the School of Biomedical Engineering dataset. Results of our
experiments significantly demonstrate our proposed framework for transfer
learning is a potential and effective method for brain tumor
multi-classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brima_Y/0/1/0/all/0/1"&gt;Yusuf Brima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tushar_M/0/1/0/all/0/1"&gt;Mossadek Hossain Kamal Tushar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabir_U/0/1/0/all/0/1"&gt;Upama Kabir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1"&gt;Tariqul Islam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Automated Quality Evaluation Framework of Psychotherapy Conversations with Local Quality Estimates. (arXiv:2106.07922v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07922</id>
        <link href="http://arxiv.org/abs/2106.07922"/>
        <updated>2021-06-16T01:21:06.932Z</updated>
        <summary type="html"><![CDATA[Computational approaches for assessing the quality of conversation-based
psychotherapy, such as Cognitive Behavioral Therapy (CBT) and Motivational
Interviewing (MI), have been developed recently to support quality assurance
and clinical training. However, due to the long session lengths and limited
modeling resources, computational methods largely rely on frequency-based
lexical features or distribution of dialogue acts. In this work, we propose a
hierarchical framework to automatically evaluate the quality of a CBT
interaction. We divide each psychotherapy session into conversation segments
and input those into a BERT-based model to produce segment embeddings. We first
fine-tune BERT for predicting segment-level (local) quality scores and then use
segment embeddings as lower-level input to a Bidirectional LSTM-based neural
network to predict session-level (global) quality estimates. In particular, the
segment-level quality scores are initialized with the session-level scores and
we model the global quality as a function of the local quality scores to
achieve the accurate segment-level quality estimates. These estimated
segment-level scores benefit theBERT fine-tuning and in learning better segment
embeddings. We evaluate the proposed framework on data drawn from real-world
CBT clinical session recordings to predict multiple session-level behavior
codes. The results indicate that our approach leads to improved evaluation
accuracy for most codes in both regression and classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhuohao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flemotomos_N/0/1/0/all/0/1"&gt;Nikolaos Flemotomos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singla_K/0/1/0/all/0/1"&gt;Karan Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Creed_T/0/1/0/all/0/1"&gt;Torrey A. Creed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atkins_D/0/1/0/all/0/1"&gt;David C. Atkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Shrikanth Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Paraphrase Detection with the Adversarial Paraphrasing Task. (arXiv:2106.07691v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07691</id>
        <link href="http://arxiv.org/abs/2106.07691"/>
        <updated>2021-06-16T01:21:06.889Z</updated>
        <summary type="html"><![CDATA[If two sentences have the same meaning, it should follow that they are
equivalent in their inferential properties, i.e., each sentence should
textually entail the other. However, many paraphrase datasets currently in
widespread use rely on a sense of paraphrase based on word overlap and syntax.
Can we teach them instead to identify paraphrases in a way that draws on the
inferential properties of the sentences, and is not over-reliant on lexical and
syntactic similarities of a sentence pair? We apply the adversarial paradigm to
this question, and introduce a new adversarial method of dataset creation for
paraphrase identification: the Adversarial Paraphrasing Task (APT), which asks
participants to generate semantically equivalent (in the sense of mutually
implicative) but lexically and syntactically disparate paraphrases. These
sentence pairs can then be used both to test paraphrase identification models
(which get barely random accuracy) and then improve their performance. To
accelerate dataset generation, we explore automation of APT using T5, and show
that the resulting dataset also improves accuracy. We discuss implications for
paraphrase detection and release our dataset in the hope of making paraphrase
detection models better able to detect sentence-level meaning equivalence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nighojkar_A/0/1/0/all/0/1"&gt;Animesh Nighojkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Licato_J/0/1/0/all/0/1"&gt;John Licato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An enriched category theory of language: from syntax to semantics. (arXiv:2106.07890v1 [math.CT])]]></title>
        <id>http://arxiv.org/abs/2106.07890</id>
        <link href="http://arxiv.org/abs/2106.07890"/>
        <updated>2021-06-16T01:21:06.883Z</updated>
        <summary type="html"><![CDATA[Given a piece of text, the ability to generate a coherent extension of it
implies some sophistication, including a knowledge of grammar and semantics. In
this paper, we propose a mathematical framework for passing from probability
distributions on extensions of given texts to an enriched category containing
semantic information. Roughly speaking, we model probability distributions on
texts as a category enriched over the unit interval. Objects of this category
are expressions in language and hom objects are conditional probabilities that
one expression is an extension of another. This category is syntactical: it
describes what goes with what. We then pass to the enriched category of unit
interval-valued copresheaves on this syntactical category to find semantic
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Bradley_T/0/1/0/all/0/1"&gt;Tai-Danae Bradley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Terilla_J/0/1/0/all/0/1"&gt;John Terilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Vlassopoulos_Y/0/1/0/all/0/1"&gt;Yiannis Vlassopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoDERT: Distilling Encoder Representations with Co-learning for Transducer-based Speech Recognition. (arXiv:2106.07734v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07734</id>
        <link href="http://arxiv.org/abs/2106.07734"/>
        <updated>2021-06-16T01:21:06.826Z</updated>
        <summary type="html"><![CDATA[We propose a simple yet effective method to compress an RNN-Transducer
(RNN-T) through the well-known knowledge distillation paradigm. We show that
the transducer's encoder outputs naturally have a high entropy and contain rich
information about acoustically similar word-piece confusions. This rich
information is suppressed when combined with the lower entropy decoder outputs
to produce the joint network logits. Consequently, we introduce an auxiliary
loss to distill the encoder logits from a teacher transducer's encoder, and
explore training strategies where this encoder distillation works effectively.
We find that tandem training of teacher and student encoders with an inplace
encoder distillation outperforms the use of a pre-trained and static teacher
transducer. We also report an interesting phenomenon we refer to as implicit
distillation, that occurs when the teacher and student encoders share the same
decoder. Our experiments show 5.37-8.4% relative word error rate reductions
(WERR) on in-house test sets, and 5.05-6.18% relative WERRs on LibriSpeech test
sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Swaminathan_R/0/1/0/all/0/1"&gt;Rupak Vignesh Swaminathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_B/0/1/0/all/0/1"&gt;Brian King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1"&gt;Grant P. Strimel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1"&gt;Jasha Droppo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1"&gt;Athanasios Mouchtaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilateral Personalized Dialogue Generation with Dynamic Persona-Aware Fusion. (arXiv:2106.07857v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07857</id>
        <link href="http://arxiv.org/abs/2106.07857"/>
        <updated>2021-06-16T01:21:06.797Z</updated>
        <summary type="html"><![CDATA[Generating personalized responses is one of the major challenges in natural
human-robot interaction. Current researches in this field mainly focus on
generating responses consistent with the robot's pre-assigned persona, while
ignoring the user's persona. Such responses may be inappropriate or even
offensive, which may lead to the bad user experience. Therefore, we propose a
bilateral personalized dialogue generation (BPDG) method with dynamic
persona-aware fusion via multi-task transfer learning to generate responses
consistent with both personas. The proposed method aims to accomplish three
learning tasks: 1) an encoder is trained with dialogue utterances added with
corresponded personalized attributes and relative position (language model
task), 2) a dynamic persona-aware fusion module predicts the persona presence
to adaptively fuse the contextual and bilateral personas encodings (persona
prediction task) and 3) a decoder generates natural, fluent and personalized
responses (dialogue generation task). To make the generated responses more
personalized and bilateral persona-consistent, the Conditional Mutual
Information Maximum (CMIM) criterion is adopted to select the final response
from the generated candidates. The experimental results show that the proposed
method outperforms several state-of-the-art methods in terms of both automatic
and manual evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Bin Sun&lt;/a&gt; (Member, IEEE), &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shutao Li&lt;/a&gt; (Fellow, IEEE)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Tags Matter for Zero-Shot Neural Machine Translation. (arXiv:2106.07930v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07930</id>
        <link href="http://arxiv.org/abs/2106.07930"/>
        <updated>2021-06-16T01:21:06.788Z</updated>
        <summary type="html"><![CDATA[Multilingual Neural Machine Translation (MNMT) has aroused widespread
interest due to its efficiency. An exciting advantage of MNMT models is that
they could also translate between unsupervised (zero-shot) language directions.
Language tag (LT) strategies are often adopted to indicate the translation
directions in MNMT. In this paper, we demonstrate that the LTs are not only
indicators for translation directions but also crucial to zero-shot translation
qualities. Unfortunately, previous work tends to ignore the importance of LT
strategies. We demonstrate that a proper LT strategy could enhance the
consistency of semantic representations and alleviate the off-target issue in
zero-shot directions. Experimental results show that by ignoring the source
language tag (SLT) and adding the target language tag (TLT) to the encoder, the
zero-shot translations could achieve a +8 BLEU score difference over other LT
strategies in IWSLT17, Europarl, TED talks translation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Shanbo Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Targeted Data Acquisition for Evolving Negotiation Agents. (arXiv:2106.07728v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.07728</id>
        <link href="http://arxiv.org/abs/2106.07728"/>
        <updated>2021-06-16T01:21:06.781Z</updated>
        <summary type="html"><![CDATA[Successful negotiators must learn how to balance optimizing for self-interest
and cooperation. Yet current artificial negotiation agents often heavily depend
on the quality of the static datasets they were trained on, limiting their
capacity to fashion an adaptive response balancing self-interest and
cooperation. For this reason, we find that these agents can achieve either high
utility or cooperation, but not both. To address this, we introduce a targeted
data acquisition framework where we guide the exploration of a reinforcement
learning agent using annotations from an expert oracle. The guided exploration
incentivizes the learning agent to go beyond its static dataset and develop new
negotiation strategies. We show that this enables our agents to obtain
higher-reward and more Pareto-optimal solutions when negotiating with both
simulated and human partners compared to standard supervised learning and
reinforcement learning methods. This trend additionally holds when comparing
agents using our targeted data acquisition framework to variants of agents
trained with a mix of supervised learning and reinforcement learning, or to
agents using tailored reward functions that explicitly optimize for utility and
Pareto-optimality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1"&gt;Minae Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1"&gt;Siddharth Karamcheti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuellar_M/0/1/0/all/0/1"&gt;Mariano-Florentino Cuellar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1"&gt;Dorsa Sadigh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Motion Vector Extrapolation for Video Object Detection. (arXiv:2104.08918v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08918</id>
        <link href="http://arxiv.org/abs/2104.08918"/>
        <updated>2021-06-16T01:21:06.774Z</updated>
        <summary type="html"><![CDATA[Despite the continued successes of computationally efficient deep neural
network architectures for video object detection, performance continually
arrives at the great trilemma of speed versus accuracy versus computational
resources (pick two). Current attempts to exploit temporal information in video
data to overcome this trilemma are bottlenecked by the state-of-the-art in
object detection models. We present, a technique which performs video object
detection through the use of off-the-shelf object detectors alongside existing
optical flow based motion estimation techniques in parallel. Through a set of
experiments on the benchmark MOT20 dataset, we demonstrate that our approach
significantly reduces the baseline latency of any given object detector without
sacrificing any accuracy. Further latency reduction, up to 25x lower than the
original latency, can be achieved with minimal accuracy loss. MOVEX enables low
latency video object detection on common CPU based systems, thus allowing for
high performance video object detection beyond the domain of GPU computing. The
code is available at https://github.com/juliantrue/movex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+True_J/0/1/0/all/0/1"&gt;Julian True&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naimul Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Beyond Classification Accuracy Metrics in Model Compression. (arXiv:2012.01604v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01604</id>
        <link href="http://arxiv.org/abs/2012.01604"/>
        <updated>2021-06-16T01:21:06.764Z</updated>
        <summary type="html"><![CDATA[With the rise in edge-computing devices, there has been an increasing demand
to deploy energy and resource-efficient models. A large body of research has
been devoted to developing methods that can reduce the size of the model
considerably without affecting the standard metrics such as top-1 accuracy.
However, these pruning approaches tend to result in a significant mismatch in
other metrics such as fairness across classes and explainability. To combat
such misalignment, we propose a novel multi-part loss function inspired by the
knowledge-distillation literature. Through extensive experiments, we
demonstrate the effectiveness of our approach across different compression
algorithms, architectures, tasks as well as datasets. In particular, we obtain
up to $4.1\times$ reduction in the number of prediction mismatches between the
compressed and reference models, and up to $5.7\times$ in cases where the
reference model makes the correct prediction; all while making no changes to
the compression algorithm, and minor modifications to the loss function.
Furthermore, we demonstrate how inducing simple alignment between the
predictions of the models naturally improves the alignment on other metrics
including fairness and attributions. Our framework can thus serve as a simple
plug-and-play component for compression algorithms in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_V/0/1/0/all/0/1"&gt;Vinu Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1"&gt;Shoaib Ahmed Siddiqui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhaskara_A/0/1/0/all/0/1"&gt;Aditya Bhaskara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muralidharan_S/0/1/0/all/0/1"&gt;Saurav Muralidharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garland_M/0/1/0/all/0/1"&gt;Michael Garland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sheraz Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1"&gt;Andreas Dengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge-Rich BERT Embeddings for Readability Assessment. (arXiv:2106.07935v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07935</id>
        <link href="http://arxiv.org/abs/2106.07935"/>
        <updated>2021-06-16T01:21:06.744Z</updated>
        <summary type="html"><![CDATA[Automatic readability assessment (ARA) is the task of evaluating the level of
ease or difficulty of text documents for a target audience. For researchers,
one of the many open problems in the field is to make such models trained for
the task show efficacy even for low-resource languages. In this study, we
propose an alternative way of utilizing the information-rich embeddings of BERT
models through a joint-learning method combined with handcrafted linguistic
features for readability assessment. Results show that the proposed method
outperforms classical approaches in readability assessment using English and
Filipino datasets, and obtaining as high as 12.4% increase in F1 performance.
We also show that the knowledge encoded in BERT embeddings can be used as a
substitute feature set for low-resource languages like Filipino with limited
semantic and syntactic NLP tools to explicitly extract feature values for the
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1"&gt;Joseph Marvin Imperial&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text Generation with Efficient (Soft) Q-Learning. (arXiv:2106.07704v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07704</id>
        <link href="http://arxiv.org/abs/2106.07704"/>
        <updated>2021-06-16T01:21:06.738Z</updated>
        <summary type="html"><![CDATA[Maximum likelihood estimation (MLE) is the predominant algorithm for training
text generation models. This paradigm relies on direct supervision examples,
which is not applicable to many applications, such as generating adversarial
attacks or generating prompts to control language models. Reinforcement
learning (RL) on the other hand offers a more flexible solution by allowing
users to plug in arbitrary task metrics as reward. Yet previous RL algorithms
for text generation, such as policy gradient (on-policy RL) and Q-learning
(off-policy RL), are often notoriously inefficient or unstable to train due to
the large sequence space and the sparse reward received only at the end of
sequences. In this paper, we introduce a new RL formulation for text generation
from the soft Q-learning perspective. It further enables us to draw from the
latest RL advances, such as path consistency learning, to combine the best of
on-/off-policy updates, and learn effectively from sparse reward. We apply the
approach to a wide range of tasks, including learning from noisy/negative
examples, adversarial attacks, and prompt generation. Experiments show our
approach consistently outperforms both task-specialized algorithms and the
previous RL methods. On standard supervised tasks where MLE prevails, our
approach also achieves competitive performance and stability by training text
generation from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Han Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1"&gt;Bowen Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengzhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"&gt;Eric P. Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhiting Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compact and adaptive multiplane images for view synthesis. (arXiv:2102.10086v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10086</id>
        <link href="http://arxiv.org/abs/2102.10086"/>
        <updated>2021-06-16T01:21:06.729Z</updated>
        <summary type="html"><![CDATA[Recently, learning methods have been designed to create Multiplane Images
(MPIs) for view synthesis. While MPIs are extremely powerful and facilitate
high quality renderings, a great amount of memory is required, making them
impractical for many applications. In this paper, we propose a learning method
that optimizes the available memory to render compact and adaptive MPIs. Our
MPIs avoid redundant information and take into account the scene geometry to
determine the depth sampling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Navarro_J/0/1/0/all/0/1"&gt;Julia Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabater_N/0/1/0/all/0/1"&gt;Neus Sabater&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGENT: A Benchmark for Core Psychological Reasoning. (arXiv:2102.12321v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12321</id>
        <link href="http://arxiv.org/abs/2102.12321"/>
        <updated>2021-06-16T01:21:06.722Z</updated>
        <summary type="html"><![CDATA[For machine agents to successfully interact with humans in real-world
settings, they will need to develop an understanding of human mental life.
Intuitive psychology, the ability to reason about hidden mental variables that
drive observable actions, comes naturally to people: even pre-verbal infants
can tell agents from objects, expecting agents to act efficiently to achieve
goals given constraints. Despite recent interest in machine agents that reason
about other agents, it is not clear if such agents learn or hold the core
psychology principles that drive human reasoning. Inspired by cognitive
development studies on intuitive psychology, we present a benchmark consisting
of a large dataset of procedurally generated 3D animations, AGENT (Action,
Goal, Efficiency, coNstraint, uTility), structured around four scenarios (goal
preferences, action efficiency, unobserved constraints, and cost-reward
trade-offs) that probe key concepts of core intuitive psychology. We validate
AGENT with human-ratings, propose an evaluation protocol emphasizing
generalization, and compare two strong baselines built on Bayesian inverse
planning and a Theory of Mind neural network. Our results suggest that to pass
the designed tests of core intuitive psychology at human levels, a model must
acquire or have built-in representations of how agents plan, combining utility
computations and core knowledge of objects and physics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1"&gt;Tianmin Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1"&gt;Abhishek Bhandwaldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kevin A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shari Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1"&gt;Dan Gutfreund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spelke_E/0/1/0/all/0/1"&gt;Elizabeth Spelke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1"&gt;Tomer D. Ullman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing the Use of Prosody in Constituency Parsing of Imperfect Transcripts. (arXiv:2106.07794v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07794</id>
        <link href="http://arxiv.org/abs/2106.07794"/>
        <updated>2021-06-16T01:21:06.702Z</updated>
        <summary type="html"><![CDATA[This work explores constituency parsing on automatically recognized
transcripts of conversational speech. The neural parser is based on a sentence
encoder that leverages word vectors contextualized with prosodic features,
jointly learning prosodic feature extraction with parsing. We assess the
utility of the prosody in parsing on imperfect transcripts, i.e. transcripts
with automatic speech recognition (ASR) errors, by applying the parser in an
N-best reranking framework. In experiments on Switchboard, we obtain 13-15% of
the oracle N-best gain relative to parsing the 1-best ASR output, with
insignificant impact on word recognition error rate. Prosody provides a
significant part of the gain, and analyses suggest that it leads to more
grammatical utterances via recovering function words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Trang Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1"&gt;Mari Ostendorf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Book Cover Design via Layout Graphs. (arXiv:2105.11088v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11088</id>
        <link href="http://arxiv.org/abs/2105.11088"/>
        <updated>2021-06-16T01:21:06.692Z</updated>
        <summary type="html"><![CDATA[Book covers are intentionally designed and provide an introduction to a book.
However, they typically require professional skills to design and produce the
cover images. Thus, we propose a generative neural network that can produce
book covers based on an easy-to-use layout graph. The layout graph contains
objects such as text, natural scene objects, and solid color spaces. This
layout graph is embedded using a graph convolutional neural network and then
used with a mask proposal generator and a bounding-box generator and filled
using an object proposal generator. Next, the objects are compiled into a
single image and the entire network is trained using a combination of
adversarial training, perceptual training, and reconstruction. Finally, a Style
Retention Network (SRNet) is used to transfer the learned font style onto the
desired text. Using the proposed method allows for easily controlled and unique
book covers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wensheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miyazono_T/0/1/0/all/0/1"&gt;Taiga Miyazono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1"&gt;Brian Kenji Iwana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DISCO: Dynamic and Invariant Sensitive Channel Obfuscation for deep neural networks. (arXiv:2012.11025v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11025</id>
        <link href="http://arxiv.org/abs/2012.11025"/>
        <updated>2021-06-16T01:21:06.676Z</updated>
        <summary type="html"><![CDATA[Recent deep learning models have shown remarkable performance in image
classification. While these deep learning systems are getting closer to
practical deployment, the common assumption made about data is that it does not
carry any sensitive information. This assumption may not hold for many
practical cases, especially in the domain where an individual's personal
information is involved, like healthcare and facial recognition systems. We
posit that selectively removing features in this latent space can protect the
sensitive information and provide a better privacy-utility trade-off.
Consequently, we propose DISCO which learns a dynamic and data driven pruning
filter to selectively obfuscate sensitive information in the feature space. We
propose diverse attack schemes for sensitive inputs \& attributes and
demonstrate the effectiveness of DISCO against state-of-the-art methods through
quantitative and qualitative evaluation. Finally, we also release an evaluation
benchmark dataset of 1 million sensitive representations to encourage rigorous
exploration of novel attack schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Abhishek Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1"&gt;Ayush Chopra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1"&gt;Vivek Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garza_E/0/1/0/all/0/1"&gt;Ethan Garza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1"&gt;Emily Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vepakomma_P/0/1/0/all/0/1"&gt;Praneeth Vepakomma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1"&gt;Ramesh Raskar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A baseline for semi-supervised learning of efficient semantic segmentation models. (arXiv:2106.07075v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07075</id>
        <link href="http://arxiv.org/abs/2106.07075"/>
        <updated>2021-06-16T01:21:06.669Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning is especially interesting in the dense prediction
context due to high cost of pixel-level ground truth. Unfortunately, most such
approaches are evaluated on outdated architectures which hamper research due to
very slow training and high requirements on GPU RAM. We address this concern by
presenting a simple and effective baseline which works very well both on
standard and efficient architectures. Our baseline is based on one-way
consistency and non-linear geometric and photometric perturbations. We show
advantage of perturbing only the student branch and present a plausible
explanation of such behaviour. Experiments on Cityscapes and CIFAR-10
demonstrate competitive performance with respect to prior work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1"&gt;Ivan Grubi&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orsic_M/0/1/0/all/0/1"&gt;Marin Or&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1"&gt;Sini&amp;#x161;a &amp;#x160;egvi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework. (arXiv:2010.04879v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04879</id>
        <link href="http://arxiv.org/abs/2010.04879"/>
        <updated>2021-06-16T01:21:06.659Z</updated>
        <summary type="html"><![CDATA[Most neural network pruning methods, such as filter-level and layer-level
prunings, prune the network model along one dimension (depth, width, or
resolution) solely to meet a computational budget. However, such a pruning
policy often leads to excessive reduction of that dimension, thus inducing a
huge accuracy loss. To alleviate this issue, we argue that pruning should be
conducted along three dimensions comprehensively. For this purpose, our pruning
framework formulates pruning as an optimization problem. Specifically, it first
casts the relationships between a certain model's accuracy and
depth/width/resolution into a polynomial regression and then maximizes the
polynomial to acquire the optimal values for the three dimensions. Finally, the
model is pruned along the three optimal dimensions accordingly. In this
framework, since collecting too much data for training the regression is very
time-costly, we propose two approaches to lower the cost: 1) specializing the
polynomial to ensure an accurate regression even with less training data; 2)
employing iterative pruning and fine-tuning to collect the data faster.
Extensive experiments show that our proposed algorithm surpasses
state-of-the-art pruning algorithms and even neural architecture search-based
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenxiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minghao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jinming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haifeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coping with Label Shift via Distributionally Robust Optimisation. (arXiv:2010.12230v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12230</id>
        <link href="http://arxiv.org/abs/2010.12230"/>
        <updated>2021-06-16T01:21:06.650Z</updated>
        <summary type="html"><![CDATA[The label shift problem refers to the supervised learning setting where the
train and test label distributions do not match. Existing work addressing label
shift usually assumes access to an \emph{unlabelled} test sample. This sample
may be used to estimate the test label distribution, and to then train a
suitably re-weighted classifier. While approaches using this idea have proven
effective, their scope is limited as it is not always feasible to access the
target domain; further, they require repeated retraining if the model is to be
deployed in \emph{multiple} test environments. Can one instead learn a
\emph{single} classifier that is robust to arbitrary label shifts from a broad
family? In this paper, we answer this question by proposing a model that
minimises an objective based on distributionally robust optimisation (DRO). We
then design and analyse a gradient descent-proximal mirror ascent algorithm
tailored for large-scale problems to optimise the proposed objective. %, and
establish its convergence. Finally, through experiments on CIFAR-100 and
ImageNet, we show that our technique can significantly improve performance over
a number of baselines in settings where label shift is present.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingzhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1"&gt;Andreas Veit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1"&gt;Srinadh Bhojanapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1"&gt;Suvrit Sra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[1$\times$N Block Pattern for Network Sparsity. (arXiv:2105.14713v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14713</id>
        <link href="http://arxiv.org/abs/2105.14713"/>
        <updated>2021-06-16T01:21:06.630Z</updated>
        <summary type="html"><![CDATA[Though network sparsity emerges as a promising direction to overcome the
drastically increasing size of neural networks, it remains an open problem to
concurrently maintain model accuracy as well as achieve significant speedups on
general CPUs. In this paper, we propose one novel concept of $1\times N$ block
sparsity pattern (block pruning) to break this limitation. In particular,
consecutive $N$ output kernels with the same input channel index are grouped
into one block, which serves as a basic pruning granularity of our pruning
pattern. Our $1 \times N$ sparsity pattern prunes these blocks considered
unimportant. We also provide a workflow of filter rearrangement that first
rearranges the weight matrix in the output channel dimension to derive more
influential blocks for accuracy improvements, and then applies similar
rearrangement to the next-layer weights in the input channel dimension to
ensure correct convolutional operations. Moreover, the output computation after
our $1 \times N$ block sparsity can be realized via a parallelized block-wise
vectorized operation, leading to significant speedups on general CPUs-based
platforms. The efficacy of our pruning pattern is proved with experiments on
ILSVRC-2012. For example, in the case of 50% sparsity and $N=4$, our pattern
obtains about 3.0% improvements over filter pruning in the top-1 accuracy of
MobileNet-V2. Meanwhile, it obtains 56.04ms inference savings on Cortex-A7 CPU
over weight pruning. Code is available at https://github.com/lmbxmu/1xN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bohong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1"&gt;Fei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training. (arXiv:2102.02887v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02887</id>
        <link href="http://arxiv.org/abs/2102.02887"/>
        <updated>2021-06-16T01:21:06.622Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a new perspective on training deep neural
networks capable of state-of-the-art performance without the need for the
expensive over-parameterization by proposing the concept of In-Time
Over-Parameterization (ITOP) in sparse training. By starting from a random
sparse network and continuously exploring sparse connectivities during
training, we can perform an Over-Parameterization in the space-time manifold,
closing the gap in the expressibility between sparse training and dense
training. We further use ITOP to understand the underlying mechanism of Dynamic
Sparse Training (DST) and indicate that the benefits of DST come from its
ability to consider across time all possible parameters when searching for the
optimal sparse connectivity. As long as there are sufficient parameters that
have been reliably explored during training, DST can outperform the dense
neural network by a large margin. We present a series of experiments to support
our conjecture and achieve the state-of-the-art sparse training performance
with ResNet-50 on ImageNet. More impressively, our method achieves dominant
performance over the overparameterization-based sparse methods at extreme
sparsity levels. When trained on CIFAR-100, our method can match the
performance of the dense model even at an extreme sparsity (98%). Code can be
found https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1"&gt;Lu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1"&gt;Decebal Constantin Mocanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EPICURE Ensemble Pretrained Models for Extracting Cancer Mutations from Literature. (arXiv:2106.07722v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07722</id>
        <link href="http://arxiv.org/abs/2106.07722"/>
        <updated>2021-06-16T01:21:06.615Z</updated>
        <summary type="html"><![CDATA[To interpret the genetic profile present in a patient sample, it is necessary
to know which mutations have important roles in the development of the
corresponding cancer type. Named entity recognition is a core step in the text
mining pipeline which facilitates mining valuable cancer information from the
scientific literature. However, due to the scarcity of related datasets,
previous NER attempts in this domain either suffer from low performance when
deep learning based models are deployed, or they apply feature based machine
learning models or rule based models to tackle this problem, which requires
intensive efforts from domain experts, and limit the model generalization
capability. In this paper, we propose EPICURE, an ensemble pre trained model
equipped with a conditional random field pattern layer and a span prediction
pattern layer to extract cancer mutations from text. We also adopt a data
augmentation strategy to expand our training set from multiple datasets.
Experimental results on three benchmark datasets show competitive results
compared to the baseline models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jiarun Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veen_E/0/1/0/all/0/1"&gt;Elke M van Veen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peek_N/0/1/0/all/0/1"&gt;Niels Peek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renehan_A/0/1/0/all/0/1"&gt;Andrew G Renehan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1"&gt;Sophia Ananiadou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcoming Domain Mismatch in Low Resource Sequence-to-Sequence ASR Models using Hybrid Generated Pseudotranscripts. (arXiv:2106.07716v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.07716</id>
        <link href="http://arxiv.org/abs/2106.07716"/>
        <updated>2021-06-16T01:21:06.609Z</updated>
        <summary type="html"><![CDATA[Sequence-to-sequence (seq2seq) models are competitive with hybrid models for
automatic speech recognition (ASR) tasks when large amounts of training data
are available. However, data sparsity and domain adaptation are more
problematic for seq2seq models than their hybrid counterparts. We examine
corpora of five languages from the IARPA MATERIAL program where the transcribed
data is conversational telephone speech (CTS) and evaluation data is broadcast
news (BN). We show that there is a sizable initial gap in such a data condition
between hybrid and seq2seq models, and the hybrid model is able to further
improve through the use of additional language model (LM) data. We use an
additional set of untranscribed data primarily in the BN domain for
semisupervised training. In semisupervised training, a seed model trained on
transcribed data generates hypothesized transcripts for unlabeled
domain-matched data for further training. By using a hybrid model with an
expanded language model for pseudotranscription, we are able to improve our
seq2seq model from an average word error rate (WER) of 66.7% across all five
languages to 29.0% WER. While this puts the seq2seq model at a competitive
operating point, hybrid models are still able to use additional LM data to
maintain an advantage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chak-Fai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keith_F/0/1/0/all/0/1"&gt;Francis Keith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartmann_W/0/1/0/all/0/1"&gt;William Hartmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snover_M/0/1/0/all/0/1"&gt;Matthew Snover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kimball_O/0/1/0/all/0/1"&gt;Owen Kimball&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval. (arXiv:2104.01894v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01894</id>
        <link href="http://arxiv.org/abs/2104.01894"/>
        <updated>2021-06-16T01:21:06.601Z</updated>
        <summary type="html"><![CDATA[Speech-based image retrieval has been studied as a proxy for joint
representation learning, usually without emphasis on retrieval itself. As such,
it is unclear how well speech-based retrieval can work in practice -- both in
an absolute sense and versus alternative strategies that combine automatic
speech recognition (ASR) with strong text encoders. In this work, we
extensively study and expand choices of encoder architectures, training
methodology (including unimodal and multimodal pretraining), and other factors.
Our experiments cover different types of speech in three datasets: Flickr
Audio, Places Audio, and Localized Narratives. Our best model configuration
achieves large gains over state of the art, e.g., pushing recall-at-one from
21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also
show our best speech-based models can match or exceed cascaded ASR-to-text
encoding when speech is spontaneous, accented, or otherwise hard to
automatically transcribe.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1"&gt;Ramon Sanabria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1"&gt;Austin Waters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1"&gt;Jason Baldridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis. (arXiv:2106.07049v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07049</id>
        <link href="http://arxiv.org/abs/2106.07049"/>
        <updated>2021-06-16T01:21:06.580Z</updated>
        <summary type="html"><![CDATA[In the last few years, deep learning classifiers have shown promising results
in image-based medical diagnosis. However, interpreting the outputs of these
models remains a challenge. In cancer diagnosis, interpretability can be
achieved by localizing the region of the input image responsible for the
output, i.e. the location of a lesion. Alternatively, segmentation or detection
models can be trained with pixel-wise annotations indicating the locations of
malignant lesions. Unfortunately, acquiring such labels is labor-intensive and
requires medical expertise. To overcome this difficulty, weakly-supervised
localization can be utilized. These methods allow neural network classifiers to
output saliency maps highlighting the regions of the input most relevant to the
classification task (e.g. malignant lesions in mammograms) using only
image-level labels (e.g. whether the patient has cancer or not) during
training. When applied to high-resolution images, existing methods produce
low-resolution saliency maps. This is problematic in applications in which
suspicious lesions are small in relation to the image size. In this work, we
introduce a novel neural network architecture to perform weakly-supervised
segmentation of high-resolution images. The proposed model selects regions of
interest via coarse-level localization, and then performs fine-grained
segmentation of those regions. We apply this model to breast cancer diagnosis
with screening mammography, and validate it on a large clinically-realistic
dataset. Measured by Dice similarity score, our approach outperforms existing
methods by a large margin in terms of localization performance of benign and
malignant lesions, relatively improving the performance by 39.6% and 20.0%,
respectively. Code and the weights of some of the models are available at
https://github.com/nyukat/GLAM]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kangning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yiqiu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1"&gt;Nan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1"&gt;Jakub Ch&amp;#x142;&amp;#x119;dowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1"&gt;Carlos Fernandez-Granda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1"&gt;Krzysztof J. Geras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Traffic Scenario Clustering by Iterative Optimisation of Self-Supervised Networks Using a Random Forest Activation Pattern Similarity. (arXiv:2105.07639v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07639</id>
        <link href="http://arxiv.org/abs/2105.07639"/>
        <updated>2021-06-16T01:21:06.573Z</updated>
        <summary type="html"><![CDATA[Traffic scenario categorisation is an essential component of automated
driving, for e.\,g., in motion planning algorithms and their validation.
Finding new relevant scenarios without handcrafted steps reduce the required
resources for the development of autonomous driving dramatically. In this work,
a method is proposed to address this challenge by introducing a clustering
technique based on a novel data-adaptive similarity measure, called Random
Forest Activation Pattern (RFAP) similarity. The RFAP similarity is generated
using a tree encoding scheme in a Random Forest algorithm. The clustering
method proposed in this work takes into account that there are labelled
scenarios available and the information from the labelled scenarios can help to
guide the clustering of unlabelled scenarios. It consists of three steps.
First, a self-supervised Convolutional Neural Network~(CNN) is trained on all
available traffic scenarios using a defined self-supervised objective. Second,
the CNN is fine-tuned for classification of the labelled scenarios. Third,
using the labelled and unlabelled scenarios an iterative optimisation procedure
is performed for clustering. In the third step at each epoch of the iterative
optimisation, the CNN is used as a feature generator for an unsupervised Random
Forest. The trained forest, in turn, provides the RFAP similarity to adapt
iteratively the feature generation process implemented by the CNN. Extensive
experiments and ablation studies have been done on the highD dataset. The
proposed method shows superior performance compared to baseline clustering
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_L/0/1/0/all/0/1"&gt;Lakshman Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wurst_J/0/1/0/all/0/1"&gt;Jonas Wurst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Botsch_M/0/1/0/all/0/1"&gt;Michael Botsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1"&gt;Ke Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S2R-DepthNet: Learning a Generalizable Depth-specific Structural Representation. (arXiv:2104.00877v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00877</id>
        <link href="http://arxiv.org/abs/2104.00877"/>
        <updated>2021-06-16T01:21:06.566Z</updated>
        <summary type="html"><![CDATA[Human can infer the 3D geometry of a scene from a sketch instead of a
realistic image, which indicates that the spatial structure plays a fundamental
role in understanding the depth of scenes. We are the first to explore the
learning of a depth-specific structural representation, which captures the
essential feature for depth estimation and ignores irrelevant style
information. Our S2R-DepthNet (Synthetic to Real DepthNet) can be well
generalized to unseen real-world data directly even though it is only trained
on synthetic data. S2R-DepthNet consists of: a) a Structure Extraction (STE)
module which extracts a domaininvariant structural representation from an image
by disentangling the image into domain-invariant structure and domain-specific
style components, b) a Depth-specific Attention (DSA) module, which learns
task-specific knowledge to suppress depth-irrelevant structures for better
depth estimation and generalization, and c) a depth prediction module (DP) to
predict depth from the depth-specific representation. Without access of any
real-world images, our method even outperforms the state-of-the-art
unsupervised domain adaptation methods which use real-world images of the
target domain for training. In addition, when using a small amount of labeled
real-world data, we achieve the state-ofthe-art performance under the
semi-supervised setting. The code and trained models are available at
https://github.com/microsoft/S2R-DepthNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaotian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuwang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuejin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepKoCo: Efficient latent planning with a robust Koopman representation. (arXiv:2011.12690v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12690</id>
        <link href="http://arxiv.org/abs/2011.12690"/>
        <updated>2021-06-16T01:21:06.559Z</updated>
        <summary type="html"><![CDATA[This paper presents DeepKoCo, a novel model-based agent that learns a latent
Koopman representation from images. This representation allows DeepKoCo to plan
efficiently using linear control methods, such as linear model predictive
control. Compared to traditional agents, DeepKoCo is robust to task-irrelevant
dynamics, thanks to the use of a tailored lossy autoencoder network that allows
DeepKoCo to learn latent dynamics that reconstruct and predict only observed
costs, rather than all observed dynamics. As our results show, DeepKoCo
achieves a similar final performance as traditional model-free methods on
complex control tasks, while being considerably more robust to distractor
dynamics, making the proposed agent more amenable for real-life applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heijden_B/0/1/0/all/0/1"&gt;Bas van der Heijden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferranti_L/0/1/0/all/0/1"&gt;Laura Ferranti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1"&gt;Jens Kober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1"&gt;Robert Babuska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Nondestructive Wear Assessment in Large Internal Combustion Engines. (arXiv:2103.08482v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08482</id>
        <link href="http://arxiv.org/abs/2103.08482"/>
        <updated>2021-06-16T01:21:06.552Z</updated>
        <summary type="html"><![CDATA[Digitalization offers a large number of promising tools for large internal
combustion engines such as condition monitoring or condition-based maintenance.
This includes the status evaluation of key engine components such as cylinder
liners, whose inner surfaces are subject to constant wear due to their movement
relative to the pistons. Existing state-of-the-art methods for quantifying wear
require disassembly and cutting of the examined liner followed by a
high-resolution microscopic surface depth measurement that quantitatively
evaluates wear based on bearing load curves (also known as Abbott-Firestone
curves). Such reference methods are destructive, time-consuming and costly. The
goal of the research presented here is to develop nondestructive yet reliable
methods for quantifying the surface condition. A deep-learning framework is
proposed that allows computation of the bearing load curves from reflection RGB
images of the liner surface that can be collected with a wide variety of simple
imaging devices, without the need to remove and destroy the investigated liner.
For this purpose, a convolutional neural network is trained to predict the
bearing load curve of the corresponding depth profile from the collected RGB
images, which in turn can be used for further wear evaluation. Training of the
network is performed using a custom-built database containing depth profiles
and reflection images of liner surfaces of large gas engines. The results of
the proposed method are visually examined and quantified considering several
probabilistic distance metrics and comparison of roughness indicators between
ground truth and model predictions. The observed success of the proposed method
suggests its great potential for quantitative wear assessment on engines during
service directly on site.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Angermann_C/0/1/0/all/0/1"&gt;Christoph Angermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jonsson_S/0/1/0/all/0/1"&gt;Steinbj&amp;#xf6;rn J&amp;#xf3;nsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1"&gt;Markus Haltmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moravova_A/0/1/0/all/0/1"&gt;Ad&amp;#xe9;la Moravov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laubichler_C/0/1/0/all/0/1"&gt;Christian Laubichler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiesling_C/0/1/0/all/0/1"&gt;Constantin Kiesling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kober_M/0/1/0/all/0/1"&gt;Martin Kober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fimml_W/0/1/0/all/0/1"&gt;Wolfgang Fimml&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning and Quantization for Deep Neural Network Acceleration: A Survey. (arXiv:2101.09671v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09671</id>
        <link href="http://arxiv.org/abs/2101.09671"/>
        <updated>2021-06-16T01:21:06.531Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been applied in many applications exhibiting
extraordinary abilities in the field of computer vision. However, complex
network architectures challenge efficient real-time deployment and require
significant computation resources and energy costs. These challenges can be
overcome through optimizations such as network compression. Network compression
can often be realized with little loss of accuracy. In some cases accuracy may
even improve. This paper provides a survey on two types of network compression:
pruning and quantization. Pruning can be categorized as static if it is
performed offline or dynamic if it is performed at run-time. We compare pruning
techniques and describe criteria used to remove redundant computations. We
discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise,
layer-wise and even network-wise pruning. Quantization reduces computations by
reducing the precision of the datatype. Weights, biases, and activations may be
quantized typically to 8-bit integers although lower bit width implementations
are also discussed including binary neural networks. Both pruning and
quantization can be used independently or combined. We compare current
techniques, analyze their strengths and weaknesses, present compressed network
accuracy results on a number of frameworks, and provide practical guidance for
compressing networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1"&gt;Tailin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glossner_J/0/1/0/all/0/1"&gt;John Glossner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shaobo Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaotong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Diffusion Halftoning Against Adversarial Examples. (arXiv:2101.09451v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09451</id>
        <link href="http://arxiv.org/abs/2101.09451"/>
        <updated>2021-06-16T01:21:06.523Z</updated>
        <summary type="html"><![CDATA[Adversarial examples contain carefully crafted perturbations that can fool
deep neural networks (DNNs) into making wrong predictions. Enhancing the
adversarial robustness of DNNs has gained considerable interest in recent
years. Although image transformation-based defenses were widely considered at
an earlier time, most of them have been defeated by adaptive attacks. In this
paper, we propose a new image transformation defense based on error diffusion
halftoning, and combine it with adversarial training to defend against
adversarial examples. Error diffusion halftoning projects an image into a 1-bit
space and diffuses quantization error to neighboring pixels. This process can
remove adversarial perturbations from a given image while maintaining
acceptable image quality in the meantime in favor of recognition. Experimental
results demonstrate that the proposed method is able to improve adversarial
robustness even under advanced adaptive attacks, while most of the other image
transformation-based defenses do not. We show that a proper image
transformation can still be an effective defense approach. Code:
https://github.com/shaoyuanlo/Halftoning-Defense]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shao-Yuan Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Technical Report: Temporal Aggregate Representations. (arXiv:2106.03152v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03152</id>
        <link href="http://arxiv.org/abs/2106.03152"/>
        <updated>2021-06-16T01:21:06.515Z</updated>
        <summary type="html"><![CDATA[This technical report extends our work presented in [9] with more
experiments. In [9], we tackle long-term video understanding, which requires
reasoning from current and past or future observations and raises several
fundamental questions. How should temporal or sequential relationships be
modelled? What temporal extent of information and context needs to be
processed? At what temporal scale should they be derived? [9] addresses these
questions with a flexible multi-granular temporal aggregation framework. In
this report, we conduct further experiments with this framework on different
tasks and a new dataset, EPIC-KITCHENS-100.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sener_F/0/1/0/all/0/1"&gt;Fadime Sener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_D/0/1/0/all/0/1"&gt;Dibyadip Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1"&gt;Angela Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v6 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10762</id>
        <link href="http://arxiv.org/abs/2104.10762"/>
        <updated>2021-06-16T01:21:06.508Z</updated>
        <summary type="html"><![CDATA[Random field and random cluster theory are used to prove certain mathematical
results concerning the probability distribution of image pixel intensities
characterized as generic $2D$ integer arrays. The size of the smallest bounded
region within an image is estimated for segmenting an image, from which, the
equilibrium distribution of intensities can be recovered. From the estimated
bounded regions, properties of the sub-optimal and equilibrium distributions of
intensities are derived, which leads to an image compression methodology
whereby only slightly more than half of all pixels are required for a
worst-case reconstruction of the original image. An example in unsupervised
object detection illustrates the mathematical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1"&gt;Robert A. Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Video Captioning Through Latent Variable Expansion. (arXiv:1910.12019v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.12019</id>
        <link href="http://arxiv.org/abs/1910.12019"/>
        <updated>2021-06-16T01:21:06.484Z</updated>
        <summary type="html"><![CDATA[Automatically describing video content with text description is challenging
but important task, which has been attracting a lot of attention in computer
vision community. Previous works mainly strive for the accuracy of the
generated sentences, while ignoring the sentences diversity, which is
inconsistent with human behavior. In this paper, we aim to caption each video
with multiple descriptions and propose a novel framework. Concretely, for a
given video, the intermediate latent variables of conventional encode-decode
process are utilized as input to the conditional generative adversarial network
(CGAN) with the purpose of generating diverse sentences. We adopt different
Convolutional Neural Networks (CNNs) as our generator that produces
descriptions conditioned on latent variables and discriminator that assesses
the quality of generated sentences. Simultaneously, a novel DCE metric is
designed to assess the diverse captions. We evaluate our method on the
benchmark datasets, where it demonstrates its ability to generate diverse
descriptions and achieves superior results against other state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Huanhou Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jinglun Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Domain Facial Expression Recognition: A Unified Evaluation Benchmark and Adversarial Graph Learning. (arXiv:2008.00923v7 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00923</id>
        <link href="http://arxiv.org/abs/2008.00923"/>
        <updated>2021-06-16T01:21:06.432Z</updated>
        <summary type="html"><![CDATA[To address the problem of data inconsistencies among different facial
expression recognition (FER) datasets, many cross-domain FER methods (CD-FERs)
have been extensively devised in recent years. Although each declares to
achieve superior performance, fair comparisons are lacking due to the
inconsistent choices of the source/target datasets and feature extractors. In
this work, we first analyze the performance effect caused by these inconsistent
choices, and then re-implement some well-performing CD-FER and recently
published domain adaptation algorithms. We ensure that all these algorithms
adopt the same source datasets and feature extractors for fair CD-FER
evaluations. We find that most of the current leading algorithms use
adversarial learning to learn holistic domain-invariant features to mitigate
domain shifts. However, these algorithms ignore local features, which are more
transferable across different datasets and carry more detailed content for
fine-grained adaptation. To address these issues, we integrate graph
representation propagation with adversarial learning for cross-domain
holistic-local feature co-adaptation by developing a novel adversarial graph
representation adaptation (AGRA) framework. Specifically, it first builds two
graphs to correlate holistic and local regions within each domain and across
different domains, respectively. Then, it extracts holistic-local features from
the input image and uses learnable per-class statistical distributions to
initialize the corresponding graph nodes. Finally, two stacked graph
convolution networks (GCNs) are adopted to propagate holistic-local features
within each domain to explore their interaction and across different domains
for holistic-local feature co-adaptation. We conduct extensive and fair
evaluations on several popular benchmarks and show that the proposed AGRA
framework outperforms previous state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianshui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_T/0/1/0/all/0/1"&gt;Tao Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hefeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Psychophysical Oriented Saliency Map Prediction Model. (arXiv:2011.04076v9 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04076</id>
        <link href="http://arxiv.org/abs/2011.04076"/>
        <updated>2021-06-16T01:21:06.287Z</updated>
        <summary type="html"><![CDATA[Visual attention is one of the most significant characteristics for selecting
and understanding the outside redundancy world. The nature of complex scenes
includes enormous redundancy. The human vision system can not process all
information simultaneously because of visual information bottleneck. The human
visual system mainly focuses on dominant parts of the scenes to reduce the
input visual redundancy information. It is commonly known as visual attention
prediction or visual saliency map. This paper proposes a new psychophysical
saliency prediction architecture, WECSF, inspired by human low-level visual
cortex function. The model consists of opponent color channels, wavelet
transform, wavelet energy map, and contrast sensitivity function for extracting
low-level image features and maximum approximation to the human visual system.
The proposed model is evaluated several datasets, including MIT1003, MIT300,
TORONTO, SID4VAM and UCF Sports dataset to explain its efficiency. We also
quantitatively and qualitatively compared the performance of saliency
prediction with other state-of-the-art models. Our model achieved very stable
and good performance. Second, we also confirmed that Fourier and
spectral-inspired saliency prediction models achieved outperformance compared
to other start-of-the-art non-neural networks and even deep neural network
models on psychophysical synthesis images. Finally, the proposed model also can
be applied to spatial-temporal saliency prediction and got better performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A White Paper on Neural Network Quantization. (arXiv:2106.08295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08295</id>
        <link href="http://arxiv.org/abs/2106.08295"/>
        <updated>2021-06-16T01:21:06.270Z</updated>
        <summary type="html"><![CDATA[While neural networks have advanced the frontiers in many applications, they
often come at a high computational cost. Reducing the power and latency of
neural network inference is key if we want to integrate modern networks into
edge devices with strict power and compute requirements. Neural network
quantization is one of the most effective ways of achieving these savings but
the additional noise it induces can lead to accuracy degradation. In this white
paper, we introduce state-of-the-art algorithms for mitigating the impact of
quantization noise on the network's performance while maintaining low-bit
weights and activations. We start with a hardware motivated introduction to
quantization and then consider two main classes of algorithms: Post-Training
Quantization (PTQ) and Quantization-Aware-Training (QAT). PTQ requires no
re-training or labelled data and is thus a lightweight push-button approach to
quantization. In most cases, PTQ is sufficient for achieving 8-bit quantization
with close to floating-point accuracy. QAT requires fine-tuning and access to
labeled training data but enables lower bit quantization with competitive
results. For both solutions, we provide tested pipelines based on existing
literature and extensive experimentation that lead to state-of-the-art
performance for common deep learning models and tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagel_M/0/1/0/all/0/1"&gt;Markus Nagel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fournarakis_M/0/1/0/all/0/1"&gt;Marios Fournarakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amjad_R/0/1/0/all/0/1"&gt;Rana Ali Amjad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bondarenko_Y/0/1/0/all/0/1"&gt;Yelysei Bondarenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baalen_M/0/1/0/all/0/1"&gt;Mart van Baalen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1"&gt;Tijmen Blankevoort&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Modular Should Neural Module Networks Be for Systematic Generalization?. (arXiv:2106.08170v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08170</id>
        <link href="http://arxiv.org/abs/2106.08170"/>
        <updated>2021-06-16T01:21:06.263Z</updated>
        <summary type="html"><![CDATA[Neural Module Networks (NMNs) aim at Visual Question Answering (VQA) via
composition of modules that tackle a sub-task. NMNs are a promising strategy to
achieve systematic generalization, i.e. overcoming biasing factors in the
training distribution. However, the aspects of NMNs that facilitate systematic
generalization are not fully understood. In this paper, we demonstrate that the
stage and the degree at which modularity is defined has large influence on
systematic generalization. In a series of experiments on three VQA datasets
(MNIST with multiple attributes, SQOOP, and CLEVR-CoGenT), our results reveal
that tuning the degree of modularity in the network, especially at the image
encoder stage, reaches substantially higher systematic generalization. These
findings lead to new NMN architectures that outperform previous ones in terms
of systematic generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1"&gt;Vanessa D&amp;#x27;Amario&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1"&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08318</id>
        <link href="http://arxiv.org/abs/2106.08318"/>
        <updated>2021-06-16T01:21:06.254Z</updated>
        <summary type="html"><![CDATA[How can neural networks be trained on large-volume temporal data efficiently?
To compute the gradients required to update parameters, backpropagation blocks
computations until the forward and backward passes are completed. For temporal
signals, this introduces high latency and hinders real-time learning. It also
creates a coupling between consecutive layers, which limits model parallelism
and increases memory consumption. In this paper, we build upon Sideways, which
avoids blocking by propagating approximate gradients forward in time, and we
propose mechanisms for temporal integration of information based on different
variants of skip connections. We also show how to decouple computation and
delegate individual neural modules to different devices, allowing distributed
and parallel training. The proposed Skip-Sideways achieves low latency
training, model parallelism, and, importantly, is capable of extracting
temporal features, leading to more stable training and improved performance on
real-world action recognition video datasets such as HMDB51, UCF101, and the
large-scale Kinetics-600. Finally, we also show that models trained with
Skip-Sideways generate better future frames than Sideways models, and hence
they can better utilize motion cues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1"&gt;Dimitrios Vytiniotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1"&gt;Grzegorz Swirszcz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1"&gt;Viorica Patraucean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Joao Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Color2Style: Real-Time Exemplar-Based Image Colorization with Self-Reference Learning and Deep Feature Modulation. (arXiv:2106.08017v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08017</id>
        <link href="http://arxiv.org/abs/2106.08017"/>
        <updated>2021-06-16T01:21:06.233Z</updated>
        <summary type="html"><![CDATA[Legacy black-and-white photos are riddled with people's nostalgia and
glorious memories of the past. To better relive the elapsed frozen moments, in
this paper, we present a deep exemplar-based image colorization approach named
Color2Style to resurrect these grayscale image media by filling them with
vibrant colors. Generally, for exemplar-based colorization, unsupervised and
unpaired training are usually adopted, due to the difficulty of obtaining input
and ground truth image pairs. To train an exemplar-based colorization model,
current algorithms usually strive to achieve two procedures: i) retrieving a
large number of reference images with high similarity in advance, which is
inevitably time-consuming and tedious; ii) designing complicated modules to
transfer the colors of the reference image to the grayscale image, by
calculating and leveraging the deep semantic correspondence between them (e.g.,
non-local operation). Contrary to the previous methods, we solve and simplify
the above two steps in one end-to-end learning procedure. First, we adopt a
self-augmented self-reference training scheme, where the reference image is
generated by graphical transformations from the original colorful one whereby
the training can be formulated in a paired manner. Second, instead of computing
complex and inexplicable correspondence maps, our method exploits a simple yet
effective deep feature modulation (DFM) module, which injects the color
embeddings extracted from the reference image into the deep representations of
the input grayscale image. Such design is much more lightweight and
intelligible, achieving appealing performance with real-time processing speed.
Moreover, our model does not require multifarious loss functions and
regularization terms like existing methods, but only two widely used loss
functions. Codes and models will be available at
https://github.com/zhaohengyuan1/Color2Style.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hengyuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning with Kernel Dependence Maximization. (arXiv:2106.08320v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.08320</id>
        <link href="http://arxiv.org/abs/2106.08320"/>
        <updated>2021-06-16T01:21:06.223Z</updated>
        <summary type="html"><![CDATA[We approach self-supervised learning of image representations from a
statistical dependence perspective, proposing Self-Supervised Learning with the
Hilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes
dependence between representations of transformed versions of an image and the
image identity, while minimizing the kernelized variance of those features.
This self-supervised learning framework yields a new understanding of InfoNCE,
a variational lower bound on the mutual information (MI) between different
transformations. While the MI itself is known to have pathologies which can
result in meaningless representations being learned, its bound is much better
behaved: we show that it implicitly approximates SSL-HSIC (with a slightly
different regularizer). Our approach also gives us insight into BYOL, since
SSL-HSIC similarly learns local neighborhoods of samples. SSL-HSIC allows us to
directly optimize statistical dependence in time linear in the batch size,
without restrictive data assumptions or indirect mutual information estimators.
Trained with or without a target network, SSL-HSIC matches the current
state-of-the-art for standard linear evaluation on ImageNet, semi-supervised
learning and transfer to other classification and vision tasks such as semantic
segmentation, depth estimation and object recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yazhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pogodin_R/0/1/0/all/0/1"&gt;Roman Pogodin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1"&gt;Danica J. Sutherland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simon Says: Evaluating and Mitigating Bias in Pruned Neural Networks with Knowledge Distillation. (arXiv:2106.07849v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07849</id>
        <link href="http://arxiv.org/abs/2106.07849"/>
        <updated>2021-06-16T01:21:06.215Z</updated>
        <summary type="html"><![CDATA[In recent years the ubiquitous deployment of AI has posed great concerns in
regards to algorithmic bias, discrimination, and fairness. Compared to
traditional forms of bias or discrimination caused by humans, algorithmic bias
generated by AI is more abstract and unintuitive therefore more difficult to
explain and mitigate. A clear gap exists in the current literature on
evaluating and mitigating bias in pruned neural networks. In this work, we
strive to tackle the challenging issues of evaluating, mitigating, and
explaining induced bias in pruned neural networks. Our paper makes three
contributions. First, we propose two simple yet effective metrics, Combined
Error Variance (CEV) and Symmetric Distance Error (SDE), to quantitatively
evaluate the induced bias prevention quality of pruned models. Second, we
demonstrate that knowledge distillation can mitigate induced bias in pruned
neural networks, even with unbalanced datasets. Third, we reveal that model
similarity has strong correlations with pruning induced bias, which provides a
powerful method to explain why bias occurs in pruned neural networks. Our code
is available at https://github.com/codestar12/pruning-distilation-bias]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blakeney_C/0/1/0/all/0/1"&gt;Cody Blakeney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huish_N/0/1/0/all/0/1"&gt;Nathaniel Huish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yan Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1"&gt;Ziliang Zong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Data Augmentation samples for Semantic Segmentation of Salt Bodies in a Synthetic Seismic Image Dataset. (arXiv:2106.08269v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08269</id>
        <link href="http://arxiv.org/abs/2106.08269"/>
        <updated>2021-06-16T01:21:06.208Z</updated>
        <summary type="html"><![CDATA[Nowadays, subsurface salt body localization and delineation, also called
semantic segmentation of salt bodies, are among the most challenging
geophysicist tasks. Thus, identifying large salt bodies is notoriously tricky
and is crucial for identifying hydrocarbon reservoirs and drill path planning.
This work proposes a Data Augmentation method based on training two generative
models to augment the number of samples in a seismic image dataset for the
semantic segmentation of salt bodies. Our method uses deep learning models to
generate pairs of seismic image patches and their respective salt masks for the
Data Augmentation. The first model is a Variational Autoencoder and is
responsible for generating patches of salt body masks. The second is a
Conditional Normalizing Flow model, which receives the generated masks as
inputs and generates the associated seismic image patches. We evaluate the
proposed method by comparing the performance of ten distinct state-of-the-art
models for semantic segmentation, trained with and without the generated
augmentations, in a dataset from two synthetic seismic images. The proposed
methodology yields an average improvement of 8.57% in the IoU metric across all
compared models. The best result is achieved by a DeeplabV3+ model variant,
which presents an IoU score of 95.17% when trained with our augmentations.
Additionally, our proposal outperformed six selected data augmentation methods,
and the most significant improvement in the comparison, of 9.77%, is achieved
by composing our DA with augmentations from an elastic transformation. At last,
we show that the proposed method is adaptable for a larger context size by
achieving results comparable to the obtained on the smaller context size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1"&gt;Luis Felipe Henriques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1"&gt;S&amp;#xe9;rgio Colcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1"&gt;Ruy Luiz Milidi&amp;#xfa;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulcao_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Bulc&amp;#xe3;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barros_P/0/1/0/all/0/1"&gt;Pablo Barros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relation Modeling in Spatio-Temporal Action Localization. (arXiv:2106.08061v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08061</id>
        <link href="http://arxiv.org/abs/2106.08061"/>
        <updated>2021-06-16T01:21:06.201Z</updated>
        <summary type="html"><![CDATA[This paper presents our solution to the AVA-Kinetics Crossover Challenge of
ActivityNet workshop at CVPR 2021. Our solution utilizes multiple types of
relation modeling methods for spatio-temporal action detection and adopts a
training strategy to integrate multiple relation modeling in end-to-end
training over the two large-scale video datasets. Learning with memory bank and
finetuning for long-tailed distribution are also investigated to further
improve the performance. In this paper, we detail the implementations of our
solution and provide experiments results and corresponding discussions. We
finally achieve 40.67 mAP on the test set of AVA-Kinetics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yutong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jianwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1"&gt;Zhiwu Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingqian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Latent Vector Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation. (arXiv:2106.08188v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08188</id>
        <link href="http://arxiv.org/abs/2106.08188"/>
        <updated>2021-06-16T01:21:06.181Z</updated>
        <summary type="html"><![CDATA[This paper addresses the domain shift problem for segmentation. As a
solution, we propose OLVA, a novel and lightweight unsupervised domain
adaptation method based on a Variational Auto-Encoder (VAE) and Optimal
Transport (OT) theory. Thanks to the VAE, our model learns a shared
cross-domain latent space that follows a normal distribution, which reduces the
domain shift. To guarantee valid segmentations, our shared latent space is
designed to model the shape rather than the intensity variations. We further
rely on an OT loss to match and align the remaining discrepancy between the two
domains in the latent space. We demonstrate OLVA's effectiveness for the
segmentation of multiple cardiac structures on the public Multi-Modality Whole
Heart Segmentation (MM-WHS) dataset, where the source domain consists of
annotated 3D MR images and the unlabelled target domain of 3D CTs. Our results
show remarkable improvements with an additional margin of 12.5\% dice score
over concurrent generative training approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chanti_D/0/1/0/all/0/1"&gt;Dawood Al Chanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mateus_D/0/1/0/all/0/1"&gt;Diana Mateus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Head: Unifying Object Detection Heads with Attentions. (arXiv:2106.08322v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08322</id>
        <link href="http://arxiv.org/abs/2106.08322"/>
        <updated>2021-06-16T01:21:06.174Z</updated>
        <summary type="html"><![CDATA[The complex nature of combining localization and classification in object
detection has resulted in the flourished development of methods. Previous works
tried to improve the performance in various object detection heads but failed
to present a unified view. In this paper, we present a novel dynamic head
framework to unify object detection heads with attentions. By coherently
combining multiple self-attention mechanisms between feature levels for
scale-awareness, among spatial locations for spatial-awareness, and within
output channels for task-awareness, the proposed approach significantly
improves the representation ability of object detection heads without any
computational overhead. Further experiments demonstrate that the effectiveness
and efficiency of the proposed dynamic head on the COCO benchmark. With a
standard ResNeXt-101-DCN backbone, we largely improve the performance over
popular object detectors and achieve a new state-of-the-art at 54.0 AP.
Furthermore, with latest transformer backbone and extra data, we can push
current best COCO result to a new record at 60.6 AP. The code will be released
at https://github.com/microsoft/DynamicHead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiyang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinpeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1"&gt;Bin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengchen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Micro-Structured Weight Unification for Neural Network Compression. (arXiv:2106.08301v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08301</id>
        <link href="http://arxiv.org/abs/2106.08301"/>
        <updated>2021-06-16T01:21:06.167Z</updated>
        <summary type="html"><![CDATA[Compressing Deep Neural Network (DNN) models to alleviate the storage and
computation requirements is essential for practical applications, especially
for resource limited devices. Although capable of reducing a reasonable amount
of model parameters, previous unstructured or structured weight pruning methods
can hardly truly accelerate inference, either due to the poor hardware
compatibility of the unstructured sparsity or due to the low sparse rate of the
structurally pruned network. Aiming at reducing both storage and computation,
as well as preserving the original task performance, we propose a generalized
weight unification framework at a hardware compatible micro-structured level to
achieve high amount of compression and acceleration. Weight coefficients of a
selected micro-structured block are unified to reduce the storage and
computation of the block without changing the neuron connections, which turns
to a micro-structured pruning special case when all unified coefficients are
set to zero, where neuron connections (hence storage and computation) are
completely removed. In addition, we developed an effective training framework
based on the alternating direction method of multipliers (ADMM), which converts
our complex constrained optimization into separately solvable subproblems.
Through iteratively optimizing the subproblems, the desired micro-structure can
be ensured with high compression ratio and low performance degradation. We
extensively evaluated our method using a variety of benchmark models and
datasets for different applications. Experimental results demonstrate
state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Sheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Songnan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation. (arXiv:1911.01529v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.01529</id>
        <link href="http://arxiv.org/abs/1911.01529"/>
        <updated>2021-06-16T01:21:06.159Z</updated>
        <summary type="html"><![CDATA[Deep learning approaches have become the standard solution to many problems
in computer vision and robotics, but obtaining sufficient training data in high
enough quality is challenging, as human labor is error prone, time consuming,
and expensive. Solutions based on simulation have become more popular in recent
years, but the gap between simulation and reality is still a major issue. In
this paper, we introduce a novel method for augmenting synthetic image data
through unsupervised image-to-image translation by applying the style of real
world images to simulated images with open source frameworks. The generated
dataset is combined with conventional augmentation methods and is then applied
to a neural network model running in real-time on autonomous soccer robots. Our
evaluation shows a significant improvement compared to models trained on images
generated entirely in simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blumenkamp_J/0/1/0/all/0/1"&gt;Jan Blumenkamp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baude_A/0/1/0/all/0/1"&gt;Andreas Baude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laue_T/0/1/0/all/0/1"&gt;Tim Laue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Thermal Human Faces for Physiological Assessment Using Thermal Sensor Auxiliary Labels. (arXiv:2106.08091v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08091</id>
        <link href="http://arxiv.org/abs/2106.08091"/>
        <updated>2021-06-16T01:21:06.152Z</updated>
        <summary type="html"><![CDATA[Thermal images reveal medically important physiological information about
human stress, signs of inflammation, and emotional mood that cannot be seen on
visible images. Providing a method to generate thermal faces from visible
images would be highly valuable for the telemedicine community in order to show
this medical information. To the best of our knowledge, there are limited works
on visible-to-thermal (VT) face translation, and many current works go the
opposite direction to generate visible faces from thermal surveillance images
(TV) for law enforcement applications. As a result, we introduce favtGAN, a VT
GAN which uses the pix2pix image translation model with an auxiliary sensor
label prediction network for generating thermal faces from visible images.
Since most TV methods are trained on only one data source drawn from one
thermal sensor, we combine datasets from faces and cityscapes. These combined
data are captured from similar sensors in order to bootstrap the training and
transfer learning task, especially valuable because visible-thermal face
datasets are limited. Experiments on these combined datasets show that favtGAN
demonstrates an increase in SSIM and PSNR scores of generated thermal faces,
compared to training on a single face dataset alone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ordun_C/0/1/0/all/0/1"&gt;Catherine Ordun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1"&gt;Edward Raff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purushotham_S/0/1/0/all/0/1"&gt;Sanjay Purushotham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08285</id>
        <link href="http://arxiv.org/abs/2106.08285"/>
        <updated>2021-06-16T01:21:06.132Z</updated>
        <summary type="html"><![CDATA[Time-lapse fluorescent microscopy (TLFM) combined with predictive
mathematical modelling is a powerful tool to study the inherently dynamic
processes of life on the single-cell level. Such experiments are costly,
complex and labour intensive. A complimentary approach and a step towards
completely in silico experiments, is to synthesise the imagery itself. Here, we
propose Multi-StyleGAN as a descriptive approach to simulate time-lapse
fluorescence microscopy imagery of living cells, based on a past experiment.
This novel generative adversarial network synthesises a multi-domain sequence
of consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple
live yeast cells in microstructured environments and train on a dataset
recorded in our laboratory. The simulation captures underlying biophysical
factors and time dependencies, such as cell morphology, growth, physical
interactions, as well as the intensity of a fluorescent reporter protein. An
immediate application is to generate additional training and validation data
for feature extraction algorithms or to aid and expedite development of
advanced experimental techniques such as online monitoring or control of cells.

Code and dataset is available at
https://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1"&gt;Tim Prangemeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1"&gt;Christoph Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1"&gt;Christian Wildner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1"&gt;Heinz Koeppl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top-Related Meta-Learning Method for Few-Shot Object Detection. (arXiv:2007.06837v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06837</id>
        <link href="http://arxiv.org/abs/2007.06837"/>
        <updated>2021-06-16T01:21:06.125Z</updated>
        <summary type="html"><![CDATA[Many meta-learning methods are proposed for few-shot detection. However,
previous most methods have two main problems, poor detection APs, and strong
bias because of imbalance and insufficient datasets. Previous works mainly
alleviate these issues by additional datasets, multi-relation attention
mechanisms and sub-modules. However, they require more cost. In this work, for
meta-learning, we find that the main challenges focus on related or irrelevant
semantic features between categories. Therefore, based on semantic features, we
propose a Top-C classification loss (i.e., TCL-C) for classification task and a
category-based grouping mechanism for category-based meta-features obtained by
the meta-model. The TCL-C exploits the true-label prediction and the most
likely C-1 false classification predictions to improve detection performance on
few-shot classes. According to similar appearance (i.e., visual appearance,
shape, and limbs etc.) and environment in which objects often appear, the
category-based grouping mechanism splits categories into disjoint groups to
make similar semantic features more compact between categories within a group
and obtain more significant difference between groups, alleviating the strong
bias problem and further improving detection APs. The whole training consists
of the base model and the fine-tuning phases. According to grouping mechanism,
we group the meta-features vectors obtained by meta-model, so that the
distribution difference between groups is obvious, and the one within each
group is less. Extensive experiments on Pascal VOC dataset demonstrate that
ours which combines the TCL-C with category-based grouping significantly
outperforms previous state-of-the-art methods for few-shot detection. Compared
with previous competitive baseline, ours improves detection APs by almost 4%
for few-shot detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1"&gt;Nan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaochun Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Duo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Dongrui Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhimin Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is this Harmful? Learning to Predict Harmfulness Ratings from Video. (arXiv:2106.08323v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08323</id>
        <link href="http://arxiv.org/abs/2106.08323"/>
        <updated>2021-06-16T01:21:06.118Z</updated>
        <summary type="html"><![CDATA[Automatically identifying harmful content in video is an important task with
a wide range of applications. However, due to the difficulty of collecting
high-quality labels as well as demanding computational requirements, the task
has not had a satisfying general approach. Typically, only small subsets of the
problem are considered, such as identifying violent content. In cases where the
general problem is tackled, rough approximations and simplifications are made
to deal with the lack of labels and computational complexity. In this work, we
identify and tackle the two main obstacles. First, we create a dataset of
approximately 4000 video clips, annotated by professionals in the field.
Secondly, we demonstrate that advances in video recognition enable training
models on our dataset that consider the full context of the scene. We conduct
an in-depth study on our modeling choices and find that we greatly benefit from
combining the visual and audio modality and that pretraining on large-scale
video recognition datasets and class balanced sampling further improves
performance. We additionally perform a qualitative study that reveals the
heavily multi-modal nature of our dataset. Our dataset will be made available
upon publication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edstedt_J/0/1/0/all/0/1"&gt;Johan Edstedt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlsson_J/0/1/0/all/0/1"&gt;Johan Karlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benavente_F/0/1/0/all/0/1"&gt;Francisca Benavente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novak_A/0/1/0/all/0/1"&gt;Anette Novak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1"&gt;Amanda Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1"&gt;Michael Felsberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-sample surface defect detection and classification based on semantic feedback neural network. (arXiv:2106.07959v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07959</id>
        <link href="http://arxiv.org/abs/2106.07959"/>
        <updated>2021-06-16T01:21:06.111Z</updated>
        <summary type="html"><![CDATA[Defect detection and classification technology has changed from traditional
artificial visual inspection to current intelligent automated inspection, but
most of the current defect detection methods are training related detection
models based on a data-driven approach, taking into account the difficulty of
collecting some sample data in the industrial field. We apply zero-shot
learning technology to the industrial field. Aiming at the problem of the
existing "Latent Feature Guide Attribute Attention" (LFGAA) zero-shot image
classification network, the output latent attributes and artificially defined
attributes are different in the semantic space, which leads to the problem of
model performance degradation, proposed an LGFAA network based on semantic
feedback, and improved model performance by constructing semantic embedded
modules and feedback mechanisms. At the same time, for the common domain shift
problem in zero-shot learning, based on the idea of co-training algorithm using
the difference information between different views of data to learn from each
other, we propose an Ensemble Co-training algorithm, which adaptively reduces
the prediction error in image tag embedding from multiple angles. Various
experiments conducted on the zero-shot dataset and the cylinder liner dataset
in the industrial field provide competitive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yibo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yiming Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1"&gt;Zhiyang Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haidi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_W/0/1/0/all/0/1"&gt;Wenhua Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingliang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixed Model OCR Training on Historical Latin Script for Out-of-the-Box Recognition and Finetuning. (arXiv:2106.07881v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07881</id>
        <link href="http://arxiv.org/abs/2106.07881"/>
        <updated>2021-06-16T01:21:06.103Z</updated>
        <summary type="html"><![CDATA[In order to apply Optical Character Recognition (OCR) to historical printings
of Latin script fully automatically, we report on our efforts to construct a
widely-applicable polyfont recognition model yielding text with a Character
Error Rate (CER) around 2% when applied out-of-the-box. Moreover, we show how
this model can be further finetuned to specific classes of printings with
little manual and computational effort. The mixed or polyfont model is trained
on a wide variety of materials, in terms of age (from the 15th to the 19th
century), typography (various types of Fraktur and Antiqua), and languages
(among others, German, Latin, and French). To optimize the results we combined
established techniques of OCR training like pretraining, data augmentation, and
voting. In addition, we used various preprocessing methods to enrich the
training data and obtain more robust models. We also implemented a two-stage
approach which first trains on all available, considerably unbalanced data and
then refines the output by training on a selected more balanced subset.
Evaluations on 29 previously unseen books resulted in a CER of 1.73%,
outperforming a widely used standard model with a CER of 2.84% by almost 40%.
Training a more specialized model for some unseen Early Modern Latin books
starting from our mixed model led to a CER of 1.47%, an improvement of up to
50% compared to training from scratch and up to 30% compared to training from
the aforementioned standard model. Our new mixed model is made openly available
to the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reul_C/0/1/0/all/0/1"&gt;Christian Reul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wick_C/0/1/0/all/0/1"&gt;Christoph Wick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noth_M/0/1/0/all/0/1"&gt;Maximilian N&amp;#xf6;th&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buttner_A/0/1/0/all/0/1"&gt;Andreas B&amp;#xfc;ttner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wehner_M/0/1/0/all/0/1"&gt;Maximilian Wehner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Springmann_U/0/1/0/all/0/1"&gt;Uwe Springmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Human Pose and Mesh Reconstruction with Transformers. (arXiv:2012.09760v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09760</id>
        <link href="http://arxiv.org/abs/2012.09760"/>
        <updated>2021-06-16T01:21:06.083Z</updated>
        <summary type="html"><![CDATA[We present a new method, called MEsh TRansfOrmer (METRO), to reconstruct 3D
human pose and mesh vertices from a single image. Our method uses a transformer
encoder to jointly model vertex-vertex and vertex-joint interactions, and
outputs 3D joint coordinates and mesh vertices simultaneously. Compared to
existing techniques that regress pose and shape parameters, METRO does not rely
on any parametric mesh models like SMPL, thus it can be easily extended to
other objects such as hands. We further relax the mesh topology and allow the
transformer self-attention mechanism to freely attend between any two vertices,
making it possible to learn non-local relationships among mesh vertices and
joints. With the proposed masked vertex modeling, our method is more robust and
effective in handling challenging situations like partial occlusions. METRO
generates new state-of-the-art results for human mesh reconstruction on the
public Human3.6M and 3DPW datasets. Moreover, we demonstrate the
generalizability of METRO to 3D hand reconstruction in the wild, outperforming
existing state-of-the-art methods on FreiHAND dataset. Code and pre-trained
models are available at https://github.com/microsoft/MeshTransformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kevin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcomplete Representations Against Adversarial Videos. (arXiv:2012.04262v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04262</id>
        <link href="http://arxiv.org/abs/2012.04262"/>
        <updated>2021-06-16T01:21:06.062Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness of deep neural networks is an extensively studied
problem in the literature and various methods have been proposed to defend
against adversarial images. However, only a handful of defense methods have
been developed for defending against attacked videos. In this paper, we propose
a novel Over-and-Under complete restoration network for Defending against
adversarial videos (OUDefend). Most restoration networks adopt an
encoder-decoder architecture that first shrinks spatial dimension then expands
it back. This approach learns undercomplete representations, which have large
receptive fields to collect global information but overlooks local details. On
the other hand, overcomplete representations have opposite properties. Hence,
OUDefend is designed to balance local and global features by learning those two
representations. We attach OUDefend to target video recognition models as a
feature restoration block and train the entire network end-to-end. Experimental
results show that the defenses focusing on images may be ineffective to videos,
while OUDefend enhances robustness against different types of adversarial
videos, ranging from additive attacks, multiplicative attacks to physically
realizable attacks. Code: https://github.com/shaoyuanlo/OUDefend]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shao-Yuan Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1"&gt;Jeya Maria Jose Valanarasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encouraging Intra-Class Diversity Through a Reverse Contrastive Loss for Better Single-Source Domain Generalization. (arXiv:2106.07916v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07916</id>
        <link href="http://arxiv.org/abs/2106.07916"/>
        <updated>2021-06-16T01:21:06.047Z</updated>
        <summary type="html"><![CDATA[Traditional deep learning algorithms often fail to generalize when they are
tested outside of the domain of training data. Because data distributions can
change dynamically in real-life applications once a learned model is deployed,
in this paper we are interested in single-source domain generalization (SDG)
which aims to develop deep learning algorithms able to generalize from a single
training domain where no information about the test domain is available at
training time. Firstly, we design two simple MNISTbased SDG benchmarks, namely
MNIST Color SDG-MP and MNIST Color SDG-UP, which highlight the two different
fundamental SDG issues of increasing difficulties: 1) a class-correlated
pattern in the training domain is missing (SDG-MP), or 2) uncorrelated with the
class (SDG-UP), in the testing data domain. This is in sharp contrast with the
current domain generalization (DG) benchmarks which mix up different
correlation and variation factors and thereby make hard to disentangle success
or failure factors when benchmarking DG algorithms. We further evaluate several
state-of-the-art SDG algorithms through our simple benchmark, namely MNIST
Color SDG-MP, and show that the issue SDG-MP is largely unsolved despite of a
decade of efforts in developing DG algorithms. Finally, we also propose a
partially reversed contrastive loss to encourage intra-class diversity and find
less strongly correlated patterns, to deal with SDG-MP and show that the
proposed approach is very effective on our MNIST Color SDG-MP benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duboudin_T/0/1/0/all/0/1"&gt;Thomas Duboudin&lt;/a&gt; (imagine), &lt;a href="http://arxiv.org/find/cs/1/au:+Dellandrea_E/0/1/0/all/0/1"&gt;Emmanuel Dellandr&amp;#xe9;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abgrall_C/0/1/0/all/0/1"&gt;Corentin Abgrall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henaff_G/0/1/0/all/0/1"&gt;Gilles H&amp;#xe9;naff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demographic Fairness in Face Identification: The Watchlist Imbalance Effect. (arXiv:2106.08049v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08049</id>
        <link href="http://arxiv.org/abs/2106.08049"/>
        <updated>2021-06-16T01:21:05.982Z</updated>
        <summary type="html"><![CDATA[Recently, different researchers have found that the gallery composition of a
face database can induce performance differentials to facial identification
systems in which a probe image is compared against up to all stored reference
images to reach a biometric decision. This negative effect is referred to as
"watchlist imbalance effect". In this work, we present a method to
theoretically estimate said effect for a biometric identification system given
its verification performance across demographic groups and the composition of
the used gallery. Further, we report results for identification experiments on
differently composed demographic subsets, i.e. females and males, of the public
academic MORPH database using the open-source ArcFace face recognition system.
It is shown that the database composition has a huge impact on performance
differentials in biometric identification systems, even if performance
differentials are less pronounced in the verification scenario. This study
represents the first detailed analysis of the watchlist imbalance effect which
is expected to be of high interest for future research in the field of facial
recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1"&gt;Pawel Drozdowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1"&gt;Christian Rathgeb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1"&gt;Christoph Busch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ResDepth: A Deep Prior For 3D Reconstruction From High-resolution Satellite Images. (arXiv:2106.08107v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08107</id>
        <link href="http://arxiv.org/abs/2106.08107"/>
        <updated>2021-06-16T01:21:05.973Z</updated>
        <summary type="html"><![CDATA[Modern optical satellite sensors enable high-resolution stereo reconstruction
from space. But the challenging imaging conditions when observing the Earth
from space push stereo matching to its limits. In practice, the resulting
digital surface models (DSMs) are fairly noisy and often do not attain the
accuracy needed for high-resolution applications such as 3D city modeling.
Arguably, stereo correspondence based on low-level image similarity is
insufficient and should be complemented with a-priori knowledge about the
expected surface geometry beyond basic local smoothness. To that end, we
introduce ResDepth, a convolutional neural network that learns such an
expressive geometric prior from example data. ResDepth refines an initial, raw
stereo DSM while conditioning the refinement on the images. I.e., it acts as a
smart, learned post-processing filter and can seamlessly complement any stereo
matching pipeline. In a series of experiments, we find that the proposed method
consistently improves stereo DSMs both quantitatively and qualitatively. We
show that the prior encoded in the network weights captures meaningful
geometric characteristics of urban design, which also generalize across
different districts and even from one city to another. Moreover, we demonstrate
that, by training on a variety of stereo pairs, ResDepth can acquire a
sufficient degree of invariance against variations in imaging conditions and
acquisition geometry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Stucker_C/0/1/0/all/0/1"&gt;Corinne Stucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting the Calibration of Modern Neural Networks. (arXiv:2106.07998v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07998</id>
        <link href="http://arxiv.org/abs/2106.07998"/>
        <updated>2021-06-16T01:21:05.905Z</updated>
        <summary type="html"><![CDATA[Accurate estimation of predictive uncertainty (model calibration) is
essential for the safe application of neural networks. Many instances of
miscalibration in modern neural networks have been reported, suggesting a trend
that newer, more accurate models produce poorly calibrated predictions. Here,
we revisit this question for recent state-of-the-art image classification
models. We systematically relate model calibration and accuracy, and find that
the most recent models, notably those not using convolutions, are among the
best calibrated. Trends observed in prior model generations, such as decay of
calibration with distribution shift or model size, are less pronounced in
recent architectures. We also show that model size and amount of pretraining do
not fully explain these differences, suggesting that architecture is a major
determinant of calibration properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1"&gt;Matthias Minderer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1"&gt;Josip Djolonga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1"&gt;Rob Romijnders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubis_F/0/1/0/all/0/1"&gt;Frances Hubis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Dustin Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1"&gt;Mario Lucic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection. (arXiv:2106.07852v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07852</id>
        <link href="http://arxiv.org/abs/2106.07852"/>
        <updated>2021-06-16T01:21:05.888Z</updated>
        <summary type="html"><![CDATA[Non-parametric face modeling aims to reconstruct 3D face only from images
without shape assumptions. While plausible facial details are predicted, the
models tend to over-depend on local color appearance and suffer from ambiguous
noise. To address such problem, this paper presents a novel Learning to
Aggregate and Personalize (LAP) framework for unsupervised robust 3D face
modeling. Instead of using controlled environment, the proposed method
implicitly disentangles ID-consistent and scene-specific face from
unconstrained photo set. Specifically, to learn ID-consistent face, LAP
adaptively aggregates intrinsic face factors of an identity based on a novel
curriculum learning approach with relaxed consistency loss. To adapt the face
for a personalized scene, we propose a novel attribute-refining network to
modify ID-consistent face with target attribute and details. Based on the
proposed method, we make unsupervised 3D face modeling benefit from meaningful
image facial structure and possibly higher resolutions. Extensive experiments
on benchmarks show LAP recovers superior or competitive face shape and texture,
compared with state-of-the-art (SOTA) methods with or without prior and
supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yanhao Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Renwang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yan Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-script Handwritten Digit Recognition Using Multi-task Learning. (arXiv:2106.08267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08267</id>
        <link href="http://arxiv.org/abs/2106.08267"/>
        <updated>2021-06-16T01:21:05.878Z</updated>
        <summary type="html"><![CDATA[Handwritten digit recognition is one of the extensively studied area in
machine learning. Apart from the wider research on handwritten digit
recognition on MNIST dataset, there are many other research works on various
script recognition. However, it is not very common for multi-script digit
recognition which encourage the development of robust and multipurpose systems.
Additionally working on multi-script digit recognition enables multi-task
learning, considering the script classification as a related task for instance.
It is evident that multi-task learning improves model performance through
inductive transfer using the information contained in related tasks. Therefore,
in this study multi-script handwritten digit recognition using multi-task
learning will be investigated. As a specific case of demonstrating the solution
to the problem, Amharic handwritten character recognition will also be
experimented. The handwritten digits of three scripts including Latin, Arabic
and Kannada are studied to show that multi-task models with reformulation of
the individual tasks have shown promising results. In this study a novel way of
using the individual tasks predictions was proposed to help classification
performance and regularize the different loss for the purpose of the main task.
This finding has outperformed the baseline and the conventional multi-task
learning models. More importantly, it avoided the need for weighting the
different losses of the tasks, which is one of the challenges in multi-task
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gondere_M/0/1/0/all/0/1"&gt;Mesay Samuel Gondere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1"&gt;Lars Schmidt-Thieme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1"&gt;Durga Prasad Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholz_R/0/1/0/all/0/1"&gt;Randolf Scholz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physion: Evaluating Physical Prediction from Vision in Humans and Machines. (arXiv:2106.08261v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.08261</id>
        <link href="http://arxiv.org/abs/2106.08261"/>
        <updated>2021-06-16T01:21:05.871Z</updated>
        <summary type="html"><![CDATA[While machine learning algorithms excel at many challenging visual tasks, it
is unclear that they can make predictions about commonplace real world physical
events. Here, we present a visual and physical prediction benchmark that
precisely measures this capability. In realistically simulating a wide variety
of physical phenomena -- rigid and soft-body collisions, stable multi-object
configurations, rolling and sliding, projectile motion -- our dataset presents
a more comprehensive challenge than existing benchmarks. Moreover, we have
collected human responses for our stimuli so that model predictions can be
directly compared to human judgments. We compare an array of algorithms --
varying in their architecture, learning objective, input-output structure, and
training data -- on their ability to make diverse physical predictions. We find
that graph neural networks with access to the physical state best capture human
behavior, whereas among models that receive only visual input, those with
object-centric representations or pretraining do best but fall far short of
human accuracy. This suggests that extracting physically meaningful
representations of scenes is the main bottleneck to achieving human-like visual
prediction. We thus demonstrate how our benchmark can identify areas for
improvement and measure progress on this key aspect of physical understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bear_D/0/1/0/all/0/1"&gt;Daniel M. Bear&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Elias Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mrowca_D/0/1/0/all/0/1"&gt;Damian Mrowca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_F/0/1/0/all/0/1"&gt;Felix J. Binder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1"&gt;Hsiau-Yu Fish Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pramod_R/0/1/0/all/0/1"&gt;R.T. Pramod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holdaway_C/0/1/0/all/0/1"&gt;Cameron Holdaway&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1"&gt;Sirui Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kevin Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanwisher_N/0/1/0/all/0/1"&gt;Nancy Kanwisher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1"&gt;Daniel L.K. Yamins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Judith E. Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Lightweight ReLU-Based Feature Fusion for Aerial Scene Classification. (arXiv:2106.07879v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.07879</id>
        <link href="http://arxiv.org/abs/2106.07879"/>
        <updated>2021-06-16T01:21:05.863Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a transfer-learning based model construction
technique for the aerial scene classification problem. The core of our
technique is a layer selection strategy, named ReLU-Based Feature Fusion
(RBFF), that extracts feature maps from a pretrained CNN-based single-object
image classification model, namely MobileNetV2, and constructs a model for the
aerial scene classification task. RBFF stacks features extracted from the batch
normalization layer of a few selected blocks of MobileNetV2, where the
candidate blocks are selected based on the characteristics of the ReLU
activation layers present in those blocks. The feature vector is then
compressed into a low-dimensional feature space using dimension reduction
algorithms on which we train a low-cost SVM classifier for the classification
of the aerial images. We validate our choice of selected features based on the
significance of the extracted features with respect to our classification
pipeline. RBFF remarkably does not involve any training of the base CNN model
except for a few parameters for the classifier, which makes the technique very
cost-effective for practical deployments. The constructed model despite being
lightweight outperforms several recently proposed models in terms of accuracy
for a number of aerial scene datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Arefeen_M/0/1/0/all/0/1"&gt;Md Adnan Arefeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nimi_S/0/1/0/all/0/1"&gt;Sumaiya Tabassum Nimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Uddin_M/0/1/0/all/0/1"&gt;Md Yusuf Sarwar Uddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEiT: BERT Pre-Training of Image Transformers. (arXiv:2106.08254v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08254</id>
        <link href="http://arxiv.org/abs/2106.08254"/>
        <updated>2021-06-16T01:21:05.844Z</updated>
        <summary type="html"><![CDATA[We introduce a self-supervised vision representation model BEiT, which stands
for Bidirectional Encoder representation from Image Transformers. Following
BERT developed in the natural language processing area, we propose a masked
image modeling task to pretrain vision Transformers. Specifically, each image
has two views in our pre-training, i.e, image patches (such as 16x16 pixels),
and visual tokens (i.e., discrete tokens). We first "tokenize" the original
image into visual tokens. Then we randomly mask some image patches and fed them
into the backbone Transformer. The pre-training objective is to recover the
original visual tokens based on the corrupted image patches. After pre-training
BEiT, we directly fine-tune the model parameters on downstream tasks by
appending task layers upon the pretrained encoder. Experimental results on
image classification and semantic segmentation show that our model achieves
competitive results with previous pre-training methods. For example, base-size
BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming
from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size
BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with
supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models
are available at https://aka.ms/beit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hangbo Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08038</id>
        <link href="http://arxiv.org/abs/2106.08038"/>
        <updated>2021-06-16T01:21:05.837Z</updated>
        <summary type="html"><![CDATA[Averaging predictions over a set of models -- an ensemble -- is widely used
to improve predictive performance and uncertainty estimation of deep learning
models. At the same time, many machine learning systems, such as search,
matching, and recommendation systems, heavily rely on embeddings.
Unfortunately, due to misalignment of features of independently trained models,
embeddings, cannot be improved with a naive deep ensemble like approach. In
this work, we look at the ensembling of representations and propose mean
embeddings with test-time augmentation (MeTTA) simple yet well-performing
recipe for ensembling representations. Empirically we demonstrate that MeTTA
significantly boosts the quality of linear evaluation on ImageNet for both
supervised and self-supervised models. Even more exciting, we draw connections
between MeTTA, image retrieval, and transformation invariant models. We believe
that spreading the success of ensembles to inference higher-quality
representations is the important step that will open many new applications of
ensembling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1"&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1"&gt;Andrei Atanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Out-of-Distribution Detection on Deep Probabilistic Generative Models. (arXiv:2106.07903v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07903</id>
        <link href="http://arxiv.org/abs/2106.07903"/>
        <updated>2021-06-16T01:21:05.830Z</updated>
        <summary type="html"><![CDATA[Out-of-distribution (OOD) detection is an important task in machine learning
systems for ensuring their reliability and safety. Deep probabilistic
generative models facilitate OOD detection by estimating the likelihood of a
data sample. However, such models frequently assign a suspiciously high
likelihood to a specific outlier. Several recent works have addressed this
issue by training a neural network with auxiliary outliers, which are generated
by perturbing the input data. In this paper, we discover that these approaches
fail for certain OOD datasets. Thus, we suggest a new detection metric that
operates without outlier exposure. We observe that our metric is robust to
diverse variations of an image compared to the previous outlier-exposing
methods. Furthermore, our proposed score requires neither auxiliary models nor
additional training. Instead, this paper utilizes the likelihood ratio
statistic in a new perspective to extract genuine properties from the given
single deep probabilistic generative model. We also apply a novel numerical
approximation to enable fast implementation. Finally, we demonstrate
comprehensive experiments on various probabilistic generative models and show
that our method achieves state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jaemoo Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1"&gt;Changyeon Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1"&gt;Jeongwoo Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Myungjoo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Total Recall in Industrial Anomaly Detection. (arXiv:2106.08265v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08265</id>
        <link href="http://arxiv.org/abs/2106.08265"/>
        <updated>2021-06-16T01:21:05.822Z</updated>
        <summary type="html"><![CDATA[Being able to spot defective parts is a critical component in large-scale
industrial manufacturing. A particular challenge that we address in this work
is the cold-start problem: fit a model using nominal (non-defective) example
images only. While handcrafted solutions per class are possible, the goal is to
build systems that work well simultaneously on many different tasks
automatically. The best peforming approaches combine embeddings from ImageNet
models with an outlier detection model. In this paper, we extend on this line
of work and propose PatchCore, which uses a maximally representative memory
bank of nominal patch-features. PatchCore offers competitive inference times
while achieving state-of-the-art performance for both detection and
localization. On the standard dataset MVTec AD, PatchCore achieves an
image-level anomaly detection AUROC score of $99.1\%$, more than halving the
error compared to the next best competitor. We further report competitive
results on two additional datasets and also find competitive results in the few
samples regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1"&gt;Karsten Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pemula_L/0/1/0/all/0/1"&gt;Latha Pemula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zepeda_J/0/1/0/all/0/1"&gt;Joaquin Zepeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1"&gt;Thomas Brox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1"&gt;Peter Gehler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object detection and Autoencoder-based 6D pose estimation for highly cluttered Bin Picking. (arXiv:2106.08045v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08045</id>
        <link href="http://arxiv.org/abs/2106.08045"/>
        <updated>2021-06-16T01:21:05.815Z</updated>
        <summary type="html"><![CDATA[Bin picking is a core problem in industrial environments and robotics, with
its main module as 6D pose estimation. However, industrial depth sensors have a
lack of accuracy when it comes to small objects. Therefore, we propose a
framework for pose estimation in highly cluttered scenes with small objects,
which mainly relies on RGB data and makes use of depth information only for
pose refinement. In this work, we compare synthetic data generation approaches
for object detection and pose estimation and introduce a pose filtering
algorithm that determines the most accurate estimated poses. We will make our]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hofer_T/0/1/0/all/0/1"&gt;Timon H&amp;#xf6;fer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamsafar_F/0/1/0/all/0/1"&gt;Faranak Shamsafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benbarka_N/0/1/0/all/0/1"&gt;Nuri Benbarka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1"&gt;Andreas Zell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated triaging of head MRI examinations using convolutional neural networks. (arXiv:2106.08176v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08176</id>
        <link href="http://arxiv.org/abs/2106.08176"/>
        <updated>2021-06-16T01:21:05.796Z</updated>
        <summary type="html"><![CDATA[The growing demand for head magnetic resonance imaging (MRI) examinations,
along with a global shortage of radiologists, has led to an increase in the
time taken to report head MRI scans around the world. For many neurological
conditions, this delay can result in increased morbidity and mortality. An
automated triaging tool could reduce reporting times for abnormal examinations
by identifying abnormalities at the time of imaging and prioritizing the
reporting of these scans. In this work, we present a convolutional neural
network for detecting clinically-relevant abnormalities in
$\text{T}_2$-weighted head MRI scans. Using a validated neuroradiology report
classifier, we generated a labelled dataset of 43,754 scans from two large UK
hospitals for model training, and demonstrate accurate classification (area
under the receiver operating curve (AUC) = 0.943) on a test set of 800 scans
labelled by a team of neuroradiologists. Importantly, when trained on scans
from only a single hospital the model generalized to scans from the other
hospital ($\Delta$AUC $\leq$ 0.02). A simulation study demonstrated that our
model would reduce the mean reporting time for abnormal examinations from 28
days to 14 days and from 9 days to 5 days at the two hospitals, demonstrating
feasibility for use in a clinical triage environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wood_D/0/1/0/all/0/1"&gt;David A. Wood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kafiabadi_S/0/1/0/all/0/1"&gt;Sina Kafiabadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Busaidi_A/0/1/0/all/0/1"&gt;Ayisha Al Busaidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guilhem_E/0/1/0/all/0/1"&gt;Emily Guilhem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Montvila_A/0/1/0/all/0/1"&gt;Antanas Montvila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Siddharth Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lynch_J/0/1/0/all/0/1"&gt;Jeremy Lynch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Townend_M/0/1/0/all/0/1"&gt;Matthew Townend&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barker_G/0/1/0/all/0/1"&gt;Gareth Barker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cole_J/0/1/0/all/0/1"&gt;James H. Cole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Booth_T/0/1/0/all/0/1"&gt;Thomas C. Booth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Photo-realistic Texture Generation for 3D Face Reconstruction. (arXiv:2106.08148v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08148</id>
        <link href="http://arxiv.org/abs/2106.08148"/>
        <updated>2021-06-16T01:21:05.789Z</updated>
        <summary type="html"><![CDATA[Although much progress has been made recently in 3D face reconstruction, most
previous work has been devoted to predicting accurate and fine-grained 3D
shapes. In contrast, relatively little work has focused on generating
high-fidelity face textures. Compared with the prosperity of photo-realistic 2D
face image generation, high-fidelity 3D face texture generation has yet to be
studied. In this paper, we proposed a novel UV map generation model that
predicts the UV map from a single face image. The model consists of a UV
sampler and a UV generator. By selectively sampling the input face image's
pixels and adjusting their relative locations, the UV sampler generates an
incomplete UV map that could faithfully reconstruct the original face. Missing
textures in the incomplete UV map are further full-filled by the UV generator.
The training is based on pseudo ground truth blended by the 3DMM texture and
the input face texture, thus weakly supervised. To deal with the artifacts in
the imperfect pseudo UV map, multiple partial UV map discriminators are
leveraged.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xiangnan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Di Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zehua Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic linear measurements of the fetal brain on MRI with deep neural networks. (arXiv:2106.08174v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08174</id>
        <link href="http://arxiv.org/abs/2106.08174"/>
        <updated>2021-06-16T01:21:05.781Z</updated>
        <summary type="html"><![CDATA[Timely, accurate and reliable assessment of fetal brain development is
essential to reduce short and long-term risks to fetus and mother. Fetal MRI is
increasingly used for fetal brain assessment. Three key biometric linear
measurements important for fetal brain evaluation are Cerebral Biparietal
Diameter (CBD), Bone Biparietal Diameter (BBD), and Trans-Cerebellum Diameter
(TCD), obtained manually by expert radiologists on reference slices, which is
time consuming and prone to human error. The aim of this study was to develop a
fully automatic method computing the CBD, BBD and TCD measurements from fetal
brain MRI. The input is fetal brain MRI volumes which may include the fetal
body and the mother's abdomen. The outputs are the measurement values and
reference slices on which the measurements were computed. The method, which
follows the manual measurements principle, consists of five stages: 1)
computation of a Region Of Interest that includes the fetal brain with an
anisotropic 3D U-Net classifier; 2) reference slice selection with a
Convolutional Neural Network; 3) slice-wise fetal brain structures segmentation
with a multiclass U-Net classifier; 4) computation of the fetal brain
midsagittal line and fetal brain orientation, and; 5) computation of the
measurements. Experimental results on 214 volumes for CBD, BBD and TCD
measurements yielded a mean $L_1$ difference of 1.55mm, 1.45mm and 1.23mm
respectively, and a Bland-Altman 95% confidence interval ($CI_{95}$) of 3.92mm,
3.98mm and 2.25mm respectively. These results are similar to the manual
inter-observer variability. The proposed automatic method for computing
biometric linear measurements of the fetal brain from MR imaging achieves human
level performance. It has the potential of being a useful method for the
assessment of fetal brain biometry in normal and pathological cases, and of
improving routine clinical practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Avisdris_N/0/1/0/all/0/1"&gt;Netanell Avisdris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yehuda_B/0/1/0/all/0/1"&gt;Bossmat Yehuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ben_Zvi_O/0/1/0/all/0/1"&gt;Ori Ben-Zvi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Link_Sourani_D/0/1/0/all/0/1"&gt;Daphna Link-Sourani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ben_Sira_L/0/1/0/all/0/1"&gt;Liat Ben-Sira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Miller_E/0/1/0/all/0/1"&gt;Elka Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zharkov_E/0/1/0/all/0/1"&gt;Elena Zharkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bashat_D/0/1/0/all/0/1"&gt;Dafna Ben Bashat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Joskowicz_L/0/1/0/all/0/1"&gt;Leo Joskowicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending Touch-based Continuous Authentication Systems from Active Adversaries Using Generative Adversarial Networks. (arXiv:2106.07867v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.07867</id>
        <link href="http://arxiv.org/abs/2106.07867"/>
        <updated>2021-06-16T01:21:05.772Z</updated>
        <summary type="html"><![CDATA[Previous studies have demonstrated that commonly studied (vanilla)
touch-based continuous authentication systems (V-TCAS) are susceptible to
population attack. This paper proposes a novel Generative Adversarial Network
assisted TCAS (G-TCAS) framework, which showed more resilience to the
population attack. G-TCAS framework was tested on a dataset of 117 users who
interacted with a smartphone and tablet pair. On average, the increase in the
false accept rates (FARs) for V-TCAS was much higher (22%) than G-TCAS (13%)
for the smartphone. Likewise, the increase in the FARs for V-TCAS was 25%
compared to G-TCAS (6%) for the tablet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_M/0/1/0/all/0/1"&gt;Mohit Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehrotra_P/0/1/0/all/0/1"&gt;Pragyan Mehrotra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Rajesh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1"&gt;Rajiv Ratn Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direction-aware Feature-level Frequency Decomposition for Single Image Deraining. (arXiv:2106.07941v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07941</id>
        <link href="http://arxiv.org/abs/2106.07941"/>
        <updated>2021-06-16T01:21:05.759Z</updated>
        <summary type="html"><![CDATA[We present a novel direction-aware feature-level frequency decomposition
network for single image deraining. Compared with existing solutions, the
proposed network has three compelling characteristics. First, unlike previous
algorithms, we propose to perform frequency decomposition at feature-level
instead of image-level, allowing both low-frequency maps containing structures
and high-frequency maps containing details to be continuously refined during
the training procedure. Second, we further establish communication channels
between low-frequency maps and high-frequency maps to interactively capture
structures from high-frequency maps and add them back to low-frequency maps
and, simultaneously, extract details from low-frequency maps and send them back
to high-frequency maps, thereby removing rain streaks while preserving more
delicate features in the input image. Third, different from existing algorithms
using convolutional filters consistent in all directions, we propose a
direction-aware filter to capture the direction of rain streaks in order to
more effectively and thoroughly purge the input images of rain streaks. We
extensively evaluate the proposed approach in three representative datasets and
experimental results corroborate our approach consistently outperforms
state-of-the-art deraining algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Sen Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yidan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1"&gt;Mingqiang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Haoran Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiping Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jonathan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao-Ping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jing Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cluster-guided Asymmetric Contrastive Learning for Unsupervised Person Re-Identification. (arXiv:2106.07846v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07846</id>
        <link href="http://arxiv.org/abs/2106.07846"/>
        <updated>2021-06-16T01:21:05.751Z</updated>
        <summary type="html"><![CDATA[Unsupervised person re-identification (Re-ID) aims to match pedestrian images
from different camera views in unsupervised setting. Existing methods for
unsupervised person Re-ID are usually built upon the pseudo labels from
clustering. However, the quality of clustering depends heavily on the quality
of the learned features, which are overwhelmingly dominated by the colors in
images especially in the unsupervised setting. In this paper, we propose a
Cluster-guided Asymmetric Contrastive Learning (CACL) approach for unsupervised
person Re-ID, in which cluster structure is leveraged to guide the feature
learning in a properly designed asymmetric contrastive learning framework. To
be specific, we propose a novel cluster-level contrastive loss to help the
siamese network effectively mine the invariance in feature learning with
respect to the cluster structure within and between different data augmentation
views, respectively. Extensive experiments conducted on three benchmark
datasets demonstrate superior performance of our proposal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mingkun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Guang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jun Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization. (arXiv:2106.07991v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.07991</id>
        <link href="http://arxiv.org/abs/2106.07991"/>
        <updated>2021-06-16T01:21:05.732Z</updated>
        <summary type="html"><![CDATA[Bi-level optimization model is able to capture a wide range of complex
learning tasks with practical interest. Due to the witnessed efficiency in
solving bi-level programs, gradient-based methods have gained popularity in the
machine learning community. In this work, we propose a new gradient-based
solution scheme, namely, the Bi-level Value-Function-based Interior-point
Method (BVFIM). Following the main idea of the log-barrier interior-point
scheme, we penalize the regularized value function of the lower level problem
into the upper level objective. By further solving a sequence of differentiable
unconstrained approximation problems, we consequently derive a sequential
programming scheme. The numerical advantage of our scheme relies on the fact
that, when gradient methods are applied to solve the approximation problem, we
successfully avoid computing any expensive Hessian-vector or Jacobian-vector
product. We prove the convergence without requiring any convexity assumption on
either the upper level or the lower level objective. Experiments demonstrate
the efficiency of the proposed BVFIM on non-convex bi-level problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Liu_R/0/1/0/all/0/1"&gt;Risheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiaoming Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shangzhi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wavelength-based Attributed Deep Neural Network for Underwater Image Restoration. (arXiv:2106.07910v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.07910</id>
        <link href="http://arxiv.org/abs/2106.07910"/>
        <updated>2021-06-16T01:21:05.725Z</updated>
        <summary type="html"><![CDATA[Underwater images, in general, suffer from low contrast and high color
distortions due to the non-uniform attenuation of the light as it propagates
through the water. In addition, the degree of attenuation varies with the
wavelength resulting in the asymmetric traversing of colors. Despite the
prolific works for underwater image restoration (UIR) using deep learning, the
above asymmetricity has not been addressed in the respective network
engineering. As the first novelty, this paper shows that attributing the right
receptive field size (context) based on the traversing range of the color
channel may lead to a substantial performance gain for the task of UIR.
Further, it is important to suppress the irrelevant multi-contextual features
and increase the representational power of the model. Therefore, as a second
novelty, we have incorporated an attentive skip mechanism to adaptively refine
the learned multi-contextual features. The proposed framework, called Deep
WaveNet, is optimized using the traditional pixel-wise and feature-based cost
functions. An extensive set of experiments have been carried out to show the
efficacy of the proposed scheme over existing best-published literature on
benchmark datasets. More importantly, we have demonstrated a comprehensive
validation of enhanced images across various high-level vision tasks, e.g.,
underwater image semantic segmentation, and diver's 2D pose estimation. A
sample video to exhibit our real-world performance is available at
\url{https://www.youtube.com/watch?v=8qtuegBdfac}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Prasen Kumar Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bisht_I/0/1/0/all/0/1"&gt;Ira Bisht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sur_A/0/1/0/all/0/1"&gt;Arijit Sur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.08208</id>
        <link href="http://arxiv.org/abs/2106.08208"/>
        <updated>2021-06-16T01:21:05.665Z</updated>
        <summary type="html"><![CDATA[Adaptive gradient methods have shown excellent performance for solving many
machine learning problems. Although multiple adaptive methods were recently
studied, they mainly focus on either empirical or theoretical aspects and also
only work for specific problems by using specific adaptive learning rates. It
is desired to design a universal framework for practical algorithms of adaptive
gradients with theoretical guarantee to solve general problems. To fill this
gap, we propose a faster and universal framework of adaptive gradients (i.e.,
SUPER-ADAM) by introducing a universal adaptive matrix that includes most
existing adaptive gradient forms. Moreover, our framework can flexibly
integrates the momentum and variance reduced techniques. In particular, our
novel framework provides the convergence analysis support for adaptive gradient
methods under the nonconvex setting. In theoretical analysis, we prove that our
new algorithm can achieve the best known complexity of
$\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point of
nonconvex optimization, which matches the lower bound for stochastic smooth
nonconvex optimization. In numerical experiments, we employ various deep
learning tasks to validate that our algorithm consistently outperforms the
existing adaptive algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Consistency Checks to Detect LiDAR Spoofing Attacks on Autonomous Vehicle Perception. (arXiv:2106.07833v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.07833</id>
        <link href="http://arxiv.org/abs/2106.07833"/>
        <updated>2021-06-16T01:21:05.654Z</updated>
        <summary type="html"><![CDATA[LiDAR sensors are used widely in Autonomous Vehicles for better perceiving
the environment which enables safer driving decisions. Recent work has
demonstrated serious LiDAR spoofing attacks with alarming consequences. In
particular, model-level LiDAR spoofing attacks aim to inject fake depth
measurements to elicit ghost objects that are erroneously detected by 3D Object
Detectors, resulting in hazardous driving decisions. In this work, we explore
the use of motion as a physical invariant of genuine objects for detecting such
attacks. Based on this, we propose a general methodology, 3D Temporal
Consistency Check (3D-TC2), which leverages spatio-temporal information from
motion prediction to verify objects detected by 3D Object Detectors. Our
preliminary design and implementation of a 3D-TC2 prototype demonstrates very
promising performance, providing more than 98% attack detection rate with a
recall of 91% for detecting spoofed Vehicle (Car) objects, and is able to
achieve real-time detection at 41Hz]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chengzeng You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hau_Z/0/1/0/all/0/1"&gt;Zhongyuan Hau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demetriou_S/0/1/0/all/0/1"&gt;Soteris Demetriou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spot the Difference: Topological Anomaly Detection via Geometric Alignment. (arXiv:2106.08233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08233</id>
        <link href="http://arxiv.org/abs/2106.08233"/>
        <updated>2021-06-16T01:21:05.635Z</updated>
        <summary type="html"><![CDATA[Geometric alignment appears in a variety of applications, ranging from domain
adaptation, optimal transport, and normalizing flows in machine learning;
optical flow and learned augmentation in computer vision and deformable
registration within biomedical imaging. A recurring challenge is the alignment
of domains whose topology is not the same; a problem that is routinely ignored,
potentially introducing bias in downstream analysis. As a first step towards
solving such alignment problems, we propose an unsupervised topological
difference detection algorithm. The model is based on a conditional variational
auto-encoder and detects topological anomalies with regards to a reference
alongside the registration step. We consider both a) topological changes in the
image under spatial variation and b) unexpected transformations. Our approach
is validated on a proxy task of unsupervised anomaly detection in images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Czolbe_S/0/1/0/all/0/1"&gt;Steffen Czolbe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1"&gt;Aasa Feragen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1"&gt;Oswin Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DFM: A Performance Baseline for Deep Feature Matching. (arXiv:2106.07791v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07791</id>
        <link href="http://arxiv.org/abs/2106.07791"/>
        <updated>2021-06-16T01:21:05.612Z</updated>
        <summary type="html"><![CDATA[A novel image matching method is proposed that utilizes learned features
extracted by an off-the-shelf deep neural network to obtain a promising
performance. The proposed method uses pre-trained VGG architecture as a feature
extractor and does not require any additional training specific to improve
matching. Inspired by well-established concepts in the psychology area, such as
the Mental Rotation paradigm, an initial warping is performed as a result of a
preliminary geometric transformation estimate. These estimates are simply based
on dense matching of nearest neighbors at the terminal layer of VGG network
outputs of the images to be matched. After this initial alignment, the same
approach is repeated again between reference and aligned images in a
hierarchical manner to reach a good localization and matching performance. Our
algorithm achieves 0.57 and 0.80 overall scores in terms of Mean Matching
Accuracy (MMA) for 1 pixel and 2 pixels thresholds respectively on Hpatches
dataset, which indicates a better performance than the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Efe_U/0/1/0/all/0/1"&gt;Ufuk Efe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ince_K/0/1/0/all/0/1"&gt;Kutalmis Gokalp Ince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1"&gt;A. Aydin Alatan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Spacecraft Dataset for Detection, Segmentation and Parts Recognition. (arXiv:2106.08186v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08186</id>
        <link href="http://arxiv.org/abs/2106.08186"/>
        <updated>2021-06-16T01:21:05.585Z</updated>
        <summary type="html"><![CDATA[Virtually all aspects of modern life depend on space technology. Thanks to
the great advancement of computer vision in general and deep learning-based
techniques in particular, over the decades, the world witnessed the growing use
of deep learning in solving problems for space applications, such as
self-driving robot, tracers, insect-like robot on cosmos and health monitoring
of spacecraft. These are just some prominent examples that has advanced space
industry with the help of deep learning. However, the success of deep learning
models requires a lot of training data in order to have decent performance,
while on the other hand, there are very limited amount of publicly available
space datasets for the training of deep learning models. Currently, there is no
public datasets for space-based object detection or instance segmentation,
partly because manually annotating object segmentation masks is very time
consuming as they require pixel-level labelling, not to mention the challenge
of obtaining images from space. In this paper, we aim to fill this gap by
releasing a dataset for spacecraft detection, instance segmentation and part
recognition. The main contribution of this work is the development of the
dataset using images of space stations and satellites, with rich annotations
including bounding boxes of spacecrafts and masks to the level of object parts,
which are obtained with a mixture of automatic processes and manual efforts. We
also provide evaluations with state-of-the-art methods in object detection and
instance segmentation as a benchmark for the dataset. The link for downloading
the proposed dataset can be found on
https://github.com/Yurushia1998/SatelliteDataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1"&gt;Dung Anh Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1"&gt;Tat-Jun Chin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07847</id>
        <link href="http://arxiv.org/abs/2106.07847"/>
        <updated>2021-06-16T01:21:05.578Z</updated>
        <summary type="html"><![CDATA[We study transfer learning in the presence of spurious correlations. We
experimentally demonstrate that directly transferring the stable feature
extractor learned on the source task may not eliminate these biases for the
target task. However, we hypothesize that the unstable features in the source
task and those in the target task are directly related. By explicitly informing
the target classifier of the source task's unstable features, we can regularize
the biases in the target task. Specifically, we derive a representation that
encodes the unstable features by contrasting different data environments in the
source task. On the target task, we cluster data from this representation, and
achieve robustness by minimizing the worst-case risk across all clusters. We
evaluate our method on both text and image classifications. Empirical results
demonstrate that our algorithm is able to maintain robustness on the target
task, outperforming the best baseline by 22.9% in absolute accuracy across 12
transfer settings. Our code is available at https://github.com/YujiaBao/Tofu.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1"&gt;Yujia Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceptually-inspired super-resolution of compressed videos. (arXiv:2106.08147v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08147</id>
        <link href="http://arxiv.org/abs/2106.08147"/>
        <updated>2021-06-16T01:21:05.563Z</updated>
        <summary type="html"><![CDATA[Spatial resolution adaptation is a technique which has often been employed in
video compression to enhance coding efficiency. This approach encodes a lower
resolution version of the input video and reconstructs the original resolution
during decoding. Instead of using conventional up-sampling filters, recent work
has employed advanced super-resolution methods based on convolutional neural
networks (CNNs) to further improve reconstruction quality. These approaches are
usually trained to minimise pixel-based losses such as Mean-Squared Error
(MSE), despite the fact that this type of loss metric does not correlate well
with subjective opinions. In this paper, a perceptually-inspired
super-resolution approach (M-SRGAN) is proposed for spatial up-sampling of
compressed video using a modified CNN model, which has been trained using a
generative adversarial network (GAN) on compressed content with perceptual loss
functions. The proposed method was integrated with HEVC HM 16.20, and has been
evaluated on the JVET Common Test Conditions (UHD test sequences) using the
Random Access configuration. The results show evident perceptual quality
improvement over the original HM 16.20, with an average bitrate saving of 35.6%
(Bj{\o}ntegaard Delta measurement) based on a perceptual quality metric, VMAF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ma_D/0/1/0/all/0/1"&gt;Di Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Afonso_M/0/1/0/all/0/1"&gt;Mariana Afonso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1"&gt;David R. Bull&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualizing Multiple Tasks via Learning to Decompose. (arXiv:2106.08112v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.08112</id>
        <link href="http://arxiv.org/abs/2106.08112"/>
        <updated>2021-06-16T01:21:05.552Z</updated>
        <summary type="html"><![CDATA[One single instance could possess multiple portraits and reveal diverse
relationships with others according to different contexts. Those ambiguities
increase the difficulty of learning a generalizable model when there exists one
concept or mixed concepts in a task. We propose a general approach Learning to
Decompose Network (LeadNet) for both two cases, which contextualizes a model
through meta-learning multiple maps for concepts discovery -- the
representations of instances are decomposed and adapted conditioned on the
contexts. Through taking a holistic view over multiple latent components over
instances in a sampled pseudo task, LeadNet learns to automatically select the
right concept via incorporating those rich semantics inside and between
objects. LeadNet demonstrates its superiority in various applications,
including exploring multiple views of confusing tasks, out-of-distribution
recognition, and few-shot image classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Da-Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lanqing Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiu-Shen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Deep Morphological Networks with Neural Architecture Search. (arXiv:2106.07714v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07714</id>
        <link href="http://arxiv.org/abs/2106.07714"/>
        <updated>2021-06-16T01:21:05.545Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) are generated by sequentially performing linear
and non-linear processes. Using a combination of linear and non-linear
procedures is critical for generating a sufficiently deep feature space. The
majority of non-linear operators are derivations of activation functions or
pooling functions. Mathematical morphology is a branch of mathematics that
provides non-linear operators for a variety of image processing problems. We
investigate the utility of integrating these operations in an end-to-end deep
learning framework in this paper. DNNs are designed to acquire a realistic
representation for a particular job. Morphological operators give topological
descriptors that convey salient information about the shapes of objects
depicted in images. We propose a method based on meta-learning to incorporate
morphological operators into DNNs. The learned architecture demonstrates how
our novel morphological operations significantly increase DNN performance on
various tasks, including picture classification and edge detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yufei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belkhir_N/0/1/0/all/0/1"&gt;Nacim Belkhir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angulo_J/0/1/0/all/0/1"&gt;Jesus Angulo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1"&gt;Angela Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1"&gt;Gianni Franchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Canonical Face Embeddings. (arXiv:2106.07822v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07822</id>
        <link href="http://arxiv.org/abs/2106.07822"/>
        <updated>2021-06-16T01:21:05.524Z</updated>
        <summary type="html"><![CDATA[We present evidence that many common convolutional neural networks (CNNs)
trained for face verification learn functions that are nearly equivalent under
rotation. More specifically, we demonstrate that one face verification model's
embeddings (i.e. last--layer activations) can be compared directly to another
model's embeddings after only a rotation or linear transformation, with little
performance penalty. This finding is demonstrated using IJB-C 1:1 verification
across the combinations of ten modern off-the-shelf CNN-based face verification
models which vary in training dataset, CNN architecture, way of using angular
loss, or some combination of the 3, and achieve a mean true accept rate of 0.96
at a false accept rate of 0.01. When instead evaluating embeddings generated
from two CNNs, where one CNN's embeddings are mapped with a linear
transformation, the mean true accept rate drops to 0.95 using the same
verification paradigm. Restricting these linear maps to only perform rotation
produces a mean true accept rate of 0.91. These mappings' existence suggests
that a common representation is learned by models with variation in training or
structure. A discovery such as this likely has broad implications, and we
provide an application in which face embeddings can be de-anonymized using a
limited number of samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McNeely_White_D/0/1/0/all/0/1"&gt;David McNeely-White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sattelberg_B/0/1/0/all/0/1"&gt;Ben Sattelberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanchard_N/0/1/0/all/0/1"&gt;Nathaniel Blanchard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beveridge_R/0/1/0/all/0/1"&gt;Ross Beveridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Learning of Keypoint Representations for Continuous Control from Images. (arXiv:2106.07995v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07995</id>
        <link href="http://arxiv.org/abs/2106.07995"/>
        <updated>2021-06-16T01:21:05.477Z</updated>
        <summary type="html"><![CDATA[In many control problems that include vision, optimal controls can be
inferred from the location of the objects in the scene. This information can be
represented using keypoints, which is a list of spatial locations in the input
image. Previous works show that keypoint representations learned during
unsupervised pre-training using encoder-decoder architectures can provide good
features for control tasks. In this paper, we show that it is possible to learn
efficient keypoint representations end-to-end, without the need for
unsupervised pre-training, decoders, or additional losses. Our proposed
architecture consists of a differentiable keypoint extractor that feeds the
coordinates of the estimated keypoints directly to a soft actor-critic agent.
The proposed algorithm yields performance competitive to the state-of-the art
on DeepMind Control Suite tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boney_R/0/1/0/all/0/1"&gt;Rinu Boney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1"&gt;Alexander Ilin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1"&gt;Juho Kannala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascading Convolutional Temporal Colour Constancy. (arXiv:2106.07955v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07955</id>
        <link href="http://arxiv.org/abs/2106.07955"/>
        <updated>2021-06-16T01:21:05.462Z</updated>
        <summary type="html"><![CDATA[Computational Colour Constancy (CCC) consists of estimating the colour of one
or more illuminants in a scene and using them to remove unwanted chromatic
distortions. Much research has focused on illuminant estimation for CCC on
single images, with few attempts of leveraging the temporal information
intrinsic in sequences of correlated images (e.g., the frames in a video), a
task known as Temporal Colour Constancy (TCC). The state-of-the-art for TCC is
TCCNet, a deep-learning architecture that uses a ConvLSTM for aggregating the
encodings produced by CNN submodules for each image in a sequence. We extend
this architecture with different models obtained by (i) substituting the TCCNet
submodules with C4, the state-of-the-art method for CCC targeting images; (ii)
adding a cascading strategy to perform an iterative improvement of the estimate
of the illuminant. We tested our models on the recently released TCC benchmark
and achieved results that surpass the state-of-the-art. Analyzing the impact of
the number of frames involved in illuminant estimation on performance, we show
that it is possible to reduce inference time by training the models on few
selected frames from the sequences while retaining comparable accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rizzo_M/0/1/0/all/0/1"&gt;Matteo Rizzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conati_C/0/1/0/all/0/1"&gt;Cristina Conati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1"&gt;Daesik Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hui Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with Unlabeled Data. (arXiv:2106.07807v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07807</id>
        <link href="http://arxiv.org/abs/2106.07807"/>
        <updated>2021-06-16T01:21:05.443Z</updated>
        <summary type="html"><![CDATA[Most existing works in few-shot learning rely on meta-learning the network on
a large base dataset which is typically from the same domain as the target
dataset. We tackle the problem of cross-domain few-shot learning where there is
a large shift between the base and target domain. The problem of cross-domain
few-shot recognition with unlabeled target data is largely unaddressed in the
literature. STARTUP was the first method that tackles this problem using
self-training. However, it uses a fixed teacher pretrained on a labeled base
dataset to create soft labels for the unlabeled target samples. As the base
dataset and unlabeled dataset are from different domains, projecting the target
images in the class-domain of the base dataset with a fixed pretrained model
might be sub-optimal. We propose a simple dynamic distillation-based approach
to facilitate unlabeled images from the novel/base dataset. We impose
consistency regularization by calculating predictions from the weakly-augmented
versions of the unlabeled images from a teacher network and matching it with
the strongly augmented versions of the same images from a student network. The
parameters of the teacher network are updated as exponential moving average of
the parameters of the student network. We show that the proposed network learns
representation that can be easily adapted to the target domain even though it
has not been trained with target-specific classes during the pretraining phase.
Our model outperforms the current state-of-the art method by 4.4% for 1-shot
and 3.6% for 5-shot classification in the BSCD-FSL benchmark, and also shows
competitive performance on traditional in-domain few-shot learning task. Our
code will be available at: https://github.com/asrafulashiq/dynamic-cdfsl.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1"&gt;Ashraful Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun-Fu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1"&gt;Rameswar Panda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1"&gt;Leonid Karlinsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radke_R/0/1/0/all/0/1"&gt;Richard J. Radke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Gradient Manifold Neural Network. (arXiv:2106.07905v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07905</id>
        <link href="http://arxiv.org/abs/2106.07905"/>
        <updated>2021-06-16T01:21:05.436Z</updated>
        <summary type="html"><![CDATA[Deep neural network (DNN) generally takes thousands of iterations to optimize
via gradient descent and thus has a slow convergence. In addition, softmax, as
a decision layer, may ignore the distribution information of the data during
classification. Aiming to tackle the referred problems, we propose a novel
manifold neural network based on non-gradient optimization, i.e., the
closed-form solutions. Considering that the activation function is generally
invertible, we reconstruct the network via forward ridge regression and low
rank backward approximation, which achieve the rapid convergence. Moreover, by
unifying the flexible Stiefel manifold and adaptive support vector machine, we
devise the novel decision layer which efficiently fits the manifold structure
of the data and label information. Consequently, a jointly non-gradient
optimization method is designed to generate the network with closed-form
results. Eventually, extensive experiments validate the superior performance of
the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Z/0/1/0/all/0/1"&gt;Ziheng Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuelong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with Adaptive Hybrid Label. (arXiv:2106.08073v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08073</id>
        <link href="http://arxiv.org/abs/2106.08073"/>
        <updated>2021-06-16T01:21:05.429Z</updated>
        <summary type="html"><![CDATA[Unmanned aerial vehicle (UAV) based visual tracking has been confronted with
numerous challenges, e.g., object motion and occlusion. These challenges
generally introduce unexpected mutations of target appearance and result in
tracking failure. However, prevalent discriminative correlation filter (DCF)
based trackers are insensitive to target mutations due to a predefined label,
which concentrates on merely the centre of the training region. Meanwhile,
appearance mutations caused by occlusion or similar objects usually lead to the
inevitable learning of wrong information. To cope with appearance mutations,
this paper proposes a novel DCF-based method to enhance the sensitivity and
resistance to mutations with an adaptive hybrid label, i.e., MSCF. The ideal
label is optimized jointly with the correlation filter and remains temporal
consistency. Besides, a novel measurement of mutations called mutation threat
factor (MTF) is applied to correct the label dynamically. Considerable
experiments are conducted on widely used UAV benchmarks. The results indicate
that the performance of MSCF tracker surpasses other 26 state-of-the-art
DCF-based and deep-based trackers. With a real-time speed of _38 frames/s, the
proposed approach is sufficient for UAV tracking commissions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guangze Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Junjie Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fuling Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1"&gt;Fangqiang Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Clinically Inspired Approach for Melanoma classification. (arXiv:2106.08021v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08021</id>
        <link href="http://arxiv.org/abs/2106.08021"/>
        <updated>2021-06-16T01:21:05.404Z</updated>
        <summary type="html"><![CDATA[Melanoma is a leading cause of deaths due to skin cancer deaths and hence,
early and effective diagnosis of melanoma is of interest. Current approaches
for automated diagnosis of melanoma either use pattern recognition or
analytical recognition like ABCDE (asymmetry, border, color, diameter and
evolving) criterion. In practice however, a differential approach wherein
outliers (ugly duckling) are detected and used to evaluate nevi/lesions.
Incorporation of differential recognition in Computer Aided Diagnosis (CAD)
systems has not been explored but can be beneficial as it can provide a
clinical justification for the derived decision. We present a method for
identifying and quantifying ugly ducklings by performing Intra-Patient
Comparative Analysis (IPCA) of neighboring nevi. This is then incorporated in a
CAD system design for melanoma detection. This design ensures flexibility to
handle cases where IPCA is not possible. Our experiments on a public dataset
show that the outlier information helps boost the sensitivity of detection by
at least 4.1 % and specificity by 4.0 % to 8.9 %, depending on the use of a
strong (EfficientNet) or moderately strong (VGG or ResNet) classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akundi_P/0/1/0/all/0/1"&gt;Prathyusha Akundi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gun_S/0/1/0/all/0/1"&gt;Soumyasis Gun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivaswamy_J/0/1/0/all/0/1"&gt;Jayanthi Sivaswamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Sketch Search. (arXiv:2106.08009v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08009</id>
        <link href="http://arxiv.org/abs/2106.08009"/>
        <updated>2021-06-16T01:21:05.395Z</updated>
        <summary type="html"><![CDATA[We present an algorithm for searching image collections using free-hand
sketches that describe the appearance and relative positions of multiple
objects. Sketch based image retrieval (SBIR) methods predominantly match
queries containing a single, dominant object invariant to its position within
an image. Our work exploits drawings as a concise and intuitive representation
for specifying entire scene compositions. We train a convolutional neural
network (CNN) to encode masked visual features from sketched objects, pooling
these into a spatial descriptor encoding the spatial relationships and
appearances of objects in the composition. Training the CNN backbone as a
Siamese network under triplet loss yields a metric search embedding for
measuring compositional similarity which may be efficiently leveraged for
visual search by applying product quantization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1"&gt;Alexander Black&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Tu Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1"&gt;Long Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hailin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1"&gt;John Collomosse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hotel Recognition via Latent Image Embedding. (arXiv:2106.08042v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08042</id>
        <link href="http://arxiv.org/abs/2106.08042"/>
        <updated>2021-06-16T01:21:05.385Z</updated>
        <summary type="html"><![CDATA[We approach the problem of hotel recognition with deep metric learning. We
overview the existing approaches and propose a modification to Contrastive loss
called Contrastive-Triplet loss. We construct a robust pipeline for
benchmarking metric learning models and perform experiments on Hotels-50K and
CUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval
on Hotels-50k. We open-source our code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tseytlin_B/0/1/0/all/0/1"&gt;Boris Tseytlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1"&gt;Ilya Makarov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EuroCrops: A Pan-European Dataset for Time Series Crop Type Classification. (arXiv:2106.08151v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08151</id>
        <link href="http://arxiv.org/abs/2106.08151"/>
        <updated>2021-06-16T01:21:05.378Z</updated>
        <summary type="html"><![CDATA[We present EuroCrops, a dataset based on self-declared field annotations for
training and evaluating methods for crop type classification and mapping,
together with its process of acquisition and harmonisation. By this, we aim to
enrich the research efforts and discussion for data-driven land cover
classification via Earth observation and remote sensing. Additionally, through
inclusion of self-declarations gathered in the scope of subsidy control from
all countries of the European Union (EU), this dataset highlights the
difficulties and pitfalls one comes across when operating on a transnational
level. We, therefore, also introduce a new taxonomy scheme, HCAT-ID, that
aspires to capture all the aspects of reference data originating from
administrative and agency databases. To address researchers from both the
remote sensing and the computer vision and machine learning communities, we
publish the dataset in different formats and processing levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schneider_M/0/1/0/all/0/1"&gt;Maja Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Broszeit_A/0/1/0/all/0/1"&gt;Amelie Broszeit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korner_M/0/1/0/all/0/1"&gt;Marco K&amp;#xf6;rner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Facial Expression Analysis For Dimensional Affect Recognition Using Geometric Features. (arXiv:2106.07817v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07817</id>
        <link href="http://arxiv.org/abs/2106.07817"/>
        <updated>2021-06-16T01:21:05.366Z</updated>
        <summary type="html"><![CDATA[Despite their continued popularity, categorical approaches to affect
recognition have limitations, especially in real-life situations. Dimensional
models of affect offer important advantages for the recognition of subtle
expressions and more fine-grained analysis. We introduce a simple but effective
facial expression analysis (FEA) system for dimensional affect, solely based on
geometric features and Partial Least Squares (PLS) regression. The system
jointly learns to estimate Arousal and Valence ratings from a set of facial
images. The proposed approach is robust, efficient, and exhibits comparable
performance to contemporary deep learning models, while requiring a fraction of
the computational resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vonikakis_V/0/1/0/all/0/1"&gt;Vassilios Vonikakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1"&gt;Stefan Winkler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cine-MRI detection of abdominal adhesions with spatio-temporal deep learning. (arXiv:2106.08094v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.08094</id>
        <link href="http://arxiv.org/abs/2106.08094"/>
        <updated>2021-06-16T01:21:05.343Z</updated>
        <summary type="html"><![CDATA[Adhesions are an important cause of chronic pain following abdominal surgery.
Recent developments in abdominal cine-MRI have enabled the non-invasive
diagnosis of adhesions. Adhesions are identified on cine-MRI by the absence of
sliding motion during movement. Diagnosis and mapping of adhesions improves the
management of patients with pain. Detection of abdominal adhesions on cine-MRI
is challenging from both a radiological and deep learning perspective. We focus
on classifying presence or absence of adhesions in sagittal abdominal cine-MRI
series. We experimented with spatio-temporal deep learning architectures
centered around a ConvGRU architecture. A hybrid architecture comprising a
ResNet followed by a ConvGRU model allows to classify a whole time-series.
Compared to a stand-alone ResNet with a two time-point (inspiration/expiration)
input, we show an increase in classification performance (AUROC) from 0.74 to
0.83 ($p<0.05$). Our full temporal classification approach adds only a small
amount (5%) of parameters to the entire architecture, which may be useful for
other medical imaging problems with a temporal dimension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wilde_B/0/1/0/all/0/1"&gt;Bram de Wilde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Broek_R/0/1/0/all/0/1"&gt;Richard P. G. ten Broek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huisman_H/0/1/0/all/0/1"&gt;Henkjan Huisman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer-aided Interpretable Features for Leaf Image Classification. (arXiv:2106.08077v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08077</id>
        <link href="http://arxiv.org/abs/2106.08077"/>
        <updated>2021-06-16T01:21:05.331Z</updated>
        <summary type="html"><![CDATA[Plant species identification is time consuming, costly, and requires lots of
efforts, and expertise knowledge. In recent, many researchers use deep learning
methods to classify plants directly using plant images. While deep learning
models have achieved a great success, the lack of interpretability limit their
widespread application. To overcome this, we explore the use of interpretable,
measurable and computer-aided features extracted from plant leaf images. Image
processing is one of the most challenging, and crucial steps in
feature-extraction. The purpose of image processing is to improve the leaf
image by removing undesired distortion. The main image processing steps of our
algorithm involves: i) Convert original image to RGB (Red-Green-Blue) image,
ii) Gray scaling, iii) Gaussian smoothing, iv) Binary thresholding, v) Remove
stalk, vi) Closing holes, and vii) Resize image. The next step after image
processing is to extract features from plant leaf images. We introduced 52
computationally efficient features to classify plant species. These features
are mainly classified into four groups as: i) shape-based features, ii)
color-based features, iii) texture-based features, and iv) scagnostic features.
Length, width, area, texture correlation, monotonicity and scagnostics are to
name few of them. We explore the ability of features to discriminate the
classes of interest under supervised learning and unsupervised learning
settings. For that, supervised dimensionality reduction technique, Linear
Discriminant Analysis (LDA), and unsupervised dimensionality reduction
technique, Principal Component Analysis (PCA) are used to convert and visualize
the images from digital-image space to feature space. The results show that the
features are sufficient to discriminate the classes of interest under both
supervised and unsupervised learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lakshika_J/0/1/0/all/0/1"&gt;Jayani P. G. Lakshika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talagala_T/0/1/0/all/0/1"&gt;Thiyanga S. Talagala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Pose and Shape Reconstruction of Two Interacting Hands With a Single Depth Camera. (arXiv:2106.08059v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08059</id>
        <link href="http://arxiv.org/abs/2106.08059"/>
        <updated>2021-06-16T01:21:05.322Z</updated>
        <summary type="html"><![CDATA[We present a novel method for real-time pose and shape reconstruction of two
strongly interacting hands. Our approach is the first two-hand tracking
solution that combines an extensive list of favorable properties, namely it is
marker-less, uses a single consumer-level depth camera, runs in real time,
handles inter- and intra-hand collisions, and automatically adjusts to the
user's hand shape. In order to achieve this, we embed a recent parametric hand
pose and shape model and a dense correspondence predictor based on a deep
neural network into a suitable energy minimization framework. For training the
correspondence prediction network, we synthesize a two-hand dataset based on
physical simulations that includes both hand pose and shape annotations while
at the same time avoiding inter-hand penetrations. To achieve real-time rates,
we phrase the model fitting in terms of a nonlinear least-squares problem so
that the energy can be optimized based on a highly efficient GPU-based
Gauss-Newton optimizer. We show state-of-the-art results in scenes that exceed
the complexity level demonstrated by previous work, including tight two-hand
grasps, significant inter-hand occlusions, and gesture interaction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1"&gt;Franziska Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_M/0/1/0/all/0/1"&gt;Micah Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1"&gt;Florian Bernard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotnychenko_O/0/1/0/all/0/1"&gt;Oleksandr Sotnychenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verschoor_M/0/1/0/all/0/1"&gt;Mickeal Verschoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otaduy_M/0/1/0/all/0/1"&gt;Miguel A. Otaduy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1"&gt;Dan Casas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReS2tAC -- UAV-Borne Real-Time SGM Stereo Optimized for Embedded ARM and CUDA Devices. (arXiv:2106.07927v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07927</id>
        <link href="http://arxiv.org/abs/2106.07927"/>
        <updated>2021-06-16T01:21:05.250Z</updated>
        <summary type="html"><![CDATA[With the emergence of low-cost robotic systems, such as unmanned aerial
vehicle, the importance of embedded high-performance image processing has
increased. For a long time, FPGAs were the only processing hardware that were
capable of high-performance computing, while at the same time preserving a low
power consumption, essential for embedded systems. However, the recently
increasing availability of embedded GPU-based systems, such as the NVIDIA
Jetson series, comprised of an ARM CPU and a NVIDIA Tegra GPU, allows for
massively parallel embedded computing on graphics hardware. With this in mind,
we propose an approach for real-time embedded stereo processing on ARM and
CUDA-enabled devices, which is based on the popular and widely used Semi-Global
Matching algorithm. In this, we propose an optimization of the algorithm for
embedded CUDA GPUs, by using massively parallel computing, as well as using the
NEON intrinsics to optimize the algorithm for vectorized SIMD processing on
embedded ARM CPUs. We have evaluated our approach with different configurations
on two public stereo benchmark datasets to demonstrate that they can reach an
error rate as low as 3.3%. Furthermore, our experiments show that the fastest
configuration of our approach reaches up to 46 FPS on VGA image resolution.
Finally, in a use-case specific qualitative evaluation, we have evaluated the
power consumption of our approach and deployed it on the DJI Manifold 2-G
attached to a DJI Matrix 210v2 RTK unmanned aerial vehicle (UAV), demonstrating
its suitability for real-time stereo processing onboard a UAV.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruf_B/0/1/0/all/0/1"&gt;Boitumelo Ruf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohrs_J/0/1/0/all/0/1"&gt;Jonas Mohrs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1"&gt;Martin Weinmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hinz_S/0/1/0/all/0/1"&gt;Stefan Hinz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyerer_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Beyerer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keep CALM and Improve Visual Feature Attribution. (arXiv:2106.07861v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07861</id>
        <link href="http://arxiv.org/abs/2106.07861"/>
        <updated>2021-06-16T01:21:05.162Z</updated>
        <summary type="html"><![CDATA[The class activation mapping, or CAM, has been the cornerstone of feature
attribution methods for multiple vision tasks. Its simplicity and effectiveness
have led to wide applications in the explanation of visual predictions and
weakly-supervised localization tasks. However, CAM has its own shortcomings.
The computation of attribution maps relies on ad-hoc calibration steps that are
not part of the training computational graph, making it difficult for us to
understand the real meaning of the attribution values. In this paper, we
improve CAM by explicitly incorporating a latent variable encoding the location
of the cue for recognition in the formulation, thereby subsuming the
attribution map into the training computational graph. The resulting model,
class activation latent mapping, or CALM, is trained with the
expectation-maximization algorithm. Our experiments show that CALM identifies
discriminative attributes for image classifiers more accurately than CAM and
other visual attribution baselines. CALM also shows performance improvements
over prior arts on the weakly-supervised object localization benchmarks. Our
code is available at https://github.com/naver-ai/calm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jae Myung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1"&gt;Junsuk Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Seong Joon Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision-Language Navigation with Random Environmental Mixup. (arXiv:2106.07876v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07876</id>
        <link href="http://arxiv.org/abs/2106.07876"/>
        <updated>2021-06-16T01:21:05.154Z</updated>
        <summary type="html"><![CDATA[Vision-language Navigation (VLN) tasks require an agent to navigate
step-by-step while perceiving the visual observations and comprehending a
natural language instruction. Large data bias, which is caused by the disparity
ratio between the small data scale and large navigation space, makes the VLN
task challenging. Previous works have proposed various data augmentation
methods to reduce data bias. However, these works do not explicitly reduce the
data bias across different house scenes. Therefore, the agent would overfit to
the seen scenes and achieve poor navigation performance in the unseen scenes.
To tackle this problem, we propose the Random Environmental Mixup (REM) method,
which generates cross-connected house scenes as augmented data via mixuping
environment. Specifically, we first select key viewpoints according to the room
connection graph for each scene. Then, we cross-connect the key views of
different scenes to construct augmented scenes. Finally, we generate augmented
instruction-path pairs in the cross-connected scenes. The experimental results
on benchmark datasets demonstrate that our augmentation data via REM help the
agent reduce its performance gap between the seen and unseen environment and
improve the overall performance, making our model the best existing approach on
the standard VLN benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Fengda Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xiaojun Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yi-Dong Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Neural Tangent Kernels via Sketching and Random Features. (arXiv:2106.07880v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07880</id>
        <link href="http://arxiv.org/abs/2106.07880"/>
        <updated>2021-06-16T01:21:05.118Z</updated>
        <summary type="html"><![CDATA[The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely-wide
neural networks trained under least squares loss by gradient descent. Recent
works also report that NTK regression can outperform finitely-wide neural
networks trained on small-scale datasets. However, the computational complexity
of kernel methods has limited its use in large-scale learning tasks. To
accelerate learning with NTK, we design a near input-sparsity time
approximation algorithm for NTK, by sketching the polynomial expansions of
arc-cosine kernels: our sketch for the convolutional counterpart of NTK (CNTK)
can transform any image using a linear runtime in the number of pixels.
Furthermore, we prove a spectral approximation guarantee for the NTK matrix, by
combining random features (based on leverage score sampling) of the arc-cosine
kernels with a sketching algorithm. We benchmark our methods on various
large-scale regression and classification tasks and show that a linear
regressor trained on our CNTK features matches the accuracy of exact CNTK on
CIFAR-10 dataset while achieving 150x speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zandieh_A/0/1/0/all/0/1"&gt;Amir Zandieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_I/0/1/0/all/0/1"&gt;Insu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avron_H/0/1/0/all/0/1"&gt;Haim Avron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoham_N/0/1/0/all/0/1"&gt;Neta Shoham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1"&gt;Chaewon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptive SiamRPN++ for Object Tracking in the Wild. (arXiv:2106.07862v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07862</id>
        <link href="http://arxiv.org/abs/2106.07862"/>
        <updated>2021-06-16T01:21:05.109Z</updated>
        <summary type="html"><![CDATA[Benefit from large-scale training data, recent advances in Siamese-based
object tracking have achieved compelling results on the normal sequences.
Whilst Siamese-based trackers assume training and test data follow an identical
distribution. Suppose there is a set of foggy or rainy test sequences, it
cannot be guaranteed that the trackers trained on the normal images perform
well on the data belonging to other domains. The problem of domain shift among
training and test data has already been discussed in object detection and
semantic segmentation areas, which, however, has not been investigated for
visual tracking. To this end, based on SiamRPN++, we introduce a Domain
Adaptive SiamRPN++, namely DASiamRPN++, to improve the cross-domain
transferability and robustness of a tracker. Inspired by A-distance theory, we
present two domain adaptive modules, Pixel Domain Adaptation (PDA) and Semantic
Domain Adaptation (SDA). The PDA module aligns the feature maps of template and
search region images to eliminate the pixel-level domain shift caused by
weather, illumination, etc. The SDA module aligns the feature representations
of the tracking target's appearance to eliminate the semantic-level domain
shift. PDA and SDA modules reduce the domain disparity by learning domain
classifiers in an adversarial training manner. The domain classifiers enforce
the network to learn domain-invariant feature representations. Extensive
experiments are performed on the standard datasets of two different domains,
including synthetic foggy and TIR sequences, which demonstrate the
transferability and domain adaptability of the proposed tracker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhongzhou Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CathAI: Fully Automated Interpretation of Coronary Angiograms Using Neural Networks. (arXiv:2106.07708v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.07708</id>
        <link href="http://arxiv.org/abs/2106.07708"/>
        <updated>2021-06-16T01:21:05.088Z</updated>
        <summary type="html"><![CDATA[Coronary heart disease (CHD) is the leading cause of adult death in the
United States and worldwide, and for which the coronary angiography procedure
is the primary gateway for diagnosis and clinical management decisions. The
standard-of-care for interpretation of coronary angiograms depends upon ad-hoc
visual assessment by the physician operator. However, ad-hoc visual
interpretation of angiograms is poorly reproducible, highly variable and bias
prone. Here we show for the first time that fully-automated angiogram
interpretation to estimate coronary artery stenosis is possible using a
sequence of deep neural network algorithms. The algorithmic pipeline we
developed--called CathAI--achieves state-of-the art performance across the
sequence of tasks required to accomplish automated interpretation of
unselected, real-world angiograms. CathAI (Algorithms 1-2) demonstrated
positive predictive value, sensitivity and F1 score of >=90% to identify the
projection angle overall and >=93% for left or right coronary artery angiogram
detection, the primary anatomic structures of interest. To predict obstructive
coronary artery stenosis (>=70% stenosis), CathAI (Algorithm 4) exhibited an
area under the receiver operating characteristic curve (AUC) of 0.862 (95% CI:
0.843-0.880). When externally validated in a healthcare system in another
country, CathAI AUC was 0.869 (95% CI: 0.830-0.907) to predict obstructive
coronary artery stenosis. Our results demonstrate that multiple purpose-built
neural networks can function in sequence to accomplish the complex series of
tasks required for automated analysis of real-world angiograms. Deployment of
CathAI may serve to increase standardization and reproducibility in coronary
stenosis assessment, while providing a robust foundation to accomplish future
tasks for algorithmic angiographic interpretation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avram_R/0/1/0/all/0/1"&gt;Robert Avram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olgin_J/0/1/0/all/0/1"&gt;Jeffrey E. Olgin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_A/0/1/0/all/0/1"&gt;Alvin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1"&gt;Zeeshan Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verreault_Julien_L/0/1/0/all/0/1"&gt;Louis Verreault-Julien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abreau_S/0/1/0/all/0/1"&gt;Sean Abreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1"&gt;Derek Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1"&gt;Derek Y. So&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soni_K/0/1/0/all/0/1"&gt;Krishan Soni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tison_G/0/1/0/all/0/1"&gt;Geoffrey H. Tison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[G$^2$DA: Geometry-Guided Dual-Alignment Learning for RGB-Infrared Person Re-Identification. (arXiv:2106.07853v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07853</id>
        <link href="http://arxiv.org/abs/2106.07853"/>
        <updated>2021-06-16T01:21:05.074Z</updated>
        <summary type="html"><![CDATA[RGB-Infrared (IR) person re-identification aims to retrieve
person-of-interest between heterogeneous modalities, suffering from large
modality discrepancy caused by different sensory devices. Existing methods
mainly focus on global-level modality alignment, whereas neglect sample-level
modality divergence to some extent, leading to performance degradation. This
paper attempts to find RGB-IR ReID solutions from tackling sample-level
modality difference, and presents a Geometry-Guided Dual-Alignment learning
framework (G$^2$DA), which jointly enhances modality-invariance and reinforces
discriminability with human topological structure in features to boost the
overall matching performance. Specifically, G$^2$DA extracts accurate body part
features with a pose estimator, serving as a semantic bridge complementing the
missing local details in global descriptor. Based on extracted local and global
features, a novel distribution constraint derived from optimal transport is
introduced to mitigate the modality gap in a fine-grained sample-level manner.
Beyond pair-wise relations across two modalities, it additionally measures the
structural similarity of different parts, thus both multi-level features and
their relations are kept consistent in the common feature space. Considering
the inherent human-topology information, we further advance a geometry-guided
graph learning module to refine each part features, where relevant regions can
be emphasized while meaningless ones are suppressed, effectively facilitating
robust feature learning. Extensive experiments on two standard benchmark
datasets validate the superiority of our proposed method, yielding competitive
performance over the state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1"&gt;Lin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zongyuan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_Q/0/1/0/all/0/1"&gt;Qianyan Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yehansen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Lijing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhihang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Feature Information Extraction for Interest Point Detection: A Comprehensive Review. (arXiv:2106.07929v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07929</id>
        <link href="http://arxiv.org/abs/2106.07929"/>
        <updated>2021-06-16T01:21:05.064Z</updated>
        <summary type="html"><![CDATA[Interest point detection is one of the most fundamental and critical problems
in computer vision and image processing. In this paper, we carry out a
comprehensive review on image feature information (IFI) extraction techniques
for interest point detection. To systematically introduce how the existing
interest point detection methods extract IFI from an input image, we propose a
taxonomy of the IFI extraction techniques for interest point detection.
According to this taxonomy, we discuss different types of IFI extraction
techniques for interest point detection. Furthermore, we identify the main
unresolved issues related to the existing IFI extraction techniques for
interest point detection and any interest point detection methods that have not
been discussed before. The existing popular datasets and evaluation standards
are provided and the performances for eighteen state-of-the-art approaches are
evaluated and discussed. Moreover, future research directions on IFI extraction
techniques for interest point detection are elaborated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jing_J/0/1/0/all/0/1"&gt;Junfeng Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1"&gt;Tian Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weichuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yongsheng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changming Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAR Image Classification Based on Spiking Neural Network through Spike-Time Dependent Plasticity and Gradient Descent. (arXiv:2106.08005v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08005</id>
        <link href="http://arxiv.org/abs/2106.08005"/>
        <updated>2021-06-16T01:21:05.053Z</updated>
        <summary type="html"><![CDATA[At present, the Synthetic Aperture Radar (SAR) image classification method
based on convolution neural network (CNN) has faced some problems such as poor
noise resistance and generalization ability. Spiking neural network (SNN) is
one of the core components of brain-like intelligence and has good application
prospects. This article constructs a complete SAR image classifier based on
unsupervised and supervised learning of SNN by using spike sequences with
complex spatio-temporal information. We firstly expound the spiking neuron
model, the receptive field of SNN, and the construction of spike sequence. Then
we put forward an unsupervised learning algorithm based on STDP and a
supervised learning algorithm based on gradient descent. The average
classification accuracy of single layer and bilayer unsupervised learning SNN
in three categories images on MSTAR dataset is 80.8\% and 85.1\%, respectively.
Furthermore, the convergent output spike sequences of unsupervised learning can
be used as teaching signals. Based on the TensorFlow framework, a single layer
supervised learning SNN is built from the bottom, and the classification
accuracy reaches 90.05\%. By comparing noise resistance and model parameters
between SNNs and CNNs, the effectiveness and outstanding advantages of SNN are
verified. Code to reproduce our experiments is available at
\url{https://github.com/Jiankun-chen/Supervised-SNN-with-GD}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiankun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xiaolan Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Chibiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yirong Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hybrid mmWave and Camera System for Long-Range Depth Imaging. (arXiv:2106.07856v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07856</id>
        <link href="http://arxiv.org/abs/2106.07856"/>
        <updated>2021-06-16T01:21:05.043Z</updated>
        <summary type="html"><![CDATA[mmWave radars offer excellent depth resolution owing to their high bandwidth
at mmWave radio frequencies. Yet, they suffer intrinsically from poor angular
resolution, that is an order-of-magnitude worse than camera systems, and are
therefore not a capable 3-D imaging solution in isolation. We propose
Metamoran, a system that combines the complimentary strengths of radar and
camera systems to obtain depth images at high azimuthal resolutions at
distances of several tens of meters with high accuracy, all from a single fixed
vantage point. Metamoran enables rich long-range depth imaging outdoors with
applications to roadside safety infrastructure, surveillance and wide-area
mapping. Our key insight is to use the high azimuth resolution from cameras
using computer vision techniques, including image segmentation and monocular
depth estimation, to obtain object shapes and use these as priors for our novel
specular beamforming algorithm. We also design this algorithm to work in
cluttered environments with weak reflections and in partially occluded
scenarios. We perform a detailed evaluation of Metamoran's depth imaging and
sensing capabilities in 200 diverse scenes at a major U.S. city. Our evaluation
shows that Metamoran estimates the depth of an object up to 60~m away with a
median error of 28~cm, an improvement of 13$\times$ compared to a naive
radar+camera baseline and 23$\times$ compared to monocular depth estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Diana Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prabhakara_A/0/1/0/all/0/1"&gt;Akarsh Prabhakara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munir_S/0/1/0/all/0/1"&gt;Sirajum Munir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1"&gt;Aswin Sankaranarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Swarun Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face Age Progression With Attribute Manipulation. (arXiv:2106.07696v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07696</id>
        <link href="http://arxiv.org/abs/2106.07696"/>
        <updated>2021-06-16T01:21:04.986Z</updated>
        <summary type="html"><![CDATA[Face is one of the predominant means of person recognition. In the process of
ageing, human face is prone to many factors such as time, attributes, weather
and other subject specific variations. The impact of these factors were not
well studied in the literature of face aging. In this paper, we propose a novel
holistic model in this regard viz., ``Face Age progression With Attribute
Manipulation (FAWAM)", i.e. generating face images at different ages while
simultaneously varying attributes and other subject specific characteristics.
We address the task in a bottom-up manner, as two submodules i.e. face age
progression and face attribute manipulation. For face aging, we use an
attribute-conscious face aging model with a pyramidal generative adversarial
network that can model age-specific facial changes while maintaining intrinsic
subject specific characteristics. For facial attribute manipulation, the age
processed facial image is manipulated with desired attributes while preserving
other details unchanged, leveraging an attribute generative adversarial network
architecture. We conduct extensive analysis in standard large scale datasets
and our model achieves significant performance both quantitatively and
qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tatikonda_S/0/1/0/all/0/1"&gt;Sinzith Tatikonda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nambiar_A/0/1/0/all/0/1"&gt;Athira Nambiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anurag Mittal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reverse Engineering of Generative Models: Inferring Model Hyperparameters from Generated Images. (arXiv:2106.07873v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07873</id>
        <link href="http://arxiv.org/abs/2106.07873"/>
        <updated>2021-06-16T01:21:04.812Z</updated>
        <summary type="html"><![CDATA[State-of-the-art (SOTA) Generative Models (GMs) can synthesize
photo-realistic images that are hard for humans to distinguish from genuine
photos. We propose to perform reverse engineering of GMs to infer the model
hyperparameters from the images generated by these models. We define a novel
problem, "model parsing", as estimating GM network architectures and training
loss functions by examining their generated images -- a task seemingly
impossible for human beings. To tackle this problem, we propose a framework
with two components: a Fingerprint Estimation Network (FEN), which estimates a
GM fingerprint from a generated image by training with four constraints to
encourage the fingerprint to have desired properties, and a Parsing Network
(PN), which predicts network architecture and loss functions from the estimated
fingerprints. To evaluate our approach, we collect a fake image dataset with
$100$K images generated by $100$ GMs. Extensive experiments show encouraging
results in parsing the hyperparameters of the unseen models. Finally, our
fingerprint estimation can be leveraged for deepfake detection and image
attribution, as we show by reporting SOTA results on both the recent Celeb-DF
and image attribution benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asnani_V/0/1/0/all/0/1"&gt;Vishal Asnani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xi Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1"&gt;Tal Hassner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flow Guided Transformable Bottleneck Networks for Motion Retargeting. (arXiv:2106.07771v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07771</id>
        <link href="http://arxiv.org/abs/2106.07771"/>
        <updated>2021-06-16T01:21:04.784Z</updated>
        <summary type="html"><![CDATA[Human motion retargeting aims to transfer the motion of one person in a
"driving" video or set of images to another person. Existing efforts leverage a
long training video from each target person to train a subject-specific motion
transfer model. However, the scalability of such methods is limited, as each
model can only generate videos for the given target subject, and such training
videos are labor-intensive to acquire and process. Few-shot motion transfer
techniques, which only require one or a few images from a target, have recently
drawn considerable attention. Methods addressing this task generally use either
2D or explicit 3D representations to transfer motion, and in doing so,
sacrifice either accurate geometric modeling or the flexibility of an
end-to-end learned representation. Inspired by the Transformable Bottleneck
Network, which renders novel views and manipulations of rigid objects, we
propose an approach based on an implicit volumetric representation of the image
content, which can then be spatially manipulated using volumetric flow fields.
We address the challenging question of how to aggregate information across
different body poses, learning flow fields that allow for combining content
from the appropriate regions of input images of highly non-rigid human subjects
performing complex motions into a single implicit volumetric representation.
This allows us to learn our 3D representation solely from videos of moving
people. Armed with both 3D object understanding and end-to-end learned
rendering, this categorically novel representation delivers state-of-the-art
image generation quality, as shown by our quantitative and qualitative
evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jian Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1"&gt;Menglei Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodford_O/0/1/0/all/0/1"&gt;Oliver J. Woodford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1"&gt;Kyle Olszewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1"&gt;Sergey Tulyakov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Field-Embedded Factorization Machines for Click-through rate prediction. (arXiv:2009.09931v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09931</id>
        <link href="http://arxiv.org/abs/2009.09931"/>
        <updated>2021-06-16T01:21:04.771Z</updated>
        <summary type="html"><![CDATA[Click-through rate (CTR) prediction models are common in many online
applications such as digital advertising and recommender systems. Field-Aware
Factorization Machine (FFM) and Field-weighted Factorization Machine (FwFM) are
state-of-the-art among the shallow models for CTR prediction. Recently, many
deep learning-based models have also been proposed. Among deeper models,
DeepFM, xDeepFM, AutoInt+, and FiBiNet are state-of-the-art models. The deeper
models combine a core architectural component, which learns explicit feature
interactions, with a deep neural network (DNN) component. We propose a novel
shallow Field-Embedded Factorization Machine (FEFM) and its deep counterpart
Deep Field-Embedded Factorization Machine (DeepFEFM). FEFM learns symmetric
matrix embeddings for each field pair along with the usual single vector
embeddings for each feature. FEFM has significantly lower model complexity than
FFM and roughly the same complexity as FwFM. FEFM also has insightful
mathematical properties about important fields and field interactions. DeepFEFM
combines the FEFM interaction vectors learned by the FEFM component with a DNN
and is thus able to learn higher order interactions. We conducted comprehensive
experiments over a wide range of hyperparameters on two large publicly
available real-world datasets. When comparing test AUC and log loss, the
results show that FEFM and DeepFEFM outperform the existing state-of-the-art
shallow and deep models for CTR prediction tasks. We have made the code of FEFM
and DeepFEFM available in the DeepCTR library
(https://github.com/shenweichen/DeepCTR).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pande_H/0/1/0/all/0/1"&gt;Harshit Pande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Potato Crop Stress Identification in Aerial Images using Deep Learning-based Object Detection. (arXiv:2106.07770v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.07770</id>
        <link href="http://arxiv.org/abs/2106.07770"/>
        <updated>2021-06-16T01:21:04.759Z</updated>
        <summary type="html"><![CDATA[Recent research on the application of remote sensing and deep learning-based
analysis in precision agriculture demonstrated a potential for improved crop
management and reduced environmental impacts of agricultural production.
Despite the promising results, the practical relevance of these technologies
for actual field deployment requires novel algorithms that are customized for
analysis of agricultural images and robust to implementation on natural field
imagery. The paper presents an approach for analyzing aerial images of a potato
crop using deep neural networks. The main objective is to demonstrate automated
spatial recognition of a healthy versus stressed crop at a plant level.
Specifically, we examine premature plant senescence resulting in drought stress
on Russet Burbank potato plants. The proposed deep learning model, named
Retina-UNet-Ag, is a variant of Retina-UNet (Jaeger et al., 2018) and includes
connections from low-level semantic dense representation maps to the feature
pyramid network. The paper also introduces a dataset of field images acquired
with a Parrot Sequoia camera carried by a Solo unmanned aerial vehicle.
Experimental validation demonstrated the ability for distinguishing healthy and
stressed plants in field images, achieving an average Dice score coefficient of
0.74. A comparison to related state-of-the-art deep learning models for object
detection revealed that the presented approach is effective for the task at
hand. The method applied here is conducive toward the assessment and
recognition of potato crop stress (early plant senescence resulting from
drought stress in this case) in natural aerial field images collected under
real conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Butte_S/0/1/0/all/0/1"&gt;Sujata Butte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakanski_A/0/1/0/all/0/1"&gt;Aleksandar Vakanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duellman_K/0/1/0/all/0/1"&gt;Kasia Duellman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirkouei_A/0/1/0/all/0/1"&gt;Amin Mirkouei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Audio-Visual Dereverberation. (arXiv:2106.07732v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.07732</id>
        <link href="http://arxiv.org/abs/2106.07732"/>
        <updated>2021-06-16T01:21:04.744Z</updated>
        <summary type="html"><![CDATA[Reverberation from audio reflecting off surfaces and objects in the
environment not only degrades the quality of speech for human perception, but
also severely impacts the accuracy of automatic speech recognition. Prior work
attempts to remove reverberation based on the audio modality only. Our idea is
to learn to dereverberate speech from audio-visual observations. The visual
environment surrounding a human speaker reveals important cues about the room
geometry, materials, and speaker location, all of which influence the precise
reverberation effects in the audio stream. We introduce Visually-Informed
Dereverberation of Audio (VIDA), an end-to-end approach that learns to remove
reverberation based on both the observed sounds and visual scene. In support of
this new task, we develop a large-scale dataset that uses realistic acoustic
renderings of speech in real-world 3D scans of homes offering a variety of room
acoustics. Demonstrating our approach on both simulated and real imagery for
speech enhancement, speech recognition, and speaker identification, we show it
achieves state-of-the-art performance and substantially improves over
traditional audio-only methods. Project page:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1"&gt;David Harwath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.07806</id>
        <link href="http://arxiv.org/abs/2106.07806"/>
        <updated>2021-06-16T01:21:04.724Z</updated>
        <summary type="html"><![CDATA[Machine learning is revolutionizing image-based diagnostics in pathology and
radiology. ML models have shown promising results in research settings, but
their lack of interoperability has been a major barrier for clinical
integration and evaluation. The DICOM a standard specifies Information Object
Definitions and Services for the representation and communication of digital
images and related information, including image-derived annotations and
analysis results. However, the complexity of the standard represents an
obstacle for its adoption in the ML community and creates a need for software
libraries and tools that simplify working with data sets in DICOM format. Here
we present the highdicom library, which provides a high-level application
programming interface for the Python programming language that abstracts
low-level details of the standard and enables encoding and decoding of
image-derived information in DICOM format in a few lines of Python code. The
highdicom library ties into the extensive Python ecosystem for image processing
and machine learning. Simultaneously, by simplifying creation and parsing of
DICOM-compliant files, highdicom achieves interoperability with the medical
imaging systems that hold the data used to train and run ML models, and
ultimately communicate and store model outputs for clinical use. We demonstrate
through experiments with slide microscopy and computed tomography imaging,
that, by bridging these two ecosystems, highdicom enables developers to train
and evaluate state-of-the-art ML models in pathology and radiology while
remaining compliant with the DICOM standard and interoperable with clinical
systems at all stages. To promote standardization of ML research and streamline
the ML model development and deployment process, we made the library available
free and open-source.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bridge_C/0/1/0/all/0/1"&gt;Christopher P. Bridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gorman_C/0/1/0/all/0/1"&gt;Chris Gorman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pieper_S/0/1/0/all/0/1"&gt;Steven Pieper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Doyle_S/0/1/0/all/0/1"&gt;Sean W. Doyle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lennerz_J/0/1/0/all/0/1"&gt;Jochen K. Lennerz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1"&gt;Jayashree Kalpathy-Cramer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Clunie_D/0/1/0/all/0/1"&gt;David A. Clunie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fedorov_A/0/1/0/all/0/1"&gt;Andriy Y. Fedorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Herrmann_M/0/1/0/all/0/1"&gt;Markus D. Herrmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10005</id>
        <link href="http://arxiv.org/abs/2105.10005"/>
        <updated>2021-06-16T01:21:04.389Z</updated>
        <summary type="html"><![CDATA[Physical processes, camera movement, and unpredictable environmental
conditions like the presence of dust can induce noise and artifacts in video
feeds. We observe that popular unsupervised MOT methods are dependent on
noise-free inputs. We show that the addition of a small amount of artificial
random noise causes a sharp degradation in model performance on benchmark
metrics. We resolve this problem by introducing a robust unsupervised
multi-object tracking (MOT) model: AttU-Net. The proposed single-head attention
model helps limit the negative impact of noise by learning visual
representations at different segment scales. AttU-Net shows better unsupervised
MOT tracking performance over variational inference-based state-of-the-art
baselines. We evaluate our method in the MNIST-MOT and the Atari game video
benchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''
which consists of moving Japanese characters and ``Fashion-MNIST MOT'' to
validate the effectiveness of the MOT models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;C.-H. Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1"&gt;Mohit Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Y.-C. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quan Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1"&gt;Tomoaki Yoshinaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1"&gt;Tomokazu Murakami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11066</id>
        <link href="http://arxiv.org/abs/2010.11066"/>
        <updated>2021-06-16T01:21:04.362Z</updated>
        <summary type="html"><![CDATA[Spoken conversational question answering (SCQA) requires machines to model
complex dialogue flow given the speech utterances and text corpora. Different
from traditional text question answering (QA) tasks, SCQA involves audio signal
processing, passage comprehension, and contextual understanding. However, ASR
systems introduce unexpected noisy signals to the transcriptions, which result
in performance degradation on SCQA. To overcome the problem, we propose CADNet,
a novel contextualized attention-based distillation approach, which applies
both cross-attention and self-attention to obtain ASR-robust contextualized
embedding representations of the passage and dialogue history for performance
improvements. We also introduce the spoken conventional knowledge distillation
framework to distill the ASR-robust knowledge from the estimated probabilities
of the teacher model to the student. We conduct extensive experiments on the
Spoken-CoQA dataset and demonstrate that our approach achieves remarkable
performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Self-supervised Multi-task Learning for COVID-19 Information Retrieval and Extraction. (arXiv:2106.08252v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08252</id>
        <link href="http://arxiv.org/abs/2106.08252"/>
        <updated>2021-06-16T01:21:04.341Z</updated>
        <summary type="html"><![CDATA[The rapidly evolving literature of COVID-19 related articles makes it
challenging for NLP models to be effectively trained for information retrieval
and extraction with the corresponding labeled data that follows the current
distribution of the pandemic. On the other hand, due to the uncertainty of the
situation, human experts' supervision would always be required to double check
the decision making of these models highlighting the importance of
interpretability. In the light of these challenges, this study proposes an
interpretable self-supervised multi-task learning model to jointly and
effectively tackle the tasks of information retrieval (IR) and extraction (IE)
during the current emergency health crisis situation. Our results show that our
model effectively leverage the multi-task and self-supervised learning to
improve generalization, data efficiency and robustness to the ongoing dataset
shift problem. Our model outperforms baselines in IE and IR tasks, respectively
by micro-f score of 0.08 (LCA-F score of 0.05), and MAP of 0.05 on average. In
IE the zero- and few-shot learning performances are on average 0.32 and 0.19
micro-f score higher than those of the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ebadi_N/0/1/0/all/0/1"&gt;Nima Ebadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1"&gt;Peyman Najafirad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detect and remove watermark in deep neural networks via generative adversarial networks. (arXiv:2106.08104v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.08104</id>
        <link href="http://arxiv.org/abs/2106.08104"/>
        <updated>2021-06-16T01:21:04.329Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNN) have achieved remarkable performance in various
fields. However, training a DNN model from scratch requires a lot of computing
resources and training data. It is difficult for most individual users to
obtain such computing resources and training data. Model copyright infringement
is an emerging problem in recent years. For instance, pre-trained models may be
stolen or abuse by illegal users without the authorization of the model owner.
Recently, many works on protecting the intellectual property of DNN models have
been proposed. In these works, embedding watermarks into DNN based on backdoor
is one of the widely used methods. However, when the DNN model is stolen, the
backdoor-based watermark may face the risk of being detected and removed by an
adversary. In this paper, we propose a scheme to detect and remove watermark in
deep neural networks via generative adversarial networks (GAN). We demonstrate
that the backdoor-based DNN watermarks are vulnerable to the proposed GAN-based
watermark removal attack. The proposed attack method includes two phases. In
the first phase, we use the GAN and few clean images to detect and reverse the
watermark in the DNN model. In the second phase, we fine-tune the watermarked
DNN based on the reversed backdoor images. Experimental evaluations on the
MNIST and CIFAR10 datasets demonstrate that, the proposed method can
effectively remove about 98% of the watermark in DNN models, as the watermark
retention rate reduces from 100% to less than 2% after applying the proposed
attack. In the meantime, the proposed attack hardly affects the model's
performance. The test accuracy of the watermarked DNN on the MNIST and the
CIFAR10 datasets drops by less than 1% and 3%, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1"&gt;Mingfu Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shichang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yushu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiqiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval. (arXiv:2104.01894v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01894</id>
        <link href="http://arxiv.org/abs/2104.01894"/>
        <updated>2021-06-16T01:21:04.317Z</updated>
        <summary type="html"><![CDATA[Speech-based image retrieval has been studied as a proxy for joint
representation learning, usually without emphasis on retrieval itself. As such,
it is unclear how well speech-based retrieval can work in practice -- both in
an absolute sense and versus alternative strategies that combine automatic
speech recognition (ASR) with strong text encoders. In this work, we
extensively study and expand choices of encoder architectures, training
methodology (including unimodal and multimodal pretraining), and other factors.
Our experiments cover different types of speech in three datasets: Flickr
Audio, Places Audio, and Localized Narratives. Our best model configuration
achieves large gains over state of the art, e.g., pushing recall-at-one from
21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also
show our best speech-based models can match or exceed cascaded ASR-to-text
encoding when speech is spontaneous, accented, or otherwise hard to
automatically transcribe.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1"&gt;Ramon Sanabria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1"&gt;Austin Waters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1"&gt;Jason Baldridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To Infinity and Beyond! Accessibility is the Future for Kids' Search Engines. (arXiv:2106.07813v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07813</id>
        <link href="http://arxiv.org/abs/2106.07813"/>
        <updated>2021-06-16T01:21:04.306Z</updated>
        <summary type="html"><![CDATA[Research in the area of search engines for children remains in its infancy.
Seminal works have studied how children use mainstream search engines, as well
as how to design and evaluate custom search engines explicitly for children.
These works, however, tend to take a one-size-fits-all view, treating children
as a unit. Nevertheless, even at the same age, children are known to possess
and exhibit different capabilities. These differences affect how children
access and use search engines. To better serve children, in this vision paper,
we spotlight accessibility and discuss why current research on children and
search engines does not, but should, focus on this significant matter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milton_A/0/1/0/all/0/1"&gt;Ashlee Milton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allen_G/0/1/0/all/0/1"&gt;Garrett Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pera_M/0/1/0/all/0/1"&gt;Maria Soledad Pera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Query Embedding on Hyper-relational Knowledge Graphs. (arXiv:2106.08166v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.08166</id>
        <link href="http://arxiv.org/abs/2106.08166"/>
        <updated>2021-06-16T01:21:04.296Z</updated>
        <summary type="html"><![CDATA[Multi-hop logical reasoning is an established problem in the field of
representation learning on knowledge graphs (KGs). It subsumes both one-hop
link prediction as well as other more complex types of logical queries.
Existing algorithms operate only on classical, triple-based graphs, whereas
modern KGs often employ a hyper-relational modeling paradigm. In this paradigm,
typed edges may have several key-value pairs known as qualifiers that provide
fine-grained context for facts. In queries, this context modifies the meaning
of relations, and usually reduces the answer set. Hyper-relational queries are
often observed in real-world KG applications, and existing approaches for
approximate query answering cannot make use of qualifier pairs. In this work,
we bridge this gap and extend the multi-hop reasoning problem to
hyper-relational KGs allowing to tackle this new type of complex queries.
Building upon recent advancements in Graph Neural Networks and query embedding
techniques, we study how to embed and answer hyper-relational conjunctive
queries. Besides that, we propose a method to answer such queries and
demonstrate in our experiments that qualifiers improve query answering on a
diverse set of query patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alivanistos_D/0/1/0/all/0/1"&gt;Dimitrios Alivanistos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1"&gt;Max Berrendorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1"&gt;Michael Cochez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1"&gt;Mikhail Galkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploration in Online Advertising Systems with Deep Uncertainty-Aware Learning. (arXiv:2012.02298v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02298</id>
        <link href="http://arxiv.org/abs/2012.02298"/>
        <updated>2021-06-16T01:21:04.267Z</updated>
        <summary type="html"><![CDATA[Modern online advertising systems inevitably rely on personalization methods,
such as click-through rate (CTR) prediction. Recent progress in CTR prediction
enjoys the rich representation capabilities of deep learning and achieves great
success in large-scale industrial applications. However, these methods can
suffer from lack of exploration. Another line of prior work addresses the
exploration-exploitation trade-off problem with contextual bandit methods,
which are recently less studied in the industry due to the difficulty in
extending their flexibility with deep models. In this paper, we propose a novel
Deep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on
Gaussian processes, which can provide predictive uncertainty estimations while
maintaining the flexibility of deep neural networks. DUAL can be easily
implemented on existing models and deployed in real-time systems with minimal
extra computational overhead. By linking the predictive uncertainty estimation
ability of DUAL to well-known bandit algorithms, we further present DUAL-based
Ad-ranking strategies to boost up long-term utilities such as the social
welfare in advertising systems. Experimental results on several public datasets
demonstrate the effectiveness of our methods. Remarkably, an online A/B test
deployed in the Alibaba display advertising platform shows an 8.2% social
welfare improvement and an 8.0% revenue lift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1"&gt;Chao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhifeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1"&gt;Shuo Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lining Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1"&gt;Yifan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1"&gt;Kun Gai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kuang-chih Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Color2Style: Real-Time Exemplar-Based Image Colorization with Self-Reference Learning and Deep Feature Modulation. (arXiv:2106.08017v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08017</id>
        <link href="http://arxiv.org/abs/2106.08017"/>
        <updated>2021-06-16T01:21:04.242Z</updated>
        <summary type="html"><![CDATA[Legacy black-and-white photos are riddled with people's nostalgia and
glorious memories of the past. To better relive the elapsed frozen moments, in
this paper, we present a deep exemplar-based image colorization approach named
Color2Style to resurrect these grayscale image media by filling them with
vibrant colors. Generally, for exemplar-based colorization, unsupervised and
unpaired training are usually adopted, due to the difficulty of obtaining input
and ground truth image pairs. To train an exemplar-based colorization model,
current algorithms usually strive to achieve two procedures: i) retrieving a
large number of reference images with high similarity in advance, which is
inevitably time-consuming and tedious; ii) designing complicated modules to
transfer the colors of the reference image to the grayscale image, by
calculating and leveraging the deep semantic correspondence between them (e.g.,
non-local operation). Contrary to the previous methods, we solve and simplify
the above two steps in one end-to-end learning procedure. First, we adopt a
self-augmented self-reference training scheme, where the reference image is
generated by graphical transformations from the original colorful one whereby
the training can be formulated in a paired manner. Second, instead of computing
complex and inexplicable correspondence maps, our method exploits a simple yet
effective deep feature modulation (DFM) module, which injects the color
embeddings extracted from the reference image into the deep representations of
the input grayscale image. Such design is much more lightweight and
intelligible, achieving appealing performance with real-time processing speed.
Moreover, our model does not require multifarious loss functions and
regularization terms like existing methods, but only two widely used loss
functions. Codes and models will be available at
https://github.com/zhaohengyuan1/Color2Style.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hengyuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hotel Recognition via Latent Image Embedding. (arXiv:2106.08042v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.08042</id>
        <link href="http://arxiv.org/abs/2106.08042"/>
        <updated>2021-06-16T01:21:03.979Z</updated>
        <summary type="html"><![CDATA[We approach the problem of hotel recognition with deep metric learning. We
overview the existing approaches and propose a modification to Contrastive loss
called Contrastive-Triplet loss. We construct a robust pipeline for
benchmarking metric learning models and perform experiments on Hotels-50K and
CUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval
on Hotels-50k. We open-source our code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tseytlin_B/0/1/0/all/0/1"&gt;Boris Tseytlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1"&gt;Ilya Makarov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Axiomatic Explanations for Neural Ranking Models. (arXiv:2106.08019v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.08019</id>
        <link href="http://arxiv.org/abs/2106.08019"/>
        <updated>2021-06-16T01:21:03.957Z</updated>
        <summary type="html"><![CDATA[Recently, neural networks have been successfully employed to improve upon
state-of-the-art performance in ad-hoc retrieval tasks via machine-learned
ranking functions. While neural retrieval models grow in complexity and impact,
little is understood about their correspondence with well-studied IR
principles. Recent work on interpretability in machine learning has provided
tools and techniques to understand neural models in general, yet there has been
little progress towards explaining ranking models.

We investigate whether one can explain the behavior of neural ranking models
in terms of their congruence with well understood principles of document
ranking by using established theories from axiomatic IR. Axiomatic analysis of
information retrieval models has formalized a set of constraints on ranking
decisions that reasonable retrieval models should fulfill. We operationalize
this axiomatic thinking to reproduce rankings based on combinations of
elementary constraints. This allows us to investigate to what extent the
ranking decisions of neural rankers can be explained in terms of retrieval
axioms, and which axioms apply in which situations. Our experimental study
considers a comprehensive set of axioms over several representative neural
rankers. While the existing axioms can already explain the particularly
confident ranking decisions rather well, future work should extend the axiom
set to also cover the other still "unexplainable" neural IR rank decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Volske_M/0/1/0/all/0/1"&gt;Michael V&amp;#xf6;lske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bondarenko_A/0/1/0/all/0/1"&gt;Alexander Bondarenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frobe_M/0/1/0/all/0/1"&gt;Maik Fr&amp;#xf6;be&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagen_M/0/1/0/all/0/1"&gt;Matthias Hagen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1"&gt;Benno Stein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Jaspreet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Avishek Anand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User-specific Adaptive Fine-tuning for Cross-domain Recommendations. (arXiv:2106.07864v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07864</id>
        <link href="http://arxiv.org/abs/2106.07864"/>
        <updated>2021-06-16T01:21:03.932Z</updated>
        <summary type="html"><![CDATA[Making accurate recommendations for cold-start users has been a longstanding
and critical challenge for recommender systems (RS). Cross-domain
recommendations (CDR) offer a solution to tackle such a cold-start problem when
there is no sufficient data for the users who have rarely used the system. An
effective approach in CDR is to leverage the knowledge (e.g., user
representations) learned from a related but different domain and transfer it to
the target domain. Fine-tuning works as an effective transfer learning
technique for this objective, which adapts the parameters of a pre-trained
model from the source domain to the target domain. However, current methods are
mainly based on the global fine-tuning strategy: the decision of which layers
of the pre-trained model to freeze or fine-tune is taken for all users in the
target domain. In this paper, we argue that users in RS are personalized and
should have their own fine-tuning policies for better preference transfer
learning. As such, we propose a novel User-specific Adaptive Fine-tuning method
(UAF), selecting which layers of the pre-trained network to fine-tune, on a
per-user basis. Specifically, we devise a policy network with three alternative
strategies to automatically decide which layers to be fine-tuned and which
layers to have their parameters frozen for each user. Extensive experiments
show that the proposed UAF exhibits significantly better and more robust
performance for user cold-start recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1"&gt;Fajie Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaxi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Min Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Domain Knowledge into Health Recommender Systems using Hyperbolic Embeddings. (arXiv:2106.07720v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07720</id>
        <link href="http://arxiv.org/abs/2106.07720"/>
        <updated>2021-06-16T01:21:03.919Z</updated>
        <summary type="html"><![CDATA[In contrast to many other domains, recommender systems in health services may
benefit particularly from the incorporation of health domain knowledge, as it
helps to provide meaningful and personalised recommendations catering to the
individual's health needs. With recent advances in representation learning
enabling the hierarchical embedding of health knowledge into the hyperbolic
Poincare space, this work proposes a content-based recommender system for
patient-doctor matchmaking in primary care based on patients' health profiles,
enriched by pre-trained Poincare embeddings of the ICD-9 codes through transfer
learning. The proposed model outperforms its conventional counterpart in terms
of recommendation accuracy and has several important business implications for
improving the patient-doctor relationship.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peito_J/0/1/0/all/0/1"&gt;Joel Peito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1"&gt;Qiwei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does your robot know? Enhancing children's information retrieval through spoken conversation with responsible robots. (arXiv:2106.07931v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07931</id>
        <link href="http://arxiv.org/abs/2106.07931"/>
        <updated>2021-06-16T01:21:03.897Z</updated>
        <summary type="html"><![CDATA[In this paper, we identify challenges in children's current information
retrieval process, and propose conversational robots as an opportunity to ease
this process in a responsible way. Tools children currently use in this
process, such as search engines on a computer or voice agents, do not always
meet their specific needs. The conversational robot we propose maintains
context, asks clarifying questions, and gives suggestions in order to better
meet children's needs. Since children are often too trusting of robots, we
propose to have the robot measure, monitor and adapt to the trust the child has
in the robot. This way, we hope to induce a critical attitude with the children
during their information retrieval process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beelen_T/0/1/0/all/0/1"&gt;T. Beelen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velner_E/0/1/0/all/0/1"&gt;E. Velner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ordelman_R/0/1/0/all/0/1"&gt;R. Ordelman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_K/0/1/0/all/0/1"&gt;K.P. Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evers_V/0/1/0/all/0/1"&gt;V. Evers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huibers_T/0/1/0/all/0/1"&gt;T. Huibers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can BERT Dig It? -- Named Entity Recognition for Information Retrieval in the Archaeology Domain. (arXiv:2106.07742v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.07742</id>
        <link href="http://arxiv.org/abs/2106.07742"/>
        <updated>2021-06-16T01:21:03.869Z</updated>
        <summary type="html"><![CDATA[The amount of archaeological literature is growing rapidly. Until recently,
these data were only accessible through metadata search. We implemented a text
retrieval engine for a large archaeological text collection ($\sim 658$ Million
words). In archaeological IR, domain-specific entities such as locations, time
periods, and artefacts, play a central role. This motivated the development of
a named entity recognition (NER) model to annotate the full collection with
archaeological named entities. In this paper, we present ArcheoBERTje, a BERT
model pre-trained on Dutch archaeological texts. We compare the model's quality
and output on a Named Entity Recognition task to a generic multilingual model
and a generic Dutch model. We also investigate ensemble methods for combining
multiple BERT models, and combining the best BERT model with a domain thesaurus
using Conditional Random Fields (CRF). We find that ArcheoBERTje outperforms
both the multilingual and Dutch model significantly with a smaller standard
deviation between runs, reaching an average F1 score of 0.735. The model also
outperforms ensemble methods combining the three models. Combining ArcheoBERTje
predictions and explicit domain knowledge from the thesaurus did not increase
the F1 score. We quantitatively and qualitatively analyse the differences
between the vocabulary and output of the BERT models on the full collection and
provide some valuable insights in the effect of fine-tuning for specific
domains. Our results indicate that for a highly specific text domain such as
archaeology, further pre-training on domain-specific data increases the model's
quality on NER by a much larger margin than shown for other domains in the
literature, and that domain-specific pre-training makes the addition of domain
knowledge from a thesaurus unnecessary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandsen_A/0/1/0/all/0/1"&gt;Alex Brandsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1"&gt;Suzan Verberne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lambers_K/0/1/0/all/0/1"&gt;Karsten Lambers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wansleeben_M/0/1/0/all/0/1"&gt;Milco Wansleeben&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full Bitcoin Blockchain Data Made Easy. (arXiv:2106.08072v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.08072</id>
        <link href="http://arxiv.org/abs/2106.08072"/>
        <updated>2021-06-16T01:21:03.828Z</updated>
        <summary type="html"><![CDATA[Despite the fact that it is publicly available, collecting and processing the
full bitcoin blockchain data is not trivial. Its mere size, history, and other
features indeed raise quite specific challenges, that we address in this paper.
The strengths of our approach are the following: it relies on very basic and
standard tools, which makes the procedure reliable and easily reproducible; it
is a purely lossless procedure ensuring that we catch and preserve all existing
data; it provides additional indexing that makes it easy to further process the
whole data and select appropriate subsets of it. We present our procedure in
details and illustrate its added value on large-scale use cases, like address
clustering. We provide an implementation online, as well as the obtained
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Emery_J/0/1/0/all/0/1"&gt;Jules Azad Emery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Latapy_M/0/1/0/all/0/1"&gt;Matthieu Latapy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup. (arXiv:2101.06983v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06983</id>
        <link href="http://arxiv.org/abs/2101.06983"/>
        <updated>2021-06-16T00:27:38.497Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has been applied successfully to learn vector
representations of text. Previous research demonstrated that learning
high-quality representations benefits from batch-wise contrastive loss with a
large number of negatives. In practice, the technique of in-batch negative is
used, where for each example in a batch, other batch examples' positives will
be taken as its negatives, avoiding encoding extra negatives. This, however,
still conditions each example's loss on all batch examples and requires fitting
the entire large batch into GPU memory. This paper introduces a gradient
caching technique that decouples backpropagation between contrastive loss and
the encoder, removing encoder backward pass data dependency along the batch
dimension. As a result, gradients can be computed for one subset of the batch
at a time, leading to almost constant memory usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Luyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1"&gt;Jamie Callan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup. (arXiv:2101.06983v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06983</id>
        <link href="http://arxiv.org/abs/2101.06983"/>
        <updated>2021-06-16T00:27:38.461Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has been applied successfully to learn vector
representations of text. Previous research demonstrated that learning
high-quality representations benefits from batch-wise contrastive loss with a
large number of negatives. In practice, the technique of in-batch negative is
used, where for each example in a batch, other batch examples' positives will
be taken as its negatives, avoiding encoding extra negatives. This, however,
still conditions each example's loss on all batch examples and requires fitting
the entire large batch into GPU memory. This paper introduces a gradient
caching technique that decouples backpropagation between contrastive loss and
the encoder, removing encoder backward pass data dependency along the batch
dimension. As a result, gradients can be computed for one subset of the batch
at a time, leading to almost constant memory usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Luyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1"&gt;Jamie Callan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Meta-Feature Selection for the Algorithm Recommendation Problem. (arXiv:2106.03954v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03954</id>
        <link href="http://arxiv.org/abs/2106.03954"/>
        <updated>2021-06-15T22:41:25.615Z</updated>
        <summary type="html"><![CDATA[With the popularity of Machine Learning (ML) solutions, algorithms and data
have been released faster than the capacity of processing them. In this
context, the problem of Algorithm Recommendation (AR) is receiving a
significant deal of attention recently. This problem has been addressed in the
literature as a learning task, often as a Meta-Learning problem where the aim
is to recommend the best alternative for a specific dataset. For such, datasets
encoded by meta-features are explored by ML algorithms that try to learn the
mapping between meta-representations and the best technique to be used. One of
the challenges for the successful use of ML is to define which features are the
most valuable for a specific dataset since several meta-features can be used,
which increases the meta-feature dimension. This paper presents an empirical
analysis of Feature Selection and Feature Extraction in the meta-level for the
AR problem. The present study was focused on three criteria: predictive
performance, dimensionality reduction, and pipeline runtime. As we verified,
applying Dimensionality Reduction (DR) methods did not improve predictive
performances in general. However, DR solutions reduced about 80% of the
meta-features, obtaining pretty much the same performance as the original setup
but with lower runtimes. The only exception was PCA, which presented about the
same runtime as the original meta-features. Experimental results also showed
that various datasets have many non-informative meta-features and that it is
possible to obtain high predictive performance using around 20% of the original
meta-features. Therefore, due to their natural trend for high dimensionality,
DR methods should be used for Meta-Feature Selection and Meta-Feature
Extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1"&gt;Geand Trindade Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1"&gt;Moises Rocha dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1"&gt;Andre Carlos Ponce de Leon Ferreira de Carvalho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustifying $\ell_\infty$ Adversarial Training to the Union of Perturbation Models. (arXiv:2105.14710v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14710</id>
        <link href="http://arxiv.org/abs/2105.14710"/>
        <updated>2021-06-15T22:41:25.605Z</updated>
        <summary type="html"><![CDATA[Classical adversarial training (AT) frameworks are designed to achieve high
adversarial accuracy against a single attack type, typically $\ell_\infty$
norm-bounded perturbations. Recent extensions in AT have focused on defending
against the union of multiple perturbations but this benefit is obtained at the
expense of a significant (up to $10\times$) increase in training complexity
over single-attack $\ell_\infty$ AT. In this work, we expand the capabilities
of widely popular single-attack $\ell_\infty$ AT frameworks to provide
robustness to the union of ($\ell_\infty, \ell_2, \ell_1$) perturbations while
preserving their training efficiency. Our technique, referred to as Shaped
Noise Augmented Processing (SNAP), exploits a well-established byproduct of
single-attack AT frameworks -- the reduction in the curvature of the decision
boundary of networks. SNAP prepends a given deep net with a shaped noise
augmentation layer whose distribution is learned along with network parameters
using any standard single-attack AT. As a result, SNAP enhances adversarial
accuracy of ResNet-18 on CIFAR-10 against the union of ($\ell_\infty, \ell_2,
\ell_1$) perturbations by 14%-to-20% for four state-of-the-art (SOTA)
single-attack $\ell_\infty$ AT frameworks, and, for the first time, establishes
a benchmark for ResNet-50 and ResNet-101 on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1"&gt;Ameya D. Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuttle_M/0/1/0/all/0/1"&gt;Michael Tuttle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1"&gt;Alexander G. Schwing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanbhag_N/0/1/0/all/0/1"&gt;Naresh R. Shanbhag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signal Transformer: Complex-valued Attention and Meta-Learning for Signal Recognition. (arXiv:2106.04392v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04392</id>
        <link href="http://arxiv.org/abs/2106.04392"/>
        <updated>2021-06-15T22:41:25.595Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been shown as a class of useful tools for
addressing signal recognition issues in recent years, especially for
identifying the nonlinear feature structures of signals. However, this power of
most deep learning techniques heavily relies on an abundant amount of training
data, so the performance of classic neural nets decreases sharply when the
number of training data samples is small or unseen data are presented in the
testing phase. This calls for an advanced strategy, i.e., model-agnostic
meta-learning (MAML), which is able to capture the invariant representation of
the data samples or signals. In this paper, inspired by the special structure
of the signal, i.e., real and imaginary parts consisted in practical
time-series signals, we propose a Complex-valued Attentional MEta Learner
(CAMEL) for the problem of few-shot signal recognition by leveraging attention
and meta-learning in the complex domain. To the best of our knowledge, this is
also the first complex-valued MAML that can find the first-order stationary
points of general nonconvex problems with theoretical convergence guarantees.
Extensive experiments results showcase the superiority of the proposed CAMEL
compared with the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yihong Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Ying Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Songtao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1"&gt;Qingjiang Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LaplaceNet: A Hybrid Energy-Neural Model for Deep Semi-Supervised Classification. (arXiv:2106.04527v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04527</id>
        <link href="http://arxiv.org/abs/2106.04527"/>
        <updated>2021-06-15T22:41:25.576Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning has received a lot of recent attention as it
alleviates the need for large amounts of labelled data which can often be
expensive, requires expert knowledge and be time consuming to collect. Recent
developments in deep semi-supervised classification have reached unprecedented
performance and the gap between supervised and semi-supervised learning is
ever-decreasing. This improvement in performance has been based on the
inclusion of numerous technical tricks, strong augmentation techniques and
costly optimisation schemes with multi-term loss functions. We propose a new
framework, LaplaceNet, for deep semi-supervised classification that has a
greatly reduced model complexity. We utilise a hybrid energy-neural network
where graph based pseudo-labels, generated by minimising the graphical
Laplacian, are used to iteratively improve a neural-network backbone. Our model
outperforms state-of-the-art methods for deep semi-supervised classification,
over several benchmark datasets. Furthermore, we consider the application of
strong-augmentations to neural networks theoretically and justify the use of a
multi-sampling approach for semi-supervised learning. We demonstrate, through
rigorous experimentation, that a multi-sampling augmentation approach improves
generalisation and reduces the sensitivity of the network to augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sellars_P/0/1/0/all/0/1"&gt;Philip Sellars&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1"&gt;Angelica I. Aviles-Rivero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation. (arXiv:2106.04563v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04563</id>
        <link href="http://arxiv.org/abs/2106.04563"/>
        <updated>2021-06-15T22:41:25.566Z</updated>
        <summary type="html"><![CDATA[While deep and large pre-trained models are the state-of-the-art for various
natural language processing tasks, their huge size poses significant challenges
for practical uses in resource constrained settings. Recent works in knowledge
distillation propose task-agnostic as well as task-specific methods to compress
these models, with task-specific ones often yielding higher compression rate.
In this work, we develop a new task-agnostic distillation framework
XtremeDistilTransformers that leverages the advantage of task-specific methods
for learning a small universal model that can be applied to arbitrary tasks and
languages. To this end, we study the transferability of several source tasks,
augmentation resources and model architecture for distillation. We evaluate our
model performance on multiple tasks, including the General Language
Understanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and
a massive multi-lingual NER dataset with 41 languages. We release three
distilled task-agnostic checkpoints with 13MM, 22MM and 33MM parameters
obtaining SOTA performance in several tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Subhabrata Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Theoretical Framework of Out-of-Distribution Generalization. (arXiv:2106.04496v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04496</id>
        <link href="http://arxiv.org/abs/2106.04496"/>
        <updated>2021-06-15T22:41:25.555Z</updated>
        <summary type="html"><![CDATA[Generalization to out-of-distribution (OOD) data, or domain generalization,
is one of the central problems in modern machine learning. Recently, there is a
surge of attempts to propose algorithms for OOD that mainly build upon the idea
of extracting invariant features. Although intuitively reasonable, theoretical
understanding of what kind of invariance can guarantee OOD generalization is
still limited, and generalization to arbitrary out-of-distribution is clearly
impossible. In this work, we take the first step towards rigorous and
quantitative definitions of 1) what is OOD; and 2) what does it mean by saying
an OOD problem is learnable. We also introduce a new concept of expansion
function, which characterizes to what extent the variance is amplified in the
test domains over the training domains, and therefore give a quantitative
meaning of invariant features. Based on these, we prove OOD generalization
error bounds. It turns out that OOD generalization largely depends on the
expansion function. As recently pointed out by Gulrajani and Lopez-Paz (2020),
any OOD learning algorithm without a model selection module is incomplete. Our
theory naturally induces a model selection criterion. Extensive experiments on
benchmark OOD datasets demonstrate that our model selection criterion has a
significant advantage over baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Haotian Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chuanlong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruichen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principled Hyperedge Prediction with Structural Spectral Features and Neural Networks. (arXiv:2106.04292v4 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04292</id>
        <link href="http://arxiv.org/abs/2106.04292"/>
        <updated>2021-06-15T22:41:25.543Z</updated>
        <summary type="html"><![CDATA[Hypergraph offers a framework to depict the multilateral relationships in
real-world complex data. Predicting higher-order relationships, i.e hyperedge,
becomes a fundamental problem for the full understanding of complicated
interactions. The development of graph neural network (GNN) has greatly
advanced the analysis of ordinary graphs with pair-wise relations. However,
these methods could not be easily extended to the case of hypergraph. In this
paper, we generalize the challenges of GNN in representing higher-order data in
principle, which are edge- and node-level ambiguities. To overcome the
challenges, we present SNALS that utilizes bipartite graph neural network with
structural features to collectively tackle the two ambiguity issues. SNALS
captures the joint interactions of a hyperedge by its local environment, which
is retrieved by collecting the spectrum information of their connections. As a
result, SNALS achieves nearly 30% performance increase compared with most
recent GNN-based models. In addition, we applied SNALS to predict genetic
higher-order interactions on 3D genome organization data. SNALS showed
consistently high prediction accuracy across different chromosomes, and
generated novel findings on 4-way gene interaction, which is further validated
by existing literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1"&gt;Changlin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Muhan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1"&gt;Wei Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Sha Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Heavy-Tail Phenomenon in SGD. (arXiv:2006.04740v5 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04740</id>
        <link href="http://arxiv.org/abs/2006.04740"/>
        <updated>2021-06-15T22:41:25.530Z</updated>
        <summary type="html"><![CDATA[In recent years, various notions of capacity and complexity have been
proposed for characterizing the generalization properties of stochastic
gradient descent (SGD) in deep learning. Some of the popular notions that
correlate well with the performance on unseen data are (i) the `flatness' of
the local minimum found by SGD, which is related to the eigenvalues of the
Hessian, (ii) the ratio of the stepsize $\eta$ to the batch-size $b$, which
essentially controls the magnitude of the stochastic gradient noise, and (iii)
the `tail-index', which measures the heaviness of the tails of the network
weights at convergence. In this paper, we argue that these three seemingly
unrelated perspectives for generalization are deeply linked to each other. We
claim that depending on the structure of the Hessian of the loss at the
minimum, and the choices of the algorithm parameters $\eta$ and $b$, the SGD
iterates will converge to a \emph{heavy-tailed} stationary distribution. We
rigorously prove this claim in the setting of quadratic optimization: we show
that even in a simple linear regression problem with independent and
identically distributed data whose distribution has finite moments of all
order, the iterates can be heavy-tailed with infinite variance. We further
characterize the behavior of the tails with respect to algorithm parameters,
the dimension, and the curvature. We then translate our results into insights
about the behavior of SGD in deep learning. We support our theory with
experiments conducted on synthetic data, fully connected, and convolutional
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1"&gt;Mert Gurbuzbalaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Simsekli_U/0/1/0/all/0/1"&gt;Umut &amp;#x15e;im&amp;#x15f;ekli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingjiong Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We Know What You Want: An Advertising Strategy Recommender System for Online Advertising. (arXiv:2105.14188v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14188</id>
        <link href="http://arxiv.org/abs/2105.14188"/>
        <updated>2021-06-15T22:41:25.502Z</updated>
        <summary type="html"><![CDATA[Advertising expenditures have become the major source of revenue for
e-commerce platforms. Providing good advertising experiences for advertisers by
reducing their costs of trial and error in discovering the optimal advertising
strategies is crucial for the long-term prosperity of online advertising. To
achieve this goal, the advertising platform needs to identify the advertiser's
optimization objectives, and then recommend the corresponding strategies to
fulfill the objectives. In this work, we first deploy a prototype of strategy
recommender system on Taobao display advertising platform, which indeed
increases the advertisers' performance and the platform's revenue, indicating
the effectiveness of strategy recommendation for online advertising. We further
augment this prototype system by explicitly learning the advertisers'
preferences over various advertising performance indicators and then
optimization objectives through their adoptions of different recommending
advertising strategies. We use contextual bandit algorithms to efficiently
learn the advertisers' preferences and maximize the recommendation adoption,
simultaneously. Simulation experiments based on Taobao online bidding data show
that the designed algorithms can effectively optimize the strategy adoption
rate of advertisers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Liyi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Junqi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhenzhe Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhiye Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1"&gt;Zhizhuang Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Fei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1"&gt;Lvyin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuning Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution Homography Estimation. (arXiv:2106.04067v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04067</id>
        <link href="http://arxiv.org/abs/2106.04067"/>
        <updated>2021-06-15T22:41:25.275Z</updated>
        <summary type="html"><![CDATA[Cross-resolution image alignment is a key problem in multiscale gigapixel
photography, which requires to estimate homography matrix using images with
large resolution gap. Existing deep homography methods concatenate the input
images or features, neglecting the explicit formulation of correspondences
between them, which leads to degraded accuracy in cross-resolution challenges.
In this paper, we consider the cross-resolution homography estimation as a
multimodal problem, and propose a local transformer network embedded within a
multiscale structure to explicitly learn correspondences between the multimodal
inputs, namely, input images with different resolutions. The proposed local
transformer adopts a local attention map specifically for each position in the
feature. By combining the local transformer with the multiscale structure, the
network is able to capture long-short range correspondences efficiently and
accurately. Experiments on both the MS-COCO dataset and the real-captured
cross-resolution dataset show that the proposed network outperforms existing
state-of-the-art feature-based and deep-learning-based homography estimation
methods, and is able to accurately align images under $10\times$ resolution
gap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1"&gt;Ruizhi Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gaochang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuemei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Ying Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yebin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation. (arXiv:2106.04563v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04563</id>
        <link href="http://arxiv.org/abs/2106.04563"/>
        <updated>2021-06-15T22:41:24.981Z</updated>
        <summary type="html"><![CDATA[While deep and large pre-trained models are the state-of-the-art for various
natural language processing tasks, their huge size poses significant challenges
for practical uses in resource constrained settings. Recent works in knowledge
distillation propose task-agnostic as well as task-specific methods to compress
these models, with task-specific ones often yielding higher compression rate.
In this work, we develop a new task-agnostic distillation framework
XtremeDistilTransformers that leverages the advantage of task-specific methods
for learning a small universal model that can be applied to arbitrary tasks and
languages. To this end, we study the transferability of several source tasks,
augmentation resources and model architecture for distillation. We evaluate our
model performance on multiple tasks, including the General Language
Understanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and
a massive multi-lingual NER dataset with 41 languages. We release three
distilled task-agnostic checkpoints with 13MM, 22MM and 33MM parameters
obtaining SOTA performance in several tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Subhabrata Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality. (arXiv:2106.04102v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04102</id>
        <link href="http://arxiv.org/abs/2106.04102"/>
        <updated>2021-06-15T22:41:24.969Z</updated>
        <summary type="html"><![CDATA[We release a new benchmark for lexical substitution, the task of finding
appropriate substitutes for a target word in a context. To assist humans with
writing, lexical substitution systems can suggest words that humans cannot
easily think of. However, existing benchmarks depend on human recall as the
only source of data, and therefore lack coverage of the substitutes that would
be most helpful to humans. Furthermore, annotators often provide substitutes of
low quality, which are not actually appropriate in the given context. We
collect higher-coverage and higher-quality data by framing lexical substitution
as a classification problem, guided by the intuition that it is easier for
humans to judge the appropriateness of candidate substitutes than conjure them
from memory. To this end, we use a context-free thesaurus to produce candidates
and rely on human judgement to determine contextual appropriateness. Compared
to the previous largest benchmark, our Swords benchmark has 4.1x more
substitutes per target word for the same level of quality, and its substitutes
are 1.5x more appropriate (based on human judgement) for the same number of
substitutes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mina Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1"&gt;Chris Donahue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Robin Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyabor_A/0/1/0/all/0/1"&gt;Alexander Iyabor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Meta-Feature Selection for the Algorithm Recommendation Problem. (arXiv:2106.03954v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03954</id>
        <link href="http://arxiv.org/abs/2106.03954"/>
        <updated>2021-06-15T22:41:24.942Z</updated>
        <summary type="html"><![CDATA[With the popularity of Machine Learning (ML) solutions, algorithms and data
have been released faster than the capacity of processing them. In this
context, the problem of Algorithm Recommendation (AR) is receiving a
significant deal of attention recently. This problem has been addressed in the
literature as a learning task, often as a Meta-Learning problem where the aim
is to recommend the best alternative for a specific dataset. For such, datasets
encoded by meta-features are explored by ML algorithms that try to learn the
mapping between meta-representations and the best technique to be used. One of
the challenges for the successful use of ML is to define which features are the
most valuable for a specific dataset since several meta-features can be used,
which increases the meta-feature dimension. This paper presents an empirical
analysis of Feature Selection and Feature Extraction in the meta-level for the
AR problem. The present study was focused on three criteria: predictive
performance, dimensionality reduction, and pipeline runtime. As we verified,
applying Dimensionality Reduction (DR) methods did not improve predictive
performances in general. However, DR solutions reduced about 80% of the
meta-features, obtaining pretty much the same performance as the original setup
but with lower runtimes. The only exception was PCA, which presented about the
same runtime as the original meta-features. Experimental results also showed
that various datasets have many non-informative meta-features and that it is
possible to obtain high predictive performance using around 20% of the original
meta-features. Therefore, due to their natural trend for high dimensionality,
DR methods should be used for Meta-Feature Selection and Meta-Feature
Extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1"&gt;Geand Trindade Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1"&gt;Moises Rocha dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1"&gt;Andre Carlos Ponce de Leon Ferreira de Carvalho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We Know What You Want: An Advertising Strategy Recommender System for Online Advertising. (arXiv:2105.14188v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14188</id>
        <link href="http://arxiv.org/abs/2105.14188"/>
        <updated>2021-06-15T22:41:24.921Z</updated>
        <summary type="html"><![CDATA[Advertising expenditures have become the major source of revenue for
e-commerce platforms. Providing good advertising experiences for advertisers by
reducing their costs of trial and error in discovering the optimal advertising
strategies is crucial for the long-term prosperity of online advertising. To
achieve this goal, the advertising platform needs to identify the advertiser's
optimization objectives, and then recommend the corresponding strategies to
fulfill the objectives. In this work, we first deploy a prototype of strategy
recommender system on Taobao display advertising platform, which indeed
increases the advertisers' performance and the platform's revenue, indicating
the effectiveness of strategy recommendation for online advertising. We further
augment this prototype system by explicitly learning the advertisers'
preferences over various advertising performance indicators and then
optimization objectives through their adoptions of different recommending
advertising strategies. We use contextual bandit algorithms to efficiently
learn the advertisers' preferences and maximize the recommendation adoption,
simultaneously. Simulation experiments based on Taobao online bidding data show
that the designed algorithms can effectively optimize the strategy adoption
rate of advertisers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Liyi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Junqi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhenzhe Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhiye Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1"&gt;Zhizhuang Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Fei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1"&gt;Lvyin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuning Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets. (arXiv:2106.04476v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04476</id>
        <link href="http://arxiv.org/abs/2106.04476"/>
        <updated>2021-06-15T22:41:24.890Z</updated>
        <summary type="html"><![CDATA[Semantic parsers map natural language utterances to meaning representations.
The lack of a single standard for meaning representations led to the creation
of a plethora of semantic parsing datasets. To unify different datasets and
train a single model for them, we investigate the use of Multi-Task Learning
(MTL) architectures. We experiment with five datasets (Geoquery, NLMaps, TOP,
Overnight, AMR). We find that an MTL architecture that shares the entire
network across datasets yields competitive or better parsing accuracies than
the single-task baselines, while reducing the total number of parameters by
68%. We further provide evidence that MTL has also better compositional
generalization than single-task models. We also present a comparison of task
sampling methods and propose a competitive alternative to widespread
proportional sampling strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Damonte_M/0/1/0/all/0/1"&gt;Marco Damonte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monti_E/0/1/0/all/0/1"&gt;Emilio Monti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Transferable Learning: A New Approach for Model Verification and Authorization. (arXiv:2106.06916v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06916</id>
        <link href="http://arxiv.org/abs/2106.06916"/>
        <updated>2021-06-15T22:07:49.359Z</updated>
        <summary type="html"><![CDATA[As Artificial Intelligence as a Service gains popularity, protecting
well-trained models as intellectual property is becoming increasingly
important. Generally speaking, there are two common protection methods:
ownership verification and usage authorization. In this paper, we propose
Non-Transferable Learning (NTL), a novel approach that captures the exclusive
data representation in the learned model and restricts the model generalization
ability to certain domains. This approach provides effective solutions to both
model verification and authorization. For ownership verification, watermarking
techniques are commonly used but are often vulnerable to sophisticated
watermark removal methods. Our NTL-based model verification approach instead
provides robust resistance to state-of-the-art watermark removal methods, as
shown in extensive experiments for four of such methods over the digits,
CIFAR10 & STL10, and VisDA datasets. For usage authorization, prior solutions
focus on authorizing specific users to use the model, but authorized users can
still apply the model to any data without restriction. Our NTL-based
authorization approach instead provides data-centric usage protection by
significantly degrading the performance of usage on unauthorized data. Its
effectiveness is also shown through experiments on a variety of datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lixu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shichao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruiqi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qi Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LE-NAS: Learning-based Ensenble with NAS for Dose Prediction. (arXiv:2106.06733v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.06733</id>
        <link href="http://arxiv.org/abs/2106.06733"/>
        <updated>2021-06-15T22:07:49.342Z</updated>
        <summary type="html"><![CDATA[Radiation therapy treatment planning is a complex process, as the target dose
prescription and normal tissue sparing are conflicting objectives. Automated
and accurate dose prediction for radiation therapy planning is in high demand.
In this study, we propose a novel learning-based ensemble approach, named
LE-NAS, which integrates neural architecture search (NAS) with knowledge
distillation for 3D radiotherapy dose prediction. Specifically, the prediction
network first exhaustively searches each block from enormous architecture
space. Then, multiple architectures are selected with promising performance and
diversity. To reduce the inference time, we adopt the teacher-student paradigm
by treating the combination of diverse outputs from multiple searched networks
as supervisions to guide the student network training. In addition, we apply
adversarial learning to optimize the student network to recover the knowledge
in teacher networks. To the best of our knowledge, we are the first to
investigate the combination of NAS and knowledge distillation. The proposed
method has been evaluated on the public OpenKBP dataset, and experimental
results demonstrate the effectiveness of our method and its superior
performance to the state-of-the-art method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yanfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingguang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guocai Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Subject Domain Adaptation for Multi-Frame EEG Images. (arXiv:2106.06769v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06769</id>
        <link href="http://arxiv.org/abs/2106.06769"/>
        <updated>2021-06-15T22:07:49.270Z</updated>
        <summary type="html"><![CDATA[Working memory (WM) is a basic part of human cognition, which plays an
important role in the study of human cognitive load. Among various brain
imaging techniques, electroencephalography has shown its advantage on easy
access and reliability. However, one of the critical challenges is that
individual difference may cause the ineffective results, especially when the
established model meets an unfamiliar subject. In this work, we propose a
cross-subject deep adaptation model with spatial attention (CS-DASA) to
generalize the workload classifications across subjects. First, we transform
time-series EEG data into multi-frame EEG images incorporating more
spatio-temporal information. First, the subject-shared module in CS-DASA
receives multi-frame EEG image data from both source and target subjects and
learns the common feature representations. Then, in subject-specific module,
the maximum mean discrepancy is implemented to measure the domain distribution
divergence in a reproducing kernel Hilbert space, which can add an effective
penalty loss for domain adaptation. Additionally, the subject-to-subject
spatial attention mechanism is employed to focus on the most discriminative
spatial feature in EEG image data. Experiments conducted on a public WM EEG
dataset containing 13 subjects show that the proposed model is capable of
achieve better performance than existing state-of-the art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Junfu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Subject Domain Adaptation for Multi-Frame EEG Images. (arXiv:2106.06769v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06769</id>
        <link href="http://arxiv.org/abs/2106.06769"/>
        <updated>2021-06-15T22:07:49.048Z</updated>
        <summary type="html"><![CDATA[Working memory (WM) is a basic part of human cognition, which plays an
important role in the study of human cognitive load. Among various brain
imaging techniques, electroencephalography has shown its advantage on easy
access and reliability. However, one of the critical challenges is that
individual difference may cause the ineffective results, especially when the
established model meets an unfamiliar subject. In this work, we propose a
cross-subject deep adaptation model with spatial attention (CS-DASA) to
generalize the workload classifications across subjects. First, we transform
time-series EEG data into multi-frame EEG images incorporating more
spatio-temporal information. First, the subject-shared module in CS-DASA
receives multi-frame EEG image data from both source and target subjects and
learns the common feature representations. Then, in subject-specific module,
the maximum mean discrepancy is implemented to measure the domain distribution
divergence in a reproducing kernel Hilbert space, which can add an effective
penalty loss for domain adaptation. Additionally, the subject-to-subject
spatial attention mechanism is employed to focus on the most discriminative
spatial feature in EEG image data. Experiments conducted on a public WM EEG
dataset containing 13 subjects show that the proposed model is capable of
achieve better performance than existing state-of-the art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Junfu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Reversible Steganography: Principles and Insights. (arXiv:2106.06924v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.06924</id>
        <link href="http://arxiv.org/abs/2106.06924"/>
        <updated>2021-06-15T22:07:48.721Z</updated>
        <summary type="html"><![CDATA[Deep-learning\textendash{centric} reversible steganography has emerged as a
promising research paradigm. A direct way of applying deep learning to
reversible steganography is to construct a pair of encoder and decoder, whose
parameters are trained jointly, thereby learning the steganographic system as a
whole. This end-to-end framework, however, falls short of the reversibility
requirement because it is difficult for this kind of monolithic system, as a
black box, to create or duplicate intricate reversible mechanisms. In response
to this issue, a recent approach is to carve up the steganographic system and
work on modules independently. In particular, neural networks are deployed in
an analytics module to learn the data distribution, while an established
mechanism is called upon to handle the remaining tasks. In this paper, we
investigate the modular framework and deploy deep neural networks in a
reversible steganographic scheme referred to as prediction-error modulation, in
which an analytics module serves the purpose of pixel intensity prediction. The
primary focus of this study is on deep-learning\textendash{based} context-aware
pixel intensity prediction. We address the unsolved issues reported in related
literature, including the impact of pixel initialisation on prediction accuracy
and the influence of uncertainty propagation in dual-layer embedding.
Furthermore, we establish a connection between context-aware pixel intensity
prediction and low-level computer vision and analyse the performance of several
advanced neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Ching-Chun Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sisheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1"&gt;Isao Echizen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1"&gt;Victor Sanchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chang-Tsun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Neural Networks for Virtual Flow Metering: An Empirical Study. (arXiv:2102.01391v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01391</id>
        <link href="http://arxiv.org/abs/2102.01391"/>
        <updated>2021-06-15T01:45:21.535Z</updated>
        <summary type="html"><![CDATA[Recent works have presented promising results from the application of machine
learning (ML) to the modeling of flow rates in oil and gas wells. Encouraging
results and advantageous properties of ML models, such as computationally cheap
evaluation and ease of calibration to new data, have sparked optimism for the
development of data-driven virtual flow meters (VFMs). Data-driven VFMs are
developed in the small data regime, where it is important to question the
uncertainty and robustness of models. The modeling of uncertainty may help to
build trust in models, which is a prerequisite for industrial applications. The
contribution of this paper is the introduction of a probabilistic VFM based on
Bayesian neural networks. Uncertainty in the model and measurements is
described, and the paper shows how to perform approximate Bayesian inference
using variational inference. The method is studied by modeling on a large and
heterogeneous dataset, consisting of 60 wells across five different oil and gas
assets. The predictive performance is analyzed on historical and future test
data, where an average error of 4-6% and 8-13% is achieved for the 50% best
performing models, respectively. Variational inference appears to provide more
robust predictions than the reference approach on future data. Prediction
performance and uncertainty calibration is explored in detail and discussed in
light of four data challenges. The findings motivate the development of
alternative strategies to improve the robustness of data-driven VFMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grimstad_B/0/1/0/all/0/1"&gt;Bjarne Grimstad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hotvedt_M/0/1/0/all/0/1"&gt;Mathilde Hotvedt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandnes_A/0/1/0/all/0/1"&gt;Anders T. Sandnes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolbjornsen_O/0/1/0/all/0/1"&gt;Odd Kolbj&amp;#xf8;rnsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imsland_L/0/1/0/all/0/1"&gt;Lars S. Imsland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization. (arXiv:2006.16205v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16205</id>
        <link href="http://arxiv.org/abs/2006.16205"/>
        <updated>2021-06-15T01:45:21.468Z</updated>
        <summary type="html"><![CDATA[We focus on prediction problems with structured outputs that are subject to
output validity constraints, e.g. pseudocode-to-code translation where the code
must compile. While labeled input-output pairs are expensive to obtain,
"unlabeled" outputs, i.e. outputs without corresponding inputs, are freely
available (e.g. code on GitHub) and provide information about output validity.
Pre-training captures this structure by training a denoiser to denoise
corrupted versions of unlabeled outputs. We first show that standard
fine-tuning after pre-training destroys some of this structure. We then propose
composed fine-tuning, which trains a predictor composed with the pre-trained
denoiser. Importantly, the denoiser is fixed to preserve output structure. Like
standard fine-tuning, the predictor is also initialized with the pre-trained
denoiser. We prove for two-layer ReLU networks that composed fine-tuning
significantly reduces the complexity of the predictor, thus improving
generalization. Empirically, we show that composed fine-tuning improves over
standard fine-tuning on two pseudocode-to-code translation datasets (3% and 6%
relative). The improvement is magnified on out-of-distribution (OOD) examples
(4% and 25% relative), suggesting that reducing predictor complexity improves
OOD extrapolation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Sang Michael Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Singular Dynamic Mode Decompositions. (arXiv:2106.02639v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02639</id>
        <link href="http://arxiv.org/abs/2106.02639"/>
        <updated>2021-06-15T01:45:21.438Z</updated>
        <summary type="html"><![CDATA[This manuscript is aimed at addressing several long standing limitations of
dynamic mode decompositions in the application of Koopman analysis. Principle
among these limitations are the convergence of associated Dynamic Mode
Decomposition algorithms and the existence of Koopman modes. To address these
limitations, two major modifications are made, where Koopman operators are
removed from the analysis in light of Liouville operators (known as Koopman
generators in special cases), and these operators are shown to be compact for
certain pairs of Hilbert spaces selected separately as the domain and range of
the operator. While eigenfunctions are discarded in the general analysis, a
viable reconstruction algorithm is still demonstrated, and the sacrifice of
eigenfunctions realizes the theoretical goals of DMD analysis that have yet to
be achieved in other contexts. However, in the case where the domain is
embedded in the range, an eigenfunction approach is still achievable, where a
more typical DMD routine is established, but that leverages a finite rank
representation that converges in norm. The manuscript concludes with the
description of two Dynamic Mode Decomposition algorithms that converges when a
dense collection of occupation kernels, arising from the data, are leveraged in
the analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rosenfeld_J/0/1/0/all/0/1"&gt;Joel A. Rosenfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kamalapurkar_R/0/1/0/all/0/1"&gt;Rushikesh Kamalapurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing the Gap Between Actor-Critic and Policy Gradient. (arXiv:2106.06932v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.06932</id>
        <link href="http://arxiv.org/abs/2106.06932"/>
        <updated>2021-06-15T01:45:21.389Z</updated>
        <summary type="html"><![CDATA[Actor-critic (AC) methods are ubiquitous in reinforcement learning. Although
it is understood that AC methods are closely related to policy gradient (PG),
their precise connection has not been fully characterized previously. In this
paper, we explain the gap between AC and PG methods by identifying the exact
adjustment to the AC objective/gradient that recovers the true policy gradient
of the cumulative reward objective (PG). Furthermore, by viewing the AC method
as a two-player Stackelberg game between the actor and critic, we show that the
Stackelberg policy gradient can be recovered as a special case of our more
general analysis. Based on these results, we develop practical algorithms,
Residual Actor-Critic and Stackelberg Actor-Critic, for estimating the
correction between AC and PG and use these to modify the standard AC algorithm.
Experiments on popular tabular and continuous environments show the proposed
corrections can improve both the sample efficiency and final performance of
existing AC methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Junfeng Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Saurabh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gummadi_R/0/1/0/all/0/1"&gt;Ramki Gummadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1"&gt;Dale Schuurmans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hybrid Variance-Reduced Method for Decentralized Stochastic Non-Convex Optimization. (arXiv:2102.06752v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06752</id>
        <link href="http://arxiv.org/abs/2102.06752"/>
        <updated>2021-06-15T01:45:21.382Z</updated>
        <summary type="html"><![CDATA[This paper considers decentralized stochastic optimization over a network of
$n$ nodes, where each node possesses a smooth non-convex local cost function
and the goal of the networked nodes is to find an $\epsilon$-accurate
first-order stationary point of the sum of the local costs. We focus on an
online setting, where each node accesses its local cost only by means of a
stochastic first-order oracle that returns a noisy version of the exact
gradient. In this context, we propose a novel single-loop decentralized hybrid
variance-reduced stochastic gradient method, called GT-HSGD, that outperforms
the existing approaches in terms of both the oracle complexity and practical
implementation. The GT-HSGD algorithm implements specialized local hybrid
stochastic gradient estimators that are fused over the network to track the
global gradient. Remarkably, GT-HSGD achieves a network topology-independent
oracle complexity of $O(n^{-1}\epsilon^{-3})$ when the required error tolerance
$\epsilon$ is small enough, leading to a linear speedup with respect to the
centralized optimal online variance-reduced approaches that operate on a single
node. Numerical experiments are provided to illustrate our main technical
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Xin_R/0/1/0/all/0/1"&gt;Ran Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Khan_U/0/1/0/all/0/1"&gt;Usman A. Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kar_S/0/1/0/all/0/1"&gt;Soummya Kar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection. (arXiv:2104.09261v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09261</id>
        <link href="http://arxiv.org/abs/2104.09261"/>
        <updated>2021-06-15T01:45:21.376Z</updated>
        <summary type="html"><![CDATA[The existence of multiple datasets for sarcasm detection prompts us to apply
transfer learning to exploit their commonality. The adversarial neural transfer
(ANT) framework utilizes multiple loss terms that encourage the source-domain
and the target-domain feature distributions to be similar while optimizing for
domain-specific performance. However, these objectives may be in conflict,
which can lead to optimization difficulties and sometimes diminished transfer.
We propose a generalized latent optimization strategy that allows different
losses to accommodate each other and improves training dynamics. The proposed
method outperforms transfer learning and meta-learning baselines. In
particular, we achieve 10.02% absolute performance gain over the previous state
of the art on the iSarcasm dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Barlow Twins: Self-Supervised Learning via Redundancy Reduction. (arXiv:2103.03230v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03230</id>
        <link href="http://arxiv.org/abs/2103.03230"/>
        <updated>2021-06-15T01:45:21.369Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (SSL) is rapidly closing the gap with supervised
methods on large computer vision benchmarks. A successful approach to SSL is to
learn embeddings which are invariant to distortions of the input sample.
However, a recurring issue with this approach is the existence of trivial
constant solutions. Most current methods avoid such solutions by careful
implementation details. We propose an objective function that naturally avoids
collapse by measuring the cross-correlation matrix between the outputs of two
identical networks fed with distorted versions of a sample, and making it as
close to the identity matrix as possible. This causes the embedding vectors of
distorted versions of a sample to be similar, while minimizing the redundancy
between the components of these vectors. The method is called Barlow Twins,
owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a
pair of identical networks. Barlow Twins does not require large batches nor
asymmetry between the network twins such as a predictor network, gradient
stopping, or a moving average on the weight updates. Intriguingly it benefits
from very high-dimensional output vectors. Barlow Twins outperforms previous
methods on ImageNet for semi-supervised classification in the low-data regime,
and is on par with current state of the art for ImageNet classification with a
linear classifier head, and for transfer tasks of classification and object
detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zbontar_J/0/1/0/all/0/1"&gt;Jure Zbontar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1"&gt;Li Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1"&gt;Ishan Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1"&gt;Yann LeCun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deny_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Deny&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LogME: Practical Assessment of Pre-trained Models for Transfer Learning. (arXiv:2102.11005v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11005</id>
        <link href="http://arxiv.org/abs/2102.11005"/>
        <updated>2021-06-15T01:45:21.320Z</updated>
        <summary type="html"><![CDATA[This paper studies task adaptive pre-trained model selection, an
underexplored problem of assessing pre-trained models for the target task and
select best ones from the model zoo \emph{without fine-tuning}. A few pilot
works addressed the problem in transferring supervised pre-trained models to
classification tasks, but they cannot handle emerging unsupervised pre-trained
models or regression tasks. In pursuit of a practical assessment method, we
propose to estimate the maximum value of label evidence given features
extracted by pre-trained models. Unlike the maximum likelihood, the maximum
evidence is \emph{immune to over-fitting}, while its expensive computation can
be dramatically reduced by our carefully designed algorithm. The Logarithm of
Maximum Evidence (LogME) can be used to assess pre-trained models for transfer
learning: a pre-trained model with a high LogME value is likely to have good
transfer performance. LogME is \emph{fast, accurate, and general},
characterizing itself as the first practical method for assessing pre-trained
models. Compared with brute-force fine-tuning, LogME brings at most
$3000\times$ speedup in wall-clock time and requires only $1\%$ memory
footprint. It outperforms prior methods by a large margin in their setting and
is applicable to new settings. It is general enough for diverse pre-trained
models (supervised pre-trained and unsupervised pre-trained), downstream tasks
(classification and regression), and modalities (vision and language). Code is
available at this repository:
\href{https://github.com/thuml/LogME}{https://github.com/thuml/LogME}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1"&gt;Kaichao You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time Series Classification via Topological Data Analysis. (arXiv:2102.01956v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01956</id>
        <link href="http://arxiv.org/abs/2102.01956"/>
        <updated>2021-06-15T01:45:21.302Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop topological data analysis methods for
classification tasks on univariate time series. As an application, we perform
binary and ternary classification tasks on two public datasets that consist of
physiological signals collected under stress and non-stress conditions. We
accomplish our goal by using persistent homology to engineer stable topological
features after we use a time delay embedding of the signals and perform a
subwindowing instead of using windows of fixed length. The combination of
methods we use can be applied to any univariate time series and in this
application allows us to reduce noise and use long window sizes without
incurring an extra computational cost. We then use machine learning models on
the features we algorithmically engineered to obtain higher accuracies with
fewer features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Karan_A/0/1/0/all/0/1"&gt;Alperen Karan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kaygun_A/0/1/0/all/0/1"&gt;Atabey Kaygun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability. (arXiv:2104.01408v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01408</id>
        <link href="http://arxiv.org/abs/2104.01408"/>
        <updated>2021-06-15T01:45:21.295Z</updated>
        <summary type="html"><![CDATA[Emotional text-to-speech synthesis (ETTS) has seen much progress in recent
years. However, the generated voice is often not perceptually identifiable by
its intended emotion category. To address this problem, we propose a new
interactive training paradigm for ETTS, denoted as i-ETTS, which seeks to
directly improve the emotion discriminability by interacting with a speech
emotion recognition (SER) model. Moreover, we formulate an iterative training
strategy with reinforcement learning to ensure the quality of i-ETTS
optimization. Experimental results demonstrate that the proposed i-ETTS
outperforms the state-of-the-art baselines by rendering speech with more
accurate emotion style. To our best knowledge, this is the first study of
reinforcement learning in emotional text-to-speech synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Rui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sisman_B/0/1/0/all/0/1"&gt;Berrak Sisman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haizhou Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RobustBench: a standardized adversarial robustness benchmark. (arXiv:2010.09670v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09670</id>
        <link href="http://arxiv.org/abs/2010.09670"/>
        <updated>2021-06-15T01:45:21.289Z</updated>
        <summary type="html"><![CDATA[As a research community, we are still lacking a systematic understanding of
the progress on adversarial robustness, which often makes it hard to identify
the most promising ideas in training robust models. A key challenge in
benchmarking robustness is that its evaluation is often error-prone, leading to
overestimation of the true robustness of models. While adaptive attacks
designed for a particular defense are a potential solution, they have to be
highly customized for particular models, which makes it difficult to compare
different methods. Our goal is to instead establish a standardized benchmark of
adversarial robustness, which as accurately as possible reflects the robustness
of the considered models within a reasonable computational budget. To evaluate
the robustness of models for our benchmark, we consider AutoAttack, an ensemble
of white- and black-box attacks which was recently shown in a large-scale study
to improve almost all robustness evaluations compared to the original
publications. We also impose some restrictions on the admitted models to rule
out defenses that only make gradient-based attacks ineffective without
improving actual robustness. Our leaderboard, hosted at
https://robustbench.github.io/, contains evaluations of 90+ models and aims at
reflecting the current state of the art on a set of well-defined tasks in
$\ell_\infty$- and $\ell_2$-threat models and on common corruptions, with
possible extensions in the future. Additionally, we open-source the library
https://github.com/RobustBench/robustbench that provides unified access to 60+
robust models to facilitate their downstream applications. Finally, based on
the collected models, we analyze the impact of robustness on the performance on
distribution shifts, calibration, out-of-distribution detection, fairness,
privacy leakage, smoothness, and transferability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1"&gt;Francesco Croce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andriushchenko_M/0/1/0/all/0/1"&gt;Maksym Andriushchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1"&gt;Vikash Sehwag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Debenedetti_E/0/1/0/all/0/1"&gt;Edoardo Debenedetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1"&gt;Nicolas Flammarion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1"&gt;Mung Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1"&gt;Prateek Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1"&gt;Matthias Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EL-Attention: Memory Efficient Lossless Attention for Generation. (arXiv:2105.04779v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04779</id>
        <link href="http://arxiv.org/abs/2105.04779"/>
        <updated>2021-06-15T01:45:21.283Z</updated>
        <summary type="html"><![CDATA[Transformer model with multi-head attention requires caching intermediate
results for efficient inference in generation tasks. However, cache brings new
memory-related costs and prevents leveraging larger batch size for faster
speed. We propose memory-efficient lossless attention (called EL-attention) to
address this issue. It avoids heavy operations for building multi-head keys and
values, cache for them is not needed. EL-attention constructs an ensemble of
attention results by expanding query while keeping key and value shared. It
produces the same result as multi-head attention with less GPU memory and
faster inference speed. We conduct extensive experiments on Transformer, BART,
and GPT-2 for summarization and question generation tasks. The results show
EL-attention speeds up existing models by 1.6x to 5.3x without accuracy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiusheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1"&gt;Weizhen Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1"&gt;Nikhil Bhendawade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yeyun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruofei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The DEformer: An Order-Agnostic Distribution Estimating Transformer. (arXiv:2106.06989v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06989</id>
        <link href="http://arxiv.org/abs/2106.06989"/>
        <updated>2021-06-15T01:45:21.277Z</updated>
        <summary type="html"><![CDATA[Order-agnostic autoregressive distribution estimation (OADE), i.e.,
autoregressive distribution estimation where the features can occur in an
arbitrary order, is a challenging problem in generative machine learning. Prior
work on OADE has encoded feature identity (e.g., pixel location) by assigning
each feature to a distinct fixed position in an input vector. As a result,
architectures built for these inputs must strategically mask either the input
or model weights to learn the various conditional distributions necessary for
inferring the full joint distribution of the dataset in an order-agnostic way.
In this paper, we propose an alternative approach for encoding feature
identities, where each feature's identity is included alongside its value in
the input. This feature identity encoding strategy allows neural architectures
designed for sequential data to be applied to the OADE task without
modification. As a proof of concept, we show that a Transformer trained on this
input (which we refer to as "the DEformer", i.e., the distribution estimating
Transformer) can effectively model binarized-MNIST, approaching the average
negative log-likelihood of fixed order autoregressive distribution estimating
algorithms while still being entirely order-agnostic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alcorn_M/0/1/0/all/0/1"&gt;Michael A. Alcorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Formalism, Method and Open Issues for Zero-Shot Coordination. (arXiv:2106.06613v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.06613</id>
        <link href="http://arxiv.org/abs/2106.06613"/>
        <updated>2021-06-15T01:45:21.260Z</updated>
        <summary type="html"><![CDATA[In many coordination problems, independently reasoning humans are able to
discover mutually compatible policies. In contrast, independently trained
self-play policies are often mutually incompatible. Zero-shot coordination
(ZSC) has recently been proposed as a new frontier in multi-agent reinforcement
learning to address this fundamental issue. Prior work approaches the ZSC
problem by assuming players can agree on a shared learning algorithm but not on
labels for actions and observations, and proposes other-play as an optimal
solution. However, until now, this "label-free" problem has only been
informally defined. We formalize this setting as the label-free coordination
(LFC) problem by defining the label-free coordination game. We show that
other-play is not an optimal solution to the LFC problem as it fails to
consistently break ties between incompatible maximizers of the other-play
objective. We introduce an extension of the algorithm, other-play with
tie-breaking, and prove that it is optimal in the LFC problem and an
equilibrium in the LFC game. Since arbitrary tie-breaking is precisely what the
ZSC setting aims to prevent, we conclude that the LFC problem does not reflect
the aims of ZSC. To address this, we introduce an alternative informal
operationalization of ZSC as a starting point for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Treutlein_J/0/1/0/all/0/1"&gt;Johannes Treutlein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dennis_M/0/1/0/all/0/1"&gt;Michael Dennis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oesterheld_C/0/1/0/all/0/1"&gt;Caspar Oesterheld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1"&gt;Jakob Foerster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECG-TCN: Wearable Cardiac Arrhythmia Detection with a Temporal Convolutional Network. (arXiv:2103.13740v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13740</id>
        <link href="http://arxiv.org/abs/2103.13740"/>
        <updated>2021-06-15T01:45:21.253Z</updated>
        <summary type="html"><![CDATA[Personalized ubiquitous healthcare solutions require energy-efficient
wearable platforms that provide an accurate classification of bio-signals while
consuming low average power for long-term battery-operated use. Single lead
electrocardiogram (ECG) signals provide the ability to detect, classify, and
even predict cardiac arrhythmia. In this paper, we propose a novel temporal
convolutional network (TCN) that achieves high accuracy while still being
feasible for wearable platform use. Experimental results on the ECG5000 dataset
show that the TCN has a similar accuracy (94.2%) score as the state-of-the-art
(SoA) network while achieving an improvement of 16.5% in the balanced accuracy
score. This accurate classification is done with 27 times fewer parameters and
37 times less multiply-accumulate operations. We test our implementation on two
publicly available platforms, the STM32L475, which is based on ARM Cortex M4F,
and the GreenWaves Technologies GAP8 on the GAPuino board, based on 1+8 RISC-V
CV32E40P cores. Measurements show that the GAP8 implementation respects the
real-time constraints while consuming 0.10 mJ per inference. With 9.91
GMAC/s/W, it is 23.0 times more energy-efficient and 46.85 times faster than an
implementation on the ARM Cortex M4F (0.43 GMAC/s/W). Overall, we obtain 8.1%
higher accuracy while consuming 19.6 times less energy and being 35.1 times
faster compared to a previous SoA embedded implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ingolfsson_T/0/1/0/all/0/1"&gt;Thorir Mar Ingolfsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hersche_M/0/1/0/all/0/1"&gt;Michael Hersche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burrello_A/0/1/0/all/0/1"&gt;Alessio Burrello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavigelli_L/0/1/0/all/0/1"&gt;Lukas Cavigelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1"&gt;Luca Benini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Data Programming with Subset Selection. (arXiv:2008.09887v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09887</id>
        <link href="http://arxiv.org/abs/2008.09887"/>
        <updated>2021-06-15T01:45:21.247Z</updated>
        <summary type="html"><![CDATA[The paradigm of data programming, which uses weak supervision in the form of
rules/labelling functions, and semi-supervised learning, which augments small
amounts of labelled data with a large unlabelled dataset, have shown great
promise in several text classification scenarios. In this work, we argue that
by not using any labelled data, data programming based approaches can yield
sub-optimal performances, particularly when the labelling functions are noisy.
The first contribution of this work is an introduction of a framework, \model
which is a semi-supervised data programming paradigm that learns a \emph{joint
model} that effectively uses the rules/labelling functions along with
semi-supervised loss functions on the feature space. Next, we also study
\modelss which additionally does subset selection on top of the joint
semi-supervised data programming objective and \emph{selects} a set of examples
that can be used as the labelled set by \model. The goal of \modelss is to
ensure that the labelled data can \emph{complement} the labelling functions,
thereby benefiting from both data-programming as well as appropriately selected
data for human labelling. We demonstrate that by effectively combining
semi-supervision, data-programming, and subset selection paradigms, we
significantly outperform the current state-of-the-art on seven publicly
available datasets. \footnote{The source code is available at
\url{https://github.com/ayushbits/Semi-Supervised-LFs-Subset-Selection}}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1"&gt;Ayush Maheshwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_O/0/1/0/all/0/1"&gt;Oishik Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1"&gt;KrishnaTeja Killamsetty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable artificial intelligence for mechanics: physics-informing neural networks for constitutive models. (arXiv:2104.10683v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10683</id>
        <link href="http://arxiv.org/abs/2104.10683"/>
        <updated>2021-06-15T01:45:21.241Z</updated>
        <summary type="html"><![CDATA[(Artificial) neural networks have become increasingly popular in mechanics as
means to accelerate computations with model order reduction techniques and as
universal models for a wide variety of materials. However, the major
disadvantage of neural networks remains: their numerous parameters are
challenging to interpret and explain. Thus, neural networks are often labeled
as black boxes, and their results often elude human interpretation. In
mechanics, the new and active field of physics-informed neural networks
attempts to mitigate this disadvantage by designing deep neural networks on the
basis of mechanical knowledge. By using this a priori knowledge, deeper and
more complex neural networks became feasible, since the mechanical assumptions
could be explained. However, the internal reasoning and explanation of neural
network parameters remain mysterious.

Complementary to the physics-informed approach, we propose a first step
towards a physics-informing approach, which explains neural networks trained on
mechanical data a posteriori. This novel explainable artificial intelligence
approach aims at elucidating the black box of neural networks and their
high-dimensional representations. Therein, the principal component analysis
decorrelates the distributed representations in cell states of RNNs and allows
the comparison to known and fundamental functions. The novel approach is
supported by a systematic hyperparameter search strategy that identifies the
best neural network architectures and training parameters. The findings of
three case studies on fundamental constitutive models (hyperelasticity,
elastoplasticity, and viscoelasticity) imply that the proposed strategy can
help identify numerical and analytical closed-form solutions to characterize
new materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koeppe_A/0/1/0/all/0/1"&gt;Arnd Koeppe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bamer_F/0/1/0/all/0/1"&gt;Franz Bamer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Selzer_M/0/1/0/all/0/1"&gt;Michael Selzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nestler_B/0/1/0/all/0/1"&gt;Britta Nestler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markert_B/0/1/0/all/0/1"&gt;Bernd Markert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double-descent curves in neural networks: a new perspective using Gaussian processes. (arXiv:2102.07238v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07238</id>
        <link href="http://arxiv.org/abs/2102.07238"/>
        <updated>2021-06-15T01:45:21.235Z</updated>
        <summary type="html"><![CDATA[Double-descent curves in neural networks describe the phenomenon that the
generalisation error initially descends with increasing parameters, then grows
after reaching an optimal number of parameters which is less than the number of
data points, but then descends again in the overparameterised regime. Here we
use a neural network Gaussian process (NNGP) which maps exactly to a fully
connected network (FCN) in the infinite width limit, combined with techniques
from random matrix theory, to calculate this generalisation behaviour, with a
particular focus on the overparameterised regime. An advantage of our NNGP
approach is that the analytical calculations are easier to interpret. We argue
that neural network generalization performance improves in the
overparameterised regime precisely because that is where they converge to their
equivalent Gaussian process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Harzli_O/0/1/0/all/0/1"&gt;Ouns El Harzli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Valle_Perez_G/0/1/0/all/0/1"&gt;Guillermo Valle-P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Louis_A/0/1/0/all/0/1"&gt;Ard A. Louis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The zoo of Fairness metrics in Machine Learning. (arXiv:2106.00467v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00467</id>
        <link href="http://arxiv.org/abs/2106.00467"/>
        <updated>2021-06-15T01:45:21.218Z</updated>
        <summary type="html"><![CDATA[In recent years, the problem of addressing fairness in Machine Learning (ML)
and automatic decision-making has attracted a lot of attention in the
scientific communities dealing with Artificial Intelligence. A plethora of
different definitions of fairness in ML have been proposed, that consider
different notions of what is a "fair decision" in situations impacting
individuals in the population. The precise differences, implications and
"orthogonality" between these notions have not yet been fully analyzed in the
literature. In this work, we try to make some order out of this zoo of
definitions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castelnovo_A/0/1/0/all/0/1"&gt;Alessandro Castelnovo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crupi_R/0/1/0/all/0/1"&gt;Riccardo Crupi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greco_G/0/1/0/all/0/1"&gt;Greta Greco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Regoli_D/0/1/0/all/0/1"&gt;Daniele Regoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Musical Prosody-Driven Emotion Classification: Interpreting Vocalists Portrayal of Emotions Through Machine Learning. (arXiv:2106.02556v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02556</id>
        <link href="http://arxiv.org/abs/2106.02556"/>
        <updated>2021-06-15T01:45:21.211Z</updated>
        <summary type="html"><![CDATA[The task of classifying emotions within a musical track has received
widespread attention within the Music Information Retrieval (MIR) community.
Music emotion recognition has traditionally relied on the use of acoustic
features, verbal features, and metadata-based filtering. The role of musical
prosody remains under-explored despite several studies demonstrating a strong
connection between prosody and emotion. In this study, we restrict the input of
traditional machine learning algorithms to the features of musical prosody.
Furthermore, our proposed approach builds upon the prior by classifying
emotions under an expanded emotional taxonomy, using the Geneva Wheel of
Emotion. We utilize a methodology for individual data collection from
vocalists, and personal ground truth labeling by the artist themselves. We
found that traditional machine learning algorithms when limited to the features
of musical prosody (1) achieve high accuracies for a single singer, (2)
maintain high accuracy when the dataset is expanded to multiple singers, and
(3) achieve high accuracies when trained on a reduced subset of the total
features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farris_N/0/1/0/all/0/1"&gt;Nicholas Farris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Model_B/0/1/0/all/0/1"&gt;Brian Model&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savery_R/0/1/0/all/0/1"&gt;Richard Savery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinberg_G/0/1/0/all/0/1"&gt;Gil Weinberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending against Backdoors in Federated Learning with Robust Learning Rate. (arXiv:2007.03767v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03767</id>
        <link href="http://arxiv.org/abs/2007.03767"/>
        <updated>2021-06-15T01:45:21.202Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) allows a set of agents to collaboratively train a
model without sharing their potentially sensitive data. This makes FL suitable
for privacy-preserving applications. At the same time, FL is susceptible to
adversarial attacks due to decentralized and unvetted data. One important line
of attacks against FL is the backdoor attacks. In a backdoor attack, an
adversary tries to embed a backdoor functionality to the model during training
that can later be activated to cause a desired misclassification. To prevent
backdoor attacks, we propose a lightweight defense that requires minimal change
to the FL protocol. At a high level, our defense is based on carefully
adjusting the aggregation server's learning rate, per dimension and per round,
based on the sign information of agents' updates. We first conjecture the
necessary steps to carry a successful backdoor attack in FL setting, and then,
explicitly formulate the defense based on our conjecture. Through experiments,
we provide empirical evidence that supports our conjecture, and we test our
defense against backdoor attacks under different settings. We observe that
either backdoor is completely eliminated, or its accuracy is significantly
reduced. Overall, our experiments suggest that our defense significantly
outperforms some of the recently proposed defenses in the literature. We
achieve this by having minimal influence over the accuracy of the trained
models. In addition, we also provide convergence rate analysis for our proposed
scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozdayi_M/0/1/0/all/0/1"&gt;Mustafa Safa Ozdayi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kantarcioglu_M/0/1/0/all/0/1"&gt;Murat Kantarcioglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gel_Y/0/1/0/all/0/1"&gt;Yulia R. Gel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Analysis from the Fourier Integral Theorem. (arXiv:2106.06608v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.06608</id>
        <link href="http://arxiv.org/abs/2106.06608"/>
        <updated>2021-06-15T01:45:21.191Z</updated>
        <summary type="html"><![CDATA[Taking the Fourier integral theorem as our starting point, in this paper we
focus on natural Monte Carlo and fully nonparametric estimators of multivariate
distributions and conditional distribution functions. We do this without the
need for any estimated covariance matrix or dependence structure between
variables. These aspects arise immediately from the integral theorem. Being
able to model multivariate data sets using conditional distribution functions
we can study a number of problems, such as prediction for Markov processes,
estimation of mixing distribution functions which depend on covariates, and
general multivariate data. Estimators are explicit Monte Carlo based and
require no recursive or iterative algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1"&gt;Nhat Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Walker_S/0/1/0/all/0/1"&gt;Stephen G. Walker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Adaptive Label Smoothing for Predictive Churn. (arXiv:2102.05140v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05140</id>
        <link href="http://arxiv.org/abs/2102.05140"/>
        <updated>2021-06-15T01:45:21.185Z</updated>
        <summary type="html"><![CDATA[Training modern neural networks is an inherently noisy process that can lead
to high \emph{prediction churn} -- disagreements between re-trainings of the
same model due to factors such as randomization in the parameter initialization
and mini-batches -- even when the trained models all attain similar accuracies.
Such prediction churn can be very undesirable in practice. In this paper, we
present several baselines for reducing churn and show that training on soft
labels obtained by adaptively smoothing each example's label based on the
example's neighboring labels often outperforms the baselines on churn while
improving accuracy on a variety of benchmark classification tasks and model
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Heinrich Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PGDOT -- Perturbed Gradient Descent Adapted with Occupation Time. (arXiv:2005.04507v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.04507</id>
        <link href="http://arxiv.org/abs/2005.04507"/>
        <updated>2021-06-15T01:45:21.166Z</updated>
        <summary type="html"><![CDATA[This paper develops further the idea of perturbed gradient descent (PGD), by
adapting perturbation with the history of states via the notion of occupation
time. The proposed algorithm, perturbed gradient descent adapted with
occupation time (PGDOT), is shown to converge at least as fast as the PGD
algorithm and is guaranteed to avoid getting stuck at saddle points. The
analysis is corroborated by empirical studies, in which a mini-batch version of
PGDOT is shown to outperform alternatives such as mini-batch gradient descent,
Adam, AMSGrad, and RMSProp in training multilayer perceptrons (MLPs). In
particular, the mini-batch PGDOT manages to escape saddle points whereas these
alternatives fail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiequn Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tajrobehkar_M/0/1/0/all/0/1"&gt;Mahan Tajrobehkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tang_W/0/1/0/all/0/1"&gt;Wenpin Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controllable Generation from Pre-trained Language Models via Inverse Prompting. (arXiv:2103.10685v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10685</id>
        <link href="http://arxiv.org/abs/2103.10685"/>
        <updated>2021-06-15T01:45:21.159Z</updated>
        <summary type="html"><![CDATA[Large-scale pre-trained language models have demonstrated strong capabilities
of generating realistic text. However, it remains challenging to control the
generation results. Previous approaches such as prompting are far from
sufficient, which limits the usage of language models. To tackle this
challenge, we propose an innovative method, inverse prompting, to better
control text generation. The core idea of inverse prompting is to use generated
text to inversely predict the prompt during beam search, which enhances the
relevance between the prompt and the generated text and provides better
controllability. Empirically, we pre-train a large-scale Chinese language model
to perform a systematic study using human evaluation on the tasks of
open-domain poem generation and open-domain long-form question answering. Our
results show that our proposed method substantially outperforms the baselines
and that our generation quality is close to human performance on some of the
tasks.

Narrators can try our poem generation demo at
https://pretrain.aminer.cn/apps/poetry.html, while our QA demo can be found at
https://pretrain.aminer.cn/app/qa. For researchers, the code is provided in
https://github.com/THUDM/InversePrompting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1"&gt;Xu Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1"&gt;Da Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1"&gt;Qingyang Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Ming Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhilin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning Dynamics Forecasting Using Task Inference. (arXiv:2102.10271v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10271</id>
        <link href="http://arxiv.org/abs/2102.10271"/>
        <updated>2021-06-15T01:45:21.141Z</updated>
        <summary type="html"><![CDATA[Current deep learning models for dynamics forecasting struggle with
generalization. They can only forecast in a specific domain and fail when
applied to systems with different parameters, external forces, or boundary
conditions. We propose a model-based meta-learning method called DyAd which can
generalize across heterogeneous domains by partitioning them into different
tasks. DyAd has two parts: an encoder which infers the time-invariant hidden
features of the task with weak supervision, and a forecaster which learns the
shared dynamics of the entire domain. The encoder adapts and controls the
forecaster during inference using adaptive instance normalization and adaptive
padding. Theoretically, we prove that the generalization error of such
procedure is related to the task relatedness in the source domain, as well as
the domain differences between source and target. Experimentally, we
demonstrate that our model outperforms state-of-the-art approaches on both
turbulent flow and real-world ocean data forecasting tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1"&gt;Robin Walters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Rose Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning MDPs from Features: Predict-Then-Optimize for Sequential Decision Problems by Reinforcement Learning. (arXiv:2106.03279v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03279</id>
        <link href="http://arxiv.org/abs/2106.03279"/>
        <updated>2021-06-15T01:45:21.135Z</updated>
        <summary type="html"><![CDATA[In the predict-then-optimize framework, the objective is to train a
predictive model, mapping from environment features to parameters of an
optimization problem, which maximizes decision quality when the optimization is
subsequently solved. Recent work on decision-focused learning shows that
embedding the optimization problem in the training pipeline can improve
decision quality and help generalize better to unseen tasks compared to relying
on an intermediate loss function for evaluating prediction quality. We study
the predict-then-optimize framework in the context of sequential decision
problems (formulated as MDPs) that are solved via reinforcement learning. In
particular, we are given environment features and a set of trajectories from
training MDPs, which we use to train a predictive model that generalizes to
unseen test MDPs without trajectories. Two significant computational challenges
arise in applying decision-focused learning to MDPs: (i) large state and action
spaces make it infeasible for existing techniques to differentiate through MDP
problems, and (ii) the high-dimensional policy space, as parameterized by a
neural network, makes differentiating through a policy expensive. We resolve
the first challenge by sampling provably unbiased derivatives to approximate
and differentiate through optimality conditions, and the second challenge by
using a low-rank approximation to the high-dimensional sample-based
derivatives. We implement both Bellman--based and policy gradient--based
decision-focused learning on three different MDP problems with missing
parameters, and show that decision-focused learning performs better in
generalization to unseen tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1"&gt;Sanket Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haipeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perrault_A/0/1/0/all/0/1"&gt;Andrew Perrault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1"&gt;Finale Doshi-Velez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1"&gt;Milind Tambe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Ignorance in Individual-Level Causal-Effect Estimates under Hidden Confounding. (arXiv:2103.04850v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04850</id>
        <link href="http://arxiv.org/abs/2103.04850"/>
        <updated>2021-06-15T01:45:21.129Z</updated>
        <summary type="html"><![CDATA[We study the problem of learning conditional average treatment effects (CATE)
from high-dimensional, observational data with unobserved confounders.
Unobserved confounders introduce ignorance -- a level of unidentifiability --
about an individual's response to treatment by inducing bias in CATE estimates.
We present a new parametric interval estimator suited for high-dimensional
data, that estimates a range of possible CATE values when given a predefined
bound on the level of hidden confounding. Further, previous interval estimators
do not account for ignorance about the CATE associated with samples that may be
underrepresented in the original study, or samples that violate the overlap
assumption. Our interval estimator also incorporates model uncertainty so that
practitioners can be made aware of out-of-distribution data. We prove that our
estimator converges to tight bounds on CATE when there may be unobserved
confounding, and assess it using semi-synthetic, high-dimensional datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jesson_A/0/1/0/all/0/1"&gt;Andrew Jesson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1"&gt;S&amp;#xf6;ren Mindermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalit_U/0/1/0/all/0/1"&gt;Uri Shalit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDE-constrained Models with Neural Network Terms: Optimization and Global Convergence. (arXiv:2105.08633v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08633</id>
        <link href="http://arxiv.org/abs/2105.08633"/>
        <updated>2021-06-15T01:45:21.065Z</updated>
        <summary type="html"><![CDATA[Recent research has used deep learning to develop partial differential
equation (PDE) models in science and engineering. The functional form of the
PDE is determined by a neural network, and the neural network parameters are
calibrated to available data. Calibration of the embedded neural network can be
performed by optimizing over the PDE. Motivated by these applications, we
rigorously study the optimization of a class of linear elliptic PDEs with
neural network terms. The neural network parameters in the PDE are optimized
using gradient descent, where the gradient is evaluated using an adjoint PDE.
As the number of parameters become large, the PDE and adjoint PDE converge to a
non-local PDE system. Using this limit PDE system, we are able to prove
convergence of the neural network-PDE to a global minimum during the
optimization. The limit PDE system contains a non-local linear operator whose
eigenvalues are positive but become arbitrarily small. The lack of a spectral
gap for the eigenvalues poses the main challenge for the global convergence
proof. Careful analysis of the spectral decomposition of the coupled PDE and
adjoint PDE system is required. Finally, we use this adjoint method to train a
neural network model for an application in fluid mechanics, in which the neural
network functions as a closure model for the Reynolds-averaged Navier-Stokes
(RANS) equations. The RANS neural network model is trained on several datasets
for turbulent channel flow and is evaluated out-of-sample at different Reynolds
numbers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sirignano_J/0/1/0/all/0/1"&gt;Justin Sirignano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MacArt_J/0/1/0/all/0/1"&gt;Jonathan MacArt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spiliopoulos_K/0/1/0/all/0/1"&gt;Konstantinos Spiliopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integer Programming for Causal Structure Learning in the Presence of Latent Variables. (arXiv:2102.03129v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03129</id>
        <link href="http://arxiv.org/abs/2102.03129"/>
        <updated>2021-06-15T01:45:20.460Z</updated>
        <summary type="html"><![CDATA[The problem of finding an ancestral acyclic directed mixed graph (ADMG) that
represents the causal relationships between a set of variables is an important
area of research on causal inference. Most existing score-based structure
learning methods focus on learning directed acyclic graph (DAG) models without
latent variables. A number of score-based methods have recently been proposed
for the ADMG learning, yet they are heuristic in nature and do not guarantee an
optimal solution. We propose a novel exact score-based method that solves an
integer programming (IP) formulation and returns a score-maximizing ancestral
ADMG for a set of continuous variables that follow a multivariate Gaussian
distribution. We generalize the state-of-the-art IP model for DAG learning
problems and derive new classes of valid inequalities to formulate an IP model
for ADMG learning. Empirically, our model can be solved efficiently for
medium-sized problems and achieves better accuracy than state-of-the-art
score-based methods as well as benchmark constraint-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Rui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1"&gt;Sanjeeb Dash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1"&gt;Tian Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Randomized Stochastic Variance-Reduced Methods for Multi-Task Stochastic Bilevel Optimization. (arXiv:2105.02266v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02266</id>
        <link href="http://arxiv.org/abs/2105.02266"/>
        <updated>2021-06-15T01:45:20.453Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider non-convex stochastic bilevel optimization (SBO)
problems that have many applications in machine learning. Although numerous
studies have proposed stochastic algorithms for solving these problems, they
are limited in two perspectives: (i) their sample complexities are high, which
do not match the state-of-the-art result for non-convex stochastic
optimization; (ii) their algorithms are tailored to problems with only one
lower-level problem. When there are many lower-level problems, it could be
prohibitive to process all these lower-level problems at each iteration. To
address these limitations, this paper proposes fast randomized stochastic
algorithms for non-convex SBO problems. First, we present a stochastic method
for non-convex SBO with only one lower problem and establish its sample
complexity of $O(1/\epsilon^3)$ for finding an $\epsilon$-stationary point
under Lipschitz continuous conditions of stochastic oracles, matching the lower
bound for stochastic smooth non-convex optimization. Second, we present a
randomized stochastic method for non-convex SBO with $m>1$ lower level problems
(multi-task SBO) by processing a constant number of lower problems at each
iteration, and establish its sample complexity no worse than $O(m/\epsilon^3)$,
which could be a better complexity than that of simply processing all $m$ lower
problems at each iteration. Lastly, we establish even faster convergence
results for gradient-dominant functions. To the best of our knowledge, this is
the first work considering multi-task SBO and developing state-of-the-art
sample complexity results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zhishuai Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Quanqi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianbao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15082</id>
        <link href="http://arxiv.org/abs/2105.15082"/>
        <updated>2021-06-15T01:45:20.439Z</updated>
        <summary type="html"><![CDATA[Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has
become a trend in model scaling. Still it is a mystery how MoE layers bring
quality gains by leveraging the parameters with sparse activation. In this
work, we investigate several key factors in sparse expert models. We observe
that load imbalance may not be a significant problem affecting model quality,
contrary to the perspectives of recent studies, while the number of sparsely
activated experts $k$ and expert capacity $C$ in top-$k$ routing can
significantly make a difference in this context. Furthermore, we take a step
forward to propose a simple method called expert prototyping that splits
experts into different prototypes and applies $k$ top-$1$ routing. This
strategy improves the model quality but maintains constant computational costs,
and our further exploration on extremely large-scale models reflects that it is
more effective in training larger models. We push the model scale to over $1$
trillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in
comparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model
achieves substantial speedup in convergence over the same-size baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Le Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xianyan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Ang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiamang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Di Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Lin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Uncertainty in Deep Spatiotemporal Forecasting. (arXiv:2105.11982v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11982</id>
        <link href="http://arxiv.org/abs/2105.11982"/>
        <updated>2021-06-15T01:45:20.433Z</updated>
        <summary type="html"><![CDATA[Deep learning is gaining increasing popularity for spatiotemporal
forecasting. However, prior works have mostly focused on point estimates
without quantifying the uncertainty of the predictions. In high stakes domains,
being able to generate probabilistic forecasts with confidence intervals is
critical to risk assessment and decision making. Hence, a systematic study of
uncertainty quantification (UQ) methods for spatiotemporal forecasting is
missing in the community. In this paper, we describe two types of
spatiotemporal forecasting problems: regular grid-based and graph-based. Then
we analyze UQ methods from both the Bayesian and the frequentist point of view,
casting in a unified framework via statistical decision theory. Through
extensive experiments on real-world road network traffic, epidemics, and air
quality forecasting tasks, we reveal the statistical and computational
trade-offs for different UQ methods: Bayesian methods are typically more robust
in mean prediction, while confidence levels obtained from frequentist methods
provide more extensive coverage over data variations. Computationally, quantile
regression type methods are cheaper for a single confidence interval but
require re-training for different intervals. Sampling based methods generate
samples that can form multiple confidence intervals, albeit at a higher
computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongxia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Liyao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1"&gt;Xinyue Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chinazzi_M/0/1/0/all/0/1"&gt;Matteo Chinazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vespignani_A/0/1/0/all/0/1"&gt;Alessandro Vespignani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi-An Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Rose Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Globally-Robust Neural Networks. (arXiv:2102.08452v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08452</id>
        <link href="http://arxiv.org/abs/2102.08452"/>
        <updated>2021-06-15T01:45:20.416Z</updated>
        <summary type="html"><![CDATA[The threat of adversarial examples has motivated work on training certifiably
robust neural networks to facilitate efficient verification of local robustness
at inference time. We formalize a notion of global robustness, which captures
the operational properties of on-line local robustness certification while
yielding a natural learning objective for robust training. We show that
widely-used architectures can be easily adapted to this objective by
incorporating efficient global Lipschitz bounds into the network, yielding
certifiably-robust models by construction that achieve state-of-the-art
verifiable accuracy. Notably, this approach requires significantly less time
and memory than recent certifiable training methods, and leads to negligible
costs when certifying points on-line; for example, our evaluation shows that it
is possible to train a large robust Tiny-Imagenet model in a matter of hours.
Our models effectively leverage inexpensive global Lipschitz bounds for
real-time certification, despite prior suggestions that tighter local bounds
are needed for good performance; we posit this is possible because our models
are specifically trained to achieve tighter global bounds. Namely, we prove
that the maximum achievable verifiable accuracy for a given dataset is not
improved by using a local bound.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leino_K/0/1/0/all/0/1"&gt;Klas Leino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zifan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1"&gt;Matt Fredrikson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Foundations and modelling of dynamic networks using Dynamic Graph Neural Networks: A survey. (arXiv:2005.07496v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07496</id>
        <link href="http://arxiv.org/abs/2005.07496"/>
        <updated>2021-06-15T01:45:20.410Z</updated>
        <summary type="html"><![CDATA[Dynamic networks are used in a wide range of fields, including social network
analysis, recommender systems, and epidemiology. Representing complex networks
as structures changing over time allow network models to leverage not only
structural but also temporal patterns. However, as dynamic network literature
stems from diverse fields and makes use of inconsistent terminology, it is
challenging to navigate. Meanwhile, graph neural networks (GNNs) have gained a
lot of attention in recent years for their ability to perform well on a range
of network science tasks, such as link prediction and node classification.
Despite the popularity of graph neural networks and the proven benefits of
dynamic network models, there has been little focus on graph neural networks
for dynamic networks. To address the challenges resulting from the fact that
this research crosses diverse fields as well as to survey dynamic graph neural
networks, this work is split into two main parts. First, to address the
ambiguity of the dynamic network terminology we establish a foundation of
dynamic networks with consistent, detailed terminology and notation. Second, we
present a comprehensive survey of dynamic graph neural network models using the
proposed terminology]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Skarding_J/0/1/0/all/0/1"&gt;Joakim Skarding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gabrys_B/0/1/0/all/0/1"&gt;Bogdan Gabrys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musial_K/0/1/0/all/0/1"&gt;Katarzyna Musial&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An error analysis of generative adversarial networks for learning distributions. (arXiv:2105.13010v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13010</id>
        <link href="http://arxiv.org/abs/2105.13010"/>
        <updated>2021-06-15T01:45:20.404Z</updated>
        <summary type="html"><![CDATA[This paper studies how well generative adversarial networks (GANs) learn
probability distributions from finite samples. Our main results establish the
convergence rates of GANs under a collection of integral probability metrics
defined through H\"older classes, including the Wasserstein distance as a
special case. We also show that GANs are able to adaptively learn data
distributions with low-dimensional structures or have H\"older densities, when
the network architectures are chosen properly. In particular, for distributions
concentrated around a low-dimensional set, we show that the learning rates of
GANs do not depend on the high ambient dimension, but on the lower intrinsic
dimension. Our analysis is based on a new oracle inequality decomposing the
estimation error into the generator and discriminator approximation error and
the statistical error, which may be of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jian Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yuling Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yunfei Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GRAD-MATCH: Gradient Matching based Data Subset Selection for Efficient Deep Model Training. (arXiv:2103.00123v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00123</id>
        <link href="http://arxiv.org/abs/2103.00123"/>
        <updated>2021-06-15T01:45:20.397Z</updated>
        <summary type="html"><![CDATA[The great success of modern machine learning models on large datasets is
contingent on extensive computational resources with high financial and
environmental costs. One way to address this is by extracting subsets that
generalize on par with the full data. In this work, we propose a general
framework, GRAD-MATCH, which finds subsets that closely match the gradient of
the training or validation set. We find such subsets effectively using an
orthogonal matching pursuit algorithm. We show rigorous theoretical and
convergence guarantees of the proposed algorithm and, through our extensive
experiments on real-world datasets, show the effectiveness of our proposed
framework. We show that GRAD-MATCH significantly and consistently outperforms
several recent data-selection algorithms and achieves the best
accuracy-efficiency trade-off. GRAD-MATCH is available as a part of the CORDS
toolkit: \url{https://github.com/decile-team/cords}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1"&gt;Krishnateja Killamsetty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1"&gt;Durga Sivasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1"&gt;Abir De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Interaction-based Convolutional Neural Network (ICNN) Towards Better Understanding of COVID-19 X-ray Images. (arXiv:2106.06911v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06911</id>
        <link href="http://arxiv.org/abs/2106.06911"/>
        <updated>2021-06-15T01:45:20.380Z</updated>
        <summary type="html"><![CDATA[The field of Explainable Artificial Intelligence (XAI) aims to build
explainable and interpretable machine learning (or deep learning) methods
without sacrificing prediction performance. Convolutional Neural Networks
(CNNs) have been successful in making predictions, especially in image
classification. However, these famous deep learning models use tens of millions
of parameters based on a large number of pre-trained filters which have been
repurposed from previous data sets. We propose a novel Interaction-based
Convolutional Neural Network (ICNN) that does not make assumptions about the
relevance of local information. Instead, we use a model-free Influence Score
(I-score) to directly extract the influential information from images to form
important variable modules. We demonstrate that the proposed method produces
state-of-the-art prediction performance of 99.8% on a real-world data set
classifying COVID-19 Chest X-ray images without sacrificing the explanatory
power of the model. This proposed design can efficiently screen COVID-19
patients before human diagnosis, and will be the benchmark for addressing
future XAI problems in large-scale data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shaw-Hwa Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yiqiao Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Inference Representation: Learning Graph Positional Embeddings with Anchor Path Encoding. (arXiv:2105.03821v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03821</id>
        <link href="http://arxiv.org/abs/2105.03821"/>
        <updated>2021-06-15T01:45:20.374Z</updated>
        <summary type="html"><![CDATA[Learning node representations that incorporate information from graph
structure benefits wide range of tasks on graph. The majority of existing graph
neural networks (GNNs) have limited power in capturing position information for
a given node. The idea of positioning nodes with selected anchors has been
exploited, yet mainly relying on explicit labeling of distance information.
Here we propose Graph Inference Representation (GIR), an anchor based GNN model
encoding path information related to pre-selected anchors for each node.
Abilities to get position-aware embeddings are theoretically and experimentally
investigated on GIR and its core variants. Further, the complementarity between
GIRs and typical GNNs is demonstrated. We show that GIRs get outperformed
results in position-aware scenarios, and performances on typical GNNs could be
improved by fusing GIR embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yuheng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jinpeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;ChuXiong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jie Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Skew Orthogonal Convolutions. (arXiv:2105.11417v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11417</id>
        <link href="http://arxiv.org/abs/2105.11417"/>
        <updated>2021-06-15T01:45:20.367Z</updated>
        <summary type="html"><![CDATA[Training convolutional neural networks with a Lipschitz constraint under the
$l_{2}$ norm is useful for provable adversarial robustness, interpretable
gradients, stable training, etc. While 1-Lipschitz networks can be designed by
imposing a 1-Lipschitz constraint on each layer, training such networks
requires each layer to be gradient norm preserving (GNP) to prevent gradients
from vanishing. However, existing GNP convolutions suffer from slow training,
lead to significant reduction in accuracy and provide no guarantees on their
approximations. In this work, we propose a GNP convolution layer called Skew
Orthogonal Convolution (SOC) that uses the following mathematical property:
when a matrix is {\it Skew-Symmetric}, its exponential function is an {\it
orthogonal} matrix. To use this property, we first construct a convolution
filter whose Jacobian is Skew-Symmetric. Then, we use the Taylor series
expansion of the Jacobian exponential to construct the SOC layer that is
orthogonal. To efficiently implement SOC, we keep a finite number of terms from
the Taylor series and provide a provable guarantee on the approximation error.
Our experiments on CIFAR-10 and CIFAR-100 show that SOC allows us to train
provably Lipschitz, large convolutional neural networks significantly faster
than prior works while achieving significant improvements for both standard and
certified robust accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1"&gt;Sahil Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1"&gt;Soheil Feizi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fantastic Four: Differentiable Bounds on Singular Values of Convolution Layers. (arXiv:1911.10258v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.10258</id>
        <link href="http://arxiv.org/abs/1911.10258"/>
        <updated>2021-06-15T01:45:20.361Z</updated>
        <summary type="html"><![CDATA[In deep neural networks, the spectral norm of the Jacobian of a layer bounds
the factor by which the norm of a signal changes during forward/backward
propagation. Spectral norm regularizations have been shown to improve
generalization, robustness and optimization of deep learning methods. Existing
methods to compute the spectral norm of convolution layers either rely on
heuristics that are efficient in computation but lack guarantees or are
theoretically-sound but computationally expensive. In this work, we obtain the
best of both worlds by deriving {\it four} provable upper bounds on the
spectral norm of a standard 2D multi-channel convolution layer. These bounds
are differentiable and can be computed efficiently during training with
negligible overhead. One of these bounds is in fact the popular heuristic
method of Miyato et al. (multiplied by a constant factor depending on filter
sizes). Each of these four bounds can achieve the tightest gap depending on
convolution filters. Thus, we propose to use the minimum of these four bounds
as a tight, differentiable and efficient upper bound on the spectral norm of
convolution layers. We show that our spectral bound is an effective regularizer
and can be used to bound either the lipschitz constant or curvature values
(eigenvalues of the Hessian) of neural networks. Through experiments on MNIST
and CIFAR-10, we demonstrate the effectiveness of our spectral bound in
improving generalization and provable robustness of deep networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1"&gt;Sahil Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1"&gt;Soheil Feizi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Mitigating Accuracy Disparity in Regression. (arXiv:2102.12013v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12013</id>
        <link href="http://arxiv.org/abs/2102.12013"/>
        <updated>2021-06-15T01:45:20.354Z</updated>
        <summary type="html"><![CDATA[With the widespread deployment of large-scale prediction systems in
high-stakes domains, e.g., face recognition, criminal justice, etc., disparity
in prediction accuracy between different demographic subgroups has called for
fundamental understanding on the source of such disparity and algorithmic
intervention to mitigate it. In this paper, we study the accuracy disparity
problem in regression. To begin with, we first propose an error decomposition
theorem, which decomposes the accuracy disparity into the distance between
marginal label distributions and the distance between conditional
representations, to help explain why such accuracy disparity appears in
practice. Motivated by this error decomposition and the general idea of
distribution alignment with statistical distances, we then propose an algorithm
to reduce this disparity, and analyze its game-theoretic optima of the proposed
objective functions. To corroborate our theoretical findings, we also conduct
experiments on five benchmark datasets. The experimental results suggest that
our proposed algorithms can effectively mitigate accuracy disparity while
maintaining the predictive power of the regression models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chi_J/0/1/0/all/0/1"&gt;Jianfeng Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuan Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1"&gt;Geoffrey J. Gordon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sinkhorn Label Allocation: Semi-Supervised Classification via Annealed Self-Training. (arXiv:2102.08622v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08622</id>
        <link href="http://arxiv.org/abs/2102.08622"/>
        <updated>2021-06-15T01:45:20.347Z</updated>
        <summary type="html"><![CDATA[Self-training is a standard approach to semi-supervised learning where the
learner's own predictions on unlabeled data are used as supervision during
training. In this paper, we reinterpret this label assignment process as an
optimal transportation problem between examples and classes, wherein the cost
of assigning an example to a class is mediated by the current predictions of
the classifier. This formulation facilitates a practical annealing strategy for
label assignment and allows for the inclusion of prior knowledge on class
proportions via flexible upper bound constraints. The solutions to these
assignment problems can be efficiently approximated using Sinkhorn iteration,
thus enabling their use in the inner loop of standard stochastic optimization
algorithms. We demonstrate the effectiveness of our algorithm on the CIFAR-10,
CIFAR-100, and SVHN datasets in comparison with FixMatch, a state-of-the-art
self-training algorithm. Our code is available at
https://github.com/stanford-futuredata/sinkhorn-label-allocation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tai_K/0/1/0/all/0/1"&gt;Kai Sheng Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bailis_P/0/1/0/all/0/1"&gt;Peter Bailis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1"&gt;Gregory Valiant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Urysohn Forest for Aleatoric Uncertainty Quantification. (arXiv:2104.01714v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01714</id>
        <link href="http://arxiv.org/abs/2104.01714"/>
        <updated>2021-06-15T01:45:20.331Z</updated>
        <summary type="html"><![CDATA[The terms tree and forest are normally associated with an ensemble of
classifiers. In this article Urysohn tree is a regression model representing
multiple discrete Urysohn operators connected as a tree, where the inputs of
one operator are outputs of the others. This structure, referred as Urysohn
tree, is not completely new. One example of such tree is known for more than
half a century. It is Kolmogorov-Arnold representation. The authors of this
paper in their recently published research offered the new computational
technique for generating of Kolmogorov-Arnold representation as a deep machine
learning process. This article is two steps further into this research. First
is a Urysohn tree with multiple hidden layers which is generalization of
Kolmogorov-Arnold model and second is a boosting algorithm for building of the
forest of such trees for modeling of aleatoric uncertainty of the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polar_A/0/1/0/all/0/1"&gt;Andrew Polar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poluektov_M/0/1/0/all/0/1"&gt;Michael Poluektov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Acceleration via Fractal Learning Rate Schedules. (arXiv:2103.01338v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01338</id>
        <link href="http://arxiv.org/abs/2103.01338"/>
        <updated>2021-06-15T01:45:20.322Z</updated>
        <summary type="html"><![CDATA[In practical applications of iterative first-order optimization, the learning
rate schedule remains notoriously difficult to understand and expensive to
tune. We demonstrate the presence of these subtleties even in the innocuous
case when the objective is a convex quadratic. We reinterpret an iterative
algorithm from the numerical analysis literature as what we call the Chebyshev
learning rate schedule for accelerating vanilla gradient descent, and show that
the problem of mitigating instability leads to a fractal ordering of step
sizes. We provide some experiments to challenge conventional beliefs about
stable learning rates in deep learning: the fractal schedule enables training
to converge with locally unstable updates which make negative progress on the
objective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1"&gt;Naman Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1"&gt;Surbhi Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cyril Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detect the Interactions that Matter in Matter: Geometric Attention for Many-Body Systems. (arXiv:2106.02549v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02549</id>
        <link href="http://arxiv.org/abs/2106.02549"/>
        <updated>2021-06-15T01:45:20.317Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms are developing into a viable alternative to
convolutional layers as elementary building block of NNs. Their main advantage
is that they are not restricted to capture local dependencies in the input, but
can draw arbitrary connections. This unprecedented capability coincides with
the long-standing problem of modeling global atomic interactions in molecular
force fields and other many-body problems. In its original formulation,
however, attention is not applicable to the continuous domains in which the
atoms live. For this purpose we propose a variant to describe geometric
relations for arbitrary atomic configurations in Euclidean space that also
respects all relevant physical symmetries. We furthermore demonstrate, how the
successive application of our learned attention matrices effectively translates
the molecular geometry into a set of individual atomic contributions
on-the-fly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frank_T/0/1/0/all/0/1"&gt;Thorben Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chmiela_S/0/1/0/all/0/1"&gt;Stefan Chmiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesized Difference in Differences. (arXiv:2105.00455v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00455</id>
        <link href="http://arxiv.org/abs/2105.00455"/>
        <updated>2021-06-15T01:45:20.310Z</updated>
        <summary type="html"><![CDATA[We consider estimating the conditional average treatment effect for everyone
by eliminating confounding and selection bias. Unfortunately, randomized
clinical trials (RCTs) eliminate confounding but impose strict exclusion
criteria that prevent sampling of the entire clinical population. Observational
datasets are more inclusive but suffer from confounding. We therefore analyze
RCT and observational data simultaneously in order to extract the strengths of
each. Our solution builds upon Difference in Differences (DD), an algorithm
that eliminates confounding from observational data by comparing outcomes
before and after treatment administration. DD requires a parallel slopes
assumption that may not apply in practice when confounding shifts across time.
We instead propose Synthesized Difference in Differences (SDD) that infers the
correct (possibly non-parallel) slopes by linearly adjusting a conditional
version of DD using additional RCT data. The algorithm achieves state of the
art performance across multiple synthetic and real datasets even when the RCT
excludes the majority of patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Strobl_E/0/1/0/all/0/1"&gt;Eric V. Strobl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lasko_T/0/1/0/all/0/1"&gt;Thomas A. Lasko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Interaction-based Methodology Towards Explainable AI with Better Understanding of Pneumonia Chest X-ray Images. (arXiv:2104.12672v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12672</id>
        <link href="http://arxiv.org/abs/2104.12672"/>
        <updated>2021-06-15T01:45:20.304Z</updated>
        <summary type="html"><![CDATA[In the field of eXplainable AI (XAI), robust ``blackbox'' algorithms such as
Convolutional Neural Networks (CNNs) are known for making high prediction
performance. However, the ability to explain and interpret these algorithms
still require innovation in the understanding of influential and, more
importantly, explainable features that directly or indirectly impact the
performance of predictivity. A number of methods existing in literature focus
on visualization techniques but the concepts of explainability and
interpretability still require rigorous definition. In view of the above needs,
this paper proposes an interaction-based methodology -- Influence Score
(I-score) -- to screen out the noisy and non-informative variables in the
images hence it nourishes an environment with explainable and interpretable
features that are directly associated to feature predictivity. We apply the
proposed method on a real world application in Pneumonia Chest X-ray Image data
set and produced state-of-the-art results. We demonstrate how to apply the
proposed approach for more general big data problems by improving the
explainability and interpretability without sacrificing the prediction
performance. The contribution of this paper opens a novel angle that moves the
community closer to the future pipelines of XAI problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shaw-Hwa Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yiqiao Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Ensembling Contrastive Learning for Semi-Supervised Medical Image Segmentation. (arXiv:2105.12924v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12924</id>
        <link href="http://arxiv.org/abs/2105.12924"/>
        <updated>2021-06-15T01:45:20.286Z</updated>
        <summary type="html"><![CDATA[Deep learning has demonstrated significant improvements in medical image
segmentation using a sufficiently large amount of training data with manual
labels. Acquiring well-representative labels requires expert knowledge and
exhaustive labors. In this paper, we aim to boost the performance of
semi-supervised learning for medical image segmentation with limited labels
using a self-ensembling contrastive learning technique. To this end, we propose
to train an encoder-decoder network at image-level with small amounts of
labeled images, and more importantly, we learn latent representations directly
at feature-level by imposing contrastive loss on unlabeled images. This method
strengthens intra-class compactness and inter-class separability, so as to get
a better pixel classifier. Moreover, we devise a student encoder for online
learning and an exponential moving average version of it, called teacher
encoder, to improve the performance iteratively in a self-ensembling manner. To
construct contrastive samples with unlabeled images, two sampling strategies
that exploit structure similarity across medical images and utilize
pseudo-labels for construction, termed region-aware and anatomical-aware
contrastive sampling, are investigated. We conduct extensive experiments on an
MRI and a CT segmentation dataset and demonstrate that in a limited label
setting, the proposed method achieves state-of-the-art performance. Moreover,
the anatomical-aware strategy that prepares contrastive samples on-the-fly
using pseudo-labels realizes better contrastive regularization on feature
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1"&gt;Jinxi Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhuowei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenji Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1"&gt;Qing Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shaoting Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks. (arXiv:2103.03212v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03212</id>
        <link href="http://arxiv.org/abs/2103.03212"/>
        <updated>2021-06-15T01:45:20.280Z</updated>
        <summary type="html"><![CDATA[The pairwise interaction paradigm of graph machine learning has predominantly
governed the modelling of relational systems. However, graphs alone cannot
capture the multi-level interactions present in many complex systems and the
expressive power of such schemes was proven to be limited. To overcome these
limitations, we propose Message Passing Simplicial Networks (MPSNs), a class of
models that perform message passing on simplicial complexes (SCs). To
theoretically analyse the expressivity of our model we introduce a Simplicial
Weisfeiler-Lehman (SWL) colouring procedure for distinguishing non-isomorphic
SCs. We relate the power of SWL to the problem of distinguishing non-isomorphic
graphs and show that SWL and MPSNs are strictly more powerful than the WL test
and not less powerful than the 3-WL test. We deepen the analysis by comparing
our model with traditional graph neural networks (GNNs) with ReLU activations
in terms of the number of linear regions of the functions they can represent.
We empirically support our theoretical claims by showing that MPSNs can
distinguish challenging strongly regular graphs for which GNNs fail and, when
equipped with orientation equivariant layers, they can improve classification
accuracy in oriented SCs compared to a GNN baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bodnar_C/0/1/0/all/0/1"&gt;Cristian Bodnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frasca_F/0/1/0/all/0/1"&gt;Fabrizio Frasca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Guang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otter_N/0/1/0/all/0/1"&gt;Nina Otter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montufar_G/0/1/0/all/0/1"&gt;Guido Mont&amp;#xfa;far&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf2;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1"&gt;Michael Bronstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DyGLIP: A Dynamic Graph Model with Link Prediction for Accurate Multi-Camera Multiple Object Tracking. (arXiv:2106.06856v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06856</id>
        <link href="http://arxiv.org/abs/2106.06856"/>
        <updated>2021-06-15T01:45:20.274Z</updated>
        <summary type="html"><![CDATA[Multi-Camera Multiple Object Tracking (MC-MOT) is a significant computer
vision problem due to its emerging applicability in several real-world
applications. Despite a large number of existing works, solving the data
association problem in any MC-MOT pipeline is arguably one of the most
challenging tasks. Developing a robust MC-MOT system, however, is still highly
challenging due to many practical issues such as inconsistent lighting
conditions, varying object movement patterns, or the trajectory occlusions of
the objects between the cameras. To address these problems, this work,
therefore, proposes a new Dynamic Graph Model with Link Prediction (DyGLIP)
approach to solve the data association task. Compared to existing methods, our
new model offers several advantages, including better feature representations
and the ability to recover from lost tracks during camera transitions.
Moreover, our model works gracefully regardless of the overlapping ratios
between the cameras. Experimental results show that we outperform existing
MC-MOT algorithms by a large margin on several practical datasets. Notably, our
model works favorably on online settings but can be extended to an incremental
approach for large-scale datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quach_K/0/1/0/all/0/1"&gt;Kha Gia Quach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Pha Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Huu Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Thanh-Dat Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1"&gt;Chi Nhan Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1"&gt;Minh-Triet Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1"&gt;Khoa Luu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoolNet: Minimizing The Energy Consumption of Binary Neural Networks. (arXiv:2106.06991v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06991</id>
        <link href="http://arxiv.org/abs/2106.06991"/>
        <updated>2021-06-15T01:45:20.268Z</updated>
        <summary type="html"><![CDATA[Recent works on Binary Neural Networks (BNNs) have made promising progress in
narrowing the accuracy gap of BNNs to their 32-bit counterparts. However, the
accuracy gains are often based on specialized model designs using additional
32-bit components. Furthermore, almost all previous BNNs use 32-bit for feature
maps and the shortcuts enclosing the corresponding binary convolution blocks,
which helps to effectively maintain the accuracy, but is not friendly to
hardware accelerators with limited memory, energy, and computing resources.
Thus, we raise the following question: How can accuracy and energy consumption
be balanced in a BNN network design? We extensively study this fundamental
problem in this work and propose a novel BNN architecture without most commonly
used 32-bit components: \textit{BoolNet}. Experimental results on ImageNet
demonstrate that BoolNet can achieve 4.6x energy reduction coupled with 1.2\%
higher accuracy than the commonly used BNN architecture Bi-RealNet. Code and
trained models are available at: https://github.com/hpi-xnor/BoolNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1"&gt;Nianhui Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_J/0/1/0/all/0/1"&gt;Joseph Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haojin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1"&gt;Kai Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1"&gt;Xuefei Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1"&gt;Christoph Meinel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Cough Classification using Machine Learning and Global Smartphone Recordings. (arXiv:2012.01926v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01926</id>
        <link href="http://arxiv.org/abs/2012.01926"/>
        <updated>2021-06-15T01:45:20.262Z</updated>
        <summary type="html"><![CDATA[We present a machine learning based COVID-19 cough classifier which can
discriminate COVID-19 positive coughs from both COVID-19 negative and healthy
coughs recorded on a smartphone. This type of screening is non-contact, easy to
apply, and can reduce the workload in testing centres as well as limit
transmission by recommending early self-isolation to those who have a cough
suggestive of COVID-19. The datasets used in this study include subjects from
all six continents and contain both forced and natural coughs, indicating that
the approach is widely applicable. The publicly available Coswara dataset
contains 92 COVID-19 positive and 1079 healthy subjects, while the second
smaller dataset was collected mostly in South Africa and contains 18 COVID-19
positive and 26 COVID-19 negative subjects who have undergone a SARS-CoV
laboratory test. Both datasets indicate that COVID-19 positive coughs are
15\%-20\% shorter than non-COVID coughs. Dataset skew was addressed by applying
the synthetic minority oversampling technique (SMOTE). A leave-$p$-out
cross-validation scheme was used to train and evaluate seven machine learning
classifiers: LR, KNN, SVM, MLP, CNN, LSTM and Resnet50. Our results show that
although all classifiers were able to identify COVID-19 coughs, the best
performance was exhibited by the Resnet50 classifier, which was best able to
discriminate between the COVID-19 positive and the healthy coughs with an area
under the ROC curve (AUC) of 0.98. An LSTM classifier was best able to
discriminate between the COVID-19 positive and COVID-19 negative coughs, with
an AUC of 0.94 after selecting the best 13 features from a sequential forward
selection (SFS). Since this type of cough audio classification is
cost-effective and easy to deploy, it is potentially a useful and viable means
of non-contact COVID-19 screening.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pahar_M/0/1/0/all/0/1"&gt;Madhurananda Pahar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klopper_M/0/1/0/all/0/1"&gt;Marisa Klopper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warren_R/0/1/0/all/0/1"&gt;Robin Warren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niesler_T/0/1/0/all/0/1"&gt;Thomas Niesler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data. (arXiv:2106.01797v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01797</id>
        <link href="http://arxiv.org/abs/2106.01797"/>
        <updated>2021-06-15T01:45:20.243Z</updated>
        <summary type="html"><![CDATA[Among ubiquitous multimodal data in the real world, text is the modality
generated by human, while image reflects the physical world honestly. In a
visual understanding application, machines are expected to understand images
like human. Inspired by this, we propose a novel self-supervised learning
method, named Text-enhanced Visual Deep InfoMax (TVDIM), to learn better visual
representations by fully utilizing the naturally-existing multimodal data. Our
core idea of self-supervised learning is to maximize the mutual information
between features extracted from multiple views of a shared context to a
rational degree. Different from previous methods which only consider multiple
views from a single modality, our work produces multiple views from different
modalities, and jointly optimizes the mutual information for features pairs of
intra-modality and inter-modality. Considering the information gap between
inter-modality features pairs from data noise, we adopt a \emph{ranking-based}
contrastive learning to optimize the mutual information. During evaluation, we
directly use the pre-trained visual representations to complete various image
classification tasks. Experimental results show that, TVDIM significantly
outperforms previous visual self-supervised methods when processing the same
set of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_P/0/1/0/all/0/1"&gt;Pengda Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1"&gt;Kefeng Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qiang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shared Cross-Modal Trajectory Prediction for Autonomous Driving. (arXiv:2004.00202v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.00202</id>
        <link href="http://arxiv.org/abs/2004.00202"/>
        <updated>2021-06-15T01:45:20.237Z</updated>
        <summary type="html"><![CDATA[Predicting future trajectories of traffic agents in highly interactive
environments is an essential and challenging problem for the safe operation of
autonomous driving systems. On the basis of the fact that self-driving vehicles
are equipped with various types of sensors (e.g., LiDAR scanner, RGB camera,
radar, etc.), we propose a Cross-Modal Embedding framework that aims to benefit
from the use of multiple input modalities. At training time, our model learns
to embed a set of complementary features in a shared latent space by jointly
optimizing the objective functions across different types of input data. At
test time, a single input modality (e.g., LiDAR data) is required to generate
predictions from the input perspective (i.e., in the LiDAR space), while taking
advantages from the model trained with multiple sensor modalities. An extensive
evaluation is conducted to show the efficacy of the proposed framework using
two benchmark driving datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1"&gt;Chiho Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Joon Hee Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1"&gt;Srikanth Malla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Continual Learning with Weighted Inter-client Transfer. (arXiv:2003.03196v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.03196</id>
        <link href="http://arxiv.org/abs/2003.03196"/>
        <updated>2021-06-15T01:45:20.231Z</updated>
        <summary type="html"><![CDATA[There has been a surge of interest in continual learning and federated
learning, both of which are important in deep neural networks in real-world
scenarios. Yet little research has been done regarding the scenario where each
client learns on a sequence of tasks from a private local data stream. This
problem of federated continual learning poses new challenges to continual
learning, such as utilizing knowledge from other clients, while preventing
interference from irrelevant knowledge. To resolve these issues, we propose a
novel federated continual learning framework, Federated Weighted Inter-client
Transfer (FedWeIT), which decomposes the network weights into global federated
parameters and sparse task-specific parameters, and each client receives
selective knowledge from other clients by taking a weighted combination of
their task-specific parameters. FedWeIT minimizes interference between
incompatible tasks, and also allows positive knowledge transfer across clients
during learning. We validate our FedWeIT against existing federated learning
and continual learning methods under varying degrees of task similarity across
clients, and our model significantly outperforms them with a large reduction in
the communication cost. Code is available at https://github.com/wyjeong/FedWeIT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1"&gt;Wonyong Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Giwoong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eunho Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Convolutional Neural Networks for the Helicity Classification of Magnetic Fields. (arXiv:2106.06718v1 [astro-ph.HE])]]></title>
        <id>http://arxiv.org/abs/2106.06718</id>
        <link href="http://arxiv.org/abs/2106.06718"/>
        <updated>2021-06-15T01:45:20.224Z</updated>
        <summary type="html"><![CDATA[The presence of non-zero helicity in intergalactic magnetic fields is a
smoking gun for their primordial origin since they have to be generated by
processes that break CP invariance. As an experimental signature for the
presence of helical magnetic fields, an estimator $Q$ based on the triple
scalar product of the wave-vectors of photons generated in electromagnetic
cascades from, e.g., TeV blazars, has been suggested previously. We propose to
apply deep learning to helicity classification employing Convolutional Neural
Networks and show that this method outperforms the $Q$ estimator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Vago_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Oreste Pinciroli Vago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Hameed_I/0/1/0/all/0/1"&gt;Ibrahim A. Hameed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kachelriess_M/0/1/0/all/0/1"&gt;Michael Kachelriess&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Testing: Sample-Efficient Model Evaluation. (arXiv:2103.05331v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05331</id>
        <link href="http://arxiv.org/abs/2103.05331"/>
        <updated>2021-06-15T01:45:20.216Z</updated>
        <summary type="html"><![CDATA[We introduce a new framework for sample-efficient model evaluation that we
call active testing. While approaches like active learning reduce the number of
labels needed for model training, existing literature largely ignores the cost
of labeling test data, typically unrealistically assuming large test sets for
model evaluation. This creates a disconnect to real applications, where test
labels are important and just as expensive, e.g. for optimizing
hyperparameters. Active testing addresses this by carefully selecting the test
points to label, ensuring model evaluation is sample-efficient. To this end, we
derive theoretically-grounded and intuitive acquisition strategies that are
specifically tailored to the goals of active testing, noting these are distinct
to those of active learning. As actively selecting labels introduces a bias; we
further show how to remove this bias while reducing the variance of the
estimator at the same time. Active testing is easy to implement and can be
applied to any supervised machine learning method. We demonstrate its
effectiveness on models including WideResNets and Gaussian processes on
datasets including Fashion-MNIST and CIFAR-100.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kossen_J/0/1/0/all/0/1"&gt;Jannik Kossen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Farquhar_S/0/1/0/all/0/1"&gt;Sebastian Farquhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recoverability Landscape of Tree Structured Markov Random Fields under Symmetric Noise. (arXiv:2102.08554v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08554</id>
        <link href="http://arxiv.org/abs/2102.08554"/>
        <updated>2021-06-15T01:45:20.198Z</updated>
        <summary type="html"><![CDATA[We study the problem of learning tree-structured Markov random fields (MRF)
on discrete random variables with common support when the observations are
corrupted by a $k$-ary symmetric noise channel with unknown probability of
error. For Ising models (support size = 2), past work has shown that graph
structure can only be recovered up to the leaf clusters (a leaf node, its
parent, and its siblings form a leaf cluster) and exact recovery is impossible.
No prior work has addressed the setting of support size of 3 or more, and
indeed this setting is far richer. As we show, when the support size is 3 or
more, the structure of the leaf clusters may be partially or fully
identifiable. We provide a precise characterization of this phenomenon and show
that the extent of recoverability is dictated by the joint PMF of the random
variables. In particular, we provide necessary and sufficient conditions for
exact recoverability. Furthermore, we present a polynomial time, sample
efficient algorithm that recovers the exact tree when this is possible, or up
to the unidentifiability as promised by our characterization, when full
recoverability is impossible. Finally, we demonstrate the efficacy of our
algorithm experimentally.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Katiyar_A/0/1/0/all/0/1"&gt;Ashish Katiyar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Basu_S/0/1/0/all/0/1"&gt;Soumya Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shah_V/0/1/0/all/0/1"&gt;Vatsal Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Caramanis_C/0/1/0/all/0/1"&gt;Constantine Caramanis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certified Robustness to Text Adversarial Attacks by Randomized [MASK]. (arXiv:2105.03743v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03743</id>
        <link href="http://arxiv.org/abs/2105.03743"/>
        <updated>2021-06-15T01:45:20.192Z</updated>
        <summary type="html"><![CDATA[Recently, few certified defense methods have been developed to provably
guarantee the robustness of a text classifier to adversarial synonym
substitutions. However, all existing certified defense methods assume that the
defenders are informed of how the adversaries generate synonyms, which is not a
realistic scenario. In this paper, we propose a certifiably robust defense
method by randomly masking a certain proportion of the words in an input text,
in which the above unrealistic assumption is no longer necessary. The proposed
method can defend against not only word substitution-based attacks, but also
character-level perturbations. We can certify the classifications of over 50%
texts to be robust to any perturbation of 5 words on AGNEWS, and 2 words on
SST2 dataset. The experimental results show that our randomized smoothing
method significantly outperforms recently proposed defense methods across
multiple datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1"&gt;Jiehang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiaoqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jianhan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Liping Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage. (arXiv:2106.03207v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03207</id>
        <link href="http://arxiv.org/abs/2106.03207"/>
        <updated>2021-06-15T01:45:20.186Z</updated>
        <summary type="html"><![CDATA[This paper studies offline Imitation Learning (IL) where an agent learns to
imitate an expert demonstrator without additional online environment
interactions. Instead, the learner is presented with a static offline dataset
of state-action-next state transition triples from a potentially less
proficient behavior policy. We introduce Model-based IL from Offline data
(MILO): an algorithmic framework that utilizes the static dataset to solve the
offline IL problem efficiently both in theory and in practice. In theory, even
if the behavior policy is highly sub-optimal compared to the expert, we show
that as long as the data from the behavior policy provides sufficient coverage
on the expert state-action traces (and with no necessity for a global coverage
over the entire state-action space), MILO can provably combat the covariate
shift issue in IL. Complementing our theory results, we also demonstrate that a
practical implementation of our approach mitigates covariate shift on benchmark
MuJoCo continuous control tasks. We demonstrate that with behavior policies
whose performances are less than half of that of the expert, MILO still
successfully imitates with an extremely low number of expert state-action pairs
while traditional offline IL method such as behavior cloning (BC) fails
completely. Source code is provided at https://github.com/jdchang1/milo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;Jonathan D. Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uehara_M/0/1/0/all/0/1"&gt;Masatoshi Uehara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivas_D/0/1/0/all/0/1"&gt;Dhruv Sreenivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kidambi_R/0/1/0/all/0/1"&gt;Rahul Kidambi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlated Weights in Infinite Limits of Deep Convolutional Neural Networks. (arXiv:2101.04097v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04097</id>
        <link href="http://arxiv.org/abs/2101.04097"/>
        <updated>2021-06-15T01:45:20.179Z</updated>
        <summary type="html"><![CDATA[Infinite width limits of deep neural networks often have tractable forms.
They have been used to analyse the behaviour of finite networks, as well as
being useful methods in their own right. When investigating infinitely wide
convolutional neural networks (CNNs), it was observed that the correlations
arising from spatial weight sharing disappear in the infinite limit. This is
undesirable, as spatial correlation is the main motivation behind CNNs. We show
that the loss of this property is not a consequence of the infinite limit, but
rather of choosing an independent weight prior. Correlating the weights
maintains the correlations in the activations. Varying the amount of
correlation interpolates between independent-weight limits and mean-pooling.
Empirical evaluation of the infinitely wide network shows that optimal
performance is achieved between the extremes, indicating that correlations can
be useful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Garriga_Alonso_A/0/1/0/all/0/1"&gt;Adri&amp;#xe0; Garriga-Alonso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wilk_M/0/1/0/all/0/1"&gt;Mark van der Wilk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Particle Filtering via Entropy-Regularized Optimal Transport. (arXiv:2102.07850v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07850</id>
        <link href="http://arxiv.org/abs/2102.07850"/>
        <updated>2021-06-15T01:45:20.173Z</updated>
        <summary type="html"><![CDATA[Particle Filtering (PF) methods are an established class of procedures for
performing inference in non-linear state-space models. Resampling is a key
ingredient of PF, necessary to obtain low variance likelihood and states
estimates. However, traditional resampling methods result in PF-based loss
functions being non-differentiable with respect to model and PF parameters. In
a variational inference context, resampling also yields high variance gradient
estimates of the PF-based evidence lower bound. By leveraging optimal transport
ideas, we introduce a principled differentiable particle filter and provide
convergence results. We demonstrate this novel method on a variety of
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Corenflos_A/0/1/0/all/0/1"&gt;Adrien Corenflos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thornton_J/0/1/0/all/0/1"&gt;James Thornton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1"&gt;George Deligiannidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment. (arXiv:2104.05632v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05632</id>
        <link href="http://arxiv.org/abs/2104.05632"/>
        <updated>2021-06-15T01:45:20.166Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning from large-scale offline datasets provides us with the
ability to learn policies without potentially unsafe or impractical
exploration. Significant progress has been made in the past few years in
dealing with the challenge of correcting for differing behavior between the
data collection and learned policies. However, little attention has been paid
to potentially changing dynamics when transferring a policy to the online
setting, where performance can be up to 90% reduced for existing methods. In
this paper we address this problem with Augmented World Models (AugWM). We
augment a learned dynamics model with simple transformations that seek to
capture potential changes in physical properties of the robot, leading to more
robust policies. We not only train our policy in this new setting, but also
provide it with the sampled augmentation as a context, allowing it to adapt to
changes in the environment. At test time we learn the context in a
self-supervised fashion by approximating the augmentation which corresponds to
the new environment. We rigorously evaluate our approach on over 100 different
changed dynamics settings, and show that this simple approach can significantly
improve the zero-shot generalization of a recent state-of-the-art baseline,
often achieving successful policies where the baseline fails.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ball_P/0/1/0/all/0/1"&gt;Philip J. Ball&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Cong Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thinking Like Transformers. (arXiv:2106.06981v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06981</id>
        <link href="http://arxiv.org/abs/2106.06981"/>
        <updated>2021-06-15T01:45:20.150Z</updated>
        <summary type="html"><![CDATA[What is the computational model behind a Transformer? Where recurrent neural
networks have direct parallels in finite state machines, allowing clear
discussion and thought around architecture variants or trained models,
Transformers have no such familiar parallel. In this paper we aim to change
that, proposing a computational model for the transformer-encoder in the form
of a programming language. We map the basic components of a transformer-encoder
-- attention and feed-forward computation -- into simple primitives, around
which we form a programming language: the Restricted Access Sequence Processing
Language (RASP). We show how RASP can be used to program solutions to tasks
that could conceivably be learned by a Transformer, and how a Transformer can
be trained to mimic a RASP solution. In particular, we provide RASP programs
for histograms, sorting, and Dyck-languages. We further use our model to relate
their difficulty in terms of the number of required layers and attention heads:
analyzing a RASP program implies a maximum number of heads and layers necessary
to encode a task in a transformer. Finally, we see how insights gained from our
abstraction might be used to explain phenomena seen in recent works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1"&gt;Gail Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1"&gt;Eran Yahav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoMb-OT: On Batch of Mini-batches Optimal Transport. (arXiv:2102.05912v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05912</id>
        <link href="http://arxiv.org/abs/2102.05912"/>
        <updated>2021-06-15T01:45:20.143Z</updated>
        <summary type="html"><![CDATA[Mini-batch optimal transport (m-OT) has been successfully used in practical
applications that involve probability measures with intractable density, or
probability measures with a very high number of supports. The m-OT solves
several sparser optimal transport problems and then returns the average of
their costs and transportation plans. Despite its scalability advantage, the
m-OT does not consider the relationship between mini-batches which leads to
undesirable estimation. Moreover, the m-OT does not approximate a proper metric
between probability measures since the identity property is not satisfied. To
address these problems, we propose a novel mini-batching scheme for optimal
transport, named Batch of Mini-batches Optimal Transport (BoMb-OT), that finds
the optimal coupling between mini-batches and it can be seen as an
approximation to a well-defined distance on the space of probability measures.
Furthermore, we show that the m-OT is a limit of the entropic regularized
version of the BoMb-OT when the regularized parameter goes to infinity.
Finally, we carry out extensive experiments to show that the BoMb-OT can
estimate a better transportation plan between two original measures than the
m-OT. It leads to a favorable performance of the BoMb-OT in the matching and
color transfer tasks. Furthermore, we observe that the BoMb-OT also provides a
better objective loss than the m-OT for doing approximate Bayesian computation,
estimating parameters of interest in parametric generative models, and learning
non-parametric generative models with gradient flow.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khai Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_Q/0/1/0/all/0/1"&gt;Quoc Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1"&gt;Nhat Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pham_T/0/1/0/all/0/1"&gt;Tung Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bui_H/0/1/0/all/0/1"&gt;Hung Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Phung_D/0/1/0/all/0/1"&gt;Dinh Phung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Le_T/0/1/0/all/0/1"&gt;Trung Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information. (arXiv:2009.01454v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.01454</id>
        <link href="http://arxiv.org/abs/2009.01454"/>
        <updated>2021-06-15T01:45:20.100Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have achieved state-of-the-art performance in
modeling graphs. Despite its great success, as with many other models, GNNs
have the risk to inherit the bias from the training data. In addition, the bias
of GNN can be magnified by the graph structures and message-passing mechanism
of GNNs. The risk of discrimination limits the adoption of GNNs in sensitive
domains such as credit score estimation. Though extensive studies of fair
classification have been conducted on i.i.d data, methods to address the
problem of discrimination on non-i.i.d data are rather limited. Furthermore,
the practical scenario of sparse annotations in sensitive attributes is rarely
considered in existing works. Therefore, we study the novel and important
problem of learning fair GNNs with limited sensitive information. We propose a
novel framework called FairGNN, which is able to reduce the bias of GNNs and
maintain high node classification accuracy by leveraging graph structured data
and sensitive information. Theoretical analysis is conducted to show that
FairGNN can ensure fairness under mild conditions given limited nodes with
known sensitive attributes. Experiments on real-world datasets demonstrated the
effectiveness of the proposed framework in eliminating discrimination while
maintaining high node classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_E/0/1/0/all/0/1"&gt;Enyan Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Suhang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying the Conceptual Error in Dimensionality Reduction. (arXiv:2106.06815v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06815</id>
        <link href="http://arxiv.org/abs/2106.06815"/>
        <updated>2021-06-15T01:45:20.083Z</updated>
        <summary type="html"><![CDATA[Dimension reduction of data sets is a standard problem in the realm of
machine learning and knowledge reasoning. They affect patterns in and
dependencies on data dimensions and ultimately influence any decision-making
processes. Therefore, a wide variety of reduction procedures are in use, each
pursuing different objectives. A so far not considered criterion is the
conceptual continuity of the reduction mapping, i.e., the preservation of the
conceptual structure with respect to the original data set. Based on the notion
scale-measure from formal concept analysis we present in this work a) the
theoretical foundations to detect and quantify conceptual errors in data
scalings; b) an experimental investigation of our approach on eleven data sets
that were respectively treated with a variant of non-negative matrix
factorization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hanika_T/0/1/0/all/0/1"&gt;Tom Hanika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirth_J/0/1/0/all/0/1"&gt;Johannes Hirth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D2C: Diffusion-Denoising Models for Few-shot Conditional Generation. (arXiv:2106.06819v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06819</id>
        <link href="http://arxiv.org/abs/2106.06819"/>
        <updated>2021-06-15T01:45:19.652Z</updated>
        <summary type="html"><![CDATA[Conditional generative models of high-dimensional images have many
applications, but supervision signals from conditions to images can be
expensive to acquire. This paper describes Diffusion-Decoding models with
Contrastive representations (D2C), a paradigm for training unconditional
variational autoencoders (VAEs) for few-shot conditional image generation. D2C
uses a learned diffusion-based prior over the latent representations to improve
generation and contrastive self-supervised learning to improve representation
quality. D2C can adapt to novel generation tasks conditioned on labels or
manipulation constraints, by learning from as few as 100 labeled examples. On
conditional generation from new labels, D2C achieves superior performance over
state-of-the-art VAEs and diffusion models. On conditional image manipulation,
D2C generations are two orders of magnitude faster to produce over StyleGAN2
ones and are preferred by 50% - 60% of the human evaluators in a double-blind
study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1"&gt;Abhishek Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jiaming Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1"&gt;Chenlin Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Minimalist Approach to Offline Reinforcement Learning. (arXiv:2106.06860v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06860</id>
        <link href="http://arxiv.org/abs/2106.06860"/>
        <updated>2021-06-15T01:45:19.652Z</updated>
        <summary type="html"><![CDATA[Offline reinforcement learning (RL) defines the task of learning from a fixed
batch of data. Due to errors in value estimation from out-of-distribution
actions, most offline RL algorithms take the approach of constraining or
regularizing the policy with the actions contained in the dataset. Built on
pre-existing RL algorithms, modifications to make an RL algorithm work offline
comes at the cost of additional complexity. Offline RL algorithms introduce new
hyperparameters and often leverage secondary components such as generative
models, while adjusting the underlying RL algorithm. In this paper we aim to
make a deep RL algorithm work while making minimal changes. We find that we can
match the performance of state-of-the-art offline RL algorithms by simply
adding a behavior cloning term to the policy update of an online RL algorithm
and normalizing the data. The resulting algorithm is a simple to implement and
tune baseline, while more than halving the overall run time by removing the
additional computational overheads of previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujimoto_S/0/1/0/all/0/1"&gt;Scott Fujimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shixiang Shane Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptation for Time Series Forecasting via Attention Sharing. (arXiv:2102.06828v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06828</id>
        <link href="http://arxiv.org/abs/2102.06828"/>
        <updated>2021-06-15T01:45:19.649Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed deep neural networks gaining increasing
popularity in the field of time series forecasting. A primary reason of their
success is their ability to effectively capture complex temporal dynamics
across multiple related time series. However, the advantages of these deep
forecasters only start to emerge in the presence of a sufficient amount of
data. This poses a challenge for typical forecasting problems in practice,
where one either has a small number of time series, or limited observations per
time series, or both. To cope with the issue of data scarcity, we propose a
novel domain adaptation framework, Domain Adaptation Forecaster (DAF), that
leverages the statistical strengths from another relevant domain with abundant
data samples (source) to improve the performance on the domain of interest with
limited data (target). In particular, we propose an attention-based shared
module with a domain discriminator across domains as well as private modules
for individual domains. This allows us to jointly train the source and target
domains by generating domain-invariant latent features while retraining
domain-specific features. Extensive experiments on various domains demonstrate
that our proposed method outperforms state-of-the-art baselines on synthetic
and real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xiaoyong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1"&gt;Youngsuk Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddix_D/0/1/0/all/0/1"&gt;Danielle C. Maddix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xifeng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disrupting Model Training with Adversarial Shortcuts. (arXiv:2106.06654v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06654</id>
        <link href="http://arxiv.org/abs/2106.06654"/>
        <updated>2021-06-15T01:45:19.642Z</updated>
        <summary type="html"><![CDATA[When data is publicly released for human consumption, it is unclear how to
prevent its unauthorized usage for machine learning purposes. Successful model
training may be preventable with carefully designed dataset modifications, and
we present a proof-of-concept approach for the image classification setting. We
propose methods based on the notion of adversarial shortcuts, which encourage
models to rely on non-robust signals rather than semantic features, and our
experiments demonstrate that these measures successfully prevent deep learning
models from achieving high accuracy on real, unmodified data examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1"&gt;Ivan Evtimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Covert_I/0/1/0/all/0/1"&gt;Ian Covert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1"&gt;Aditya Kusupati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohno_T/0/1/0/all/0/1"&gt;Tadayoshi Kohno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orderly Dual-Teacher Knowledge Distillation for Lightweight Human Pose Estimation. (arXiv:2104.10414v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10414</id>
        <link href="http://arxiv.org/abs/2104.10414"/>
        <updated>2021-06-15T01:45:19.636Z</updated>
        <summary type="html"><![CDATA[Although deep convolution neural networks (DCNN) have achieved excellent
performance in human pose estimation, these networks often have a large number
of parameters and computations, leading to the slow inference speed. For this
issue, an effective solution is knowledge distillation, which transfers
knowledge from a large pre-trained network (teacher) to a small network
(student). However, there are some defects in the existing approaches: (I) Only
a single teacher is adopted, neglecting the potential that a student can learn
from multiple teachers. (II) The human segmentation mask can be regarded as
additional prior information to restrict the location of keypoints, which is
never utilized. (III) A student with a small number of parameters cannot fully
imitate heatmaps provided by datasets and teachers. (IV) There exists noise in
heatmaps generated by teachers, which causes model degradation. To overcome
these defects, we propose an orderly dual-teacher knowledge distillation (ODKD)
framework, which consists of two teachers with different capabilities.
Specifically, the weaker one (primary teacher, PT) is used to teach keypoints
information, the stronger one (senior teacher, ST) is utilized to transfer
segmentation and keypoints information by adding the human segmentation mask.
Taking dual-teacher together, an orderly learning strategy is proposed to
promote knowledge absorbability. Moreover, we employ a binarization operation
which further improves the learning ability of the student and reduces noise in
heatmaps. Experimental results on COCO and OCHuman keypoints datasets show that
our proposed ODKD can improve the performance of different lightweight models
by a large margin, and HRNet-W16 equipped with ODKD achieves state-of-the-art
performance for lightweight human pose estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhong-Qiu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yuchen Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_W/0/1/0/all/0/1"&gt;Weidong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Comparison of Off-policy Prediction Learning Algorithms on the Collision Task. (arXiv:2106.00922v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00922</id>
        <link href="http://arxiv.org/abs/2106.00922"/>
        <updated>2021-06-15T01:45:19.632Z</updated>
        <summary type="html"><![CDATA[Off-policy prediction -- learning the value function for one policy from data
generated while following another policy -- is one of the most challenging
subproblems in reinforcement learning. This paper presents empirical results
with eleven prominent off-policy learning algorithms that use linear function
approximation: five Gradient-TD methods, two Emphatic-TD methods, Off-policy
TD($\lambda$), Vtrace, and versions of Tree Backup and ABQ modified to apply to
a prediction setting. Our experiments used the Collision task, a small
idealized off-policy problem analogous to that of an autonomous car trying to
predict whether it will collide with an obstacle. We assessed the performance
of the algorithms according to their learning rate, asymptotic error level, and
sensitivity to step-size and bootstrapping parameters. By these measures, the
eleven algorithms can be partially ordered on the Collision task. In the top
tier, the two Emphatic-TD algorithms learned the fastest, reached the lowest
errors, and were robust to parameter settings. In the middle tier, the five
Gradient-TD algorithms and Off-policy TD($\lambda$) were more sensitive to the
bootstrapping parameter. The bottom tier comprised Vtrace, Tree Backup, and
ABQ; these algorithms were no faster and had higher asymptotic error than the
others. Our results are definitive for this task, though of course experiments
with more tasks are needed before an overall assessment of the algorithms'
merits can be made.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghiassian_S/0/1/0/all/0/1"&gt;Sina Ghiassian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1"&gt;Richard S. Sutton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoundDet: Polyphonic Sound Event Detection and Localization from Raw Waveform. (arXiv:2106.06969v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.06969</id>
        <link href="http://arxiv.org/abs/2106.06969"/>
        <updated>2021-06-15T01:45:19.631Z</updated>
        <summary type="html"><![CDATA[We present a new framework SoundDet, which is an end-to-end trainable and
light-weight framework, for polyphonic moving sound event detection and
localization. Prior methods typically approach this problem by preprocessing
raw waveform into time-frequency representations, which is more amenable to
process with well-established image processing pipelines. Prior methods also
detect in segment-wise manner, leading to incomplete and partial detections.
SoundDet takes a novel approach and directly consumes the raw, multichannel
waveform and treats the spatio-temporal sound event as a complete
``sound-object" to be detected. Specifically, SoundDet consists of a backbone
neural network and two parallel heads for temporal detection and spatial
localization, respectively. Given the large sampling rate of raw waveform, the
backbone network first learns a set of phase-sensitive and frequency-selective
bank of filters to explicitly retain direction-of-arrival information, whilst
being highly computationally and parametrically efficient than standard 1D/2D
convolution. A dense sound event proposal map is then constructed to handle the
challenges of predicting events with large varying temporal duration.
Accompanying the dense proposal map are a temporal overlapness map and a motion
smoothness map that measure a proposal's confidence to be an event from
temporal detection accuracy and movement consistency perspective. Involving the
two maps guarantees SoundDet to be trained in a spatio-temporally unified
manner. Experimental results on the public DCASE dataset show the advantage of
SoundDet on both segment-based and our newly proposed event-based evaluation
system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuhang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1"&gt;Niki Trigoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1"&gt;Andrew Markham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hedging with Linear Regressions and Neural Networks. (arXiv:2004.08891v3 [q-fin.RM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.08891</id>
        <link href="http://arxiv.org/abs/2004.08891"/>
        <updated>2021-06-15T01:45:19.625Z</updated>
        <summary type="html"><![CDATA[We study neural networks as nonparametric estimation tools for the hedging of
options. To this end, we design a network, named HedgeNet, that directly
outputs a hedging strategy. This network is trained to minimise the hedging
error instead of the pricing error. Applied to end-of-day and tick prices of
S&P 500 and Euro Stoxx 50 options, the network is able to reduce the mean
squared hedging error of the Black-Scholes benchmark significantly. However, a
similar benefit arises by simple linear regressions that incorporate the
leverage effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Ruf_J/0/1/0/all/0/1"&gt;Johannes Ruf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiguan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Higher Education Throughput in South Africa Using a Tree-Based Ensemble Technique. (arXiv:2106.06805v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2106.06805</id>
        <link href="http://arxiv.org/abs/2106.06805"/>
        <updated>2021-06-15T01:45:19.624Z</updated>
        <summary type="html"><![CDATA[We use gradient boosting machines and logistic regression to predict academic
throughput at a South African university. The results highlight the significant
influence of socio-economic factors and field of study as predictors of
throughput. We further find that socio-economic factors become less of a
predictor relative to the field of study as the time to completion increases.
We provide recommendations on interventions to counteract the identified
effects, which include academic, psychosocial and financial support.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mbuvha_R/0/1/0/all/0/1"&gt;Rendani Mbuvha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zondo_P/0/1/0/all/0/1"&gt;Patience Zondo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mauda_A/0/1/0/all/0/1"&gt;Aluwani Mauda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marwala_T/0/1/0/all/0/1"&gt;Tshilidzi Marwala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP. (arXiv:2106.06830v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06830</id>
        <link href="http://arxiv.org/abs/2106.06830"/>
        <updated>2021-06-15T01:45:19.623Z</updated>
        <summary type="html"><![CDATA[Retrieval is a core component for open-domain NLP tasks. In open-domain
tasks, multiple entities can share a name, making disambiguation an inherent
yet under-explored problem. We propose an evaluation benchmark for assessing
the entity disambiguation capabilities of these retrievers, which we call
Ambiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection
of entities that share a name along with queries about those entities. By
covering the set of entities for polysemous names, AmbER sets act as a
challenging test of entity disambiguation. We create AmbER sets for three
popular open-domain tasks: fact checking, slot filling, and question answering,
and evaluate a diverse set of retrievers. We find that the retrievers exhibit
popularity bias, significantly under-performing on rarer entities that share a
name, e.g., they are twice as likely to retrieve erroneous documents on queries
for the less popular entity under the same name. These experiments on AmbER
sets show their utility as an evaluation tool and highlight the weaknesses of
popular retrieval systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Anthony Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gudipati_P/0/1/0/all/0/1"&gt;Pallavi Gudipati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1"&gt;Shayne Longpre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1"&gt;Xiao Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functional optimal transport: map estimation and domain adaptation for functional data. (arXiv:2102.03895v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03895</id>
        <link href="http://arxiv.org/abs/2102.03895"/>
        <updated>2021-06-15T01:45:19.623Z</updated>
        <summary type="html"><![CDATA[We introduce a formulation of optimal transport problem for distributions on
function spaces, where the stochastic map between functional domains can be
partially represented in terms of an (infinite-dimensional) Hilbert-Schmidt
operator mapping a Hilbert space of functions to another. For numerous machine
learning tasks, data can be naturally viewed as samples drawn from spaces of
functions, such as curves and surfaces, in high dimensions. Optimal transport
for functional data analysis provides a useful framework of treatment for such
domains. In this work, we develop an efficient algorithm for finding the
stochastic transport map between functional domains and provide theoretical
guarantees on the existence, uniqueness, and consistency of our estimate for
the Hilbert-Schmidt operator. We validate our method on synthetic datasets and
study the geometric properties of the transport map. Experiments on real-world
datasets of robot arm trajectories further demonstrate the effectiveness of our
method on applications in domain adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiacheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Guha_A/0/1/0/all/0/1"&gt;Aritra Guha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Do_D/0/1/0/all/0/1"&gt;Dat Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengdi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_X/0/1/0/all/0/1"&gt;XuanLong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions. (arXiv:1812.03664v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.03664</id>
        <link href="http://arxiv.org/abs/1812.03664"/>
        <updated>2021-06-15T01:45:19.621Z</updated>
        <summary type="html"><![CDATA[Learning with limited data is a key challenge for visual recognition. Many
few-shot learning methods address this challenge by learning an instance
embedding function from seen classes and apply the function to instances from
unseen classes with limited labels. This style of transfer learning is
task-agnostic: the embedding function is not learned optimally discriminative
with respect to the unseen classes, where discerning among them leads to the
target task. In this paper, we propose a novel approach to adapt the instance
embeddings to the target classification task with a set-to-set function,
yielding embeddings that are task-specific and are discriminative. We
empirically investigated various instantiations of such set-to-set functions
and observed the Transformer is most effective -- as it naturally satisfies key
properties of our desired model. We denote this model as FEAT (few-shot
embedding adaptation w/ Transformer) and validate it on both the standard
few-shot classification benchmark and four extended few-shot learning settings
with essential use cases, i.e., cross-domain, transductive, generalized
few-shot learning, and low-shot learning. It archived consistent improvements
over baseline models as well as previous methods and established the new
state-of-the-art results on two benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hexiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1"&gt;Fei Sha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Cost Proxies Meet Differentiable Architecture Search. (arXiv:2106.06799v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06799</id>
        <link href="http://arxiv.org/abs/2106.06799"/>
        <updated>2021-06-15T01:45:19.620Z</updated>
        <summary type="html"><![CDATA[Differentiable neural architecture search (NAS) has attracted significant
attention in recent years due to its ability to quickly discover promising
architectures of deep neural networks even in very large search spaces. Despite
its success, DARTS lacks robustness in certain cases, e.g. it may degenerate to
trivial architectures with excessive parametric-free operations such as skip
connection or random noise, leading to inferior performance. In particular,
operation selection based on the magnitude of architectural parameters was
recently proven to be fundamentally wrong showcasing the need to rethink this
aspect. On the other hand, zero-cost proxies have been recently studied in the
context of sample-based NAS showing promising results -- speeding up the search
process drastically in some cases but also failing on some of the large search
spaces typical for differentiable NAS. In this work we propose a novel
operation selection paradigm in the context of differentiable NAS which
utilises zero-cost proxies. Our perturbation-based zero-cost operation
selection (Zero-Cost-PT) improves searching time and, in many cases, accuracy
compared to the best available differentiable architecture search, regardless
of the search space size. Specifically, we are able to find comparable
architectures to DARTS-PT on the DARTS CNN search space while being over 40x
faster (total searching time 25 minutes on a single GPU).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1"&gt;Lichuan Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dudziak_L/0/1/0/all/0/1"&gt;&amp;#x141;ukasz Dudziak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdelfattah_M/0/1/0/all/0/1"&gt;Mohamed S. Abdelfattah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_T/0/1/0/all/0/1"&gt;Thomas Chau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1"&gt;Nicholas D. Lane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1"&gt;Hongkai Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Corruption-Robust Offline Reinforcement Learning. (arXiv:2106.06630v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06630</id>
        <link href="http://arxiv.org/abs/2106.06630"/>
        <updated>2021-06-15T01:45:19.619Z</updated>
        <summary type="html"><![CDATA[We study the adversarial robustness in offline reinforcement learning. Given
a batch dataset consisting of tuples $(s, a, r, s')$, an adversary is allowed
to arbitrarily modify $\epsilon$ fraction of the tuples. From the corrupted
dataset the learner aims to robustly identify a near-optimal policy. We first
show that a worst-case $\Omega(d\epsilon)$ optimality gap is unavoidable in
linear MDP of dimension $d$, even if the adversary only corrupts the reward
element in a tuple. This contrasts with dimension-free results in robust
supervised learning and best-known lower-bound in the online RL setting with
corruption. Next, we propose robust variants of the Least-Square Value
Iteration (LSVI) algorithm utilizing robust supervised learning oracles, which
achieve near-matching performances in cases both with and without full data
coverage. The algorithm requires the knowledge of $\epsilon$ to design the
pessimism bonus in the no-coverage case. Surprisingly, in this case, the
knowledge of $\epsilon$ is necessary, as we show that being adaptive to unknown
$\epsilon$ is impossible.This again contrasts with recent results on
corruption-robust online RL and implies that robust offline RL is a strictly
harder problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuezhou Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiding Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jerry Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Independent Prototype Propagation for Zero-Shot Compositionality. (arXiv:2106.00305v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00305</id>
        <link href="http://arxiv.org/abs/2106.00305"/>
        <updated>2021-06-15T01:45:19.618Z</updated>
        <summary type="html"><![CDATA[Humans are good at compositional zero-shot reasoning; someone who has never
seen a zebra before could nevertheless recognize one when we tell them it looks
like a horse with black and white stripes. Machine learning systems, on the
other hand, usually leverage spurious correlations in the training data, and
while such correlations can help recognize objects in context, they hurt
generalization. To be able to deal with underspecified datasets while still
leveraging contextual clues during classification, we propose ProtoProp, a
novel prototype propagation graph method. First we learn prototypical
representations of objects (e.g., zebra) that are conditionally independent
w.r.t. their attribute labels (e.g., stripes) and vice versa. Next we propagate
the independent prototypes through a compositional graph, to learn
compositional prototypes of novel attribute-object combinations that reflect
the dependencies of the target distribution. The method does not rely on any
external data, such as class hierarchy graphs or pretrained word embeddings. We
evaluate our approach on AO-Clever, a synthetic and strongly visual dataset
with clean labels, and UT-Zappos, a noisy real-world dataset of fine-grained
shoe types. We show that in the generalized compositional zero-shot setting we
outperform state-of-the-art results, and through ablations we show the
importance of each part of the method and their contribution to the final
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruis_F/0/1/0/all/0/1"&gt;Frank Ruis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1"&gt;Gertjan Burghouts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bucur_D/0/1/0/all/0/1"&gt;Doina Bucur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Visual-Semantic Entanglement Network for Zero-shot Image Recognition. (arXiv:2006.04648v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04648</id>
        <link href="http://arxiv.org/abs/2006.04648"/>
        <updated>2021-06-15T01:45:19.617Z</updated>
        <summary type="html"><![CDATA[Zero-shot learning uses semantic attributes to connect the search space of
unseen objects. In recent years, although the deep convolutional network brings
powerful visual modeling capabilities to the ZSL task, its visual features have
severe pattern inertia and lack of representation of semantic relationships,
which leads to severe bias and ambiguity. In response to this, we propose the
Graph-based Visual-Semantic Entanglement Network to conduct graph modeling of
visual features, which is mapped to semantic attributes by using a knowledge
graph, it contains several novel designs: 1. it establishes a multi-path
entangled network with the convolutional neural network (CNN) and the graph
convolutional network (GCN), which input the visual features from CNN to GCN to
model the implicit semantic relations, then GCN feedback the graph modeled
information to CNN features; 2. it uses attribute word vectors as the target
for the graph semantic modeling of GCN, which forms a self-consistent
regression for graph modeling and supervise GCN to learn more personalized
attribute relations; 3. it fuses and supplements the hierarchical
visual-semantic features refined by graph modeling into visual embedding. Our
method outperforms state-of-the-art approaches on multiple representative ZSL
datasets: AwA2, CUB, and SUN by promoting the semantic linkage modelling of
visual features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_G/0/1/0/all/0/1"&gt;Guihua Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chapman_A/0/1/0/all/0/1"&gt;Adriane Chapman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Pei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1"&gt;Mingnan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yingxue Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Dan Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hall_W/0/1/0/all/0/1"&gt;Wendy Hall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kwame: A Bilingual AI Teaching Assistant for Online SuaCode Courses. (arXiv:2010.11387v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11387</id>
        <link href="http://arxiv.org/abs/2010.11387"/>
        <updated>2021-06-15T01:45:19.615Z</updated>
        <summary type="html"><![CDATA[Introductory hands-on courses such as our smartphone-based coding course,
SuaCode require a lot of support for students to accomplish learning goals.
Online environments make it even more difficult to get assistance especially
more recently because of COVID-19. Given the multilingual context of SuaCode
students - learners across 42 African countries that are mostly Anglophone or
Francophone - in this work, we developed a bilingual Artificial Intelligence
(AI) Teaching Assistant (TA) - Kwame - that provides answers to students'
coding questions from SuaCode courses in English and French. Kwame is a
Sentence-BERT (SBERT)-based question-answering (QA) system that we trained and
evaluated offline using question-answer pairs created from the course's
quizzes, lesson notes and students' questions in past cohorts. Kwame finds the
paragraph most semantically similar to the question via cosine similarity. We
compared the system with TF-IDF and Universal Sentence Encoder. Our results
showed that fine-tuning on the course data and returning the top 3 and 5
answers improved the accuracy results. Kwame will make it easy for students to
get quick and accurate answers to questions in SuaCode courses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1"&gt;George Boateng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Hypergraph Laplacian for Hypergraph Learning. (arXiv:2106.06666v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06666</id>
        <link href="http://arxiv.org/abs/2106.06666"/>
        <updated>2021-06-15T01:45:19.614Z</updated>
        <summary type="html"><![CDATA[HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their
potential in modeling high-order relations preserved in graph structured data.
However, most existing convolution filters are localized and determined by the
pre-defined initial hypergraph topology, neglecting to explore implicit and
long-ange relations in real-world data. In this paper, we propose the first
learning-based method tailored for constructing adaptive hypergraph structure,
termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic
plug-in-play module for improving the representational power of HGCNNs.
Specifically, HERALD adaptively optimizes the adjacency relationship between
hypernodes and hyperedges in an end-to-end manner and thus the task-aware
hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism
to capture the non-local paired-nodes relation. Extensive experiments on
various popular hypergraph datasets for node classification and graph
classification tasks demonstrate that our approach obtains consistent and
considerable performance enhancement, proving its effectiveness and
generalization ability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiying Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuzhao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xi Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1"&gt;Runiu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1"&gt;Shu-Tao Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Robustness under Long-Tailed Distribution. (arXiv:2104.02703v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02703</id>
        <link href="http://arxiv.org/abs/2104.02703"/>
        <updated>2021-06-15T01:45:19.612Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness has attracted extensive studies recently by revealing
the vulnerability and intrinsic characteristics of deep networks. However,
existing works on adversarial robustness mainly focus on balanced datasets,
while real-world data usually exhibits a long-tailed distribution. To push
adversarial robustness towards more realistic scenarios, in this work we
investigate the adversarial vulnerability as well as defense under long-tailed
distributions. In particular, we first reveal the negative impacts induced by
imbalanced data on both recognition performance and adversarial robustness,
uncovering the intrinsic challenges of this problem. We then perform a
systematic study on existing long-tailed recognition methods in conjunction
with the adversarial training framework. Several valuable observations are
obtained: 1) natural accuracy is relatively easy to improve, 2) fake gain of
robust accuracy exists under unreliable evaluation, and 3) boundary error
limits the promotion of robustness. Inspired by these observations, we propose
a clean yet effective framework, RoBal, which consists of two dedicated
modules, a scale-invariant classifier and data re-balancing via both margin
engineering at training stage and boundary adjustment during inference.
Extensive experiments demonstrate the superiority of our approach over other
state-of-the-art defense methods. To our best knowledge, we are the first to
tackle adversarial robustness under long-tailed distributions, which we believe
would be a significant step towards real-world robustness. Our code is
available at: https://github.com/wutong16/Adversarial_Long-Tail .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingqiu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dahua Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Reinforcement Learning Approach to Marginalized Importance Sampling with the Successor Representation. (arXiv:2106.06854v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06854</id>
        <link href="http://arxiv.org/abs/2106.06854"/>
        <updated>2021-06-15T01:45:19.610Z</updated>
        <summary type="html"><![CDATA[Marginalized importance sampling (MIS), which measures the density ratio
between the state-action occupancy of a target policy and that of a sampling
distribution, is a promising approach for off-policy evaluation. However,
current state-of-the-art MIS methods rely on complex optimization tricks and
succeed mostly on simple toy problems. We bridge the gap between MIS and deep
reinforcement learning by observing that the density ratio can be computed from
the successor representation of the target policy. The successor representation
can be trained through deep reinforcement learning methodology and decouples
the reward optimization from the dynamics of the environment, making the
resulting algorithm stable and applicable to high-dimensional domains. We
evaluate the empirical performance of our approach on a variety of challenging
Atari and MuJoCo environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujimoto_S/0/1/0/all/0/1"&gt;Scott Fujimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1"&gt;David Meger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Generating Circuits. (arXiv:2102.09768v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09768</id>
        <link href="http://arxiv.org/abs/2102.09768"/>
        <updated>2021-06-15T01:45:19.607Z</updated>
        <summary type="html"><![CDATA[Generating functions, which are widely used in combinatorics and probability
theory, encode function values into the coefficients of a polynomial. In this
paper, we explore their use as a tractable probabilistic model, and propose
probabilistic generating circuits (PGCs) for their efficient representation.
PGCs are strictly more expressive efficient than many existing tractable
probabilistic models, including determinantal point processes (DPPs),
probabilistic circuits (PCs) such as sum-product networks, and tractable
graphical models. We contend that PGCs are not just a theoretical framework
that unifies vastly different existing models, but also show great potential in
modeling realistic data. We exhibit a simple class of PGCs that are not
trivially subsumed by simple combinations of PCs and DPPs, and obtain
competitive performance on a suite of density estimation benchmarks. We also
highlight PGCs' connection to the theory of strongly Rayleigh distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Honghua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juba_B/0/1/0/all/0/1"&gt;Brendan Juba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1"&gt;Guy Van den Broeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions. (arXiv:1812.03664v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.03664</id>
        <link href="http://arxiv.org/abs/1812.03664"/>
        <updated>2021-06-15T01:45:19.604Z</updated>
        <summary type="html"><![CDATA[Learning with limited data is a key challenge for visual recognition. Many
few-shot learning methods address this challenge by learning an instance
embedding function from seen classes and apply the function to instances from
unseen classes with limited labels. This style of transfer learning is
task-agnostic: the embedding function is not learned optimally discriminative
with respect to the unseen classes, where discerning among them leads to the
target task. In this paper, we propose a novel approach to adapt the instance
embeddings to the target classification task with a set-to-set function,
yielding embeddings that are task-specific and are discriminative. We
empirically investigated various instantiations of such set-to-set functions
and observed the Transformer is most effective -- as it naturally satisfies key
properties of our desired model. We denote this model as FEAT (few-shot
embedding adaptation w/ Transformer) and validate it on both the standard
few-shot classification benchmark and four extended few-shot learning settings
with essential use cases, i.e., cross-domain, transductive, generalized
few-shot learning, and low-shot learning. It archived consistent improvements
over baseline models as well as previous methods and established the new
state-of-the-art results on two benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hexiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1"&gt;Fei Sha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Link Prediction with Persistent Homology: An Interactive View. (arXiv:2102.10255v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10255</id>
        <link href="http://arxiv.org/abs/2102.10255"/>
        <updated>2021-06-15T01:45:19.601Z</updated>
        <summary type="html"><![CDATA[Link prediction is an important learning task for graph-structured data. In
this paper, we propose a novel topological approach to characterize
interactions between two nodes. Our topological feature, based on the extended
persistent homology, encodes rich structural information regarding the
multi-hop paths connecting nodes. Based on this feature, we propose a graph
neural network method that outperforms state-of-the-arts on different
benchmarks. As another contribution, we propose a novel algorithm to more
efficiently compute the extended persistence diagrams for graphs. This
algorithm can be generally applied to accelerate many other topological methods
for graph learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zuoyu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengfei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Liangcai Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse PointPillars: Exploiting Sparsity in Birds-Eye-View Object Detection. (arXiv:2106.06882v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06882</id>
        <link href="http://arxiv.org/abs/2106.06882"/>
        <updated>2021-06-15T01:45:19.600Z</updated>
        <summary type="html"><![CDATA[Bird's Eye View (BEV) is a popular representation for processing 3D point
clouds, and by its nature is fundamentally sparse. Motivated by the
computational limitations of mobile robot platforms, we take a fast
high-performance BEV 3D object detector - PointPillars - and modify its
backbone to exploit this sparsity, leading to decreased runtimes. We present
preliminary results demonstrating decreased runtimes with either the same
performance or a modest decrease in performance, which we anticipate will be
remedied by model specific hyperparameter tuning. Our work is a first step
towards a new class of 3D object detectors that exploit sparsity throughout
their entire pipeline in order to reduce runtime and resource usage while
maintaining good detection performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vedder_K/0/1/0/all/0/1"&gt;Kyle Vedder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1"&gt;Eric Eaton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Combinatorial Algorithms for Combinatorial Bandits: Corruptions and Approximations. (arXiv:2106.06712v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06712</id>
        <link href="http://arxiv.org/abs/2106.06712"/>
        <updated>2021-06-15T01:45:19.599Z</updated>
        <summary type="html"><![CDATA[We consider the stochastic combinatorial semi-bandit problem with adversarial
corruptions. We provide a simple combinatorial algorithm that can achieve a
regret of $\tilde{O}\left(C+d^2K/\Delta_{min}\right)$ where $C$ is the total
amount of corruptions, $d$ is the maximal number of arms one can play in each
round, $K$ is the number of arms. If one selects only one arm in each round, we
achieves a regret of $\tilde{O}\left(C+\sum_{\Delta_i>0}(1/\Delta_i)\right)$.
Our algorithm is combinatorial and improves on the previous combinatorial
algorithm by [Gupta et al., COLT2019] (their bound is
$\tilde{O}\left(KC+\sum_{\Delta_i>0}(1/\Delta_i)\right)$), and almost matches
the best known bounds obtained by [Zimmert et al., ICML2019] and [Zimmert and
Seldin, AISTATS2019] (up to logarithmic factor). Note that the algorithms in
[Zimmert et al., ICML2019] and [Zimmert and Seldin, AISTATS2019] require one to
solve complex convex programs while our algorithm is combinatorial, very easy
to implement, requires weaker assumptions and has very low oracle complexity
and running time. We also study the setting where we only get access to an
approximation oracle for the stochastic combinatorial semi-bandit problem. Our
algorithm achieves an (approximation) regret bound of
$\tilde{O}\left(d\sqrt{KT}\right)$. Our algorithm is very simple, only worse
than the best known regret bound by $\sqrt{d}$, and has much lower oracle
complexity than previous work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haike Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A3C-S: Automated Agent Accelerator Co-Search towards Efficient Deep Reinforcement Learning. (arXiv:2106.06577v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06577</id>
        <link href="http://arxiv.org/abs/2106.06577"/>
        <updated>2021-06-15T01:45:19.597Z</updated>
        <summary type="html"><![CDATA[Driven by the explosive interest in applying deep reinforcement learning
(DRL) agents to numerous real-time control and decision-making applications,
there has been a growing demand to deploy DRL agents to empower daily-life
intelligent devices, while the prohibitive complexity of DRL stands at odds
with limited on-device resources. In this work, we propose an Automated Agent
Accelerator Co-Search (A3C-S) framework, which to our best knowledge is the
first to automatically co-search the optimally matched DRL agents and
accelerators that maximize both test scores and hardware efficiency. Extensive
experiments consistently validate the superiority of our A3C-S over
state-of-the-art techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yonggan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chaojian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhongzhi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yingyan Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lightweight Cross-Lingual Sentence Representation Learning. (arXiv:2105.13856v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13856</id>
        <link href="http://arxiv.org/abs/2105.13856"/>
        <updated>2021-06-15T01:45:19.596Z</updated>
        <summary type="html"><![CDATA[Large-scale models for learning fixed-dimensional cross-lingual sentence
representations like LASER (Artetxe and Schwenk, 2019b) lead to significant
improvement in performance on downstream tasks. However, further increases and
modifications based on such large-scale models are usually impractical due to
memory limitations. In this work, we introduce a lightweight dual-transformer
architecture with just 2 layers for generating memory-efficient cross-lingual
sentence representations. We explore different training tasks and observe that
current cross-lingual training tasks leave a lot to be desired for this shallow
architecture. To ameliorate this, we propose a novel cross-lingual language
model, which combines the existing single-word masked language model with the
newly proposed cross-lingual token-level reconstruction task. We further
augment the training task by the introduction of two computationally-lite
sentence-level contrastive learning tasks to enhance the alignment of
cross-lingual sentence representation space, which compensates for the learning
bottleneck of the lightweight transformer for generative tasks. Our comparisons
with competing models on cross-lingual sentence retrieval and multilingual
document classification confirm the effectiveness of the newly proposed
training tasks for a shallow model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1"&gt;Zhuoyuan Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Prakhar Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1"&gt;Chenhui Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1"&gt;Sadao Kurohashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Representation Learning via Perceptual Similarity Metrics. (arXiv:2106.06620v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06620</id>
        <link href="http://arxiv.org/abs/2106.06620"/>
        <updated>2021-06-15T01:45:19.595Z</updated>
        <summary type="html"><![CDATA[A fundamental challenge in artificial intelligence is learning useful
representations of data that yield good performance on a downstream task,
without overfitting to spurious input features. Extracting such task-relevant
predictive information is particularly difficult for real-world datasets. In
this work, we propose Contrastive Input Morphing (CIM), a representation
learning framework that learns input-space transformations of the data to
mitigate the effect of irrelevant input features on downstream performance. Our
method leverages a perceptual similarity metric via a triplet loss to ensure
that the transformation preserves task-relevant information.Empirically, we
demonstrate the efficacy of our approach on tasks which typically suffer from
the presence of spurious correlations: classification with nuisance
information, out-of-distribution generalization, and preservation of subgroup
accuracies. We additionally show that CIM is complementary to other mutual
information-based representation learning techniques, and demonstrate that it
improves the performance of variational information bottleneck (VIB) when used
together.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taghanaki_S/0/1/0/all/0/1"&gt;Saeid Asgari Taghanaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1"&gt;Kristy Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1"&gt;Amir Khasahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1"&gt;Anirudh Goyal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PerSim: Data-Efficient Offline Reinforcement Learning with Heterogeneous Agents via Personalized Simulators. (arXiv:2102.06961v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06961</id>
        <link href="http://arxiv.org/abs/2102.06961"/>
        <updated>2021-06-15T01:45:19.590Z</updated>
        <summary type="html"><![CDATA[We consider offline reinforcement learning (RL) with heterogeneous agents
under severe data scarcity, i.e., we only observe a single historical
trajectory for every agent under an unknown, potentially sub-optimal policy. We
find that the performance of state-of-the-art offline and model-based RL
methods degrade significantly given such limited data availability, even for
commonly perceived "solved" benchmark settings such as "MountainCar" and
"CartPole". To address this challenge, we propose PerSim, a model-based offline
RL approach which first learns a personalized simulator for each agent by
collectively using the historical trajectories across all agents, prior to
learning a policy. We do so by positing that the transition dynamics across
agents can be represented as a latent function of latent factors associated
with agents, states, and actions; subsequently, we theoretically establish that
this function is well-approximated by a "low-rank" decomposition of separable
agent, state, and action latent functions. This representation suggests a
simple, regularized neural network architecture to effectively learn the
transition dynamics per agent, even with scarce, offline data. We perform
extensive experiments across several benchmark environments and RL methods. The
consistent improvement of our approach, measured in terms of both state
dynamics prediction and eventual reward, confirms the efficacy of our framework
in leveraging limited historical data to simultaneously learn personalized
policies across agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Anish Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alomar_A/0/1/0/all/0/1"&gt;Abdullah Alomar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alumootil_V/0/1/0/all/0/1"&gt;Varkey Alumootil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1"&gt;Devavrat Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1"&gt;Dennis Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cindy Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Segmentation Loss for Sketch Colorization. (arXiv:2102.06192v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06192</id>
        <link href="http://arxiv.org/abs/2102.06192"/>
        <updated>2021-06-15T01:45:19.589Z</updated>
        <summary type="html"><![CDATA[We introduce a new method for generating color images from sketches or edge
maps. Current methods either require some form of additional user-guidance or
are limited to the "paired" translation approach. We argue that segmentation
information could provide valuable guidance for sketch colorization. To this
end, we propose to leverage semantic image segmentation, as provided by a
general purpose panoptic segmentation network, to create an additional
adversarial loss function. Our loss function can be integrated to any baseline
GAN model. Our method is not limited to datasets that contain segmentation
labels, and it can be trained for "unpaired" translation tasks. We show the
effectiveness of our method on four different datasets spanning scene level
indoor, outdoor, and children book illustration images using qualitative,
quantitative and user study analysis. Our model improves its baseline up to 35
points on the FID metric. Our code and pretrained models can be found at
https://github.com/giddyyupp/AdvSegLoss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hicsonmez_S/0/1/0/all/0/1"&gt;Samet Hicsonmez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samet_N/0/1/0/all/0/1"&gt;Nermin Samet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1"&gt;Emre Akbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duygulu_P/0/1/0/all/0/1"&gt;Pinar Duygulu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks. (arXiv:2101.06969v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06969</id>
        <link href="http://arxiv.org/abs/2101.06969"/>
        <updated>2021-06-15T01:45:19.587Z</updated>
        <summary type="html"><![CDATA[Pre-trained models (PTMs) have been widely used in various downstream tasks.
The parameters of PTMs are distributed on the Internet and may suffer backdoor
attacks. In this work, we demonstrate the universal vulnerability of PTMs,
where fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary
downstream tasks. Specifically, attackers can add a simple pre-training task,
which restricts the output representations of trigger instances to pre-defined
vectors, namely neuron-level backdoor attack (NeuBA). If the backdoor
functionality is not eliminated during fine-tuning, the triggers can make the
fine-tuned model predict fixed labels by pre-defined vectors. In the
experiments of both natural language processing (NLP) and computer vision (CV),
we show that NeuBA absolutely controls the predictions for trigger instances
without any knowledge of downstream tasks. Finally, we apply several defense
methods to NeuBA and find that model pruning is a promising direction to resist
NeuBA by excluding backdoored neurons. Our findings sound a red alarm for the
wide use of PTMs. Our source code and models are available at
\url{https://github.com/thunlp/NeuBA}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1"&gt;Guangxuan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yongwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1"&gt;Tian Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Fanchao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yasheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction. (arXiv:2106.06935v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06935</id>
        <link href="http://arxiv.org/abs/2106.06935"/>
        <updated>2021-06-15T01:45:19.585Z</updated>
        <summary type="html"><![CDATA[Link prediction is a very fundamental task on graphs. Inspired by traditional
path-based methods, in this paper we propose a general and flexible
representation learning framework based on paths for link prediction.
Specifically, we define the representation of a pair of nodes as the
generalized sum of all path representations, with each path representation as
the generalized product of the edge representations in the path. Motivated by
the Bellman-Ford algorithm for solving the shortest path problem, we show that
the proposed path formulation can be efficiently solved by the generalized
Bellman-Ford algorithm. To further improve the capacity of the path
formulation, we propose the Neural Bellman-Ford Network (NBFNet), a general
graph neural network framework that solves the path formulation with learned
operators in the generalized Bellman-Ford algorithm. The NBFNet parameterizes
the generalized Bellman-Ford algorithm with 3 neural components, namely
INDICATOR, MESSAGE and AGGREGATE functions, which corresponds to the boundary
condition, multiplication operator, and summation operator respectively. The
NBFNet is very general, covers many traditional path-based methods, and can be
applied to both homogeneous graphs and multi-relational graphs (e.g., knowledge
graphs) in both transductive and inductive settings. Experiments on both
homogeneous graphs and knowledge graphs show that the proposed NBFNet
outperforms existing methods by a large margin in both transductive and
inductive settings, achieving new state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhaocheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zuobai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xhonneux_L/0/1/0/all/0/1"&gt;Louis-Pascal Xhonneux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DANCE: Enhancing saliency maps using decoys. (arXiv:2002.00526v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00526</id>
        <link href="http://arxiv.org/abs/2002.00526"/>
        <updated>2021-06-15T01:45:19.581Z</updated>
        <summary type="html"><![CDATA[Saliency methods can make deep neural network predictions more interpretable
by identifying a set of critical features in an input sample, such as pixels
that contribute most strongly to a prediction made by an image classifier.
Unfortunately, recent evidence suggests that many saliency methods poorly
perform, especially in situations where gradients are saturated, inputs contain
adversarial perturbations, or predictions rely upon inter-feature dependence.
To address these issues, we propose a framework that improves the robustness of
saliency methods by following a two-step procedure. First, we introduce a
perturbation mechanism that subtly varies the input sample without changing its
intermediate representations. Using this approach, we can gather a corpus of
perturbed data samples while ensuring that the perturbed and original input
samples follow the same distribution. Second, we compute saliency maps for the
perturbed samples and propose a new method to aggregate saliency maps. With
this design, we offset the gradient saturation influence upon interpretation.
From a theoretical perspective, we show the aggregated saliency map could not
only capture inter-feature dependence but, more importantly, robustify
interpretation against previously described adversarial perturbation methods.
Following our theoretical analysis, we present experimental results suggesting
that, both qualitatively and quantitatively, our saliency method outperforms
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wenbo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1"&gt;Xinyu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noble_W/0/1/0/all/0/1"&gt;William Stafford Noble&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-Implicit Neural Representation for Fonts. (arXiv:2106.06866v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06866</id>
        <link href="http://arxiv.org/abs/2106.06866"/>
        <updated>2021-06-15T01:45:19.572Z</updated>
        <summary type="html"><![CDATA[Fonts are ubiquitous across documents and come in a variety of styles. They
are either represented in a native vector format or rasterized to produce fixed
resolution images. In the first case, the non-standard representation prevents
benefiting from latest network architectures for neural representations; while,
in the latter case, the rasterized representation, when encoded via networks,
results in loss of data fidelity, as font-specific discontinuities like edges
and corners are difficult to represent using neural networks. Based on the
observation that complex fonts can be represented by a superposition of a set
of simpler occupancy functions, we introduce \textit{multi-implicits} to
represent fonts as a permutation-invariant set of learned implict functions,
without losing features (e.g., edges and corners). However, while
multi-implicits locally preserve font features, obtaining supervision in the
form of ground truth multi-channel signals is a problem in itself. Instead, we
propose how to train such a representation with only local supervision, while
the proposed neural architecture directly finds globally consistent
multi-implicits for font families. We extensively evaluate the proposed
representation for various tasks including reconstruction, interpolation, and
synthesis to demonstrate clear advantages with existing alternatives.
Additionally, the representation naturally enables glyph completion, wherein a
single characteristic font is used to synthesize a whole font family in the
target style.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_P/0/1/0/all/0/1"&gt;Pradyumna Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhifei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1"&gt;Matthew Fisher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hailin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaowen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy J. Mitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Privacy-preserving Deep Learning-based Network Intrusion Detection in Data Distribution Services. (arXiv:2106.06765v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06765</id>
        <link href="http://arxiv.org/abs/2106.06765"/>
        <updated>2021-06-15T01:45:19.564Z</updated>
        <summary type="html"><![CDATA[Data Distribution Service (DDS) is an innovative approach towards
communication in ICS/IoT infrastructure and robotics. Being based on the
cross-platform and cross-language API to be applicable in any computerised
device, it offers the benefits of modern programming languages and the
opportunities to develop more complex and advanced systems. However, the DDS
complexity equally increases its vulnerability, while the existing security
measures are limited to plug-ins and static rules, with the rest of the
security provided by third-party applications and operating system.
Specifically, traditional intrusion detection systems (IDS) do not detect any
anomalies in the publish/subscribe method. With the exponentially growing
global communication exchange, securing DDS is of the utmost importance to
futureproofing industrial, public, and even personal devices and systems. This
report presents an experimental work on the simulation of several specific
attacks against DDS, and the application of Deep Learning for their detection.
The findings show that even though Deep Learning allows to detect all simulated
attacks using only metadata analysis, their detection level varies, with some
of the advanced attacks being harder to detect. The limitations imposed by the
attempts to preserve privacy significantly decrease the detection rate. The
report also reviews the drawbacks and limitations of the Deep Learning approach
and proposes a set of selected solutions and configurations, that can further
improve the DDS security.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abaimov_S/0/1/0/all/0/1"&gt;Stanislav Abaimov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Achieving Near Instance-Optimality and Minimax-Optimality in Stochastic and Adversarial Linear Bandits Simultaneously. (arXiv:2102.05858v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05858</id>
        <link href="http://arxiv.org/abs/2102.05858"/>
        <updated>2021-06-15T01:45:19.554Z</updated>
        <summary type="html"><![CDATA[In this work, we develop linear bandit algorithms that automatically adapt to
different environments. By plugging a novel loss estimator into the
optimization problem that characterizes the instance-optimal strategy, our
first algorithm not only achieves nearly instance-optimal regret in stochastic
environments, but also works in corrupted environments with additional regret
being the amount of corruption, while the state-of-the-art (Li et al., 2019)
achieves neither instance-optimality nor the optimal dependence on the
corruption amount. Moreover, by equipping this algorithm with an adversarial
component and carefully-designed testings, our second algorithm additionally
enjoys minimax-optimal regret in completely adversarial environments, which is
the first of this kind to our knowledge. Finally, all our guarantees hold with
high probability, while existing instance-optimal guarantees only hold in
expectation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chung-Wei Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Chen-Yu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengxiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaojin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label Inference Attacks from Log-loss Scores. (arXiv:2105.08266v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08266</id>
        <link href="http://arxiv.org/abs/2105.08266"/>
        <updated>2021-06-15T01:45:19.545Z</updated>
        <summary type="html"><![CDATA[Log-loss (also known as cross-entropy loss) metric is ubiquitously used
across machine learning applications to assess the performance of
classification algorithms. In this paper, we investigate the problem of
inferring the labels of a dataset from single (or multiple) log-loss score(s),
without any other access to the dataset. Surprisingly, we show that for any
finite number of label classes, it is possible to accurately infer the labels
of the dataset from the reported log-loss score of a single carefully
constructed prediction vector if we allow arbitrary precision arithmetic.
Additionally, we present label inference algorithms (attacks) that succeed even
under addition of noise to the log-loss scores and under limited precision
arithmetic. All our algorithms rely on ideas from number theory and
combinatorics and require no model training. We run experimental simulations on
some real datasets to demonstrate the ease of running these attacks in
practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_A/0/1/0/all/0/1"&gt;Abhinav Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasiviswanathan_S/0/1/0/all/0/1"&gt;Shiva Prasad Kasiviswanathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zekun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feyisetan_O/0/1/0/all/0/1"&gt;Oluwaseyi Feyisetan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teissier_N/0/1/0/all/0/1"&gt;Nathanael Teissier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lattice protein design using Bayesian learning. (arXiv:2003.06601v5 [physics.bio-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.06601</id>
        <link href="http://arxiv.org/abs/2003.06601"/>
        <updated>2021-06-15T01:45:19.534Z</updated>
        <summary type="html"><![CDATA[Protein design is the inverse approach of the three-dimensional (3D)
structure prediction for elucidating the relationship between the 3D structures
and amino acid sequences. In general, the computation of the protein design
involves a double loop: a loop for amino acid sequence changes and a loop for
an exhaustive conformational search for each amino acid sequence. Herein, we
propose a novel statistical mechanical design method using Bayesian learning,
which can design lattice proteins without the exhaustive conformational search.
We consider a thermodynamic hypothesis of the evolution of proteins and apply
it to the prior distribution of amino acid sequences. Furthermore, we take the
water effect into account in view of the grand canonical picture. As a result,
on applying the 2D lattice hydrophobic-polar (HP) model, our design method
successfully finds an amino acid sequence for which the target conformation has
a unique ground state. However, the performance was not as good for the 3D
lattice HP models compared to the 2D models. The performance of the 3D model
improves on using a 20-letter lattice proteins. Furthermore, we find a strong
linearity between the chemical potential of water and the number of surface
residues, thereby revealing the relationship between protein structure and the
effect of water molecules. The advantage of our method is that it greatly
reduces computation time, because it does not require long calculations for the
partition function corresponding to an exhaustive conformational search. As our
method uses a general form of Bayesian learning and statistical mechanics and
is not limited to lattice proteins, the results presented here elucidate some
heuristics used successfully in previous protein design methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Takahashi_T/0/1/0/all/0/1"&gt;Tomoei Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chikenji_G/0/1/0/all/0/1"&gt;George Chikenji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tokita_K/0/1/0/all/0/1"&gt;Kei Tokita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Short-term forecasting of global solar irradiance with incomplete data. (arXiv:2106.06868v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06868</id>
        <link href="http://arxiv.org/abs/2106.06868"/>
        <updated>2021-06-15T01:45:19.498Z</updated>
        <summary type="html"><![CDATA[Accurate mechanisms for forecasting solar irradiance and insolation provide
important information for the planning of renewable energy and agriculture
projects as well as for environmental and socio-economical studies. This
research introduces a pipeline for the one-day ahead forecasting of solar
irradiance and insolation that only requires solar irradiance historical data
for training. Furthermore, our approach is able to deal with missing data since
it includes a data imputation state. In the prediction stage, we consider four
data-driven approaches: Autoregressive Integrated Moving Average (ARIMA),
Single Layer Feed Forward Network (SL-FNN), Multiple Layer Feed Forward Network
(FL-FNN), and Long Short-Term Memory (LSTM). The experiments are performed in a
real-world dataset collected with 12 Automatic Weather Stations (AWS) located
in the Nari\~no - Colombia. The results show that the neural network-based
models outperform ARIMA in most cases. Furthermore, LSTM exhibits better
performance in cloudy environments (where more randomness is expected).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoyos_Gomez_L/0/1/0/all/0/1"&gt;Laura S. Hoyos-G&amp;#xf3;mez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_Munoz_J/0/1/0/all/0/1"&gt;Jose F. Ruiz-Mu&amp;#xf1;oz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_Mendoza_B/0/1/0/all/0/1"&gt;Belizza J. Ruiz-Mendoza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compression of Deep Learning Models for Text: A Survey. (arXiv:2008.05221v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05221</id>
        <link href="http://arxiv.org/abs/2008.05221"/>
        <updated>2021-06-15T01:45:19.480Z</updated>
        <summary type="html"><![CDATA[In recent years, the fields of natural language processing (NLP) and
information retrieval (IR) have made tremendous progress thanksto deep learning
models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and
Long Short-Term Memory (LSTMs)networks, and Transformer [120] based models like
Bidirectional Encoder Representations from Transformers (BERT) [24],
GenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network
(MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer
transformer (T5) [95], T-NLG [98] and GShard [63]. But these models are
humongous in size. On the other hand,real world applications demand small model
size, low response times and low computational power wattage. In this survey,
wediscuss six different types of methods (Pruning, Quantization, Knowledge
Distillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic
Transformer based methods) for compression of such models to enable their
deployment in real industry NLP projects.Given the critical need of building
applications with efficient and small models, and the large amount of recently
published work inthis area, we believe that this survey organizes the plethora
of work done by the 'deep learning for NLP' community in the past fewyears and
presents it as a coherent story.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Manish Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1"&gt;Puneet Agrawal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarks, Algorithms, and Metrics for Hierarchical Disentanglement. (arXiv:2102.05185v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05185</id>
        <link href="http://arxiv.org/abs/2102.05185"/>
        <updated>2021-06-15T01:45:19.464Z</updated>
        <summary type="html"><![CDATA[In representation learning, there has been recent interest in developing
algorithms to disentangle the ground-truth generative factors behind a dataset,
and metrics to quantify how fully this occurs. However, these algorithms and
metrics often assume that both representations and ground-truth factors are
flat, continuous, and factorized, whereas many real-world generative processes
involve rich hierarchical structure, mixtures of discrete and continuous
variables with dependence between them, and even varying intrinsic
dimensionality. In this work, we develop benchmarks, algorithms, and metrics
for learning such hierarchical representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1"&gt;Andrew Slavin Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1"&gt;Finale Doshi-Velez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HPNet: Deep Primitive Segmentation Using Hybrid Representations. (arXiv:2105.10620v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10620</id>
        <link href="http://arxiv.org/abs/2105.10620"/>
        <updated>2021-06-15T01:45:19.463Z</updated>
        <summary type="html"><![CDATA[This paper introduces HPNet, a novel deep-learning approach for segmenting a
3D shape represented as a point cloud into primitive patches. The key to deep
primitive segmentation is learning a feature representation that can separate
points of different primitives. Unlike utilizing a single feature
representation, HPNet leverages hybrid representations that combine one learned
semantic descriptor, two spectral descriptors derived from predicted geometric
parameters, as well as an adjacency matrix that encodes sharp edges. Moreover,
instead of merely concatenating the descriptors, HPNet optimally combines
hybrid representations by learning combination weights. This weighting module
builds on the entropy of input features. The output primitive segmentation is
obtained from a mean-shift clustering module. Experimental results on benchmark
datasets ANSI and ABCParts show that HPNet leads to significant performance
gains from baseline approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Siming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhenpei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chongyang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haibin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vouga_E/0/1/0/all/0/1"&gt;Etienne Vouga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qixing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Breaking the Limit of Graph Neural Networks by Improving the Assortativity of Graphs with Local Mixing Patterns. (arXiv:2106.06586v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06586</id>
        <link href="http://arxiv.org/abs/2106.06586"/>
        <updated>2021-06-15T01:45:19.461Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have achieved tremendous success on multiple
graph-based learning tasks by fusing network structure and node features.
Modern GNN models are built upon iterative aggregation of neighbor's/proximity
features by message passing. Its prediction performance has been shown to be
strongly bounded by assortative mixing in the graph, a key property wherein
nodes with similar attributes mix/connect with each other. We observe that real
world networks exhibit heterogeneous or diverse mixing patterns and the
conventional global measurement of assortativity, such as global assortativity
coefficient, may not be a representative statistic in quantifying this mixing.
We adopt a generalized concept, node-level assortativity, one that is based at
the node level to better represent the diverse patterns and accurately quantify
the learnability of GNNs. We find that the prediction performance of a wide
range of GNN models is highly correlated with the node level assortativity. To
break this limit, in this work, we focus on transforming the input graph into a
computation graph which contains both proximity and structural information as
distinct type of edges. The resulted multi-relational graph has an enhanced
level of assortativity and, more importantly, preserves rich information from
the original graph. We then propose to run GNNs on this computation graph and
show that adaptively choosing between structure and proximity leads to improved
performance under diverse mixing. Empirically, we show the benefits of adopting
our transformation framework for semi-supervised node classification task on a
variety of real world graph learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1"&gt;Susheel Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Budde_V/0/1/0/all/0/1"&gt;Vinith Budde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neville_J/0/1/0/all/0/1"&gt;Jennifer Neville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianzhu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedScale: Benchmarking Model and System Performance of Federated Learning. (arXiv:2105.11367v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11367</id>
        <link href="http://arxiv.org/abs/2105.11367"/>
        <updated>2021-06-15T01:45:19.459Z</updated>
        <summary type="html"><![CDATA[We present FedScale, a diverse set of challenging and realistic benchmark
datasets to facilitate scalable, comprehensive, and reproducible federated
learning (FL) research. FedScale datasets are large-scale, encompassing a
diverse range of important FL tasks, such as image classification, object
detection, language modeling, speech recognition, and reinforcement learning.
For each dataset, we provide a unified evaluation protocol using realistic data
splits and evaluation metrics. To meet the pressing need for reproducing
realistic FL at scale, we have also built an efficient evaluation platform to
simplify and standardize the process of FL experimental setup and model
evaluation. Our evaluation platform provides flexible APIs to implement new FL
algorithms and includes new execution backends with minimal developer efforts.
Finally, we perform indepth benchmark experiments on these datasets. Our
experiments suggest fruitful opportunities in heterogeneity-aware
co-optimizations of the system and statistical efficiency under realistic FL
characteristics. FedScale is open-source with permissive licenses and actively
maintained,1 and we welcome feedback and contributions from the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1"&gt;Fan Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yinwei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiangfeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Mosharaf Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On The Radon-Nikodym Spectral Approach With Optimal Clustering. (arXiv:1906.00460v16 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.00460</id>
        <link href="http://arxiv.org/abs/1906.00460"/>
        <updated>2021-06-15T01:45:19.457Z</updated>
        <summary type="html"><![CDATA[Problems of interpolation, classification, and clustering are considered. In
the tenets of Radon--Nikodym approach $\langle f(\mathbf{x})\psi^2 \rangle /
\langle\psi^2\rangle$, where the $\psi(\mathbf{x})$ is a linear function on
input attributes, all the answers are obtained from a generalized eigenproblem
$|f|\psi^{[i]}\rangle = \lambda^{[i]} |\psi^{[i]}\rangle$. The solution to the
interpolation problem is a regular Radon-Nikodym derivative. The solution to
the classification problem requires prior and posterior probabilities that are
obtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian
approach new observations change only outcome probabilities, in the
Radon-Nikodym approach not only outcome probabilities but also the probability
space $|\psi^{[i]}\rangle$ change with new observations. This is a remarkable
feature of the approach: both the probabilities and the probability space are
constructed from the data. The Lebesgue quadrature technique can be also
applied to the optimal clustering problem. The problem is solved by
constructing a Gaussian quadrature on the Lebesgue measure. A distinguishing
feature of the Radon-Nikodym approach is the knowledge of the invariant group:
all the answers are invariant relatively any non-degenerated linear transform
of input vector $\mathbf{x}$ components. A software product implementing the
algorithms of interpolation, classification, and optimal clustering is
available from the authors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malyshkin_V/0/1/0/all/0/1"&gt;Vladislav Gennadievich Malyshkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search. (arXiv:2010.07003v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07003</id>
        <link href="http://arxiv.org/abs/2010.07003"/>
        <updated>2021-06-15T01:45:19.445Z</updated>
        <summary type="html"><![CDATA[Despite transformers' impressive accuracy, their computational cost is often
prohibitive to use with limited computational resources. Most previous
approaches to improve inference efficiency require a separate model for each
possible computational budget. In this paper, we extend PoWER-BERT (Goyal et
al., 2020) and propose Length-Adaptive Transformer that can be used for various
inference scenarios after one-shot training. We train a transformer with
LengthDrop, a structural variant of dropout, which stochastically determines a
sequence length at each layer. We then conduct a multi-objective evolutionary
search to find a length configuration that maximizes the accuracy and minimizes
the efficiency metric under any given computational budget. Additionally, we
significantly extend the applicability of PoWER-BERT beyond sequence-level
classification into token-level classification with Drop-and-Restore process
that drops word-vectors temporarily in intermediate layers and restores at the
last layer if necessary. We empirically verify the utility of the proposed
approach by demonstrating the superior accuracy-efficiency trade-off under
various setups, including span-based question answering and text
classification. Code is available at
https://github.com/clovaai/length-adaptive-transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gyuwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Graph Neural Networks and Their Applications in Power Systems. (arXiv:2101.10025v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10025</id>
        <link href="http://arxiv.org/abs/2101.10025"/>
        <updated>2021-06-15T01:45:19.437Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have revolutionized many machine learning tasks in power
systems, ranging from pattern recognition to signal processing. The data in
these tasks is typically represented in Euclidean domains. Nevertheless, there
is an increasing number of applications in power systems, where data are
collected from non-Euclidean domains and represented as graph-structured data
with high dimensional features and interdependency among nodes. The complexity
of graph-structured data has brought significant challenges to the existing
deep neural networks defined in Euclidean domains. Recently, many publications
generalizing deep neural networks for graph-structured data in power systems
have emerged. In this paper, a comprehensive overview of graph neural networks
(GNNs) in power systems is proposed. Specifically, several classical paradigms
of GNNs structures (e.g., graph convolutional networks) are summarized, and key
applications in power systems, such as fault scenario application, time series
prediction, power flow calculation, and data generation are reviewed in detail.
Furthermore, main issues and some research trends about the applications of
GNNs in power systems are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1"&gt;Wenlong Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bak_Jensen_B/0/1/0/all/0/1"&gt;Birgitte Bak-Jensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pillai_J/0/1/0/all/0/1"&gt;Jayakrishnan Radhakrishna Pillai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuelong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yusen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Coordination As a Realistic Scenario for Lifelong Learning. (arXiv:2103.03216v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03216</id>
        <link href="http://arxiv.org/abs/2103.03216"/>
        <updated>2021-06-15T01:45:19.414Z</updated>
        <summary type="html"><![CDATA[Current deep reinforcement learning (RL) algorithms are still highly
task-specific and lack the ability to generalize to new environments. Lifelong
learning (LLL), however, aims at solving multiple tasks sequentially by
efficiently transferring and using knowledge between tasks. Despite a surge of
interest in lifelong RL in recent years, the lack of a realistic testbed makes
robust evaluation of LLL algorithms difficult. Multi-agent RL (MARL), on the
other hand, can be seen as a natural scenario for lifelong RL due to its
inherent non-stationarity, since the agents' policies change over time. In this
work, we introduce a multi-agent lifelong learning testbed that supports both
zero-shot and few-shot settings. Our setup is based on Hanabi -- a
partially-observable, fully cooperative multi-agent game that has been shown to
be challenging for zero-shot coordination. Its large strategy space makes it a
desirable environment for lifelong RL tasks. We evaluate several recent MARL
methods, and benchmark state-of-the-art LLL algorithms in limited memory and
computation regimes to shed light on their strengths and weaknesses. This
continual learning paradigm also provides us with a pragmatic way of going
beyond centralized training which is the most commonly used training protocol
in MARL. We empirically show that the agents trained in our setup are able to
coordinate well with unseen agents, without any additional assumptions made by
previous works. The code and all pre-trained models are available at
https://github.com/chandar-lab/Lifelong-Hanabi.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nekoei_H/0/1/0/all/0/1"&gt;Hadi Nekoei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badrinaaraayanan_A/0/1/0/all/0/1"&gt;Akilesh Badrinaaraayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Sarath Chandar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dirty Road Can Attack: Security of Deep Learning based Automated Lane Centering under Physical-World Attack. (arXiv:2009.06701v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06701</id>
        <link href="http://arxiv.org/abs/2009.06701"/>
        <updated>2021-06-15T01:45:19.398Z</updated>
        <summary type="html"><![CDATA[Automated Lane Centering (ALC) systems are convenient and widely deployed
today, but also highly security and safety critical. In this work, we are the
first to systematically study the security of state-of-the-art deep learning
based ALC systems in their designed operational domains under physical-world
adversarial attacks. We formulate the problem with a safety-critical attack
goal, and a novel and domain-specific attack vector: dirty road patches. To
systematically generate the attack, we adopt an optimization-based approach and
overcome domain-specific design challenges such as camera frame
inter-dependencies due to attack-influenced vehicle control, and the lack of
objective function design for lane detection models.

We evaluate our attack on a production ALC using 80 scenarios from real-world
driving traces. The results show that our attack is highly effective with over
97.5% success rates and less than 0.903 sec average success time, which is
substantially lower than the average driver reaction time. This attack is also
found (1) robust to various real-world factors such as lighting conditions and
view angles, (2) general to different model designs, and (3) stealthy from the
driver's view. To understand the safety impacts, we conduct experiments using
software-in-the-loop simulation and attack trace injection in a real vehicle.
The results show that our attack can cause a 100% collision rate in different
scenarios, including when tested with common safety features such as automatic
emergency braking. We also evaluate and discuss defenses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1"&gt;Takami Sato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Junjie Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Ningfei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Yunhan Jack Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qi Alfred Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Spiking Neural Networks. (arXiv:2106.06579v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06579</id>
        <link href="http://arxiv.org/abs/2106.06579"/>
        <updated>2021-06-15T01:45:19.397Z</updated>
        <summary type="html"><![CDATA[As neural networks get widespread adoption in resource-constrained embedded
devices, there is a growing need for low-power neural systems. Spiking Neural
Networks (SNNs)are emerging to be an energy-efficient alternative to the
traditional Artificial Neural Networks (ANNs) which are known to be
computationally intensive. From an application perspective, as federated
learning involves multiple energy-constrained devices, there is a huge scope to
leverage energy efficiency provided by SNNs. Despite its importance, there has
been little attention on training SNNs on a large-scale distributed system like
federated learning. In this paper, we bring SNNs to a more realistic federated
learning scenario. Specifically, we propose a federated learning framework for
decentralized and privacy-preserving training of SNNs. To validate the proposed
federated learning framework, we experimentally evaluate the advantages of SNNs
on various aspects of federated learning with CIFAR10 and CIFAR100 benchmarks.
We observe that SNNs outperform ANNs in terms of overall accuracy by over 15%
when the data is distributed across a large number of clients in the federation
while providing up to5.3x energy efficiency. In addition to efficiency, we also
analyze the sensitivity of the proposed federated SNN framework to data
distribution among the clients, stragglers, and gradient noise and perform a
comprehensive comparison with ANNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesha_Y/0/1/0/all/0/1"&gt;Yeshwanth Venkatesha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngeun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tassiulas_L/0/1/0/all/0/1"&gt;Leandros Tassiulas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1"&gt;Priyadarshini Panda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Artificial Intelligence for Manufacturing Cost Estimation and Machining Feature Visualization. (arXiv:2010.14824v2 [cs.CG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14824</id>
        <link href="http://arxiv.org/abs/2010.14824"/>
        <updated>2021-06-15T01:45:19.390Z</updated>
        <summary type="html"><![CDATA[Studies on manufacturing cost prediction based on deep learning have begun in
recent years, but the cost prediction rationale cannot be explained because the
models are still used as a black box. This study aims to propose a
manufacturing cost prediction process for 3D computer-aided design (CAD) models
using explainable artificial intelligence. The proposed process can visualize
the machining features of the 3D CAD model that are influencing the increase in
manufacturing costs. The proposed process consists of (1) data collection and
pre-processing, (2) 3D deep learning architecture exploration, and (3)
visualization to explain the prediction results. The proposed deep learning
model shows high predictability of manufacturing cost for the computer
numerical control (CNC) machined parts. In particular, using 3D
gradient-weighted class activation mapping proves that the proposed model not
only can detect the CNC machining features but also can differentiate the
machining difficulty for the same feature. Using the proposed process, we can
provide a design guidance to engineering designers in reducing manufacturing
costs during the conceptual design phase. We can also provide real-time
quotations and redesign proposals to online manufacturing platform customers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1"&gt;Soyoung Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_N/0/1/0/all/0/1"&gt;Namwoo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Super-Resolution Transformer. (arXiv:2106.06847v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06847</id>
        <link href="http://arxiv.org/abs/2106.06847"/>
        <updated>2021-06-15T01:45:19.384Z</updated>
        <summary type="html"><![CDATA[Video super-resolution (VSR), with the aim to restore a high-resolution video
from its corresponding low-resolution version, is a spatial-temporal sequence
prediction problem. Recently, Transformer has been gaining popularity due to
its parallel computing ability for sequence-to-sequence modeling. Thus, it
seems to be straightforward to apply the vision Transformer to solve VSR.
However, the typical block design of Transformer with a fully connected
self-attention layer and a token-wise feed-forward layer does not fit well for
VSR due to the following two reasons. First, the fully connected self-attention
layer neglects to exploit the data locality because this layer relies on linear
layers to compute attention maps. Second, the token-wise feed-forward layer
lacks the feature alignment which is important for VSR since this layer
independently processes each of the input token embeddings without any
interaction among them. In this paper, we make the first attempt to adapt
Transformer for VSR. Specifically, to tackle the first issue, we present a
spatial-temporal convolutional self-attention layer with a theoretical
understanding to exploit the locality information. For the second issue, we
design a bidirectional optical flow-based feed-forward layer to discover the
correlations across different video frames and also align features. Extensive
experiments on several benchmark datasets demonstrate the effectiveness of our
proposed method. The code will be available at
https://github.com/caojiezhang/VSR-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jiezhang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yawei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantitative Understanding of VAE as a Non-linearly Scaled Isometric Embedding. (arXiv:2007.15190v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15190</id>
        <link href="http://arxiv.org/abs/2007.15190"/>
        <updated>2021-06-15T01:45:19.378Z</updated>
        <summary type="html"><![CDATA[Variational autoencoder (VAE) estimates the posterior parameters (mean and
variance) of latent variables corresponding to each input data. While it is
used for many tasks, the transparency of the model is still an underlying
issue. This paper provides a quantitative understanding of VAE property through
the differential geometric and information-theoretic interpretations of VAE.
According to the Rate-distortion theory, the optimal transform coding is
achieved by using an orthonormal transform with PCA basis where the transform
space is isometric to the input. Considering the analogy of transform coding to
VAE, we clarify theoretically and experimentally that VAE can be mapped to an
implicit isometric embedding with a scale factor derived from the posterior
parameter. As a result, we can estimate the data probabilities in the input
space from the prior, loss metrics, and corresponding posterior parameters, and
further, the quantitative importance of each latent variable can be evaluated
like the eigenvalue of PCA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nakagawa_A/0/1/0/all/0/1"&gt;Akira Nakagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kato_K/0/1/0/all/0/1"&gt;Keizo Kato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network. (arXiv:2106.06555v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06555</id>
        <link href="http://arxiv.org/abs/2106.06555"/>
        <updated>2021-06-15T01:45:19.372Z</updated>
        <summary type="html"><![CDATA[Knowledge Graph (KG) completion research usually focuses on densely connected
benchmark datasets that are not representative of real KGs. We curate two KG
datasets that include biomedical and encyclopedic knowledge and use an existing
commonsense KG dataset to explore KG completion in the more realistic setting
where dense connectivity is not guaranteed. We develop a deep convolutional
network that utilizes textual entity representations and demonstrate that our
model outperforms recent KG completion methods in this challenging setting. We
find that our model's performance improvements stem primarily from its
robustness to sparsity. We then distill the knowledge from the convolutional
network into a student network that re-ranks promising candidate entities. This
re-ranking stage leads to further improvements in performance and demonstrates
the effectiveness of entity re-ranking for KG completion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lovelace_J/0/1/0/all/0/1"&gt;Justin Lovelace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newman_Griffis_D/0/1/0/all/0/1"&gt;Denis Newman-Griffis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1"&gt;Shikhar Vashishth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1"&gt;Jill Fain Lehman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1"&gt;Carolyn Penstein Ros&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decreasing scaling transition from adaptive gradient descent to stochastic gradient descent. (arXiv:2106.06749v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06749</id>
        <link href="http://arxiv.org/abs/2106.06749"/>
        <updated>2021-06-15T01:45:19.354Z</updated>
        <summary type="html"><![CDATA[Currently, researchers have proposed the adaptive gradient descent algorithm
and its variants, such as AdaGrad, RMSProp, Adam, AmsGrad, etc. Although these
algorithms have a faster speed in the early stage, the generalization ability
in the later stage of training is often not as good as the stochastic gradient
descent. Recently, some researchers have combined the adaptive gradient descent
and stochastic gradient descent to obtain the advantages of both and achieved
good results. Based on this research, we propose a decreasing scaling
transition from adaptive gradient descent to stochastic gradient descent
method(DSTAda). For the training stage of the stochastic gradient descent, we
use a learning rate that decreases linearly with the number of iterations
instead of a constant learning rate. We achieve a smooth and stable transition
from adaptive gradient descent to stochastic gradient descent through scaling.
At the same time, we give a theoretical proof of the convergence of DSTAda
under the framework of online learning. Our experimental results show that the
DSTAda algorithm has a faster convergence speed, higher accuracy, and better
stability and robustness. Our implementation is available at:
https://github.com/kunzeng/DSTAdam.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1"&gt;Kun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinlan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhixia Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dongpo Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Optimization for Multilingual Translation with Imbalanced Data. (arXiv:2104.07639v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07639</id>
        <link href="http://arxiv.org/abs/2104.07639"/>
        <updated>2021-06-15T01:45:19.347Z</updated>
        <summary type="html"><![CDATA[Multilingual models are parameter-efficient with the prospect improving
low-resource languages by leveraging crosslingual transfer. Despite recent
advance in massive multilingual translation with ever-growing model and data,
how to effectively train multilingual models has not been well understood. In
this paper, we show that a common situation in multilingual training, data
imbalance among languages, poses optimization tension between high resource and
low resource languages where the found multilingual solution is often
sub-optimal for low resources. We show that common training method which
upsamples low resources can not robustly optimize population loss with risks of
either underfitting high resource languages or overfitting low resource ones.
Drawing on recent findings on the geometry of loss landscape and its effect on
generalization, we propose a principled optimization algorithm, Curvature Aware
Task Scaling (CATS), which adaptively rescales gradients from different tasks
with a meta objective of guiding multilingual training to low-curvature
neighborhoods with uniformly low loss for all languages. We ran experiments on
common benchmarks (TED, WMT and OPUS-100) with varying degrees of data
imbalance. CATS effectively improved multilingual optimization and as a result
demonstrated consistent gains on low resources ( to BLEU) without hurting high
resources. In addition, CATS is robust to overparameterization and large batch
size training, making it a promising training method for massive multilingual
models that truly improve low resource languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1"&gt;Hongyu Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tracking Peaceful Tractors on Social Media -- XAI-enabled analysis of Red Fort Riots 2021. (arXiv:2104.13352v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13352</id>
        <link href="http://arxiv.org/abs/2104.13352"/>
        <updated>2021-06-15T01:45:19.340Z</updated>
        <summary type="html"><![CDATA[On 26 January 2021, India witnessed a national embarrassment from the
demographic least expected from - farmers. People across the nation watched in
horror as a pseudo-patriotic mob of farmers stormed capital Delhi and
vandalized the national pride- Red Fort. Investigations that followed the event
revealed the existence of a social media trail that led to the likes of such an
event. Consequently, it became essential and necessary to archive this trail
for social media analysis - not only to understand the bread-crumbs that are
dispersed across the trail but also to visualize the role played by
misinformation and fake news in this event. In this paper, we propose the
tractor2twitter dataset which contains around 0.05 million tweets that were
posted before, during, and after this event. Also, we benchmark our dataset
with an Explainable AI ML model for classification of each tweet into either of
the three categories - disinformation, misinformation, and opinion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Ajay Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_B/0/1/0/all/0/1"&gt;Basant Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling transition from momentum stochastic gradient descent to plain stochastic gradient descent. (arXiv:2106.06753v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06753</id>
        <link href="http://arxiv.org/abs/2106.06753"/>
        <updated>2021-06-15T01:45:19.338Z</updated>
        <summary type="html"><![CDATA[The plain stochastic gradient descent and momentum stochastic gradient
descent have extremely wide applications in deep learning due to their simple
settings and low computational complexity. The momentum stochastic gradient
descent uses the accumulated gradient as the updated direction of the current
parameters, which has a faster training speed. Because the direction of the
plain stochastic gradient descent has not been corrected by the accumulated
gradient. For the parameters that currently need to be updated, it is the
optimal direction, and its update is more accurate. We combine the advantages
of the momentum stochastic gradient descent with fast training speed and the
plain stochastic gradient descent with high accuracy, and propose a scaling
transition from momentum stochastic gradient descent to plain stochastic
gradient descent(TSGD) method. At the same time, a learning rate that decreases
linearly with the iterations is used instead of a constant learning rate. The
TSGD algorithm has a larger step size in the early stage to speed up the
training, and training with a smaller step size in the later stage can steadily
converge. Our experimental results show that the TSGD algorithm has faster
training speed, higher accuracy and better stability. Our implementation is
available at: https://github.com/kunzeng/TSGD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1"&gt;Kun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinlan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhixia Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dongpo Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Omnidirectional Transfer for Quasilinear Lifelong Learning. (arXiv:2004.12908v7 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.12908</id>
        <link href="http://arxiv.org/abs/2004.12908"/>
        <updated>2021-06-15T01:45:19.332Z</updated>
        <summary type="html"><![CDATA[In biological learning, data are used to improve performance not only on the
current task, but also on previously encountered and as yet unencountered
tasks. In contrast, classical machine learning starts from a blank slate, or
tabula rasa, using data only for the single task at hand. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
catastrophic forgetting). Many recent approaches for continual or lifelong
learning have attempted to maintain performance given new tasks. But striving
to avoid forgetting sets the goal unnecessarily low: the goal of lifelong
learning, whether biological or artificial, should be to improve performance on
all tasks (including past and future) with any new data. We propose
omnidirectional transfer learning algorithms, which includes two special cases
of interest: decision forests and deep networks. Our key insight is the
development of the omni-voter layer, which ensembles representations learned
independently on all tasks to jointly decide how to proceed on any given new
data point, thereby improving performance on both past and future tasks. Our
algorithms demonstrate omnidirectional transfer in a variety of simulated and
real data scenarios, including tabular data, image data, spoken data, and
adversarial tasks. Moreover, they do so with quasilinear space and time
complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1"&gt;Joshua T. Vogelstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dey_J/0/1/0/all/0/1"&gt;Jayanta Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1"&gt;Hayden S. Helm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1"&gt;Will LeVine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1"&gt;Ronak D. Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geisa_A/0/1/0/all/0/1"&gt;Ali Geisa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1"&gt;Gido M. van de Ven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1"&gt;Emily Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chenyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Weiwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tower_B/0/1/0/all/0/1"&gt;Bryan Tower&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larson_J/0/1/0/all/0/1"&gt;Jonathan Larson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Christopher M. White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1"&gt;Carey E. Priebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving weakly supervised sound event detection with self-supervised auxiliary tasks. (arXiv:2106.06858v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.06858</id>
        <link href="http://arxiv.org/abs/2106.06858"/>
        <updated>2021-06-15T01:45:19.330Z</updated>
        <summary type="html"><![CDATA[While multitask and transfer learning has shown to improve the performance of
neural networks in limited data settings, they require pretraining of the model
on large datasets beforehand. In this paper, we focus on improving the
performance of weakly supervised sound event detection in low data and noisy
settings simultaneously without requiring any pretraining task. To that extent,
we propose a shared encoder architecture with sound event detection as a
primary task and an additional secondary decoder for a self-supervised
auxiliary task. We empirically evaluate the proposed framework for weakly
supervised sound event detection on a remix dataset of the DCASE 2019 task 1
acoustic scene data with DCASE 2018 Task 2 sounds event data under 0, 10 and 20
dB SNR. To ensure we retain the localisation information of multiple sound
events, we propose a two-step attention pooling mechanism that provides a
time-frequency localisation of multiple audio events in the clip. The proposed
framework with two-step attention outperforms existing benchmark models by
22.3%, 12.8%, 5.9% on 0, 10 and 20 dB SNR respectively. We carry out an
ablation study to determine the contribution of the auxiliary task and two-step
attention pooling to the SED performance improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Deshmukh_S/0/1/0/all/0/1"&gt;Soham Deshmukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raj_B/0/1/0/all/0/1"&gt;Bhiksha Raj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rita Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment. (arXiv:2101.00148v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00148</id>
        <link href="http://arxiv.org/abs/2101.00148"/>
        <updated>2021-06-15T01:45:19.323Z</updated>
        <summary type="html"><![CDATA[Bilingual lexicons map words in one language to their translations in
another, and are typically induced by learning linear projections to align
monolingual word embedding spaces. In this paper, we show it is possible to
produce much higher quality lexicons with methods that combine (1) unsupervised
bitext mining and (2) unsupervised word alignment. Directly applying a pipeline
that uses recent algorithms for both subproblems significantly improves induced
lexicon quality and further gains are possible by learning to filter the
resulting lexical entries, with both unsupervised and semi-supervised schemes.
Our final model outperforms the state of the art on the BUCC 2020 shared task
by 14 $F_1$ points averaged over 12 language pairs, while also providing a more
interpretable approach that allows for rich reasoning of word meaning in
context. Further analysis of our output and the standard reference lexicons
suggests they are of comparable quality, and new benchmarks may be needed to
measure further progress on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Haoyue Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sida I. Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Overview on 5G-and-Beyond Networks with UAVs: From Communications to Sensing and Intelligence. (arXiv:2010.09317v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09317</id>
        <link href="http://arxiv.org/abs/2010.09317"/>
        <updated>2021-06-15T01:45:19.304Z</updated>
        <summary type="html"><![CDATA[Due to the advancements in cellular technologies and the dense deployment of
cellular infrastructure, integrating unmanned aerial vehicles (UAVs) into the
fifth-generation (5G) and beyond cellular networks is a promising solution to
achieve safe UAV operation as well as enabling diversified applications with
mission-specific payload data delivery. In particular, 5G networks need to
support three typical usage scenarios, namely, enhanced mobile broadband
(eMBB), ultra-reliable low-latency communications (URLLC), and massive
machine-type communications (mMTC). On the one hand, UAVs can be leveraged as
cost-effective aerial platforms to provide ground users with enhanced
communication services by exploiting their high cruising altitude and
controllable maneuverability in three-dimensional (3D) space. On the other
hand, providing such communication services simultaneously for both UAV and
ground users poses new challenges due to the need for ubiquitous 3D signal
coverage as well as the strong air-ground network interference. Besides the
requirement of high-performance wireless communications, the ability to support
effective and efficient sensing as well as network intelligence is also
essential for 5G-and-beyond 3D heterogeneous wireless networks with coexisting
aerial and ground users. In this paper, we provide a comprehensive overview of
the latest research efforts on integrating UAVs into cellular networks, with an
emphasis on how to exploit advanced techniques (e.g., intelligent reflecting
surface, short packet transmission, energy harvesting, joint communication and
radar sensing, and edge intelligence) to meet the diversified service
requirements of next-generation wireless systems. Moreover, we highlight
important directions for further investigation in future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qingqing Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1"&gt;Yong Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1"&gt;Derrick Wing Kwan Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Dhahir_N/0/1/0/all/0/1"&gt;Naofal Al-Dhahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schober_R/0/1/0/all/0/1"&gt;Robert Schober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swindlehurst_A/0/1/0/all/0/1"&gt;A. Lee Swindlehurst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bellman-consistent Pessimism for Offline Reinforcement Learning. (arXiv:2106.06926v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06926</id>
        <link href="http://arxiv.org/abs/2106.06926"/>
        <updated>2021-06-15T01:45:19.298Z</updated>
        <summary type="html"><![CDATA[The use of pessimism, when reasoning about datasets lacking exhaustive
exploration has recently gained prominence in offline reinforcement learning.
Despite the robustness it adds to the algorithm, overly pessimistic reasoning
can be equally damaging in precluding the discovery of good policies, which is
an issue for the popular bonus-based pessimism. In this paper, we introduce the
notion of Bellman-consistent pessimism for general function approximation:
instead of calculating a point-wise lower bound for the value function, we
implement pessimism at the initial state over the set of functions consistent
with the Bellman equations. Our theoretical guarantees only require Bellman
closedness as standard in the exploratory setting, in which case bonus-based
pessimism fails to provide guarantees. Even in the special case of linear MDPs
where stronger function-approximation assumptions hold, our result improves
upon a recent bonus-based approach by $\mathcal{O}(d)$ in its sample complexity
when the action space is finite. Remarkably, our algorithms automatically adapt
to the best bias-variance tradeoff in the hindsight, whereas most prior
approaches require tuning extra hyperparameters a priori.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tengyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Ching-An Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1"&gt;Nan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1"&gt;Paul Mineiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Alekh Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Independent Prototype Propagation for Zero-Shot Compositionality. (arXiv:2106.00305v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00305</id>
        <link href="http://arxiv.org/abs/2106.00305"/>
        <updated>2021-06-15T01:45:19.291Z</updated>
        <summary type="html"><![CDATA[Humans are good at compositional zero-shot reasoning; someone who has never
seen a zebra before could nevertheless recognize one when we tell them it looks
like a horse with black and white stripes. Machine learning systems, on the
other hand, usually leverage spurious correlations in the training data, and
while such correlations can help recognize objects in context, they hurt
generalization. To be able to deal with underspecified datasets while still
leveraging contextual clues during classification, we propose ProtoProp, a
novel prototype propagation graph method. First we learn prototypical
representations of objects (e.g., zebra) that are conditionally independent
w.r.t. their attribute labels (e.g., stripes) and vice versa. Next we propagate
the independent prototypes through a compositional graph, to learn
compositional prototypes of novel attribute-object combinations that reflect
the dependencies of the target distribution. The method does not rely on any
external data, such as class hierarchy graphs or pretrained word embeddings. We
evaluate our approach on AO-Clever, a synthetic and strongly visual dataset
with clean labels, and UT-Zappos, a noisy real-world dataset of fine-grained
shoe types. We show that in the generalized compositional zero-shot setting we
outperform state-of-the-art results, and through ablations we show the
importance of each part of the method and their contribution to the final
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruis_F/0/1/0/all/0/1"&gt;Frank Ruis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1"&gt;Gertjan Burghouts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bucur_D/0/1/0/all/0/1"&gt;Doina Bucur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverting Adversarially Robust Networks for Image Synthesis. (arXiv:2106.06927v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06927</id>
        <link href="http://arxiv.org/abs/2106.06927"/>
        <updated>2021-06-15T01:45:19.290Z</updated>
        <summary type="html"><![CDATA[Recent research in adversarially robust classifiers suggests their
representations tend to be aligned with human perception, which makes them
attractive for image synthesis and restoration applications. Despite favorable
empirical results on a few downstream tasks, their advantages are limited to
slow and sensitive optimization-based techniques. Moreover, their use on
generative models remains unexplored. This work proposes the use of robust
representations as a perceptual primitive for feature inversion models, and
show its benefits with respect to standard non-robust image features. We
empirically show that adopting robust representations as an image prior
significantly improves the reconstruction accuracy of CNN-based feature
inversion models. Furthermore, it allows reconstructing images at multiple
scales out-of-the-box. Following these findings, we propose an
encoding-decoding network based on robust representations and show its
advantages for applications such as anomaly detection, style transfer and image
denoising.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rojas_Gomez_R/0/1/0/all/0/1"&gt;Renan A. Rojas-Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_R/0/1/0/all/0/1"&gt;Raymond A. Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1"&gt;Minh N. Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lite-FPN for Keypoint-based Monocular 3D Object Detection. (arXiv:2105.00268v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00268</id>
        <link href="http://arxiv.org/abs/2105.00268"/>
        <updated>2021-06-15T01:45:19.277Z</updated>
        <summary type="html"><![CDATA[3D object detection with a single image is an essential and challenging task
for autonomous driving. Recently, keypoint-based monocular 3D object detection
has made tremendous progress and achieved great speed-accuracy trade-off.
However, there still exists a huge gap with LIDAR-based methods in terms of
accuracy. To improve their performance without sacrificing efficiency, we
propose a sort of lightweight feature pyramid network called Lite-FPN to
achieve multi-scale feature fusion in an effective and efficient way, which can
boost the multi-scale detection capability of keypoint-based detectors.
Besides, the misalignment between classification score and localization
precision is further relieved by introducing a novel regression loss named
attention loss. With the proposed loss, predictions with high confidence but
poor localization are treated with more attention during the training phase.
Comparative experiments based on several state-of-the-art keypoint-based
detectors on the KITTI dataset show that our proposed methods manage to achieve
significant improvements in both accuracy and frame rate. The code and
pretrained models will be released at
\url{https://github.com/yanglei18/Lite-FPN}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Li Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Minghan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepShift: Towards Multiplication-Less Neural Networks. (arXiv:1905.13298v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.13298</id>
        <link href="http://arxiv.org/abs/1905.13298"/>
        <updated>2021-06-15T01:45:19.259Z</updated>
        <summary type="html"><![CDATA[The high computation, memory, and power budgets of inferring convolutional
neural networks (CNNs) are major bottlenecks of model deployment to edge
computing platforms, e.g., mobile devices and IoT. Moreover, training CNNs is
time and energy-intensive even on high-grade servers. Convolution layers and
fully connected layers, because of their intense use of multiplications, are
the dominant contributor to this computation budget.

We propose to alleviate this problem by introducing two new operations:
convolutional shifts and fully-connected shifts which replace multiplications
with bitwise shift and sign flipping during both training and inference. During
inference, both approaches require only 5 bits (or less) to represent the
weights. This family of neural network architectures (that use convolutional
shifts and fully connected shifts) is referred to as DeepShift models. We
propose two methods to train DeepShift models: DeepShift-Q which trains regular
weights constrained to powers of 2, and DeepShift-PS that trains the values of
the shifts and sign flips directly.

Very close accuracy, and in some cases higher accuracy, to baselines are
achieved. Converting pre-trained 32-bit floating-point baseline models of
ResNet18, ResNet50, VGG16, and GoogleNet to DeepShift and training them for 15
to 30 epochs, resulted in Top-1/Top-5 accuracies higher than that of the
original model.

Last but not least, we implemented the convolutional shifts and fully
connected shift GPU kernels and showed a reduction in latency time of 25% when
inferring ResNet18 compared to unoptimized multiplication-based GPU kernels.
The code can be found at https://github.com/mostafaelhoushi/DeepShift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elhoushi_M/0/1/0/all/0/1"&gt;Mostafa Elhoushi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zihao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiq_F/0/1/0/all/0/1"&gt;Farhan Shafiq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Ye Henry Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Joey Yiwei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Co-registration for Sentinel-1 SAR and Sentinel-2 Optical images. (arXiv:2005.11092v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.11092</id>
        <link href="http://arxiv.org/abs/2005.11092"/>
        <updated>2021-06-15T01:45:19.241Z</updated>
        <summary type="html"><![CDATA[Co-registering the Sentinel-1 SAR and Sentinel-2 optical data of European
Space Agency (ESA) is of great importance for many remote sensing applications.
However, we find that there are evident misregistration shifts between the
Sentinel-1 SAR and Sentinel-2 optical images that are directly downloaded from
the official website. To address that, this paper presents a fast and effective
registration method for the two types of images. In the proposed method, a
block-based scheme is first designed to extract evenly distributed interest
points. Then the correspondences are detected by using the similarity of
structural features between the SAR and optical images, where the three
dimension (3D) phase correlation (PC) is used as the similarity measure for
accelerating image matching. Finally, the obtained correspondences are employed
to measure the misregistration shifts between the images. Moreover, to
eliminate the misregistration, we use some representative geometric
transformation models such as polynomial models, projective models, and
rational function models for the co-registration of the two types of images,
and compare and analyze their registration accuracy under different numbers of
control points and different terrains. Six pairs of the Sentinel-1 SAR L1 and
Sentinel-2 optical L1C images covering three different terrains are tested in
our experiments. Experimental results show that the proposed method can achieve
precise correspondences between the images, and the 3rd. Order polynomial
achieves the most satisfactory registration results. Its registration accuracy
of the flat areas is less than 1.0 10m pixels, and that of the hilly areas is
about 1.5 10m pixels, and that of the mountainous areas is between 1.7 and 2.3
10m pixels, which significantly improves the co-registration accuracy of the
Sentinel-1 SAR and Sentinel-2 optical images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yuanxin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_B/0/1/0/all/0/1"&gt;Bai Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1"&gt;Youquan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_H/0/1/0/all/0/1"&gt;Huarong Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning with Optimism and Delay. (arXiv:2106.06885v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06885</id>
        <link href="http://arxiv.org/abs/2106.06885"/>
        <updated>2021-06-15T01:45:19.232Z</updated>
        <summary type="html"><![CDATA[Inspired by the demands of real-time climate and weather forecasting, we
develop optimistic online learning algorithms that require no parameter tuning
and have optimal regret guarantees under delayed feedback. Our algorithms --
DORM, DORMP, and AdaHedgeD -- arise from a novel reduction of delayed online
learning to optimistic online learning that reveals how optimistic hints can
mitigate the regret penalty caused by delay. We pair this delay-as-optimism
perspective with a new analysis of optimistic learning that exposes its
robustness to hinting errors and a new meta-algorithm for learning effective
hinting strategies in the presence of delay. We conclude by benchmarking our
algorithms on four subseasonal climate forecasting tasks, demonstrating low
regret relative to state-of-the-art forecasting models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Flaspohler_G/0/1/0/all/0/1"&gt;Genevieve Flaspohler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orabona_F/0/1/0/all/0/1"&gt;Francesco Orabona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1"&gt;Judah Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouatadid_S/0/1/0/all/0/1"&gt;Soukayna Mouatadid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oprescu_M/0/1/0/all/0/1"&gt;Miruna Oprescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orenstein_P/0/1/0/all/0/1"&gt;Paulo Orenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1"&gt;Lester Mackey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Visual-Semantic Entanglement Network for Zero-shot Image Recognition. (arXiv:2006.04648v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04648</id>
        <link href="http://arxiv.org/abs/2006.04648"/>
        <updated>2021-06-15T01:45:19.224Z</updated>
        <summary type="html"><![CDATA[Zero-shot learning uses semantic attributes to connect the search space of
unseen objects. In recent years, although the deep convolutional network brings
powerful visual modeling capabilities to the ZSL task, its visual features have
severe pattern inertia and lack of representation of semantic relationships,
which leads to severe bias and ambiguity. In response to this, we propose the
Graph-based Visual-Semantic Entanglement Network to conduct graph modeling of
visual features, which is mapped to semantic attributes by using a knowledge
graph, it contains several novel designs: 1. it establishes a multi-path
entangled network with the convolutional neural network (CNN) and the graph
convolutional network (GCN), which input the visual features from CNN to GCN to
model the implicit semantic relations, then GCN feedback the graph modeled
information to CNN features; 2. it uses attribute word vectors as the target
for the graph semantic modeling of GCN, which forms a self-consistent
regression for graph modeling and supervise GCN to learn more personalized
attribute relations; 3. it fuses and supplements the hierarchical
visual-semantic features refined by graph modeling into visual embedding. Our
method outperforms state-of-the-art approaches on multiple representative ZSL
datasets: AwA2, CUB, and SUN by promoting the semantic linkage modelling of
visual features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_G/0/1/0/all/0/1"&gt;Guihua Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chapman_A/0/1/0/all/0/1"&gt;Adriane Chapman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Pei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1"&gt;Mingnan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yingxue Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Dan Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hall_W/0/1/0/all/0/1"&gt;Wendy Hall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Streaming End-to-End Speech Recognition with Trie-Based Deep Biasing and Shallow Fusion. (arXiv:2104.02194v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02194</id>
        <link href="http://arxiv.org/abs/2104.02194"/>
        <updated>2021-06-15T01:45:19.216Z</updated>
        <summary type="html"><![CDATA[How to leverage dynamic contextual information in end-to-end speech
recognition has remained an active research area. Previous solutions to this
problem were either designed for specialized use cases that did not generalize
well to open-domain scenarios, did not scale to large biasing lists, or
underperformed on rare long-tail words. We address these limitations by
proposing a novel solution that combines shallow fusion, trie-based deep
biasing, and neural network language model contextualization. These techniques
result in significant 19.5% relative Word Error Rate improvement over existing
contextual biasing approaches and 5.4%-9.3% improvement compared to a strong
hybrid baseline on both open-domain and constrained contextualization tasks,
where the targets consist of mostly rare long-tail words. Our final system
remains lightweight and modular, allowing for quick modification without model
re-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1"&gt;Duc Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1"&gt;Mahaveer Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keren_G/0/1/0/all/0/1"&gt;Gil Keren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Suyoun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yangyang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahadeokar_J/0/1/0/all/0/1"&gt;Jay Mahadeokar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1"&gt;Julian Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1"&gt;Yuan Shangguan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuegen_C/0/1/0/all/0/1"&gt;Christian Fuegen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1"&gt;Ozlem Kalinli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1"&gt;Yatharth Saraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1"&gt;Michael L. Seltzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Harmonization with Flow-based Causal Inference. (arXiv:2106.06845v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06845</id>
        <link href="http://arxiv.org/abs/2106.06845"/>
        <updated>2021-06-15T01:45:19.205Z</updated>
        <summary type="html"><![CDATA[Heterogeneity in medical data, e.g., from data collected at different sites
and with different protocols in a clinical study, is a fundamental hurdle for
accurate prediction using machine learning models, as such models often fail to
generalize well. This paper presents a normalizing-flow-based method to perform
counterfactual inference upon a structural causal model (SCM) to harmonize such
data. We formulate a causal model for observed effects (brain magnetic
resonance imaging data) that result from known confounders (site, gender and
age) and exogenous noise variables. Our method exploits the bijection induced
by flow for harmonization. We can infer the posterior of exogenous variables,
intervene on observations, and draw samples from the resultant SCM to obtain
counterfactuals. We evaluate on multiple, large, real-world medical datasets to
observe that this method leads to better cross-domain generalization compared
to state-of-the-art algorithms. Further experiments that evaluate the quality
of confounder-independent data generated by our model using regression and
classification tasks are provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rongguang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1"&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davatzikos_C/0/1/0/all/0/1"&gt;Christos Davatzikos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformation Importance with Applications to Cosmology. (arXiv:2003.01926v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01926</id>
        <link href="http://arxiv.org/abs/2003.01926"/>
        <updated>2021-06-15T01:45:19.188Z</updated>
        <summary type="html"><![CDATA[Machine learning lies at the heart of new possibilities for scientific
discovery, knowledge generation, and artificial intelligence. Its potential
benefits to these fields requires going beyond predictive accuracy and focusing
on interpretability. In particular, many scientific problems require
interpretations in a domain-specific interpretable feature space (e.g. the
frequency domain) whereas attributions to the raw features (e.g. the pixel
space) may be unintelligible or even misleading. To address this challenge, we
propose TRIM (TRansformation IMportance), a novel approach which attributes
importances to features in a transformed space and can be applied post-hoc to a
fully trained model. TRIM is motivated by a cosmological parameter estimation
problem using deep neural networks (DNNs) on simulated data, but it is
generally applicable across domains/models and can be combined with any local
interpretation method. In our cosmology example, combining TRIM with contextual
decomposition shows promising results for identifying which frequencies a DNN
uses, helping cosmologists to understand and validate that the model learns
appropriate physical features rather than simulation artifacts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Singh_C/0/1/0/all/0/1"&gt;Chandan Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ha_W/0/1/0/all/0/1"&gt;Wooseok Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lanusse_F/0/1/0/all/0/1"&gt;Francois Lanusse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Boehm_V/0/1/0/all/0/1"&gt;Vanessa Boehm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bin Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoLoss: Automated Loss Function Search in Recommendations. (arXiv:2106.06713v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.06713</id>
        <link href="http://arxiv.org/abs/2106.06713"/>
        <updated>2021-06-15T01:45:19.177Z</updated>
        <summary type="html"><![CDATA[Designing an effective loss function plays a crucial role in training deep
recommender systems. Most existing works often leverage a predefined and fixed
loss function that could lead to suboptimal recommendation quality and training
efficiency. Some recent efforts rely on exhaustively or manually searched
weights to fuse a group of candidate loss functions, which is exceptionally
costly in computation and time. They also neglect the various convergence
behaviors of different data examples. In this work, we propose an AutoLoss
framework that can automatically and adaptively search for the appropriate loss
function from a set of candidates. To be specific, we develop a novel
controller network, which can dynamically adjust the loss probabilities in a
differentiable manner. Unlike existing algorithms, the proposed controller can
adaptively generate the loss probabilities for different data examples
according to their varied convergence behaviors. Such design improves the
model's generalizability and transferability between deep recommender systems
and datasets. We evaluate the proposed framework on two benchmark datasets. The
results show that AutoLoss outperforms representative baselines. Further
experiments have been conducted to deepen our understandings of AutoLoss,
including its transferability, components and training efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haochen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1"&gt;Wenqi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Active Regression. (arXiv:2106.06676v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06676</id>
        <link href="http://arxiv.org/abs/2106.06676"/>
        <updated>2021-06-15T01:45:19.150Z</updated>
        <summary type="html"><![CDATA[Labelled data often comes at a high cost as it may require recruiting human
labelers or running costly experiments. At the same time, in many practical
scenarios, one already has access to a partially labelled, potentially biased
dataset that can help with the learning task at hand. Motivated by such
settings, we formally initiate a study of $semi-supervised$ $active$ $learning$
through the frame of linear regression. In this setting, the learner has access
to a dataset $X \in \mathbb{R}^{(n_1+n_2) \times d}$ which is composed of $n_1$
unlabelled examples that an algorithm can actively query, and $n_2$ examples
labelled a-priori. Concretely, denoting the true labels by $Y \in
\mathbb{R}^{n_1 + n_2}$, the learner's objective is to find $\widehat{\beta}
\in \mathbb{R}^d$ such that, \begin{equation}

\| X \widehat{\beta} - Y \|_2^2 \le (1 + \epsilon) \min_{\beta \in
\mathbb{R}^d} \| X \beta - Y \|_2^2 \end{equation} while making as few
additional label queries as possible. In order to bound the label queries, we
introduce an instance dependent parameter called the reduced rank, denoted by
$R_X$, and propose an efficient algorithm with query complexity
$O(R_X/\epsilon)$. This result directly implies improved upper bounds for two
important special cases: (i) active ridge regression, and (ii) active kernel
ridge regression, where the reduced-rank equates to the statistical dimension,
$sd_\lambda$ and effective dimension, $d_\lambda$ of the problem respectively,
where $\lambda \ge 0$ denotes the regularization parameter. For active ridge
regression we also prove a matching lower bound of $O(sd_\lambda / \epsilon)$
on the query complexity of any algorithm. This subsumes prior work that only
considered the unregularized case, i.e., $\lambda = 0$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devvrit_F/0/1/0/all/0/1"&gt;Fnu Devvrit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajaraman_N/0/1/0/all/0/1"&gt;Nived Rajaraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1"&gt;Pranjal Awasthi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Label Annotation of Chest Abdomen Pelvis Computed Tomography Text Reports Using Deep Learning. (arXiv:2102.02959v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02959</id>
        <link href="http://arxiv.org/abs/2102.02959"/>
        <updated>2021-06-15T01:45:19.142Z</updated>
        <summary type="html"><![CDATA[Purpose: To develop high throughput multi-label annotators for body (chest,
abdomen, and pelvis) Computed Tomography (CT) reports that can be applied
across a variety of abnormalities, organs, and disease states.

Approach: We used a dictionary approach to develop rule-based algorithms
(RBA) for extraction of disease labels from radiology text reports. We targeted
three organ systems (lungs/pleura, liver/gallbladder, kidneys/ureters) with
four diseases per system based on their prevalence in our dataset. To expand
the algorithms beyond pre-defined keywords, attention-guided recurrent neural
networks (RNN) were trained using the RBA-extracted labels to classify reports
as being positive for one or more diseases or normal for each organ system.
Confounding effects on model performance were evaluated using random
initialization or pre-trained embedding as well as different sizes of training
datasets. Performance was evaluated using the receiver operating characteristic
(ROC) area under the curve (AUC) against 2,158 manually obtained labels.

Results: Our models extracted disease labels from 261,229 radiology reports
of 112,501 unique subjects. Pre-trained models outperformed random
initialization across all diseases. As the training dataset size was reduced,
performance was robust except for a few diseases with relatively small number
of cases. Pre-trained classification AUCs achieved > 0.95 for all five disease
outcomes across all three organ systems.

Conclusions: Our label-extracting pipeline was able to encompass a variety of
cases and diseases by generalizing beyond strict rules with exceptional
accuracy. This method can be easily adapted to enable automated labeling of
hospital-scale medical data sets for training image-based disease classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAnniballe_V/0/1/0/all/0/1"&gt;Vincent M. D&amp;#x27;Anniballe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tushar_F/0/1/0/all/0/1"&gt;Fakrul I. Tushar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faryna_K/0/1/0/all/0/1"&gt;Khrystyna Faryna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Songyue Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1"&gt;Maciej A. Mazurowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_G/0/1/0/all/0/1"&gt;Geoffrey D. Rubin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_J/0/1/0/all/0/1"&gt;Joseph Y. Lo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!). (arXiv:2106.06561v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06561</id>
        <link href="http://arxiv.org/abs/2106.06561"/>
        <updated>2021-06-15T01:45:19.128Z</updated>
        <summary type="html"><![CDATA[We show how to learn a map that takes a content code, derived from a face
image, and a randomly chosen style code to an anime image. We derive an
adversarial loss from our simple and effective definitions of style and
content. This adversarial loss guarantees the map is diverse -- a very wide
range of anime can be produced from a single content code. Under plausible
assumptions, the map is not just diverse, but also correctly represents the
probability of an anime, conditioned on an input face. In contrast, current
multimodal generation procedures cannot capture the complex styles that appear
in anime. Extensive quantitative experiments support the idea the map is
correct. Extensive qualitative results show that the method can generate a much
more diverse range of styles than SOTA comparisons. Finally, we show that our
formalization of content and style allows us to perform video to video
translation without ever training on videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1"&gt;Min Jin Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1"&gt;David Forsyth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconstruction of turbulent data with deep generative models for semantic inpainting from TURB-Rot database. (arXiv:2006.09179v2 [physics.flu-dyn] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09179</id>
        <link href="http://arxiv.org/abs/2006.09179"/>
        <updated>2021-06-15T01:45:19.109Z</updated>
        <summary type="html"><![CDATA[We study the applicability of tools developed by the computer vision
community for features learning and semantic image inpainting to perform data
reconstruction of fluid turbulence configurations. The aim is twofold. First,
we explore on a quantitative basis, the capability of Convolutional Neural
Networks embedded in a Deep Generative Adversarial Model (Deep-GAN) to generate
missing data in turbulence, a paradigmatic high dimensional chaotic system. In
particular, we investigate their use in reconstructing two-dimensional damaged
snapshots extracted from a large database of numerical configurations of 3d
turbulence in the presence of rotation, a case with multi-scale random features
where both large-scale organised structures and small-scale highly intermittent
and non-Gaussian fluctuations are present. Second, following a reverse
engineering approach, we aim to rank the input flow properties (features) in
terms of their qualitative and quantitative importance to obtain a better set
of reconstructed fields. We present two approaches both based on Context
Encoders. The first one infers the missing data via a minimization of the L2
pixel-wise reconstruction loss, plus a small adversarial penalisation. The
second searches for the closest encoding of the corrupted flow configuration
from a previously trained generator. Finally, we present a comparison with a
different data assimilation tool, based on Nudging, an equation-informed
unbiased protocol, well known in the numerical weather prediction community.
The TURB-Rot database, this http URL, of roughly 300K 2d
turbulent images is released and details on how to download it are given.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Buzzicotti_M/0/1/0/all/0/1"&gt;M. Buzzicotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bonaccorso_F/0/1/0/all/0/1"&gt;F. Bonaccorso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Leoni_P/0/1/0/all/0/1"&gt;P. Clark Di Leoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Biferale_L/0/1/0/all/0/1"&gt;L. Biferale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hippocampus segmentation in magnetic resonance images of Alzheimer's patients using Deep machine learning. (arXiv:2106.06743v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.06743</id>
        <link href="http://arxiv.org/abs/2106.06743"/>
        <updated>2021-06-15T01:45:19.102Z</updated>
        <summary type="html"><![CDATA[Background: Alzheimers disease is a progressive neurodegenerative disorder
and the main cause of dementia in aging. Hippocampus is prone to changes in the
early stages of Alzheimers disease. Detection and observation of the
hippocampus changes using magnetic resonance imaging (MRI) before the onset of
Alzheimers disease leads to the faster preventive and therapeutic measures.
Objective: The aim of this study was the segmentation of the hippocampus in
magnetic resonance (MR) images of Alzheimers patients using deep machine
learning method. Methods: U-Net architecture of convolutional neural network
was proposed to segment the hippocampus in the real MRI data. The MR images of
the 100 and 35 patients available in Alzheimers disease Neuroimaging Initiative
(ADNI) dataset, was used for the train and test of the model, respectively. The
performance of the proposed method was compared with manual segmentation by
measuring the similarity metrics. Results: The desired segmentation achieved
after 10 iterations. A Dice similarity coefficient (DSC) = 92.3%, sensitivity =
96.5%, positive predicted value (PPV) = 90.4%, and Intersection over Union
(IoU) value for the train 92.94 and test 92.93 sets were obtained which are
acceptable. Conclusion: The proposed approach is promising and can be extended
in the prognosis of Alzheimers disease by the prediction of the hippocampus
volume changes in the early stage of the disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Varmazyar_H/0/1/0/all/0/1"&gt;Hadi Varmazyar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yousefi_Banaem_H/0/1/0/all/0/1"&gt;Hossein Yousefi-Banaem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Malekzadeh_S/0/1/0/all/0/1"&gt;Saber Malekzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gharehaghaji_N/0/1/0/all/0/1"&gt;Nahideh Gharehaghaji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-adaptive Fall Detection Using Deep Adversarial Training. (arXiv:2012.10911v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10911</id>
        <link href="http://arxiv.org/abs/2012.10911"/>
        <updated>2021-06-15T01:45:19.094Z</updated>
        <summary type="html"><![CDATA[Fall detection (FD) systems are important assistive technologies for
healthcare that can detect emergency fall events and alert caregivers. However,
it is not easy to obtain large-scale annotated fall events with various
specifications of sensors or sensor positions during the implementation of
accurate FD systems. Moreover, the knowledge obtained through machine learning
has been restricted to tasks in the same domain. The mismatch between different
domains might hinder the performance of FD systems. Cross-domain knowledge
transfer is very beneficial for machine-learning-based FD systems to train a
reliable FD model with well-labeled data in new environments. In this study, we
propose domain-adaptive fall detection (DAFD) using deep adversarial training
(DAT) to tackle cross-domain problems, such as cross-position and
cross-configuration. The proposed DAFD can transfer knowledge from the source
domain to the target domain by minimizing the domain discrepancy to avoid
mismatch problems. The experimental results show that the average F1-score
improvement when using DAFD ranges from 1.5% to 7% in the cross-position
scenario, and from 3.5% to 12% in the cross-configuration scenario, compared to
using the conventional FD model without domain adaptation training. The results
demonstrate that the proposed DAFD successfully helps to deal with cross-domain
problems and to achieve better detection performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kai-Chun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Can_M/0/1/0/all/0/1"&gt;Michael Can&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuo_H/0/1/0/all/0/1"&gt;Heng-Cheng Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Chia-Yeh Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hsiang-Yun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1"&gt;Chia-Tai Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1"&gt;Yu Tsao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Ordering of Characters in Japanese Historical Documents. (arXiv:2106.06786v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06786</id>
        <link href="http://arxiv.org/abs/2106.06786"/>
        <updated>2021-06-15T01:45:19.088Z</updated>
        <summary type="html"><![CDATA[Japan is a unique country with a distinct cultural heritage, which is
reflected in billions of historical documents that have been preserved.
However, the change in Japanese writing system in 1900 made these documents
inaccessible for the general public. A major research project has been to make
these historical documents accessible and understandable. An increasing amount
of research has focused on the character recognition task and the location of
characters on image, yet less research has focused on how to predict the
sequential ordering of the characters. This is because sequence in classical
Japanese is very different from modern Japanese. Ordering characters into a
sequence is important for making the document text easily readable and
searchable. Additionally, it is a necessary step for any kind of natural
language processing on the data (e.g. machine translation, language modeling,
and word embeddings). We explore a few approaches to the task of predicting the
sequential ordering of the characters: one using simple hand-crafted rules,
another using hand-crafted rules with adaptive thresholds, and another using a
deep recurrent sequence model trained with teacher forcing. We provide a
quantitative and qualitative comparison of these techniques as well as their
distinct trade-offs. Our best-performing system has an accuracy of 98.65\% and
has a perfect accuracy on 49\% of the books in our dataset, suggesting that the
technique is able to predict the order of the characters well enough for many
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamb_A/0/1/0/all/0/1"&gt;Alex Lamb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clanuwat_T/0/1/0/all/0/1"&gt;Tarin Clanuwat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Siyu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bober_Irizar_M/0/1/0/all/0/1"&gt;Mikel Bober-Irizar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamoto_A/0/1/0/all/0/1"&gt;Asanobu Kitamoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandwidth-Agile Image Transmission with Deep Joint Source-Channel Coding. (arXiv:2009.12480v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12480</id>
        <link href="http://arxiv.org/abs/2009.12480"/>
        <updated>2021-06-15T01:45:19.066Z</updated>
        <summary type="html"><![CDATA[We propose deep learning based communication methods for adaptive-bandwidth
transmission of images over wireless channels. We consider the scenario in
which images are transmitted progressively in layers over time or frequency,
and such layers can be aggregated by receivers in order to increase the quality
of their reconstructions. We investigate two scenarios, one in which the layers
are sent sequentially, and incrementally contribute to the refinement of a
reconstruction, and another in which the layers are independent and can be
retrieved in any order. Those scenarios correspond to the well known problems
of \textit{successive refinement} and \textit{multiple descriptions},
respectively, in the context of joint source-channel coding (JSCC). We propose
DeepJSCC-$l$, an innovative solution that uses convolutional autoencoders, and
present three architectures with different complexity trade-offs. To the best
of our knowledge, this is the first practical multiple-description JSCC scheme
developed and tested for practical information sources and channels. Numerical
results show that DeepJSCC-$l$ can learn to transmit the source progressively
with negligible losses in the end-to-end performance compared with a single
transmission. Moreover, DeepJSCC-$l$ has comparable performance with state of
the art digital progressive transmission schemes in the challenging low
signal-to-noise ratio (SNR) and small bandwidth regimes, with the additional
advantage of graceful degradation with channel SNR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kurka_D/0/1/0/all/0/1"&gt;David Burth Kurka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz G&amp;#xfc;nd&amp;#xfc;z&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining the Black-box Smoothly- A Counterfactual Approach. (arXiv:2101.04230v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04230</id>
        <link href="http://arxiv.org/abs/2101.04230"/>
        <updated>2021-06-15T01:45:19.036Z</updated>
        <summary type="html"><![CDATA[We propose a BlackBox \emph{Counterfactual Explainer} that is explicitly
developed for medical imaging applications. Classical approaches (e.g. saliency
maps) assessing feature importance do not explain \emph{how} and \emph{why}
variations in a particular anatomical region is relevant to the outcome, which
is crucial for transparent decision making in healthcare application. Our
framework explains the outcome by gradually \emph{exaggerating} the semantic
effect of the given outcome label. Given a query input to a classifier,
Generative Adversarial Networks produce a progressive set of perturbations to
the query image that gradually changes the posterior probability from its
original class to its negation. We design the loss function to ensure that
essential and potentially relevant details, such as support devices, are
preserved in the counterfactually generated images. We provide an extensive
evaluation of different classification tasks on the chest X-Ray images. Our
experiments show that a counterfactually generated visual explanation is
consistent with the disease's clinical relevant measurements, both
quantitatively and qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1"&gt;Sumedha Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollack_B/0/1/0/all/0/1"&gt;Brian Pollack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_S/0/1/0/all/0/1"&gt;Stephen Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1"&gt;Kayhan Batmanghelich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization. (arXiv:2106.06607v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06607</id>
        <link href="http://arxiv.org/abs/2106.06607"/>
        <updated>2021-06-15T01:45:19.021Z</updated>
        <summary type="html"><![CDATA[The invariance principle from causality is at the heart of notable approaches
such as invariant risk minimization (IRM) that seek to address
out-of-distribution (OOD) generalization failures. Despite the promising
theory, invariance principle-based approaches fail in common classification
tasks, where invariant (causal) features capture all the information about the
label. Are these failures due to the methods failing to capture the invariance?
Or is the invariance principle itself insufficient? To answer these questions,
we revisit the fundamental assumptions in linear regression tasks, where
invariance-based approaches were shown to provably generalize OOD. In contrast
to the linear regression tasks, we show that for linear classification tasks we
need much stronger restrictions on the distribution shifts, or otherwise OOD
generalization is impossible. Furthermore, even with appropriate restrictions
on distribution shifts in place, we show that the invariance principle alone is
insufficient. We prove that a form of the information bottleneck constraint
along with invariance helps address key failures when invariant features
capture all the information about the label and also retains the existing
success when they do not. We propose an approach that incorporates both of
these principles and demonstrate its effectiveness in several experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1"&gt;Kartik Ahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caballero_E/0/1/0/all/0/1"&gt;Ethan Caballero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dinghuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitliagkas_I/0/1/0/all/0/1"&gt;Ioannis Mitliagkas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1"&gt;Irina Rish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup. (arXiv:2101.06983v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06983</id>
        <link href="http://arxiv.org/abs/2101.06983"/>
        <updated>2021-06-15T01:45:19.013Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has been applied successfully to learn vector
representations of text. Previous research demonstrated that learning
high-quality representations benefits from batch-wise contrastive loss with a
large number of negatives. In practice, the technique of in-batch negative is
used, where for each example in a batch, other batch examples' positives will
be taken as its negatives, avoiding encoding extra negatives. This, however,
still conditions each example's loss on all batch examples and requires fitting
the entire large batch into GPU memory. This paper introduces a gradient
caching technique that decouples backpropagation between contrastive loss and
the encoder, removing encoder backward pass data dependency along the batch
dimension. As a result, gradients can be computed for one subset of the batch
at a time, leading to almost constant memory usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Luyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1"&gt;Jamie Callan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guaranteed Fixed-Confidence Best Arm Identification in Multi-Armed Bandit. (arXiv:2106.06848v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06848</id>
        <link href="http://arxiv.org/abs/2106.06848"/>
        <updated>2021-06-15T01:45:19.004Z</updated>
        <summary type="html"><![CDATA[We consider the problem of finding, through adaptive sampling, which of n
populations (arms) has the largest mean. Our objective is to determine a rule
which identifies the best population with a fixed minimum confidence using as
few observations as possible, i.e. fixed-confidence (FC) best arm
identification (BAI) in multi-armed bandits. We study such problems under the
Bayesian setting with both Bernoulli and Gaussian populations. We propose to
use the classical vector at a time (VT) rule, which samples each alive
population once in each round. We show how VT can be implemented and analyzed
in our Bayesian setting and be improved by early elimination. We also propose
and analyze a variant of the classical play the winner (PW) algorithm.
Numerical results show that these rules compare favorably with state-of-art
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1"&gt;MohammadJavad Azizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ross_S/0/1/0/all/0/1"&gt;Sheldon M Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep manifold learning reveals hidden dynamics of proteasome autoregulation. (arXiv:2012.12854v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12854</id>
        <link href="http://arxiv.org/abs/2012.12854"/>
        <updated>2021-06-15T01:45:18.985Z</updated>
        <summary type="html"><![CDATA[The 2.5-MDa 26S proteasome maintains proteostasis and regulates myriad
cellular processes. How polyubiquitylated substrate interactions regulate
proteasome activity is not understood. Here we introduce a deep manifold
learning framework, named AlphaCryo4D, which enables atomic-level cryogenic
electron microscopy (cryo-EM) reconstructions of nonequilibrium conformational
continuum and reconstitutes hidden dynamics of proteasome autoregulation in the
act of substrate degradation. AlphaCryo4D integrates 3D deep residual learning
with manifold embedding of free-energy landscapes, which directs 3D clustering
via an energy-based particle-voting algorithm. In blind assessments using
simulated heterogeneous cryo-EM datasets, AlphaCryo4D achieved 3D
classification accuracy three times that of conventional method and
reconstructed continuous conformational changes of a 130-kDa protein at
sub-3-angstrom resolution. By using AlphaCryo4D to analyze a single
experimental cryo-EM dataset, we identified 64 conformers of the
substrate-bound human 26S proteasome, revealing conformational entanglement of
two regulatory particles in the doubly capped holoenzymes and their energetic
differences with singly capped ones. Novel ubiquitin-binding sites are
discovered on the RPN2, RPN10 and Alpha5 subunits to remodel polyubiquitin
chains for deubiquitylation and recycle. Importantly, AlphaCryo4D choreographs
single-nucleotide-exchange dynamics of proteasomal AAA-ATPase motor during
translocation initiation, which upregulates proteolytic activity by
allosterically promoting nucleophilic attack. Our systemic analysis illuminates
a grand hierarchical allostery for proteasome autoregulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhaolong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Li Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yinping Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yuanchen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Youdong Mao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Can I Do Here? Learning New Skills by Imagining Visual Affordances. (arXiv:2106.00671v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00671</id>
        <link href="http://arxiv.org/abs/2106.00671"/>
        <updated>2021-06-15T01:45:18.979Z</updated>
        <summary type="html"><![CDATA[A generalist robot equipped with learned skills must be able to perform many
tasks in many different environments. However, zero-shot generalization to new
settings is not always possible. When the robot encounters a new environment or
object, it may need to finetune some of its previously learned skills to
accommodate this change. But crucially, previously learned behaviors and models
should still be suitable to accelerate this relearning. In this paper, we aim
to study how generative models of possible outcomes can allow a robot to learn
visual representations of affordances, so that the robot can sample potentially
possible outcomes in new situations, and then further train its policy to
achieve those outcomes. In effect, prior data is used to learn what kinds of
outcomes may be possible, such that when the robot encounters an unfamiliar
setting, it can sample potential outcomes from its model, attempt to reach
them, and thereby update both its skills and its outcome model. This approach,
visuomotor affordance learning (VAL), can be used to train goal-conditioned
policies that operate on raw image inputs, and can rapidly learn to manipulate
new objects via our proposed affordance-directed exploration scheme. We show
that VAL can utilize prior data to solve real-world tasks such drawer opening,
grasping, and placing objects in new scenes with only five minutes of online
experience in the new scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khazatsky_A/0/1/0/all/0/1"&gt;Alexander Khazatsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1"&gt;Ashvin Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_D/0/1/0/all/0/1"&gt;Daniel Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Saddle-Point Problems: Lower Bounds, Optimal and Robust Algorithms. (arXiv:2010.13112v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13112</id>
        <link href="http://arxiv.org/abs/2010.13112"/>
        <updated>2021-06-15T01:45:18.972Z</updated>
        <summary type="html"><![CDATA[This paper focuses on the distributed optimization of smooth stochastic
saddle-point problems. The first part of the paper is devoted to lower bounds
for the cenralized and decentralized distributed methods for smooth
(strongly-)convex-(strongly-)concave saddle-point problems as well as the
optimal algorithms by which these bounds are achieved. Next, we present a new
federated algorithm for saddle-point problems - Extra Step Local SGD.
Theoretical analysis of the new method is carried out for
(strongly-)convex-(strongly-)concave and non-convex-non-concave problems. In
the experimental part of the paper, we show the effectiveness of our method in
practice. In particular, we train GANs in a distributed manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beznosikov_A/0/1/0/all/0/1"&gt;Aleksandr Beznosikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samokhin_V/0/1/0/all/0/1"&gt;Valentin Samokhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasnikov_A/0/1/0/all/0/1"&gt;Alexander Gasnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double/Debiased Machine Learning for Dynamic Treatment Effects via g-Estimation. (arXiv:2002.07285v4 [econ.EM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.07285</id>
        <link href="http://arxiv.org/abs/2002.07285"/>
        <updated>2021-06-15T01:45:18.958Z</updated>
        <summary type="html"><![CDATA[We consider the estimation of treatment effects in settings when multiple
treatments are assigned over time and treatments can have a causal effect on
future outcomes or the state of the treated unit. We propose an extension of
the double/debiased machine learning framework to estimate the dynamic effects
of treatments, which can be viewed as a Neyman orthogonal (locally robust)
cross-fitted version of $g$-estimation in the dynamic treatment regime. Our
method applies to a general class of non-linear dynamic treatment models known
as Structural Nested Mean Models and allows the use of machine learning methods
to control for potentially high dimensional state variables, subject to a mean
square error guarantee, while still allowing parametric estimation and
construction of confidence intervals for the structural parameters of interest.
These structural parameters can be used for off-policy evaluation of any target
dynamic policy at parametric rates, subject to semi-parametric restrictions on
the data generating process. Our work is based on a recursive peeling process,
typical in $g$-estimation, and formulates a strongly convex objective at each
stage, which allows us to extend the $g$-estimation framework in multiple
directions: i) to provide finite sample guarantees, ii) to estimate non-linear
effect heterogeneity with respect to fixed unit characteristics, within
arbitrary function spaces, enabling a dynamic analogue of the RLearner
algorithm for heterogeneous effects, iii) to allow for high-dimensional sparse
parameterizations of the target structural functions, enabling automated model
selection via a recursive lasso algorithm. We also provide guarantees for data
stemming from a single treated unit over a long horizon and under stationarity
conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Lewis_G/0/1/0/all/0/1"&gt;Greg Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Syrgkanis_V/0/1/0/all/0/1"&gt;Vasilis Syrgkanis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Deep Architectures Without End-to-End Backpropagation: A Brief Survey. (arXiv:2101.03419v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.03419</id>
        <link href="http://arxiv.org/abs/2101.03419"/>
        <updated>2021-06-15T01:45:18.946Z</updated>
        <summary type="html"><![CDATA[This tutorial paper surveys training alternatives to end-to-end
backpropagation (E2EBP) -- the de facto standard for training deep
architectures. Modular training refers to strictly local training without both
the forward and the backward pass, i.e., dividing a deep architecture into
several nonoverlapping modules and training them separately without any
end-to-end operation. Between the fully global E2EBP and the strictly local
modular training, there are "weakly modular" hybrids performing training
without the backward pass only. These alternatives can match or surpass the
performance of E2EBP on challenging datasets such as ImageNet, and are gaining
increased attention primarily because they offer practical advantages over
E2EBP, which will be enumerated herein. In particular, they allow for greater
modularity and transparency in deep learning workflows, aligning deep learning
with the mainstream computer science engineering that heavily exploits
modularization for scalability. Modular training has also revealed novel
insights about learning and has further implications on other important
research domains. Specifically, it induces natural and effective solutions to
some important practical problems such as data efficiency and transferability
estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1"&gt;Shiyu Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1"&gt;Jose C. Principe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Generalization in Adversarial Training via the Bias-Variance Decomposition. (arXiv:2103.09947v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09947</id>
        <link href="http://arxiv.org/abs/2103.09947"/>
        <updated>2021-06-15T01:45:18.919Z</updated>
        <summary type="html"><![CDATA[Adversarially trained models exhibit a large generalization gap: they can
interpolate the training set even for large perturbation radii, but at the cost
of large test error on clean samples. To investigate this gap, we decompose the
test risk into its bias and variance components and study their behavior as a
function of adversarial training perturbation radii ($\varepsilon$). We find
that the bias increases monotonically with $\varepsilon$ and is the dominant
term in the risk. Meanwhile, the variance is unimodal as a function of
$\varepsilon$, peaking near the interpolation threshold for the training set.
This characteristic behavior occurs robustly across different datasets and also
for other robust training procedures such as randomized smoothing. It thus
provides a test for proposed explanations of the generalization gap. We find
that some existing explanations fail this test--for instance, by predicting a
monotonically increasing variance curve. This underscores the power of
bias-variance decompositions in modern settings-by providing two measurements
instead of one, they can rule out more explanations than test accuracy alone.
We also show that bias and variance can provide useful guidance for scalably
reducing the generalization gap, highlighting pre-training and unlabeled data
as promising routes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zitong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1"&gt;Edgar Dobriban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1"&gt;Jacob Steinhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Distributed Model-Free Ride-Sharing Approach for Joint Matching, Pricing, and Dispatching using Deep Reinforcement Learning. (arXiv:2010.01755v2 [cs.MA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01755</id>
        <link href="http://arxiv.org/abs/2010.01755"/>
        <updated>2021-06-15T01:45:18.897Z</updated>
        <summary type="html"><![CDATA[Significant development of ride-sharing services presents a plethora of
opportunities to transform urban mobility by providing personalized and
convenient transportation while ensuring efficiency of large-scale ride
pooling. However, a core problem for such services is route planning for each
driver to fulfill the dynamically arriving requests while satisfying given
constraints. Current models are mostly limited to static routes with only two
rides per vehicle (optimally) or three (with heuristics). In this paper, we
present a dynamic, demand aware, and pricing-based vehicle-passenger matching
and route planning framework that (1) dynamically generates optimal routes for
each vehicle based on online demand, pricing associated with each ride, vehicle
capacities and locations. This matching algorithm starts greedily and optimizes
over time using an insertion operation, (2) involves drivers in the
decision-making process by allowing them to propose a different price based on
the expected reward for a particular ride as well as the destination locations
for future rides, which is influenced by supply-and demand computed by the Deep
Q-network, (3) allows customers to accept or reject rides based on their set of
preferences with respect to pricing and delay windows, vehicle type and
carpooling preferences, and (4) based on demand prediction, our approach
re-balances idle vehicles by dispatching them to the areas of anticipated high
demand using deep Reinforcement Learning (RL). Our framework is validated using
the New York City Taxi public dataset; however, we consider different vehicle
types and designed customer utility functions to validate the setup and study
different settings. Experimental results show the effectiveness of our approach
in real-time and large scale settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haliem_M/0/1/0/all/0/1"&gt;Marina Haliem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mani_G/0/1/0/all/0/1"&gt;Ganapathy Mani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1"&gt;Vaneet Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhargava_B/0/1/0/all/0/1"&gt;Bharat Bhargava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers. (arXiv:2106.06560v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06560</id>
        <link href="http://arxiv.org/abs/2106.06560"/>
        <updated>2021-06-15T01:45:18.890Z</updated>
        <summary type="html"><![CDATA[High-resolution representations (HR) are essential for dense prediction tasks
such as segmentation, detection, and pose estimation. Learning HR
representations is typically ignored in previous Neural Architecture Search
(NAS) methods that focus on image classification. This work proposes a novel
NAS method, called HR-NAS, which is able to find efficient and accurate
networks for different tasks, by effectively encoding multiscale contextual
information while maintaining high-resolution representations. In HR-NAS, we
renovate the NAS search space as well as its searching strategy. To better
encode multiscale image contexts in the search space of HR-NAS, we first
carefully design a lightweight transformer, whose computational complexity can
be dynamically changed with respect to different objective functions and
computation budgets. To maintain high-resolution representations of the learned
networks, HR-NAS adopts a multi-branch architecture that provides convolutional
encoding of multiple feature resolutions, inspired by HRNet. Last, we proposed
an efficient fine-grained search strategy to train HR-NAS, which effectively
explores the search space, and finds optimal architectures given various tasks
and computation resources. HR-NAS is capable of achieving state-of-the-art
trade-offs between performance and FLOPs for three dense prediction tasks and
an image classification task, given only small computational budgets. For
example, HR-NAS surpasses SqueezeNAS that is specially designed for semantic
segmentation while improving efficiency by 45.9%. Code is available at
https://github.com/dingmyu/HR-NAS]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Mingyu Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1"&gt;Xiaochen Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Linjie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xiaojie Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhiwu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Foveated Video Quality Using Entropic Differencing. (arXiv:2106.06817v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06817</id>
        <link href="http://arxiv.org/abs/2106.06817"/>
        <updated>2021-06-15T01:45:18.857Z</updated>
        <summary type="html"><![CDATA[Virtual Reality is regaining attention due to recent advancements in hardware
technology. Immersive images / videos are becoming widely adopted to carry
omnidirectional visual information. However, due to the requirements for higher
spatial and temporal resolution of real video data, immersive videos require
significantly larger bandwidth consumption. To reduce stresses on bandwidth,
foveated video compression is regaining popularity, whereby the space-variant
spatial resolution of the retina is exploited. Towards advancing the progress
of foveated video compression, we propose a full reference (FR) foveated image
quality assessment algorithm, which we call foveated entropic differencing
(FED), which employs the natural scene statistics of bandpass responses by
applying differences of local entropies weighted by a foveation-based error
sensitivity function. We evaluate the proposed algorithm by measuring the
correlations of the predictions that FED makes against human judgements on the
newly created 2D and 3D LIVE-FBT-FCVR databases for Virtual Reality (VR). The
performance of the proposed algorithm yields state-of-the-art as compared with
other existing full reference algorithms. Software for FED has been made
available at: this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yize Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patney_A/0/1/0/all/0/1"&gt;Anjul Patney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1"&gt;Alan Bovik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PMP-Net: Point Cloud Completion by Learning Multi-step Point Moving Paths. (arXiv:2012.03408v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03408</id>
        <link href="http://arxiv.org/abs/2012.03408"/>
        <updated>2021-06-15T01:45:18.828Z</updated>
        <summary type="html"><![CDATA[The task of point cloud completion aims to predict the missing part for an
incomplete 3D shape. A widely used strategy is to generate a complete point
cloud from the incomplete one. However, the unordered nature of point clouds
will degrade the generation of high-quality 3D shapes, as the detailed topology
and structure of discrete points are hard to be captured by the generative
process only using a latent code. In this paper, we address the above problem
by reconsidering the completion task from a new perspective, where we formulate
the prediction as a point cloud deformation process. Specifically, we design a
novel neural network, named PMP-Net, to mimic the behavior of an earth mover.
It moves each point of the incomplete input to complete the point cloud, where
the total distance of point moving paths (PMP) should be shortest. Therefore,
PMP-Net predicts a unique point moving path for each point according to the
constraint of total point moving distances. As a result, the network learns a
strict and unique correspondence on point-level, which can capture the detailed
topology and structure relationships between the incomplete shape and the
complete target, and thus improves the quality of the predicted complete shape.
We conduct comprehensive experiments on Completion3D and PCN datasets, which
demonstrate our advantages over the state-of-the-art point cloud completion
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1"&gt;Xin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1"&gt;Peng Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhizhong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yan-Pei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1"&gt;Pengfei Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wen Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu-Shen Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks with Local Graph Parameters. (arXiv:2106.06707v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06707</id>
        <link href="http://arxiv.org/abs/2106.06707"/>
        <updated>2021-06-15T01:45:18.817Z</updated>
        <summary type="html"><![CDATA[Various recent proposals increase the distinguishing power of Graph Neural
Networks GNNs by propagating features between $k$-tuples of vertices. The
distinguishing power of these "higher-order'' GNNs is known to be bounded by
the $k$-dimensional Weisfeiler-Leman (WL) test, yet their $\mathcal O(n^k)$
memory requirements limit their applicability. Other proposals infuse GNNs with
local higher-order graph structural information from the start, hereby
inheriting the desirable $\mathcal O(n)$ memory requirement from GNNs at the
cost of a one-time, possibly non-linear, preprocessing step. We propose local
graph parameter enabled GNNs as a framework for studying the latter kind of
approaches and precisely characterize their distinguishing power, in terms of a
variant of the WL test, and in terms of the graph structural properties that
they can take into account. Local graph parameters can be added to any GNN
architecture, and are cheap to compute. In terms of expressive power, our
proposal lies in the middle of GNNs and their higher-order counterparts.
Further, we propose several techniques to aide in choosing the right local
graph parameters. Our results connect GNNs with deep results in finite model
theory and finite variable logics. Our experimental evaluation shows that
adding local graph parameters often has a positive effect for a variety of
GNNs, datasets and graph learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barcelo_P/0/1/0/all/0/1"&gt;Pablo Barcel&amp;#xf3;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geerts_F/0/1/0/all/0/1"&gt;Floris Geerts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reutter_J/0/1/0/all/0/1"&gt;Juan Reutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryschkov_M/0/1/0/all/0/1"&gt;Maksimilian Ryschkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15082</id>
        <link href="http://arxiv.org/abs/2105.15082"/>
        <updated>2021-06-15T01:45:18.811Z</updated>
        <summary type="html"><![CDATA[Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has
become a trend in model scaling. Still it is a mystery how MoE layers bring
quality gains by leveraging the parameters with sparse activation. In this
work, we investigate several key factors in sparse expert models. We observe
that load imbalance may not be a significant problem affecting model quality,
contrary to the perspectives of recent studies, while the number of sparsely
activated experts $k$ and expert capacity $C$ in top-$k$ routing can
significantly make a difference in this context. Furthermore, we take a step
forward to propose a simple method called expert prototyping that splits
experts into different prototypes and applies $k$ top-$1$ routing. This
strategy improves the model quality but maintains constant computational costs,
and our further exploration on extremely large-scale models reflects that it is
more effective in training larger models. We push the model scale to over $1$
trillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in
comparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model
achieves substantial speedup in convergence over the same-size baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Le Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xianyan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Ang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiamang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Di Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Lin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What can linearized neural networks actually say about generalization?. (arXiv:2106.06770v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06770</id>
        <link href="http://arxiv.org/abs/2106.06770"/>
        <updated>2021-06-15T01:45:18.785Z</updated>
        <summary type="html"><![CDATA[For certain infinitely-wide neural networks, the neural tangent kernel (NTK)
theory fully characterizes generalization. However, for the networks used in
practice, the empirical NTK represents only a rough first-order approximation
of these architectures. Still, a growing body of work keeps leveraging this
approximation to successfully analyze important deep learning phenomena and
derive algorithms for new applications. In our work, we provide strong
empirical evidence to determine the practical validity of such approximation by
conducting a systematic comparison of the behaviour of different neural
networks and their linear approximations on different tasks. We show that the
linear approximations can indeed rank the learning complexity of certain tasks
for neural networks, albeit with important nuances. Specifically, we discover
that, in contrast to what was previously observed, neural networks do not
always perform better than their kernel approximations, and reveal that their
performance gap heavily depends on architecture, number of samples and training
task. In fact, we show that during training, deep networks increase the
alignment of their empirical NTK with the target task, which explains why
linear approximations at the end of training can better explain the dynamics of
deep networks. Overall, our work provides concrete examples of novel deep
learning phenomena which can inspire future theoretical research, as well as
provides a new perspective on the use of the NTK approximation in deep
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ortiz_Jimenez_G/0/1/0/all/0/1"&gt;Guillermo Ortiz-Jim&amp;#xe9;nez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1"&gt;Seyed-Mohsen Moosavi-Dezfooli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1"&gt;Pascal Frossard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-NBA: Efficient and Effective Search Over the Joint Space of Networks, Bitwidths, and Accelerators. (arXiv:2106.06575v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06575</id>
        <link href="http://arxiv.org/abs/2106.06575"/>
        <updated>2021-06-15T01:45:18.779Z</updated>
        <summary type="html"><![CDATA[While maximizing deep neural networks' (DNNs') acceleration efficiency
requires a joint search/design of three different yet highly coupled aspects,
including the networks, bitwidths, and accelerators, the challenges associated
with such a joint search have not yet been fully understood and addressed. The
key challenges include (1) the dilemma of whether to explode the memory
consumption due to the huge joint space or achieve sub-optimal designs, (2) the
discrete nature of the accelerator design space that is coupled yet different
from that of the networks and bitwidths, and (3) the chicken and egg problem
associated with network-accelerator co-search, i.e., co-search requires
operation-wise hardware cost, which is lacking during search as the optimal
accelerator depending on the whole network is still unknown during search. To
tackle these daunting challenges towards optimal and fast development of DNN
accelerators, we propose a framework dubbed Auto-NBA to enable jointly
searching for the Networks, Bitwidths, and Accelerators, by efficiently
localizing the optimal design within the huge joint design space for each
target dataset and acceleration specification. Our Auto-NBA integrates a
heterogeneous sampling strategy to achieve unbiased search with constant memory
consumption, and a novel joint-search pipeline equipped with a generic
differentiable accelerator search engine. Extensive experiments and ablation
studies validate that both Auto-NBA generated networks and accelerators
consistently outperform state-of-the-art designs (including
co-search/exploration techniques, hardware-aware NAS methods, and DNN
accelerators), in terms of search time, task accuracy, and accelerator
efficiency. Our codes are available at: https://github.com/RICE-EIC/Auto-NBA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yonggan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1"&gt;David Cox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yingyan Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relaxing Local Robustness. (arXiv:2106.06624v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06624</id>
        <link href="http://arxiv.org/abs/2106.06624"/>
        <updated>2021-06-15T01:45:18.764Z</updated>
        <summary type="html"><![CDATA[Certifiable local robustness, which rigorously precludes small-norm
adversarial examples, has received significant attention as a means of
addressing security concerns in deep learning. However, for some classification
problems, local robustness is not a natural objective, even in the presence of
adversaries; for example, if an image contains two classes of subjects, the
correct label for the image may be considered arbitrary between the two, and
thus enforcing strict separation between them is unnecessary. In this work, we
introduce two relaxed safety properties for classifiers that address this
observation: (1) relaxed top-k robustness, which serves as the analogue of
top-k accuracy; and (2) affinity robustness, which specifies which sets of
labels must be separated by a robustness margin, and which can be
$\epsilon$-close in $\ell_p$ space. We show how to construct models that can be
efficiently certified against each relaxed robustness property, and trained
with very little overhead relative to standard gradient descent. Finally, we
demonstrate experimentally that these relaxed variants of robustness are
well-suited to several significant classification problems, leading to lower
rejection rates and higher certified accuracies than can be obtained when
certifying "standard" local robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leino_K/0/1/0/all/0/1"&gt;Klas Leino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1"&gt;Matt Fredrikson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low Budget Active Learning via Wasserstein Distance: An Integer Programming Approach. (arXiv:2106.02968v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02968</id>
        <link href="http://arxiv.org/abs/2106.02968"/>
        <updated>2021-06-15T01:45:18.727Z</updated>
        <summary type="html"><![CDATA[Given restrictions on the availability of data, active learning is the
process of training a model with limited labeled data by selecting a core
subset of an unlabeled data pool to label. Although selecting the most useful
points for training is an optimization problem, the scale of deep learning data
sets forces most selection strategies to employ efficient heuristics. Instead,
we propose a new integer optimization problem for selecting a core set that
minimizes the discrete Wasserstein distance from the unlabeled pool. We
demonstrate that this problem can be tractably solved with a Generalized
Benders Decomposition algorithm. Our strategy requires high-quality latent
features which we obtain by unsupervised learning on the unlabeled pool.
Numerical results on several data sets show that our optimization approach is
competitive with baselines and particularly outperforms them in the low budget
regime where less than one percent of the data set is labeled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1"&gt;Rafid Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1"&gt;Marc T. Law&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation. (arXiv:2106.03153v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03153</id>
        <link href="http://arxiv.org/abs/2106.03153"/>
        <updated>2021-06-15T01:45:18.700Z</updated>
        <summary type="html"><![CDATA[With rapid progress in neural text-to-speech (TTS) models, personalized
speech generation is now in high demand for many applications. For practical
applicability, a TTS model should generate high-quality speech with only a few
audio samples from the given speaker, that are also short in length. However,
existing methods either require to fine-tune the model or achieve low
adaptation quality without fine-tuning. In this work, we propose StyleSpeech, a
new TTS model which not only synthesizes high-quality speech but also
effectively adapts to new speakers. Specifically, we propose Style-Adaptive
Layer Normalization (SALN) which aligns gain and bias of the text input
according to the style extracted from a reference speech audio. With SALN, our
model effectively synthesizes speech in the style of the target speaker even
from single speech audio. Furthermore, to enhance StyleSpeech's adaptation to
speech from new speakers, we extend it to Meta-StyleSpeech by introducing two
discriminators trained with style prototypes, and performing episodic training.
The experimental results show that our models generate high-quality speech
which accurately follows the speaker's voice with single short-duration (1-3
sec) speech audio, significantly outperforming baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Min_D/0/1/0/all/0/1"&gt;Dongchan Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dong Bok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eunho Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Squared: A Meta AutoML System. (arXiv:2012.05390v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05390</id>
        <link href="http://arxiv.org/abs/2012.05390"/>
        <updated>2021-06-15T01:45:18.406Z</updated>
        <summary type="html"><![CDATA[There are currently many barriers that prevent non-experts from exploiting
machine learning solutions ranging from the lack of intuition on statistical
learning techniques to the trickiness of hyperparameter tuning. Such barriers
have led to an explosion of interest in automated machine learning (AutoML),
whereby an off-the-shelf system can take care of many of the steps for
end-users without the need for expertise in machine learning. This paper
presents Ensemble Squared (Ensemble$^2$), an AutoML system that ensembles the
results of state-of-the-art open-source AutoML systems. Ensemble$^2$ exploits
the diversity of existing AutoML systems by leveraging the differences in their
model search space and heuristics. Empirically, we show that diversity of each
AutoML system is sufficient to justify ensembling at the AutoML system level.
In demonstrating this, we also establish new state-of-the-art AutoML results on
the OpenML tabular classification benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1"&gt;Jason Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_T/0/1/0/all/0/1"&gt;Tony Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yung_D/0/1/0/all/0/1"&gt;Dylan Yung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasseri_S/0/1/0/all/0/1"&gt;S. Ali Nasseri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1"&gt;Frank Wood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Randomized Entity-wise Factorization for Multi-Agent Reinforcement Learning. (arXiv:2006.04222v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04222</id>
        <link href="http://arxiv.org/abs/2006.04222"/>
        <updated>2021-06-15T01:45:18.397Z</updated>
        <summary type="html"><![CDATA[Multi-agent settings in the real world often involve tasks with varying types
and quantities of agents and non-agent entities; however, common patterns of
behavior often emerge among these agents/entities. Our method aims to leverage
these commonalities by asking the question: ``What is the expected utility of
each agent when only considering a randomly selected sub-group of its observed
entities?'' By posing this counterfactual question, we can recognize
state-action trajectories within sub-groups of entities that we may have
encountered in another task and use what we learned in that task to inform our
prediction in the current one. We then reconstruct a prediction of the full
returns as a combination of factors considering these disjoint groups of
entities and train this ``randomly factorized" value function as an auxiliary
objective for value-based multi-agent reinforcement learning. By doing so, our
model can recognize and leverage similarities across tasks to improve learning
efficiency in a multi-task setting. Our approach, Randomized Entity-wise
Factorization for Imagined Learning (REFIL), outperforms all strong baselines
by a significant margin in challenging multi-task StarCraft micromanagement
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_S/0/1/0/all/0/1"&gt;Shariq Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1"&gt;Christian A. Schroeder de Witt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1"&gt;Bei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bohmer_W/0/1/0/all/0/1"&gt;Wendelin B&amp;#xf6;hmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1"&gt;Shimon Whiteson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1"&gt;Fei Sha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RobustBench: a standardized adversarial robustness benchmark. (arXiv:2010.09670v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09670</id>
        <link href="http://arxiv.org/abs/2010.09670"/>
        <updated>2021-06-15T01:45:18.388Z</updated>
        <summary type="html"><![CDATA[As a research community, we are still lacking a systematic understanding of
the progress on adversarial robustness, which often makes it hard to identify
the most promising ideas in training robust models. A key challenge in
benchmarking robustness is that its evaluation is often error-prone, leading to
overestimation of the true robustness of models. While adaptive attacks
designed for a particular defense are a potential solution, they have to be
highly customized for particular models, which makes it difficult to compare
different methods. Our goal is to instead establish a standardized benchmark of
adversarial robustness, which as accurately as possible reflects the robustness
of the considered models within a reasonable computational budget. To evaluate
the robustness of models for our benchmark, we consider AutoAttack, an ensemble
of white- and black-box attacks which was recently shown in a large-scale study
to improve almost all robustness evaluations compared to the original
publications. We also impose some restrictions on the admitted models to rule
out defenses that only make gradient-based attacks ineffective without
improving actual robustness. Our leaderboard, hosted at
https://robustbench.github.io/, contains evaluations of 90+ models and aims at
reflecting the current state of the art on a set of well-defined tasks in
$\ell_\infty$- and $\ell_2$-threat models and on common corruptions, with
possible extensions in the future. Additionally, we open-source the library
https://github.com/RobustBench/robustbench that provides unified access to 60+
robust models to facilitate their downstream applications. Finally, based on
the collected models, we analyze the impact of robustness on the performance on
distribution shifts, calibration, out-of-distribution detection, fairness,
privacy leakage, smoothness, and transferability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1"&gt;Francesco Croce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andriushchenko_M/0/1/0/all/0/1"&gt;Maksym Andriushchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1"&gt;Vikash Sehwag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Debenedetti_E/0/1/0/all/0/1"&gt;Edoardo Debenedetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1"&gt;Nicolas Flammarion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1"&gt;Mung Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1"&gt;Prateek Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1"&gt;Matthias Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoScore-Survival: Developing interpretable machine learning-based time-to-event scores with right-censored survival data. (arXiv:2106.06957v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06957</id>
        <link href="http://arxiv.org/abs/2106.06957"/>
        <updated>2021-06-15T01:45:18.378Z</updated>
        <summary type="html"><![CDATA[Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.

AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.

The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.

Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1"&gt;Feng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1"&gt;Yilin Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Han Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_B/0/1/0/all/0/1"&gt;Benjamin Alan Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_M/0/1/0/all/0/1"&gt;Marcus Eng Hock Ong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Nan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1"&gt;Bibhas Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User Label Leakage from Gradients in Federated Learning. (arXiv:2105.09369v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09369</id>
        <link href="http://arxiv.org/abs/2105.09369"/>
        <updated>2021-06-15T01:45:18.371Z</updated>
        <summary type="html"><![CDATA[Federated learning enables multiple users to build a joint model by sharing
their model updates (gradients), while their raw data remains local on their
devices. In contrast to the common belief that this provides privacy benefits,
we here add to the very recent results on privacy risks when sharing gradients.
Specifically, we propose Label Leakage from Gradients (LLG), a novel attack to
extract the labels of the users' training data from their shared gradients. The
attack exploits the direction and magnitude of gradients to determine the
presence or absence of any label. LLG is simple yet effective, capable of
leaking potential sensitive information represented by labels, and scales well
to arbitrary batch sizes and multiple classes. We empirically and
mathematically demonstrate the validity of our attack under different settings.
Moreover, empirical results show that LLG successfully extracts labels with
high accuracy at the early stages of model training. We also discuss different
defense mechanisms against such leakage. Our findings suggest that gradient
compression is a practical technique to prevent our attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wainakh_A/0/1/0/all/0/1"&gt;Aidmar Wainakh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1"&gt;Fabrizio Ventola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mussig_T/0/1/0/all/0/1"&gt;Till M&amp;#xfc;&amp;#xdf;ig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keim_J/0/1/0/all/0/1"&gt;Jens Keim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cordero_C/0/1/0/all/0/1"&gt;Carlos Garcia Cordero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_E/0/1/0/all/0/1"&gt;Ephraim Zimmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grube_T/0/1/0/all/0/1"&gt;Tim Grube&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muhlhauser_M/0/1/0/all/0/1"&gt;Max M&amp;#xfc;hlh&amp;#xe4;user&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape of Elephant: Study of Macro Properties of Word Embeddings Spaces. (arXiv:2106.06964v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06964</id>
        <link href="http://arxiv.org/abs/2106.06964"/>
        <updated>2021-06-15T01:45:18.360Z</updated>
        <summary type="html"><![CDATA[Pre-trained word representations became a key component in many NLP tasks.
However, the global geometry of the word embeddings remains poorly understood.
In this paper, we demonstrate that a typical word embeddings cloud is shaped as
a high-dimensional simplex with interpretable vertices and propose a simple yet
effective method for enumeration of these vertices. We show that the proposed
method can detect and describe vertices of the simplex for GloVe and fasttext
spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1"&gt;Alexey Tikhonov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Gaussian Process Regression Based on Iterative Trimming. (arXiv:2011.11057v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11057</id>
        <link href="http://arxiv.org/abs/2011.11057"/>
        <updated>2021-06-15T01:45:18.336Z</updated>
        <summary type="html"><![CDATA[The Gaussian process (GP) regression can be severely biased when the data are
contaminated by outliers. This paper presents a new robust GP regression
algorithm that iteratively trims the most extreme data points. While the new
algorithm retains the attractive properties of the standard GP as a
nonparametric and flexible regression method, it can greatly improve the model
accuracy for contaminated data even in the presence of extreme or abundant
outliers. It is also easier to implement compared with previous robust GP
variants that rely on approximate inference. Applied to a wide range of
experiments with different contamination levels, the proposed method
significantly outperforms the standard GP and the popular robust GP variant
with the Student-t likelihood in most test cases. In addition, as a practical
example in the astrophysical study, we show that this method can precisely
determine the main-sequence ridge line in the color-magnitude diagram of star
clusters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhao-Zhou Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhengyi Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariant Networks for Pixelized Spheres. (arXiv:2106.06662v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06662</id>
        <link href="http://arxiv.org/abs/2106.06662"/>
        <updated>2021-06-15T01:45:18.324Z</updated>
        <summary type="html"><![CDATA[Pixelizations of Platonic solids such as the cube and icosahedron have been
widely used to represent spherical data, from climate records to Cosmic
Microwave Background maps. Platonic solids have well-known global symmetries.
Once we pixelize each face of the solid, each face also possesses its own local
symmetries in the form of Euclidean isometries. One way to combine these
symmetries is through a hierarchy. However, this approach does not adequately
model the interplay between the two levels of symmetry transformations. We show
how to model this interplay using ideas from group theory, identify the
equivariant linear maps, and introduce equivariant padding that respects these
symmetries. Deep networks that use these maps as their building blocks
generalize gauge equivariant CNNs on pixelized spheres. These deep networks
achieve state-of-the-art results on semantic segmentation for climate data and
omnidirectional image processing. Code is available at https://git.io/JGiZA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shakerinava_M/0/1/0/all/0/1"&gt;Mehran Shakerinava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravanbakhsh_S/0/1/0/all/0/1"&gt;Siamak Ravanbakhsh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search. (arXiv:2010.07003v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07003</id>
        <link href="http://arxiv.org/abs/2010.07003"/>
        <updated>2021-06-15T01:45:18.309Z</updated>
        <summary type="html"><![CDATA[Despite transformers' impressive accuracy, their computational cost is often
prohibitive to use with limited computational resources. Most previous
approaches to improve inference efficiency require a separate model for each
possible computational budget. In this paper, we extend PoWER-BERT (Goyal et
al., 2020) and propose Length-Adaptive Transformer that can be used for various
inference scenarios after one-shot training. We train a transformer with
LengthDrop, a structural variant of dropout, which stochastically determines a
sequence length at each layer. We then conduct a multi-objective evolutionary
search to find a length configuration that maximizes the accuracy and minimizes
the efficiency metric under any given computational budget. Additionally, we
significantly extend the applicability of PoWER-BERT beyond sequence-level
classification into token-level classification with Drop-and-Restore process
that drops word-vectors temporarily in intermediate layers and restores at the
last layer if necessary. We empirically verify the utility of the proposed
approach by demonstrating the superior accuracy-efficiency trade-off under
various setups, including span-based question answering and text
classification. Code is available at
https://github.com/clovaai/length-adaptive-transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gyuwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task Transformer Network for Joint MRI Reconstruction and Super-Resolution. (arXiv:2106.06742v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06742</id>
        <link href="http://arxiv.org/abs/2106.06742"/>
        <updated>2021-06-15T01:45:18.279Z</updated>
        <summary type="html"><![CDATA[The core problem of Magnetic Resonance Imaging (MRI) is the trade off between
acceleration and image quality. Image reconstruction and super-resolution are
two crucial techniques in Magnetic Resonance Imaging (MRI). Current methods are
designed to perform these tasks separately, ignoring the correlations between
them. In this work, we propose an end-to-end task transformer network
(T$^2$Net) for joint MRI reconstruction and super-resolution, which allows
representations and feature transmission to be shared between multiple task to
achieve higher-quality, super-resolved and motion-artifacts-free images from
highly undersampled and degenerated MRI data. Our framework combines both
reconstruction and super-resolution, divided into two sub-branches, whose
features are expressed as queries and keys. Specifically, we encourage joint
feature learning between the two tasks, thereby transferring accurate task
information. We first use two separate CNN branches to extract task-specific
features. Then, a task transformer module is designed to embed and synthesize
the relevance between the two tasks. Experimental results show that our
multi-task model significantly outperforms advanced sequential methods, both
quantitatively and qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chun-Mei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yunlu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-free Reinforcement Learning for Branching Markov Decision Processes. (arXiv:2106.06777v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06777</id>
        <link href="http://arxiv.org/abs/2106.06777"/>
        <updated>2021-06-15T01:45:18.273Z</updated>
        <summary type="html"><![CDATA[We study reinforcement learning for the optimal control of Branching Markov
Decision Processes (BMDPs), a natural extension of (multitype) Branching Markov
Chains (BMCs). The state of a (discrete-time) BMCs is a collection of entities
of various types that, while spawning other entities, generate a payoff. In
comparison with BMCs, where the evolution of a each entity of the same type
follows the same probabilistic pattern, BMDPs allow an external controller to
pick from a range of options. This permits us to study the best/worst behaviour
of the system. We generalise model-free reinforcement learning techniques to
compute an optimal control strategy of an unknown BMDP in the limit. We present
results of an implementation that demonstrate the practicality of the approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hahn_E/0/1/0/all/0/1"&gt;Ernst Moritz Hahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1"&gt;Mateo Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schewe_S/0/1/0/all/0/1"&gt;Sven Schewe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Somenzi_F/0/1/0/all/0/1"&gt;Fabio Somenzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Ashutosh Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wojtczak_D/0/1/0/all/0/1"&gt;Dominik Wojtczak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep manifold learning reveals hidden dynamics of proteasome autoregulation. (arXiv:2012.12854v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12854</id>
        <link href="http://arxiv.org/abs/2012.12854"/>
        <updated>2021-06-15T01:45:18.267Z</updated>
        <summary type="html"><![CDATA[The 2.5-MDa 26S proteasome maintains proteostasis and regulates myriad
cellular processes. How polyubiquitylated substrate interactions regulate
proteasome activity is not understood. Here we introduce a deep manifold
learning framework, named AlphaCryo4D, which enables atomic-level cryogenic
electron microscopy (cryo-EM) reconstructions of nonequilibrium conformational
continuum and reconstitutes hidden dynamics of proteasome autoregulation in the
act of substrate degradation. AlphaCryo4D integrates 3D deep residual learning
with manifold embedding of free-energy landscapes, which directs 3D clustering
via an energy-based particle-voting algorithm. In blind assessments using
simulated heterogeneous cryo-EM datasets, AlphaCryo4D achieved 3D
classification accuracy three times that of conventional method and
reconstructed continuous conformational changes of a 130-kDa protein at
sub-3-angstrom resolution. By using AlphaCryo4D to analyze a single
experimental cryo-EM dataset, we identified 64 conformers of the
substrate-bound human 26S proteasome, revealing conformational entanglement of
two regulatory particles in the doubly capped holoenzymes and their energetic
differences with singly capped ones. Novel ubiquitin-binding sites are
discovered on the RPN2, RPN10 and Alpha5 subunits to remodel polyubiquitin
chains for deubiquitylation and recycle. Importantly, AlphaCryo4D choreographs
single-nucleotide-exchange dynamics of proteasomal AAA-ATPase motor during
translocation initiation, which upregulates proteolytic activity by
allosterically promoting nucleophilic attack. Our systemic analysis illuminates
a grand hierarchical allostery for proteasome autoregulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhaolong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Li Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yinping Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yuanchen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Youdong Mao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Persistent Exploration in Continuous Control Tasks with Sparse Rewards. (arXiv:2012.13658v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13658</id>
        <link href="http://arxiv.org/abs/2012.13658"/>
        <updated>2021-06-15T01:45:18.261Z</updated>
        <summary type="html"><![CDATA[A major challenge in reinforcement learning is the design of exploration
strategies, especially for environments with sparse reward structures and
continuous state and action spaces. Intuitively, if the reinforcement signal is
very scarce, the agent should rely on some form of short-term memory in order
to cover its environment efficiently. We propose a new exploration method,
based on two intuitions: (1) the choice of the next exploratory action should
depend not only on the (Markovian) state of the environment, but also on the
agent's trajectory so far, and (2) the agent should utilize a measure of spread
in the state space to avoid getting stuck in a small region. Our method
leverages concepts often used in statistical physics to provide explanations
for the behavior of simplified (polymer) chains in order to generate persistent
(locally self-avoiding) trajectories in state space. We discuss the theoretical
properties of locally self-avoiding walks and their ability to provide a kind
of short-term memory through a decaying temporal correlation within the
trajectory. We provide empirical evaluations of our approach in a simulated 2D
navigation task, as well as higher-dimensional MuJoCo continuous control
locomotion tasks with sparse rewards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amin_S/0/1/0/all/0/1"&gt;Susan Amin&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Gomrokchi_M/0/1/0/all/0/1"&gt;Maziar Gomrokchi&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Aboutalebi_H/0/1/0/all/0/1"&gt;Hossein Aboutalebi&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Satija_H/0/1/0/all/0/1"&gt;Harsh Satija&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt; (1 and 2) ((1) McGill University, (2) Mila- Quebec Artificial Intelligence Institute, (3) University of Waterloo)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matrix games with bandit feedback. (arXiv:2006.05145v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05145</id>
        <link href="http://arxiv.org/abs/2006.05145"/>
        <updated>2021-06-15T01:45:18.255Z</updated>
        <summary type="html"><![CDATA[We study a version of the classical zero-sum matrix game with unknown payoff
matrix and bandit feedback, where the players only observe each others actions
and a noisy payoff. This generalizes the usual matrix game, where the payoff
matrix is known to the players. Despite numerous applications, this problem has
received relatively little attention. Although adversarial bandit algorithms
achieve low regret, they do not exploit the matrix structure and perform poorly
relative to the new algorithms. The main contributions are regret analyses of
variants of UCB and K-learning that hold for any opponent, e.g., even when the
opponent adversarially plays the best-response to the learner's mixed strategy.
Along the way, we show that Thompson fails catastrophically in this setting and
provide empirical comparison to existing algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+ODonoghue_B/0/1/0/all/0/1"&gt;Brendan O&amp;#x27;Donoghue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lattimore_T/0/1/0/all/0/1"&gt;Tor Lattimore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1"&gt;Ian Osband&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack. (arXiv:2106.06895v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.06895</id>
        <link href="http://arxiv.org/abs/2106.06895"/>
        <updated>2021-06-15T01:45:18.236Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNN) have shown impressive performance in
computer vision, natural language processing, and many other applications, but
they exhibit high computations and substantial memory requirements. To address
these limitations, especially in resource-constrained devices, the use of cloud
computing for CNNs is becoming more popular. This comes with privacy and
latency concerns that have motivated the designers to develop embedded hardware
accelerators for CNNs. However, designing a specialized accelerator increases
the time-to-market and cost of production. Therefore, to reduce the
time-to-market and access to state-of-the-art techniques, CNN hardware mapping
and deployment on embedded accelerators are often outsourced to untrusted third
parties, which is going to be more prevalent in futuristic artificial
intelligence of things (AIoT) systems. These AIoT systems anticipate horizontal
collaboration among different resource-constrained AIoT node devices, where CNN
layers are partitioned and these devices collaboratively compute complex CNN
tasks Therefore, there is a dire need to explore this attack surface for
designing secure embedded hardware accelerators for CNNs. Towards this goal, in
this paper, we exploited this attack surface to propose an HT-based attack
called FeSHI. This attack exploits the statistical distribution i.e., Gaussian
distribution, of the layer-by-layer feature maps of the CNN to design two
triggers for stealthy HT with a very low probability of triggering. To
illustrate the effectiveness of the proposed attack, we deployed the LeNet and
LeNet-3D on PYNQ to classify the MNIST and CIFAR-10 datasets, respectively, and
tested FeSHI. The experimental results show that FeSHI utilizes up to 2% extra
LUTs, and the overall resource overhead is less than 1% compared to the
original designs]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1"&gt;Tolulope Odetola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khalid_F/0/1/0/all/0/1"&gt;Faiq Khalid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandefur_T/0/1/0/all/0/1"&gt;Travis Sandefur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammed_H/0/1/0/all/0/1"&gt;Hawzhin Mohammed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1"&gt;Syed Rafay Hasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling the Roles of Curation, Data-Augmentation and the Prior in the Cold Posterior Effect. (arXiv:2106.06596v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06596</id>
        <link href="http://arxiv.org/abs/2106.06596"/>
        <updated>2021-06-15T01:45:18.194Z</updated>
        <summary type="html"><![CDATA[The "cold posterior effect" (CPE) in Bayesian deep learning describes the
uncomforting observation that the predictive performance of Bayesian neural
networks can be significantly improved if the Bayes posterior is artificially
sharpened using a temperature parameter T<1. The CPE is problematic in theory
and practice and since the effect was identified many researchers have proposed
hypotheses to explain the phenomenon. However, despite this intensive research
effort the effect remains poorly understood. In this work we provide novel and
nuanced evidence relevant to existing explanations for the cold posterior
effect, disentangling three hypotheses: 1. The dataset curation hypothesis of
Aitchison (2020): we show empirically that the CPE does not arise in a real
curated data set but can be produced in a controlled experiment with varying
curation strength. 2. The data augmentation hypothesis of Izmailov et al.
(2021) and Fortuin et al. (2021): we show empirically that data augmentation is
sufficient but not necessary for the CPE to be present. 3. The bad prior
hypothesis of Wenzel et al. (2020): we use a simple experiment evaluating the
relative importance of the prior and the likelihood, strongly linking the CPE
to the prior. Our results demonstrate how the CPE can arise in isolation from
synthetic curation, data augmentation, and bad priors. Cold posteriors observed
"in the wild" are therefore unlikely to arise from a single simple cause; as a
result, we do not expect a simple "fix" for cold posteriors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noci_L/0/1/0/all/0/1"&gt;Lorenzo Noci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1"&gt;Kevin Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachmann_G/0/1/0/all/0/1"&gt;Gregor Bachmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowozin_S/0/1/0/all/0/1"&gt;Sebastian Nowozin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1"&gt;Thomas Hofmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-Timescale Actor-Critic Provably Finds Globally Optimal Policy. (arXiv:2008.00483v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00483</id>
        <link href="http://arxiv.org/abs/2008.00483"/>
        <updated>2021-06-15T01:45:18.183Z</updated>
        <summary type="html"><![CDATA[We study the global convergence and global optimality of actor-critic, one of
the most popular families of reinforcement learning algorithms. While most
existing works on actor-critic employ bi-level or two-timescale updates, we
focus on the more practical single-timescale setting, where the actor and
critic are updated simultaneously. Specifically, in each iteration, the critic
update is obtained by applying the Bellman evaluation operator only once while
the actor is updated in the policy gradient direction computed using the
critic. Moreover, we consider two function approximation settings where both
the actor and critic are represented by linear or deep neural networks. For
both cases, we prove that the actor sequence converges to a globally optimal
policy at a sublinear $O(K^{-1/2})$ rate, where $K$ is the number of
iterations. To the best of our knowledge, we establish the rate of convergence
and global optimality of single-timescale actor-critic with linear function
approximation for the first time. Moreover, under the broader scope of policy
optimization with nonlinear function approximation, we prove that actor-critic
with deep neural network finds the globally optimal policy at a sublinear rate
for the first time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zuyue Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoran Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Optimization for Multilingual Translation with Imbalanced Data. (arXiv:2104.07639v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07639</id>
        <link href="http://arxiv.org/abs/2104.07639"/>
        <updated>2021-06-15T01:45:18.156Z</updated>
        <summary type="html"><![CDATA[Multilingual models are parameter-efficient with the prospect improving
low-resource languages by leveraging crosslingual transfer. Despite recent
advance in massive multilingual translation with ever-growing model and data,
how to effectively train multilingual models has not been well understood. In
this paper, we show that a common situation in multilingual training, data
imbalance among languages, poses optimization tension between high resource and
low resource languages where the found multilingual solution is often
sub-optimal for low resources. We show that common training method which
upsamples low resources can not robustly optimize population loss with risks of
either underfitting high resource languages or overfitting low resource ones.
Drawing on recent findings on the geometry of loss landscape and its effect on
generalization, we propose a principled optimization algorithm, Curvature Aware
Task Scaling (CATS), which adaptively rescales gradients from different tasks
with a meta objective of guiding multilingual training to low-curvature
neighborhoods with uniformly low loss for all languages. We ran experiments on
common benchmarks (TED, WMT and OPUS-100) with varying degrees of data
imbalance. CATS effectively improved multilingual optimization and as a result
demonstrated consistent gains on low resources ( to BLEU) without hurting high
resources. In addition, CATS is robust to overparameterization and large batch
size training, making it a promising training method for massive multilingual
models that truly improve low resource languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1"&gt;Hongyu Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Network-Based Anomaly Detection in Multivariate Time Series. (arXiv:2106.06947v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06947</id>
        <link href="http://arxiv.org/abs/2106.06947"/>
        <updated>2021-06-15T01:45:18.141Z</updated>
        <summary type="html"><![CDATA[Given high-dimensional time series data (e.g., sensor data), how can we
detect anomalous events, such as system faults and attacks? More challengingly,
how can we do this in a way that captures complex inter-sensor relationships,
and detects and explains anomalies which deviate from these relationships?
Recently, deep learning approaches have enabled improvements in anomaly
detection in high-dimensional datasets; however, existing methods do not
explicitly learn the structure of existing relationships between variables, or
use them to predict the expected behavior of time series. Our approach combines
a structure learning approach with graph neural networks, additionally using
attention weights to provide explainability for the detected anomalies.
Experiments on two real-world sensor datasets with ground truth anomalies show
that our method detects anomalies more accurately than baseline approaches,
accurately captures correlations between sensors, and allows users to deduce
the root cause of a detected anomaly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_A/0/1/0/all/0/1"&gt;Ailin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1"&gt;Bryan Hooi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FeatureNorm: L2 Feature Normalization for Dynamic Graph Embedding. (arXiv:2103.00164v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00164</id>
        <link href="http://arxiv.org/abs/2103.00164"/>
        <updated>2021-06-15T01:45:18.070Z</updated>
        <summary type="html"><![CDATA[Dynamic graphs arise in a plethora of practical scenarios such as social
networks, communication networks, and financial transaction networks. Given a
dynamic graph, it is fundamental and essential to learn a graph representation
that is expected not only to preserve structural proximity but also jointly
capture the time-evolving patterns. Recently, graph convolutional network (GCN)
has been widely explored and used in non-Euclidean application domains. The
main success of GCN, especially in handling dependencies and passing messages
within nodes, lies in its approximation to Laplacian smoothing. As a matter of
fact, this smoothing technique can not only encourage must-link node pairs to
get closer but also push cannot-link pairs to shrink together, which
potentially cause serious feature shrink or oversmoothing problem, especially
when stacking graph convolution in multiple layers or steps. For learning
time-evolving patterns, a natural solution is to preserve historical state and
combine it with the current interactions to obtain the most recent
representation. Then the serious feature shrink or oversmoothing problem could
happen when stacking graph convolution explicitly or implicitly according to
current prevalent methods, which would make nodes too similar to distinguish
each other. To solve this problem in dynamic graph embedding, we analyze the
shrinking properties in the node embedding space at first, and then design a
simple yet versatile method, which exploits L2 feature normalization constraint
to rescale all nodes to hypersphere of a unit ball so that nodes would not
shrink together, and yet similar nodes can still get closer. Extensive
experiments on four real-world dynamic graph datasets compared with competitive
baseline models demonstrate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Menglin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Ziqiao Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tutorial on Sparse Gaussian Processes and Variational Inference. (arXiv:2012.13962v10 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13962</id>
        <link href="http://arxiv.org/abs/2012.13962"/>
        <updated>2021-06-15T01:45:18.022Z</updated>
        <summary type="html"><![CDATA[Gaussian processes (GPs) provide a framework for Bayesian inference that can
offer principled uncertainty estimates for a large range of problems. For
example, if we consider regression problems with Gaussian likelihoods, a GP
model enjoys a posterior in closed form. However, identifying the posterior GP
scales cubically with the number of training examples and requires to store all
examples in memory. In order to overcome these obstacles, sparse GPs have been
proposed that approximate the true posterior GP with pseudo-training examples.
Importantly, the number of pseudo-training examples is user-defined and enables
control over computational and memory complexity. In the general case, sparse
GPs do not enjoy closed-form solutions and one has to resort to approximate
inference. In this context, a convenient choice for approximate inference is
variational inference (VI), where the problem of Bayesian inference is cast as
an optimization problem -- namely, to maximize a lower bound of the log
marginal likelihood. This paves the way for a powerful and versatile framework,
where pseudo-training examples are treated as optimization arguments of the
approximate posterior that are jointly identified together with hyperparameters
of the generative model (i.e. prior and likelihood). The framework can
naturally handle a wide scope of supervised learning problems, ranging from
regression with heteroscedastic and non-Gaussian likelihoods to classification
problems with discrete labels, but also multilabel problems. The purpose of
this tutorial is to provide access to the basic matter for readers without
prior knowledge in both GPs and VI. A proper exposition to the subject enables
also access to more recent advances (like importance-weighted VI as well as
interdomain, multioutput and deep GPs) that can serve as an inspiration for new
research ideas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leibfried_F/0/1/0/all/0/1"&gt;Felix Leibfried&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutordoir_V/0/1/0/all/0/1"&gt;Vincent Dutordoir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+John_S/0/1/0/all/0/1"&gt;ST John&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrande_N/0/1/0/all/0/1"&gt;Nicolas Durrande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LaProp: Separating Momentum and Adaptivity in Adam. (arXiv:2002.04839v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.04839</id>
        <link href="http://arxiv.org/abs/2002.04839"/>
        <updated>2021-06-15T01:45:18.015Z</updated>
        <summary type="html"><![CDATA[We identity a by-far-unrecognized problem of Adam-style optimizers which
results from unnecessary coupling between momentum and adaptivity. The coupling
leads to instability and divergence when the momentum and adaptivity parameters
are mismatched. In this work, we propose a method, Laprop, which decouples
momentum and adaptivity in the Adam-style methods. We show that the decoupling
leads to greater flexibility in the hyperparameters and allows for a
straightforward interpolation between the signed gradient methods and the
adaptive gradient methods. We experimentally show that Laprop has consistently
improved speed and stability over Adam on a variety of tasks. We also bound the
regret of Laprop on a convex problem and show that our bound differs from that
of Adam by a key factor, which demonstrates its advantage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1"&gt;Liu Ziyin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhikang T.Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1"&gt;Masahito Ueda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Can I Do Here? Learning New Skills by Imagining Visual Affordances. (arXiv:2106.00671v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00671</id>
        <link href="http://arxiv.org/abs/2106.00671"/>
        <updated>2021-06-15T01:45:18.003Z</updated>
        <summary type="html"><![CDATA[A generalist robot equipped with learned skills must be able to perform many
tasks in many different environments. However, zero-shot generalization to new
settings is not always possible. When the robot encounters a new environment or
object, it may need to finetune some of its previously learned skills to
accommodate this change. But crucially, previously learned behaviors and models
should still be suitable to accelerate this relearning. In this paper, we aim
to study how generative models of possible outcomes can allow a robot to learn
visual representations of affordances, so that the robot can sample potentially
possible outcomes in new situations, and then further train its policy to
achieve those outcomes. In effect, prior data is used to learn what kinds of
outcomes may be possible, such that when the robot encounters an unfamiliar
setting, it can sample potential outcomes from its model, attempt to reach
them, and thereby update both its skills and its outcome model. This approach,
visuomotor affordance learning (VAL), can be used to train goal-conditioned
policies that operate on raw image inputs, and can rapidly learn to manipulate
new objects via our proposed affordance-directed exploration scheme. We show
that VAL can utilize prior data to solve real-world tasks such drawer opening,
grasping, and placing objects in new scenes with only five minutes of online
experience in the new scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khazatsky_A/0/1/0/all/0/1"&gt;Alexander Khazatsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1"&gt;Ashvin Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_D/0/1/0/all/0/1"&gt;Daniel Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learngene: From Open-World to Your Learning Task. (arXiv:2106.06788v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06788</id>
        <link href="http://arxiv.org/abs/2106.06788"/>
        <updated>2021-06-15T01:45:17.959Z</updated>
        <summary type="html"><![CDATA[Although deep learning has made significant progress on fixed large-scale
datasets, it typically encounters challenges regarding improperly detecting
new/unseen classes in the open-world classification, over-parametrized, and
overfitting small samples. In contrast, biological systems can overcome the
above difficulties very well. Individuals inherit an innate gene from
collective creatures that have evolved over hundreds of millions of years, and
can learn new skills through a few examples. Inspired by this, we propose a
practical collective-individual paradigm where open-world tasks are trained in
sequence using an evolution (expandable) network. To be specific, we
innovatively introduce learngene that inherits the meta-knowledge from the
collective model and reconstructs a new lightweight individual model for the
target task, to realize the collective-individual paradigm. Particularly, we
present a novel criterion that can discover the learngene in the collective
model, according to the gradient information. Finally, the individual model is
trained only with a few samples in the absence of the source data. We
demonstrate the effectiveness of our approach in an extensive empirical study
and theoretical analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiufeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1"&gt;Xin Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shuxia Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1"&gt;Shiyu Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lei Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1"&gt;Ning Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning. (arXiv:2101.07393v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07393</id>
        <link href="http://arxiv.org/abs/2101.07393"/>
        <updated>2021-06-15T01:45:17.930Z</updated>
        <summary type="html"><![CDATA[We investigate the use of natural language to drive the generalization of
control policies and introduce the new multi-task environment Messenger with
free-form text manuals describing the environment dynamics. Unlike previous
work, Messenger does not assume prior knowledge connecting text and state
observations $-$ the control policy must simultaneously ground the game manual
to entity symbols and dynamics in the environment. We develop a new model, EMMA
(Entity Mapper with Multi-modal Attention) which uses an entity-conditioned
attention module that allows for selective focus over relevant descriptions in
the manual for each entity in the environment. EMMA is end-to-end
differentiable and learns a latent grounding of entities and dynamics from text
to observations using only environment rewards. EMMA achieves successful
zero-shot generalization to unseen games with new dynamics, obtaining a 40%
higher win rate compared to multiple baselines. However, win rate on the
hardest stage of Messenger remains low (10%), demonstrating the need for
additional work in this direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hanjie_A/0/1/0/all/0/1"&gt;Austin W. Hanjie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1"&gt;Victor Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning Bidirectional Update Rules. (arXiv:2104.04657v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04657</id>
        <link href="http://arxiv.org/abs/2104.04657"/>
        <updated>2021-06-15T01:45:17.924Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a new type of generalized neural network where
neurons and synapses maintain multiple states. We show that classical
gradient-based backpropagation in neural networks can be seen as a special case
of a two-state network where one state is used for activations and another for
gradients, with update rules derived from the chain rule. In our generalized
framework, networks have neither explicit notion of nor ever receive gradients.
The synapses and neurons are updated using a bidirectional Hebb-style update
rule parameterized by a shared low-dimensional "genome". We show that such
genomes can be meta-learned from scratch, using either conventional
optimization techniques, or evolutionary strategies, such as CMA-ES. Resulting
update rules generalize to unseen tasks and train faster than gradient descent
based optimizers for several standard computer vision and synthetic tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1"&gt;Mark Sandler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vladymyrov_M/0/1/0/all/0/1"&gt;Max Vladymyrov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1"&gt;Andrey Zhmoginov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_N/0/1/0/all/0/1"&gt;Nolan Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jackson_A/0/1/0/all/0/1"&gt;Andrew Jackson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madams_T/0/1/0/all/0/1"&gt;Tom Madams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1"&gt;Blaise Aguera y Arcas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Markov Decision Processes with Long-Term Average Constraints. (arXiv:2106.06680v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06680</id>
        <link href="http://arxiv.org/abs/2106.06680"/>
        <updated>2021-06-15T01:45:17.918Z</updated>
        <summary type="html"><![CDATA[We consider the problem of constrained Markov Decision Process (CMDP) where
an agent interacts with a unichain Markov Decision Process. At every
interaction, the agent obtains a reward. Further, there are $K$ cost functions.
The agent aims to maximize the long-term average reward while simultaneously
keeping the $K$ long-term average costs lower than a certain threshold. In this
paper, we propose CMDP-PSRL, a posterior sampling based algorithm using which
the agent can learn optimal policies to interact with the CMDP. Further, for
MDP with $S$ states, $A$ actions, and diameter $D$, we prove that following
CMDP-PSRL algorithm, the agent can bound the regret of not accumulating rewards
from optimal policy by $\Tilde{O}(poly(DSA)\sqrt{T})$. Further, we show that
the violations for any of the $K$ constraints is also bounded by
$\Tilde{O}(poly(DSA)\sqrt{T})$. To the best of our knowledge, this is the first
work which obtains a $\Tilde{O}(\sqrt{T})$ regret bounds for ergodic MDPs with
long-term average constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1"&gt;Mridul Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1"&gt;Qinbo Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1"&gt;Vaneet Aggarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple Unified Framework for High Dimensional Bandit Problems. (arXiv:2102.09626v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09626</id>
        <link href="http://arxiv.org/abs/2102.09626"/>
        <updated>2021-06-15T01:45:17.912Z</updated>
        <summary type="html"><![CDATA[Stochastic high dimensional bandit problems with low dimensional structures
are useful in different applications such as online advertising and drug
discovery. In this work, we propose a simple unified algorithm for such
problems and present a general analysis framework for the regret upper bound of
our algorithm. We show that under some mild unified assumptions, our algorithm
can be applied to different high dimensional bandit problems. Our framework
utilizes the low dimensional structure to guide the parameter estimation in the
problem, therefore our algorithm achieves the best regret bounds in the LASSO
bandit, as well as novel bounds in the low-rank matrix bandit, the group sparse
matrix bandit, and in a new problem: the multi-agent LASSO bandit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barik_A/0/1/0/all/0/1"&gt;Adarsh Barik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honorio_J/0/1/0/all/0/1"&gt;Jean Honorio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recomposing the Reinforcement Learning Building Blocks with Hypernetworks. (arXiv:2106.06842v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06842</id>
        <link href="http://arxiv.org/abs/2106.06842"/>
        <updated>2021-06-15T01:45:17.835Z</updated>
        <summary type="html"><![CDATA[The Reinforcement Learning (RL) building blocks, i.e. Q-functions and policy
networks, usually take elements from the cartesian product of two domains as
input. In particular, the input of the Q-function is both the state and the
action, and in multi-task problems (Meta-RL) the policy can take a state and a
context. Standard architectures tend to ignore these variables' underlying
interpretations and simply concatenate their features into a single vector. In
this work, we argue that this choice may lead to poor gradient estimation in
actor-critic algorithms and high variance learning steps in Meta-RL algorithms.
To consider the interaction between the input variables, we suggest using a
Hypernetwork architecture where a primary network determines the weights of a
conditional dynamic network. We show that this approach improves the gradient
approximation and reduces the learning step variance, which both accelerates
learning and improves the final performance. We demonstrate a consistent
improvement across different locomotion tasks and different algorithms both in
RL (TD3 and SAC) and in Meta-RL (MAML and PEARL).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keynan_S/0/1/0/all/0/1"&gt;Shai Keynan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarafian_E/0/1/0/all/0/1"&gt;Elad Sarafian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kraus_S/0/1/0/all/0/1"&gt;Sarit Kraus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning on Non-IID Data: A Survey. (arXiv:2106.06843v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06843</id>
        <link href="http://arxiv.org/abs/2106.06843"/>
        <updated>2021-06-15T01:45:17.830Z</updated>
        <summary type="html"><![CDATA[Federated learning is an emerging distributed machine learning framework for
privacy preservation. However, models trained in federated learning usually
have worse performance than those trained in the standard centralized learning
mode, especially when the training data are not independent and identically
distributed (Non-IID) on the local devices. In this survey, we pro-vide a
detailed analysis of the influence of Non-IID data on both parametric and
non-parametric machine learning models in both horizontal and vertical
federated learning. In addition, cur-rent research work on handling challenges
of Non-IID data in federated learning are reviewed, and both advantages and
disadvantages of these approaches are discussed. Finally, we suggest several
future research directions before concluding the paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hangyu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinjin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yaochu Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating Treatment Effects with Observed Confounders and Mediators. (arXiv:2003.11991v3 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.11991</id>
        <link href="http://arxiv.org/abs/2003.11991"/>
        <updated>2021-06-15T01:45:17.824Z</updated>
        <summary type="html"><![CDATA[Given a causal graph, the do-calculus can express treatment effects as
functionals of the observational joint distribution that can be estimated
empirically. Sometimes the do-calculus identifies multiple valid formulae,
prompting us to compare the statistical properties of the corresponding
estimators. For example, the backdoor formula applies when all confounders are
observed and the frontdoor formula applies when an observed mediator transmits
the causal effect. In this paper, we investigate the over-identified scenario
where both confounders and mediators are observed, rendering both estimators
valid. Addressing the linear Gaussian causal model, we demonstrate that either
estimator can dominate the other by an unbounded constant factor. Next, we
derive an optimal estimator, which leverages all observed variables, and bound
its finite-sample variance. We show that it strictly outperforms the backdoor
and frontdoor estimators and that this improvement can be unbounded. We also
present a procedure for combining two datasets, one with observed confounders
and another with observed mediators. Finally, we evaluate our methods on both
simulated data and the IHDP and JTPA datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Shantanu Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Childers_D/0/1/0/all/0/1"&gt;David Childers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning of Neural Architectures for Few-Shot Learning. (arXiv:1911.11090v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.11090</id>
        <link href="http://arxiv.org/abs/1911.11090"/>
        <updated>2021-06-15T01:45:17.818Z</updated>
        <summary type="html"><![CDATA[The recent progress in neural architecture search (NAS) has allowed scaling
the automated design of neural architectures to real-world domains, such as
object detection and semantic segmentation. However, one prerequisite for the
application of NAS are large amounts of labeled data and compute resources.
This renders its application challenging in few-shot learning scenarios, where
many related tasks need to be learned, each with limited amounts of data and
compute time. Thus, few-shot learning is typically done with a fixed neural
architecture. To improve upon this, we propose MetaNAS, the first method which
fully integrates NAS with gradient-based meta-learning. MetaNAS optimizes a
meta-architecture along with the meta-weights during meta-training. During
meta-testing, architectures can be adapted to a novel task with a few steps of
the task optimizer, that is: task adaptation becomes computationally cheap and
requires only little data per task. Moreover, MetaNAS is agnostic in that it
can be used with arbitrary model-agnostic meta-learning algorithms and
arbitrary gradient-based NAS methods. %We present encouraging results for
MetaNAS with a combination of DARTS and REPTILE on few-shot classification
benchmarks. Empirical results on standard few-shot classification benchmarks
show that MetaNAS with a combination of DARTS and REPTILE yields
state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elsken_T/0/1/0/all/0/1"&gt;Thomas Elsken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Staffler_B/0/1/0/all/0/1"&gt;Benedikt Staffler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1"&gt;Jan Hendrik Metzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Randomized Smoothing with Variance Reduced Classifiers. (arXiv:2106.06946v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06946</id>
        <link href="http://arxiv.org/abs/2106.06946"/>
        <updated>2021-06-15T01:45:17.801Z</updated>
        <summary type="html"><![CDATA[Randomized Smoothing (RS) is a promising method for obtaining robustness
certificates by evaluating a base model under noise. In this work we: (i)
theoretically motivate why ensembles are a particularly suitable choice as base
models for RS, and (ii) empirically confirm this choice, obtaining state of the
art results in multiple settings. The key insight of our work is that the
reduced variance of ensembles over the perturbations introduced in RS leads to
significantly more consistent classifications for a given input, in turn
leading to substantially increased certifiable radii for difficult samples. We
also introduce key optimizations which enable an up to 50-fold decrease in
sample complexity of RS, thus drastically reducing its computational overhead.
Experimentally, we show that ensembles of only 3 to 10 classifiers consistently
improve on the strongest single model with respect to their average certified
radius (ACR) by 5% to 21% on both CIFAR-10 and ImageNet. On the latter, we
achieve a state-of-the-art ACR of 1.11. We release all code and models required
to reproduce our results upon publication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Horvath_M/0/1/0/all/0/1"&gt;Mikl&amp;#xf3;s Z. Horv&amp;#xe1;th&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1"&gt;Mark Niklas M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1"&gt;Marc Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1"&gt;Martin Vechev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online learning in MDPs with linear function approximation and bandit feedback. (arXiv:2007.01612v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.01612</id>
        <link href="http://arxiv.org/abs/2007.01612"/>
        <updated>2021-06-15T01:45:17.795Z</updated>
        <summary type="html"><![CDATA[We consider an online learning problem where the learner interacts with a
Markov decision process in a sequence of episodes, where the reward function is
allowed to change between episodes in an adversarial manner and the learner
only gets to observe the rewards associated with its actions. We allow the
state space to be arbitrarily large, but we assume that all action-value
functions can be represented as linear functions in terms of a known
low-dimensional feature map, and that the learner has access to a simulator of
the environment that allows generating trajectories from the true MDP dynamics.
Our main contribution is developing a computationally efficient algorithm that
we call MDP-LinExp3, and prove that its regret is bounded by
$\widetilde{\mathcal{O}}\big(H^2 T^{2/3} (dK)^{1/3}\big)$, where $T$ is the
number of episodes, $H$ is the number of steps in each episode, $K$ is the
number of actions, and $d$ is the dimension of the feature map. We also show
that the regret can be improved to $\widetilde{\mathcal{O}}\big(H^2
\sqrt{TdK}\big)$ under much stronger assumptions on the MDP dynamics. To our
knowledge, MDP-LinExp3 is the first provably efficient algorithm for this
problem setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neu_G/0/1/0/all/0/1"&gt;Gergely Neu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olkhovskaya_J/0/1/0/all/0/1"&gt;Julia Olkhovskaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hippocampus segmentation in magnetic resonance images of Alzheimer's patients using Deep machine learning. (arXiv:2106.06743v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.06743</id>
        <link href="http://arxiv.org/abs/2106.06743"/>
        <updated>2021-06-15T01:45:17.789Z</updated>
        <summary type="html"><![CDATA[Background: Alzheimers disease is a progressive neurodegenerative disorder
and the main cause of dementia in aging. Hippocampus is prone to changes in the
early stages of Alzheimers disease. Detection and observation of the
hippocampus changes using magnetic resonance imaging (MRI) before the onset of
Alzheimers disease leads to the faster preventive and therapeutic measures.
Objective: The aim of this study was the segmentation of the hippocampus in
magnetic resonance (MR) images of Alzheimers patients using deep machine
learning method. Methods: U-Net architecture of convolutional neural network
was proposed to segment the hippocampus in the real MRI data. The MR images of
the 100 and 35 patients available in Alzheimers disease Neuroimaging Initiative
(ADNI) dataset, was used for the train and test of the model, respectively. The
performance of the proposed method was compared with manual segmentation by
measuring the similarity metrics. Results: The desired segmentation achieved
after 10 iterations. A Dice similarity coefficient (DSC) = 92.3%, sensitivity =
96.5%, positive predicted value (PPV) = 90.4%, and Intersection over Union
(IoU) value for the train 92.94 and test 92.93 sets were obtained which are
acceptable. Conclusion: The proposed approach is promising and can be extended
in the prognosis of Alzheimers disease by the prediction of the hippocampus
volume changes in the early stage of the disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Varmazyar_H/0/1/0/all/0/1"&gt;Hadi Varmazyar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yousefi_Banaem_H/0/1/0/all/0/1"&gt;Hossein Yousefi-Banaem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Malekzadeh_S/0/1/0/all/0/1"&gt;Saber Malekzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gharehaghaji_N/0/1/0/all/0/1"&gt;Nahideh Gharehaghaji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Not All Memories are Created Equal: Learning to Forget by Expiring. (arXiv:2105.06548v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06548</id>
        <link href="http://arxiv.org/abs/2105.06548"/>
        <updated>2021-06-15T01:45:17.782Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms have shown promising results in sequence modeling tasks
that require long-term memory. Recent work investigated mechanisms to reduce
the computational cost of preserving and storing memories. However, not all
content in the past is equally important to remember. We propose Expire-Span, a
method that learns to retain the most important information and expire the
irrelevant information. This forgetting of memories enables Transformers to
scale to attend over tens of thousands of previous timesteps efficiently, as
not all states from previous timesteps are preserved. We demonstrate that
Expire-Span can help models identify and retain critical information and show
it can achieve strong performance on reinforcement learning tasks specifically
designed to challenge this functionality. Next, we show that Expire-Span can
scale to memories that are tens of thousands in size, setting a new state of
the art on incredibly long context tasks such as character-level language
modeling and a frame-by-frame moving objects task. Finally, we analyze the
efficiency of Expire-Span compared to existing approaches and demonstrate that
it trains faster and uses less memory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_D/0/1/0/all/0/1"&gt;Da Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poff_S/0/1/0/all/0/1"&gt;Spencer Poff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1"&gt;Angela Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning. (arXiv:2105.15134v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15134</id>
        <link href="http://arxiv.org/abs/2105.15134"/>
        <updated>2021-06-15T01:45:17.776Z</updated>
        <summary type="html"><![CDATA[How can neural networks trained by contrastive learning extract features from
the unlabeled data? Why does contrastive learning usually need much stronger
data augmentations than supervised learning to ensure good representations?
These questions involve both the optimization and statistical aspects of deep
learning, but can hardly be answered by analyzing supervised learning, where
the target functions are the highest pursuit. Indeed, in self-supervised
learning, it is inevitable to relate to the optimization/generalization of
neural networks to how they can encode the latent structures in the data, which
we refer to as the feature learning process.

In this work, we formally study how contrastive learning learns the feature
representations for neural networks by analyzing its feature learning process.
We consider the case where our data are comprised of two types of features: the
more semantically aligned sparse features which we want to learn from, and the
other dense features we want to avoid. Theoretically, we prove that contrastive
learning using $\mathbf{ReLU}$ networks provably learns the desired sparse
features if proper augmentations are adopted. We present an underlying
principle called $\textbf{feature decoupling}$ to explain the effects of
augmentations, where we theoretically characterize how augmentations can reduce
the correlations of dense features between positive samples while keeping the
correlations of sparse features intact, thereby forcing the neural networks to
learn from the self-supervision of sparse features. Empirically, we verified
that the feature decoupling principle matches the underlying mechanism of
contrastive learning in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1"&gt;Zixin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Sparsification-Amplified Privacy and Adaptive Optimization. (arXiv:2008.01558v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01558</id>
        <link href="http://arxiv.org/abs/2008.01558"/>
        <updated>2021-06-15T01:45:17.756Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) enables distributed agents to collaboratively learn a
centralized model without sharing their raw data with each other. However, data
locality does not provide sufficient privacy protection, and it is desirable to
facilitate FL with rigorous differential privacy (DP) guarantee. Existing DP
mechanisms would introduce random noise with magnitude proportional to the
model size, which can be quite large in deep neural networks. In this paper, we
propose a new FL framework with sparsification-amplified privacy. Our approach
integrates random sparsification with gradient perturbation on each agent to
amplify privacy guarantee. Since sparsification would increase the number of
communication rounds required to achieve a certain target accuracy, which is
unfavorable for DP guarantee, we further introduce acceleration techniques to
help reduce the privacy cost. We rigorously analyze the convergence of our
approach and utilize Renyi DP to tightly account the end-to-end DP guarantee.
Extensive experiments on benchmark datasets validate that our approach
outperforms previous differentially-private FL approaches in both privacy
guarantee and communication efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Rui Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yanmin Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuanxiong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fictitious play in zero-sum stochastic games. (arXiv:2010.04223v4 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04223</id>
        <link href="http://arxiv.org/abs/2010.04223"/>
        <updated>2021-06-15T01:45:17.749Z</updated>
        <summary type="html"><![CDATA[We present fictitious play dynamics for stochastic games and analyze its
convergence properties in zero-sum stochastic games. Our dynamics involves
players forming beliefs on opponent strategy and their own continuation payoff
(Q-function), and playing a greedy best response using estimated continuation
payoffs. Players update their beliefs from observations of opponent actions. A
key property of the learning dynamics is that update of the beliefs on
Q-functions occurs at a slower timescale than update of the beliefs on
strategies. We show both in the model-based and model-free cases (without
knowledge of player payoff functions and state transition probabilities), the
beliefs on strategies converge to a stationary mixed Nash equilibrium of the
zero-sum stochastic game.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sayin_M/0/1/0/all/0/1"&gt;Muhammed O. Sayin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parise_F/0/1/0/all/0/1"&gt;Francesca Parise&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozdaglar_A/0/1/0/all/0/1"&gt;Asuman Ozdaglar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Randomly Perturbed Structured Predictors for Direct Loss Minimization. (arXiv:2007.05724v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05724</id>
        <link href="http://arxiv.org/abs/2007.05724"/>
        <updated>2021-06-15T01:45:17.741Z</updated>
        <summary type="html"><![CDATA[Direct loss minimization is a popular approach for learning predictors over
structured label spaces. This approach is computationally appealing as it
replaces integration with optimization and allows to propagate gradients in a
deep net using loss-perturbed prediction. Recently, this technique was extended
to generative models, while introducing a randomized predictor that samples a
structure from a randomly perturbed score function. In this work, we learn the
variance of these randomized structured predictors and show that it balances
better between the learned score function and the randomized noise in
structured prediction. We demonstrate empirically the effectiveness of learning
the balance between the signal and the random noise in structured discrete
spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Indelman_H/0/1/0/all/0/1"&gt;Hedda Cohen Indelman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hazan_T/0/1/0/all/0/1"&gt;Tamir Hazan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Apprenticeship Learning via Kernel-based Inverse Reinforcement Learning. (arXiv:2002.10904v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.10904</id>
        <link href="http://arxiv.org/abs/2002.10904"/>
        <updated>2021-06-15T01:45:17.735Z</updated>
        <summary type="html"><![CDATA[It has been well demonstrated that inverse reinforcement learning (IRL) is an
effective technique for teaching machines to perform tasks at human skill
levels given human demonstrations (i.e., human to machine apprenticeship
learning). This paper seeks to show that a similar application can be
demonstrated with human learners. That is, given demonstrations from human
experts inverse reinforcement learning techniques can be used to teach other
humans to perform at higher skill levels (i.e., human to human apprenticeship
learning). To show this two experiments were conducted using a simple,
real-time web game where players were asked to touch targets in order to earn
as many points as possible. For the experiment player performance was defined
as the number of targets a player touched, irrespective of the points that a
player actually earned. This allowed for in-game points to be modified and the
effect of these alterations on performance measured. At no time were
participants told the true performance metric. To determine the point
modifications IRL was applied on demonstrations of human experts playing the
game. The results of the experiment show with significance that performance
improved over the control for select treatment groups. Finally, in addition to
the experiment, we also detail the algorithmic challenges we faced when
conducting the experiment and the techniques we used to overcome them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rucker_M/0/1/0/all/0/1"&gt;Mark A. Rucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watson_L/0/1/0/all/0/1"&gt;Layne T. Watson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_L/0/1/0/all/0/1"&gt;Laura E. Barnes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerber_M/0/1/0/all/0/1"&gt;Matthew S. Gerber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Root-finding Approaches for Computing Conformal Prediction Set. (arXiv:2104.06648v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06648</id>
        <link href="http://arxiv.org/abs/2104.06648"/>
        <updated>2021-06-15T01:45:17.727Z</updated>
        <summary type="html"><![CDATA[Conformal prediction constructs a confidence set for an unobserved response
of a feature vector based on previous identically distributed and exchangeable
observations of responses and features. It has a coverage guarantee at any
nominal level without additional assumptions on their distribution. Its
computation deplorably requires a refitting procedure for all replacement
candidates of the target response. In regression settings, this corresponds to
an infinite number of model fit. Apart from relatively simple estimators that
can be written as pieces of linear function of the response, efficiently
computing such sets is difficult and is still considered as an open problem. We
exploit the fact that, \emph{often}, conformal prediction sets are intervals
whose boundaries can be efficiently approximated by classical root-finding
algorithm. We investigate how this approach can overcome many limitations of
formerly used strategies and we discuss its complexity and drawbacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ndiaye_E/0/1/0/all/0/1"&gt;Eugene Ndiaye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1"&gt;Ichiro Takeuchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06896</id>
        <link href="http://arxiv.org/abs/2106.06896"/>
        <updated>2021-06-15T01:45:17.709Z</updated>
        <summary type="html"><![CDATA[The monitoring of coastal wetlands is of great importance to the protection
of marine and terrestrial ecosystems. However, due to the complex environment,
severe vegetation mixture, and difficulty of access, it is impossible to
accurately classify coastal wetlands and identify their species with
traditional classifiers. Despite the integration of multisource remote sensing
data for performance enhancement, there are still challenges with acquiring and
exploiting the complementary merits from multisource data. In this paper, the
Deepwise Feature Interaction Network (DFINet) is proposed for wetland
classification. A depthwise cross attention module is designed to extract
self-correlation and cross-correlation from multisource feature pairs. In this
way, meaningful complementary information is emphasized for classification.
DFINet is optimized by coordinating consistency loss, discrimination loss, and
classification loss. Accordingly, DFINet reaches the standard solution-space
under the regularity of loss functions, while the spatial consistency and
feature discrimination are preserved. Comprehensive experimental results on two
hyperspectral and multispectral wetland datasets demonstrate that the proposed
DFINet outperforms other competitive methods in terms of overall accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yunhao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengmeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianbu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Weiwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1"&gt;Ran Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1"&gt;Qian Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Markov Neural Operators for Learning Chaotic Systems. (arXiv:2106.06898v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06898</id>
        <link href="http://arxiv.org/abs/2106.06898"/>
        <updated>2021-06-15T01:45:17.702Z</updated>
        <summary type="html"><![CDATA[Chaotic systems are notoriously challenging to predict because of their
instability. Small errors accumulate in the simulation of each time step,
resulting in completely different trajectories. However, the trajectories of
many prominent chaotic systems live in a low-dimensional subspace (attractor).
If the system is Markovian, the attractor is uniquely determined by the Markov
operator that maps the evolution of infinitesimal time steps. This makes it
possible to predict the behavior of the chaotic system by learning the Markov
operator even if we cannot predict the exact trajectory. Recently, a new
framework for learning resolution-invariant solution operators for PDEs was
proposed, known as neural operators. In this work, we train a Markov neural
operator (MNO) with only the local one-step evolution information. We then
compose the learned operator to obtain the global attractor and invariant
measure. Such a Markov neural operator forms a discrete semigroup and we
empirically observe that does not collapse or blow up. Experiments show neural
operators are more accurate and stable compared to previous methods on chaotic
systems such as the Kuramoto-Sivashinsky and Navier-Stokes equations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zongyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovachki_N/0/1/0/all/0/1"&gt;Nikola Kovachki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1"&gt;Kamyar Azizzadenesheli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Burigede Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_K/0/1/0/all/0/1"&gt;Kaushik Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stuart_A/0/1/0/all/0/1"&gt;Andrew Stuart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Event Outlier Detection in Continuous Time. (arXiv:1912.09522v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.09522</id>
        <link href="http://arxiv.org/abs/1912.09522"/>
        <updated>2021-06-15T01:45:17.696Z</updated>
        <summary type="html"><![CDATA[Continuous-time event sequences represent discrete events occurring in
continuous time. Such sequences arise frequently in real-life. Usually we
expect the sequences to follow some regular pattern over time. However,
sometimes these patterns may be interrupted by unexpected absence or
occurrences of events. Identification of these unexpected cases can be very
important as they may point to abnormal situations that need human attention.
In this work, we study and develop methods for detecting outliers in
continuous-time event sequences, including unexpected absence and unexpected
occurrences of events. Since the patterns that event sequences tend to follow
may change in different contexts, we develop outlier detection methods based on
point processes that can take context information into account. Our methods are
based on Bayesian decision theory and hypothesis testing with theoretical
guarantees. To test the performance of the methods, we conduct experiments on
both synthetic data and real-world clinical data and show the effectiveness of
the proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauskrecht_M/0/1/0/all/0/1"&gt;Milos Hauskrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functorial Manifold Learning. (arXiv:2011.07435v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07435</id>
        <link href="http://arxiv.org/abs/2011.07435"/>
        <updated>2021-06-15T01:45:17.688Z</updated>
        <summary type="html"><![CDATA[We adapt previous research on category theory and topological unsupervised
learning to develop a functorial perspective on manifold learning. We first
characterize manifold learning algorithms as functors that map pseudometric
spaces to optimization objectives and factor through hierachical clustering
functors. We then use this characterization to prove refinement bounds on
manifold learning loss functions and construct a hierarchy of manifold learning
algorithms based on their invariants. We express several popular manifold
learning algorithms as functors at different levels of this hierarchy,
including Metric Multidimensional Scaling, IsoMap, and UMAP. Next, we use
interleaving distance to study the stability of a broad class of manifold
learning algorithms. We present bounds on how closely the embeddings these
algorithms produce from noisy data approximate the embeddings they would learn
from noiseless data. Finally, we use our framework to derive a set of novel
manifold learning algorithms, which we experimentally demonstrate are
competitive with the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shiebler_D/0/1/0/all/0/1"&gt;Dan Shiebler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding Iterative Magnitude Pruning: Why Lottery Tickets Win. (arXiv:2106.06955v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06955</id>
        <link href="http://arxiv.org/abs/2106.06955"/>
        <updated>2021-06-15T01:45:17.682Z</updated>
        <summary type="html"><![CDATA[The lottery ticket hypothesis states that sparse subnetworks exist in
randomly initialized dense networks that can be trained to the same accuracy as
the dense network they reside in. However, the subsequent work has failed to
replicate this on large-scale models and required rewinding to an early stable
state instead of initialization. We show that by using a training method that
is stable with respect to linear mode connectivity, large networks can also be
entirely rewound to initialization. Our subsequent experiments on common vision
tasks give strong credence to the hypothesis in Evci et al. (2020b) that
lottery tickets simply retrain to the same regions (although not necessarily to
the same basin). These results imply that existing lottery tickets could not
have been found without the preceding dense training by iterative magnitude
pruning, raising doubts about the use of the lottery ticket hypothesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maene_J/0/1/0/all/0/1"&gt;Jaron Maene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mingxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1"&gt;Marie-Francine Moens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidential Machine Learning on Untrusted Platforms: A Survey. (arXiv:2012.08156v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08156</id>
        <link href="http://arxiv.org/abs/2012.08156"/>
        <updated>2021-06-15T01:45:17.676Z</updated>
        <summary type="html"><![CDATA[With the ever-growing data and the need for developing powerful machine
learning models, data owners increasingly depend on various untrusted platforms
(e.g., public clouds, edges, and machine learning service providers) for
scalable processing or collaborative learning. Thus, sensitive data and models
are in danger of unauthorized access, misuse, and privacy compromises. A
relatively new body of research confidentially trains machine learning models
on protected data to address these concerns. In this survey, we summarize
notable studies in this emerging area of research. With a unified framework, we
highlight the critical challenges and innovations in outsourcing machine
learning confidentially. We focus on the cryptographic approaches for
confidential machine learning (CML), primarily on model training, while also
covering other directions such as perturbation-based approaches and CML in the
hardware-assisted computing environment. The discussion will take a holistic
way to consider a rich context of the related threat models, security
assumptions, design principles, and associated trade-offs amongst data utility,
cost, and confidentiality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Sagar Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Keke Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning. (arXiv:2011.00382v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00382</id>
        <link href="http://arxiv.org/abs/2011.00382"/>
        <updated>2021-06-15T01:45:17.659Z</updated>
        <summary type="html"><![CDATA[A fundamental challenge in multiagent reinforcement learning is to learn
beneficial behaviors in a shared environment with other simultaneously learning
agents. In particular, each agent perceives the environment as effectively
non-stationary due to the changing policies of other agents. Moreover, each
agent is itself constantly learning, leading to natural non-stationarity in the
distribution of experiences encountered. In this paper, we propose a novel
meta-multiagent policy gradient theorem that directly accounts for the
non-stationary policy dynamics inherent to multiagent learning settings. This
is achieved by modeling our gradient updates to consider both an agent's own
non-stationary policy dynamics and the non-stationary policy dynamics of other
agents in the environment. We show that our theoretically grounded approach
provides a general solution to the multiagent learning problem, which
inherently comprises all key aspects of previous state of the art approaches on
this topic. We test our method on a diverse suite of multiagent benchmarks and
demonstrate a more efficient ability to adapt to new agents as they learn than
baseline methods across the full spectrum of mixed incentive, competitive, and
cooperative domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dong-Ki Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Miao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1"&gt;Matthew Riemer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chuangchuang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdulhai_M/0/1/0/all/0/1"&gt;Marwa Abdulhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habibi_G/0/1/0/all/0/1"&gt;Golnaz Habibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Cot_S/0/1/0/all/0/1"&gt;Sebastian Lopez-Cot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tesauro_G/0/1/0/all/0/1"&gt;Gerald Tesauro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1"&gt;Jonathan P. How&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Disease Classification of 13,667 Body CT Scans Using Weakly Supervised Deep Learning. (arXiv:2008.01158v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01158</id>
        <link href="http://arxiv.org/abs/2008.01158"/>
        <updated>2021-06-15T01:45:17.653Z</updated>
        <summary type="html"><![CDATA[Background: Training deep learning classifiers typically requires massive
amounts of manual annotation. Weak supervision may leverage existing medical
data to classify multiple diseases and organ systems. Purpose: To design
multi-disease classifiers for body computed tomography (CT) scans using
automatically extracted labels from radiology text reports. Materials &
Methods: This retrospective study deployed rule-based algorithms to extract
19,255 disease labels from reports of 13,667 body CT scans of 12,092 subjects
for training. Using a 3D DenseVNet, three organ systems were segmented:
lungs/pleura, liver/gallbladder, and kidneys/ureters. For each organ, a 3D
convolutional neural network classified normality versus four common diseases.
Testing was performed on an additional 2,158 CT volumes relative to 2,875
manually derived reference labels. Results: Manual validation of the extracted
labels confirmed 91 to 99% accuracy. Performance using the receiver operating
characteristic area under the curve (AUC) for lungs/pleura labels were as
follows: atelectasis 0.77 (95% CI: 0.74 to 0.81), nodule 0.65 (0.61 to 0.69),
emphysema 0.89 (0.86 to 0.92), effusion 0.97 (0.96 to 0.98), and normal 0.89
(0.87 to 0.91). For liver/gallbladder: stone 0.62 (0.56 to 0.67), lesion 0.73
(0.69 to 0.77), dilation 0.87 (0.84 to 0.90), fatty 0.89 (0.86 to 0.92), and
normal 0.82 (0.78 to 0.85). For kidneys/ureters: stone 0.83 (0.79 to 0.87),
atrophy 0.92 (0.89 to 0.94), lesion 0.68 (0.64 to 0.72), cyst 0.70 (0.66 to
0.73), and normal 0.79 (0.75 to 0.83). Conclusion: Weakly supervised deep
learning classifiers leveraged massive amounts of unannotated body CT data to
classify multiple organ systems and diverse diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tushar_F/0/1/0/all/0/1"&gt;Fakrul Islam Tushar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAnniballe_V/0/1/0/all/0/1"&gt;Vincent M. D&amp;#x27;Anniballe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1"&gt;Rui Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1"&gt;Maciej A. Mazurowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1"&gt;Wanyi Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samei_E/0/1/0/all/0/1"&gt;Ehsan Samei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_G/0/1/0/all/0/1"&gt;Geoffrey D. Rubin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_J/0/1/0/all/0/1"&gt;Joseph Y. Lo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Game-Theoretic Approach to Multi-Agent Trust Region Optimization. (arXiv:2106.06828v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2106.06828</id>
        <link href="http://arxiv.org/abs/2106.06828"/>
        <updated>2021-06-15T01:45:17.645Z</updated>
        <summary type="html"><![CDATA[Trust region methods are widely applied in single-agent reinforcement
learning problems due to their monotonic performance-improvement guarantee at
every iteration. Nonetheless, when applied in multi-agent settings, the
guarantee of trust region methods no longer holds because an agent's payoff is
also affected by other agents' adaptive behaviors. To tackle this problem, we
conduct a game-theoretical analysis in the policy space, and propose a
multi-agent trust region learning method (MATRL), which enables trust region
optimization for multi-agent learning. Specifically, MATRL finds a stable
improvement direction that is guided by the solution concept of Nash
equilibrium at the meta-game level. We derive the monotonic improvement
guarantee in multi-agent settings and empirically show the local convergence of
MATRL to stable fixed points in the two-player rotational differential game. To
test our method, we evaluate MATRL in both discrete and continuous multiplayer
general-sum games including checker and switch grid worlds, multi-agent MuJoCo,
and Atari games. Results suggest that MATRL significantly outperforms strong
multi-agent reinforcement learning baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Ying Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yaodong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zheng Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Minne Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Piecewise-constant Neural ODEs. (arXiv:2106.06621v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06621</id>
        <link href="http://arxiv.org/abs/2106.06621"/>
        <updated>2021-06-15T01:45:17.639Z</updated>
        <summary type="html"><![CDATA[Neural networks are a popular tool for modeling sequential data but they
generally do not treat time as a continuous variable. Neural ODEs represent an
important exception: they parameterize the time derivative of a hidden state
with a neural network and then integrate over arbitrary amounts of time. But
these parameterizations, which have arbitrary curvature, can be hard to
integrate and thus train and evaluate. In this paper, we propose making a
piecewise-constant approximation to Neural ODEs to mitigate these issues. Our
model can be integrated exactly via Euler integration and can generate
autoregressive samples in 3-20 times fewer steps than comparable RNN and
ODE-RNN models. We evaluate our model on several synthetic physics tasks and a
planning task inspired by the game of billiards. We find that it matches the
performance of baseline approaches while requiring less time to train and
evaluate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Greydanus_S/0/1/0/all/0/1"&gt;Sam Greydanus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Stefan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1"&gt;Alan Fern&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relearning ensemble selection based on new generated features. (arXiv:2106.06761v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06761</id>
        <link href="http://arxiv.org/abs/2106.06761"/>
        <updated>2021-06-15T01:45:17.623Z</updated>
        <summary type="html"><![CDATA[The ensemble methods are meta-algorithms that combine several base machine
learning techniques to increase the effectiveness of the classification. Many
existing committees of classifiers use the classifier selection process to
determine the optimal set of base classifiers. In this article, we propose the
classifiers selection framework with relearning base classifiers. Additionally,
we use in the proposed framework the new generated feature, which can be
obtained after the relearning process. The proposed technique was compared with
state-of-the-art ensemble methods using three benchmark datasets and one
synthetic dataset. Four classification performance measures are used to
evaluate the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burduk_R/0/1/0/all/0/1"&gt;Robert Burduk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Unlearning for Random Forests. (arXiv:2009.05567v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05567</id>
        <link href="http://arxiv.org/abs/2009.05567"/>
        <updated>2021-06-15T01:45:17.617Z</updated>
        <summary type="html"><![CDATA[Responding to user data deletion requests, removing noisy examples, or
deleting corrupted training data are just a few reasons for wanting to delete
instances from a machine learning (ML) model. However, efficiently removing
this data from an ML model is generally difficult. In this paper, we introduce
data removal-enabled (DaRE) forests, a variant of random forests that enables
the removal of training data with minimal retraining. Model updates for each
DaRE tree in the forest are exact, meaning that removing instances from a DaRE
model yields exactly the same model as retraining from scratch on updated data.

DaRE trees use randomness and caching to make data deletion efficient. The
upper levels of DaRE trees use random nodes, which choose split attributes and
thresholds uniformly at random. These nodes rarely require updates because they
only minimally depend on the data. At the lower levels, splits are chosen to
greedily optimize a split criterion such as Gini index or mutual information.
DaRE trees cache statistics at each node and training data at each leaf, so
that only the necessary subtrees are updated as data is removed. For numerical
attributes, greedy nodes optimize over a random subset of thresholds, so that
they can maintain statistics while approximating the optimal threshold. By
adjusting the number of thresholds considered for greedy nodes, and the number
of random nodes, DaRE trees can trade off between more accurate predictions and
more efficient updates.

In experiments on 13 real-world datasets and one synthetic dataset, we find
DaRE forests delete data orders of magnitude faster than retraining from
scratch while sacrificing little to no predictive power.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brophy_J/0/1/0/all/0/1"&gt;Jonathan Brophy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lowd_D/0/1/0/all/0/1"&gt;Daniel Lowd&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning and Distributed Control for Residential Demand Response. (arXiv:2010.05153v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05153</id>
        <link href="http://arxiv.org/abs/2010.05153"/>
        <updated>2021-06-15T01:45:17.611Z</updated>
        <summary type="html"><![CDATA[This paper studies the automated control method for regulating air
conditioner (AC) loads in incentive-based residential demand response (DR). The
critical challenge is that the customer responses to load adjustment are
uncertain and unknown in practice. In this paper, we formulate the AC control
problem in a DR event as a multi-period stochastic optimization that integrates
the indoor thermal dynamics and customer opt-out status transition.
Specifically, machine learning techniques including Gaussian process and
logistic regression are employed to learn the unknown thermal dynamics model
and customer opt-out behavior model, respectively. We consider two typical DR
objectives for AC load control: 1) minimizing the total demand, 2) closely
tracking a regulated power trajectory. Based on the Thompson sampling
framework, we propose an online DR control algorithm to learn customer
behaviors and make real-time AC control schemes. This algorithm considers the
influence of various environmental factors on customer behaviors and is
implemented in a distributed fashion to preserve the privacy of customers.
Numerical simulations demonstrate the control optimality and learning
efficiency of the proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shimada_J/0/1/0/all/0/1"&gt;Jun Shimada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1"&gt;Na Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution-free uncertainty quantification for classification under label shift. (arXiv:2103.03323v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03323</id>
        <link href="http://arxiv.org/abs/2103.03323"/>
        <updated>2021-06-15T01:45:17.605Z</updated>
        <summary type="html"><![CDATA[Trustworthy deployment of ML models requires a proper measure of uncertainty,
especially in safety-critical applications. We focus on uncertainty
quantification (UQ) for classification problems via two avenues -- prediction
sets using conformal prediction and calibration of probabilistic predictors by
post-hoc binning -- since these possess distribution-free guarantees for i.i.d.
data. Two common ways of generalizing beyond the i.i.d. setting include
handling covariate and label shift. Within the context of distribution-free UQ,
the former has already received attention, but not the latter. It is known that
label shift hurts prediction, and we first argue that it also hurts UQ, by
showing degradation in coverage and calibration. Piggybacking on recent
progress in addressing label shift (for better prediction), we examine the
right way to achieve UQ by reweighting the aforementioned conformal and
calibration procedures whenever some unlabeled data from the target
distribution is available. We examine these techniques theoretically in a
distribution-free framework and demonstrate their excellent practical
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Podkopaev_A/0/1/0/all/0/1"&gt;Aleksandr Podkopaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Dynamic Pruning for Non-IID Federated Learning. (arXiv:2106.06921v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06921</id>
        <link href="http://arxiv.org/abs/2106.06921"/>
        <updated>2021-06-15T01:45:17.599Z</updated>
        <summary type="html"><![CDATA[Federated Learning~(FL) has emerged as a new paradigm of training machine
learning models without sacrificing data security and privacy. Learning models
at edge devices such as cell phones is one of the most common use case of FL.
However, the limited computing power and energy constraints of edge devices
hinder the adoption of FL for both model training and deployment, especially
for the resource-hungry Deep Neural Networks~(DNNs). To this end, many model
compression methods have been proposed and network pruning is among the most
well-known. However, a pruning policy for a given model is highly
dataset-dependent, which is not suitable for non-Independent and Identically
Distributed~(Non-IID) FL edge devices. In this paper, we present an adaptive
pruning scheme for edge devices in an FL system, which applies dataset-aware
dynamic pruning for inference acceleration on Non-IID datasets. Our evaluation
shows that the proposed method accelerates inference by $2\times$~($50\%$ FLOPs
reduction) while maintaining the model's quality on edge devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Sixing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1"&gt;Ali Anwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1"&gt;Ali Jannesari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Case Study on Detecting COVID-19 Health-Related Misinformation in Social Media. (arXiv:2106.06811v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.06811</id>
        <link href="http://arxiv.org/abs/2106.06811"/>
        <updated>2021-06-15T01:45:17.592Z</updated>
        <summary type="html"><![CDATA[COVID-19 pandemic has generated what public health officials called an
infodemic of misinformation. As social distancing and stay-at-home orders came
into effect, many turned to social media for socializing. This increase in
social media usage has made it a prime vehicle for the spreading of
misinformation. This paper presents a mechanism to detect COVID-19
health-related misinformation in social media following an interdisciplinary
approach. Leveraging social psychology as a foundation and existing
misinformation frameworks, we defined misinformation themes and associated
keywords incorporated into the misinformation detection mechanism using applied
machine learning techniques. Next, using the Twitter dataset, we explored the
performance of the proposed methodology using multiple state-of-the-art machine
learning classifiers. Our method shows promising results with at most 78%
accuracy in classifying health-related misinformation versus true information
using uni-gram-based NLP feature generations from tweets and the Decision Tree
classifier. We also provide suggestions on alternatives for countering
misinformation and ethical consideration for the study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pritom_M/0/1/0/all/0/1"&gt;Mir Mehedi A. Pritom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_R/0/1/0/all/0/1"&gt;Rosana Montanez Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Asad Ali Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_S/0/1/0/all/0/1"&gt;Sebastian A. Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alrashydah_E/0/1/0/all/0/1"&gt;Esra&amp;#x27;a Alrashydah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_B/0/1/0/all/0/1"&gt;Beatrice N. Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rios_A/0/1/0/all/0/1"&gt;Anthony Rios&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Auto-Regressive Gaussian Processes for Continual Learning. (arXiv:2006.05468v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05468</id>
        <link href="http://arxiv.org/abs/2006.05468"/>
        <updated>2021-06-15T01:45:17.576Z</updated>
        <summary type="html"><![CDATA[Through sequential construction of posteriors on observing data online,
Bayes' theorem provides a natural framework for continual learning. We develop
Variational Auto-Regressive Gaussian Processes (VAR-GPs), a principled
posterior updating mechanism to solve sequential tasks in continual learning.
By relying on sparse inducing point approximations for scalable posteriors, we
propose a novel auto-regressive variational distribution which reveals two
fruitful connections to existing results in Bayesian inference, expectation
propagation and orthogonal inducing points. Mean predictive entropy estimates
show VAR-GPs prevent catastrophic forgetting, which is empirically supported by
strong performance on modern continual learning benchmarks against competitive
baselines. A thorough ablation study demonstrates the efficacy of our modeling
choices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kapoor_S/0/1/0/all/0/1"&gt;Sanyam Kapoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karaletsos_T/0/1/0/all/0/1"&gt;Theofanis Karaletsos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bui_T/0/1/0/all/0/1"&gt;Thang D. Bui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COHORTNEY: Non-Parametric Clustering of Event Sequences. (arXiv:2104.01440v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01440</id>
        <link href="http://arxiv.org/abs/2104.01440"/>
        <updated>2021-06-15T01:45:17.569Z</updated>
        <summary type="html"><![CDATA[Cohort analysis is a pervasive activity in web analytics. One divides users
into groups according to specific criteria and tracks their behavior over time.
Despite its extensive use, academic circles do not discuss cohort analysis to
evaluate user behavior online. This work introduces an unsupervised
non-parametric approach to group Internet users based on their activities. In
comparison, canonical methods in marketing and engineering-based techniques
underperform. COHORTNEY is the first machine learning-based cohort analysis
algorithm with a robust theoretical explanation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhuzhel_V/0/1/0/all/0/1"&gt;Vladislav Zhuzhel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rivera_Castro_R/0/1/0/all/0/1"&gt;Rodrigo Rivera-Castro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaploukhaya_N/0/1/0/all/0/1"&gt;Nina Kaploukhaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mironova_L/0/1/0/all/0/1"&gt;Liliya Mironova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1"&gt;Alexey Zaytsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Crowds by Modeling Common Confusions. (arXiv:2012.13052v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13052</id>
        <link href="http://arxiv.org/abs/2012.13052"/>
        <updated>2021-06-15T01:45:17.563Z</updated>
        <summary type="html"><![CDATA[Crowdsourcing provides a practical way to obtain large amounts of labeled
data at a low cost. However, the annotation quality of annotators varies
considerably, which imposes new challenges in learning a high-quality model
from the crowdsourced annotations. In this work, we provide a new perspective
to decompose annotation noise into common noise and individual noise and
differentiate the source of confusion based on instance difficulty and
annotator expertise on a per-instance-annotator basis. We realize this new
crowdsourcing model by an end-to-end learning solution with two types of noise
adaptation layers: one is shared across annotators to capture their commonly
shared confusions, and the other one is pertaining to each annotator to realize
individual confusion. To recognize the source of noise in each annotation, we
use an auxiliary network to choose the two noise adaptation layers with respect
to both instances and annotators. Extensive experiments on both synthesized and
real-world benchmarks demonstrate the effectiveness of our proposed common
noise adaptation solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1"&gt;Zhendong Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongning Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Robustness via Fisher-Rao Regularization. (arXiv:2106.06685v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06685</id>
        <link href="http://arxiv.org/abs/2106.06685"/>
        <updated>2021-06-15T01:45:17.555Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness has become a topic of growing interest in machine
learning since it was observed that neural networks tend to be brittle. We
propose an information-geometric formulation of adversarial defense and
introduce FIRE, a new Fisher-Rao regularization for the categorical
cross-entropy loss, which is based on the geodesic distance between natural and
perturbed input features. Based on the information-geometric properties of the
class of softmax distributions, we derive an explicit characterization of the
Fisher-Rao Distance (FRD) for the binary and multiclass cases, and draw some
interesting properties as well as connections with standard regularization
metrics. Furthermore, for a simple linear and Gaussian model, we show that all
Pareto-optimal points in the accuracy-robustness region can be reached by FIRE
while other state-of-the-art methods fail. Empirically, we evaluate the
performance of various classifiers trained with the proposed loss on standard
datasets, showing up to 2\% of improvements in terms of robustness while
reducing the training time by 20\% over the best-performing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Picot_M/0/1/0/all/0/1"&gt;Marine Picot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Messina_F/0/1/0/all/0/1"&gt;Francisco Messina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1"&gt;Malik Boudiaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labeau_F/0/1/0/all/0/1"&gt;Fabrice Labeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ismail Ben Ayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1"&gt;Pablo Piantanida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Spiking Neural Networks. (arXiv:2106.06579v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06579</id>
        <link href="http://arxiv.org/abs/2106.06579"/>
        <updated>2021-06-15T01:45:17.549Z</updated>
        <summary type="html"><![CDATA[As neural networks get widespread adoption in resource-constrained embedded
devices, there is a growing need for low-power neural systems. Spiking Neural
Networks (SNNs)are emerging to be an energy-efficient alternative to the
traditional Artificial Neural Networks (ANNs) which are known to be
computationally intensive. From an application perspective, as federated
learning involves multiple energy-constrained devices, there is a huge scope to
leverage energy efficiency provided by SNNs. Despite its importance, there has
been little attention on training SNNs on a large-scale distributed system like
federated learning. In this paper, we bring SNNs to a more realistic federated
learning scenario. Specifically, we propose a federated learning framework for
decentralized and privacy-preserving training of SNNs. To validate the proposed
federated learning framework, we experimentally evaluate the advantages of SNNs
on various aspects of federated learning with CIFAR10 and CIFAR100 benchmarks.
We observe that SNNs outperform ANNs in terms of overall accuracy by over 15%
when the data is distributed across a large number of clients in the federation
while providing up to5.3x energy efficiency. In addition to efficiency, we also
analyze the sensitivity of the proposed federated SNN framework to data
distribution among the clients, stragglers, and gradient noise and perform a
comprehensive comparison with ANNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesha_Y/0/1/0/all/0/1"&gt;Yeshwanth Venkatesha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngeun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tassiulas_L/0/1/0/all/0/1"&gt;Leandros Tassiulas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1"&gt;Priyadarshini Panda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning disentangled representations via product manifold projection. (arXiv:2103.01638v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01638</id>
        <link href="http://arxiv.org/abs/2103.01638"/>
        <updated>2021-06-15T01:45:17.528Z</updated>
        <summary type="html"><![CDATA[We propose a novel approach to disentangle the generative factors of
variation underlying a given set of observations. Our method builds upon the
idea that the (unknown) low-dimensional manifold underlying the data space can
be explicitly modeled as a product of submanifolds. This definition of
disentanglement gives rise to a novel weakly-supervised algorithm for
recovering the unknown explanatory factors behind the data. At training time,
our algorithm only requires pairs of non i.i.d. data samples whose elements
share at least one, possibly multidimensional, generative factor of variation.
We require no knowledge on the nature of these transformations, and do not make
any limiting assumption on the properties of each subspace. Our approach is
easy to implement, and can be successfully applied to different kinds of data
(from images to 3D surfaces) undergoing arbitrary transformations. In addition
to standard synthetic benchmarks, we showcase our method in challenging
real-world applications, where we compare favorably with the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fumero_M/0/1/0/all/0/1"&gt;Marco Fumero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cosmo_L/0/1/0/all/0/1"&gt;Luca Cosmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melzi_S/0/1/0/all/0/1"&gt;Simone Melzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1"&gt;Emanuele Rodol&amp;#xe0;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A fast randomized incremental gradient method for decentralized non-convex optimization. (arXiv:2011.03853v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03853</id>
        <link href="http://arxiv.org/abs/2011.03853"/>
        <updated>2021-06-15T01:45:17.519Z</updated>
        <summary type="html"><![CDATA[We study decentralized non-convex finite-sum minimization problems described
over a network of nodes, where each node possesses a local batch of data
samples. In this context, we analyze a single-timescale randomized incremental
gradient method, called GT-SAGA. GT-SAGA is computationally efficient as it
evaluates one component gradient per node per iteration and achieves provably
fast and robust performance by leveraging node-level variance reduction and
network-level gradient tracking. For general smooth non-convex problems, we
show the almost sure and mean-squared convergence of GT-SAGA to a first-order
stationary point and further describe regimes of practical significance where
it outperforms the existing approaches and achieves a network
topology-independent iteration complexity respectively. When the global
function satisfies the Polyak-Lojaciewisz condition, we show that GT-SAGA
exhibits linear convergence to an optimal solution in expectation and describe
regimes of practical interest where the performance is network
topology-independent and improves upon the existing methods. Numerical
experiments are included to highlight the main convergence aspects of GT-SAGA
in non-convex settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Xin_R/0/1/0/all/0/1"&gt;Ran Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Khan_U/0/1/0/all/0/1"&gt;Usman A. Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kar_S/0/1/0/all/0/1"&gt;Soummya Kar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Streaming End-to-End Speech Recognition with Trie-Based Deep Biasing and Shallow Fusion. (arXiv:2104.02194v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02194</id>
        <link href="http://arxiv.org/abs/2104.02194"/>
        <updated>2021-06-15T01:45:17.513Z</updated>
        <summary type="html"><![CDATA[How to leverage dynamic contextual information in end-to-end speech
recognition has remained an active research area. Previous solutions to this
problem were either designed for specialized use cases that did not generalize
well to open-domain scenarios, did not scale to large biasing lists, or
underperformed on rare long-tail words. We address these limitations by
proposing a novel solution that combines shallow fusion, trie-based deep
biasing, and neural network language model contextualization. These techniques
result in significant 19.5% relative Word Error Rate improvement over existing
contextual biasing approaches and 5.4%-9.3% improvement compared to a strong
hybrid baseline on both open-domain and constrained contextualization tasks,
where the targets consist of mostly rare long-tail words. Our final system
remains lightweight and modular, allowing for quick modification without model
re-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1"&gt;Duc Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1"&gt;Mahaveer Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keren_G/0/1/0/all/0/1"&gt;Gil Keren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Suyoun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yangyang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahadeokar_J/0/1/0/all/0/1"&gt;Jay Mahadeokar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1"&gt;Julian Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1"&gt;Yuan Shangguan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuegen_C/0/1/0/all/0/1"&gt;Christian Fuegen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1"&gt;Ozlem Kalinli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1"&gt;Yatharth Saraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1"&gt;Michael L. Seltzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.06804</id>
        <link href="http://arxiv.org/abs/2106.06804"/>
        <updated>2021-06-15T01:45:17.495Z</updated>
        <summary type="html"><![CDATA[Explainable artificial intelligence has rapidly emerged since lawmakers have
started requiring interpretable models for safety-critical domains.
Concept-based neural networks have arisen as explainable-by-design methods as
they leverage human-understandable symbols (i.e. concepts) to predict class
memberships. However, most of these approaches focus on the identification of
the most relevant concepts but do not provide concise, formal explanations of
how such concepts are leveraged by the classifier to make predictions. In this
paper, we propose a novel end-to-end differentiable approach enabling the
extraction of logic explanations from neural networks using the formalism of
First-Order Logic. The method relies on an entropy-based criterion which
automatically identifies the most relevant concepts. We consider four different
case studies to demonstrate that: (i) this entropy-based criterion enables the
distillation of concise logic explanations in safety-critical domains from
clinical data to computer vision; (ii) the proposed approach outperforms
state-of-the-art white-box models in terms of classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1"&gt;Pietro Barbiero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1"&gt;Gabriele Ciravegna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1"&gt;Francesco Giannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf3;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1"&gt;Marco Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1"&gt;Stefano Melacci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BRAIN2DEPTH: Lightweight CNN Model for Classification of Cognitive States from EEG Recordings. (arXiv:2106.06688v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06688</id>
        <link href="http://arxiv.org/abs/2106.06688"/>
        <updated>2021-06-15T01:45:17.475Z</updated>
        <summary type="html"><![CDATA[Several Convolutional Deep Learning models have been proposed to classify the
cognitive states utilizing several neuro-imaging domains. These models have
achieved significant results, but they are heavily designed with millions of
parameters, which increases train and test time, making the model complex and
less suitable for real-time analysis. This paper proposes a simple, lightweight
CNN model to classify cognitive states from Electroencephalograph (EEG)
recordings. We develop a novel pipeline to learn distinct cognitive
representation consisting of two stages. The first stage is to generate the 2D
spectral images from neural time series signals in a particular frequency band.
Images are generated to preserve the relationship between the neighboring
electrodes and the spectral property of the cognitive events. The second is to
develop a time-efficient, computationally less loaded, and high-performing
model. We design a network containing 4 blocks and major components include
standard and depth-wise convolution for increasing the performance and followed
by separable convolution to decrease the number of parameters which maintains
the tradeoff between time and performance. We experiment on open access EEG
meditation dataset comprising expert, nonexpert meditative, and control states.
We compare performance with six commonly used machine learning classifiers and
four state of the art deep learning models. We attain comparable performance
utilizing less than 4\% of the parameters of other models. This model can be
employed in a real-time computation environment such as neurofeedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1"&gt;Pankaj Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miyapuram_K/0/1/0/all/0/1"&gt;Krishna Prasad Miyapuram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-efficient Transformers via Top-$k$ Attention. (arXiv:2106.06899v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06899</id>
        <link href="http://arxiv.org/abs/2106.06899"/>
        <updated>2021-06-15T01:45:17.458Z</updated>
        <summary type="html"><![CDATA[Following the success of dot-product attention in Transformers, numerous
approximations have been recently proposed to address its quadratic complexity
with respect to the input length. While these variants are memory and compute
efficient, it is not possible to directly use them with popular pre-trained
language models trained using vanilla attention, without an expensive
corrective pre-training stage. In this work, we propose a simple yet highly
accurate approximation for vanilla attention. We process the queries in chunks,
and for each query, compute the top-$k$ scores with respect to the keys. Our
approach offers several advantages: (a) its memory usage is linear in the input
size, similar to linear attention variants, such as Performer and RFA (b) it is
a drop-in replacement for vanilla attention that does not require any
corrective pre-training, and (c) it can also lead to significant memory savings
in the feed-forward layers after casting them into the familiar query-key-value
framework. We evaluate the quality of top-$k$ approximation for multi-head
attention layers on the Long Range Arena Benchmark, and for feed-forward layers
of T5 and UnifiedQA on multiple QA datasets. We show our approach leads to
accuracy that is nearly-identical to vanilla attention in multiple setups
including training from scratch, fine-tuning, and zero-shot inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Ankit Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dar_G/0/1/0/all/0/1"&gt;Guy Dar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_S/0/1/0/all/0/1"&gt;Shaya Goodman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciprut_D/0/1/0/all/0/1"&gt;David Ciprut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1"&gt;Jonathan Berant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Scale Hourglass Hierarchical Fusion Network for Single Image Deraining. (arXiv:2104.12100v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12100</id>
        <link href="http://arxiv.org/abs/2104.12100"/>
        <updated>2021-06-15T01:45:17.405Z</updated>
        <summary type="html"><![CDATA[Rain streaks bring serious blurring and visual quality degradation, which
often vary in size, direction and density. Current CNN-based methods achieve
encouraging performance, while are limited to depict rain characteristics and
recover image details in the poor visibility environment. To address these
issues, we present a Multi-scale Hourglass Hierarchical Fusion Network
(MH2F-Net) in end-to-end manner, to exactly captures rain streak features with
multi-scale extraction, hierarchical distillation and information aggregation.
For better extracting the features, a novel Multi-scale Hourglass Extraction
Block (MHEB) is proposed to get local and global features across different
scales through down- and up-sample process. Besides, a Hierarchical Attentive
Distillation Block (HADB) then employs the dual attention feature responses to
adaptively recalibrate the hierarchical features and eliminate the redundant
ones. Further, we introduce a Residual Projected Feature Fusion (RPFF) strategy
to progressively discriminate feature learning and aggregate different features
instead of directly concatenating or adding. Extensive experiments on both
synthetic and real rainy datasets demonstrate the effectiveness of the designed
MH2F-Net by comparing with recent state-of-the-art deraining algorithms. Our
source code will be available on the GitHub:
https://github.com/cxtalk/MH2F-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yufeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BigEarthNet Dataset with A New Class-Nomenclature for Remote Sensing Image Understanding. (arXiv:2001.06372v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.06372</id>
        <link href="http://arxiv.org/abs/2001.06372"/>
        <updated>2021-06-15T01:45:17.272Z</updated>
        <summary type="html"><![CDATA[This paper presents BigEarthNet that is a large-scale Sentinel-2
multispectral image dataset with a new class nomenclature to advance deep
learning (DL) studies in remote sensing (RS). BigEarthNet is made up of 590,326
image patches annotated with multi-labels provided by the CORINE Land Cover
(CLC) map of 2018 based on its most thematic detailed Level-3 class
nomenclature. Initial research demonstrates that some CLC classes are
challenging to be accurately described by considering only Sentinel-2 images.
To increase the effectiveness of BigEarthNet, in this paper we introduce an
alternative class-nomenclature to allow DL models for better learning and
describing the complex spatial and spectral information content of the
Sentinel-2 images. This is achieved by interpreting and arranging the CLC
Level-3 nomenclature based on the properties of Sentinel-2 images in a new
nomenclature of 19 classes. Then, the new class-nomenclature of BigEarthNet is
used within state-of-the-art DL models in the context of multi-label
classification. Results show that the models trained from scratch on
BigEarthNet outperform those pre-trained on ImageNet, especially in relation to
some complex classes including agriculture, other vegetated and natural
environments. All DL models are made publicly available at
this http URL, offering an important resource to guide future
progress on RS image analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1"&gt;Gencer Sumbul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1"&gt;Jian Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreuziger_T/0/1/0/all/0/1"&gt;Tristan Kreuziger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marcelino_F/0/1/0/all/0/1"&gt;Filipe Marcelino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costa_H/0/1/0/all/0/1"&gt;Hugo Costa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benevides_P/0/1/0/all/0/1"&gt;Pedro Benevides&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caetano_M/0/1/0/all/0/1"&gt;Mario Caetano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1"&gt;Beg&amp;#xfc;m Demir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Perfect Filtering Enough Leading to Perfect Phase Correction for dMRI data?. (arXiv:2106.06992v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06992</id>
        <link href="http://arxiv.org/abs/2106.06992"/>
        <updated>2021-06-15T01:45:17.265Z</updated>
        <summary type="html"><![CDATA[Being complex-valued and low in signal-to-noise ratios, magnitude-based
diffusion MRI is confounded by the noise-floor that falsely elevates signal
magnitude and incurs bias to the commonly used diffusion indices, such as
fractional anisotropy (FA). To avoid noise-floor, most existing phase
correction methods explore improving filters to estimate the noise-free
background phase. In this work, after diving into the phase correction
procedures, we argue that even a perfect filter is insufficient for phase
correction because the correction procedures are incapable of distinguishing
sign-symbols of noise, resulting in artifacts (\textit{i.e.}, arbitrary signal
loss). With this insight, we generalize the definition of noise-floor to a
complex polar coordinate system and propose a calibration procedure that could
conveniently distinguish noise sign symbols. The calibration procedure is
conceptually simple and easy to implement without relying on any external
technique while keeping distinctly effective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feihong_L/0/1/0/all/0/1"&gt;Liu Feihong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junwei_Y/0/1/0/all/0/1"&gt;Yang Junwei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiaowei_H/0/1/0/all/0/1"&gt;He Xiaowei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luping_Z/0/1/0/all/0/1"&gt;Zhou Luping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jun_F/0/1/0/all/0/1"&gt;Feng Jun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinggang_S/0/1/0/all/0/1"&gt;Shen Dinggang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speak or Chat with Me: End-to-End Spoken Language Understanding System with Flexible Inputs. (arXiv:2104.05752v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05752</id>
        <link href="http://arxiv.org/abs/2104.05752"/>
        <updated>2021-06-15T01:45:17.245Z</updated>
        <summary type="html"><![CDATA[A major focus of recent research in spoken language understanding (SLU) has
been on the end-to-end approach where a single model can predict intents
directly from speech inputs without intermediate transcripts. However, this
approach presents some challenges. First, since speech can be considered as
personally identifiable information, in some cases only automatic speech
recognition (ASR) transcripts are accessible. Second, intent-labeled speech
data is scarce. To address the first challenge, we propose a novel system that
can predict intents from flexible types of inputs: speech, ASR transcripts, or
both. We demonstrate strong performance for either modality separately, and
when both speech and ASR transcripts are available, through system combination,
we achieve better results than using a single input modality. To address the
second challenge, we leverage a semantically robust pre-trained BERT model and
adopt a cross-modal system that co-trains text embeddings and acoustic
embeddings in a shared latent space. We further enhance this system by
utilizing an acoustic module pre-trained on LibriSpeech and domain-adapting the
text module on our target datasets. Our experiments show significant advantages
for these pre-training and fine-tuning strategies, resulting in a system that
achieves competitive intent-classification performance on Snips SLU and Fluent
Speech Commands datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1"&gt;Sujeong Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1"&gt;Wangrui Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1"&gt;Hyun Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phung_M/0/1/0/all/0/1"&gt;My Phung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1"&gt;Michael Picheny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1"&gt;Hong-Kwang Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1"&gt;Samuel Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morais_E/0/1/0/all/0/1"&gt;Edmilson Morais&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09356</id>
        <link href="http://arxiv.org/abs/2105.09356"/>
        <updated>2021-06-15T01:45:17.236Z</updated>
        <summary type="html"><![CDATA[Despite the empirical success of neural architecture search (NAS) in deep
learning applications, the optimality, reproducibility and cost of NAS schemes
remain hard to assess. In this paper, we propose Generative Adversarial NAS
(GA-NAS) with theoretically provable convergence guarantees, promoting
stability and reproducibility in neural architecture search. Inspired by
importance sampling, GA-NAS iteratively fits a generator to previously
discovered top architectures, thus increasingly focusing on important parts of
a large search space. Furthermore, we propose an efficient adversarial learning
approach, where the generator is trained by reinforcement learning based on
rewards provided by a discriminator, thus being able to explore the search
space without evaluating a large number of architectures. Extensive experiments
show that GA-NAS beats the best published results under several cases on three
public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search
constraints and search spaces. We show that GA-NAS can be used to improve
already optimized baselines found by other NAS methods, including EfficientNet
and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in
their original search space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1"&gt;Seyed Saeed Changiz Rezaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1"&gt;Fred X. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1"&gt;Mohammad Salameh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1"&gt;Keith Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1"&gt;Shuo Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1"&gt;Shangling Jui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven Prediction of General Hamiltonian Dynamics via Learning Exactly-Symplectic Maps. (arXiv:2103.05632v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05632</id>
        <link href="http://arxiv.org/abs/2103.05632"/>
        <updated>2021-06-15T01:45:17.227Z</updated>
        <summary type="html"><![CDATA[We consider the learning and prediction of nonlinear time series generated by
a latent symplectic map. A special case is (not necessarily separable)
Hamiltonian systems, whose solution flows give such symplectic maps. For this
special case, both generic approaches based on learning the vector field of the
latent ODE and specialized approaches based on learning the Hamiltonian that
generates the vector field exist. Our method, however, is different as it does
not rely on the vector field nor assume its existence; instead, it directly
learns the symplectic evolution map in discrete time. Moreover, we do so by
representing the symplectic map via a generating function, which we approximate
by a neural network (hence the name GFNN). This way, our approximation of the
evolution map is always \emph{exactly} symplectic. This additional geometric
structure allows the local prediction error at each step to accumulate in a
controlled fashion, and we will prove, under reasonable assumptions, that the
global prediction error grows at most \emph{linearly} with long prediction
time, which significantly improves an otherwise exponential growth. In
addition, as a map-based and thus purely data-driven method, GFNN avoids two
additional sources of inaccuracies common in vector-field based approaches,
namely the error in approximating the vector field by finite difference of the
data, and the error in numerical integration of the vector field for making
predictions. Numerical experiments further demonstrate our claims.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Renyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1"&gt;Molei Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoPE: Conditional image generation using Polynomial Expansions. (arXiv:2104.05077v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05077</id>
        <link href="http://arxiv.org/abs/2104.05077"/>
        <updated>2021-06-15T01:45:17.218Z</updated>
        <summary type="html"><![CDATA[Generative modeling has evolved to a notable field of machine learning. Deep
polynomial neural networks (PNNs) have demonstrated impressive results in
unsupervised image generation, where the task is to map an input vector (i.e.,
noise) to a synthesized image. However, the success of PNNs has not been
replicated in conditional generation tasks, such as super-resolution. Existing
PNNs focus on single-variable polynomial expansions which do not fare well to
two-variable inputs, i.e., the noise variable and the conditional variable. In
this work, we introduce a general framework, called CoPE, that enables a
polynomial expansion of two input variables and captures their auto- and
cross-correlations. We exhibit how CoPE can be trivially augmented to accept an
arbitrary number of input variables. CoPE is evaluated in five tasks
(class-conditional generation, inverse problems, edges-to-image translation,
image-to-image translation, attribute-guided generation) involving eight
datasets. The thorough evaluation suggests that CoPE can be useful for tackling
diverse conditional generation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1"&gt;Grigorios G Chrysos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgopoulos_M/0/1/0/all/0/1"&gt;Markos Georgopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panagakis_Y/0/1/0/all/0/1"&gt;Yannis Panagakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based Robust Motion Planning with Guaranteed Stability: A Contraction Theory Approach. (arXiv:2102.12668v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12668</id>
        <link href="http://arxiv.org/abs/2102.12668"/>
        <updated>2021-06-15T01:45:17.212Z</updated>
        <summary type="html"><![CDATA[This paper presents Learning-based Autonomous Guidance with RObustness and
Stability guarantees (LAG-ROS), which provides machine learning-based nonlinear
motion planners with formal robustness and stability guarantees, by designing a
differential Lyapunov function using contraction theory. LAG-ROS utilizes a
neural network to model a robust tracking controller independently of a target
trajectory, for which we show that the Euclidean distance between the target
and controlled trajectories is exponentially bounded linearly in the learning
error, even under the existence of bounded external disturbances. We also
present a convex optimization approach that minimizes the steady-state bound of
the tracking error to construct the robust control law for neural network
training. In numerical simulations, it is demonstrated that the proposed method
indeed possesses superior properties of robustness and nonlinear stability
resulting from contraction theory, whilst retaining the computational
efficiency of existing learning-based motion planners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsukamoto_H/0/1/0/all/0/1"&gt;Hiroyasu Tsukamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1"&gt;Soon-Jo Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strategic Classification Made Practical. (arXiv:2103.01826v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01826</id>
        <link href="http://arxiv.org/abs/2103.01826"/>
        <updated>2021-06-15T01:45:17.204Z</updated>
        <summary type="html"><![CDATA[Strategic classification regards the problem of learning in settings where
users can strategically modify their features to improve outcomes. This setting
applies broadly and has received much recent attention. But despite its
practical significance, work in this space has so far been predominantly
theoretical. In this paper we present a learning framework for strategic
classification that is practical. Our approach directly minimizes the
"strategic" empirical risk, achieved by differentiating through the strategic
response of users. This provides flexibility that allows us to extend beyond
the original problem formulation and towards more realistic learning scenarios.
A series of experiments demonstrates the effectiveness of our approach on
various learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levanon_S/0/1/0/all/0/1"&gt;Sagi Levanon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenfeld_N/0/1/0/all/0/1"&gt;Nir Rosenfeld&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconstruction of turbulent data with deep generative models for semantic inpainting from TURB-Rot database. (arXiv:2006.09179v2 [physics.flu-dyn] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09179</id>
        <link href="http://arxiv.org/abs/2006.09179"/>
        <updated>2021-06-15T01:45:17.173Z</updated>
        <summary type="html"><![CDATA[We study the applicability of tools developed by the computer vision
community for features learning and semantic image inpainting to perform data
reconstruction of fluid turbulence configurations. The aim is twofold. First,
we explore on a quantitative basis, the capability of Convolutional Neural
Networks embedded in a Deep Generative Adversarial Model (Deep-GAN) to generate
missing data in turbulence, a paradigmatic high dimensional chaotic system. In
particular, we investigate their use in reconstructing two-dimensional damaged
snapshots extracted from a large database of numerical configurations of 3d
turbulence in the presence of rotation, a case with multi-scale random features
where both large-scale organised structures and small-scale highly intermittent
and non-Gaussian fluctuations are present. Second, following a reverse
engineering approach, we aim to rank the input flow properties (features) in
terms of their qualitative and quantitative importance to obtain a better set
of reconstructed fields. We present two approaches both based on Context
Encoders. The first one infers the missing data via a minimization of the L2
pixel-wise reconstruction loss, plus a small adversarial penalisation. The
second searches for the closest encoding of the corrupted flow configuration
from a previously trained generator. Finally, we present a comparison with a
different data assimilation tool, based on Nudging, an equation-informed
unbiased protocol, well known in the numerical weather prediction community.
The TURB-Rot database, this http URL, of roughly 300K 2d
turbulent images is released and details on how to download it are given.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Buzzicotti_M/0/1/0/all/0/1"&gt;M. Buzzicotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bonaccorso_F/0/1/0/all/0/1"&gt;F. Bonaccorso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Leoni_P/0/1/0/all/0/1"&gt;P. Clark Di Leoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Biferale_L/0/1/0/all/0/1"&gt;L. Biferale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fre-GAN: Adversarial Frequency-consistent Audio Synthesis. (arXiv:2106.02297v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02297</id>
        <link href="http://arxiv.org/abs/2106.02297"/>
        <updated>2021-06-15T01:45:17.166Z</updated>
        <summary type="html"><![CDATA[Although recent works on neural vocoder have improved the quality of
synthesized audio, there still exists a gap between generated and ground-truth
audio in frequency space. This difference leads to spectral artifacts such as
hissing noise or reverberation, and thus degrades the sample quality. In this
paper, we propose Fre-GAN which achieves frequency-consistent audio synthesis
with highly improved generation quality. Specifically, we first present
resolution-connected generator and resolution-wise discriminators, which help
learn various scales of spectral distributions over multiple frequency bands.
Additionally, to reproduce high-frequency components accurately, we leverage
discrete wavelet transform in the discriminators. From our experiments, Fre-GAN
achieves high-fidelity waveform generation with a gap of only 0.03 MOS compared
to ground-truth audio while outperforming standard models in quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1"&gt;Ji-Hoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sang-Hoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1"&gt;Ji-Hyun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Approach Towards Physics Informed Lung Ultrasound Image Scoring Neural Network for Diagnostic Assistance in COVID-19. (arXiv:2106.06980v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.06980</id>
        <link href="http://arxiv.org/abs/2106.06980"/>
        <updated>2021-06-15T01:45:17.157Z</updated>
        <summary type="html"><![CDATA[Ultrasound is fast becoming an inevitable diagnostic tool for regular and
continuous monitoring of the lung with the recent outbreak of COVID-19. In this
work, a novel approach is presented to extract acoustic propagation-based
features to automatically highlight the region below pleura, which is an
important landmark in lung ultrasound (LUS). Subsequently, a multichannel input
formed by using the acoustic physics-based feature maps is fused to train a
neural network, referred to as LUSNet, to classify the LUS images into five
classes of varying severity of lung infection to track the progression of
COVID-19. In order to ensure that the proposed approach is agnostic to the type
of acquisition, the LUSNet, which consists of a U-net architecture is trained
in an unsupervised manner with the acoustic feature maps to ensure that the
encoder-decoder architecture is learning features in the pleural region of
interest. A novel combination of the U-net output and the U-net encoder output
is employed for the classification of severity of infection in the lung. A
detailed analysis of the proposed approach on LUS images over the infection to
full recovery period of ten confirmed COVID-19 subjects shows an average
five-fold cross-validation accuracy, sensitivity, and specificity of 97%, 93%,
and 98% respectively over 5000 frames of COVID-19 videos. The analysis also
shows that, when the input dataset is limited and diverse as in the case of
COVID-19 pandemic, an aided effort of combining acoustic propagation-based
features along with the gray scale images, as proposed in this work, improves
the performance of the neural network significantly and also aids the labelling
and triaging process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Panicker_M/0/1/0/all/0/1"&gt;Mahesh Raveendranatha Panicker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yale Tung Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+M_G/0/1/0/all/0/1"&gt;Gayathri M&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+N_M/0/1/0/all/0/1"&gt;Madhavanunni A N&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Narayan_K/0/1/0/all/0/1"&gt;Kiran Vishnu Narayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kesavadas_C/0/1/0/all/0/1"&gt;C Kesavadas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vinod_A/0/1/0/all/0/1"&gt;A P Vinod&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Complexity of Linear Minimization and Projection on Some Sets. (arXiv:2101.10040v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10040</id>
        <link href="http://arxiv.org/abs/2101.10040"/>
        <updated>2021-06-15T01:45:17.151Z</updated>
        <summary type="html"><![CDATA[The Frank-Wolfe algorithm is a method for constrained optimization that
relies on linear minimizations, as opposed to projections. Therefore, a
motivation put forward in a large body of work on the Frank-Wolfe algorithm is
the computational advantage of solving linear minimizations instead of
projections. However, the discussions supporting this advantage are often too
succinct or incomplete. In this paper, we review the complexity bounds for both
tasks on several sets commonly used in optimization. Projection methods onto
the $\ell_p$-ball, $p\in\left]1,2\right[\cup\left]2,+\infty\right[$, and the
Birkhoff polytope are also proposed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Combettes_C/0/1/0/all/0/1"&gt;Cyrille W. Combettes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Pokutta_S/0/1/0/all/0/1"&gt;Sebastian Pokutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CATE: Computation-aware Neural Architecture Encoding with Transformers. (arXiv:2102.07108v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07108</id>
        <link href="http://arxiv.org/abs/2102.07108"/>
        <updated>2021-06-15T01:45:17.145Z</updated>
        <summary type="html"><![CDATA[Recent works (White et al., 2020a; Yan et al., 2020) demonstrate the
importance of architecture encodings in Neural Architecture Search (NAS). These
encodings encode either structure or computation information of the neural
architectures. Compared to structure-aware encodings, computation-aware
encodings map architectures with similar accuracies to the same region, which
improves the downstream architecture search performance (Zhang et al., 2019;
White et al., 2020a). In this work, we introduce a Computation-Aware
Transformer-based Encoding method called CATE. Different from existing
computation-aware encodings based on fixed transformation (e.g. path encoding),
CATE employs a pairwise pre-training scheme to learn computation-aware
encodings using Transformers with cross-attention. Such learned encodings
contain dense and contextualized computation information of neural
architectures. We compare CATE with eleven encodings under three major
encoding-dependent NAS subroutines in both small and large search spaces. Our
experiments show that CATE is beneficial to the downstream search, especially
in the large search space. Moreover, the outside search space experiment
demonstrates its superior generalization ability beyond the search space on
which it was trained. Our code is available at:
https://github.com/MSU-MLSys-Lab/CATE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Shen Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1"&gt;Kaiqiang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Noise Transition Matrix from Only Noisy Labels via Total Variation Regularization. (arXiv:2102.02414v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02414</id>
        <link href="http://arxiv.org/abs/2102.02414"/>
        <updated>2021-06-15T01:45:17.125Z</updated>
        <summary type="html"><![CDATA[Many weakly supervised classification methods employ a noise transition
matrix to capture the class-conditional label corruption. To estimate the
transition matrix from noisy data, existing methods often need to estimate the
noisy class-posterior, which could be unreliable due to the overconfidence of
neural networks. In this work, we propose a theoretically grounded method that
can estimate the noise transition matrix and learn a classifier simultaneously,
without relying on the error-prone noisy class-posterior estimation.
Concretely, inspired by the characteristics of the stochastic label corruption
process, we propose total variation regularization, which encourages the
predicted probabilities to be more distinguishable from each other. Under mild
assumptions, the proposed method yields a consistent estimator of the
transition matrix. We show the effectiveness of the proposed method through
experiments on benchmark and real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yivan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating BERT Inference for Sequence Labeling via Early-Exit. (arXiv:2105.13878v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13878</id>
        <link href="http://arxiv.org/abs/2105.13878"/>
        <updated>2021-06-15T01:45:17.117Z</updated>
        <summary type="html"><![CDATA[Both performance and efficiency are crucial factors for sequence labeling
tasks in many real-world scenarios. Although the pre-trained models (PTMs) have
significantly improved the performance of various sequence labeling tasks,
their computational cost is expensive. To alleviate this problem, we extend the
recent successful early-exit mechanism to accelerate the inference of PTMs for
sequence labeling tasks. However, existing early-exit mechanisms are
specifically designed for sequence-level tasks, rather than sequence labeling.
In this paper, we first propose a simple extension of sentence-level early-exit
for sequence labeling tasks. To further reduce the computational cost, we also
propose a token-level early-exit mechanism that allows partial tokens to exit
early at different layers. Considering the local dependency inherent in
sequence labeling, we employed a window-based criterion to decide for a token
whether or not to exit. The token-level early-exit brings the gap between
training and inference, so we introduce an extra self-sampling fine-tuning
stage to alleviate it. The extensive experiments on three popular sequence
labeling tasks show that our approach can save up to 66%-75% inference cost
with minimal performance degradation. Compared with competitive compressed
models such as DistilBERT, our approach can achieve better performance under
the same speed-up ratios of 2X, 3X, and 4X.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaonan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yunfan Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1"&gt;Tianxiang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Multi-Agent Reinforcement Learning with Selective Parameter Sharing. (arXiv:2102.07475v2 [cs.MA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07475</id>
        <link href="http://arxiv.org/abs/2102.07475"/>
        <updated>2021-06-15T01:45:17.007Z</updated>
        <summary type="html"><![CDATA[Sharing parameters in multi-agent deep reinforcement learning has played an
essential role in allowing algorithms to scale to a large number of agents.
Parameter sharing between agents significantly decreases the number of
trainable parameters, shortening training times to tractable levels, and has
been linked to more efficient learning. However, having all agents share the
same parameters can also have a detrimental effect on learning. We demonstrate
the impact of parameter sharing methods on training speed and converged
returns, establishing that when applied indiscriminately, their effectiveness
is highly dependent on the environment. We propose a novel method to
automatically identify agents which may benefit from sharing parameters by
partitioning them based on their abilities and goals. Our approach combines the
increased sample efficiency of parameter sharing with the representational
capacity of multiple independent networks to reduce training time and increase
final returns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1"&gt;Filippos Christianos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papoudakis_G/0/1/0/all/0/1"&gt;Georgios Papoudakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1"&gt;Arrasy Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1"&gt;Stefano V. Albrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network. (arXiv:2105.08949v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08949</id>
        <link href="http://arxiv.org/abs/2105.08949"/>
        <updated>2021-06-15T01:45:17.001Z</updated>
        <summary type="html"><![CDATA[Super-resolution (SR) plays a crucial role in improving the image quality of
magnetic resonance imaging (MRI). MRI produces multi-contrast images and can
provide a clear display of soft tissues. However, current super-resolution
methods only employ a single contrast, or use a simple multi-contrast fusion
mechanism, ignoring the rich relations among different contrasts, which are
valuable for improving SR. In this work, we propose a multi-stage integration
network (i.e., MINet) for multi-contrast MRI SR, which explicitly models the
dependencies between multi-contrast images at different stages to guide image
SR. In particular, our MINet first learns a hierarchical feature representation
from multiple convolutional stages for each of different-contrast image.
Subsequently, we introduce a multi-stage integration module to mine the
comprehensive relations between the representations of the multi-contrast
images. Specifically, the module matches each representation with all other
features, which are integrated in terms of their similarities to obtain an
enriched representation. Extensive experiments on fastMRI and real-world
clinical datasets demonstrate that 1) our MINet outperforms state-of-the-art
multi-contrast SR methods in terms of various metrics and 2) our multi-stage
integration module is able to excavate complex interactions among
multi-contrast features at different stages, leading to improved target-image
quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chun-Mei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_S/0/1/0/all/0/1"&gt;Shuhao Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09356</id>
        <link href="http://arxiv.org/abs/2105.09356"/>
        <updated>2021-06-15T01:45:16.975Z</updated>
        <summary type="html"><![CDATA[Despite the empirical success of neural architecture search (NAS) in deep
learning applications, the optimality, reproducibility and cost of NAS schemes
remain hard to assess. In this paper, we propose Generative Adversarial NAS
(GA-NAS) with theoretically provable convergence guarantees, promoting
stability and reproducibility in neural architecture search. Inspired by
importance sampling, GA-NAS iteratively fits a generator to previously
discovered top architectures, thus increasingly focusing on important parts of
a large search space. Furthermore, we propose an efficient adversarial learning
approach, where the generator is trained by reinforcement learning based on
rewards provided by a discriminator, thus being able to explore the search
space without evaluating a large number of architectures. Extensive experiments
show that GA-NAS beats the best published results under several cases on three
public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search
constraints and search spaces. We show that GA-NAS can be used to improve
already optimized baselines found by other NAS methods, including EfficientNet
and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in
their original search space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1"&gt;Seyed Saeed Changiz Rezaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1"&gt;Fred X. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1"&gt;Mohammad Salameh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1"&gt;Keith Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1"&gt;Shuo Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1"&gt;Shangling Jui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue. (arXiv:2101.00151v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00151</id>
        <link href="http://arxiv.org/abs/2101.00151"/>
        <updated>2021-06-15T01:45:16.958Z</updated>
        <summary type="html"><![CDATA[A video-grounded dialogue system is required to understand both dialogue,
which contains semantic dependencies from turn to turn, and video, which
contains visual cues of spatial and temporal scene variations. Building such
dialogue systems is a challenging problem, involving various reasoning types on
both visual and language inputs. Existing benchmarks do not have enough
annotations to thoroughly analyze dialogue systems and understand their
capabilities and limitations in isolation. These benchmarks are also not
explicitly designed to minimise biases that models can exploit without actual
reasoning. To address these limitations, in this paper, we present DVD, a
Diagnostic Dataset for Video-grounded Dialogues. The dataset is designed to
contain minimal biases and has detailed annotations for the different types of
reasoning over the spatio-temporal space of video. Dialogues are synthesized
over multiple question turns, each of which is injected with a set of
cross-turn semantic relationships. We use DVD to analyze existing approaches,
providing interesting insights into their abilities and limitations. In total,
DVD is built from $11k$ CATER synthetic videos and contains $10$ instances of
$10$-round dialogues for each video, resulting in more than $100k$ dialogues
and $1M$ question-answer pairs. Our code and dataset are publicly available at
https://github.com/facebookresearch/DVDialogues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hung Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1"&gt;Chinnadhurai Sankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1"&gt;Seungwhan Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1"&gt;Ahmad Beirami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1"&gt;Alborz Geramifard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1"&gt;Satwik Kottur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Precise characterization of the prior predictive distribution of deep ReLU networks. (arXiv:2106.06615v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06615</id>
        <link href="http://arxiv.org/abs/2106.06615"/>
        <updated>2021-06-15T01:45:16.951Z</updated>
        <summary type="html"><![CDATA[Recent works on Bayesian neural networks (BNNs) have highlighted the need to
better understand the implications of using Gaussian priors in combination with
the compositional structure of the network architecture. Similar in spirit to
the kind of analysis that has been developed to devise better initialization
schemes for neural networks (cf. He- or Xavier initialization), we derive a
precise characterization of the prior predictive distribution of finite-width
ReLU networks with Gaussian weights. While theoretical results have been
obtained for their heavy-tailedness, the full characterization of the prior
predictive distribution (i.e. its density, CDF and moments), remained unknown
prior to this work. Our analysis, based on the Meijer-G function, allows us to
quantify the influence of architectural choices such as the width or depth of
the network on the resulting shape of the prior predictive distribution. We
also formally connect our results to previous work in the infinite width
setting, demonstrating that the moments of the distribution converge to those
of a normal log-normal mixture in the infinite depth limit. Finally, our
results provide valuable guidance on prior design: for instance, controlling
the predictive variance with depth- and width-informed priors on the weights of
the network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noci_L/0/1/0/all/0/1"&gt;Lorenzo Noci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachmann_G/0/1/0/all/0/1"&gt;Gregor Bachmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1"&gt;Kevin Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowozin_S/0/1/0/all/0/1"&gt;Sebastian Nowozin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1"&gt;Thomas Hofmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Go Small and Similar: A Simple Output Decay Brings Better Performance. (arXiv:2106.06726v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06726</id>
        <link href="http://arxiv.org/abs/2106.06726"/>
        <updated>2021-06-15T01:45:16.943Z</updated>
        <summary type="html"><![CDATA[Regularization and data augmentation methods have been widely used and become
increasingly indispensable in deep learning training. Researchers who devote
themselves to this have considered various possibilities. But so far, there has
been little discussion about regularizing outputs of the model. This paper
begins with empirical observations that better performances are significantly
associated with output distributions, that have smaller average values and
variances. By audaciously assuming there is causality involved, we propose a
novel regularization term, called Output Decay, that enforces the model to
assign smaller and similar output values on each class. Though being
counter-intuitive, such a small modification result in a remarkable improvement
on performance. Extensive experiments demonstrate the wide applicability,
versatility, and compatibility of Output Decay.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tianshu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaomin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jiali Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Minghui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning of Competitive Equilibria in Exchange Economies. (arXiv:2106.06616v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06616</id>
        <link href="http://arxiv.org/abs/2106.06616"/>
        <updated>2021-06-15T01:45:16.926Z</updated>
        <summary type="html"><![CDATA[The sharing of scarce resources among multiple rational agents is one of the
classical problems in economics. In exchange economies, which are used to model
such situations, agents begin with an initial endowment of resources and
exchange them in a way that is mutually beneficial until they reach a
competitive equilibrium (CE). CE allocations are Pareto efficient and fair.
Consequently, they are used widely in designing mechanisms for fair division.
However, computing CEs requires the knowledge of agent preferences which are
unknown in several applications of interest. In this work, we explore a new
online learning mechanism, which, on each round, allocates resources to the
agents and collects stochastic feedback on their experience in using that
allocation. Its goal is to learn the agent utilities via this feedback and
imitate the allocations at a CE in the long run. We quantify CE behavior via
two losses and propose a randomized algorithm which achieves
$\bigOtilde(\sqrt{T})$ loss after $T$ rounds under both criteria. Empirically,
we demonstrate the effectiveness of this mechanism through numerical
simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wenshuo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandasamy_K/0/1/0/all/0/1"&gt;Kirthevasan Kandasamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1"&gt;Ion Stoica&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cycle4Completion: Unpaired Point Cloud Completion using Cycle Transformation with Missing Region Coding. (arXiv:2103.07838v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07838</id>
        <link href="http://arxiv.org/abs/2103.07838"/>
        <updated>2021-06-15T01:45:16.919Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel unpaired point cloud completion network,
named Cycle4Completion, to infer the complete geometries from a partial 3D
object. Previous unpaired completion methods merely focus on the learning of
geometric correspondence from incomplete shapes to complete shapes, and ignore
the learning in the reverse direction, which makes them suffer from low
completion accuracy due to the limited 3D shape understanding ability. To
address this problem, we propose two simultaneous cycle transformations between
the latent spaces of complete shapes and incomplete ones. The insight of cycle
transformation is to promote networks to understand 3D shapes by learning to
generate complete or incomplete shapes from their complementary ones.
Specifically, the first cycle transforms shapes from incomplete domain to
complete domain, and then projects them back to the incomplete domain. This
process learns the geometric characteristic of complete shapes, and maintains
the shape consistency between the complete prediction and the incomplete input.
Similarly, the inverse cycle transformation starts from complete domain to
incomplete domain, and goes back to complete domain to learn the characteristic
of incomplete shapes. We provide a comprehensive evaluation in experiments,
which shows that our model with the learned bidirectional geometry
correspondence outperforms state-of-the-art unpaired completion methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1"&gt;Xin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhizhong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yan-Pei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1"&gt;Pengfei Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wen Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu-Shen Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Game of GANs: Game Theoretical Models for Generative Adversarial Networks. (arXiv:2106.06976v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06976</id>
        <link href="http://arxiv.org/abs/2106.06976"/>
        <updated>2021-06-15T01:45:16.912Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Network, as a promising research direction in the AI
community, recently attracts considerable attention due to its ability to
generating high-quality realistic data. GANs are a competing game between two
neural networks trained in an adversarial manner to reach a Nash equilibrium.
Despite the improvement accomplished in GANs in the last years, there remain
several issues to solve. In this way, how to tackle these issues and make
advances leads to rising research interests. This paper reviews literature that
leverages the game theory in GANs and addresses how game models can relieve
specific generative models' challenges and improve the GAN's performance. In
particular, we firstly review some preliminaries, including the basic GAN model
and some game theory backgrounds. After that, we present our taxonomy to
summarize the state-of-the-art solutions into three significant categories:
modified game model, modified architecture, and modified learning method. The
classification is based on the modifications made in the basic model by the
proposed approaches from the game-theoretic perspective. We further classify
each category into several subcategories. Following the proposed taxonomy, we
explore the main objective of each class and review the recent work in each
group. Finally, we discuss the remaining challenges in this field and present
the potential future research topics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moghadam_M/0/1/0/all/0/1"&gt;Monireh Mohebbi Moghadam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boroumand_B/0/1/0/all/0/1"&gt;Bahar Boroumand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalali_M/0/1/0/all/0/1"&gt;Mohammad Jalali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zareian_A/0/1/0/all/0/1"&gt;Arman Zareian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javad_A/0/1/0/all/0/1"&gt;Alireza Daei Javad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manshaei_M/0/1/0/all/0/1"&gt;Mohammad Hossein Manshaei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Evidence Lower Bound of Variational Autoencoders Converges to a Sum of Three Entropies. (arXiv:2010.14860v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14860</id>
        <link href="http://arxiv.org/abs/2010.14860"/>
        <updated>2021-06-15T01:45:16.905Z</updated>
        <summary type="html"><![CDATA[The central objective function of a variational autoencoder (VAE) is its
variational lower bound. Here we show that for standard VAEs the variational
bound converges to a value given by the sum of three entropies: the (negative)
entropy of the latent distribution, the expected (negative) entropy of the
observable distribution, and the average entropy of the variational
distributions. Our derived analytical results are exact and apply for small as
well as complex neural networks for decoder and encoder. Furthermore, they
apply for finitely and infinitely many data points and at any stationary point
(including local and global maxima). As a consequence, we show that the
variance parameters of encoder and decoder play the key role in determining the
values of variational bounds at stationary points. Furthermore, the obtained
results can allow for closed-form analytical expressions at points of
convergence, which may be unexpected as neither variational lower bounds of
VAEs nor log-likelihoods of VAEs are closed-form during learning. As our main
contribution, we provide the proofs for convergence of standard VAEs to sums of
entropies. Furthermore, we numerically verify our analytical results and
discuss some potential applications. The obtained equality to entropy sums
provides novel information on those points in parameter space that variational
learning converges to. As such, we believe, they can contribute to our
understanding of established as well as novel VAE approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lucke_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg L&amp;#xfc;cke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Forster_D/0/1/0/all/0/1"&gt;Dennis Forster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zhenwen Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-modal Scene-compliant User Intention Estimation for Navigation. (arXiv:2106.06920v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.06920</id>
        <link href="http://arxiv.org/abs/2106.06920"/>
        <updated>2021-06-15T01:45:16.897Z</updated>
        <summary type="html"><![CDATA[A multi-modal framework to generated user intention distributions when
operating a mobile vehicle is proposed in this work. The model learns from past
observed trajectories and leverages traversability information derived from the
visual surroundings to produce a set of future trajectories, suitable to be
directly embedded into a perception-action shared control strategy on a mobile
agent, or as a safety layer to supervise the prudent operation of the vehicle.
We base our solution on a conditional Generative Adversarial Network with
Long-Short Term Memory cells to capture trajectory distributions conditioned on
past trajectories, further fused with traversability probabilities derived from
visual segmentation with a Convolutional Neural Network. The proposed
data-driven framework results in a significant reduction in error of the
predicted trajectories (versus the ground truth) from comparable strategies in
the literature (e.g. Social-GAN) that fail to account for information other
than the agent's past history. Experiments were conducted on a dataset
collected with a custom wheelchair model built onto the open-source urban
driving simulator CARLA, proving also that the proposed framework can be used
with a small, un-annotated dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Katuwandeniya_K/0/1/0/all/0/1"&gt;Kavindie Katuwandeniya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiss_S/0/1/0/all/0/1"&gt;Stefan H. Kiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1"&gt;Lei Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miro_J/0/1/0/all/0/1"&gt;Jaime Valls Miro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards annotation-efficient segmentation via image-to-image translation. (arXiv:1904.01636v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.01636</id>
        <link href="http://arxiv.org/abs/1904.01636"/>
        <updated>2021-06-15T01:45:16.892Z</updated>
        <summary type="html"><![CDATA[Often in medical imaging, it is prohibitively challenging to produce enough
boundary annotations to train deep neural networks for accurate tumor
segmentation. We propose the use of weak labels about whether an image presents
tumor or whether it is absent to extend training over images that lack these
annotations. Specifically, we propose a semi-supervised framework that employs
unpaired image-to-image translation between two domains, presence vs. absence
of cancer, as the unsupervised objective. We conjecture that translation helps
segmentation -- both require the target to be separated from the background. We
encode images into two codes: one that is common to both domains and one that
is unique to the presence domain. Decoding from the common code yields healthy
images; decoding with the addition of the unique code produces a residual
change to this image that adds cancer. Translation proceeds from presence to
absence and vice versa. In the first case, the tumor is re-added to the image
and we successfully exploit the residual decoder to also perform segmentation.
In the second case, unique codes are sampled, producing a distribution of
possible tumors. To validate the method, we created challenging synthetic tasks
and tumor segmentation datasets from public BRATS (brain, MRI) and LitS (liver,
CT) datasets. We show a clear improvement (0.83 Dice on brain, 0.74 on liver)
over baseline semi-supervised training with autoencoding (0.73, 0.66) and a
mean teacher approach (0.75, 0.69), demonstrating the ability to generalize
from smaller distributions of annotated samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vorontsov_E/0/1/0/all/0/1"&gt;Eugene Vorontsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1"&gt;Pavlo Molchanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beckham_C/0/1/0/all/0/1"&gt;Christopher Beckham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1"&gt;Jan Kautz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadoury_S/0/1/0/all/0/1"&gt;Samuel Kadoury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributionally Robust Optimization with Markovian Data. (arXiv:2106.06741v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.06741</id>
        <link href="http://arxiv.org/abs/2106.06741"/>
        <updated>2021-06-15T01:45:16.884Z</updated>
        <summary type="html"><![CDATA[We study a stochastic program where the probability distribution of the
uncertain problem parameters is unknown and only indirectly observed via
finitely many correlated samples generated by an unknown Markov chain with $d$
states. We propose a data-driven distributionally robust optimization model to
estimate the problem's objective function and optimal solution. By leveraging
results from large deviations theory, we derive statistical guarantees on the
quality of these estimators. The underlying worst-case expectation problem is
nonconvex and involves $\mathcal O(d^2)$ decision variables. Thus, it cannot be
solved efficiently for large $d$. By exploiting the structure of this problem,
we devise a customized Frank-Wolfe algorithm with convex direction-finding
subproblems of size $\mathcal O(d)$. We prove that this algorithm finds a
stationary point efficiently under mild conditions. The efficiency of the
method is predicated on a dimensionality reduction enabled by a dual
reformulation. Numerical experiments indicate that our approach has better
computational and statistical properties than the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Li_M/0/1/0/all/0/1"&gt;Mengmeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sutter_T/0/1/0/all/0/1"&gt;Tobias Sutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kuhn_D/0/1/0/all/0/1"&gt;Daniel Kuhn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding self-supervised Learning Dynamics without Contrastive Pairs. (arXiv:2102.06810v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06810</id>
        <link href="http://arxiv.org/abs/2102.06810"/>
        <updated>2021-06-15T01:45:16.878Z</updated>
        <summary type="html"><![CDATA[While contrastive approaches of self-supervised learning (SSL) learn
representations by minimizing the distance between two augmented views of the
same data point (positive pairs) and maximizing views from different data
points (negative pairs), recent \emph{non-contrastive} SSL (e.g., BYOL and
SimSiam) show remarkable performance {\it without} negative pairs, with an
extra learnable predictor and a stop-gradient operation. A fundamental question
arises: why do these methods not collapse into trivial representations? We
answer this question via a simple theoretical study and propose a novel
approach, DirectPred, that \emph{directly} sets the linear predictor based on
the statistics of its inputs, without gradient training. On ImageNet, it
performs comparably with more complex two-layer non-linear predictors that
employ BatchNorm and outperforms a linear predictor by $2.5\%$ in 300-epoch
training (and $5\%$ in 60-epoch). DirectPred is motivated by our theoretical
study of the nonlinear learning dynamics of non-contrastive SSL in simple
linear networks. Our study yields conceptual insights into how non-contrastive
SSL methods learn, how they avoid representational collapse, and how multiple
factors, like predictor networks, stop-gradients, exponential moving averages,
and weight decay all come into play. Our simple theory recapitulates the
results of real-world ablation studies in both STL-10 and ImageNet. Code is
released\footnote{\url{https://github.com/facebookresearch/luckmatters/tree/master/ssl}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuandong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinlei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1"&gt;Surya Ganguli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DS-TransUNet:Dual Swin Transformer U-Net for Medical Image Segmentation. (arXiv:2106.06716v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06716</id>
        <link href="http://arxiv.org/abs/2106.06716"/>
        <updated>2021-06-15T01:45:16.856Z</updated>
        <summary type="html"><![CDATA[Automatic medical image segmentation has made great progress benefit from the
development of deep learning. However, most existing methods are based on
convolutional neural networks (CNNs), which fail to build long-range
dependencies and global context connections due to the limitation of receptive
field in convolution operation. Inspired by the success of Transformer in
modeling the long-range contextual information, some researchers have expended
considerable efforts in designing the robust variants of Transformer-based
U-Net. Moreover, the patch division used in vision transformers usually ignores
the pixel-level intrinsic structural features inside each patch. To alleviate
these problems, we propose a novel deep medical image segmentation framework
called Dual Swin Transformer U-Net (DS-TransUNet), which might be the first
attempt to concurrently incorporate the advantages of hierarchical Swin
Transformer into both encoder and decoder of the standard U-shaped architecture
to enhance the semantic segmentation quality of varying medical images. Unlike
many prior Transformer-based solutions, the proposed DS-TransUNet first adopts
dual-scale encoder subnetworks based on Swin Transformer to extract the coarse
and fine-grained feature representations of different semantic scales. As the
core component for our DS-TransUNet, a well-designed Transformer Interactive
Fusion (TIF) module is proposed to effectively establish global dependencies
between features of different scales through the self-attention mechanism.
Furthermore, we also introduce the Swin Transformer block into decoder to
further explore the long-range contextual information during the up-sampling
process. Extensive experiments across four typical tasks for medical image
segmentation demonstrate the effectiveness of DS-TransUNet, and show that our
approach significantly outperforms the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1"&gt;Ailiang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bingzhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiayu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guangming Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Free Lunch From ANN: Towards Efficient, Accurate Spiking Neural Networks Calibration. (arXiv:2106.06984v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06984</id>
        <link href="http://arxiv.org/abs/2106.06984"/>
        <updated>2021-06-15T01:45:16.794Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Network (SNN) has been recognized as one of the next
generation of neural networks. Conventionally, SNN can be converted from a
pre-trained ANN by only replacing the ReLU activation to spike activation while
keeping the parameters intact. Perhaps surprisingly, in this work we show that
a proper way to calibrate the parameters during the conversion of ANN to SNN
can bring significant improvements. We introduce SNN Calibration, a cheap but
extraordinarily effective method by leveraging the knowledge within a
pre-trained Artificial Neural Network (ANN). Starting by analyzing the
conversion error and its propagation through layers theoretically, we propose
the calibration algorithm that can correct the error layer-by-layer. The
calibration only takes a handful number of training data and several minutes to
finish. Moreover, our calibration algorithm can produce SNN with
state-of-the-art architecture on the large-scale ImageNet dataset, including
MobileNet and RegNet. Extensive experiments demonstrate the effectiveness and
efficiency of our algorithm. For example, our advanced pipeline can increase up
to 69% top-1 accuracy when converting MobileNet on ImageNet compared to
baselines. Codes are released at https://github.com/yhhhli/SNN_Calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shikuang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Ruihao Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shi Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D3DLO: Deep 3D LiDAR Odometry. (arXiv:2101.12242v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12242</id>
        <link href="http://arxiv.org/abs/2101.12242"/>
        <updated>2021-06-15T01:45:16.786Z</updated>
        <summary type="html"><![CDATA[LiDAR odometry (LO) describes the task of finding an alignment of subsequent
LiDAR point clouds. This alignment can be used to estimate the motion of the
platform where the LiDAR sensor is mounted on. Currently, on the well-known
KITTI Vision Benchmark Suite state-of-the-art algorithms are non-learning
approaches. We propose a network architecture that learns LO by directly
processing 3D point clouds. It is trained on the KITTI dataset in an end-to-end
manner without the necessity of pre-defining corresponding pairs of points. An
evaluation on the KITTI Vision Benchmark Suite shows similar performance to a
previously published work, DeepCLR [1], even though our model uses only around
3.56% of the number of network parameters thereof. Furthermore, a plane point
extraction is applied which leads to a marginal performance decrease while
simultaneously reducing the input size by up to 50%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adis_P/0/1/0/all/0/1"&gt;Philipp Adis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horst_N/0/1/0/all/0/1"&gt;Nicolas Horst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wien_M/0/1/0/all/0/1"&gt;Mathias Wien&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CNN-based Lung CT Registration with Multiple Anatomical Constraints. (arXiv:2011.14372v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14372</id>
        <link href="http://arxiv.org/abs/2011.14372"/>
        <updated>2021-06-15T01:45:16.771Z</updated>
        <summary type="html"><![CDATA[Deep-learning-based registration methods emerged as a fast alternative to
conventional registration methods. However, these methods often still cannot
achieve the same performance as conventional registration methods because they
are either limited to small deformation or they fail to handle a superposition
of large and small deformations without producing implausible deformation
fields with foldings inside.

In this paper, we identify important strategies of conventional registration
methods for lung registration and successfully developed the deep-learning
counterpart. We employ a Gaussian-pyramid-based multilevel framework that can
solve the image registration optimization in a coarse-to-fine fashion.
Furthermore, we prevent foldings of the deformation field and restrict the
determinant of the Jacobian to physiologically meaningful values by combining a
volume change penalty with a curvature regularizer in the loss function.
Keypoint correspondences are integrated to focus on the alignment of smaller
structures.

We perform an extensive evaluation to assess the accuracy, the robustness,
the plausibility of the estimated deformation fields, and the transferability
of our registration approach. We show that it achieves state-of-the-art results
on the COPDGene dataset compared to conventional registration method with much
shorter execution time. In our experiments on the DIRLab exhale to inhale lung
registration, we demonstrate substantial improvements (TRE below $1.2$ mm) over
other deep learning methods. Our algorithm is publicly available at
https://grand-challenge.org/algorithms/deep-learning-based-ct-lung-registration/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hering_A/0/1/0/all/0/1"&gt;Alessa Hering&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hager_S/0/1/0/all/0/1"&gt;Stephanie H&amp;#xe4;ger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moltz_J/0/1/0/all/0/1"&gt;Jan Moltz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lessmann_N/0/1/0/all/0/1"&gt;Nikolas Lessmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heldmann_S/0/1/0/all/0/1"&gt;Stefan Heldmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ginneken_B/0/1/0/all/0/1"&gt;Bram van Ginneken&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multistream ValidNet: Improving 6D Object Pose Estimation by Automatic Multistream Validation. (arXiv:2106.06684v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06684</id>
        <link href="http://arxiv.org/abs/2106.06684"/>
        <updated>2021-06-15T01:45:16.750Z</updated>
        <summary type="html"><![CDATA[This work presents a novel approach to improve the results of pose estimation
by detecting and distinguishing between the occurrence of True and False
Positive results. It achieves this by training a binary classifier on the
output of an arbitrary pose estimation algorithm, and returns a binary label
indicating the validity of the result. We demonstrate that our approach
improves upon a state-of-the-art pose estimation result on the Sil\'eane
dataset, outperforming a variation of the alternative CullNet method by 4.15%
in average class accuracy and 0.73% in overall accuracy at validation. Applying
our method can also improve the pose estimation average precision results of
Op-Net by 6.06% on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazumder_J/0/1/0/all/0/1"&gt;Joy Mazumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zand_M/0/1/0/all/0/1"&gt;Mohsen Zand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1"&gt;Michael Greenspan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Reversible Steganography: Principles and Insights. (arXiv:2106.06924v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06924</id>
        <link href="http://arxiv.org/abs/2106.06924"/>
        <updated>2021-06-15T01:45:16.735Z</updated>
        <summary type="html"><![CDATA[Deep-learning\textendash{centric} reversible steganography has emerged as a
promising research paradigm. A direct way of applying deep learning to
reversible steganography is to construct a pair of encoder and decoder, whose
parameters are trained jointly, thereby learning the steganographic system as a
whole. This end-to-end framework, however, falls short of the reversibility
requirement because it is difficult for this kind of monolithic system, as a
black box, to create or duplicate intricate reversible mechanisms. In response
to this issue, a recent approach is to carve up the steganographic system and
work on modules independently. In particular, neural networks are deployed in
an analytics module to learn the data distribution, while an established
mechanism is called upon to handle the remaining tasks. In this paper, we
investigate the modular framework and deploy deep neural networks in a
reversible steganographic scheme referred to as prediction-error modulation, in
which an analytics module serves the purpose of pixel intensity prediction. The
primary focus of this study is on deep-learning\textendash{based} context-aware
pixel intensity prediction. We address the unsolved issues reported in related
literature, including the impact of pixel initialisation on prediction accuracy
and the influence of uncertainty propagation in dual-layer embedding.
Furthermore, we establish a connection between context-aware pixel intensity
prediction and low-level computer vision and analyse the performance of several
advanced neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Ching-Chun Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sisheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1"&gt;Isao Echizen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1"&gt;Victor Sanchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chang-Tsun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EL-Attention: Memory Efficient Lossless Attention for Generation. (arXiv:2105.04779v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04779</id>
        <link href="http://arxiv.org/abs/2105.04779"/>
        <updated>2021-06-15T01:45:16.717Z</updated>
        <summary type="html"><![CDATA[Transformer model with multi-head attention requires caching intermediate
results for efficient inference in generation tasks. However, cache brings new
memory-related costs and prevents leveraging larger batch size for faster
speed. We propose memory-efficient lossless attention (called EL-attention) to
address this issue. It avoids heavy operations for building multi-head keys and
values, cache for them is not needed. EL-attention constructs an ensemble of
attention results by expanding query while keeping key and value shared. It
produces the same result as multi-head attention with less GPU memory and
faster inference speed. We conduct extensive experiments on Transformer, BART,
and GPT-2 for summarization and question generation tasks. The results show
EL-attention speeds up existing models by 1.6x to 5.3x without accuracy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiusheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1"&gt;Weizhen Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1"&gt;Nikhil Bhendawade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yeyun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruofei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving PDEs on Unknown Manifolds with Machine Learning. (arXiv:2106.06682v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2106.06682</id>
        <link href="http://arxiv.org/abs/2106.06682"/>
        <updated>2021-06-15T01:45:16.711Z</updated>
        <summary type="html"><![CDATA[This paper proposes a mesh-free computational framework and machine learning
theory for solving elliptic PDEs on unknown manifolds, identified with point
clouds, based on diffusion maps (DM) and deep learning. The PDE solver is
formulated as a supervised learning task to solve a least-squares regression
problem that imposes an algebraic equation approximating a PDE (and boundary
conditions if applicable). This algebraic equation involves a graph-Laplacian
type matrix obtained via DM asymptotic expansion, which is a consistent
estimator of second-order elliptic differential operators. The resulting
numerical method is to solve a highly non-convex empirical risk minimization
problem subjected to a solution from a hypothesis space of neural-network type
functions. In a well-posed elliptic PDE setting, when the hypothesis space
consists of feedforward neural networks with either infinite width or depth, we
show that the global minimizer of the empirical loss function is a consistent
solution in the limit of large training data. When the hypothesis space is a
two-layer neural network, we show that for a sufficiently large width, the
gradient descent method can identify a global minimizer of the empirical loss
function. Supporting numerical examples demonstrate the convergence of the
solutions and the effectiveness of the proposed solver in avoiding numerical
issues that hampers the traditional approach when a large data set becomes
available, e.g., large matrix inversion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Liang_S/0/1/0/all/0/1"&gt;Senwei Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shixiao W. Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Harlim_J/0/1/0/all/0/1"&gt;John Harlim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shared Cross-Modal Trajectory Prediction for Autonomous Driving. (arXiv:2004.00202v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.00202</id>
        <link href="http://arxiv.org/abs/2004.00202"/>
        <updated>2021-06-15T01:45:16.697Z</updated>
        <summary type="html"><![CDATA[Predicting future trajectories of traffic agents in highly interactive
environments is an essential and challenging problem for the safe operation of
autonomous driving systems. On the basis of the fact that self-driving vehicles
are equipped with various types of sensors (e.g., LiDAR scanner, RGB camera,
radar, etc.), we propose a Cross-Modal Embedding framework that aims to benefit
from the use of multiple input modalities. At training time, our model learns
to embed a set of complementary features in a shared latent space by jointly
optimizing the objective functions across different types of input data. At
test time, a single input modality (e.g., LiDAR data) is required to generate
predictions from the input perspective (i.e., in the LiDAR space), while taking
advantages from the model trained with multiple sensor modalities. An extensive
evaluation is conducted to show the efficacy of the proposed framework using
two benchmark driving datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1"&gt;Chiho Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Joon Hee Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1"&gt;Srikanth Malla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Client Scheduling and Resource Allocation under Channel Uncertainty in Federated Learning. (arXiv:2106.06796v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06796</id>
        <link href="http://arxiv.org/abs/2106.06796"/>
        <updated>2021-06-15T01:45:16.689Z</updated>
        <summary type="html"><![CDATA[The performance of federated learning (FL) over wireless networks depend on
the reliability of the client-server connectivity and clients' local
computation capabilities. In this article we investigate the problem of client
scheduling and resource block (RB) allocation to enhance the performance of
model training using FL, over a pre-defined training duration under imperfect
channel state information (CSI) and limited local computing resources. First,
we analytically derive the gap between the training losses of FL with clients
scheduling and a centralized training method for a given training duration.
Then, we formulate the gap of the training loss minimization over client
scheduling and RB allocation as a stochastic optimization problem and solve it
using Lyapunov optimization. A Gaussian process regression-based channel
prediction method is leveraged to learn and track the wireless channel, in
which, the clients' CSI predictions and computing power are incorporated into
the scheduling decision. Using an extensive set of simulations, we validate the
robustness of the proposed method under both perfect and imperfect CSI over an
array of diverse data distributions. Results show that the proposed method
reduces the gap of the training accuracy loss by up to 40.7% compared to
state-of-theart client scheduling and RB allocation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wadu_M/0/1/0/all/0/1"&gt;Madhusanka Manimel Wadu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samarakoon_S/0/1/0/all/0/1"&gt;Sumudu Samarakoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1"&gt;Mehdi Bennis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anisotropic Stroke Control for Multiple Artists Style Transfer. (arXiv:2010.08175v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08175</id>
        <link href="http://arxiv.org/abs/2010.08175"/>
        <updated>2021-06-15T01:45:16.677Z</updated>
        <summary type="html"><![CDATA[Though significant progress has been made in artistic style transfer,
semantic information is usually difficult to be preserved in a fine-grained
locally consistent manner by most existing methods, especially when multiple
artists styles are required to transfer within one single model. To circumvent
this issue, we propose a Stroke Control Multi-Artist Style Transfer framework.
On the one hand, we develop a multi-condition single-generator structure which
first performs multi-artist style transfer. On the one hand, we design an
Anisotropic Stroke Module (ASM) which realizes the dynamic adjustment of
style-stroke between the non-trivial and the trivial regions. ASM endows the
network with the ability of adaptive semantic-consistency among various styles.
On the other hand, we present an novel Multi-Scale Projection Discriminator} to
realize the texture-level conditional generation. In contrast to the
single-scale conditional discriminator, our discriminator is able to capture
multi-scale texture clue to effectively distinguish a wide range of artistic
styles. Extensive experimental results well demonstrate the feasibility and
effectiveness of our approach. Our framework can transform a photograph into
different artistic style oil painting via only ONE single model. Furthermore,
the results are with distinctive artistic style and retain the anisotropic
semantic information. The code is already available on github:
https://github.com/neuralchen/ASMAGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuanhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xirui Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Naiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_T/0/1/0/all/0/1"&gt;Ting Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Failures of Deep Networks via Robust Feature Extraction. (arXiv:2012.01750v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01750</id>
        <link href="http://arxiv.org/abs/2012.01750"/>
        <updated>2021-06-15T01:45:16.660Z</updated>
        <summary type="html"><![CDATA[Traditional evaluation metrics for learned models that report aggregate
scores over a test set are insufficient for surfacing important and informative
patterns of failure over features and instances. We introduce and study a
method aimed at characterizing and explaining failures by identifying visual
attributes whose presence or absence results in poor performance. In
distinction to previous work that relies upon crowdsourced labels for visual
attributes, we leverage the representation of a separate robust model to
extract interpretable features and then harness these features to identify
failure modes. We further propose a visualization method aimed at enabling
humans to understand the meaning encoded in such features and we test the
comprehensibility of the features. An evaluation of the methods on the ImageNet
dataset demonstrates that: (i) the proposed workflow is effective for
discovering important failure modes, (ii) the visualization techniques help
humans to understand the extracted features, and (iii) the extracted insights
can assist engineers with error analysis and debugging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1"&gt;Sahil Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nushi_B/0/1/0/all/0/1"&gt;Besmira Nushi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1"&gt;Shital Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1"&gt;Ece Kamar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1"&gt;Eric Horvitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective. (arXiv:2003.00330v7 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00330</id>
        <link href="http://arxiv.org/abs/2003.00330"/>
        <updated>2021-06-15T01:45:16.653Z</updated>
        <summary type="html"><![CDATA[Neural-symbolic computing has now become the subject of interest of both
academic and industry research laboratories. Graph Neural Networks (GNN) have
been widely used in relational and symbolic domains, with widespread
application of GNNs in combinatorial optimization, constraint satisfaction,
relational reasoning and other scientific domains. The need for improved
explainability, interpretability and trust of AI systems in general demands
principled methodologies, as suggested by neural-symbolic computing. In this
paper, we review the state-of-the-art on the use of GNNs as a model of
neural-symbolic computing. This includes the application of GNNs in several
domains as well as its relationship to current developments in neural-symbolic
computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamb_L/0/1/0/all/0/1"&gt;Luis C. Lamb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcez_A/0/1/0/all/0/1"&gt;Artur Garcez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1"&gt;Marco Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prates_M/0/1/0/all/0/1"&gt;Marcelo Prates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avelar_P/0/1/0/all/0/1"&gt;Pedro Avelar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vardi_M/0/1/0/all/0/1"&gt;Moshe Vardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deception Detection and Remote Physiological Monitoring: A Dataset and Baseline Experimental Results. (arXiv:2106.06583v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06583</id>
        <link href="http://arxiv.org/abs/2106.06583"/>
        <updated>2021-06-15T01:45:16.646Z</updated>
        <summary type="html"><![CDATA[We present the Deception Detection and Physiological Monitoring (DDPM)
dataset and initial baseline results on this dataset. Our application context
is an interview scenario in which the interviewee attempts to deceive the
interviewer on selected responses. The interviewee is recorded in RGB,
near-infrared, and long-wave infrared, along with cardiac pulse, blood
oxygenation, and audio. After collection, data were annotated for
interviewer/interviewee, curated, ground-truthed, and organized into train /
test parts for a set of canonical deception detection experiments. Baseline
experiments found random accuracy for micro-expressions as an indicator of
deception, but that saccades can give a statistically significant response. We
also estimated subject heart rates from face videos (remotely) with a mean
absolute error as low as 3.16 bpm. The database contains almost 13 hours of
recordings of 70 subjects, and over 8 million visible-light, near-infrared, and
thermal video frames, along with appropriate meta, audio and pulse oximeter
data. To our knowledge, this is the only collection offering recordings of five
modalities in an interview scenario that can be used in both deception
detection and remote photoplethysmography research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Speth_J/0/1/0/all/0/1"&gt;Jeremy Speth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vance_N/0/1/0/all/0/1"&gt;Nathan Vance&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1"&gt;Adam Czajka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1"&gt;Kevin W. Bowyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wright_D/0/1/0/all/0/1"&gt;Diane Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flynn_P/0/1/0/all/0/1"&gt;Patrick Flynn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Networks based Distortion Classification and Ranking for Laparoscopic Image Quality Assessment. (arXiv:2106.06784v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.06784</id>
        <link href="http://arxiv.org/abs/2106.06784"/>
        <updated>2021-06-15T01:45:16.640Z</updated>
        <summary type="html"><![CDATA[Laparoscopic images and videos are often affected by different types of
distortion like noise, smoke, blur and nonuniform illumination. Automatic
detection of these distortions, followed generally by application of
appropriate image quality enhancement methods, is critical to avoid errors
during surgery. In this context, a crucial step involves an objective
assessment of the image quality, which is a two-fold problem requiring both the
classification of the distortion type affecting the image and the estimation of
the severity level of that distortion. Unlike existing image quality measures
which focus mainly on estimating a quality score, we propose in this paper to
formulate the image quality assessment task as a multi-label classification
problem taking into account both the type as well as the severity level (or
rank) of distortions. Here, this problem is then solved by resorting to a deep
neural networks based approach. The obtained results on a laparoscopic image
dataset show the efficiency of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khan_Z/0/1/0/all/0/1"&gt;Zohaib Amjad Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beghdadi_A/0/1/0/all/0/1"&gt;Azeddine Beghdadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kaaniche_M/0/1/0/all/0/1"&gt;Mounir Kaaniche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheikh_F/0/1/0/all/0/1"&gt;Faouzi Alaya Cheikh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPADE: A Spectral Method for Black-Box Adversarial Robustness Evaluation. (arXiv:2102.03716v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03716</id>
        <link href="http://arxiv.org/abs/2102.03716"/>
        <updated>2021-06-15T01:45:16.631Z</updated>
        <summary type="html"><![CDATA[A black-box spectral method is introduced for evaluating the adversarial
robustness of a given machine learning (ML) model. Our approach, named SPADE,
exploits bijective distance mapping between the input/output graphs constructed
for approximating the manifolds corresponding to the input/output data. By
leveraging the generalized Courant-Fischer theorem, we propose a SPADE score
for evaluating the adversarial robustness of a given model, which is proved to
be an upper bound of the best Lipschitz constant under the manifold setting. To
reveal the most non-robust data samples highly vulnerable to adversarial
attacks, we develop a spectral graph embedding procedure leveraging dominant
generalized eigenvectors. This embedding step allows assigning each data sample
a robustness score that can be further harnessed for more effective adversarial
training. Our experiments show the proposed SPADE method leads to promising
empirical results for neural network models that are adversarially trained with
the MNIST and CIFAR-10 data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1"&gt;Wuxinlin Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1"&gt;Chenhui Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhiqiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yaohui Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiru Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhuo Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Feedforward Computation via Parallel Nonlinear Equation Solving. (arXiv:2002.03629v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.03629</id>
        <link href="http://arxiv.org/abs/2002.03629"/>
        <updated>2021-06-15T01:45:16.611Z</updated>
        <summary type="html"><![CDATA[Feedforward computation, such as evaluating a neural network or sampling from
an autoregressive model, is ubiquitous in machine learning. The sequential
nature of feedforward computation, however, requires a strict order of
execution and cannot be easily accelerated with parallel computing. To enable
parallelization, we frame the task of feedforward computation as solving a
system of nonlinear equations. We then propose to find the solution using a
Jacobi or Gauss-Seidel fixed-point iteration method, as well as hybrid methods
of both. Crucially, Jacobi updates operate independently on each equation and
can be executed in parallel. Our method is guaranteed to give exactly the same
values as the original feedforward computation with a reduced (or equal) number
of parallelizable iterations, and hence reduced time given sufficient parallel
computing power. Experimentally, we demonstrate the effectiveness of our
approach in accelerating (i) backpropagation of RNNs, (ii) evaluation of
DenseNets, and (iii) autoregressive sampling of MADE and PixelCNN++, with
speedup factors between 2.1 and 26 under various settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1"&gt;Chenlin Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1"&gt;Renjie Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Clone Transformer for Efficient Convolutional Neural Netwoks. (arXiv:2106.06778v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06778</id>
        <link href="http://arxiv.org/abs/2106.06778"/>
        <updated>2021-06-15T01:45:16.602Z</updated>
        <summary type="html"><![CDATA[Convolutional networks (ConvNets) have shown impressive capability to solve
various vision tasks. Nevertheless, the trade-off between performance and
efficiency is still a challenge for a feasible model deployment on
resource-constrained platforms. In this paper, we introduce a novel concept
termed multi-path fully connected pattern (MPFC) to rethink the
interdependencies of topology pattern, accuracy and efficiency for ConvNets.
Inspired by MPFC, we further propose a dual-branch module named dynamic clone
transformer (DCT) where one branch generates multiple replicas from inputs and
another branch reforms those clones through a series of difference vectors
conditional on inputs itself to produce more variants. This operation allows
the self-expansion of channel-wise information in a data-driven way with little
computational cost while providing sufficient learning capacity, which is a
potential unit to replace computationally expensive pointwise convolution as an
expansion layer in the bottleneck structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1"&gt;Longqing Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Place Recognition with Deep Embedding Learning over Radar Videos. (arXiv:2106.06703v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06703</id>
        <link href="http://arxiv.org/abs/2106.06703"/>
        <updated>2021-06-15T01:45:16.596Z</updated>
        <summary type="html"><![CDATA[We learn, in an unsupervised way, an embedding from sequences of radar images
that is suitable for solving place recognition problem using complex radar
data. We experiment on 280 km of data and show performance exceeding
state-of-the-art supervised approaches, localising correctly 98.38% of the time
when using just the nearest database candidate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gadd_M/0/1/0/all/0/1"&gt;Matthew Gadd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martini_D/0/1/0/all/0/1"&gt;Daniele De Martini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newman_P/0/1/0/all/0/1"&gt;Paul Newman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding self-supervised Learning Dynamics without Contrastive Pairs. (arXiv:2102.06810v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06810</id>
        <link href="http://arxiv.org/abs/2102.06810"/>
        <updated>2021-06-15T01:45:16.589Z</updated>
        <summary type="html"><![CDATA[While contrastive approaches of self-supervised learning (SSL) learn
representations by minimizing the distance between two augmented views of the
same data point (positive pairs) and maximizing views from different data
points (negative pairs), recent \emph{non-contrastive} SSL (e.g., BYOL and
SimSiam) show remarkable performance {\it without} negative pairs, with an
extra learnable predictor and a stop-gradient operation. A fundamental question
arises: why do these methods not collapse into trivial representations? We
answer this question via a simple theoretical study and propose a novel
approach, DirectPred, that \emph{directly} sets the linear predictor based on
the statistics of its inputs, without gradient training. On ImageNet, it
performs comparably with more complex two-layer non-linear predictors that
employ BatchNorm and outperforms a linear predictor by $2.5\%$ in 300-epoch
training (and $5\%$ in 60-epoch). DirectPred is motivated by our theoretical
study of the nonlinear learning dynamics of non-contrastive SSL in simple
linear networks. Our study yields conceptual insights into how non-contrastive
SSL methods learn, how they avoid representational collapse, and how multiple
factors, like predictor networks, stop-gradients, exponential moving averages,
and weight decay all come into play. Our simple theory recapitulates the
results of real-world ablation studies in both STL-10 and ImageNet. Code is
released\footnote{\url{https://github.com/facebookresearch/luckmatters/tree/master/ssl}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuandong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinlei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1"&gt;Surya Ganguli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[About exchanging expectation and supremum for conditional Wasserstein GANs. (arXiv:2103.13906v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13906</id>
        <link href="http://arxiv.org/abs/2103.13906"/>
        <updated>2021-06-15T01:45:16.578Z</updated>
        <summary type="html"><![CDATA[In cases where a Wasserstein GAN depends on a condition the latter is usually
handled via an expectation within the loss function. Depending on the way this
is motivated, the discriminator is either required to be Lipschitz-1 in both or
in only one of its arguments. For the weaker requirement to become usable one
needs to exchange a supremum and an expectation. This is a mathematically
perilous operation, which is, so far, only partially justified in the
literature. This short mathematical note intends to fill this gap and provides
the mathematical rationale for discriminators that are only partially
Lipschitz-1 for cases where this approach is more appropriate or successful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg Martin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Lingual Abstractive Summarization with Limited Parallel Resources. (arXiv:2105.13648v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13648</id>
        <link href="http://arxiv.org/abs/2105.13648"/>
        <updated>2021-06-15T01:45:16.554Z</updated>
        <summary type="html"><![CDATA[Parallel cross-lingual summarization data is scarce, requiring models to
better use the limited available cross-lingual resources. Existing methods to
do so often adopt sequence-to-sequence networks with multi-task frameworks.
Such approaches apply multiple decoders, each of which is utilized for a
specific task. However, these independent decoders share no parameters, hence
fail to capture the relationships between the discrete phrases of summaries in
different languages, breaking the connections in order to transfer the
knowledge of the high-resource languages to low-resource languages. To bridge
these connections, we propose a novel Multi-Task framework for Cross-Lingual
Abstractive Summarization (MCLAS) in a low-resource setting. Employing one
unified decoder to generate the sequential concatenation of monolingual and
cross-lingual summaries, MCLAS makes the monolingual summarization task a
prerequisite of the cross-lingual summarization (CLS) task. In this way, the
shared decoder learns interactions involving alignments and summary patterns
across languages, which encourages attaining knowledge transfer. Experiments on
two CLS datasets demonstrate that our model significantly outperforms three
baseline models in both low-resource and full-dataset scenarios. Moreover,
in-depth analysis on the generated summaries and attention heads verifies that
interactions are learned well using MCLAS, which benefits the CLS task under
limited parallel resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yu Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heyan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning. (arXiv:2012.10630v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10630</id>
        <link href="http://arxiv.org/abs/2012.10630"/>
        <updated>2021-06-15T01:45:16.546Z</updated>
        <summary type="html"><![CDATA[Large scale machine learning and deep models are extremely data-hungry.
Unfortunately, obtaining large amounts of labeled data is expensive, and
training state-of-the-art models (with hyperparameter tuning) requires
significant computing resources and time. Secondly, real-world data is noisy
and imbalanced. As a result, several recent papers try to make the training
process more efficient and robust. However, most existing work either focuses
on robustness or efficiency, but not both. In this work, we introduce Glister,
a GeneraLIzation based data Subset selecTion for Efficient and Robust learning
framework. We formulate Glister as a mixed discrete-continuous bi-level
optimization problem to select a subset of the training data, which maximizes
the log-likelihood on a held-out validation set. Next, we propose an iterative
online algorithm Glister-Online, which performs data selection iteratively
along with the parameter updates and can be applied to any loss-based learning
algorithm. We then show that for a rich class of loss functions including
cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete
data selection is an instance of (weakly) submodular optimization, and we
analyze conditions for which Glister-Online reduces the validation loss and
converges. Finally, we propose Glister-Active, an extension to batch active
learning, and we empirically demonstrate the performance of Glister on a wide
range of tasks including, (a) data selection to reduce training time, (b)
robust learning under label noise and imbalance settings, and (c) batch-active
learning with several deep and shallow models. We show that our framework
improves upon state of the art both in efficiency and accuracy (in cases (a)
and (c)) and is more efficient compared to other state-of-the-art robust
learning algorithms in case (b).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1"&gt;Krishnateja Killamsetty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1"&gt;Durga Sivasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up. (arXiv:2102.07074v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07074</id>
        <link href="http://arxiv.org/abs/2102.07074"/>
        <updated>2021-06-15T01:45:16.538Z</updated>
        <summary type="html"><![CDATA[The recent explosive interest on transformers has suggested their potential
to become powerful ``universal" models for computer vision tasks, such as
classification, detection, and segmentation. While those attempts mainly study
the discriminative models, we explore transformers on some more notoriously
difficult vision tasks, e.g., generative adversarial networks (GANs). Our goal
is to conduct the first pilot study in building a GAN completely free of
convolutions, using only pure transformer-based architectures. Our vanilla GAN
architecture, dubbed TransGAN, consists of a memory-friendly transformer-based
generator that progressively increases feature resolution, and correspondingly
a multi-scale discriminator to capture simultaneously semantic contexts and
low-level textures. On top of them, we introduce the new module of grid
self-attention for alleviating the memory bottleneck further, in order to scale
up TransGAN to high-resolution generation. We also develop a unique training
recipe including a series of techniques that can mitigate the training
instability issues of TransGAN, such as data augmentation, modified
normalization, and relative position encoding. Our best architecture achieves
highly competitive performance compared to current state-of-the-art GANs using
convolutional backbones. Specifically, TransGAN sets new state-of-the-art
inception score of 10.43 and FID of 18.28 on STL-10, outperforming StyleGAN-V2.
When it comes to higher-resolution (e.g. 256 x 256) generation tasks, such as
on CelebA-HQ and LSUN-Church, TransGAN continues to produce diverse visual
examples with high fidelity and impressive texture details. In addition, we
dive deep into the transformer-based generation models to understand how their
behaviors differ from convolutional ones, by visualizing training dynamics. The
code is available at https://github.com/VITA-Group/TransGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yifan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-facet Contextual Bandits: A Neural Network Perspective. (arXiv:2106.03039v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03039</id>
        <link href="http://arxiv.org/abs/2106.03039"/>
        <updated>2021-06-15T01:45:16.532Z</updated>
        <summary type="html"><![CDATA[Contextual multi-armed bandit has shown to be an effective tool in
recommender systems. In this paper, we study a novel problem of multi-facet
bandits involving a group of bandits, each characterizing the users' needs from
one unique aspect. In each round, for the given user, we need to select one arm
from each bandit, such that the combination of all arms maximizes the final
reward. This problem can find immediate applications in E-commerce, healthcare,
etc. To address this problem, we propose a novel algorithm, named MuFasa, which
utilizes an assembled neural network to jointly learn the underlying reward
functions of multiple bandits. It estimates an Upper Confidence Bound (UCB)
linked with the expected reward to balance between exploitation and
exploration. Under mild assumptions, we provide the regret analysis of MuFasa.
It can achieve the near-optimal $\widetilde{ \mathcal{O}}((K+1)\sqrt{T})$
regret bound where $K$ is the number of bandits and $T$ is the number of played
rounds. Furthermore, we conduct extensive experiments to show that MuFasa
outperforms strong baselines on real-world data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1"&gt;Yikun Ban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jingrui He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cook_C/0/1/0/all/0/1"&gt;Curtiss B. Cook&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability. (arXiv:2006.14512v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14512</id>
        <link href="http://arxiv.org/abs/2006.14512"/>
        <updated>2021-06-15T01:45:16.524Z</updated>
        <summary type="html"><![CDATA[Knowledge transferability, or transfer learning, has been widely adopted to
allow a pre-trained model in the source domain to be effectively adapted to
downstream tasks in the target domain. It is thus important to explore and
understand the factors affecting knowledge transferability. In this paper, as
the first work, we analyze and demonstrate the connections between knowledge
transferability and another important phenomenon--adversarial transferability,
\emph{i.e.}, adversarial examples generated against one model can be
transferred to attack other models. Our theoretical studies show that
adversarial transferability indicates knowledge transferability and vice versa.
Moreover, based on the theoretical insights, we propose two practical
adversarial transferability metrics to characterize this process, serving as
bidirectional indicators between adversarial and knowledge transferability. We
conduct extensive experiments for different scenarios on diverse datasets,
showing a positive correlation between adversarial transferability and
knowledge transferability. Our findings will shed light on future research
about effective knowledge transfer learning and adversarial transferability
analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1"&gt;Kaizhao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jacky Y. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1"&gt;Oluwasanmi Koyejo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Risk Minimization. (arXiv:2105.03818v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03818</id>
        <link href="http://arxiv.org/abs/2105.03818"/>
        <updated>2021-06-15T01:45:16.505Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms with empirical risk minimization usually suffer
from poor generalization performance due to the greedy exploitation of
correlations among the training data, which are not stable under distributional
shifts. Recently, some invariant learning methods for out-of-distribution (OOD)
generalization have been proposed by leveraging multiple training environments
to find invariant relationships. However, modern datasets are frequently
assembled by merging data from multiple sources without explicit source labels.
The resultant unobserved heterogeneity renders many invariant learning methods
inapplicable. In this paper, we propose Heterogeneous Risk Minimization (HRM)
framework to achieve joint learning of latent heterogeneity among the data and
invariant relationship, which leads to stable prediction despite distributional
shifts. We theoretically characterize the roles of the environment labels in
invariant learning and justify our newly proposed HRM framework. Extensive
experimental results validate the effectiveness of our HRM framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiashuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zheyuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1"&gt;Peng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zheyan Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feasible Actor-Critic: Constrained Reinforcement Learning for Ensuring Statewise Safety. (arXiv:2105.10682v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10682</id>
        <link href="http://arxiv.org/abs/2105.10682"/>
        <updated>2021-06-15T01:45:16.498Z</updated>
        <summary type="html"><![CDATA[The safety constraints commonly used by existing safe reinforcement learning
(RL) methods are defined only on expectation of initial states, but allow each
certain state to be unsafe, which is unsatisfying for real-world
safety-critical tasks. In this paper, we introduce the feasible actor-critic
(FAC) algorithm, which is the first model-free constrained RL method that
considers statewise safety, e.g, safety for each initial state. We claim that
some states are inherently unsafe no matter what policy we choose, while for
other states there exist policies ensuring safety, where we say such states and
policies are feasible. By constructing a statewise Lagrange function available
on RL sampling and adopting an additional neural network to approximate the
statewise Lagrange multiplier, we manage to obtain the optimal feasible policy
which ensures safety for each feasible state and the safest possible policy for
infeasible states. Furthermore, the trained multiplier net can indicate whether
a given state is feasible or not through the statewise complementary slackness
condition. We provide theoretical guarantees that FAC outperforms previous
expectation-based constrained RL methods in terms of both constraint
satisfaction and reward optimization. Experimental results on both robot
locomotive tasks and safe exploration tasks verify the safety enhancement and
feasibility interpretation of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Haitong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yang Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shegnbo Eben Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangteng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Sifa Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jianyu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cluster-to-Conquer: A Framework for End-to-End Multi-Instance Learning for Whole Slide Image Classification. (arXiv:2103.10626v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10626</id>
        <link href="http://arxiv.org/abs/2103.10626"/>
        <updated>2021-06-15T01:45:16.492Z</updated>
        <summary type="html"><![CDATA[In recent years, the availability of digitized Whole Slide Images (WSIs) has
enabled the use of deep learning-based computer vision techniques for automated
disease diagnosis. However, WSIs present unique computational and algorithmic
challenges. WSIs are gigapixel-sized ($\sim$100K pixels), making them
infeasible to be used directly for training deep neural networks. Also, often
only slide-level labels are available for training as detailed annotations are
tedious and can be time-consuming for experts. Approaches using
multiple-instance learning (MIL) frameworks have been shown to overcome these
challenges. Current state-of-the-art approaches divide the learning framework
into two decoupled parts: a convolutional neural network (CNN) for encoding the
patches followed by an independent aggregation approach for slide-level
prediction. In this approach, the aggregation step has no bearing on the
representations learned by the CNN encoder. We have proposed an end-to-end
framework that clusters the patches from a WSI into ${k}$-groups, samples
${k}'$ patches from each group for training, and uses an adaptive attention
mechanism for slide level prediction; Cluster-to-Conquer (C2C). We have
demonstrated that dividing a WSI into clusters can improve the model training
by exposing it to diverse discriminative features extracted from the patches.
We regularized the clustering mechanism by introducing a KL-divergence loss
between the attention weights of patches in a cluster and the uniform
distribution. The framework is optimized end-to-end on slide-level
cross-entropy, patch-level cross-entropy, and KL-divergence loss
(Implementation: https://github.com/YashSharma/C2C).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sharma_Y/0/1/0/all/0/1"&gt;Yash Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Aman Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ehsan_L/0/1/0/all/0/1"&gt;Lubaina Ehsan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moskaluk_C/0/1/0/all/0/1"&gt;Christopher A. Moskaluk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Syed_S/0/1/0/all/0/1"&gt;Sana Syed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brown_D/0/1/0/all/0/1"&gt;Donald E. Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Buffered Asynchronous Aggregation. (arXiv:2106.06639v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06639</id>
        <link href="http://arxiv.org/abs/2106.06639"/>
        <updated>2021-06-15T01:45:16.485Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) trains a shared model across distributed devices
while keeping the training data on the devices. Most FL schemes are
synchronous: they perform a synchronized aggregation of model updates from
individual devices. Synchronous training can be slow because of late-arriving
devices (stragglers). On the other hand, completely asynchronous training makes
FL less private because of incompatibility with secure aggregation. In this
work, we propose a model aggregation scheme, FedBuff, that combines the best
properties of synchronous and asynchronous FL. Similar to synchronous FL,
FedBuff is compatible with secure aggregation. Similar to asynchronous FL,
FedBuff is robust to stragglers. In FedBuff, clients trains asynchronously and
send updates to the server. The server aggregates client updates in a private
buffer until updates have been received, at which point a server model update
is immediately performed. We provide theoretical convergence guarantees for
FedBuff in a non-convex setting. Empirically, FedBuff converges up to 3.8x
faster than previous proposals for synchronous FL (e.g., FedAvgM), and up to
2.5x faster than previous proposals for asynchronous FL (e.g., FedAsync). We
show that FedBuff is robust to different staleness distributions and is more
scalable than synchronous FL techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_J/0/1/0/all/0/1"&gt;John Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1"&gt;Kshitiz Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1"&gt;Hongyuan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yousefpour_A/0/1/0/all/0/1"&gt;Ashkan Yousefpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1"&gt;Michael Rabbat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esmaeili_M/0/1/0/all/0/1"&gt;Mani Malek Esmaeili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huba_D/0/1/0/all/0/1"&gt;Dzmitry Huba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Power Modeling for Effective Datacenter Planning and Compute Management. (arXiv:2103.13308v2 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13308</id>
        <link href="http://arxiv.org/abs/2103.13308"/>
        <updated>2021-06-15T01:45:16.479Z</updated>
        <summary type="html"><![CDATA[Datacenter power demand has been continuously growing and is the key driver
of its cost. An accurate mapping of compute resources (CPU, RAM, etc.) and
hardware types (servers, accelerators, etc.) to power consumption has emerged
as a critical requirement for major Web and cloud service providers. With the
global growth in datacenter capacity and associated power consumption, such
models are essential for important decisions around datacenter design and
operation. In this paper, we discuss two classes of statistical power models
designed and validated to be accurate, simple, interpretable and applicable to
all hardware configurations and workloads across hyperscale datacenters of
Google fleet. To the best of our knowledge, this is the largest scale power
modeling study of this kind, in both the scope of diverse datacenter planning
and real-time management use cases, as well as the variety of hardware
configurations and workload types used for modeling and validation. We
demonstrate that the proposed statistical modeling techniques, while simple and
scalable, predict power with less than 5% Mean Absolute Percent Error (MAPE)
for more than 95% diverse Power Distribution Units (more than 2000) using only
4 features. This performance matches the reported accuracy of the previous
started-of-the-art methods, while using significantly less features and
covering a wider range of use cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Radovanovic_A/0/1/0/all/0/1"&gt;Ana Radovanovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bokan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talukdar_S/0/1/0/all/0/1"&gt;Saurav Talukdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Binz Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duarte_A/0/1/0/all/0/1"&gt;Alexandre Duarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahbazi_M/0/1/0/all/0/1"&gt;Mahya Shahbazi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InFillmore: Frame-Guided Language Generation with Bidirectional Context. (arXiv:2103.04941v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04941</id>
        <link href="http://arxiv.org/abs/2103.04941"/>
        <updated>2021-06-15T01:45:16.461Z</updated>
        <summary type="html"><![CDATA[We propose a structured extension to bidirectional-context conditional
language generation, or "infilling," inspired by Frame Semantic theory
(Fillmore, 1976). Guidance is provided through two approaches: (1) model
fine-tuning, conditioning directly on observed symbolic frames, and (2) a novel
extension to disjunctive lexically constrained decoding that leverages frame
semantic lexical units. Automatic and human evaluations confirm that
frame-guided generation allows for explicit manipulation of intended infill
semantics, with minimal loss in distinguishability from human-generated text.
Our methods flexibly apply to a variety of use scenarios, and we provide a
codebase and interactive demo available from
https://nlp.jhu.edu/demos/infillmore.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1"&gt;Jiefu Ou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weir_N/0/1/0/all/0/1"&gt;Nathaniel Weir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belyy_A/0/1/0/all/0/1"&gt;Anton Belyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Felix Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1"&gt;Benjamin Van Durme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias-Variance Reduced Local SGD for Less Heterogeneous Federated Learning. (arXiv:2102.03198v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03198</id>
        <link href="http://arxiv.org/abs/2102.03198"/>
        <updated>2021-06-15T01:45:16.455Z</updated>
        <summary type="html"><![CDATA[Recently, local SGD has got much attention and been extensively studied in
the distributed learning community to overcome the communication bottleneck
problem. However, the superiority of local SGD to minibatch SGD only holds in
quite limited situations. In this paper, we study a new local algorithm called
Bias-Variance Reduced Local SGD (BVR-L-SGD) for nonconvex distributed
optimization. Algorithmically, our proposed bias and variance reduced local
gradient estimator fully utilizes small second-order heterogeneity of local
objectives and suggests randomly picking up one of the local models instead of
taking the average of them when workers are synchronized. Theoretically, under
small heterogeneity of local objectives, we show that BVR-L-SGD achieves better
communication complexity than both the previous non-local and local methods
under mild conditions, and particularly BVR-L-SGD is the first method that
breaks the barrier of communication complexity $\Theta(1/\varepsilon)$ for
general nonconvex smooth objectives when the heterogeneity is small and the
local computation budget is large. Numerical results are given to verify the
theoretical findings and give empirical evidence of the superiority of our
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murata_T/0/1/0/all/0/1"&gt;Tomoya Murata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NLHD: A Pixel-Level Non-Local Retinex Model for Low-Light Image Enhancement. (arXiv:2106.06971v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06971</id>
        <link href="http://arxiv.org/abs/2106.06971"/>
        <updated>2021-06-15T01:45:16.448Z</updated>
        <summary type="html"><![CDATA[Retinex model has been applied to low-light image enhancement in many
existing methods. More appropriate decomposition of a low-light image can help
achieve better image enhancement. In this paper, we propose a new pixel-level
non-local Haar transform based illumination and reflectance decomposition
method (NLHD). The unique low-frequency coefficient of Haar transform on each
similar pixel group is used to reconstruct the illumination component, and the
rest of all high-frequency coefficients are employed to reconstruct the
reflectance component. The complete similarity of pixels in a matched similar
pixel group and the simple separable Haar transform help to obtain more
appropriate image decomposition; thus, the image is hardly sharpened in the
image brightness enhancement procedure. The exponential transform and
logarithmic transform are respectively implemented on the illumination
component. Then a minimum fusion strategy on the results of these two
transforms is utilized to achieve more natural illumination component
enhancement. It can alleviate the mosaic artifacts produced in the darker
regions by the exponential transform with a gamma value less than 1 and reduce
information loss caused by excessive enhancement of the brighter regions due to
the logarithmic transform. Finally, the Retinex model is applied to the
enhanced illumination and reflectance to achieve image enhancement. We also
develop a local noise level estimation based noise suppression method and a
non-local saturation reduction based color deviation correction method. These
two methods can respectively attenuate noise or color deviation usually
presented in the enhanced results of the extremely dark low-light images.
Experiments on benchmark datasets show that the proposed method can achieve
better low-light image enhancement results on subjective and objective
evaluations than most existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1"&gt;Hou Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yingkun_H/0/1/0/all/0/1"&gt;Hou Yingkun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuxuan_S/0/1/0/all/0/1"&gt;Shi Yuxuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benzheng_W/0/1/0/all/0/1"&gt;Wei Benzheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jun_X/0/1/0/all/0/1"&gt;Xu Jun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding the Stochastic Shortest Path with Low Regret: The Adversarial Cost and Unknown Transition Case. (arXiv:2102.05284v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05284</id>
        <link href="http://arxiv.org/abs/2102.05284"/>
        <updated>2021-06-15T01:45:16.442Z</updated>
        <summary type="html"><![CDATA[We make significant progress toward the stochastic shortest path problem with
adversarial costs and unknown transition. Specifically, we develop algorithms
that achieve $\widetilde{O}(\sqrt{S^2ADT_\star K})$ regret for the
full-information setting and $\widetilde{O}(\sqrt{S^3A^2DT_\star K})$ regret
for the bandit feedback setting, where $D$ is the diameter, $T_\star$ is the
expected hitting time of the optimal policy, $S$ is the number of states, $A$
is the number of actions, and $K$ is the number of episodes. Our work strictly
improves (Rosenberg and Mansour, 2020) in the full information setting, extends
(Chen et al., 2020) from known transition to unknown transition, and is also
the first to consider the most challenging combination: bandit feedback with
adversarial costs and unknown transition. To remedy the gap between our upper
bounds and the current best lower bounds constructed via a stochastically
oblivious adversary, we also propose algorithms with near-optimal regret for
this special case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Achieving Efficiency in Black Box Simulation of Distribution Tails with Self-structuring Importance Samplers. (arXiv:2102.07060v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07060</id>
        <link href="http://arxiv.org/abs/2102.07060"/>
        <updated>2021-06-15T01:45:16.435Z</updated>
        <summary type="html"><![CDATA[Motivated by the increasing adoption of models which facilitate greater
automation in risk management and decision-making, this paper presents a novel
Importance Sampling (IS) scheme for measuring distribution tails of objectives
modelled with enabling tools such as feature-based decision rules, mixed
integer linear programs, deep neural networks, etc. Conventional efficient IS
approaches suffer from feasibility and scalability concerns due to the need to
intricately tailor the sampler to the underlying probability distribution and
the objective. This challenge is overcome in the proposed black-box scheme by
automating the selection of an effective IS distribution with a transformation
that implicitly learns and replicates the concentration properties observed in
less rare samples. This novel approach is guided by a large deviations
principle that brings out the phenomenon of self-similarity of optimal IS
distributions. The proposed sampler is the first to attain asymptotically
optimal variance reduction across a spectrum of multivariate distributions
despite being oblivious to the underlying structure. The large deviations
principle additionally results in new distribution tail asymptotics capable of
yielding operational insights. The applicability is illustrated by considering
product distribution networks and portfolio credit risk models informed by
neural networks as examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Deo_A/0/1/0/all/0/1"&gt;Anand Deo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Murthy_K/0/1/0/all/0/1"&gt;Karthyek Murthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Attention for Automatic Chest X-ray Report Generation. (arXiv:2106.06965v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06965</id>
        <link href="http://arxiv.org/abs/2106.06965"/>
        <updated>2021-06-15T01:45:16.418Z</updated>
        <summary type="html"><![CDATA[Recently, chest X-ray report generation, which aims to automatically generate
descriptions of given chest X-ray images, has received growing research
interests. The key challenge of chest X-ray report generation is to accurately
capture and describe the abnormal regions. In most cases, the normal regions
dominate the entire chest X-ray image, and the corresponding descriptions of
these normal regions dominate the final report. Due to such data bias,
learning-based models may fail to attend to abnormal regions. In this work, to
effectively capture and describe abnormal regions, we propose the Contrastive
Attention (CA) model. Instead of solely focusing on the current input image,
the CA model compares the current input image with normal images to distill the
contrastive information. The acquired contrastive information can better
represent the visual features of abnormal regions. According to the experiments
on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into
several existing models can boost their performance across most metrics. In
addition, according to the analysis, the CA model can help existing models
better attend to the abnormal regions and provide more accurate descriptions
which are crucial for an interpretable diagnosis. Specifically, we achieve the
state-of-the-art results on the two public datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fenglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1"&gt;Changchang Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shen Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Ping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Binary Decision Trees by Argmin Differentiation. (arXiv:2010.04627v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04627</id>
        <link href="http://arxiv.org/abs/2010.04627"/>
        <updated>2021-06-15T01:45:16.411Z</updated>
        <summary type="html"><![CDATA[We address the problem of learning binary decision trees that partition data
for some downstream task. We propose to learn discrete parameters (i.e., for
tree traversals and node pruning) and continuous parameters (i.e., for tree
split functions and prediction functions) simultaneously using argmin
differentiation. We do so by sparsely relaxing a mixed-integer program for the
discrete parameters, to allow gradients to pass through the program to
continuous parameters. We derive customized algorithms to efficiently compute
the forward and backward passes. This means that our tree learning procedure
can be used as an (implicit) layer in arbitrary deep networks, and can be
optimized with arbitrary loss functions. We demonstrate that our approach
produces binary trees that are competitive with existing single tree and
ensemble approaches, in both supervised and unsupervised settings. Further,
apart from greedy approaches (which do not have competitive accuracies), our
method is faster to train than all other tree-learning baselines we compare
with. The code for reproducing the results is available at
https://github.com/vzantedeschi/LatentTrees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zantedeschi_V/0/1/0/all/0/1"&gt;Valentina Zantedeschi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1"&gt;Matt J. Kusner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1"&gt;Vlad Niculae&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kwame: A Bilingual AI Teaching Assistant for Online SuaCode Courses. (arXiv:2010.11387v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11387</id>
        <link href="http://arxiv.org/abs/2010.11387"/>
        <updated>2021-06-15T01:45:16.398Z</updated>
        <summary type="html"><![CDATA[Introductory hands-on courses such as our smartphone-based coding course,
SuaCode require a lot of support for students to accomplish learning goals.
Online environments make it even more difficult to get assistance especially
more recently because of COVID-19. Given the multilingual context of SuaCode
students - learners across 42 African countries that are mostly Anglophone or
Francophone - in this work, we developed a bilingual Artificial Intelligence
(AI) Teaching Assistant (TA) - Kwame - that provides answers to students'
coding questions from SuaCode courses in English and French. Kwame is a
Sentence-BERT (SBERT)-based question-answering (QA) system that we trained and
evaluated offline using question-answer pairs created from the course's
quizzes, lesson notes and students' questions in past cohorts. Kwame finds the
paragraph most semantically similar to the question via cosine similarity. We
compared the system with TF-IDF and Universal Sentence Encoder. Our results
showed that fine-tuning on the course data and returning the top 3 and 5
answers improved the accuracy results. Kwame will make it easy for students to
get quick and accurate answers to questions in SuaCode courses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1"&gt;George Boateng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Spatio-Temporal Poisson Point Process: A Simple Model for the Alignment of Event Camera Data. (arXiv:2106.06887v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06887</id>
        <link href="http://arxiv.org/abs/2106.06887"/>
        <updated>2021-06-15T01:45:16.386Z</updated>
        <summary type="html"><![CDATA[Event cameras, inspired by biological vision systems, provide a natural and
data efficient representation of visual information. Visual information is
acquired in the form of events that are triggered by local brightness changes.
Each pixel location of the camera's sensor records events asynchronously and
independently with very high temporal resolution. However, because most
brightness changes are triggered by relative motion of the camera and the
scene, the events recorded at a single sensor location seldom correspond to the
same world point. To extract meaningful information from event cameras, it is
helpful to register events that were triggered by the same underlying world
point. In this work we propose a new model of event data that captures its
natural spatio-temporal structure. We start by developing a model for aligned
event data. That is, we develop a model for the data as though it has been
perfectly registered already. In particular, we model the aligned data as a
spatio-temporal Poisson point process. Based on this model, we develop a
maximum likelihood approach to registering events that are not yet aligned.
That is, we find transformations of the observed events that make them as
likely as possible under our model. In particular we extract the camera
rotation that leads to the best event alignment. We show new state of the art
accuracy for rotational velocity estimation on the DAVIS 240C dataset. In
addition, our method is also faster and has lower computational complexity than
several competing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1"&gt;Cheng Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Learned_Miller_E/0/1/0/all/0/1"&gt;Erik Learned-Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheldon_D/0/1/0/all/0/1"&gt;Daniel Sheldon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1"&gt;Guillermo Gallego&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bideau_P/0/1/0/all/0/1"&gt;Pia Bideau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Quantized Neural Nets by Coarse Gradient Method for Non-linear Classification. (arXiv:2011.11256v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11256</id>
        <link href="http://arxiv.org/abs/2011.11256"/>
        <updated>2021-06-15T01:45:16.379Z</updated>
        <summary type="html"><![CDATA[Quantized or low-bit neural networks are attractive due to their inference
efficiency. However, training deep neural networks with quantized activations
involves minimizing a discontinuous and piecewise constant loss function. Such
a loss function has zero gradients almost everywhere (a.e.), which makes the
conventional gradient-based algorithms inapplicable. To this end, we study a
novel class of \emph{biased} first-order oracle, termed coarse gradient, for
overcoming the vanished gradient issue. A coarse gradient is generated by
replacing the a.e. zero derivatives of quantized (i.e., stair-case) ReLU
activation composited in the chain rule with some heuristic proxy derivative
called straight-through estimator (STE). Although having been widely used in
training quantized networks empirically, fundamental questions like when and
why the ad-hoc STE trick works, still lacks theoretical understanding. In this
paper, we propose a class of STEs with certain monotonicity, and consider their
applications to the training of a two-linear-layer network with quantized
activation functions for non-linear multi-category classification. We establish
performance guarantees for the proposed STEs by showing that the corresponding
coarse gradient methods converge to the global minimum, which leads to a
perfect classification. Lastly, we present experimental results on synthetic
data as well as MNIST dataset to verify our theoretical findings and
demonstrate the effectiveness of our proposed STEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1"&gt;Ziang Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1"&gt;Penghang Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Jack Xin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LE-NAS: Learning-based Ensenble with NAS for Dose Prediction. (arXiv:2106.06733v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06733</id>
        <link href="http://arxiv.org/abs/2106.06733"/>
        <updated>2021-06-15T01:45:16.362Z</updated>
        <summary type="html"><![CDATA[Radiation therapy treatment planning is a complex process, as the target dose
prescription and normal tissue sparing are conflicting objectives. Automated
and accurate dose prediction for radiation therapy planning is in high demand.
In this study, we propose a novel learning-based ensemble approach, named
LE-NAS, which integrates neural architecture search (NAS) with knowledge
distillation for 3D radiotherapy dose prediction. Specifically, the prediction
network first exhaustively searches each block from enormous architecture
space. Then, multiple architectures are selected with promising performance and
diversity. To reduce the inference time, we adopt the teacher-student paradigm
by treating the combination of diverse outputs from multiple searched networks
as supervisions to guide the student network training. In addition, we apply
adversarial learning to optimize the student network to recover the knowledge
in teacher networks. To the best of our knowledge, we are the first to
investigate the combination of NAS and knowledge distillation. The proposed
method has been evaluated on the public OpenKBP dataset, and experimental
results demonstrate the effectiveness of our method and its superior
performance to the state-of-the-art method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yanfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingguang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guocai Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.06804</id>
        <link href="http://arxiv.org/abs/2106.06804"/>
        <updated>2021-06-15T01:45:16.341Z</updated>
        <summary type="html"><![CDATA[Explainable artificial intelligence has rapidly emerged since lawmakers have
started requiring interpretable models for safety-critical domains.
Concept-based neural networks have arisen as explainable-by-design methods as
they leverage human-understandable symbols (i.e. concepts) to predict class
memberships. However, most of these approaches focus on the identification of
the most relevant concepts but do not provide concise, formal explanations of
how such concepts are leveraged by the classifier to make predictions. In this
paper, we propose a novel end-to-end differentiable approach enabling the
extraction of logic explanations from neural networks using the formalism of
First-Order Logic. The method relies on an entropy-based criterion which
automatically identifies the most relevant concepts. We consider four different
case studies to demonstrate that: (i) this entropy-based criterion enables the
distillation of concise logic explanations in safety-critical domains from
clinical data to computer vision; (ii) the proposed approach outperforms
state-of-the-art white-box models in terms of classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1"&gt;Pietro Barbiero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1"&gt;Gabriele Ciravegna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1"&gt;Francesco Giannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf3;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1"&gt;Marco Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1"&gt;Stefano Melacci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Descent for Visual 3D Human Pose and Shape. (arXiv:2008.06910v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.06910</id>
        <link href="http://arxiv.org/abs/2008.06910"/>
        <updated>2021-06-15T01:45:16.329Z</updated>
        <summary type="html"><![CDATA[We present deep neural network methodology to reconstruct the 3d pose and
shape of people, given an input RGB image. We rely on a recently introduced,
expressivefull body statistical 3d human model, GHUM, trained end-to-end, and
learn to reconstruct its pose and shape state in a self-supervised regime.
Central to our methodology, is a learning to learn and optimize approach,
referred to as HUmanNeural Descent (HUND), which avoids both second-order
differentiation when training the model parameters,and expensive state gradient
descent in order to accurately minimize a semantic differentiable rendering
loss at test time. Instead, we rely on novel recurrent stages to update the
pose and shape parameters such that not only losses are minimized effectively,
but the process is meta-regularized in order to ensure end-progress. HUND's
symmetry between training and testing makes it the first 3d human sensing
architecture to natively support different operating regimes including
self-supervised ones. In diverse tests, we show that HUND achieves very
competitive results in datasets like H3.6M and 3DPW, aswell as good quality 3d
reconstructions for complex imagery collected in-the-wild.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zanfir_A/0/1/0/all/0/1"&gt;Andrei Zanfir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bazavan_E/0/1/0/all/0/1"&gt;Eduard Gabriel Bazavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanfir_M/0/1/0/all/0/1"&gt;Mihai Zanfir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukthankar_R/0/1/0/all/0/1"&gt;Rahul Sukthankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1"&gt;Cristian Sminchisescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAR-Net: Unsupervised Co-Attention Guided Registration Network for Joint Registration and Structure Learning. (arXiv:2106.06637v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06637</id>
        <link href="http://arxiv.org/abs/2106.06637"/>
        <updated>2021-06-15T01:45:16.323Z</updated>
        <summary type="html"><![CDATA[Image registration is a fundamental building block for various applications
in medical image analysis. To better explore the correlation between the fixed
and moving images and improve registration performance, we propose a novel deep
learning network, Co-Attention guided Registration Network (CAR-Net). CAR-Net
employs a co-attention block to learn a new representation of the inputs, which
drives the registration of the fixed and moving images. Experiments on UK
Biobank cardiac cine-magnetic resonance image data demonstrate that CAR-Net
obtains higher registration accuracy and smoother deformation fields than
state-of-the-art unsupervised registration methods, while achieving comparable
or better registration performance than corresponding weakly-supervised
variants. In addition, our approach can provide critical structural information
of the input fixed and moving images simultaneously in a completely
unsupervised manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1"&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F Frangi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape-Preserving Dimensionality Reduction : An Algorithm and Measures of Topological Equivalence. (arXiv:2106.02096v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02096</id>
        <link href="http://arxiv.org/abs/2106.02096"/>
        <updated>2021-06-15T01:45:16.315Z</updated>
        <summary type="html"><![CDATA[We introduce a linear dimensionality reduction technique preserving
topological features via persistent homology. The method is designed to find
linear projection $L$ which preserves the persistent diagram of a point cloud
$\mathbb{X}$ via simulated annealing. The projection $L$ induces a set of
canonical simplicial maps from the Rips (or \v{C}ech) filtration of
$\mathbb{X}$ to that of $L\mathbb{X}$. In addition to the distance between
persistent diagrams, the projection induces a map between filtrations, called
filtration homomorphism. Using the filtration homomorphism, one can measure the
difference between shapes of two filtrations directly comparing simplicial
complexes with respect to quasi-isomorphism $\mu_{\operatorname{quasi-iso}}$ or
strong homotopy equivalence $\mu_{\operatorname{equiv}}$. These
$\mu_{\operatorname{quasi-iso}}$ and $\mu_{\operatorname{equiv}}$ measures how
much portion of corresponding simplicial complexes is quasi-isomorphic or
homotopy equivalence respectively. We validate the effectiveness of our
framework with simple examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1"&gt;Byeongsu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+You_K/0/1/0/all/0/1"&gt;Kisung You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SKIing on Simplices: Kernel Interpolation on the Permutohedral Lattice for Scalable Gaussian Processes. (arXiv:2106.06695v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06695</id>
        <link href="http://arxiv.org/abs/2106.06695"/>
        <updated>2021-06-15T01:45:16.284Z</updated>
        <summary type="html"><![CDATA[State-of-the-art methods for scalable Gaussian processes use iterative
algorithms, requiring fast matrix vector multiplies (MVMs) with the covariance
kernel. The Structured Kernel Interpolation (SKI) framework accelerates these
MVMs by performing efficient MVMs on a grid and interpolating back to the
original space. In this work, we develop a connection between SKI and the
permutohedral lattice used for high-dimensional fast bilateral filtering. Using
a sparse simplicial grid instead of a dense rectangular one, we can perform GP
inference exponentially faster in the dimension than SKI. Our approach,
Simplex-GP, enables scaling SKI to high dimensions, while maintaining strong
predictive performance. We additionally provide a CUDA implementation of
Simplex-GP, which enables significant GPU acceleration of MVM based inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1"&gt;Sanyam Kapoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finzi_M/0/1/0/all/0/1"&gt;Marc Finzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Ke Alexander Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"&gt;Andrew Gordon Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization. (arXiv:2008.10898v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10898</id>
        <link href="http://arxiv.org/abs/2008.10898"/>
        <updated>2021-06-15T01:45:16.278Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel stochastic gradient estimator --
ProbAbilistic Gradient Estimator (PAGE) -- for nonconvex optimization. PAGE is
easy to implement as it is designed via a small adjustment to vanilla SGD: in
each iteration, PAGE uses the vanilla minibatch SGD update with probability
$p_t$ or reuses the previous gradient with a small adjustment, at a much lower
computational cost, with probability $1-p_t$. We give a simple formula for the
optimal choice of $p_t$. Moreover, we prove the first tight lower bound
$\Omega(n+\frac{\sqrt{n}}{\epsilon^2})$ for nonconvex finite-sum problems,
which also leads to a tight lower bound $\Omega(b+\frac{\sqrt{b}}{\epsilon^2})$
for nonconvex online problems, where $b:= \min\{\frac{\sigma^2}{\epsilon^2},
n\}$. Then, we show that PAGE obtains the optimal convergence results
$O(n+\frac{\sqrt{n}}{\epsilon^2})$ (finite-sum) and
$O(b+\frac{\sqrt{b}}{\epsilon^2})$ (online) matching our lower bounds for both
nonconvex finite-sum and online problems. Besides, we also show that for
nonconvex functions satisfying the Polyak-\L{}ojasiewicz (PL) condition, PAGE
can automatically switch to a faster linear convergence rate $O(\cdot\log
\frac{1}{\epsilon})$. Finally, we conduct several deep learning experiments
(e.g., LeNet, VGG, ResNet) on real datasets in PyTorch showing that PAGE not
only converges much faster than SGD in training but also achieves the higher
test accuracy, validating the optimal theoretical results and confirming the
practical superiority of PAGE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhize Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hongyan Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangliang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prioritized Level Replay. (arXiv:2010.03934v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03934</id>
        <link href="http://arxiv.org/abs/2010.03934"/>
        <updated>2021-06-15T01:45:16.272Z</updated>
        <summary type="html"><![CDATA[Environments with procedurally generated content serve as important
benchmarks for testing systematic generalization in deep reinforcement
learning. In this setting, each level is an algorithmically created environment
instance with a unique configuration of its factors of variation. Training on a
prespecified subset of levels allows for testing generalization to unseen
levels. What can be learned from a level depends on the current policy, yet
prior work defaults to uniform sampling of training levels independently of the
policy. We introduce Prioritized Level Replay (PLR), a general framework for
selectively sampling the next training level by prioritizing those with higher
estimated learning potential when revisited in the future. We show TD-errors
effectively estimate a level's future learning potential and, when used to
guide the sampling procedure, induce an emergent curriculum of increasingly
difficult levels. By adapting the sampling of training levels, PLR
significantly improves sample efficiency and generalization on Procgen
Benchmark--matching the previous state-of-the-art in test return--and readily
combines with other methods. Combined with the previous leading method, PLR
raises the state-of-the-art to over 76% improvement in test return relative to
standard RL baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Minqi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1"&gt;Edward Grefenstette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1"&gt;Tim Rockt&amp;#xe4;schel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Capsule Attention for Multimodal EEG-EOG Representation Learning with Application to Driver Vigilance Estimation. (arXiv:1912.07812v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.07812</id>
        <link href="http://arxiv.org/abs/1912.07812"/>
        <updated>2021-06-15T01:45:16.266Z</updated>
        <summary type="html"><![CDATA[Driver vigilance estimation is an important task for transportation safety.
Wearable and portable brain-computer interface devices provide a powerful means
for real-time monitoring of the vigilance level of drivers to help with
avoiding distracted or impaired driving. In this paper, we propose a novel
multimodal architecture for in-vehicle vigilance estimation from
Electroencephalogram and Electrooculogram. To enable the system to focus on the
most salient parts of the learned multimodal representations, we propose an
architecture composed of a capsule attention mechanism following a deep Long
Short-Term Memory (LSTM) network. Our model learns hierarchical dependencies in
the data through the LSTM and capsule feature representation layers. To better
explore the discriminative ability of the learned representations, we study the
effect of the proposed capsule attention mechanism including the number of
dynamic routing iterations as well as other parameters. Experiments show the
robustness of our method by outperforming other solutions and baseline
techniques, setting a new state-of-the-art. We then provide an analysis on
different frequency bands and brain regions to evaluate their suitability for
driver vigilance estimation. Lastly, an analysis on the role of capsule
attention, multimodality, and robustness to noise is performed, highlighting
the advantages of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guangyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1"&gt;Ali Etemad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse PointPillars: Exploiting Sparsity in Birds-Eye-View Object Detection. (arXiv:2106.06882v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06882</id>
        <link href="http://arxiv.org/abs/2106.06882"/>
        <updated>2021-06-15T01:45:16.254Z</updated>
        <summary type="html"><![CDATA[Bird's Eye View (BEV) is a popular representation for processing 3D point
clouds, and by its nature is fundamentally sparse. Motivated by the
computational limitations of mobile robot platforms, we take a fast
high-performance BEV 3D object detector - PointPillars - and modify its
backbone to exploit this sparsity, leading to decreased runtimes. We present
preliminary results demonstrating decreased runtimes with either the same
performance or a modest decrease in performance, which we anticipate will be
remedied by model specific hyperparameter tuning. Our work is a first step
towards a new class of 3D object detectors that exploit sparsity throughout
their entire pipeline in order to reduce runtime and resource usage while
maintaining good detection performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vedder_K/0/1/0/all/0/1"&gt;Kyle Vedder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1"&gt;Eric Eaton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Break-It-Fix-It: Unsupervised Learning for Program Repair. (arXiv:2106.06600v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06600</id>
        <link href="http://arxiv.org/abs/2106.06600"/>
        <updated>2021-06-15T01:45:16.232Z</updated>
        <summary type="html"><![CDATA[We consider repair tasks: given a critic (e.g., compiler) that assesses the
quality of an input, the goal is to train a fixer that converts a bad example
(e.g., code with syntax errors) into a good one (e.g., code with no errors).
Existing works create training data consisting of (bad, good) pairs by
corrupting good examples using heuristics (e.g., dropping tokens). However,
fixers trained on this synthetically-generated data do not extrapolate well to
the real distribution of bad inputs. To bridge this gap, we propose a new
training approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use
the critic to check a fixer's output on real bad inputs and add good (fixed)
outputs to the training data, and (ii) we train a breaker to generate realistic
bad code from good code. Based on these ideas, we iteratively update the
breaker and the fixer while using them in conjunction to generate more paired
data. We evaluate BIFI on two code repair datasets: GitHub-Python, a new
dataset we introduce where the goal is to repair Python code with AST parse
errors; and DeepFix, where the goal is to repair C code with compiler errors.
BIFI outperforms existing methods, obtaining 90.5% repair accuracy on
GitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not
require any labeled data; we hope it will be a strong starting point for
unsupervised learning of various repair tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1"&gt;Michihiro Yasunaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale Unsupervised Object Discovery. (arXiv:2106.06650v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06650</id>
        <link href="http://arxiv.org/abs/2106.06650"/>
        <updated>2021-06-15T01:45:16.226Z</updated>
        <summary type="html"><![CDATA[Existing approaches to unsupervised object discovery (UOD) do not scale up to
large datasets without approximations which compromise their performance. We
propose a novel formulation of UOD as a ranking problem, amenable to the
arsenal of distributed methods available for eigenvalue problems and link
analysis. Extensive experiments with COCO and OpenImages demonstrate that, in
the single-object discovery setting where a single prominent object is sought
in each image, the proposed LOD (Large-scale Object Discovery) approach is on
par with, or better than the state of the art for medium-scale datasets (up to
120K images), and over 37% better than the only other algorithms capable of
scaling up to 1.7M images. In the multi-object discovery setting where multiple
objects are sought in each image, the proposed LOD is over 14% better in
average precision (AP) than all other methods for datasets ranging from 20K to
1.7M images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1"&gt;Huy V. Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sizikova_E/0/1/0/all/0/1"&gt;Elena Sizikova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1"&gt;Cordelia Schmid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1"&gt;Patrick P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1"&gt;Jean Ponce&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fortify Machine Learning Production Systems: Detect and Classify Adversarial Attacks. (arXiv:2102.09695v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09695</id>
        <link href="http://arxiv.org/abs/2102.09695"/>
        <updated>2021-06-15T01:45:16.173Z</updated>
        <summary type="html"><![CDATA[Production machine learning systems are consistently under attack by
adversarial actors. Various deep learning models must be capable of accurately
detecting fake or adversarial input while maintaining speed. In this work, we
propose one piece of the production protection system: detecting an incoming
adversarial attack and its characteristics. Detecting types of adversarial
attacks has two primary effects: the underlying model can be trained in a
structured manner to be robust from those attacks and the attacks can be
potentially filtered out in real-time before causing any downstream damage. The
adversarial image classification space is explored for models commonly used in
transfer learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ciolino_M/0/1/0/all/0/1"&gt;Matthew Ciolino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalin_J/0/1/0/all/0/1"&gt;Josh Kalin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1"&gt;David Noever&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum n-times Coverage for Vaccine Design. (arXiv:2101.10902v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10902</id>
        <link href="http://arxiv.org/abs/2101.10902"/>
        <updated>2021-06-15T01:45:16.147Z</updated>
        <summary type="html"><![CDATA[We introduce the maximum $n$-times coverage problem that selects $k$ overlays
to maximize the summed coverage of weighted elements, where each element must
be covered at least $n$ times. We also define the min-cost $n$-times coverage
problem where the objective is to select the minimum set of overlays such that
the sum of the weights of elements that are covered at least $n$ times is at
least $\tau$. Maximum $n$-times coverage is a generalization of the multi-set
multi-cover problem, is NP-complete, and is not submodular. We introduce two
new practical solutions for $n$-times coverage based on integer linear
programming and sequential greedy optimization. We show that maximum $n$-times
coverage is a natural way to frame peptide vaccine design, and find that it
produces a pan-strain COVID-19 vaccine design that is superior to 29 other
published designs in predicted population coverage and the expected number of
peptides displayed by each individual's HLA molecules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Liu_G/0/1/0/all/0/1"&gt;Ge Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Dimitrakakis_A/0/1/0/all/0/1"&gt;Alexander Dimitrakakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Carter_B/0/1/0/all/0/1"&gt;Brandon Carter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Gifford_D/0/1/0/all/0/1"&gt;David Gifford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Cardiovascular Modelling with Deep Reinforcement Learning for Uncertainty Aware Control of Sepsis Treatment. (arXiv:2101.08477v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08477</id>
        <link href="http://arxiv.org/abs/2101.08477"/>
        <updated>2021-06-15T01:45:16.122Z</updated>
        <summary type="html"><![CDATA[Sepsis is a potentially life threatening inflammatory response to infection
or severe tissue damage. It has a highly variable clinical course, requiring
constant monitoring of the patient's state to guide the management of
intravenous fluids and vasopressors, among other interventions. Despite decades
of research, there's still debate among experts on optimal treatment. Here, we
combine for the first time, distributional deep reinforcement learning with
mechanistic physiological models to find personalized sepsis treatment
strategies. Our method handles partial observability by leveraging known
cardiovascular physiology, introducing a novel physiology-driven recurrent
autoencoder, and quantifies the uncertainty of its own results. Moreover, we
introduce a framework for uncertainty aware decision support with humans in the
loop. We show that our method learns physiologically explainable, robust
policies that are consistent with clinical knowledge. Further our method
consistently identifies high risk states that lead to death, which could
potentially benefit from more frequent vasopressor administration, providing
valuable guidance for future research]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nanayakkara_T/0/1/0/all/0/1"&gt;Thesath Nanayakkara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clermont_G/0/1/0/all/0/1"&gt;Gilles Clermont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langmead_C/0/1/0/all/0/1"&gt;Christopher James Langmead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swigon_D/0/1/0/all/0/1"&gt;David Swigon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TDGIA:Effective Injection Attacks on Graph Neural Networks. (arXiv:2106.06663v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06663</id>
        <link href="http://arxiv.org/abs/2106.06663"/>
        <updated>2021-06-15T01:45:16.046Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have achieved promising performance in various
real-world applications. However, recent studies have shown that GNNs are
vulnerable to adversarial attacks. In this paper, we study a
recently-introduced realistic attack scenario on graphs -- graph injection
attack (GIA). In the GIA scenario, the adversary is not able to modify the
existing link structure and node attributes of the input graph, instead the
attack is performed by injecting adversarial nodes into it. We present an
analysis on the topological vulnerability of GNNs under GIA setting, based on
which we propose the Topological Defective Graph Injection Attack (TDGIA) for
effective injection attacks. TDGIA first introduces the topological defective
edge selection strategy to choose the original nodes for connecting with the
injected ones. It then designs the smooth feature optimization objective to
generate the features for the injected nodes. Extensive experiments on
large-scale datasets show that TDGIA can consistently and significantly
outperform various attack baselines in attacking dozens of defense GNN models.
Notably, the performance drop on target GNNs resultant from TDGIA is more than
double the damage brought by the best attack solution among hundreds of
submissions on KDD-CUP 2020.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1"&gt;Xu Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1"&gt;Qinkai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yuxiao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1"&gt;Xinyu Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1"&gt;Evgeny Kharlamov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jialiang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Adversarial Robustness for Speaker Verification by Self-Supervised Learning. (arXiv:2106.00273v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00273</id>
        <link href="http://arxiv.org/abs/2106.00273"/>
        <updated>2021-06-15T01:45:16.036Z</updated>
        <summary type="html"><![CDATA[Previous works have shown that automatic speaker verification (ASV) is
seriously vulnerable to malicious spoofing attacks, such as replay, synthetic
speech, and recently emerged adversarial attacks. Great efforts have been
dedicated to defending ASV against replay and synthetic speech; however, only a
few approaches have been explored to deal with adversarial attacks. All the
existing approaches to tackle adversarial attacks for ASV require the knowledge
for adversarial samples generation, but it is impractical for defenders to know
the exact attack algorithms that are applied by the in-the-wild attackers. This
work is among the first to perform adversarial defense for ASV without knowing
the specific attack algorithms. Inspired by self-supervised learning models
(SSLMs) that possess the merits of alleviating the superficial noise in the
inputs and reconstructing clean samples from the interrupted ones, this work
regards adversarial perturbations as one kind of noise and conducts adversarial
defense for ASV by SSLMs. Specifically, we propose to perform adversarial
defense from two perspectives: 1) adversarial perturbation purification and 2)
adversarial perturbation detection. Experimental results show that our
detection module effectively shields the ASV by detecting adversarial samples
with an accuracy of around 80%. Moreover, since there is no common metric for
evaluating the adversarial defense performance for ASV, this work also
formalizes evaluation metrics for adversarial defense considering both
purification and detection based approaches into account. We sincerely
encourage future works to benchmark their approaches based on the proposed
evaluation framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haibin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Andy T. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1"&gt;Helen Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strategic Classification in the Dark. (arXiv:2102.11592v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11592</id>
        <link href="http://arxiv.org/abs/2102.11592"/>
        <updated>2021-06-15T01:45:16.000Z</updated>
        <summary type="html"><![CDATA[Strategic classification studies the interaction between a classification
rule and the strategic agents it governs. Under the assumption that the
classifier is known, rational agents respond to it by manipulating their
features. However, in many real-life scenarios of high-stake classification
(e.g., credit scoring), the classifier is not revealed to the agents, which
leads agents to attempt to learn the classifier and game it too. In this paper
we generalize the strategic classification model to such scenarios. We define
the price of opacity as the difference in prediction error between opaque and
transparent strategy-robust classifiers, characterize it, and give a sufficient
condition for this price to be strictly positive, in which case transparency is
the recommended policy. Our experiments show how Hardt et al.'s robust
classifier is affected by keeping agents in the dark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghalme_G/0/1/0/all/0/1"&gt;Ganesh Ghalme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1"&gt;Vineet Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eilat_I/0/1/0/all/0/1"&gt;Itay Eilat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talgam_Cohen_I/0/1/0/all/0/1"&gt;Inbal Talgam-Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenfeld_N/0/1/0/all/0/1"&gt;Nir Rosenfeld&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Deflation Process in Over-parametrized Tensor Decomposition. (arXiv:2106.06573v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06573</id>
        <link href="http://arxiv.org/abs/2106.06573"/>
        <updated>2021-06-15T01:45:15.988Z</updated>
        <summary type="html"><![CDATA[In this paper we study the training dynamics for gradient flow on
over-parametrized tensor decomposition problems. Empirically, such training
process often first fits larger components and then discovers smaller
components, which is similar to a tensor deflation process that is commonly
used in tensor decomposition algorithms. We prove that for orthogonally
decomposable tensor, a slightly modified version of gradient flow would follow
a tensor deflation process and recover all the tensor components. Our proof
suggests that for orthogonal tensors, gradient flow dynamics works similarly as
greedy low-rank learning in the matrix setting, which is a first step towards
understanding the implicit regularization effect of over-parametrized models
for low-rank tensors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ge_R/0/1/0/all/0/1"&gt;Rong Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yunwei Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mo Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Learning with Class Imbalance. (arXiv:2101.02523v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02523</id>
        <link href="http://arxiv.org/abs/2101.02523"/>
        <updated>2021-06-15T01:45:15.982Z</updated>
        <summary type="html"><![CDATA[Few-Shot Learning (FSL) algorithms are commonly trained through Meta-Learning
(ML), which exposes models to batches of tasks sampled from a meta-dataset to
mimic tasks seen during evaluation. However, the standard training procedures
overlook the real-world dynamics where classes commonly occur at different
frequencies. While it is generally understood that class imbalance harms the
performance of supervised methods, limited research examines the impact of
imbalance on the FSL evaluation task. Our analysis compares 10 state-of-the-art
meta-learning and FSL methods on different imbalance distributions and
rebalancing techniques. Our results reveal that 1) some FSL methods display a
natural disposition against imbalance while most other approaches produce a
performance drop by up to 17\% compared to the balanced task without the
appropriate mitigation; 2) contrary to popular belief, many meta-learning
algorithms will not automatically learn to balance from exposure to imbalanced
training tasks; 3) classical rebalancing strategies, such as random
oversampling, can still be very effective, leading to state-of-the-art
performances and should not be overlooked; 4) FSL methods are more robust
against meta-dataset imbalance than imbalance at the task-level with a similar
imbalance ratio ($\rho<20$), with the effect holding even in long-tail datasets
under a larger imbalance ($\rho=65$).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ochal_M/0/1/0/all/0/1"&gt;Mateusz Ochal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patacchiola_M/0/1/0/all/0/1"&gt;Massimiliano Patacchiola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1"&gt;Amos Storkey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vazquez_J/0/1/0/all/0/1"&gt;Jose Vazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning for Electric Vehicle Routing Problem with Time Windows. (arXiv:2010.02068v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02068</id>
        <link href="http://arxiv.org/abs/2010.02068"/>
        <updated>2021-06-15T01:45:15.966Z</updated>
        <summary type="html"><![CDATA[The past decade has seen a rapid penetration of electric vehicles (EV) in the
market, more and more logistics and transportation companies start to deploy
EVs for service provision. In order to model the operations of a commercial EV
fleet, we utilize the EV routing problem with time windows (EVRPTW). In this
research, we propose an end-to-end deep reinforcement learning framework to
solve the EVRPTW. In particular, we develop an attention model incorporating
the pointer network and a graph embedding technique to parameterize a
stochastic policy for solving the EVRPTW. The model is then trained using
policy gradient with rollout baseline. Our numerical studies show that the
proposed model is able to efficiently solve EVRPTW instances of large sizes
that are not solvable with any existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Bo Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghaddar_B/0/1/0/all/0/1"&gt;Bissan Ghaddar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nathwani_J/0/1/0/all/0/1"&gt;Jatin Nathwani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from History for Byzantine Robust Optimization. (arXiv:2012.10333v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10333</id>
        <link href="http://arxiv.org/abs/2012.10333"/>
        <updated>2021-06-15T01:45:15.959Z</updated>
        <summary type="html"><![CDATA[Byzantine robustness has received significant attention recently given its
importance for distributed and federated learning. In spite of this, we
identify severe flaws in existing algorithms even when the data across the
participants is identically distributed. First, we show realistic examples
where current state of the art robust aggregation rules fail to converge even
in the absence of any Byzantine attackers. Secondly, we prove that even if the
aggregation rules may succeed in limiting the influence of the attackers in a
single round, the attackers can couple their attacks across time eventually
leading to divergence. To address these issues, we present two surprisingly
simple strategies: a new robust iterative clipping procedure, and incorporating
worker momentum to overcome time-coupled attacks. This is the first provably
robust method for the standard stochastic optimization setting. Our code is
open sourced at https://github.com/epfml/byzantine-robust-optimizer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1"&gt;Sai Praneeth Karimireddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lie He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cluster-to-Conquer: A Framework for End-to-End Multi-Instance Learning for Whole Slide Image Classification. (arXiv:2103.10626v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10626</id>
        <link href="http://arxiv.org/abs/2103.10626"/>
        <updated>2021-06-15T01:45:15.952Z</updated>
        <summary type="html"><![CDATA[In recent years, the availability of digitized Whole Slide Images (WSIs) has
enabled the use of deep learning-based computer vision techniques for automated
disease diagnosis. However, WSIs present unique computational and algorithmic
challenges. WSIs are gigapixel-sized ($\sim$100K pixels), making them
infeasible to be used directly for training deep neural networks. Also, often
only slide-level labels are available for training as detailed annotations are
tedious and can be time-consuming for experts. Approaches using
multiple-instance learning (MIL) frameworks have been shown to overcome these
challenges. Current state-of-the-art approaches divide the learning framework
into two decoupled parts: a convolutional neural network (CNN) for encoding the
patches followed by an independent aggregation approach for slide-level
prediction. In this approach, the aggregation step has no bearing on the
representations learned by the CNN encoder. We have proposed an end-to-end
framework that clusters the patches from a WSI into ${k}$-groups, samples
${k}'$ patches from each group for training, and uses an adaptive attention
mechanism for slide level prediction; Cluster-to-Conquer (C2C). We have
demonstrated that dividing a WSI into clusters can improve the model training
by exposing it to diverse discriminative features extracted from the patches.
We regularized the clustering mechanism by introducing a KL-divergence loss
between the attention weights of patches in a cluster and the uniform
distribution. The framework is optimized end-to-end on slide-level
cross-entropy, patch-level cross-entropy, and KL-divergence loss
(Implementation: https://github.com/YashSharma/C2C).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sharma_Y/0/1/0/all/0/1"&gt;Yash Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Aman Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ehsan_L/0/1/0/all/0/1"&gt;Lubaina Ehsan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moskaluk_C/0/1/0/all/0/1"&gt;Christopher A. Moskaluk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Syed_S/0/1/0/all/0/1"&gt;Sana Syed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brown_D/0/1/0/all/0/1"&gt;Donald E. Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Continuous Local BDD-Based Search for Hybrid SAT Solving. (arXiv:2012.07983v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07983</id>
        <link href="http://arxiv.org/abs/2012.07983"/>
        <updated>2021-06-15T01:45:15.945Z</updated>
        <summary type="html"><![CDATA[We explore the potential of continuous local search (CLS) in SAT solving by
proposing a novel approach for finding a solution of a hybrid system of Boolean
constraints. The algorithm is based on CLS combined with belief propagation on
binary decision diagrams (BDDs). Our framework accepts all Boolean constraints
that admit compact BDDs, including symmetric Boolean constraints and
small-coefficient pseudo-Boolean constraints as interesting families. We
propose a novel algorithm for efficiently computing the gradient needed by CLS.
We study the capabilities and limitations of our versatile CLS solver, GradSAT,
by applying it on many benchmark instances. The experimental results indicate
that GradSAT can be a useful addition to the portfolio of existing SAT and
MaxSAT solvers for solving Boolean satisfiability and optimization problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1"&gt;Anastasios Kyrillidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vardi_M/0/1/0/all/0/1"&gt;Moshe Y. Vardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiwei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Neural Predictive Coding: Learning Cumulatively Online without Forgetting. (arXiv:1905.10696v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.10696</id>
        <link href="http://arxiv.org/abs/1905.10696"/>
        <updated>2021-06-15T01:45:15.922Z</updated>
        <summary type="html"><![CDATA[In lifelong learning systems, especially those based on artificial neural
networks, one of the biggest obstacles is the severe inability to retain old
knowledge as new information is encountered. This phenomenon is known as
catastrophic forgetting. In this article, we propose a new kind of
connectionist architecture, the Sequential Neural Coding Network, that is
robust to forgetting when learning from streams of data points and, unlike
networks of today, does not learn via the immensely popular back-propagation of
errors. Grounded in the neurocognitive theory of predictive processing, our
model adapts its synapses in a biologically-plausible fashion, while another,
complementary neural system rapidly learns to direct and control this
cortex-like structure by mimicking the task-executive control functionality of
the basal ganglia. In our experiments, we demonstrate that our self-organizing
system experiences significantly less forgetting as compared to standard neural
models and outperforms a wide swath of previously proposed methods even though
it is trained across task datasets in a stream-like fashion. The promising
performance of our complementary system on benchmarks, e.g., SplitMNIST, Split
Fashion MNIST, and Split NotMNIST, offers evidence that by incorporating
mechanisms prominent in real neuronal systems, such as competition, sparse
activation patterns, and iterative input processing, a new possibility for
tackling the grand challenge of lifelong machine learning opens up.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1"&gt;Alexander Ororbia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mali_A/0/1/0/all/0/1"&gt;Ankur Mali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kifer_D/0/1/0/all/0/1"&gt;Daniel Kifer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giles_C/0/1/0/all/0/1"&gt;C. Lee Giles&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alpha Matte Generation from Single Input for Portrait Matting. (arXiv:2106.03210v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03210</id>
        <link href="http://arxiv.org/abs/2106.03210"/>
        <updated>2021-06-15T01:45:15.916Z</updated>
        <summary type="html"><![CDATA[Portrait matting is an important research problem with a wide range of
applications, such as video conference app, image/video editing, and
post-production. The goal is to predict an alpha matte that identifies the
effect of each pixel on the foreground subject. Traditional approaches and most
of the existing works utilized an additional input, e.g., trimap, background
image, to predict alpha matte. However, providing additional input is not
always practical. Besides, models are too sensitive to these additional inputs.
In this paper, we introduce an additional input-free approach to perform
portrait matting using Generative Adversarial Nets (GANs). We divide the main
task into two subtasks. For this, we propose a segmentation network for the
person segmentation and the alpha generation network for alpha matte
prediction. While the segmentation network takes an input image and produces a
coarse segmentation map, the alpha generation network utilizes the same input
image as well as a coarse segmentation map that is produced by the segmentation
network to predict the alpha matte. Besides, we present a segmentation encoding
block to downsample the coarse segmentation map and provide feature
representation to the residual block. Furthermore, we propose border loss to
penalize only the borders of the subject separately which is more likely to be
challenging and we also adapt perceptual loss for portrait matting. To train
the proposed system, we combine two different popular training datasets to
improve the amount of data as well as diversity to address domain shift
problems in the inference time. We tested our model on three different
benchmark datasets, namely Adobe Image Matting dataset, Portrait Matting
dataset, and Distinctions dataset. The proposed method outperformed the MODNet
method that also takes a single input.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1"&gt;Dogucan Yaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1"&gt;Haz&amp;#x131;m Kemal Ekenel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1"&gt;Alexander Waibel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Label Annotation of Chest Abdomen Pelvis Computed Tomography Text Reports Using Deep Learning. (arXiv:2102.02959v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02959</id>
        <link href="http://arxiv.org/abs/2102.02959"/>
        <updated>2021-06-15T01:45:15.907Z</updated>
        <summary type="html"><![CDATA[Purpose: To develop high throughput multi-label annotators for body (chest,
abdomen, and pelvis) Computed Tomography (CT) reports that can be applied
across a variety of abnormalities, organs, and disease states.

Approach: We used a dictionary approach to develop rule-based algorithms
(RBA) for extraction of disease labels from radiology text reports. We targeted
three organ systems (lungs/pleura, liver/gallbladder, kidneys/ureters) with
four diseases per system based on their prevalence in our dataset. To expand
the algorithms beyond pre-defined keywords, attention-guided recurrent neural
networks (RNN) were trained using the RBA-extracted labels to classify reports
as being positive for one or more diseases or normal for each organ system.
Confounding effects on model performance were evaluated using random
initialization or pre-trained embedding as well as different sizes of training
datasets. Performance was evaluated using the receiver operating characteristic
(ROC) area under the curve (AUC) against 2,158 manually obtained labels.

Results: Our models extracted disease labels from 261,229 radiology reports
of 112,501 unique subjects. Pre-trained models outperformed random
initialization across all diseases. As the training dataset size was reduced,
performance was robust except for a few diseases with relatively small number
of cases. Pre-trained classification AUCs achieved > 0.95 for all five disease
outcomes across all three organ systems.

Conclusions: Our label-extracting pipeline was able to encompass a variety of
cases and diseases by generalizing beyond strict rules with exceptional
accuracy. This method can be easily adapted to enable automated labeling of
hospital-scale medical data sets for training image-based disease classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAnniballe_V/0/1/0/all/0/1"&gt;Vincent M. D&amp;#x27;Anniballe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tushar_F/0/1/0/all/0/1"&gt;Fakrul I. Tushar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faryna_K/0/1/0/all/0/1"&gt;Khrystyna Faryna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Songyue Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1"&gt;Maciej A. Mazurowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_G/0/1/0/all/0/1"&gt;Geoffrey D. Rubin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_J/0/1/0/all/0/1"&gt;Joseph Y. Lo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Low-Rank Tensor Learning for Spatiotemporal Traffic Data Imputation. (arXiv:2008.03194v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03194</id>
        <link href="http://arxiv.org/abs/2008.03194"/>
        <updated>2021-06-15T01:45:15.899Z</updated>
        <summary type="html"><![CDATA[Missing value problem in spatiotemporal traffic data has long been a
challenging topic, in particular for large-scale and high-dimensional data with
complex missing mechanisms and diverse degrees of missingness. Recent studies
based on tensor nuclear norm have demonstrated the superiority of tensor
learning in imputation tasks by effectively characterizing the complex
correlations/dependencies in spatiotemporal data. However, despite the
promising results, these approaches do not scale well to large data tensors. In
this paper, we focus on addressing the missing data imputation problem for
large-scale spatiotemporal traffic data. To achieve both high accuracy and
efficiency, we develop a scalable tensor learning model -- Low-Tubal-Rank
Smoothing Tensor Completion (LSTC-Tubal) -- based on the existing framework of
Low-Rank Tensor Completion, which is well-suited for spatiotemporal traffic
data that is characterized by multidimensional structure of location$\times$
time of day $\times$ day. In particular, the proposed LSTC-Tubal model involves
a scalable tensor nuclear norm minimization scheme by integrating linear
unitary transformation. Therefore, tensor nuclear norm minimization can be
solved by singular value thresholding on the transformed matrix of each day
while the day-to-day correlation can be effectively preserved by the unitary
transform matrix. We compare LSTC-Tubal with state-of-the-art baseline models,
and find that LSTC-Tubal can achieve competitive accuracy with a significantly
lower computational cost. In addition, the LSTC-Tubal will also benefit other
tasks in modeling large-scale spatiotemporal traffic data, such as
network-level traffic forecasting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yixian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Saunier_N/0/1/0/all/0/1"&gt;Nicolas Saunier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lijun Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Graph-based Public Good Games with Tree Search and Imitation Learning. (arXiv:2106.06762v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.06762</id>
        <link href="http://arxiv.org/abs/2106.06762"/>
        <updated>2021-06-15T01:45:15.893Z</updated>
        <summary type="html"><![CDATA[Public goods games represent insightful settings for studying incentives for
individual agents to make contributions that, while costly for each of them,
benefit the wider society. In this work, we adopt the perspective of a central
planner with a global view of a network of self-interested agents and the goal
of maximizing some desired property in the context of a best-shot public goods
game. Existing algorithms for this known NP-complete problem find solutions
that are sub-optimal and cannot optimize for criteria other than social
welfare.

In order to efficiently solve public goods games, our proposed method
directly exploits the correspondence between equilibria and the Maximal
Independent Set (mIS) structural property of graphs. In particular, we define a
Markov Decision Process, which incrementally generates an mIS, and adopt a
planning method to search for equilibria, outperforming existing methods.
Furthermore, we devise an imitation learning technique that uses demonstrations
of the search to obtain a graph neural network parametrized policy which
quickly generalizes to unseen game instances. Our evaluation results show that
this policy is able to reach 99.5% of the performance of the planning method
while being approximately three orders of magnitude faster to evaluate on the
largest graphs tested. The methods presented in this work can be applied to a
large class of public goods games of potentially high societal impact.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Darvariu_V/0/1/0/all/0/1"&gt;Victor-Alexandru Darvariu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hailes_S/0/1/0/all/0/1"&gt;Stephen Hailes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1"&gt;Mirco Musolesi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Barlow Twins: Self-Supervised Learning via Redundancy Reduction. (arXiv:2103.03230v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03230</id>
        <link href="http://arxiv.org/abs/2103.03230"/>
        <updated>2021-06-15T01:45:15.887Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (SSL) is rapidly closing the gap with supervised
methods on large computer vision benchmarks. A successful approach to SSL is to
learn embeddings which are invariant to distortions of the input sample.
However, a recurring issue with this approach is the existence of trivial
constant solutions. Most current methods avoid such solutions by careful
implementation details. We propose an objective function that naturally avoids
collapse by measuring the cross-correlation matrix between the outputs of two
identical networks fed with distorted versions of a sample, and making it as
close to the identity matrix as possible. This causes the embedding vectors of
distorted versions of a sample to be similar, while minimizing the redundancy
between the components of these vectors. The method is called Barlow Twins,
owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a
pair of identical networks. Barlow Twins does not require large batches nor
asymmetry between the network twins such as a predictor network, gradient
stopping, or a moving average on the weight updates. Intriguingly it benefits
from very high-dimensional output vectors. Barlow Twins outperforms previous
methods on ImageNet for semi-supervised classification in the low-data regime,
and is on par with current state of the art for ImageNet classification with a
linear classifier head, and for transfer tasks of classification and object
detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zbontar_J/0/1/0/all/0/1"&gt;Jure Zbontar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1"&gt;Li Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1"&gt;Ishan Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1"&gt;Yann LeCun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deny_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Deny&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Embeddings for Cross-Modal Retrieval. (arXiv:2101.05068v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05068</id>
        <link href="http://arxiv.org/abs/2101.05068"/>
        <updated>2021-06-15T01:45:15.867Z</updated>
        <summary type="html"><![CDATA[Cross-modal retrieval methods build a common representation space for samples
from multiple modalities, typically from the vision and the language domains.
For images and their captions, the multiplicity of the correspondences makes
the task particularly challenging. Given an image (respectively a caption),
there are multiple captions (respectively images) that equally make sense. In
this paper, we argue that deterministic functions are not sufficiently powerful
to capture such one-to-many correspondences. Instead, we propose to use
Probabilistic Cross-Modal Embedding (PCME), where samples from the different
modalities are represented as probabilistic distributions in the common
embedding space. Since common benchmarks such as COCO suffer from
non-exhaustive annotations for cross-modal matches, we propose to additionally
evaluate retrieval on the CUB dataset, a smaller yet clean database where all
possible image-caption pairs are annotated. We extensively ablate PCME and
demonstrate that it not only improves the retrieval performance over its
deterministic counterpart but also provides uncertainty estimates that render
the embeddings more interpretable. Code is available at
https://github.com/naver-ai/pcme]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1"&gt;Sanghyuk Chun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Seong Joon Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezende_R/0/1/0/all/0/1"&gt;Rafael Sampaio de Rezende&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalantidis_Y/0/1/0/all/0/1"&gt;Yannis Kalantidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larlus_D/0/1/0/all/0/1"&gt;Diane Larlus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Obfuscation of Graph Neural Networks. (arXiv:2009.13504v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13504</id>
        <link href="http://arxiv.org/abs/2009.13504"/>
        <updated>2021-06-15T01:45:15.855Z</updated>
        <summary type="html"><![CDATA[While the advent of Graph Neural Networks (GNNs) has greatly improved node
and graph representation learning in many applications, the neighborhood
aggregation scheme exposes additional vulnerabilities to adversaries seeking to
extract node-level information about sensitive attributes. In this paper, we
study the problem of protecting sensitive attributes by information obfuscation
when learning with graph structured data. We propose a framework to locally
filter out pre-determined sensitive attributes via adversarial training with
the total variation and the Wasserstein distance. Our method creates a strong
defense against inference attacks, while only suffering small loss in task
performance. Theoretically, we analyze the effectiveness of our framework
against a worst-case adversary, and characterize an inherent trade-off between
maximizing predictive accuracy and minimizing information leakage. Experiments
across multiple datasets from recommender systems, knowledge graphs and quantum
chemistry demonstrate that the proposed approach provides a robust defense
across various graph structures and tasks, while producing competitive GNN
encoders for downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_P/0/1/0/all/0/1"&gt;Peiyuan Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Keyulu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1"&gt;Tommi Jaakkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1"&gt;Geoffrey Gordon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1"&gt;Stefanie Jegelka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Case Study on Detecting COVID-19 Health-Related Misinformation in Social Media. (arXiv:2106.06811v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.06811</id>
        <link href="http://arxiv.org/abs/2106.06811"/>
        <updated>2021-06-15T01:45:15.849Z</updated>
        <summary type="html"><![CDATA[COVID-19 pandemic has generated what public health officials called an
infodemic of misinformation. As social distancing and stay-at-home orders came
into effect, many turned to social media for socializing. This increase in
social media usage has made it a prime vehicle for the spreading of
misinformation. This paper presents a mechanism to detect COVID-19
health-related misinformation in social media following an interdisciplinary
approach. Leveraging social psychology as a foundation and existing
misinformation frameworks, we defined misinformation themes and associated
keywords incorporated into the misinformation detection mechanism using applied
machine learning techniques. Next, using the Twitter dataset, we explored the
performance of the proposed methodology using multiple state-of-the-art machine
learning classifiers. Our method shows promising results with at most 78%
accuracy in classifying health-related misinformation versus true information
using uni-gram-based NLP feature generations from tweets and the Decision Tree
classifier. We also provide suggestions on alternatives for countering
misinformation and ethical consideration for the study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pritom_M/0/1/0/all/0/1"&gt;Mir Mehedi A. Pritom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_R/0/1/0/all/0/1"&gt;Rosana Montanez Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Asad Ali Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_S/0/1/0/all/0/1"&gt;Sebastian A. Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alrashydah_E/0/1/0/all/0/1"&gt;Esra&amp;#x27;a Alrashydah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_B/0/1/0/all/0/1"&gt;Beatrice N. Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rios_A/0/1/0/all/0/1"&gt;Anthony Rios&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prompting Contrastive Explanations for Commonsense Reasoning Tasks. (arXiv:2106.06823v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06823</id>
        <link href="http://arxiv.org/abs/2106.06823"/>
        <updated>2021-06-15T01:45:15.842Z</updated>
        <summary type="html"><![CDATA[Many commonsense reasoning NLP tasks involve choosing between one or more
possible answers to a question or prompt based on knowledge that is often
implicit. Large pretrained language models (PLMs) can achieve near-human
performance on such tasks, while providing little human-interpretable evidence
of the underlying reasoning they use. In this work, we show how to use these
same models to generate such evidence: inspired by the contrastive nature of
human explanations, we use PLMs to complete explanation prompts which contrast
alternatives according to the key attribute(s) required to justify the correct
answer (for example, peanuts are usually salty while raisins are sweet).
Conditioning model decisions on these explanations improves performance on two
commonsense reasoning benchmarks, as compared to previous non-contrastive
alternatives. These explanations are also judged by humans to be more relevant
for solving the task, and facilitate a novel method to evaluate explanation
faithfulfness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paranjape_B/0/1/0/all/0/1"&gt;Bhargavi Paranjape&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1"&gt;Julian Michael&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1"&gt;Marjan Ghazvininejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1"&gt;Hannaneh Hajishirzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NDPNet: A novel non-linear data projection network for few-shot fine-gained image classification. (arXiv:2106.06988v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06988</id>
        <link href="http://arxiv.org/abs/2106.06988"/>
        <updated>2021-06-15T01:45:15.826Z</updated>
        <summary type="html"><![CDATA[Metric-based few-shot fine-grained image classification (FSFGIC) aims to
learn a transferable feature embedding network by estimating the similarities
between query images and support classes from very few examples. In this work,
we propose, for the first time, to introduce the non-linear data projection
concept into the design of FSFGIC architecture in order to address the limited
sample problem in few-shot learning and at the same time to increase the
discriminability of the model for fine-grained image classification.
Specifically, we first design a feature re-abstraction embedding network that
has the ability to not only obtain the required semantic features for effective
metric learning but also re-enhance such features with finer details from input
images. Then the descriptors of the query images and the support classes are
projected into different non-linear spaces in our proposed similarity metric
learning network to learn discriminative projection factors. This design can
effectively operate in the challenging and restricted condition of a FSFGIC
task for making the distance between the samples within the same class smaller
and the distance between samples from different classes larger and for reducing
the coupling relationship between samples from different categories.
Furthermore, a novel similarity measure based on the proposed non-linear data
project is presented for evaluating the relationships of feature information
between a query image and a support set. It is worth to note that our proposed
architecture can be easily embedded into any episodic training mechanisms for
end-to-end training from scratch. Extensive experiments on FSFGIC tasks
demonstrate the superiority of the proposed methods over the state-of-the-art
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhangy_W/0/1/0/all/0/1"&gt;Weichuan Zhangy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liuy_X/0/1/0/all/0/1"&gt;Xuefang Liuy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zhe Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yongsheng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changming Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FPT Approximation for Socially Fair Clustering. (arXiv:2106.06755v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.06755</id>
        <link href="http://arxiv.org/abs/2106.06755"/>
        <updated>2021-06-15T01:45:15.819Z</updated>
        <summary type="html"><![CDATA[In this work, we study the socially fair $k$-median/$k$-means problem. We are
given a set of points $P$ in a metric space $\mathcal{X}$ with a distance
function $d(.,.)$. There are $\ell$ groups: $P_1,\dotsc,P_{\ell} \subseteq P$.
We are also given a set $F$ of feasible centers in $\mathcal{X}$. The goal of
the socially fair $k$-median problem is to find a set $C \subseteq F$ of $k$
centers that minimizes the maximum average cost over all the groups. That is,
find $C$ that minimizes the objective function $\Phi(C,P) \equiv \max_{j}
\sum_{x \in P_j} d(C,x)/|P_j|$, where $d(C,x)$ is the distance of $x$ to the
closest center in $C$. The socially fair $k$-means problem is defined similarly
by using squared distances, i.e., $d^{2}(.,.)$ instead of $d(.,.)$. In this
work, we design $(5+\varepsilon)$ and $(33 + \varepsilon)$ approximation
algorithms for the socially fair $k$-median and $k$-means problems,
respectively. For the parameters: $k$ and $\ell$, the algorithms have an FPT
(fixed parameter tractable) running time of $f(k,\ell,\varepsilon) \cdot n$ for
$f(k,\ell,\varepsilon) = 2^{{O}(k \, \ell/\varepsilon)}$ and $n = |P \cup F|$.
We also study a special case of the problem where the centers are allowed to be
chosen from the point set $P$, i.e., $P \subseteq F$. For this special case,
our algorithms give better approximation guarantees of $(4+\varepsilon)$ and
$(18+\varepsilon)$ for the socially fair $k$-median and $k$-means problems,
respectively. Furthermore, we convert these algorithms to constant pass
log-space streaming algorithms. Lastly, we show FPT hardness of approximation
results for the problem with a small gap between our upper and lower bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_D/0/1/0/all/0/1"&gt;Dishant Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaiswal_R/0/1/0/all/0/1"&gt;Ragesh Jaiswal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Path Features and Neural Path Kernel : Understanding the role of gates in deep learning. (arXiv:2006.10529v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10529</id>
        <link href="http://arxiv.org/abs/2006.10529"/>
        <updated>2021-06-15T01:45:15.813Z</updated>
        <summary type="html"><![CDATA[Rectified linear unit (ReLU) activations can also be thought of as 'gates',
which, either pass or stop their pre-activation input when they are 'on' (when
the pre-activation input is positive) or 'off' (when the pre-activation input
is negative) respectively. A deep neural network (DNN) with ReLU activations
has many gates, and the on/off status of each gate changes across input
examples as well as network weights. For a given input example, only a subset
of gates are 'active', i.e., on, and the sub-network of weights connected to
these active gates is responsible for producing the output. At randomised
initialisation, the active sub-network corresponding to a given input example
is random. During training, as the weights are learnt, the active sub-networks
are also learnt, and potentially hold very valuable information. In this paper,
we analytically characterise the role of active sub-networks in deep learning.
To this end, we encode the on/off state of the gates of a given input in a
novel 'neural path feature' (NPF), and the weights of the DNN are encoded in a
novel 'neural path value' (NPV). Further, we show that the output of network is
indeed the inner product of NPF and NPV. The main result of the paper shows
that the 'neural path kernel' associated with the NPF is a fundamental quantity
that characterises the information stored in the gates of a DNN. We show via
experiments (on MNIST and CIFAR-10) that in standard DNNs with ReLU activations
NPFs are learnt during training and such learning is key for generalisation.
Furthermore, NPFs and NPVs can be learnt in two separate networks and such
learning also generalises well in experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_C/0/1/0/all/0/1"&gt;Chandrashekar Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Amit Vikram Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GENIE: A Leaderboard for Human-in-the-Loop Evaluation of Text Generation. (arXiv:2101.06561v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06561</id>
        <link href="http://arxiv.org/abs/2101.06561"/>
        <updated>2021-06-15T01:45:15.806Z</updated>
        <summary type="html"><![CDATA[Leaderboards have eased model development for many NLP datasets by
standardizing their evaluation and delegating it to an independent external
repository. Their adoption, however, is so far limited to tasks that can be
reliably evaluated in an automatic manner. This work introduces GENIE, an
extensible human evaluation leaderboard, which brings the ease of leaderboards
to text generation tasks. GENIE automatically posts leaderboard submissions to
crowdsourcing platforms asking human annotators to evaluate them on various
axes (e.g., correctness, conciseness, fluency) and compares their answers to
various automatic metrics. We introduce several datasets in English to GENIE,
representing four core challenges in text generation: machine translation,
summarization, commonsense reasoning, and machine comprehension. We provide
formal granular evaluation metrics and identify areas for future research. We
make GENIE publicly available and hope that it will spur progress in language
generation models as well as their automatic and manual evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1"&gt;Daniel Khashabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1"&gt;Gabriel Stanovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1"&gt;Jonathan Bragg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lourie_N/0/1/0/all/0/1"&gt;Nicholas Lourie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1"&gt;Jungo Kasai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1"&gt;Daniel S. Weld&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability. (arXiv:2006.14512v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14512</id>
        <link href="http://arxiv.org/abs/2006.14512"/>
        <updated>2021-06-15T01:45:15.792Z</updated>
        <summary type="html"><![CDATA[Knowledge transferability, or transfer learning, has been widely adopted to
allow a pre-trained model in the source domain to be effectively adapted to
downstream tasks in the target domain. It is thus important to explore and
understand the factors affecting knowledge transferability. In this paper, as
the first work, we analyze and demonstrate the connections between knowledge
transferability and another important phenomenon--adversarial transferability,
\emph{i.e.}, adversarial examples generated against one model can be
transferred to attack other models. Our theoretical studies show that
adversarial transferability indicates knowledge transferability and vice versa.
Moreover, based on the theoretical insights, we propose two practical
adversarial transferability metrics to characterize this process, serving as
bidirectional indicators between adversarial and knowledge transferability. We
conduct extensive experiments for different scenarios on diverse datasets,
showing a positive correlation between adversarial transferability and
knowledge transferability. Our findings will shed light on future research
about effective knowledge transfer learning and adversarial transferability
analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1"&gt;Kaizhao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jacky Y. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1"&gt;Oluwasanmi Koyejo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction with Unpredictable Feature Evolution. (arXiv:1904.12171v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.12171</id>
        <link href="http://arxiv.org/abs/1904.12171"/>
        <updated>2021-06-15T01:45:15.780Z</updated>
        <summary type="html"><![CDATA[Learning with feature evolution studies the scenario where the features of
the data streams can evolve, i.e., old features vanish and new features emerge.
Its goal is to keep the model always performing well even when the features
happen to evolve. To tackle this problem, canonical methods assume that the old
features will vanish simultaneously and the new features themselves will emerge
simultaneously as well. They also assume there is an overlapping period where
old and new features both exist when the feature space starts to change.
However, in reality, the feature evolution could be unpredictable, which means
the features can vanish or emerge arbitrarily, causing the overlapping period
incomplete. In this paper, we propose a novel paradigm: Prediction with
Unpredictable Feature Evolution (PUFE) where the feature evolution is
unpredictable. To address this problem, we fill the incomplete overlapping
period and formulate it as a new matrix completion problem. We give a
theoretical bound on the least number of observed entries to make the
overlapping period intact. With this intact overlapping period, we leverage an
ensemble method to take the advantage of both the old and new feature spaces
without manually deciding which base models should be incorporated. Theoretical
and experimental results validate that our method can always follow the best
base models and thus realize the goal of learning with feature evolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1"&gt;Bo-Jian Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhi-Hua Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DONet: Dual-Octave Network for Fast MR Image Reconstruction. (arXiv:2105.05980v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05980</id>
        <link href="http://arxiv.org/abs/2105.05980"/>
        <updated>2021-06-15T01:45:15.761Z</updated>
        <summary type="html"><![CDATA[Magnetic resonance (MR) image acquisition is an inherently prolonged process,
whose acceleration has long been the subject of research. This is commonly
achieved by obtaining multiple undersampled images, simultaneously, through
parallel imaging. In this paper, we propose the Dual-Octave Network (DONet),
which is capable of learning multi-scale spatial-frequency features from both
the real and imaginary components of MR data, for fast parallel MR image
reconstruction. More specifically, our DONet consists of a series of
Dual-Octave convolutions (Dual-OctConv), which are connected in a dense manner
for better reuse of features. In each Dual-OctConv, the input feature maps and
convolutional kernels are first split into two components (ie, real and
imaginary), and then divided into four groups according to their spatial
frequencies. Then, our Dual-OctConv conducts intra-group information updating
and inter-group information exchange to aggregate the contextual information
across different groups. Our framework provides three appealing benefits: (i)
It encourages information interaction and fusion between the real and imaginary
components at various spatial frequencies to achieve richer representational
capacity. (ii) The dense connections between the real and imaginary groups in
each Dual-OctConv make the propagation of features more efficient by feature
reuse. (iii) DONet enlarges the receptive field by learning multiple
spatial-frequency features of both the real and imaginary components. Extensive
experiments on two popular datasets (ie, clinical knee and fastMRI), under
different undersampling patterns and acceleration factors, demonstrate the
superiority of our model in accelerated parallel MR image reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chun-Mei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhanyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transferability of Spectral Graph Convolutional Neural Networks. (arXiv:1907.12972v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.12972</id>
        <link href="http://arxiv.org/abs/1907.12972"/>
        <updated>2021-06-15T01:45:15.743Z</updated>
        <summary type="html"><![CDATA[This paper focuses on spectral graph convolutional neural networks
(ConvNets), where filters are defined as elementwise multiplication in the
frequency domain of a graph. In machine learning settings where the dataset
consists of signals defined on many different graphs, the trained ConvNet
should generalize to signals on graphs unseen in the training set. It is thus
important to transfer ConvNets between graphs. Transferability, which is a
certain type of generalization capability, can be loosely defined as follows:
if two graphs describe the same phenomenon, then a single filter or ConvNet
should have similar repercussions on both graphs. This paper aims at debunking
the common misconception that spectral filters are not transferable. We show
that if two graphs discretize the same "continuous" space, then a spectral
filter or ConvNet has approximately the same repercussion on both graphs. Our
analysis is more permissive than the standard analysis. Transferability is
typically described as the robustness of the filter to small graph
perturbations and re-indexing of the vertices. Our analysis accounts also for
large graph perturbations. We prove transferability between graphs that can
have completely different dimensions and topologies, only requiring that both
graphs discretize the same underlying space in some generic sense.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levie_R/0/1/0/all/0/1"&gt;Ron Levie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bucci_L/0/1/0/all/0/1"&gt;Lorenzo Bucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1"&gt;Michael M. Bronstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1"&gt;Gitta Kutyniok&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disrupting Model Training with Adversarial Shortcuts. (arXiv:2106.06654v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06654</id>
        <link href="http://arxiv.org/abs/2106.06654"/>
        <updated>2021-06-15T01:45:15.737Z</updated>
        <summary type="html"><![CDATA[When data is publicly released for human consumption, it is unclear how to
prevent its unauthorized usage for machine learning purposes. Successful model
training may be preventable with carefully designed dataset modifications, and
we present a proof-of-concept approach for the image classification setting. We
propose methods based on the notion of adversarial shortcuts, which encourage
models to rely on non-robust signals rather than semantic features, and our
experiments demonstrate that these measures successfully prevent deep learning
models from achieving high accuracy on real, unmodified data examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1"&gt;Ivan Evtimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Covert_I/0/1/0/all/0/1"&gt;Ian Covert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1"&gt;Aditya Kusupati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohno_T/0/1/0/all/0/1"&gt;Tadayoshi Kohno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Shuffling Framework for Local Differential Privacy. (arXiv:2106.06603v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06603</id>
        <link href="http://arxiv.org/abs/2106.06603"/>
        <updated>2021-06-15T01:45:15.722Z</updated>
        <summary type="html"><![CDATA[ldp deployments are vulnerable to inference attacks as an adversary can link
the noisy responses to their identity and subsequently, auxiliary information
using the order of the data. An alternative model, shuffle DP, prevents this by
shuffling the noisy responses uniformly at random. However, this limits the
data learnability -- only symmetric functions (input order agnostic) can be
learned. In this paper, we strike a balance and propose a generalized shuffling
framework that interpolates between the two deployment models. We show that
systematic shuffling of the noisy responses can thwart specific inference
attacks while retaining some meaningful data learnability. To this end, we
propose a novel privacy guarantee, d-sigma privacy, that captures the privacy
of the order of a data sequence. d-sigma privacy allows tuning the granularity
at which the ordinal information is maintained, which formalizes the degree the
resistance to inference attacks trading it off with data learnability.
Additionally, we propose a novel shuffling mechanism that can achieve d-sigma
privacy and demonstrate the practicality of our mechanism via evaluation on
real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meehan_C/0/1/0/all/0/1"&gt;Casey Meehan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1"&gt;Amrita Roy Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1"&gt;Kamalika Chaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On The Radon-Nikodym Spectral Approach With Optimal Clustering. (arXiv:1906.00460v16 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.00460</id>
        <link href="http://arxiv.org/abs/1906.00460"/>
        <updated>2021-06-15T01:45:15.716Z</updated>
        <summary type="html"><![CDATA[Problems of interpolation, classification, and clustering are considered. In
the tenets of Radon--Nikodym approach $\langle f(\mathbf{x})\psi^2 \rangle /
\langle\psi^2\rangle$, where the $\psi(\mathbf{x})$ is a linear function on
input attributes, all the answers are obtained from a generalized eigenproblem
$|f|\psi^{[i]}\rangle = \lambda^{[i]} |\psi^{[i]}\rangle$. The solution to the
interpolation problem is a regular Radon-Nikodym derivative. The solution to
the classification problem requires prior and posterior probabilities that are
obtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian
approach new observations change only outcome probabilities, in the
Radon-Nikodym approach not only outcome probabilities but also the probability
space $|\psi^{[i]}\rangle$ change with new observations. This is a remarkable
feature of the approach: both the probabilities and the probability space are
constructed from the data. The Lebesgue quadrature technique can be also
applied to the optimal clustering problem. The problem is solved by
constructing a Gaussian quadrature on the Lebesgue measure. A distinguishing
feature of the Radon-Nikodym approach is the knowledge of the invariant group:
all the answers are invariant relatively any non-degenerated linear transform
of input vector $\mathbf{x}$ components. A software product implementing the
algorithms of interpolation, classification, and optimal clustering is
available from the authors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malyshkin_V/0/1/0/all/0/1"&gt;Vladislav Gennadievich Malyshkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NDPNet: A novel non-linear data projection network for few-shot fine-gained image classification. (arXiv:2106.06988v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06988</id>
        <link href="http://arxiv.org/abs/2106.06988"/>
        <updated>2021-06-15T01:45:15.709Z</updated>
        <summary type="html"><![CDATA[Metric-based few-shot fine-grained image classification (FSFGIC) aims to
learn a transferable feature embedding network by estimating the similarities
between query images and support classes from very few examples. In this work,
we propose, for the first time, to introduce the non-linear data projection
concept into the design of FSFGIC architecture in order to address the limited
sample problem in few-shot learning and at the same time to increase the
discriminability of the model for fine-grained image classification.
Specifically, we first design a feature re-abstraction embedding network that
has the ability to not only obtain the required semantic features for effective
metric learning but also re-enhance such features with finer details from input
images. Then the descriptors of the query images and the support classes are
projected into different non-linear spaces in our proposed similarity metric
learning network to learn discriminative projection factors. This design can
effectively operate in the challenging and restricted condition of a FSFGIC
task for making the distance between the samples within the same class smaller
and the distance between samples from different classes larger and for reducing
the coupling relationship between samples from different categories.
Furthermore, a novel similarity measure based on the proposed non-linear data
project is presented for evaluating the relationships of feature information
between a query image and a support set. It is worth to note that our proposed
architecture can be easily embedded into any episodic training mechanisms for
end-to-end training from scratch. Extensive experiments on FSFGIC tasks
demonstrate the superiority of the proposed methods over the state-of-the-art
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhangy_W/0/1/0/all/0/1"&gt;Weichuan Zhangy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liuy_X/0/1/0/all/0/1"&gt;Xuefang Liuy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zhe Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yongsheng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changming Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Clone Transformer for Efficient Convolutional Neural Netwoks. (arXiv:2106.06778v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06778</id>
        <link href="http://arxiv.org/abs/2106.06778"/>
        <updated>2021-06-15T01:45:15.703Z</updated>
        <summary type="html"><![CDATA[Convolutional networks (ConvNets) have shown impressive capability to solve
various vision tasks. Nevertheless, the trade-off between performance and
efficiency is still a challenge for a feasible model deployment on
resource-constrained platforms. In this paper, we introduce a novel concept
termed multi-path fully connected pattern (MPFC) to rethink the
interdependencies of topology pattern, accuracy and efficiency for ConvNets.
Inspired by MPFC, we further propose a dual-branch module named dynamic clone
transformer (DCT) where one branch generates multiple replicas from inputs and
another branch reforms those clones through a series of difference vectors
conditional on inputs itself to produce more variants. This operation allows
the self-expansion of channel-wise information in a data-driven way with little
computational cost while providing sufficient learning capacity, which is a
potential unit to replace computationally expensive pointwise convolution as an
expansion layer in the bottleneck structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1"&gt;Longqing Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expected Tight Bounds for Robust Training. (arXiv:1905.12418v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.12418</id>
        <link href="http://arxiv.org/abs/1905.12418"/>
        <updated>2021-06-15T01:45:15.697Z</updated>
        <summary type="html"><![CDATA[Training Deep Neural Networks that are robust to norm bounded adversarial
attacks remains an elusive problem. While exact and inexact verification-based
methods are generally too expensive to train large networks, it was
demonstrated that bounded input intervals can be inexpensively propagated from
a layer to another through deep networks. This interval bound propagation
approach (IBP) not only has improved both robustness and certified accuracy but
was the first to be employed on large/deep networks. However, due to the very
loose nature of the IBP bounds, the required training procedure is complex and
involved. In this paper, we closely examine the bounds of a block of layers
composed in the form of Affine-ReLU-Affine. To this end, we propose expected
tight bounds (true bounds in expectation), referred to as ETB, which are
provably tighter than IBP bounds in expectation. We then extend this result to
deeper networks through blockwise propagation and show that we can achieve
orders of magnitudes tighter bounds compared to IBP. Furthermore, using a
simple standard training procedure, we can achieve impressive
robustness-accuracy trade-off on both MNIST and CIFAR10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alsubaihi_S/0/1/0/all/0/1"&gt;Salman Alsubaihi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1"&gt;Adel Bibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfadly_M/0/1/0/all/0/1"&gt;Modar Alfadly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1"&gt;Abdullah Hamdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning. (arXiv:2105.15134v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15134</id>
        <link href="http://arxiv.org/abs/2105.15134"/>
        <updated>2021-06-15T01:45:15.691Z</updated>
        <summary type="html"><![CDATA[How can neural networks trained by contrastive learning extract features from
the unlabeled data? Why does contrastive learning usually need much stronger
data augmentations than supervised learning to ensure good representations?
These questions involve both the optimization and statistical aspects of deep
learning, but can hardly be answered by analyzing supervised learning, where
the target functions are the highest pursuit. Indeed, in self-supervised
learning, it is inevitable to relate to the optimization/generalization of
neural networks to how they can encode the latent structures in the data, which
we refer to as the feature learning process.

In this work, we formally study how contrastive learning learns the feature
representations for neural networks by analyzing its feature learning process.
We consider the case where our data are comprised of two types of features: the
more semantically aligned sparse features which we want to learn from, and the
other dense features we want to avoid. Theoretically, we prove that contrastive
learning using $\mathbf{ReLU}$ networks provably learns the desired sparse
features if proper augmentations are adopted. We present an underlying
principle called $\textbf{feature decoupling}$ to explain the effects of
augmentations, where we theoretically characterize how augmentations can reduce
the correlations of dense features between positive samples while keeping the
correlations of sparse features intact, thereby forcing the neural networks to
learn from the self-supervision of sparse features. Empirically, we verified
that the feature decoupling principle matches the underlying mechanism of
contrastive learning in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1"&gt;Zixin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-supervised Graph Meta-learning for Few-shot Node Classification. (arXiv:2106.06873v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06873</id>
        <link href="http://arxiv.org/abs/2106.06873"/>
        <updated>2021-06-15T01:45:15.684Z</updated>
        <summary type="html"><![CDATA[Graphs are widely used to model the relational structure of data, and the
research of graph machine learning (ML) has a wide spectrum of applications
ranging from drug design in molecular graphs to friendship recommendation in
social networks. Prevailing approaches for graph ML typically require abundant
labeled instances in achieving satisfactory results, which is commonly
infeasible in real-world scenarios since labeled data for newly emerged
concepts (e.g., new categorizations of nodes) on graphs is limited. Though
meta-learning has been applied to different few-shot graph learning problems,
most existing efforts predominately assume that all the data from those seen
classes is gold-labeled, while those methods may lose their efficacy when the
seen data is weakly-labeled with severe label noise. As such, we aim to
investigate a novel problem of weakly-supervised graph meta-learning for
improving the model robustness in terms of knowledge transfer. To achieve this
goal, we propose a new graph meta-learning framework -- Graph Hallucination
Networks (Meta-GHN) in this paper. Based on a new robustness-enhanced episodic
training, Meta-GHN is meta-learned to hallucinate clean node representations
from weakly-labeled data and extracts highly transferable meta-knowledge, which
enables the model to quickly adapt to unseen tasks with few labeled instances.
Extensive experiments demonstrate the superiority of Meta-GHN over existing
graph meta-learning studies on the task of weakly-supervised few-shot node
classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1"&gt;Kaize Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianling Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jundong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1"&gt;James Caverlee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Counterfactual Explanations in Tree Ensembles. (arXiv:2106.06631v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06631</id>
        <link href="http://arxiv.org/abs/2106.06631"/>
        <updated>2021-06-15T01:45:15.678Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations are usually generated through heuristics that are
sensitive to the search's initial conditions. The absence of guarantees of
performance and robustness hinders trustworthiness. In this paper, we take a
disciplined approach towards counterfactual explanations for tree ensembles. We
advocate for a model-based search aiming at "optimal" explanations and propose
efficient mixed-integer programming approaches. We show that isolation forests
can be modeled within our framework to focus the search on plausible
explanations with a low outlier score. We provide comprehensive coverage of
additional constraints that model important objectives, heterogeneous data
types, structural constraints on the feature space, along with resource and
actionability restrictions. Our experimental analyses demonstrate that the
proposed search approach requires a computational effort that is orders of
magnitude smaller than previous mathematical programming algorithms. It scales
up to large data sets and tree ensembles, where it provides, within seconds,
systematic explanations grounded on well-defined models solved to optimality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parmentier_A/0/1/0/all/0/1"&gt;Axel Parmentier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_T/0/1/0/all/0/1"&gt;Thibaut Vidal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalars are universal: Gauge-equivariant machine learning, structured like classical physics. (arXiv:2106.06610v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06610</id>
        <link href="http://arxiv.org/abs/2106.06610"/>
        <updated>2021-06-15T01:45:15.671Z</updated>
        <summary type="html"><![CDATA[There has been enormous progress in the last few years in designing
conceivable (though not always practical) neural networks that respect the
gauge symmetries -- or coordinate freedom -- of physical law. Some of these
frameworks make use of irreducible representations, some make use of higher
order tensor objects, and some apply symmetry-enforcing constraints. Different
physical laws obey different combinations of fundamental symmetries, but a
large fraction (possibly all) of classical physics is equivariant to
translation, rotation, reflection (parity), boost (relativity), and
permutations. Here we show that it is simple to parameterize universally
approximating polynomial functions that are equivariant under these symmetries,
or under the Euclidean, Lorentz, and Poincar\'e groups, at any dimensionality
$d$. The key observation is that nonlinear O($d$)-equivariant (and
related-group-equivariant) functions can be expressed in terms of a lightweight
collection of scalars -- scalar products and scalar contractions of the scalar,
vector, and tensor inputs. These results demonstrate theoretically that
gauge-invariant deep learning models for classical physics with good scaling
for large problems are feasible right now.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Villar_S/0/1/0/all/0/1"&gt;Soledad Villar&lt;/a&gt; (JHU), &lt;a href="http://arxiv.org/find/cs/1/au:+Hogg_D/0/1/0/all/0/1"&gt;David W.Hogg&lt;/a&gt; (Flatiron, NYU), &lt;a href="http://arxiv.org/find/cs/1/au:+Storey_Fisher_K/0/1/0/all/0/1"&gt;Kate Storey-Fisher&lt;/a&gt; (NYU), &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1"&gt;Weichi Yao&lt;/a&gt; (NYU), &lt;a href="http://arxiv.org/find/cs/1/au:+Blum_Smith_B/0/1/0/all/0/1"&gt;Ben Blum-Smith&lt;/a&gt; (NYU)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ATRAS: Adversarially Trained Robust Architecture Search. (arXiv:2106.06917v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06917</id>
        <link href="http://arxiv.org/abs/2106.06917"/>
        <updated>2021-06-15T01:45:15.638Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore the effect of architecture completeness on
adversarial robustness. We train models with different architectures on
CIFAR-10 and MNIST dataset. For each model, we vary different number of layers
and different number of nodes in the layer. For every architecture candidate,
we use Fast Gradient Sign Method (FGSM) to generate untargeted adversarial
attacks and use adversarial training to defend against those attacks. For each
architecture candidate, we report pre-attack, post-attack and post-defense
accuracy for the model as well as the architecture parameters and the impact of
completeness to the model accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alparslan_Y/0/1/0/all/0/1"&gt;Yigit Alparslan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1"&gt;Edward Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doubly Non-Central Beta Matrix Factorization for DNA Methylation Data. (arXiv:2106.06691v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06691</id>
        <link href="http://arxiv.org/abs/2106.06691"/>
        <updated>2021-06-15T01:45:15.626Z</updated>
        <summary type="html"><![CDATA[We present a new non-negative matrix factorization model for $(0,1)$
bounded-support data based on the doubly non-central beta (DNCB) distribution,
a generalization of the beta distribution. The expressiveness of the DNCB
distribution is particularly useful for modeling DNA methylation datasets,
which are typically highly dispersed and multi-modal; however, the model
structure is sufficiently general that it can be adapted to many other domains
where latent representations of $(0,1)$ bounded-support data are of interest.
Although the DNCB distribution lacks a closed-form conjugate prior, several
augmentations let us derive an efficient posterior inference algorithm composed
entirely of analytic updates. Our model improves out-of-sample predictive
performance on both real and synthetic DNA methylation datasets over
state-of-the-art methods in bioinformatics. In addition, our model yields
meaningful latent representations that accord with existing biological
knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Schein_A/0/1/0/all/0/1"&gt;Aaron Schein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nagulpally_A/0/1/0/all/0/1"&gt;Anjali Nagulpally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wallach_H/0/1/0/all/0/1"&gt;Hanna Wallach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Flaherty_P/0/1/0/all/0/1"&gt;Patrick Flaherty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Shuffling Beats SGD Only After Many Epochs on Ill-Conditioned Problems. (arXiv:2106.06880v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06880</id>
        <link href="http://arxiv.org/abs/2106.06880"/>
        <updated>2021-06-15T01:45:15.617Z</updated>
        <summary type="html"><![CDATA[Recently, there has been much interest in studying the convergence rates of
without-replacement SGD, and proving that it is faster than with-replacement
SGD in the worst case. However, these works ignore or do not provide tight
bounds in terms of the problem's geometry, including its condition number.
Perhaps surprisingly, we prove that when the condition number is taken into
account, without-replacement SGD \emph{does not} significantly improve on
with-replacement SGD in terms of worst-case bounds, unless the number of epochs
(passes over the data) is larger than the condition number. Since many problems
in machine learning and other areas are both ill-conditioned and involve large
datasets, this indicates that without-replacement does not necessarily improve
over with-replacement sampling for realistic iteration budgets. We show this by
providing new lower and upper bounds which are tight (up to log factors), for
quadratic problems with commuting quadratic terms, precisely quantifying the
dependence on the problem parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safran_I/0/1/0/all/0/1"&gt;Itay Safran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1"&gt;Ohad Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D2C: Diffusion-Denoising Models for Few-shot Conditional Generation. (arXiv:2106.06819v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06819</id>
        <link href="http://arxiv.org/abs/2106.06819"/>
        <updated>2021-06-15T01:45:15.596Z</updated>
        <summary type="html"><![CDATA[Conditional generative models of high-dimensional images have many
applications, but supervision signals from conditions to images can be
expensive to acquire. This paper describes Diffusion-Decoding models with
Contrastive representations (D2C), a paradigm for training unconditional
variational autoencoders (VAEs) for few-shot conditional image generation. D2C
uses a learned diffusion-based prior over the latent representations to improve
generation and contrastive self-supervised learning to improve representation
quality. D2C can adapt to novel generation tasks conditioned on labels or
manipulation constraints, by learning from as few as 100 labeled examples. On
conditional generation from new labels, D2C achieves superior performance over
state-of-the-art VAEs and diffusion models. On conditional image manipulation,
D2C generations are two orders of magnitude faster to produce over StyleGAN2
ones and are preferred by 50% - 60% of the human evaluators in a double-blind
study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1"&gt;Abhishek Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jiaming Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1"&gt;Chenlin Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Inference Gaussian Process Multiproxy Alignment of Continuous Signals (BIGMACS): Applications for Paleoceanography. (arXiv:1907.08738v4 [stat.AP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.08738</id>
        <link href="http://arxiv.org/abs/1907.08738"/>
        <updated>2021-06-15T01:45:15.577Z</updated>
        <summary type="html"><![CDATA[We first introduce a novel profile-based alignment algorithm, the multiple
continuous Signal Alignment algorithm with Gaussian Process Regression profiles
(SA-GPR). SA-GPR addresses the limitations of currently available signal
alignment methods by adopting a hybrid of the particle smoothing and
Markov-chain Monte Carlo (MCMC) algorithms to align signals, and by applying
the Gaussian process regression to construct profiles to be aligned
continuously. SA-GPR shares all the strengths of the existing alignment
algorithms that depend on profiles but is more exact in the sense that profiles
do not need to be discretized as sequential bins. The uncertainty of
performance over the resolution of such bins is thereby eliminated. This
methodology produces alignments that are consistent, that regularize extreme
cases, and that properly reflect the inherent uncertainty.

Then we extend SA-GPR to a specific problem in the field of paleoceanography
with a method called Bayesian Inference Gaussian Process Multiproxy Alignment
of Continuous Signals (BIGMACS). The goal of BIGMACS is to infer continuous
ages for ocean sediment cores using two classes of age proxies: proxies that
explicitly return calendar ages (e.g., radiocarbon) and those used to
synchronize ages in multiple marine records (e.g., an oxygen isotope based
marine proxy known as benthic ${\delta}^{18}{\rm O}$). BIGMACS integrates these
two proxies by iteratively performing two steps: profile construction from
benthic ${\delta}^{18}{\rm O}$ age models and alignment of each core to the
profile also reflecting radiocarbon dates. We use BIGMACS to construct a new
Deep Northeastern Atlantic stack (i.e., a profile from a particular benthic
${\delta}^{18}{\rm O}$ records) of five ocean sediment cores. We conclude by
constructing multiproxy age models for two additional cores from the same
region by aligning them to the stack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lee_T/0/1/0/all/0/1"&gt;Taehee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lisiecki_L/0/1/0/all/0/1"&gt;Lorraine E. Lisiecki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rand_D/0/1/0/all/0/1"&gt;Devin Rand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gebbie_G/0/1/0/all/0/1"&gt;Geoffrey Gebbie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lawrence_C/0/1/0/all/0/1"&gt;Charles E. Lawrence&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CARTL: Cooperative Adversarially-Robust Transfer Learning. (arXiv:2106.06667v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06667</id>
        <link href="http://arxiv.org/abs/2106.06667"/>
        <updated>2021-06-15T01:45:15.548Z</updated>
        <summary type="html"><![CDATA[Transfer learning eases the burden of training a well-performed model from
scratch, especially when training data is scarce and computation power is
limited. In deep learning, a typical strategy for transfer learning is to
freeze the early layers of a pre-trained model and fine-tune the rest of its
layers on the target domain. Previous work focuses on the accuracy of the
transferred model but neglects the transfer of adversarial robustness. In this
work, we first show that transfer learning improves the accuracy on the target
domain but degrades the inherited robustness of the target model. To address
such a problem, we propose a novel cooperative adversarially-robust transfer
learning (CARTL) by pre-training the model via feature distance minimization
and fine-tuning the pre-trained model with non-expansive fine-tuning for target
domain tasks. Empirical results show that CARTL improves the inherited
robustness by about 28% at most compared with the baseline with the same degree
of accuracy. Furthermore, we study the relationship between the batch
normalization (BN) layers and the robustness in the context of transfer
learning, and we reveal that freezing BN layers can further boost the
robustness transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hongxin Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yinli Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-way Spectrum Pursuit for CUR Decomposition and Its Application in Joint Column/Row Subset Selection. (arXiv:2106.06983v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06983</id>
        <link href="http://arxiv.org/abs/2106.06983"/>
        <updated>2021-06-15T01:45:15.525Z</updated>
        <summary type="html"><![CDATA[The problem of simultaneous column and row subset selection is addressed in
this paper. The column space and row space of a matrix are spanned by its left
and right singular vectors, respectively. However, the singular vectors are not
within actual columns/rows of the matrix. In this paper, an iterative approach
is proposed to capture the most structural information of columns/rows via
selecting a subset of actual columns/rows. This algorithm is referred to as
two-way spectrum pursuit (TWSP) which provides us with an accurate solution for
the CUR matrix decomposition. TWSP is applicable in a wide range of
applications since it enjoys a linear complexity w.r.t. number of original
columns/rows. We demonstrated the application of TWSP for joint channel and
sensor selection in cognitive radio networks, informative users and contents
detection, and efficient supervised data reduction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esmaeili_A/0/1/0/all/0/1"&gt;Ashkan Esmaeili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joneidi_M/0/1/0/all/0/1"&gt;Mohsen Joneidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salimitari_M/0/1/0/all/0/1"&gt;Mehrdad Salimitari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khalid_U/0/1/0/all/0/1"&gt;Umar Khalid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1"&gt;Nazanin Rahnavard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Alternating Direction Method of Multipliers for Byzantine-Robust Distributed Learning. (arXiv:2106.06891v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.06891</id>
        <link href="http://arxiv.org/abs/2106.06891"/>
        <updated>2021-06-15T01:45:15.518Z</updated>
        <summary type="html"><![CDATA[This paper aims to solve a distributed learning problem under Byzantine
attacks. In the underlying distributed system, a number of unknown but
malicious workers (termed as Byzantine workers) can send arbitrary messages to
the master and bias the learning process, due to data corruptions, computation
errors or malicious attacks. Prior work has considered a total variation (TV)
norm-penalized approximation formulation to handle the Byzantine attacks, where
the TV norm penalty forces the regular workers' local variables to be close,
and meanwhile, tolerates the outliers sent by the Byzantine workers. To solve
the TV norm-penalized approximation formulation, we propose a Byzantine-robust
stochastic alternating direction method of multipliers (ADMM) that fully
utilizes the separable problem structure. Theoretically, we prove that the
proposed method converges to a bounded neighborhood of the optimal solution at
a rate of O(1/k) under mild assumptions, where k is the number of iterations
and the size of neighborhood is determined by the number of Byzantine workers.
Numerical experiments on the MNIST and COVERTYPE datasets demonstrate the
effectiveness of the proposed method to various Byzantine attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Lin_F/0/1/0/all/0/1"&gt;Feng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_W/0/1/0/all/0/1"&gt;Weiyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ling_Q/0/1/0/all/0/1"&gt;Qing Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PVRED: A Position-Velocity Recurrent Encoder-Decoder for Human Motion Prediction. (arXiv:1906.06514v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.06514</id>
        <link href="http://arxiv.org/abs/1906.06514"/>
        <updated>2021-06-15T01:45:15.508Z</updated>
        <summary type="html"><![CDATA[Human motion prediction, which aims to predict future human poses given past
poses, has recently seen increased interest. Many recent approaches are based
on Recurrent Neural Networks (RNN) which model human poses with exponential
maps. These approaches neglect the pose velocity as well as temporal relation
of different poses, and tend to converge to the mean pose or fail to generate
natural-looking poses. We therefore propose a novel Position-Velocity Recurrent
Encoder-Decoder (PVRED) for human motion prediction, which makes full use of
pose velocities and temporal positional information. A temporal position
embedding method is presented and a Position-Velocity RNN (PVRNN) is proposed.
We also emphasize the benefits of quaternion parameterization of poses and
design a novel trainable Quaternion Transformation (QT) layer, which is
combined with a robust loss function during training. We provide quantitative
results for both short-term prediction in the future 0.5 seconds and long-term
prediction in the future 0.5 to 1 seconds. Experiments on several benchmarks
show that our approach considerably outperforms the state-of-the-art methods.
In addition, qualitative visualizations in the future 4 seconds show that our
approach could predict future human-like and meaningful poses in very long time
horizons. Code is publicly available on GitHub:
\textcolor{red}{https://github.com/hongsong-wang/PVRNN}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongsong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jian Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1"&gt;Bin Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Consolidation based Class Incremental Online Learning with Limited Data. (arXiv:2106.06795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06795</id>
        <link href="http://arxiv.org/abs/2106.06795"/>
        <updated>2021-06-15T01:45:15.488Z</updated>
        <summary type="html"><![CDATA[We propose a novel approach for class incremental online learning in a
limited data setting. This problem setting is challenging because of the
following constraints: (1) Classes are given incrementally, which necessitates
a class incremental learning approach; (2) Data for each class is given in an
online fashion, i.e., each training example is seen only once during training;
(3) Each class has very few training examples; and (4) We do not use or assume
access to any replay/memory to store data from previous classes. Therefore, in
this setting, we have to handle twofold problems of catastrophic forgetting and
overfitting. In our approach, we learn robust representations that are
generalizable across tasks without suffering from the problems of catastrophic
forgetting and overfitting to accommodate future classes with limited samples.
Our proposed method leverages the meta-learning framework with knowledge
consolidation. The meta-learning framework helps the model for rapid learning
when samples appear in an online fashion. Simultaneously, knowledge
consolidation helps to learn a robust representation against forgetting under
online updates to facilitate future learning. Our approach significantly
outperforms other methods on several benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1"&gt;Mohammed Asad Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1"&gt;Vinay Kumar Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Pravendra Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay Namboodiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1"&gt;Piyush Rai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v2 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.03743</id>
        <link href="http://arxiv.org/abs/2106.03743"/>
        <updated>2021-06-15T01:45:15.459Z</updated>
        <summary type="html"><![CDATA[We investigate the reasons for the performance degradation incurred with
batch-independent normalization. We find that the prototypical techniques of
layer normalization and instance normalization both induce the appearance of
failure modes in the neural network's pre-activations: (i) layer normalization
induces a collapse towards channel-wise constant functions; (ii) instance
normalization induces a lack of variability in instance statistics, symptomatic
of an alteration of the expressivity. To alleviate failure mode (i) without
aggravating failure mode (ii), we introduce the technique "Proxy Normalization"
that normalizes post-activations using a proxy distribution. When combined with
layer normalization or group normalization, this batch-independent
normalization emulates batch normalization's behavior and consistently matches
or exceeds its performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Labatie_A/0/1/0/all/0/1"&gt;Antoine Labatie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masters_D/0/1/0/all/0/1"&gt;Dominic Masters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1"&gt;Zach Eaton-Rosen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1"&gt;Carlo Luschi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Accurate and Realistic Outfits Visualization with Attention to Details. (arXiv:2106.06593v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06593</id>
        <link href="http://arxiv.org/abs/2106.06593"/>
        <updated>2021-06-15T01:45:15.436Z</updated>
        <summary type="html"><![CDATA[Virtual try-on methods aim to generate images of fashion models wearing
arbitrary combinations of garments. This is a challenging task because the
generated image must appear realistic and accurately display the interaction
between garments. Prior works produce images that are filled with artifacts and
fail to capture important visual details necessary for commercial applications.
We propose Outfit Visualization Net (OVNet) to capture these important details
(e.g. buttons, shading, textures, realistic hemlines, and interactions between
garments) and produce high quality multiple-garment virtual try-on images.
OVNet consists of 1) a semantic layout generator and 2) an image generation
pipeline using multiple coordinated warps. We train the warper to output
multiple warps using a cascade loss, which refines each successive warp to
focus on poorly generated regions of a previous warp and yields consistent
improvements in detail. In addition, we introduce a method for matching outfits
with the most suitable model and produce significant improvements for both our
and other previous try-on methods. Through quantitative and qualitative
analysis, we demonstrate our method generates substantially higher-quality
studio images compared to prior works for multi-garment outfits. An interactive
interface powered by this method has been deployed on fashion e-commerce
websites and received overwhelmingly positive feedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kedan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1"&gt;Min jin Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jeffrey Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingen Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Adaptation across Multiway Domains via Representation Learning. (arXiv:2106.06657v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06657</id>
        <link href="http://arxiv.org/abs/2106.06657"/>
        <updated>2021-06-15T01:45:15.425Z</updated>
        <summary type="html"><![CDATA[This paper studies zero-shot domain adaptation where each domain is indexed
on a multi-dimensional array, and we only have data from a small subset of
domains. Our goal is to produce predictors that perform well on \emph{unseen}
domains. We propose a model which consists of a domain-invariant latent
representation layer and a domain-specific linear prediction layer with a
low-rank tensor structure. Theoretically, we present explicit sample complexity
bounds to characterize the prediction error on unseen domains in terms of the
number of domains with training data and the number of data per domain. To our
knowledge, this is the first finite-sample guarantee for zero-shot domain
adaptation. In addition, we provide experiments on two-way MNIST and four-way
fiber sensing datasets to demonstrate the effectiveness of our proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhili Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shaobo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Learning with Class Imbalance. (arXiv:2101.02523v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02523</id>
        <link href="http://arxiv.org/abs/2101.02523"/>
        <updated>2021-06-15T01:45:15.417Z</updated>
        <summary type="html"><![CDATA[Few-Shot Learning (FSL) algorithms are commonly trained through Meta-Learning
(ML), which exposes models to batches of tasks sampled from a meta-dataset to
mimic tasks seen during evaluation. However, the standard training procedures
overlook the real-world dynamics where classes commonly occur at different
frequencies. While it is generally understood that class imbalance harms the
performance of supervised methods, limited research examines the impact of
imbalance on the FSL evaluation task. Our analysis compares 10 state-of-the-art
meta-learning and FSL methods on different imbalance distributions and
rebalancing techniques. Our results reveal that 1) some FSL methods display a
natural disposition against imbalance while most other approaches produce a
performance drop by up to 17\% compared to the balanced task without the
appropriate mitigation; 2) contrary to popular belief, many meta-learning
algorithms will not automatically learn to balance from exposure to imbalanced
training tasks; 3) classical rebalancing strategies, such as random
oversampling, can still be very effective, leading to state-of-the-art
performances and should not be overlooked; 4) FSL methods are more robust
against meta-dataset imbalance than imbalance at the task-level with a similar
imbalance ratio ($\rho<20$), with the effect holding even in long-tail datasets
under a larger imbalance ($\rho=65$).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ochal_M/0/1/0/all/0/1"&gt;Mateusz Ochal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patacchiola_M/0/1/0/all/0/1"&gt;Massimiliano Patacchiola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1"&gt;Amos Storkey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vazquez_J/0/1/0/all/0/1"&gt;Jose Vazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Obfuscation of Graph Neural Networks. (arXiv:2009.13504v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13504</id>
        <link href="http://arxiv.org/abs/2009.13504"/>
        <updated>2021-06-15T01:45:15.403Z</updated>
        <summary type="html"><![CDATA[While the advent of Graph Neural Networks (GNNs) has greatly improved node
and graph representation learning in many applications, the neighborhood
aggregation scheme exposes additional vulnerabilities to adversaries seeking to
extract node-level information about sensitive attributes. In this paper, we
study the problem of protecting sensitive attributes by information obfuscation
when learning with graph structured data. We propose a framework to locally
filter out pre-determined sensitive attributes via adversarial training with
the total variation and the Wasserstein distance. Our method creates a strong
defense against inference attacks, while only suffering small loss in task
performance. Theoretically, we analyze the effectiveness of our framework
against a worst-case adversary, and characterize an inherent trade-off between
maximizing predictive accuracy and minimizing information leakage. Experiments
across multiple datasets from recommender systems, knowledge graphs and quantum
chemistry demonstrate that the proposed approach provides a robust defense
across various graph structures and tasks, while producing competitive GNN
encoders for downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_P/0/1/0/all/0/1"&gt;Peiyuan Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Keyulu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1"&gt;Tommi Jaakkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1"&gt;Geoffrey Gordon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1"&gt;Stefanie Jegelka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Interaction-based Methodology Towards Explainable AI with Better Understanding of Pneumonia Chest X-ray Images. (arXiv:2104.12672v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12672</id>
        <link href="http://arxiv.org/abs/2104.12672"/>
        <updated>2021-06-15T01:45:15.383Z</updated>
        <summary type="html"><![CDATA[In the field of eXplainable AI (XAI), robust ``blackbox'' algorithms such as
Convolutional Neural Networks (CNNs) are known for making high prediction
performance. However, the ability to explain and interpret these algorithms
still require innovation in the understanding of influential and, more
importantly, explainable features that directly or indirectly impact the
performance of predictivity. A number of methods existing in literature focus
on visualization techniques but the concepts of explainability and
interpretability still require rigorous definition. In view of the above needs,
this paper proposes an interaction-based methodology -- Influence Score
(I-score) -- to screen out the noisy and non-informative variables in the
images hence it nourishes an environment with explainable and interpretable
features that are directly associated to feature predictivity. We apply the
proposed method on a real world application in Pneumonia Chest X-ray Image data
set and produced state-of-the-art results. We demonstrate how to apply the
proposed approach for more general big data problems by improving the
explainability and interpretability without sacrificing the prediction
performance. The contribution of this paper opens a novel angle that moves the
community closer to the future pipelines of XAI problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shaw-Hwa Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yiqiao Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation. (arXiv:2106.06751v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06751</id>
        <link href="http://arxiv.org/abs/2106.06751"/>
        <updated>2021-06-15T01:45:15.372Z</updated>
        <summary type="html"><![CDATA[Although teacher forcing has become the main training paradigm for neural
machine translation, it usually makes predictions only conditioned on past
information, and hence lacks global planning for the future. To address this
problem, we introduce another decoder, called seer decoder, into the
encoder-decoder framework during training, which involves future information in
target predictions. Meanwhile, we force the conventional decoder to simulate
the behaviors of the seer decoder via knowledge distillation. In this way, at
test the conventional decoder can perform like the seer decoder without the
attendance of it. Experiment results on the Chinese-English, English-German and
English-Romanian translation tasks show our method can outperform competitive
baselines significantly and achieves greater improvements on the bigger data
sets. Besides, the experiments also prove knowledge distillation the best way
to transfer knowledge from the seer decoder to the conventional decoder
compared to adversarial learning and L2 regularization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shuhao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1"&gt;Dengji Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhengxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1"&gt;Chenze Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoPE: Conditional image generation using Polynomial Expansions. (arXiv:2104.05077v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05077</id>
        <link href="http://arxiv.org/abs/2104.05077"/>
        <updated>2021-06-15T01:45:15.327Z</updated>
        <summary type="html"><![CDATA[Generative modeling has evolved to a notable field of machine learning. Deep
polynomial neural networks (PNNs) have demonstrated impressive results in
unsupervised image generation, where the task is to map an input vector (i.e.,
noise) to a synthesized image. However, the success of PNNs has not been
replicated in conditional generation tasks, such as super-resolution. Existing
PNNs focus on single-variable polynomial expansions which do not fare well to
two-variable inputs, i.e., the noise variable and the conditional variable. In
this work, we introduce a general framework, called CoPE, that enables a
polynomial expansion of two input variables and captures their auto- and
cross-correlations. We exhibit how CoPE can be trivially augmented to accept an
arbitrary number of input variables. CoPE is evaluated in five tasks
(class-conditional generation, inverse problems, edges-to-image translation,
image-to-image translation, attribute-guided generation) involving eight
datasets. The thorough evaluation suggests that CoPE can be useful for tackling
diverse conditional generation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1"&gt;Grigorios G Chrysos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgopoulos_M/0/1/0/all/0/1"&gt;Markos Georgopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panagakis_Y/0/1/0/all/0/1"&gt;Yannis Panagakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Classification Perspective on Scene Text Recognition. (arXiv:2102.10884v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10884</id>
        <link href="http://arxiv.org/abs/2102.10884"/>
        <updated>2021-06-15T01:45:15.321Z</updated>
        <summary type="html"><![CDATA[The prevalent perspectives of scene text recognition are from sequence to
sequence (seq2seq) and segmentation. Nevertheless, the former is composed of
many components which makes implementation and deployment complicated, while
the latter requires character level annotations that is expensive. In this
paper, we revisit classification perspective that models scene text recognition
as an image classification problem. Classification perspective has a simple
pipeline and only needs word level annotations. We revive classification
perspective by devising a scene text recognition model named as CSTR, which
performs as well as methods from other perspectives. The CSTR model consists of
CPNet (classification perspective network) and SPPN (separated conv with global
average pooling prediction network). CSTR is as simple as image classification
model like ResNet \cite{he2016deep} which makes it easy to implement and
deploy. We demonstrate the effectiveness of the classification perspective on
scene text recognition with extensive experiments. Futhermore, CSTR achieves
nearly state-of-the-art performance on six public benchmarks including regular
text, irregular text. The code will be available at
https://github.com/Media-Smart/vedastr.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Hongxiang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yichao Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning. (arXiv:2101.07393v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07393</id>
        <link href="http://arxiv.org/abs/2101.07393"/>
        <updated>2021-06-15T01:45:15.298Z</updated>
        <summary type="html"><![CDATA[We investigate the use of natural language to drive the generalization of
control policies and introduce the new multi-task environment Messenger with
free-form text manuals describing the environment dynamics. Unlike previous
work, Messenger does not assume prior knowledge connecting text and state
observations $-$ the control policy must simultaneously ground the game manual
to entity symbols and dynamics in the environment. We develop a new model, EMMA
(Entity Mapper with Multi-modal Attention) which uses an entity-conditioned
attention module that allows for selective focus over relevant descriptions in
the manual for each entity in the environment. EMMA is end-to-end
differentiable and learns a latent grounding of entities and dynamics from text
to observations using only environment rewards. EMMA achieves successful
zero-shot generalization to unseen games with new dynamics, obtaining a 40%
higher win rate compared to multiple baselines. However, win rate on the
hardest stage of Messenger remains low (10%), demonstrating the need for
additional work in this direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hanjie_A/0/1/0/all/0/1"&gt;Austin W. Hanjie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1"&gt;Victor Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing Multilingual Fairness in Pre-trained Multimodal Representations. (arXiv:2106.06683v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06683</id>
        <link href="http://arxiv.org/abs/2106.06683"/>
        <updated>2021-06-15T01:45:15.289Z</updated>
        <summary type="html"><![CDATA[Recently pre-trained multimodal models, such as CLIP, have received a surge
of attention for their exceptional capabilities towards connecting images and
natural language. The textual representations in English can be desirably
transferred to multilingualism and support promising downstream multimodal
tasks for different languages. Nevertheless, previous fairness discourse in
vision-and-language learning mainly focuses on monolingual representational
biases, and rarely scrutinizes the principles of multilingual fairness in this
multimodal setting, where one language is equated to a group of individuals and
images provide the universal grounding for bridging different languages.

In this paper, we provide a nuanced understanding of individual fairness and
group fairness by viewing language as the recipient of fairness notions. We
define new fairness notions within multilingual context and analytically
articulate that, pre-trained vision-and-language representations are
individually fair across languages but not guaranteed to group fairness.
Furthermore, we conduct extensive experiments to explore the prevalent group
disparity across languages and protected groups including race, gender and age.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jialu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Eric Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPADE: A Spectral Method for Black-Box Adversarial Robustness Evaluation. (arXiv:2102.03716v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03716</id>
        <link href="http://arxiv.org/abs/2102.03716"/>
        <updated>2021-06-15T01:45:15.281Z</updated>
        <summary type="html"><![CDATA[A black-box spectral method is introduced for evaluating the adversarial
robustness of a given machine learning (ML) model. Our approach, named SPADE,
exploits bijective distance mapping between the input/output graphs constructed
for approximating the manifolds corresponding to the input/output data. By
leveraging the generalized Courant-Fischer theorem, we propose a SPADE score
for evaluating the adversarial robustness of a given model, which is proved to
be an upper bound of the best Lipschitz constant under the manifold setting. To
reveal the most non-robust data samples highly vulnerable to adversarial
attacks, we develop a spectral graph embedding procedure leveraging dominant
generalized eigenvectors. This embedding step allows assigning each data sample
a robustness score that can be further harnessed for more effective adversarial
training. Our experiments show the proposed SPADE method leads to promising
empirical results for neural network models that are adversarially trained with
the MNIST and CIFAR-10 data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1"&gt;Wuxinlin Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1"&gt;Chenhui Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhiqiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yaohui Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiru Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhuo Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks. (arXiv:2101.06969v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06969</id>
        <link href="http://arxiv.org/abs/2101.06969"/>
        <updated>2021-06-15T01:45:15.267Z</updated>
        <summary type="html"><![CDATA[Pre-trained models (PTMs) have been widely used in various downstream tasks.
The parameters of PTMs are distributed on the Internet and may suffer backdoor
attacks. In this work, we demonstrate the universal vulnerability of PTMs,
where fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary
downstream tasks. Specifically, attackers can add a simple pre-training task,
which restricts the output representations of trigger instances to pre-defined
vectors, namely neuron-level backdoor attack (NeuBA). If the backdoor
functionality is not eliminated during fine-tuning, the triggers can make the
fine-tuned model predict fixed labels by pre-defined vectors. In the
experiments of both natural language processing (NLP) and computer vision (CV),
we show that NeuBA absolutely controls the predictions for trigger instances
without any knowledge of downstream tasks. Finally, we apply several defense
methods to NeuBA and find that model pruning is a promising direction to resist
NeuBA by excluding backdoored neurons. Our findings sound a red alarm for the
wide use of PTMs. Our source code and models are available at
\url{https://github.com/thunlp/NeuBA}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1"&gt;Guangxuan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yongwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1"&gt;Tian Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Fanchao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yasheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Translation into Low-resource Language Varieties. (arXiv:2106.06797v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06797</id>
        <link href="http://arxiv.org/abs/2106.06797"/>
        <updated>2021-06-15T01:45:15.254Z</updated>
        <summary type="html"><![CDATA[State-of-the-art machine translation (MT) systems are typically trained to
generate the "standard" target language; however, many languages have multiple
varieties (regional varieties, dialects, sociolects, non-native varieties) that
are different from the standard language. Such varieties are often
low-resource, and hence do not benefit from contemporary NLP solutions, MT
included. We propose a general framework to rapidly adapt MT systems to
generate language varieties that are close to, but different from, the standard
target language, using no parallel (source--variety) data. This also includes
adaptation of MT systems to low-resource typologically-related target
languages. We experiment with adapting an English--Russian MT system to
generate Ukrainian and Belarusian, an English--Norwegian Bokm{\aa}l system to
generate Nynorsk, and an English--Arabic system to generate four Arabic
dialects, obtaining significant improvements over competitive baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sachin Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1"&gt;Antonios Anastasopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wintner_S/0/1/0/all/0/1"&gt;Shuly Wintner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Study of sampling methods in sentiment analysis of imbalanced data. (arXiv:2106.06673v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06673</id>
        <link href="http://arxiv.org/abs/2106.06673"/>
        <updated>2021-06-15T01:45:15.220Z</updated>
        <summary type="html"><![CDATA[This work investigates the application of sampling methods for sentiment
analysis on two different highly imbalanced datasets. One dataset contains
online user reviews from the cooking platform Epicurious and the other contains
comments given to the Planned Parenthood organization. In both these datasets,
the classes of interest are rare. Word n-grams were used as features from these
datasets. A feature selection technique based on information gain is first
applied to reduce the number of features to a manageable space. A number of
different sampling methods were then applied to mitigate the class imbalance
problem which are then analyzed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sayyed_Z/0/1/0/all/0/1"&gt;Zeeshan Ali Sayyed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes. (arXiv:2012.05522v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05522</id>
        <link href="http://arxiv.org/abs/2012.05522"/>
        <updated>2021-06-15T01:45:15.214Z</updated>
        <summary type="html"><![CDATA[Synthesizing 3D human motion plays an important role in many graphics
applications as well as understanding human activity. While many efforts have
been made on generating realistic and natural human motion, most approaches
neglect the importance of modeling human-scene interactions and affordance. On
the other hand, affordance reasoning (e.g., standing on the floor or sitting on
the chair) has mainly been studied with static human pose and gestures, and it
has rarely been addressed with human motion. In this paper, we propose to
bridge human motion synthesis and scene affordance reasoning. We present a
hierarchical generative framework to synthesize long-term 3D human motion
conditioning on the 3D scene structure. Building on this framework, we further
enforce multiple geometry constraints between the human mesh and scene point
clouds via optimization to improve realistic synthesis. Our experiments show
significant improvements over previous approaches on generating natural and
physically plausible human motion in a scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiashun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Huazhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jingwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sifei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Robust Certifiable Estimation of the Relative Pose Between Two Calibrated Cameras. (arXiv:2101.08524v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08524</id>
        <link href="http://arxiv.org/abs/2101.08524"/>
        <updated>2021-06-15T01:45:15.205Z</updated>
        <summary type="html"><![CDATA[This work contributes an efficient algorithm to compute the Relative Pose
problem (RPp) between calibrated cameras and certify the optimality of the
solution, given a set of pair-wise feature correspondences affected by noise
and probably corrupted by wrong matches. We propose a family of certifiers that
is shown to increase the ratio of detected optimal solutions. This set of
certifiers is incorporated into a fast essential matrix estimation pipeline
that, given any initial guess for the RPp, refines it iteratively on the
product space of 3D rotations and 2-sphere. In addition, this fast certifiable
pipeline is integrated into a robust framework that combines Graduated
Non-convexity and the Black-Rangarajan duality between robust functions and
line processes.

We proved through extensive experiments on synthetic and real data that the
proposed framework provides a fast and robust relative pose estimation. We make
the code publicly available
\url{https://github.com/mergarsal/FastCertRelPose.git}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Salguero_M/0/1/0/all/0/1"&gt;Mercedes Garcia-Salguero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Jimenez_J/0/1/0/all/0/1"&gt;Javier Gonzalez-Jimenez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Disease Classification of 13,667 Body CT Scans Using Weakly Supervised Deep Learning. (arXiv:2008.01158v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01158</id>
        <link href="http://arxiv.org/abs/2008.01158"/>
        <updated>2021-06-15T01:45:15.197Z</updated>
        <summary type="html"><![CDATA[Background: Training deep learning classifiers typically requires massive
amounts of manual annotation. Weak supervision may leverage existing medical
data to classify multiple diseases and organ systems. Purpose: To design
multi-disease classifiers for body computed tomography (CT) scans using
automatically extracted labels from radiology text reports. Materials &
Methods: This retrospective study deployed rule-based algorithms to extract
19,255 disease labels from reports of 13,667 body CT scans of 12,092 subjects
for training. Using a 3D DenseVNet, three organ systems were segmented:
lungs/pleura, liver/gallbladder, and kidneys/ureters. For each organ, a 3D
convolutional neural network classified normality versus four common diseases.
Testing was performed on an additional 2,158 CT volumes relative to 2,875
manually derived reference labels. Results: Manual validation of the extracted
labels confirmed 91 to 99% accuracy. Performance using the receiver operating
characteristic area under the curve (AUC) for lungs/pleura labels were as
follows: atelectasis 0.77 (95% CI: 0.74 to 0.81), nodule 0.65 (0.61 to 0.69),
emphysema 0.89 (0.86 to 0.92), effusion 0.97 (0.96 to 0.98), and normal 0.89
(0.87 to 0.91). For liver/gallbladder: stone 0.62 (0.56 to 0.67), lesion 0.73
(0.69 to 0.77), dilation 0.87 (0.84 to 0.90), fatty 0.89 (0.86 to 0.92), and
normal 0.82 (0.78 to 0.85). For kidneys/ureters: stone 0.83 (0.79 to 0.87),
atrophy 0.92 (0.89 to 0.94), lesion 0.68 (0.64 to 0.72), cyst 0.70 (0.66 to
0.73), and normal 0.79 (0.75 to 0.83). Conclusion: Weakly supervised deep
learning classifiers leveraged massive amounts of unannotated body CT data to
classify multiple organ systems and diverse diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tushar_F/0/1/0/all/0/1"&gt;Fakrul Islam Tushar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAnniballe_V/0/1/0/all/0/1"&gt;Vincent M. D&amp;#x27;Anniballe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1"&gt;Rui Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1"&gt;Maciej A. Mazurowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1"&gt;Wanyi Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samei_E/0/1/0/all/0/1"&gt;Ehsan Samei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_G/0/1/0/all/0/1"&gt;Geoffrey D. Rubin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_J/0/1/0/all/0/1"&gt;Joseph Y. Lo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Parallel Corpora to Improve Multilingual Embedding based Document and Sentence Alignment. (arXiv:2106.06766v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06766</id>
        <link href="http://arxiv.org/abs/2106.06766"/>
        <updated>2021-06-15T01:45:15.190Z</updated>
        <summary type="html"><![CDATA[Multilingual sentence representations pose a great advantage for low-resource
languages that do not have enough data to build monolingual models on their
own. These multilingual sentence representations have been separately exploited
by few research for document and sentence alignment. However, most of the
low-resource languages are under-represented in these pre-trained models. Thus,
in the context of low-resource languages, these models have to be fine-tuned
for the task at hand, using additional data sources. This paper presents a
weighting mechanism that makes use of available small-scale parallel corpora to
improve the performance of multilingual sentence representations on document
and sentence alignment. Experiments are conducted with respect to two
low-resource languages, Sinhala and Tamil. Results on a newly created dataset
of Sinhala-English, Tamil-English, and Sinhala-Tamil show that this new
weighting mechanism significantly improves both document and sentence
alignment. This dataset, as well as the source-code, is publicly released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sachintha_D/0/1/0/all/0/1"&gt;Dilan Sachintha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piyarathna_L/0/1/0/all/0/1"&gt;Lakmali Piyarathna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajitha_C/0/1/0/all/0/1"&gt;Charith Rajitha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1"&gt;Surangika Ranathunga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Probabilistic Models for 3D Point Cloud Generation. (arXiv:2103.01458v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01458</id>
        <link href="http://arxiv.org/abs/2103.01458"/>
        <updated>2021-06-15T01:45:15.162Z</updated>
        <summary type="html"><![CDATA[We present a probabilistic model for point cloud generation, which is
fundamental for various 3D vision tasks such as shape completion, upsampling,
synthesis and data augmentation. Inspired by the diffusion process in
non-equilibrium thermodynamics, we view points in point clouds as particles in
a thermodynamic system in contact with a heat bath, which diffuse from the
original distribution to a noise distribution. Point cloud generation thus
amounts to learning the reverse diffusion process that transforms the noise
distribution to the distribution of a desired shape. Specifically, we propose
to model the reverse diffusion process for point clouds as a Markov chain
conditioned on certain shape latent. We derive the variational bound in closed
form for training and provide implementations of the model. Experimental
results demonstrate that our model achieves competitive performance in point
cloud generation and auto-encoding. The code is available at
\url{https://github.com/luost26/diffusion-point-cloud}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shitong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Language Usage and Listener Engagement in Podcasts. (arXiv:2106.06605v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06605</id>
        <link href="http://arxiv.org/abs/2106.06605"/>
        <updated>2021-06-15T01:45:15.153Z</updated>
        <summary type="html"><![CDATA[While there is an abundance of popular writing targeted to podcast creators
on how to speak in ways that engage their listeners, there has been little
data-driven analysis of podcasts that relates linguistic style with listener
engagement. In this paper, we investigate how various factors -- vocabulary
diversity, distinctiveness, emotion, and syntax, among others -- correlate with
engagement, based on analysis of the creators' written descriptions and
transcripts of the audio. We build models with different textual
representations, and show that the identified features are highly predictive of
engagement. Our analysis tests popular wisdom about stylistic elements in
high-engagement podcasts, corroborating some aspects, and adding new
perspectives on others.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1"&gt;Sravana Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazarova_M/0/1/0/all/0/1"&gt;Marina Lazarova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yongze Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1"&gt;Rosie Jones&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation. (arXiv:2106.06963v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06963</id>
        <link href="http://arxiv.org/abs/2106.06963"/>
        <updated>2021-06-15T01:45:15.146Z</updated>
        <summary type="html"><![CDATA[Automatically generating radiology reports can improve current clinical
practice in diagnostic radiology. On one hand, it can relieve radiologists from
the heavy burden of report writing; On the other hand, it can remind
radiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.
Yet, this task remains a challenging job for data-driven neural networks, due
to the serious visual and textual data biases. To this end, we propose a
Posterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to
imitate the working patterns of radiologists, who will first examine the
abnormal regions and assign the disease topic tags to the abnormal regions, and
then rely on the years of prior medical knowledge and prior working experience
accumulations to write reports. Thus, the PPKED includes three modules:
Posterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and
Multi-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior
knowledge, which provides explicit abnormal visual regions to alleviate visual
data bias; PrKE explores the prior knowledge from the prior medical knowledge
graph (medical knowledge) and prior radiology reports (working experience) to
alleviate textual data bias. The explored knowledge is distilled by the MKD to
generate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our
method is able to outperform previous state-of-the-art models on these two
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fenglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shen Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1"&gt;Wei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Training with Text Data for End-to-End Speech Recognition. (arXiv:2010.14318v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14318</id>
        <link href="http://arxiv.org/abs/2010.14318"/>
        <updated>2021-06-15T01:45:15.137Z</updated>
        <summary type="html"><![CDATA[We propose a multitask training method for attention-based end-to-end speech
recognition models. We regularize the decoder in a listen, attend, and spell
model by multitask training it on both audio-text and text-only data. Trained
on the 100-hour subset of LibriSpeech, the proposed method, without requiring
an additional language model, leads to an 11% relative performance improvement
over the baseline and approaches the performance of language model shallow
fusion on the test-clean evaluation set. We observe a similar trend on the
whole 960-hour LibriSpeech training set. Analyses of different types of errors
and sample output sentences demonstrate that the proposed method can
incorporate language level information, suggesting its effectiveness in
real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peidong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1"&gt;Tara N. Sainath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_R/0/1/0/all/0/1"&gt;Ron J. Weiss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Attention for Automatic Chest X-ray Report Generation. (arXiv:2106.06965v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06965</id>
        <link href="http://arxiv.org/abs/2106.06965"/>
        <updated>2021-06-15T01:45:15.131Z</updated>
        <summary type="html"><![CDATA[Recently, chest X-ray report generation, which aims to automatically generate
descriptions of given chest X-ray images, has received growing research
interests. The key challenge of chest X-ray report generation is to accurately
capture and describe the abnormal regions. In most cases, the normal regions
dominate the entire chest X-ray image, and the corresponding descriptions of
these normal regions dominate the final report. Due to such data bias,
learning-based models may fail to attend to abnormal regions. In this work, to
effectively capture and describe abnormal regions, we propose the Contrastive
Attention (CA) model. Instead of solely focusing on the current input image,
the CA model compares the current input image with normal images to distill the
contrastive information. The acquired contrastive information can better
represent the visual features of abnormal regions. According to the experiments
on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into
several existing models can boost their performance across most metrics. In
addition, according to the analysis, the CA model can help existing models
better attend to the abnormal regions and provide more accurate descriptions
which are crucial for an interpretable diagnosis. Specifically, we achieve the
state-of-the-art results on the two public datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fenglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1"&gt;Changchang Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shen Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Ping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Integrated Approach to Produce Robust Models with High Efficiency. (arXiv:2008.13305v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.13305</id>
        <link href="http://arxiv.org/abs/2008.13305"/>
        <updated>2021-06-15T01:45:15.114Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) needs to be both efficient and robust for
practical uses. Quantization and structure simplification are promising ways to
adapt DNNs to mobile devices, and adversarial training is the most popular
method to make DNNs robust. In this work, we try to obtain both features by
applying a convergent relaxation quantization algorithm, Binary-Relax (BR), to
a robust adversarial-trained model, ResNets Ensemble via Feynman-Kac Formalism
(EnResNet). We also discover that high precision, such as ternary (tnn) and
4-bit, quantization will produce sparse DNNs. However, this sparsity is
unstructured under advarsarial training. To solve the problems that adversarial
training jeopardizes DNNs' accuracy on clean images and the struture of
sparsity, we design a trade-off loss function that helps DNNs preserve their
natural accuracy and improve the channel sparsity. With our trade-off loss
function, we achieve both goals with no reduction of resistance under weak
attacks and very minor reduction of resistance under strong attcks. Together
with quantized EnResNet with trade-off loss function, we provide robust models
that have high efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhijian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Jack Xin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Crowds by Modeling Common Confusions. (arXiv:2012.13052v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13052</id>
        <link href="http://arxiv.org/abs/2012.13052"/>
        <updated>2021-06-15T01:45:15.093Z</updated>
        <summary type="html"><![CDATA[Crowdsourcing provides a practical way to obtain large amounts of labeled
data at a low cost. However, the annotation quality of annotators varies
considerably, which imposes new challenges in learning a high-quality model
from the crowdsourced annotations. In this work, we provide a new perspective
to decompose annotation noise into common noise and individual noise and
differentiate the source of confusion based on instance difficulty and
annotator expertise on a per-instance-annotator basis. We realize this new
crowdsourcing model by an end-to-end learning solution with two types of noise
adaptation layers: one is shared across annotators to capture their commonly
shared confusions, and the other one is pertaining to each annotator to realize
individual confusion. To recognize the source of noise in each annotation, we
use an auxiliary network to choose the two noise adaptation layers with respect
to both instances and annotators. Extensive experiments on both synthesized and
real-world benchmarks demonstrate the effectiveness of our proposed common
noise adaptation solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1"&gt;Zhendong Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongning Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Span-based Semantic Parsing for Compositional Generalization. (arXiv:2009.06040v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06040</id>
        <link href="http://arxiv.org/abs/2009.06040"/>
        <updated>2021-06-15T01:45:15.086Z</updated>
        <summary type="html"><![CDATA[Despite the success of sequence-to-sequence (seq2seq) models in semantic
parsing, recent work has shown that they fail in compositional generalization,
i.e., the ability to generalize to new structures built of components observed
during training. In this work, we posit that a span-based parser should lead to
better compositional generalization. we propose SpanBasedSP, a parser that
predicts a span tree over an input utterance, explicitly encoding how partial
programs compose over spans in the input. SpanBasedSP extends Pasupat et al.
(2019) to be comparable to seq2seq models by (i) training from programs,
without access to gold trees, treating trees as latent variables, (ii) parsing
a class of non-projective trees through an extension to standard CKY. On
GeoQuery, SCAN and CLOSURE datasets, SpanBasedSP performs similarly to strong
seq2seq baselines on random splits, but dramatically improves performance
compared to baselines on splits that require compositional generalization: from
$61.0 \rightarrow 88.9$ average accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1"&gt;Jonathan Herzig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1"&gt;Jonathan Berant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning. (arXiv:2106.06937v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06937</id>
        <link href="http://arxiv.org/abs/2106.06937"/>
        <updated>2021-06-15T01:45:15.080Z</updated>
        <summary type="html"><![CDATA[Commonsense reasoning research has so far been limited to English. We aim to
evaluate and improve popular multilingual language models (ML-LMs) to help
advance commonsense reasoning (CSR) beyond English. We collect the Mickey
Corpus, consisting of 561k sentences in 11 different languages, which can be
used for analyzing and improving ML-LMs. We propose Mickey Probe, a
language-agnostic probing task for fairly evaluating the common sense of
popular ML-LMs across different languages. In addition, we also create two new
datasets, X-CSQA and X-CODAH, by translating their English versions to 15 other
languages, so that we can evaluate popular ML-LMs for cross-lingual commonsense
reasoning. To improve the performance beyond English, we propose a simple yet
effective method -- multilingual contrastive pre-training (MCP). It
significantly enhances sentence representations, yielding a large performance
gain on both benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seyeon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_X/0/1/0/all/0/1"&gt;Xiaoyang Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the Imaging Landmarks: Unsupervised Key point Detection in Lung Ultrasound Videos. (arXiv:2106.06987v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.06987</id>
        <link href="http://arxiv.org/abs/2106.06987"/>
        <updated>2021-06-15T01:45:15.073Z</updated>
        <summary type="html"><![CDATA[Lung ultrasound (LUS) is an increasingly popular diagnostic imaging modality
for continuous and periodic monitoring of lung infection, given its advantages
of non-invasiveness, non-ionizing nature, portability and easy disinfection.
The major landmarks assessed by clinicians for triaging using LUS are pleura, A
and B lines. There have been many efforts for the automatic detection of these
landmarks. However, restricting to a few pre-defined landmarks may not reveal
the actual imaging biomarkers particularly in case of new pathologies like
COVID-19. Rather, the identification of key landmarks should be driven by data
given the availability of a plethora of neural network algorithms. This work is
a first of its kind attempt towards unsupervised detection of the key LUS
landmarks in LUS videos of COVID-19 subjects during various stages of
infection. We adapted the relatively newer approach of transporter neural
networks to automatically mark and track pleura, A and B lines based on their
periodic motion and relatively stable appearance in the videos. Initial results
on unsupervised pleura detection show an accuracy of 91.8% employing 1081 LUS
video frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tripathi_A/0/1/0/all/0/1"&gt;Arpan Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Panicker_M/0/1/0/all/0/1"&gt;Mahesh Raveendranatha Panicker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1"&gt;Abhilash R Hareendranathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yale Tung Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jaremko_J/0/1/0/all/0/1"&gt;Jacob L Jaremko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Narayan_K/0/1/0/all/0/1"&gt;Kiran Vishnu Narayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+C_K/0/1/0/all/0/1"&gt;Kesavadas C&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-efficient Transformers via Top-$k$ Attention. (arXiv:2106.06899v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06899</id>
        <link href="http://arxiv.org/abs/2106.06899"/>
        <updated>2021-06-15T01:45:15.055Z</updated>
        <summary type="html"><![CDATA[Following the success of dot-product attention in Transformers, numerous
approximations have been recently proposed to address its quadratic complexity
with respect to the input length. While these variants are memory and compute
efficient, it is not possible to directly use them with popular pre-trained
language models trained using vanilla attention, without an expensive
corrective pre-training stage. In this work, we propose a simple yet highly
accurate approximation for vanilla attention. We process the queries in chunks,
and for each query, compute the top-$k$ scores with respect to the keys. Our
approach offers several advantages: (a) its memory usage is linear in the input
size, similar to linear attention variants, such as Performer and RFA (b) it is
a drop-in replacement for vanilla attention that does not require any
corrective pre-training, and (c) it can also lead to significant memory savings
in the feed-forward layers after casting them into the familiar query-key-value
framework. We evaluate the quality of top-$k$ approximation for multi-head
attention layers on the Long Range Arena Benchmark, and for feed-forward layers
of T5 and UnifiedQA on multiple QA datasets. We show our approach leads to
accuracy that is nearly-identical to vanilla attention in multiple setups
including training from scratch, fine-tuning, and zero-shot inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Ankit Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dar_G/0/1/0/all/0/1"&gt;Guy Dar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_S/0/1/0/all/0/1"&gt;Shaya Goodman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciprut_D/0/1/0/all/0/1"&gt;David Ciprut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1"&gt;Jonathan Berant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compression of Deep Learning Models for Text: A Survey. (arXiv:2008.05221v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05221</id>
        <link href="http://arxiv.org/abs/2008.05221"/>
        <updated>2021-06-15T01:45:15.047Z</updated>
        <summary type="html"><![CDATA[In recent years, the fields of natural language processing (NLP) and
information retrieval (IR) have made tremendous progress thanksto deep learning
models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and
Long Short-Term Memory (LSTMs)networks, and Transformer [120] based models like
Bidirectional Encoder Representations from Transformers (BERT) [24],
GenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network
(MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer
transformer (T5) [95], T-NLG [98] and GShard [63]. But these models are
humongous in size. On the other hand,real world applications demand small model
size, low response times and low computational power wattage. In this survey,
wediscuss six different types of methods (Pruning, Quantization, Knowledge
Distillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic
Transformer based methods) for compression of such models to enable their
deployment in real industry NLP projects.Given the critical need of building
applications with efficient and small models, and the large amount of recently
published work inthis area, we believe that this survey organizes the plethora
of work done by the 'deep learning for NLP' community in the past fewyears and
presents it as a coherent story.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Manish Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1"&gt;Puneet Agrawal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualization Techniques to Enhance Automated Event Extraction. (arXiv:2106.06588v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06588</id>
        <link href="http://arxiv.org/abs/2106.06588"/>
        <updated>2021-06-15T01:45:15.040Z</updated>
        <summary type="html"><![CDATA[Robust visualization of complex data is critical for the effective use of NLP
for event classification, as the volume of data is large and the
high-dimensional structure of text makes data challenging to summarize
succinctly. In event extraction tasks in particular, visualization can aid in
understanding and illustrating the textual relationships from which machine
learning tools produce insights. Through our case study which seeks to identify
potential triggers of state-led mass killings from news articles using NLP, we
demonstrate how visualizations can aid in each stage, from exploratory analysis
of raw data, to machine learning training analysis, and finally post-inference
validation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henn_S/0/1/0/all/0/1"&gt;Sophia Henn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sticha_A/0/1/0/all/0/1"&gt;Abigail Sticha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burley_T/0/1/0/all/0/1"&gt;Timothy Burley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verdeja_E/0/1/0/all/0/1"&gt;Ernesto Verdeja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brenner_P/0/1/0/all/0/1"&gt;Paul Brenner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thinking Like Transformers. (arXiv:2106.06981v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06981</id>
        <link href="http://arxiv.org/abs/2106.06981"/>
        <updated>2021-06-15T01:45:15.033Z</updated>
        <summary type="html"><![CDATA[What is the computational model behind a Transformer? Where recurrent neural
networks have direct parallels in finite state machines, allowing clear
discussion and thought around architecture variants or trained models,
Transformers have no such familiar parallel. In this paper we aim to change
that, proposing a computational model for the transformer-encoder in the form
of a programming language. We map the basic components of a transformer-encoder
-- attention and feed-forward computation -- into simple primitives, around
which we form a programming language: the Restricted Access Sequence Processing
Language (RASP). We show how RASP can be used to program solutions to tasks
that could conceivably be learned by a Transformer, and how a Transformer can
be trained to mimic a RASP solution. In particular, we provide RASP programs
for histograms, sorting, and Dyck-languages. We further use our model to relate
their difficulty in terms of the number of required layers and attention heads:
analyzing a RASP program implies a maximum number of heads and layers necessary
to encode a task in a transformer. Finally, we see how insights gained from our
abstraction might be used to explain phenomena seen in recent works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1"&gt;Gail Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1"&gt;Eran Yahav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Capsule Attention for Multimodal EEG-EOG Representation Learning with Application to Driver Vigilance Estimation. (arXiv:1912.07812v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.07812</id>
        <link href="http://arxiv.org/abs/1912.07812"/>
        <updated>2021-06-15T01:45:15.006Z</updated>
        <summary type="html"><![CDATA[Driver vigilance estimation is an important task for transportation safety.
Wearable and portable brain-computer interface devices provide a powerful means
for real-time monitoring of the vigilance level of drivers to help with
avoiding distracted or impaired driving. In this paper, we propose a novel
multimodal architecture for in-vehicle vigilance estimation from
Electroencephalogram and Electrooculogram. To enable the system to focus on the
most salient parts of the learned multimodal representations, we propose an
architecture composed of a capsule attention mechanism following a deep Long
Short-Term Memory (LSTM) network. Our model learns hierarchical dependencies in
the data through the LSTM and capsule feature representation layers. To better
explore the discriminative ability of the learned representations, we study the
effect of the proposed capsule attention mechanism including the number of
dynamic routing iterations as well as other parameters. Experiments show the
robustness of our method by outperforming other solutions and baseline
techniques, setting a new state-of-the-art. We then provide an analysis on
different frequency bands and brain regions to evaluate their suitability for
driver vigilance estimation. Lastly, an analysis on the role of capsule
attention, multimodality, and robustness to noise is performed, highlighting
the advantages of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guangyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1"&gt;Ali Etemad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Image Classification Using A Low-Pass Activation Function and DCT Augmentation. (arXiv:2007.09453v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.09453</id>
        <link href="http://arxiv.org/abs/2007.09453"/>
        <updated>2021-06-15T01:45:14.986Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Network's (CNN's) performance disparity on clean and
corrupted datasets has recently come under scrutiny. In this work, we analyse
common corruptions in the frequency domain, i.e., High Frequency corruptions
(HFc, e.g., noise) and Low Frequency corruptions (LFc, e.g., blur). Although a
simple solution to HFc is low-pass filtering, ReLU -- a widely used Activation
Function (AF), does not have any filtering mechanism. In this work, we instill
low-pass filtering into the AF (LP-ReLU) to improve robustness against HFc. To
deal with LFc, we complement LP-ReLU with Discrete Cosine Transform based
augmentation. LP-ReLU, coupled with DCT augmentation, enables a deep network to
tackle the entire spectrum of corruption. We use CIFAR-10-C and Tiny ImageNet-C
for evaluation and demonstrate improvements of 5% and 7.3% in accuracy
respectively, compared to the State-Of-The-Art (SOTA). We further evaluate our
method's stability on a variety of perturbations in CIFAR-10-P and Tiny
ImageNet-P, achieving new SOTA in these experiments as well. To further
strengthen our understanding regarding CNN's lack of robustness, a decision
space visualisation process is proposed and presented in this work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1"&gt;Md Tahmid Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1"&gt;Shyh Wei Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohel_F/0/1/0/all/0/1"&gt;Ferdous Sohel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guojun Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation. (arXiv:2106.06963v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06963</id>
        <link href="http://arxiv.org/abs/2106.06963"/>
        <updated>2021-06-15T01:45:14.960Z</updated>
        <summary type="html"><![CDATA[Automatically generating radiology reports can improve current clinical
practice in diagnostic radiology. On one hand, it can relieve radiologists from
the heavy burden of report writing; On the other hand, it can remind
radiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.
Yet, this task remains a challenging job for data-driven neural networks, due
to the serious visual and textual data biases. To this end, we propose a
Posterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to
imitate the working patterns of radiologists, who will first examine the
abnormal regions and assign the disease topic tags to the abnormal regions, and
then rely on the years of prior medical knowledge and prior working experience
accumulations to write reports. Thus, the PPKED includes three modules:
Posterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and
Multi-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior
knowledge, which provides explicit abnormal visual regions to alleviate visual
data bias; PrKE explores the prior knowledge from the prior medical knowledge
graph (medical knowledge) and prior radiology reports (working experience) to
alleviate textual data bias. The explored knowledge is distilled by the MKD to
generate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our
method is able to outperform previous state-of-the-art models on these two
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fenglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shen Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1"&gt;Wei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feedback Pyramid Attention Networks for Single Image Super-Resolution. (arXiv:2106.06966v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06966</id>
        <link href="http://arxiv.org/abs/2106.06966"/>
        <updated>2021-06-15T01:45:14.927Z</updated>
        <summary type="html"><![CDATA[Recently, convolutional neural network (CNN) based image super-resolution
(SR) methods have achieved significant performance improvement. However, most
CNN-based methods mainly focus on feed-forward architecture design and neglect
to explore the feedback mechanism, which usually exists in the human visual
system. In this paper, we propose feedback pyramid attention networks (FPAN) to
fully exploit the mutual dependencies of features. Specifically, a novel
feedback connection structure is developed to enhance low-level feature
expression with high-level information. In our method, the output of each layer
in the first stage is also used as the input of the corresponding layer in the
next state to re-update the previous low-level filters. Moreover, we introduce
a pyramid non-local structure to model global contextual information in
different scales and improve the discriminative representation of the network.
Extensive experimental results on various datasets demonstrate the superiority
of our FPAN in comparison with the state-of-the-art SR methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Huapeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gui_J/0/1/0/all/0/1"&gt;Jie Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1"&gt;James T. Kwok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhihui Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepMMSA: A Novel Multimodal Deep Learning Method for Non-small Cell Lung Cancer Survival Analysis. (arXiv:2106.06744v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06744</id>
        <link href="http://arxiv.org/abs/2106.06744"/>
        <updated>2021-06-15T01:45:14.921Z</updated>
        <summary type="html"><![CDATA[Lung cancer is the leading cause of cancer death worldwide. The critical
reason for the deaths is delayed diagnosis and poor prognosis. With the
accelerated development of deep learning techniques, it has been successfully
applied extensively in many real-world applications, including health sectors
such as medical image interpretation and disease diagnosis. By combining more
modalities that being engaged in the processing of information, multimodal
learning can extract better features and improve predictive ability. The
conventional methods for lung cancer survival analysis normally utilize
clinical data and only provide a statistical probability. To improve the
survival prediction accuracy and help prognostic decision-making in clinical
practice for medical experts, we for the first time propose a multimodal deep
learning method for non-small cell lung cancer (NSCLC) survival analysis, named
DeepMMSA. This method leverages CT images in combination with clinical data,
enabling the abundant information hold within medical images to be associate
with lung cancer survival information. We validate our method on the data of
422 NSCLC patients from The Cancer Imaging Archive (TCIA). Experimental results
support our hypothesis that there is an underlying relationship between
prognostic information and radiomic images. Besides, quantitative results
showing that the established multimodal model can be applied to traditional
method and has the potential to break bottleneck of existing methods and
increase the the percentage of concordant pairs(right predicted pairs) in
overall population by 4%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yujiao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jie Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaoshui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1"&gt;Sai Ho Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1"&gt;Steven Weidong Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Dynamic Pruning for Non-IID Federated Learning. (arXiv:2106.06921v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06921</id>
        <link href="http://arxiv.org/abs/2106.06921"/>
        <updated>2021-06-15T01:45:14.914Z</updated>
        <summary type="html"><![CDATA[Federated Learning~(FL) has emerged as a new paradigm of training machine
learning models without sacrificing data security and privacy. Learning models
at edge devices such as cell phones is one of the most common use case of FL.
However, the limited computing power and energy constraints of edge devices
hinder the adoption of FL for both model training and deployment, especially
for the resource-hungry Deep Neural Networks~(DNNs). To this end, many model
compression methods have been proposed and network pruning is among the most
well-known. However, a pruning policy for a given model is highly
dataset-dependent, which is not suitable for non-Independent and Identically
Distributed~(Non-IID) FL edge devices. In this paper, we present an adaptive
pruning scheme for edge devices in an FL system, which applies dataset-aware
dynamic pruning for inference acceleration on Non-IID datasets. Our evaluation
shows that the proposed method accelerates inference by $2\times$~($50\%$ FLOPs
reduction) while maintaining the model's quality on edge devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Sixing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1"&gt;Ali Anwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1"&gt;Ali Jannesari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dirty Road Can Attack: Security of Deep Learning based Automated Lane Centering under Physical-World Attack. (arXiv:2009.06701v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06701</id>
        <link href="http://arxiv.org/abs/2009.06701"/>
        <updated>2021-06-15T01:45:14.895Z</updated>
        <summary type="html"><![CDATA[Automated Lane Centering (ALC) systems are convenient and widely deployed
today, but also highly security and safety critical. In this work, we are the
first to systematically study the security of state-of-the-art deep learning
based ALC systems in their designed operational domains under physical-world
adversarial attacks. We formulate the problem with a safety-critical attack
goal, and a novel and domain-specific attack vector: dirty road patches. To
systematically generate the attack, we adopt an optimization-based approach and
overcome domain-specific design challenges such as camera frame
inter-dependencies due to attack-influenced vehicle control, and the lack of
objective function design for lane detection models.

We evaluate our attack on a production ALC using 80 scenarios from real-world
driving traces. The results show that our attack is highly effective with over
97.5% success rates and less than 0.903 sec average success time, which is
substantially lower than the average driver reaction time. This attack is also
found (1) robust to various real-world factors such as lighting conditions and
view angles, (2) general to different model designs, and (3) stealthy from the
driver's view. To understand the safety impacts, we conduct experiments using
software-in-the-loop simulation and attack trace injection in a real vehicle.
The results show that our attack can cause a 100% collision rate in different
scenarios, including when tested with common safety features such as automatic
emergency braking. We also evaluate and discuss defenses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1"&gt;Takami Sato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Junjie Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Ningfei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Yunhan Jack Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qi Alfred Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[1st Place Solution for YouTubeVOS Challenge 2021:Video Instance Segmentation. (arXiv:2106.06649v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06649</id>
        <link href="http://arxiv.org/abs/2106.06649"/>
        <updated>2021-06-15T01:45:14.875Z</updated>
        <summary type="html"><![CDATA[Video Instance Segmentation (VIS) is a multi-task problem performing
detection, segmentation, and tracking simultaneously. Extended from image set
applications, video data additionally induces the temporal information, which,
if handled appropriately, is very useful to identify and predict object
motions. In this work, we design a unified model to mutually learn these tasks.
Specifically, we propose two modules, named Temporally Correlated Instance
Segmentation (TCIS) and Bidirectional Tracking (BiTrack), to take the benefit
of the temporal correlation between the object's instance masks across adjacent
frames. On the other hand, video data is often redundant due to the frame's
overlap. Our analysis shows that this problem is particularly severe for the
YoutubeVOS-VIS2021 data. Therefore, we propose a Multi-Source Data (MSD)
training mechanism to compensate for the data deficiency. By combining these
techniques with a bag of tricks, the network performance is significantly
boosted compared to the baseline, and outperforms other methods by a
considerable margin on the YoutubeVOS-VIS 2019 and 2021 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thuy C. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tuan N. Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phan_N/0/1/0/all/0/1"&gt;Nam LH. Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Chuong H. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamazaki_M/0/1/0/all/0/1"&gt;Masayuki Yamazaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamanaka_M/0/1/0/all/0/1"&gt;Masao Yamanaka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06896</id>
        <link href="http://arxiv.org/abs/2106.06896"/>
        <updated>2021-06-15T01:45:14.837Z</updated>
        <summary type="html"><![CDATA[The monitoring of coastal wetlands is of great importance to the protection
of marine and terrestrial ecosystems. However, due to the complex environment,
severe vegetation mixture, and difficulty of access, it is impossible to
accurately classify coastal wetlands and identify their species with
traditional classifiers. Despite the integration of multisource remote sensing
data for performance enhancement, there are still challenges with acquiring and
exploiting the complementary merits from multisource data. In this paper, the
Deepwise Feature Interaction Network (DFINet) is proposed for wetland
classification. A depthwise cross attention module is designed to extract
self-correlation and cross-correlation from multisource feature pairs. In this
way, meaningful complementary information is emphasized for classification.
DFINet is optimized by coordinating consistency loss, discrimination loss, and
classification loss. Accordingly, DFINet reaches the standard solution-space
under the regularity of loss functions, while the spatial consistency and
feature discrimination are preserved. Comprehensive experimental results on two
hyperspectral and multispectral wetland datasets demonstrate that the proposed
DFINet outperforms other competitive methods in terms of overall accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yunhao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengmeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianbu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Weiwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1"&gt;Ran Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1"&gt;Qian Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impact of Encoding and Segmentation Strategies on End-to-End Simultaneous Speech Translation. (arXiv:2104.14470v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14470</id>
        <link href="http://arxiv.org/abs/2104.14470"/>
        <updated>2021-06-15T01:45:14.799Z</updated>
        <summary type="html"><![CDATA[Boosted by the simultaneous translation shared task at IWSLT 2020, promising
end-to-end online speech translation approaches were recently proposed. They
consist in incrementally encoding a speech input (in a source language) and
decoding the corresponding text (in a target language) with the best possible
trade-off between latency and translation quality. This paper investigates two
key aspects of end-to-end simultaneous speech translation: (a) how to encode
efficiently the continuous speech flow, and (b) how to segment the speech flow
in order to alternate optimally between reading (R: encoding input) and writing
(W: decoding output) operations. We extend our previously proposed end-to-end
online decoding strategy and show that while replacing BLSTM by ULSTM encoding
degrades performance in offline mode, it actually improves both efficiency and
performance in online mode. We also measure the impact of different methods to
segment the speech signal (using fixed interval boundaries, oracle word
boundaries or randomly set boundaries) and show that our best end-to-end online
decoding strategy is surprisingly the one that alternates R/W operations on
fixed size blocks on our English-German speech translation setup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Ha Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1"&gt;Yannick Est&amp;#xe8;ve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1"&gt;Laurent Besacier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Convolutional Neural Networks for the Helicity Classification of Magnetic Fields. (arXiv:2106.06718v1 [astro-ph.HE])]]></title>
        <id>http://arxiv.org/abs/2106.06718</id>
        <link href="http://arxiv.org/abs/2106.06718"/>
        <updated>2021-06-15T01:45:14.776Z</updated>
        <summary type="html"><![CDATA[The presence of non-zero helicity in intergalactic magnetic fields is a
smoking gun for their primordial origin since they have to be generated by
processes that break CP invariance. As an experimental signature for the
presence of helical magnetic fields, an estimator $Q$ based on the triple
scalar product of the wave-vectors of photons generated in electromagnetic
cascades from, e.g., TeV blazars, has been suggested previously. We propose to
apply deep learning to helicity classification employing Convolutional Neural
Networks and show that this method outperforms the $Q$ estimator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Vago_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Oreste Pinciroli Vago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Hameed_I/0/1/0/all/0/1"&gt;Ibrahim A. Hameed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kachelriess_M/0/1/0/all/0/1"&gt;Michael Kachelriess&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverting Adversarially Robust Networks for Image Synthesis. (arXiv:2106.06927v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06927</id>
        <link href="http://arxiv.org/abs/2106.06927"/>
        <updated>2021-06-15T01:45:14.712Z</updated>
        <summary type="html"><![CDATA[Recent research in adversarially robust classifiers suggests their
representations tend to be aligned with human perception, which makes them
attractive for image synthesis and restoration applications. Despite favorable
empirical results on a few downstream tasks, their advantages are limited to
slow and sensitive optimization-based techniques. Moreover, their use on
generative models remains unexplored. This work proposes the use of robust
representations as a perceptual primitive for feature inversion models, and
show its benefits with respect to standard non-robust image features. We
empirically show that adopting robust representations as an image prior
significantly improves the reconstruction accuracy of CNN-based feature
inversion models. Furthermore, it allows reconstructing images at multiple
scales out-of-the-box. Following these findings, we propose an
encoding-decoding network based on robust representations and show its
advantages for applications such as anomaly detection, style transfer and image
denoising.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rojas_Gomez_R/0/1/0/all/0/1"&gt;Renan A. Rojas-Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_R/0/1/0/all/0/1"&gt;Raymond A. Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1"&gt;Minh N. Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation. (arXiv:2106.06801v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06801</id>
        <link href="http://arxiv.org/abs/2106.06801"/>
        <updated>2021-06-15T01:45:14.702Z</updated>
        <summary type="html"><![CDATA[Contrastive Learning (CL) is a recent representation learning approach, which
achieves promising results by encouraging inter-class separability and
intra-class compactness in learned image representations. Because medical
images often contain multiple classes of interest per image, a standard
image-level CL for these images is not applicable. In this work, we present a
novel semi-supervised 2D medical segmentation solution that applies CL on image
patches, instead of full images. These patches are meaningfully constructed
using the semantic information of different classes obtained via pseudo
labeling. We also propose a novel consistency regularization scheme, which
works in synergy with contrastive learning. It addresses the problem of
confirmation bias often observed in semi-supervised settings, and encourages
better clustering in the feature space. We evaluate our method on four public
medical segmentation datasets along with a novel histopathology dataset that we
introduce. Our method obtains consistent improvements over the state-of-the-art
semi-supervised segmentation approaches for all datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1"&gt;Prashant Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1"&gt;Ajey Pai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1"&gt;Nisarg Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1"&gt;Prasenjit Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makharia_G/0/1/0/all/0/1"&gt;Govind Makharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1"&gt;Prathosh AP&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1"&gt;Mausam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Randomized Smoothing with Variance Reduced Classifiers. (arXiv:2106.06946v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06946</id>
        <link href="http://arxiv.org/abs/2106.06946"/>
        <updated>2021-06-15T01:45:14.670Z</updated>
        <summary type="html"><![CDATA[Randomized Smoothing (RS) is a promising method for obtaining robustness
certificates by evaluating a base model under noise. In this work we: (i)
theoretically motivate why ensembles are a particularly suitable choice as base
models for RS, and (ii) empirically confirm this choice, obtaining state of the
art results in multiple settings. The key insight of our work is that the
reduced variance of ensembles over the perturbations introduced in RS leads to
significantly more consistent classifications for a given input, in turn
leading to substantially increased certifiable radii for difficult samples. We
also introduce key optimizations which enable an up to 50-fold decrease in
sample complexity of RS, thus drastically reducing its computational overhead.
Experimentally, we show that ensembles of only 3 to 10 classifiers consistently
improve on the strongest single model with respect to their average certified
radius (ACR) by 5% to 21% on both CIFAR-10 and ImageNet. On the latter, we
achieve a state-of-the-art ACR of 1.11. We release all code and models required
to reproduce our results upon publication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Horvath_M/0/1/0/all/0/1"&gt;Mikl&amp;#xf3;s Z. Horv&amp;#xe1;th&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1"&gt;Mark Niklas M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1"&gt;Marc Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1"&gt;Martin Vechev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP. (arXiv:2104.08620v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08620</id>
        <link href="http://arxiv.org/abs/2104.08620"/>
        <updated>2021-06-15T01:45:14.661Z</updated>
        <summary type="html"><![CDATA[Cryptic crosswords, the dominant English-language crossword variety in the
United Kingdom, can be solved by expert humans using flexible, creative
intelligence and knowledge of language. Cryptic clues read like fluent natural
language, but they are adversarially composed of two parts: a definition and a
wordplay cipher requiring sub-word or character-level manipulations. As such,
they are a promising target for evaluating and advancing NLP systems that seek
to process language in more creative, human-like ways. We present a dataset of
cryptic crossword clues from a major newspaper that can be used as a benchmark
and train a sequence-to-sequence model to solve them. We also develop related
benchmarks that can guide development of approaches to this challenging task.
We show that performance can be substantially improved using a novel curriculum
learning approach in which the model is pre-trained on related tasks involving,
e.g, unscrambling words, before it is trained to solve cryptics. However, even
this curricular approach does not generalize to novel clue types in the way
that humans can, and so cryptic crosswords remain a challenge for NLP systems
and a potential source of future innovation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rozner_J/0/1/0/all/0/1"&gt;Josh Rozner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1"&gt;Christopher Potts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1"&gt;Kyle Mahowald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A One-Shot Texture-Perceiving Generative Adversarial Network for Unsupervised Surface Inspection. (arXiv:2106.06792v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06792</id>
        <link href="http://arxiv.org/abs/2106.06792"/>
        <updated>2021-06-15T01:45:14.642Z</updated>
        <summary type="html"><![CDATA[Visual surface inspection is a challenging task owing to the highly diverse
appearance of target surfaces and defective regions. Previous attempts heavily
rely on vast quantities of training examples with manual annotation. However,
in some practical cases, it is difficult to obtain a large number of samples
for inspection. To combat it, we propose a hierarchical texture-perceiving
generative adversarial network (HTP-GAN) that is learned from the one-shot
normal image in an unsupervised scheme. Specifically, the HTP-GAN contains a
pyramid of convolutional GANs that can capture the global structure and
fine-grained representation of an image simultaneously. This innovation helps
distinguishing defective surface regions from normal ones. In addition, in the
discriminator, a texture-perceiving module is devised to capture the spatially
invariant representation of normal image via directional convolutions, making
it more sensitive to defective areas. Experiments on a variety of datasets
consistently demonstrate the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1"&gt;Lingyun Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaokui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stronger Baseline for Ego-Centric Action Detection. (arXiv:2106.06942v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06942</id>
        <link href="http://arxiv.org/abs/2106.06942"/>
        <updated>2021-06-15T01:45:14.636Z</updated>
        <summary type="html"><![CDATA[This technical report analyzes an egocentric video action detection method we
used in the 2021 EPIC-KITCHENS-100 competition hosted in CVPR2021 Workshop. The
goal of our task is to locate the start time and the end time of the action in
the long untrimmed video, and predict action category. We adopt sliding window
strategy to generate proposals, which can better adapt to short-duration
actions. In addition, we show that classification and proposals are conflict in
the same network. The separation of the two tasks boost the detection
performance with high efficiency. By simply employing these strategy, we
achieved 16.10\% performance on the test set of EPIC-KITCHENS-100 Action
Detection challenge using a single model, surpassing the baseline method by
11.7\% in terms of average mAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1"&gt;Zhiwu Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yutong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jianwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingqian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Changxin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1"&gt;Marcelo H. Ang Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1"&gt;Nong Sang&lt;/a&gt;,</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure-Regularized Attention for Deformable Object Representation. (arXiv:2106.06672v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06672</id>
        <link href="http://arxiv.org/abs/2106.06672"/>
        <updated>2021-06-15T01:45:14.628Z</updated>
        <summary type="html"><![CDATA[Capturing contextual dependencies has proven useful to improve the
representational power of deep neural networks. Recent approaches that focus on
modeling global context, such as self-attention and non-local operation,
achieve this goal by enabling unconstrained pairwise interactions between
elements. In this work, we consider learning representations for deformable
objects which can benefit from context exploitation by modeling the structural
dependencies that the data intrinsically possesses. To this end, we provide a
novel structure-regularized attention mechanism, which formalizes feature
interaction as structural factorization through the use of a pair of
light-weight operations. The instantiated building blocks can be directly
incorporated into modern convolutional neural networks, to boost the
representational power in an efficient manner. Comprehensive studies on
multiple tasks and empirical comparisons with modern attention mechanisms
demonstrate the gains brought by our method in terms of both performance and
model complexity. We further investigate its effect on feature representations,
showing that our trained models can capture diversified representations
characterizing object parts without resorting to extra supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shenao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-level Attention Fusion Network for Audio-visual Event Recognition. (arXiv:2106.06736v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06736</id>
        <link href="http://arxiv.org/abs/2106.06736"/>
        <updated>2021-06-15T01:45:14.617Z</updated>
        <summary type="html"><![CDATA[Event classification is inherently sequential and multimodal. Therefore, deep
neural models need to dynamically focus on the most relevant time window and/or
modality of a video. In this study, we propose the Multi-level Attention Fusion
network (MAFnet), an architecture that can dynamically fuse visual and audio
information for event recognition. Inspired by prior studies in neuroscience,
we couple both modalities at different levels of visual and audio paths.
Furthermore, the network dynamically highlights a modality at a given time
window relevant to classify events. Experimental results in AVE (Audio-Visual
Event), UCF51, and Kinetics-Sounds datasets show that the approach can
effectively improve the accuracy in audio-visual event classification. Code is
available at: https://github.com/numediart/MAFnet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brousmiche_M/0/1/0/all/0/1"&gt;Mathilde Brousmiche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1"&gt;Jean Rouat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dupont_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Dupont&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speak or Chat with Me: End-to-End Spoken Language Understanding System with Flexible Inputs. (arXiv:2104.05752v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05752</id>
        <link href="http://arxiv.org/abs/2104.05752"/>
        <updated>2021-06-15T01:45:14.609Z</updated>
        <summary type="html"><![CDATA[A major focus of recent research in spoken language understanding (SLU) has
been on the end-to-end approach where a single model can predict intents
directly from speech inputs without intermediate transcripts. However, this
approach presents some challenges. First, since speech can be considered as
personally identifiable information, in some cases only automatic speech
recognition (ASR) transcripts are accessible. Second, intent-labeled speech
data is scarce. To address the first challenge, we propose a novel system that
can predict intents from flexible types of inputs: speech, ASR transcripts, or
both. We demonstrate strong performance for either modality separately, and
when both speech and ASR transcripts are available, through system combination,
we achieve better results than using a single input modality. To address the
second challenge, we leverage a semantically robust pre-trained BERT model and
adopt a cross-modal system that co-trains text embeddings and acoustic
embeddings in a shared latent space. We further enhance this system by
utilizing an acoustic module pre-trained on LibriSpeech and domain-adapting the
text module on our target datasets. Our experiments show significant advantages
for these pre-training and fine-tuning strategies, resulting in a system that
achieves competitive intent-classification performance on Snips SLU and Fluent
Speech Commands datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1"&gt;Sujeong Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1"&gt;Wangrui Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1"&gt;Hyun Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phung_M/0/1/0/all/0/1"&gt;My Phung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1"&gt;Michael Picheny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1"&gt;Hong-Kwang Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1"&gt;Samuel Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morais_E/0/1/0/all/0/1"&gt;Edmilson Morais&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Representation Learning via Perceptual Similarity Metrics. (arXiv:2106.06620v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06620</id>
        <link href="http://arxiv.org/abs/2106.06620"/>
        <updated>2021-06-15T01:45:14.594Z</updated>
        <summary type="html"><![CDATA[A fundamental challenge in artificial intelligence is learning useful
representations of data that yield good performance on a downstream task,
without overfitting to spurious input features. Extracting such task-relevant
predictive information is particularly difficult for real-world datasets. In
this work, we propose Contrastive Input Morphing (CIM), a representation
learning framework that learns input-space transformations of the data to
mitigate the effect of irrelevant input features on downstream performance. Our
method leverages a perceptual similarity metric via a triplet loss to ensure
that the transformation preserves task-relevant information.Empirically, we
demonstrate the efficacy of our approach on tasks which typically suffer from
the presence of spurious correlations: classification with nuisance
information, out-of-distribution generalization, and preservation of subgroup
accuracies. We additionally show that CIM is complementary to other mutual
information-based representation learning techniques, and demonstrate that it
improves the performance of variational information bottleneck (VIB) when used
together.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taghanaki_S/0/1/0/all/0/1"&gt;Saeid Asgari Taghanaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1"&gt;Kristy Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1"&gt;Amir Khasahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1"&gt;Anirudh Goyal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation and Correlation Enhanced Encoder-Decoder Framework for Scene Text Recognition. (arXiv:2106.06960v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06960</id>
        <link href="http://arxiv.org/abs/2106.06960"/>
        <updated>2021-06-15T01:45:14.574Z</updated>
        <summary type="html"><![CDATA[Attention-based encoder-decoder framework is widely used in the scene text
recognition task. However, for the current state-of-the-art(SOTA) methods,
there is room for improvement in terms of the efficient usage of local visual
and global context information of the input text image, as well as the robust
correlation between the scene processing module(encoder) and the text
processing module(decoder). In this paper, we propose a Representation and
Correlation Enhanced Encoder-Decoder Framework(RCEED) to address these
deficiencies and break performance bottleneck. In the encoder module, local
visual feature, global context feature, and position information are aligned
and fused to generate a small-size comprehensive feature map. In the decoder
module, two methods are utilized to enhance the correlation between scene and
text feature space. 1) The decoder initialization is guided by the holistic
feature and global glimpse vector exported from the encoder. 2) The feature
enriched glimpse vector produced by the Multi-Head General Attention is used to
assist the RNN iteration and the character prediction at each time step.
Meanwhile, we also design a Layernorm-Dropout LSTM cell to improve model's
generalization towards changeable texts. Extensive experiments on the
benchmarks demonstrate the advantageous performance of RCEED in scene text
recognition tasks, especially the irregular ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1"&gt;Mengmeng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinjin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Accurate and Realistic Outfits Visualization with Attention to Details. (arXiv:2106.06593v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06593</id>
        <link href="http://arxiv.org/abs/2106.06593"/>
        <updated>2021-06-15T01:45:14.568Z</updated>
        <summary type="html"><![CDATA[Virtual try-on methods aim to generate images of fashion models wearing
arbitrary combinations of garments. This is a challenging task because the
generated image must appear realistic and accurately display the interaction
between garments. Prior works produce images that are filled with artifacts and
fail to capture important visual details necessary for commercial applications.
We propose Outfit Visualization Net (OVNet) to capture these important details
(e.g. buttons, shading, textures, realistic hemlines, and interactions between
garments) and produce high quality multiple-garment virtual try-on images.
OVNet consists of 1) a semantic layout generator and 2) an image generation
pipeline using multiple coordinated warps. We train the warper to output
multiple warps using a cascade loss, which refines each successive warp to
focus on poorly generated regions of a previous warp and yields consistent
improvements in detail. In addition, we introduce a method for matching outfits
with the most suitable model and produce significant improvements for both our
and other previous try-on methods. Through quantitative and qualitative
analysis, we demonstrate our method generates substantially higher-quality
studio images compared to prior works for multi-garment outfits. An interactive
interface powered by this method has been deployed on fashion e-commerce
websites and received overwhelmingly positive feedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kedan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1"&gt;Min jin Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jeffrey Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingen Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Robustness via Fisher-Rao Regularization. (arXiv:2106.06685v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06685</id>
        <link href="http://arxiv.org/abs/2106.06685"/>
        <updated>2021-06-15T01:45:14.557Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness has become a topic of growing interest in machine
learning since it was observed that neural networks tend to be brittle. We
propose an information-geometric formulation of adversarial defense and
introduce FIRE, a new Fisher-Rao regularization for the categorical
cross-entropy loss, which is based on the geodesic distance between natural and
perturbed input features. Based on the information-geometric properties of the
class of softmax distributions, we derive an explicit characterization of the
Fisher-Rao Distance (FRD) for the binary and multiclass cases, and draw some
interesting properties as well as connections with standard regularization
metrics. Furthermore, for a simple linear and Gaussian model, we show that all
Pareto-optimal points in the accuracy-robustness region can be reached by FIRE
while other state-of-the-art methods fail. Empirically, we evaluate the
performance of various classifiers trained with the proposed loss on standard
datasets, showing up to 2\% of improvements in terms of robustness while
reducing the training time by 20\% over the best-performing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Picot_M/0/1/0/all/0/1"&gt;Marine Picot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Messina_F/0/1/0/all/0/1"&gt;Francisco Messina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1"&gt;Malik Boudiaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labeau_F/0/1/0/all/0/1"&gt;Fabrice Labeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ismail Ben Ayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1"&gt;Pablo Piantanida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs. (arXiv:2106.06959v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06959</id>
        <link href="http://arxiv.org/abs/2106.06959"/>
        <updated>2021-06-15T01:45:14.537Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a method to find local-geometry-aware traversal
directions on the intermediate latent space of Generative Adversarial Networks
(GANs). These directions are defined as an ordered basis of tangent space at a
latent code. Motivated by the intrinsic sparsity of the latent space, the basis
is discovered by solving the low-rank approximation problem of the differential
of the partial network. Moreover, the local traversal basis leads to a natural
iterative traversal on the latent space. Iterative Curve-Traversal shows stable
traversal on images, since the trajectory of latent code stays close to the
latent space even under the strong perturbations compared to the linear
traversal. This stability provides far more diverse variations of the given
image. Although the proposed method can be applied to various GAN models, we
focus on the W-space of the StyleGAN2, which is renowned for showing the better
disentanglement of the latent factors of variation. Our quantitative and
qualitative analysis provides evidence showing that the W-space is still
globally warped while showing a certain degree of global consistency of
interpretable variation. In particular, we introduce some metrics on the
Grassmannian manifolds to quantify the global warpage of the W-space and the
subspace traversal to test the stability of traversal directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jaewoong Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1"&gt;Changyeon Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jung Ho Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1"&gt;Geonho Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Myungjoo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pay Attention with Focus: A Novel Learning Scheme for Classification of Whole Slide Images. (arXiv:2106.06623v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06623</id>
        <link href="http://arxiv.org/abs/2106.06623"/>
        <updated>2021-06-15T01:45:14.510Z</updated>
        <summary type="html"><![CDATA[Deep learning methods such as convolutional neural networks (CNNs) are
difficult to directly utilize to analyze whole slide images (WSIs) due to the
large image dimensions. We overcome this limitation by proposing a novel
two-stage approach. First, we extract a set of representative patches (called
mosaic) from a WSI. Each patch of a mosaic is encoded to a feature vector using
a deep network. The feature extractor model is fine-tuned using hierarchical
target labels of WSIs, i.e., anatomic site and primary diagnosis. In the second
stage, a set of encoded patch-level features from a WSI is used to compute the
primary diagnosis probability through the proposed Pay Attention with Focus
scheme, an attention-weighted averaging of predicted probabilities for all
patches of a mosaic modulated by a trainable focal factor. Experimental results
show that the proposed model can be robust, and effective for the
classification of WSIs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalra_S/0/1/0/all/0/1"&gt;Shivam Kalra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adnan_M/0/1/0/all/0/1"&gt;Mohammed Adnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hemati_S/0/1/0/all/0/1"&gt;Sobhan Hemati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehkharghanian_T/0/1/0/all/0/1"&gt;Taher Dehkharghanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahnamayan_S/0/1/0/all/0/1"&gt;Shahryar Rahnamayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1"&gt;Hamid Tizhoosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rapid COVID-19 Risk Screening by Eye-region Manifestations. (arXiv:2106.06664v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.06664</id>
        <link href="http://arxiv.org/abs/2106.06664"/>
        <updated>2021-06-15T01:45:14.491Z</updated>
        <summary type="html"><![CDATA[It is still nontrivial to develop a new fast COVID-19 screening method with
the easier access and lower cost, due to the technical and cost limitations of
the current testing methods in the medical resource-poor districts. On the
other hand, there are more and more ocular manifestations that have been
reported in the COVID-19 patients as growing clinical evidence[1]. This
inspired this project. We have conducted the joint clinical research since
January 2021 at the ShiJiaZhuang City, Heibei province, China, which approved
by the ethics committee of The fifth hospital of ShiJiaZhuang of Hebei Medical
University. We undertake several blind tests of COVID-19 patients by Union
Hospital, Tongji Medical College, Huazhong University of Science and
Technology, Wuhan, China. Meantime as an important part of the ongoing globally
COVID-19 eye test program by AIMOMICS since February 2020, we propose a new
fast screening method of analyzing the eye-region images, captured by common
CCD and CMOS cameras. This could reliably make a rapid risk screening of
COVID-19 with the sustainable stable high performance in different countries
and races. Our model for COVID-19 rapid prescreening have the merits of the
lower cost, fully self-performed, non-invasive, importantly real-time, and thus
enables the continuous health surveillance. We further implement it as the open
accessible APIs, and provide public service to the world. Our pilot experiments
show that our model is ready to be usable to all kinds of surveillance
scenarios, such as infrared temperature measurement device at airports and
stations, or directly pushing to the target people groups smartphones as a
packaged application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Lei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Haojie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Qiang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1"&gt;Li Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;Hong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xue_X/0/1/0/all/0/1"&gt;Xiangyang Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1"&gt;Feng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pei_Y/0/1/0/all/0/1"&gt;Yantao Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiuqi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yanhua Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gu1_H/0/1/0/all/0/1"&gt;Hongxia Tian Mengwei Gu1&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dise\~no y desarrollo de aplicaci\'on m\'ovil para la clasificaci\'on de flora nativa chilena utilizando redes neuronales convolucionales. (arXiv:2106.06592v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06592</id>
        <link href="http://arxiv.org/abs/2106.06592"/>
        <updated>2021-06-15T01:45:14.465Z</updated>
        <summary type="html"><![CDATA[Introduction: Mobile apps, through artificial vision, are capable of
recognizing vegetable species in real time. However, the existing species
recognition apps do not take in consideration the wide variety of endemic and
native (Chilean) species, which leads to wrong species predictions. This study
introduces the development of a chilean species dataset and an optimized
classification model implemented to a mobile app. Method: the data set was
built by putting together pictures of several species captured on the field and
by selecting some pictures available from other datasets available online.
Convolutional neural networks were used in order to develop the images
prediction models. The networks were trained by performing a sensitivity
analysis, validating with k-fold cross validation and performing tests with
different hyper-parameters, optimizers, convolutional layers, and learning
rates in order to identify and choose the best models and then put them
together in one classification model. Results: The final data set was
compounded by 46 species, including native species, endemic and exotic from
Chile, with 6120 training pictures and 655 testing pictures. The best models
were implemented on a mobile app, obtaining a 95% correct prediction rate with
respect to the set of tests. Conclusion: The app developed in this study is
capable of classifying species with a high level of accuracy, depending on the
state of the art of the artificial vision and it can also show relevant
information related to the classified species.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Munoz_I/0/1/0/all/0/1"&gt;Ignacio Mu&amp;#xf1;oz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolt_A/0/1/0/all/0/1"&gt;Alfredo Bolt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Modal Attention Consistency for Video-Audio Unsupervised Learning. (arXiv:2106.06939v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06939</id>
        <link href="http://arxiv.org/abs/2106.06939"/>
        <updated>2021-06-15T01:45:14.438Z</updated>
        <summary type="html"><![CDATA[Cross-modal correlation provides an inherent supervision for video
unsupervised representation learning. Existing methods focus on distinguishing
different video clips by visual and audio representations. We human visual
perception could attend to regions where sounds are made, and our auditory
perception could also ground their frequencies of sounding objects, which we
call bidirectional local correspondence. Such supervision is intuitive but not
well explored in the contrastive learning framework. This paper introduces a
pretext task, Cross-Modal Attention Consistency (CMAC), for exploring the
bidirectional local correspondence property. The CMAC approach aims to align
the regional attention generated purely from the visual signal with the target
attention generated under the guidance of acoustic signal, and do a similar
alignment for frequency grounding on the acoustic attention. Accompanied by a
remoulded cross-modal contrastive loss where we consider additional
within-modal interactions, the CMAC approach works effectively for enforcing
the bidirectional alignment. Extensive experiments on six downstream benchmarks
demonstrate that CMAC can improve the state-of-the-art performance on both
visual and audio modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1"&gt;Shaobo Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1"&gt;Qi Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Hongtao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingdong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!). (arXiv:2106.06561v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06561</id>
        <link href="http://arxiv.org/abs/2106.06561"/>
        <updated>2021-06-15T01:45:14.431Z</updated>
        <summary type="html"><![CDATA[We show how to learn a map that takes a content code, derived from a face
image, and a randomly chosen style code to an anime image. We derive an
adversarial loss from our simple and effective definitions of style and
content. This adversarial loss guarantees the map is diverse -- a very wide
range of anime can be produced from a single content code. Under plausible
assumptions, the map is not just diverse, but also correctly represents the
probability of an anime, conditioned on an input face. In contrast, current
multimodal generation procedures cannot capture the complex styles that appear
in anime. Extensive quantitative experiments support the idea the map is
correct. Extensive qualitative results show that the method can generate a much
more diverse range of styles than SOTA comparisons. Finally, we show that our
formalization of content and style allows us to perform video to video
translation without ever training on videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1"&gt;Min Jin Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1"&gt;David Forsyth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mirror3D: Depth Refinement for Mirror Surfaces. (arXiv:2106.06629v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06629</id>
        <link href="http://arxiv.org/abs/2106.06629"/>
        <updated>2021-06-15T01:45:14.389Z</updated>
        <summary type="html"><![CDATA[Despite recent progress in depth sensing and 3D reconstruction, mirror
surfaces are a significant source of errors. To address this problem, we create
the Mirror3D dataset: a 3D mirror plane dataset based on three RGBD datasets
(Matterport3D, NYUv2 and ScanNet) containing 7,011 mirror instance masks and 3D
planes. We then develop Mirror3DNet: a module that refines raw sensor depth or
estimated depth to correct errors on mirror surfaces. Our key idea is to
estimate the 3D mirror plane based on RGB input and surrounding depth context,
and use this estimate to directly regress mirror surface depth. Our experiments
show that Mirror3DNet significantly mitigates errors from a variety of input
depth data, including raw sensor depth and depth estimation or completion
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1"&gt;Jiaqi Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weijie Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1"&gt;Angel X. Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1"&gt;Manolis Savva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation. (arXiv:2106.03153v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03153</id>
        <link href="http://arxiv.org/abs/2106.03153"/>
        <updated>2021-06-15T01:45:14.370Z</updated>
        <summary type="html"><![CDATA[With rapid progress in neural text-to-speech (TTS) models, personalized
speech generation is now in high demand for many applications. For practical
applicability, a TTS model should generate high-quality speech with only a few
audio samples from the given speaker, that are also short in length. However,
existing methods either require to fine-tune the model or achieve low
adaptation quality without fine-tuning. In this work, we propose StyleSpeech, a
new TTS model which not only synthesizes high-quality speech but also
effectively adapts to new speakers. Specifically, we propose Style-Adaptive
Layer Normalization (SALN) which aligns gain and bias of the text input
according to the style extracted from a reference speech audio. With SALN, our
model effectively synthesizes speech in the style of the target speaker even
from single speech audio. Furthermore, to enhance StyleSpeech's adaptation to
speech from new speakers, we extend it to Meta-StyleSpeech by introducing two
discriminators trained with style prototypes, and performing episodic training.
The experimental results show that our models generate high-quality speech
which accurately follows the speaker's voice with single short-duration (1-3
sec) speech audio, significantly outperforming baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Min_D/0/1/0/all/0/1"&gt;Dongchan Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dong Bok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eunho Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Librispeech Transducer Model with Internal Language Model Prior Correction. (arXiv:2104.03006v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03006</id>
        <link href="http://arxiv.org/abs/2104.03006"/>
        <updated>2021-06-15T01:45:14.323Z</updated>
        <summary type="html"><![CDATA[We present our transducer model on Librispeech. We study variants to include
an external language model (LM) with shallow fusion and subtract an estimated
internal LM. This is justified by a Bayesian interpretation where the
transducer model prior is given by the estimated internal LM. The subtraction
of the internal LM gives us over 14% relative improvement over normal shallow
fusion. Our transducer has a separate probability distribution for the
non-blank labels which allows for easier combination with the external LM, and
easier estimation of the internal LM. We additionally take care of including
the end-of-sentence (EOS) probability of the external LM in the last blank
probability which further improves the performance. All our code and setups are
published.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1"&gt;Albert Zeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merboldt_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Merboldt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michel_W/0/1/0/all/0/1"&gt;Wilfried Michel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controllable Generation from Pre-trained Language Models via Inverse Prompting. (arXiv:2103.10685v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10685</id>
        <link href="http://arxiv.org/abs/2103.10685"/>
        <updated>2021-06-15T01:45:14.275Z</updated>
        <summary type="html"><![CDATA[Large-scale pre-trained language models have demonstrated strong capabilities
of generating realistic text. However, it remains challenging to control the
generation results. Previous approaches such as prompting are far from
sufficient, which limits the usage of language models. To tackle this
challenge, we propose an innovative method, inverse prompting, to better
control text generation. The core idea of inverse prompting is to use generated
text to inversely predict the prompt during beam search, which enhances the
relevance between the prompt and the generated text and provides better
controllability. Empirically, we pre-train a large-scale Chinese language model
to perform a systematic study using human evaluation on the tasks of
open-domain poem generation and open-domain long-form question answering. Our
results show that our proposed method substantially outperforms the baselines
and that our generation quality is close to human performance on some of the
tasks.

Narrators can try our poem generation demo at
https://pretrain.aminer.cn/apps/poetry.html, while our QA demo can be found at
https://pretrain.aminer.cn/app/qa. For researchers, the code is provided in
https://github.com/THUDM/InversePrompting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1"&gt;Xu Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1"&gt;Da Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1"&gt;Qingyang Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Ming Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhilin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Neural Semantic Parsing for Low-Resourced Languages. (arXiv:2106.03469v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03469</id>
        <link href="http://arxiv.org/abs/2106.03469"/>
        <updated>2021-06-15T01:45:14.234Z</updated>
        <summary type="html"><![CDATA[Multilingual semantic parsing is a cost-effective method that allows a single
model to understand different languages. However, researchers face a great
imbalance of availability of training data, with English being resource rich,
and other languages having much less data. To tackle the data limitation
problem, we propose using machine translation to bootstrap multilingual
training data from the more abundant English data. To compensate for the data
quality of machine translated training data, we utilize transfer learning from
pretrained multilingual encoders to further improve the model. To evaluate our
multilingual models on human-written sentences as opposed to machine translated
ones, we introduce a new multilingual semantic parsing dataset in English,
Italian and Japanese based on the Facebook Task Oriented Parsing (TOP) dataset.
We show that joint multilingual training with pretrained encoders substantially
outperforms our baselines on the TOP dataset and outperforms the
state-of-the-art model on the public NLMaps dataset. We also establish a new
baseline for zero-shot learning on the TOP dataset. We find that a semantic
parser trained only on English data achieves a zero-shot performance of 44.9%
exact-match accuracy on Italian sentences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1"&gt;Menglin Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monti_E/0/1/0/all/0/1"&gt;Emilio Monti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reverse-engineer the Distributional Structure of Infant Egocentric Views for Training Generalizable Image Classifiers. (arXiv:2106.06694v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06694</id>
        <link href="http://arxiv.org/abs/2106.06694"/>
        <updated>2021-06-15T01:45:14.222Z</updated>
        <summary type="html"><![CDATA[We analyze egocentric views of attended objects from infants. This paper
shows 1) empirical evidence that children's egocentric views have more diverse
distributions compared to adults' views, 2) we can computationally simulate the
infants' distribution, and 3) the distribution is beneficial for training more
generalized image classifiers not only for infant egocentric vision but for
third-person computer vision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsutsui_S/0/1/0/all/0/1"&gt;Satoshi Tsutsui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1"&gt;David Crandall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chen Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Local Insights from Global Labels: Supervised & Zero-Shot Sequence Labeling via a Convolutional Decomposition. (arXiv:1906.01154v6 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.01154</id>
        <link href="http://arxiv.org/abs/1906.01154"/>
        <updated>2021-06-15T01:45:14.214Z</updated>
        <summary type="html"><![CDATA[We propose a new, more actionable view of neural network interpretability and
data analysis by leveraging the remarkable matching effectiveness of
representations derived from deep networks, guided by an approach for
class-conditional feature detection. The decomposition of the filter-ngram
interactions of a convolutional neural network and a linear layer over a
pre-trained deep network yields a strong binary sequence labeler, with
flexibility in producing predictions at -- and defining loss functions for --
varying label granularities, from the fully-supervised sequence labeling
setting to the challenging zero-shot sequence labeling setting, in which we
seek token-level predictions but only have document-level labels for training.
From this sequence-labeling layer we derive dense representations of the input
that can then be matched to instances from training, or a support set with
known labels. Such introspection with inference-time decision rules provides a
means, in some settings, of making local updates to the model by altering the
labels or instances in the support set without re-training the full model.
Finally, we construct a particular K-nearest neighbors (K-NN) model from
matched exemplar representations that approximates the original model's
predictions and is at least as effective a predictor with respect to the
ground-truth labels. This additionally yields interpretable heuristics at the
token level for determining when predictions are less likely to be reliable,
and for screening input dissimilar to the support set. In effect, we show that
we can transform the deep network into a simple weighting over exemplars and
associated labels, yielding an introspectable -- and modestly updatable --
version of the original model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schmaltz_A/0/1/0/all/0/1"&gt;Allen Schmaltz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization on Medical Imaging Classification using Episodic Training with Task Augmentation. (arXiv:2106.06908v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06908</id>
        <link href="http://arxiv.org/abs/2106.06908"/>
        <updated>2021-06-15T01:45:14.157Z</updated>
        <summary type="html"><![CDATA[Medical imaging datasets usually exhibit domain shift due to the variations
of scanner vendors, imaging protocols, etc. This raises the concern about the
generalization capacity of machine learning models. Domain generalization (DG),
which aims to learn a model from multiple source domains such that it can be
directly generalized to unseen test domains, seems particularly promising to
medical imaging community. To address DG, recent model-agnostic meta-learning
(MAML) has been introduced, which transfers the knowledge from previous
training tasks to facilitate the learning of novel testing tasks. However, in
clinical practice, there are usually only a few annotated source domains
available, which decreases the capacity of training task generation and thus
increases the risk of overfitting to training tasks in the paradigm. In this
paper, we propose a novel DG scheme of episodic training with task augmentation
on medical imaging classification. Based on meta-learning, we develop the
paradigm of episodic training to construct the knowledge transfer from episodic
training-task simulation to the real testing task of DG. Motivated by the
limited number of source domains in real-world medical deployment, we consider
the unique task-level overfitting and we propose task augmentation to enhance
the variety during training task generation to alleviate it. With the
established learning framework, we further exploit a novel meta-objective to
regularize the deep embedding of training domains. To validate the
effectiveness of the proposed method, we perform experiments on
histopathological images and abdominal CT images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1"&gt;Qi Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xinghao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1"&gt;Dong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yizhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Russian News Clustering and Headline Selection Shared Task. (arXiv:2105.00981v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00981</id>
        <link href="http://arxiv.org/abs/2105.00981"/>
        <updated>2021-06-15T01:45:14.144Z</updated>
        <summary type="html"><![CDATA[This paper presents the results of the Russian News Clustering and Headline
Selection shared task. As a part of it, we propose the tasks of Russian news
event detection, headline selection, and headline generation. These tasks are
accompanied by datasets and baselines. The presented datasets for event
detection and headline selection are the first public Russian datasets for
their tasks. The headline generation dataset is based on clustering and
provides multiple reference headlines for every cluster, unlike the previous
datasets. Finally, the approaches proposed by the shared task participants are
reported and analyzed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gusev_I/0/1/0/all/0/1"&gt;Ilya Gusev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smurov_I/0/1/0/all/0/1"&gt;Ivan Smurov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection. (arXiv:2104.09261v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09261</id>
        <link href="http://arxiv.org/abs/2104.09261"/>
        <updated>2021-06-15T01:45:14.126Z</updated>
        <summary type="html"><![CDATA[The existence of multiple datasets for sarcasm detection prompts us to apply
transfer learning to exploit their commonality. The adversarial neural transfer
(ANT) framework utilizes multiple loss terms that encourage the source-domain
and the target-domain feature distributions to be similar while optimizing for
domain-specific performance. However, these objectives may be in conflict,
which can lead to optimization difficulties and sometimes diminished transfer.
We propose a generalized latent optimization strategy that allows different
losses to accommodate each other and improves training dynamics. The proposed
method outperforms transfer learning and meta-learning baselines. In
particular, we achieve 10.02% absolute performance gain over the previous state
of the art on the iSarcasm dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Consolidation based Class Incremental Online Learning with Limited Data. (arXiv:2106.06795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06795</id>
        <link href="http://arxiv.org/abs/2106.06795"/>
        <updated>2021-06-15T01:45:14.118Z</updated>
        <summary type="html"><![CDATA[We propose a novel approach for class incremental online learning in a
limited data setting. This problem setting is challenging because of the
following constraints: (1) Classes are given incrementally, which necessitates
a class incremental learning approach; (2) Data for each class is given in an
online fashion, i.e., each training example is seen only once during training;
(3) Each class has very few training examples; and (4) We do not use or assume
access to any replay/memory to store data from previous classes. Therefore, in
this setting, we have to handle twofold problems of catastrophic forgetting and
overfitting. In our approach, we learn robust representations that are
generalizable across tasks without suffering from the problems of catastrophic
forgetting and overfitting to accommodate future classes with limited samples.
Our proposed method leverages the meta-learning framework with knowledge
consolidation. The meta-learning framework helps the model for rapid learning
when samples appear in an online fashion. Simultaneously, knowledge
consolidation helps to learn a robust representation against forgetting under
online updates to facilitate future learning. Our approach significantly
outperforms other methods on several benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1"&gt;Mohammed Asad Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1"&gt;Vinay Kumar Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Pravendra Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay Namboodiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1"&gt;Piyush Rai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SASICM A Multi-Task Benchmark For Subtext Recognition. (arXiv:2106.06944v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06944</id>
        <link href="http://arxiv.org/abs/2106.06944"/>
        <updated>2021-06-15T01:45:14.106Z</updated>
        <summary type="html"><![CDATA[Subtext is a kind of deep semantics which can be acquired after one or more
rounds of expression transformation. As a popular way of expressing one's
intentions, it is well worth studying. In this paper, we try to make computers
understand whether there is a subtext by means of machine learning. We build a
Chinese dataset whose source data comes from the popular social media (e.g.
Weibo, Netease Music, Zhihu, and Bilibili). In addition, we also build a
baseline model called SASICM to deal with subtext recognition. The F1 score of
SASICMg, whose pretrained model is GloVe, is as high as 64.37%, which is 3.97%
higher than that of BERT based model, 12.7% higher than that of traditional
methods on average, including support vector machine, logistic regression
classifier, maximum entropy classifier, naive bayes classifier and decision
tree and 2.39% higher than that of the state-of-the-art, including MARIN and
BTM. The F1 score of SASICMBERT, whose pretrained model is BERT, is 65.12%,
which is 0.75% higher than that of SASICMg. The accuracy rates of SASICMg and
SASICMBERT are 71.16% and 70.76%, respectively, which can compete with those of
other methods which are mentioned before.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hua Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Weikang Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1"&gt;Feng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1"&gt;Furao Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lattice-Based Minimum-Distortion Data Hiding. (arXiv:2105.13096v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13096</id>
        <link href="http://arxiv.org/abs/2105.13096"/>
        <updated>2021-06-15T01:45:14.088Z</updated>
        <summary type="html"><![CDATA[Lattices have been conceived as a powerful tool for data hiding. While
conventional studies and applications focus on achieving the optimal robustness
versus distortion tradeoff, in some applications such as data hiding in
medical/physiological signals, the primary concern is to achieve a minimum
amount of distortion to the cover signal. In this paper, we revisit the
celebrated quantization index modulation (QIM) scheme and propose a
minimum-distortion version of it, referred to as MD-QIM. The crux of MD-QIM is
to move the data point to only the boundary of the Voronoi region of the
lattice point indexed by a message, which suffices for subsequent correct
decoding. At any fixed code rate, the scheme achieves the minimum amount of
distortion by sacrificing the robustness to the additive white Gaussian noise
(AWGN) attacks. Simulation results confirm that our scheme significantly
outperforms QIM in terms of mean square error (MSE), peak signal to noise ratio
(PSNR) and percentage residual difference (PRD).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jieni Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Junren Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Shanxiang Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1"&gt;Bingwen Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiabo Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective. (arXiv:2003.00330v7 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00330</id>
        <link href="http://arxiv.org/abs/2003.00330"/>
        <updated>2021-06-15T01:45:14.041Z</updated>
        <summary type="html"><![CDATA[Neural-symbolic computing has now become the subject of interest of both
academic and industry research laboratories. Graph Neural Networks (GNN) have
been widely used in relational and symbolic domains, with widespread
application of GNNs in combinatorial optimization, constraint satisfaction,
relational reasoning and other scientific domains. The need for improved
explainability, interpretability and trust of AI systems in general demands
principled methodologies, as suggested by neural-symbolic computing. In this
paper, we review the state-of-the-art on the use of GNNs as a model of
neural-symbolic computing. This includes the application of GNNs in several
domains as well as its relationship to current developments in neural-symbolic
computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamb_L/0/1/0/all/0/1"&gt;Luis C. Lamb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcez_A/0/1/0/all/0/1"&gt;Artur Garcez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1"&gt;Marco Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prates_M/0/1/0/all/0/1"&gt;Marcelo Prates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avelar_P/0/1/0/all/0/1"&gt;Pedro Avelar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vardi_M/0/1/0/all/0/1"&gt;Moshe Vardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue. (arXiv:2101.00151v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00151</id>
        <link href="http://arxiv.org/abs/2101.00151"/>
        <updated>2021-06-15T01:45:14.016Z</updated>
        <summary type="html"><![CDATA[A video-grounded dialogue system is required to understand both dialogue,
which contains semantic dependencies from turn to turn, and video, which
contains visual cues of spatial and temporal scene variations. Building such
dialogue systems is a challenging problem, involving various reasoning types on
both visual and language inputs. Existing benchmarks do not have enough
annotations to thoroughly analyze dialogue systems and understand their
capabilities and limitations in isolation. These benchmarks are also not
explicitly designed to minimise biases that models can exploit without actual
reasoning. To address these limitations, in this paper, we present DVD, a
Diagnostic Dataset for Video-grounded Dialogues. The dataset is designed to
contain minimal biases and has detailed annotations for the different types of
reasoning over the spatio-temporal space of video. Dialogues are synthesized
over multiple question turns, each of which is injected with a set of
cross-turn semantic relationships. We use DVD to analyze existing approaches,
providing interesting insights into their abilities and limitations. In total,
DVD is built from $11k$ CATER synthetic videos and contains $10$ instances of
$10$-round dialogues for each video, resulting in more than $100k$ dialogues
and $1M$ question-answer pairs. Our code and dataset are publicly available at
https://github.com/facebookresearch/DVDialogues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hung Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1"&gt;Chinnadhurai Sankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1"&gt;Seungwhan Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1"&gt;Ahmad Beirami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1"&gt;Alborz Geramifard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1"&gt;Satwik Kottur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Effects of Linguistic Properties. (arXiv:2010.12919v5 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12919</id>
        <link href="http://arxiv.org/abs/2010.12919"/>
        <updated>2021-06-15T01:45:14.004Z</updated>
        <summary type="html"><![CDATA[We consider the problem of using observational data to estimate the causal
effects of linguistic properties. For example, does writing a complaint
politely lead to a faster response time? How much will a positive product
review increase sales? This paper addresses two technical challenges related to
the problem before developing a practical method. First, we formalize the
causal quantity of interest as the effect of a writer's intent, and establish
the assumptions necessary to identify this from observational data. Second, in
practice, we only have access to noisy proxies for the linguistic properties of
interest -- e.g., predictions from classifiers and lexicons. We propose an
estimator for this setting and prove that its bias is bounded when we perform
an adjustment for the text. Based on these results, we introduce TextCause, an
algorithm for estimating causal effects of linguistic properties. The method
leverages (1) distant supervision to improve the quality of noisy proxies, and
(2) a pre-trained language model (BERT) to adjust for the text. We show that
the proposed method outperforms related approaches when estimating the effect
of Amazon review sentiment on semi-simulated sales figures. Finally, we present
an applied case study investigating the effects of complaint politeness on
bureaucratic response times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1"&gt;Reid Pryzant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1"&gt;Dallas Card&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1"&gt;Dan Jurafsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1"&gt;Victor Veitch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridhar_D/0/1/0/all/0/1"&gt;Dhanya Sridhar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Transformer Language Models Predict Psychometric Properties?. (arXiv:2106.06849v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06849</id>
        <link href="http://arxiv.org/abs/2106.06849"/>
        <updated>2021-06-15T01:45:13.991Z</updated>
        <summary type="html"><![CDATA[Transformer-based language models (LMs) continue to advance state-of-the-art
performance on NLP benchmark tasks, including tasks designed to mimic
human-inspired "commonsense" competencies. To better understand the degree to
which LMs can be said to have certain linguistic reasoning skills, researchers
are beginning to adapt the tools and concepts of the field of psychometrics.
But to what extent can the benefits flow in the other direction? I.e., can LMs
be of use in predicting what the psychometric properties of test items will be
when those items are given to human participants? We gather responses from
numerous human participants and LMs (transformer and non-transformer-based) on
a broad diagnostic test of linguistic competencies. We then use the responses
to calculate standard psychometric properties of the items in the diagnostic
test, using the human responses and the LM responses separately. We then
determine how well these two sets of predictions match. We find cases in which
transformer-based LMs predict psychometric properties consistently well in
certain categories but consistently poorly in others, thus providing new
insights into fundamental similarities and differences between human and LM
reasoning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laverghetta_A/0/1/0/all/0/1"&gt;Antonio Laverghetta Jr.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nighojkar_A/0/1/0/all/0/1"&gt;Animesh Nighojkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirzakhalov_J/0/1/0/all/0/1"&gt;Jamshidbek Mirzakhalov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Licato_J/0/1/0/all/0/1"&gt;John Licato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InfoBehavior: Self-supervised Representation Learning for Ultra-long Behavior Sequence via Hierarchical Grouping. (arXiv:2106.06905v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06905</id>
        <link href="http://arxiv.org/abs/2106.06905"/>
        <updated>2021-06-15T01:45:13.984Z</updated>
        <summary type="html"><![CDATA[E-commerce companies have to face abnormal sellers who sell potentially-risky
products. Typically, the risk can be identified by jointly considering product
content (e.g., title and image) and seller behavior. This work focuses on
behavior feature extraction as behavior sequences can provide valuable clues
for the risk discovery by reflecting the sellers' operation habits. Traditional
feature extraction techniques heavily depend on domain experts and adapt poorly
to new tasks. In this paper, we propose a self-supervised method InfoBehavior
to automatically extract meaningful representations from ultra-long raw
behavior sequences instead of the costly feature selection procedure.
InfoBehavior utilizes Bidirectional Transformer as feature encoder due to its
excellent capability in modeling long-term dependency. However, it is
intractable for commodity GPUs because the time and memory required by
Transformer grow quadratically with the increase of sequence length. Thus, we
propose a hierarchical grouping strategy to aggregate ultra-long raw behavior
sequences to length-processable high-level embedding sequences. Moreover, we
introduce two types of pretext tasks. Sequence-related pretext task defines a
contrastive-based training objective to correctly select the masked-out
coarse-grained/fine-grained behavior sequences against other "distractor"
behavior sequences; Domain-related pretext task designs a classification
training objective to correctly predict the domain-specific statistical results
of anomalous behavior. We show that behavior representations from the
pre-trained InfoBehavior can be directly used or integrated with features from
other side information to support a wide range of downstream tasks.
Experimental results demonstrate that InfoBehavior significantly improves the
performance of Product Risk Management and Intellectual Property Protection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Runshi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_P/0/1/0/all/0/1"&gt;Pengda Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1"&gt;Weigao Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1"&gt;Kefeng Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qiang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining the Deep Natural Language Processing by Mining Textual Interpretable Features. (arXiv:2106.06697v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06697</id>
        <link href="http://arxiv.org/abs/2106.06697"/>
        <updated>2021-06-15T01:45:13.965Z</updated>
        <summary type="html"><![CDATA[Despite the high accuracy offered by state-of-the-art deep natural-language
models (e.g. LSTM, BERT), their application in real-life settings is still
widely limited, as they behave like a black-box to the end-user. Hence,
explainability is rapidly becoming a fundamental requirement of
future-generation data-driven systems based on deep-learning approaches.
Several attempts to fulfill the existing gap between accuracy and
interpretability have been done. However, robust and specialized xAI
(Explainable Artificial Intelligence) solutions tailored to deep
natural-language models are still missing. We propose a new framework, named
T-EBAnO, which provides innovative prediction-local and class-based
model-global explanation strategies tailored to black-box deep natural-language
models. Given a deep NLP model and the textual input data, T-EBAnO provides an
objective, human-readable, domain-specific assessment of the reasons behind the
automatic decision-making process. Specifically, the framework extracts sets of
interpretable features mining the inner knowledge of the model. Then, it
quantifies the influence of each feature during the prediction process by
exploiting the novel normalized Perturbation Influence Relation index at the
local level and the novel Global Absolute Influence and Global Relative
Influence indexes at the global level. The effectiveness and the quality of the
local and global explanations obtained with T-EBAnO are proved on (i) a
sentiment analysis task performed by a fine-tuned BERT model, and (ii) a toxic
comment classification task performed by an LSTM model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ventura_F/0/1/0/all/0/1"&gt;Francesco Ventura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greco_S/0/1/0/all/0/1"&gt;Salvatore Greco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Apiletti_D/0/1/0/all/0/1"&gt;Daniele Apiletti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cerquitelli_T/0/1/0/all/0/1"&gt;Tania Cerquitelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP. (arXiv:2106.06830v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06830</id>
        <link href="http://arxiv.org/abs/2106.06830"/>
        <updated>2021-06-15T01:45:13.954Z</updated>
        <summary type="html"><![CDATA[Retrieval is a core component for open-domain NLP tasks. In open-domain
tasks, multiple entities can share a name, making disambiguation an inherent
yet under-explored problem. We propose an evaluation benchmark for assessing
the entity disambiguation capabilities of these retrievers, which we call
Ambiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection
of entities that share a name along with queries about those entities. By
covering the set of entities for polysemous names, AmbER sets act as a
challenging test of entity disambiguation. We create AmbER sets for three
popular open-domain tasks: fact checking, slot filling, and question answering,
and evaluate a diverse set of retrievers. We find that the retrievers exhibit
popularity bias, significantly under-performing on rarer entities that share a
name, e.g., they are twice as likely to retrieve erroneous documents on queries
for the less popular entity under the same name. These experiments on AmbER
sets show their utility as an evaluation tool and highlight the weaknesses of
popular retrieval systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Anthony Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gudipati_P/0/1/0/all/0/1"&gt;Pallavi Gudipati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1"&gt;Shayne Longpre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1"&gt;Xiao Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiWOZ 2.3: A multi-domain task-oriented dialogue dataset enhanced with annotation corrections and co-reference annotation. (arXiv:2010.05594v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05594</id>
        <link href="http://arxiv.org/abs/2010.05594"/>
        <updated>2021-06-15T01:45:13.945Z</updated>
        <summary type="html"><![CDATA[Task-oriented dialogue systems have made unprecedented progress with multiple
state-of-the-art (SOTA) models underpinned by a number of publicly available
MultiWOZ datasets. Dialogue state annotations are error-prone, leading to
sub-optimal performance. Various efforts have been put in rectifying the
annotation errors presented in the original MultiWOZ dataset. In this paper, we
introduce MultiWOZ 2.3, in which we differentiate incorrect annotations in
dialogue acts from dialogue states, identifying a lack of co-reference when
publishing the updated dataset. To ensure consistency between dialogue acts and
dialogue states, we implement co-reference features and unify annotations of
dialogue acts and dialogue states. We update the state of the art performance
of natural language understanding and dialogue state tracking on MultiWOZ 2.3,
where the results show significant improvements than on previous versions of
MultiWOZ datasets (2.0-2.2).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1"&gt;Ting Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Ximing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1"&gt;Ryuichi Takanobu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1"&gt;Yixin Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chongxuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1"&gt;Dazhen Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direct Simultaneous Speech-to-Text Translation Assisted by Synchronized Streaming ASR. (arXiv:2106.06636v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06636</id>
        <link href="http://arxiv.org/abs/2106.06636"/>
        <updated>2021-06-15T01:45:13.937Z</updated>
        <summary type="html"><![CDATA[Simultaneous speech-to-text translation is widely useful in many scenarios.
The conventional cascaded approach uses a pipeline of streaming ASR followed by
simultaneous MT, but suffers from error propagation and extra latency. To
alleviate these issues, recent efforts attempt to directly translate the source
speech into target text simultaneously, but this is much harder due to the
combination of two separate tasks. We instead propose a new paradigm with the
advantages of both cascaded and end-to-end approaches. The key idea is to use
two separate, but synchronized, decoders on streaming ASR and direct
speech-to-text translation (ST), respectively, and the intermediate results of
ASR guide the decoding policy of (but is not fed as input to) ST. During
training time, we use multitask learning to jointly learn these two tasks with
a shared encoder. En-to-De and En-to-Es experiments on the MuSTC dataset
demonstrate that our proposed technique achieves substantially better
translation quality at similar levels of latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Junkun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Mingbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Renjie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Liang Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Controllable Model of Grounded Response Generation. (arXiv:2005.00613v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00613</id>
        <link href="http://arxiv.org/abs/2005.00613"/>
        <updated>2021-06-15T01:45:13.922Z</updated>
        <summary type="html"><![CDATA[Current end-to-end neural conversation models inherently lack the flexibility
to impose semantic control in the response generation process, often resulting
in uninteresting responses. Attempts to boost informativeness alone come at the
expense of factual accuracy, as attested by pretrained language models'
propensity to "hallucinate" facts. While this may be mitigated by access to
background knowledge, there is scant guarantee of relevance and informativeness
in generated responses. We propose a framework that we call controllable
grounded response generation (CGRG), in which lexical control phrases are
either provided by a user or automatically extracted by a control phrase
predictor from dialogue context and grounding knowledge. Quantitative and
qualitative results show that, using this framework, a transformer based model
with a novel inductive attention mechanism, trained on a conversation-like
Reddit dataset, outperforms strong generation baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zeqiu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1"&gt;Michel Galley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockett_C/0/1/0/all/0/1"&gt;Chris Brockett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yizhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quirk_C/0/1/0/all/0/1"&gt;Chris Quirk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1"&gt;Rik Koncel-Kedziorski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1"&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1"&gt;Mari Ostendorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1"&gt;Bill Dolan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Ordering of Characters in Japanese Historical Documents. (arXiv:2106.06786v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06786</id>
        <link href="http://arxiv.org/abs/2106.06786"/>
        <updated>2021-06-15T01:45:13.898Z</updated>
        <summary type="html"><![CDATA[Japan is a unique country with a distinct cultural heritage, which is
reflected in billions of historical documents that have been preserved.
However, the change in Japanese writing system in 1900 made these documents
inaccessible for the general public. A major research project has been to make
these historical documents accessible and understandable. An increasing amount
of research has focused on the character recognition task and the location of
characters on image, yet less research has focused on how to predict the
sequential ordering of the characters. This is because sequence in classical
Japanese is very different from modern Japanese. Ordering characters into a
sequence is important for making the document text easily readable and
searchable. Additionally, it is a necessary step for any kind of natural
language processing on the data (e.g. machine translation, language modeling,
and word embeddings). We explore a few approaches to the task of predicting the
sequential ordering of the characters: one using simple hand-crafted rules,
another using hand-crafted rules with adaptive thresholds, and another using a
deep recurrent sequence model trained with teacher forcing. We provide a
quantitative and qualitative comparison of these techniques as well as their
distinct trade-offs. Our best-performing system has an accuracy of 98.65\% and
has a perfect accuracy on 49\% of the books in our dataset, suggesting that the
technique is able to predict the order of the characters well enough for many
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamb_A/0/1/0/all/0/1"&gt;Alex Lamb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clanuwat_T/0/1/0/all/0/1"&gt;Tarin Clanuwat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Siyu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bober_Irizar_M/0/1/0/all/0/1"&gt;Mikel Bober-Irizar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamoto_A/0/1/0/all/0/1"&gt;Asanobu Kitamoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compression of Deep Learning Models for Text: A Survey. (arXiv:2008.05221v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05221</id>
        <link href="http://arxiv.org/abs/2008.05221"/>
        <updated>2021-06-15T01:45:13.888Z</updated>
        <summary type="html"><![CDATA[In recent years, the fields of natural language processing (NLP) and
information retrieval (IR) have made tremendous progress thanksto deep learning
models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and
Long Short-Term Memory (LSTMs)networks, and Transformer [120] based models like
Bidirectional Encoder Representations from Transformers (BERT) [24],
GenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network
(MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer
transformer (T5) [95], T-NLG [98] and GShard [63]. But these models are
humongous in size. On the other hand,real world applications demand small model
size, low response times and low computational power wattage. In this survey,
wediscuss six different types of methods (Pruning, Quantization, Knowledge
Distillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic
Transformer based methods) for compression of such models to enable their
deployment in real industry NLP projects.Given the critical need of building
applications with efficient and small models, and the large amount of recently
published work inthis area, we believe that this survey organizes the plethora
of work done by the 'deep learning for NLP' community in the past fewyears and
presents it as a coherent story.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Manish Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1"&gt;Puneet Agrawal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. (arXiv:2106.06909v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.06909</id>
        <link href="http://arxiv.org/abs/2106.06909"/>
        <updated>2021-06-15T01:45:13.880Z</updated>
        <summary type="html"><![CDATA[This paper introduces GigaSpeech, an evolving, multi-domain English speech
recognition corpus with 10,000 hours of high quality labeled audio suitable for
supervised training, and 40,000 hours of total audio suitable for
semi-supervised and unsupervised training. Around 40,000 hours of transcribed
audio is first collected from audiobooks, podcasts and YouTube, covering both
read and spontaneous speaking styles, and a variety of topics, such as arts,
science, sports, etc. A new forced alignment and segmentation pipeline is
proposed to create sentence segments suitable for speech recognition training,
and to filter out segments with low-quality transcription. For system training,
GigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h,
and 10000h. For our 10,000-hour XL training subset, we cap the word error rate
at 4% during the filtering/validation stage, and for all our other smaller
training subsets, we cap it at 0%. The DEV and TEST evaluation sets, on the
other hand, are re-processed by professional human transcribers to ensure high
transcription quality. Baseline systems are provided for popular speech
recognition toolkits, namely Athena, ESPnet, Kaldi and Pika.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guoguo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_S/0/1/0/all/0/1"&gt;Shuzhou Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1"&gt;Jiayu Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei-Qiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1"&gt;Chao Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1"&gt;Dan Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Povey_D/0/1/0/all/0/1"&gt;Daniel Povey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trmal_J/0/1/0/all/0/1"&gt;Jan Trmal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junbo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1"&gt;Mingjie Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khudanpur_S/0/1/0/all/0/1"&gt;Sanjeev Khudanpur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuaijiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1"&gt;Wei Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiangang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xuchen Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yujun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1"&gt;Zhao You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhiyong Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Pre-trained Language Model for Speech Sentiment Analysis. (arXiv:2106.06598v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06598</id>
        <link href="http://arxiv.org/abs/2106.06598"/>
        <updated>2021-06-15T01:45:13.854Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore the use of pre-trained language models to learn
sentiment information of written texts for speech sentiment analysis. First, we
investigate how useful a pre-trained language model would be in a 2-step
pipeline approach employing Automatic Speech Recognition (ASR) and
transcripts-based sentiment analysis separately. Second, we propose a pseudo
label-based semi-supervised training strategy using a language model on an
end-to-end speech sentiment approach to take advantage of a large, but
unlabeled speech dataset for training. Although spoken and written texts have
different linguistic characteristics, they can complement each other in
understanding sentiment. Therefore, the proposed system can not only model
acoustic characteristics to bear sentiment-specific information in speech
signals, but learn latent information to carry sentiments in the text
representation. In these experiments, we demonstrate the proposed approaches
improve F1 scores consistently compared to systems without a language model.
Moreover, we also show that the proposed framework can reduce 65% of human
supervision by leveraging a large amount of data without human sentiment
annotation and boost performance in a low-resource condition where the human
sentiment annotation is not available enough.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shon_S/0/1/0/all/0/1"&gt;Suwon Shon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brusco_P/0/1/0/all/0/1"&gt;Pablo Brusco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1"&gt;Jing Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kyu J. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample-efficient Linguistic Generalizations through Program Synthesis: Experiments with Phonology Problems. (arXiv:2106.06566v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06566</id>
        <link href="http://arxiv.org/abs/2106.06566"/>
        <updated>2021-06-15T01:45:13.785Z</updated>
        <summary type="html"><![CDATA[Neural models excel at extracting statistical patterns from large amounts of
data, but struggle to learn patterns or reason about language from only a few
examples. In this paper, we ask: Can we learn explicit rules that generalize
well from only a few examples? We explore this question using program
synthesis. We develop a synthesis model to learn phonology rules as programs in
a domain-specific language. We test the ability of our models to generalize
from few training examples using our new dataset of problems from the
Linguistics Olympiad, a challenging set of tasks that require strong linguistic
reasoning ability. In addition to being highly sample-efficient, our approach
generates human-readable programs, and allows control over the generalizability
of the learnt programs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaduguru_S/0/1/0/all/0/1"&gt;Saujas Vaduguru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sathe_A/0/1/0/all/0/1"&gt;Aalok Sathe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1"&gt;Monojit Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1"&gt;Dipti Misra Sharma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape of Elephant: Study of Macro Properties of Word Embeddings Spaces. (arXiv:2106.06964v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06964</id>
        <link href="http://arxiv.org/abs/2106.06964"/>
        <updated>2021-06-15T01:45:13.760Z</updated>
        <summary type="html"><![CDATA[Pre-trained word representations became a key component in many NLP tasks.
However, the global geometry of the word embeddings remains poorly understood.
In this paper, we demonstrate that a typical word embeddings cloud is shaped as
a high-dimensional simplex with interpretable vertices and propose a simple yet
effective method for enumeration of these vertices. We show that the proposed
method can detect and describe vertices of the simplex for GloVe and fasttext
spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1"&gt;Alexey Tikhonov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Summary Knowledge Graphs from Long Documents. (arXiv:2009.09162v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09162</id>
        <link href="http://arxiv.org/abs/2009.09162"/>
        <updated>2021-06-15T01:45:13.751Z</updated>
        <summary type="html"><![CDATA[Knowledge graphs capture entities and relations from long documents and can
facilitate reasoning in many downstream applications. Extracting compact
knowledge graphs containing only salient entities and relations is important
but challenging for understanding and summarizing long documents. We introduce
a new text-to-graph task of predicting summarized knowledge graphs from long
documents. We develop a dataset of 200k document/graph pairs using automatic
and human annotations. We also develop strong baselines for this task based on
graph learning and text summarization, and provide quantitative and qualitative
studies of their effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zeqiu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1"&gt;Rik Koncel-Kedziorski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1"&gt;Mari Ostendorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1"&gt;Hannaneh Hajishirzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing Multilingual Fairness in Pre-trained Multimodal Representations. (arXiv:2106.06683v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06683</id>
        <link href="http://arxiv.org/abs/2106.06683"/>
        <updated>2021-06-15T01:45:13.490Z</updated>
        <summary type="html"><![CDATA[Recently pre-trained multimodal models, such as CLIP, have received a surge
of attention for their exceptional capabilities towards connecting images and
natural language. The textual representations in English can be desirably
transferred to multilingualism and support promising downstream multimodal
tasks for different languages. Nevertheless, previous fairness discourse in
vision-and-language learning mainly focuses on monolingual representational
biases, and rarely scrutinizes the principles of multilingual fairness in this
multimodal setting, where one language is equated to a group of individuals and
images provide the universal grounding for bridging different languages.

In this paper, we provide a nuanced understanding of individual fairness and
group fairness by viewing language as the recipient of fairness notions. We
define new fairness notions within multilingual context and analytically
articulate that, pre-trained vision-and-language representations are
individually fair across languages but not guaranteed to group fairness.
Furthermore, we conduct extensive experiments to explore the prevalent group
disparity across languages and protected groups including race, gender and age.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jialu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Eric Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Socially-Aware Self-Supervised Tri-Training for Recommendation. (arXiv:2106.03569v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03569</id>
        <link href="http://arxiv.org/abs/2106.03569"/>
        <updated>2021-06-15T01:45:13.457Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (SSL), which can automatically generate ground-truth
samples from raw data, holds vast potential to improve recommender systems.
Most existing SSL-based methods perturb the raw data graph with uniform
node/edge dropout to generate new data views and then conduct the
self-discrimination based contrastive learning over different views to learn
generalizable representations. Under this scheme, only a bijective mapping is
built between nodes in two different views, which means that the
self-supervision signals from other nodes are being neglected. Due to the
widely observed homophily in recommender systems, we argue that the supervisory
signals from other nodes are also highly likely to benefit the representation
learning for recommendation. To capture these signals, a general socially-aware
SSL framework that integrates tri-training is proposed in this paper.
Technically, our framework first augments the user data views with the user
social information. And then under the regime of tri-training for multi-view
encoding, the framework builds three graph encoders (one for recommendation)
upon the augmented views and iteratively improves each encoder with
self-supervision signals from other users, generated by the other two encoders.
Since the tri-training operates on the augmented views of the same data sources
for self-supervision signals, we name it self-supervised tri-training.
Extensive experiments on multiple real-world datasets consistently validate the
effectiveness of the self-supervised tri-training framework for improving
recommendation. The code is released at https://github.com/Coder-Yu/QRec.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Junliang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1"&gt;Min Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1"&gt;Xin Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangliang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hung_N/0/1/0/all/0/1"&gt;Nguyen Quoc Viet Hung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentiment Analysis of Covid-19 Tweets using Evolutionary Classification-Based LSTM Model. (arXiv:2106.06910v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06910</id>
        <link href="http://arxiv.org/abs/2106.06910"/>
        <updated>2021-06-15T01:45:13.433Z</updated>
        <summary type="html"><![CDATA[As the Covid-19 outbreaks rapidly all over the world day by day and also
affects the lives of million, a number of countries declared complete lock-down
to check its intensity. During this lockdown period, social media plat-forms
have played an important role to spread information about this pandemic across
the world, as people used to express their feelings through the social
networks. Considering this catastrophic situation, we developed an experimental
approach to analyze the reactions of people on Twitter taking into ac-count the
popular words either directly or indirectly based on this pandemic. This paper
represents the sentiment analysis on collected large number of tweets on
Coronavirus or Covid-19. At first, we analyze the trend of public sentiment on
the topics related to Covid-19 epidemic using an evolutionary classification
followed by the n-gram analysis. Then we calculated the sentiment ratings on
collected tweet based on their class. Finally, we trained the long-short term
network using two types of rated tweets to predict sentiment on Covid-19 data
and obtained an overall accuracy of 84.46%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1"&gt;Arunava Kumar Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Sourav Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolya_A/0/1/0/all/0/1"&gt;Anup Kumar Kolya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-level Attention Fusion Network for Audio-visual Event Recognition. (arXiv:2106.06736v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06736</id>
        <link href="http://arxiv.org/abs/2106.06736"/>
        <updated>2021-06-15T01:45:13.424Z</updated>
        <summary type="html"><![CDATA[Event classification is inherently sequential and multimodal. Therefore, deep
neural models need to dynamically focus on the most relevant time window and/or
modality of a video. In this study, we propose the Multi-level Attention Fusion
network (MAFnet), an architecture that can dynamically fuse visual and audio
information for event recognition. Inspired by prior studies in neuroscience,
we couple both modalities at different levels of visual and audio paths.
Furthermore, the network dynamically highlights a modality at a given time
window relevant to classify events. Experimental results in AVE (Audio-Visual
Event), UCF51, and Kinetics-Sounds datasets show that the approach can
effectively improve the accuracy in audio-visual event classification. Code is
available at: https://github.com/numediart/MAFnet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brousmiche_M/0/1/0/all/0/1"&gt;Mathilde Brousmiche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1"&gt;Jean Rouat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dupont_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Dupont&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Break-It-Fix-It: Unsupervised Learning for Program Repair. (arXiv:2106.06600v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06600</id>
        <link href="http://arxiv.org/abs/2106.06600"/>
        <updated>2021-06-15T01:45:13.411Z</updated>
        <summary type="html"><![CDATA[We consider repair tasks: given a critic (e.g., compiler) that assesses the
quality of an input, the goal is to train a fixer that converts a bad example
(e.g., code with syntax errors) into a good one (e.g., code with no errors).
Existing works create training data consisting of (bad, good) pairs by
corrupting good examples using heuristics (e.g., dropping tokens). However,
fixers trained on this synthetically-generated data do not extrapolate well to
the real distribution of bad inputs. To bridge this gap, we propose a new
training approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use
the critic to check a fixer's output on real bad inputs and add good (fixed)
outputs to the training data, and (ii) we train a breaker to generate realistic
bad code from good code. Based on these ideas, we iteratively update the
breaker and the fixer while using them in conjunction to generate more paired
data. We evaluate BIFI on two code repair datasets: GitHub-Python, a new
dataset we introduce where the goal is to repair Python code with AST parse
errors; and DeepFix, where the goal is to repair C code with compiler errors.
BIFI outperforms existing methods, obtaining 90.5% repair accuracy on
GitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not
require any labeled data; we hope it will be a strong starting point for
unsupervised learning of various repair tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1"&gt;Michihiro Yasunaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Combinatory Constituency Parsing. (arXiv:2106.06689v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06689</id>
        <link href="http://arxiv.org/abs/2106.06689"/>
        <updated>2021-06-15T01:45:13.402Z</updated>
        <summary type="html"><![CDATA[We propose two fast neural combinatory models for constituency parsing:
binary and multi-branching. Our models decompose the bottom-up parsing process
into 1) classification of tags, labels, and binary orientations or chunks and
2) vector composition based on the computed orientations or chunks. These
models have theoretical sub-quadratic complexity and empirical linear
complexity. The binary model achieves an F1 score of 92.54 on Penn Treebank,
speeding at 1327.2 sents/sec. Both the models with XLNet provide near
state-of-the-art accuracies for English. Syntactic branching tendency and
headedness of a language are observed during the training and inference
processes for Penn Treebank, Chinese Treebank, and Keyaki Treebank (Japanese).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhousi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Longtu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imankulova_A/0/1/0/all/0/1"&gt;Aizhan Imankulova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Komachi_M/0/1/0/all/0/1"&gt;Mamoru Komachi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning based Group Recommender System. (arXiv:2106.06900v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.06900</id>
        <link href="http://arxiv.org/abs/2106.06900"/>
        <updated>2021-06-15T01:45:13.386Z</updated>
        <summary type="html"><![CDATA[Group recommender systems are widely used in current web applications. In
this paper, we propose a novel group recommender system based on the deep
reinforcement learning. We introduce the MovieLens data at first and generate
one random group dataset, MovieLens-Rand, from it. This randomly generated
dataset is described and analyzed. We also present experimental settings and
two state-of-art baselines, AGREE and GroupIM. The framework of our novel
model, the Deep Reinforcement learning based Group Recommender system (DRGR),
is proposed. Actor-critic networks are implemented with the deep deterministic
policy gradient algorithm. The DRGR model is applied on the MovieLens-Rand
dataset with two baselines. Compared with baselines, we conclude that DRGR
performs better than GroupIM due to long interaction histories but worse than
AGREE because of the self-attention mechanism. We express advantages and
shortcomings of DRGR and also give future improvement directions at the end.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zefang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1"&gt;Shuran Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_Y/0/1/0/all/0/1"&gt;Yinzhu Quan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Engineering Knowledge Graph from Patent Database. (arXiv:2106.06739v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.06739</id>
        <link href="http://arxiv.org/abs/2106.06739"/>
        <updated>2021-06-15T01:45:13.365Z</updated>
        <summary type="html"><![CDATA[We propose a large, scalable engineering knowledge graph, comprising sets of
(entity, relationship, entity) triples that are real-world engineering facts
found in the patent database. We apply a set of rules based on the syntactic
and lexical properties of claims in a patent document to extract facts. We
aggregate these facts within each patent document and integrate the aggregated
sets of facts across the patent database to obtain the engineering knowledge
graph. Such a knowledge graph is expected to support inference, reasoning, and
recalling in various engineering tasks. The knowledge graph has a greater size
and coverage in comparison with the previously used knowledge graphs and
semantic networks in the engineering literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siddharth_L/0/1/0/all/0/1"&gt;L Siddharth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1"&gt;Lucienne T.M. Blessing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wood_K/0/1/0/all/0/1"&gt;Kristin L. Wood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jianxi Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-sentence Neural Language Models for Conversational Speech Recognition. (arXiv:2106.06922v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06922</id>
        <link href="http://arxiv.org/abs/2106.06922"/>
        <updated>2021-06-15T01:45:13.356Z</updated>
        <summary type="html"><![CDATA[An important research direction in automatic speech recognition (ASR) has
centered around the development of effective methods to rerank the output
hypotheses of an ASR system with more sophisticated language models (LMs) for
further gains. A current mainstream school of thoughts for ASR N-best
hypothesis reranking is to employ a recurrent neural network (RNN)-based LM or
its variants, with performance superiority over the conventional n-gram LMs
across a range of ASR tasks. In real scenarios such as a long conversation, a
sequence of consecutive sentences may jointly contain ample cues of
conversation-level information such as topical coherence, lexical entrainment
and adjacency pairs, which however remains to be underexplored. In view of
this, we first formulate ASR N-best reranking as a prediction problem, putting
forward an effective cross-sentence neural LM approach that reranks the ASR
N-best hypotheses of an upcoming sentence by taking into consideration the word
usage in its precedent sentences. Furthermore, we also explore to extract
task-specific global topical information of the cross-sentence history in an
unsupervised manner for better ASR performance. Extensive experiments conducted
on the AMI conversational benchmark corpus indicate the effectiveness and
feasibility of our methods in comparison to several state-of-the-art reranking
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1"&gt;Shih-Hsuan Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_T/0/1/0/all/0/1"&gt;Tien-Hong Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Berlin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Don't Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine Translation Data. (arXiv:2106.06875v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06875</id>
        <link href="http://arxiv.org/abs/2106.06875"/>
        <updated>2021-06-15T01:45:13.316Z</updated>
        <summary type="html"><![CDATA[High-performing machine translation (MT) systems can help overcome language
barriers while making it possible for everyone to communicate and use language
technologies in the language of their choice. However, such systems require
large amounts of parallel sentences for training, and translators can be
difficult to find and expensive. Here, we present a data collection strategy
for MT which, in contrast, is cheap and simple, as it does not require
bilingual speakers. Based on the insight that humans pay specific attention to
movements, we use graphics interchange formats (GIFs) as a pivot to collect
parallel sentences from monolingual annotators. We use our strategy to collect
data in Hindi, Tamil and English. As a baseline, we also collect data using
images as a pivot. We perform an intrinsic evaluation by manually evaluating a
subset of the sentence pairs and an extrinsic evaluation by finetuning mBART on
the collected data. We find that sentences collected via GIFs are indeed of
higher quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatnagar_R/0/1/0/all/0/1"&gt;Rajat Bhatnagar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesh_A/0/1/0/all/0/1"&gt;Ananya Ganesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1"&gt;Katharina Kann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Every Bite Is an Experience: Key Point Analysis of Business Reviews. (arXiv:2106.06758v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06758</id>
        <link href="http://arxiv.org/abs/2106.06758"/>
        <updated>2021-06-15T01:45:13.307Z</updated>
        <summary type="html"><![CDATA[Previous work on review summarization focused on measuring the sentiment
toward the main aspects of the reviewed product or business, or on creating a
textual summary. These approaches provide only a partial view of the data:
aspect-based sentiment summaries lack sufficient explanation or justification
for the aspect rating, while textual summaries do not quantify the significance
of each element, and are not well-suited for representing conflicting views.
Recently, Key Point Analysis (KPA) has been proposed as a summarization
framework that provides both textual and quantitative summary of the main
points in the data. We adapt KPA to review data by introducing Collective Key
Point Mining for better key point extraction; integrating sentiment analysis
into KPA; identifying good key point candidates for review summaries; and
leveraging the massive amount of available reviews and their metadata. We show
empirically that these novel extensions of KPA substantially improve its
performance. We demonstrate that promising results can be achieved without any
domain-specific annotation, while human supervision can lead to further
improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bar_Haim_R/0/1/0/all/0/1"&gt;Roy Bar-Haim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eden_L/0/1/0/all/0/1"&gt;Lilach Eden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kantor_Y/0/1/0/all/0/1"&gt;Yoav Kantor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedman_R/0/1/0/all/0/1"&gt;Roni Friedman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1"&gt;Noam Slonim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating External POS Tagger for Punctuation Restoration. (arXiv:2106.06731v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06731</id>
        <link href="http://arxiv.org/abs/2106.06731"/>
        <updated>2021-06-15T01:45:13.271Z</updated>
        <summary type="html"><![CDATA[Punctuation restoration is an important post-processing step in automatic
speech recognition. Among other kinds of external information, part-of-speech
(POS) taggers provide informative tags, suggesting each input token's syntactic
role, which has been shown to be beneficial for the punctuation restoration
task. In this work, we incorporate an external POS tagger and fuse its
predicted labels into the existing language model to provide syntactic
information. Besides, we propose sequence boundary sampling (SBS) to learn
punctuation positions more efficiently as a sequence tagging task. Experimental
results show that our methods can consistently obtain performance gains and
achieve a new state-of-the-art on the common IWSLT benchmark. Further ablation
studies illustrate that both large pre-trained language models and the external
POS tagger take essential parts to improve the model's performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1"&gt;Ning Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Boxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinfeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouhan Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Pseudo Label-wise Attention Network for Automatic ICD Coding. (arXiv:2106.06822v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06822</id>
        <link href="http://arxiv.org/abs/2106.06822"/>
        <updated>2021-06-15T01:45:13.259Z</updated>
        <summary type="html"><![CDATA[Automatic International Classification of Diseases (ICD) coding is defined as
a kind of text multi-label classification problem, which is difficult because
the number of labels is very large and the distribution of labels is
unbalanced. The label-wise attention mechanism is widely used in automatic ICD
coding because it can assign weights to every word in full Electronic Medical
Records (EMR) for different ICD codes. However, the label-wise attention
mechanism is computational redundant and costly. In this paper, we propose a
pseudo label-wise attention mechanism to tackle the problem. Instead of
computing different attention modes for different ICD codes, the pseudo
label-wise attention mechanism automatically merges similar ICD codes and
computes only one attention mode for the similar ICD codes, which greatly
compresses the number of attention modes and improves the predicted accuracy.
In addition, we apply a more convenient and effective way to obtain the ICD
vectors, and thus our model can predict new ICD codes by calculating the
similarities between EMR vectors and ICD vectors. Extensive experiments show
the superior performance of our model. On the public MIMIC-III dataset and
private Xiangya dataset, our model achieves micro f1 of 0.575 and 0.796,
respectively, which outperforms other competing models. Furthermore, we verify
the ability of our model in predicting new ICD codes. The case study shows how
pseudo label-wise attention works, and demonstrates the effectiveness of pseudo
label-wise attention mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yifan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1"&gt;Min Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Ying Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Min Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Unsupervised Dialogue Topic Segmentation with Utterance-Pair Coherence Scoring. (arXiv:2106.06719v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06719</id>
        <link href="http://arxiv.org/abs/2106.06719"/>
        <updated>2021-06-15T01:45:13.245Z</updated>
        <summary type="html"><![CDATA[Dialogue topic segmentation is critical in several dialogue modeling
problems. However, popular unsupervised approaches only exploit surface
features in assessing topical coherence among utterances. In this work, we
address this limitation by leveraging supervisory signals from the
utterance-pair coherence scoring task. First, we present a simple yet effective
strategy to generate a training corpus for utterance-pair coherence scoring.
Then, we train a BERT-based neural utterance-pair coherence model with the
obtained training corpus. Finally, such model is used to measure the topical
relevance between utterances, acting as the basis of the segmentation
inference. Experiments on three public datasets in English and Chinese
demonstrate that our proposal outperforms the state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1"&gt;Linzi Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1"&gt;Giuseppe Carenini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Sentence-level Hierarchical BERT Model for Document Classification with Limited Labelled Data. (arXiv:2106.06738v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06738</id>
        <link href="http://arxiv.org/abs/2106.06738"/>
        <updated>2021-06-15T01:45:13.232Z</updated>
        <summary type="html"><![CDATA[Training deep learning models with limited labelled data is an attractive
scenario for many NLP tasks, including document classification. While with the
recent emergence of BERT, deep learning language models can achieve reasonably
good performance in document classification with few labelled instances, there
is a lack of evidence in the utility of applying BERT-like models on long
document classification. This work introduces a long-text-specific model -- the
Hierarchical BERT Model (HBM) -- that learns sentence-level features of the
text and works well in scenarios with limited labelled data. Various evaluation
experiments have demonstrated that HBM can achieve higher performance in
document classification than the previous state-of-the-art methods with only 50
to 200 labelled instances, especially when documents are long. Also, as an
extra benefit of HBM, the salient sentences identified by learned HBM are
useful as explanations for labelling documents based on a user study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jinghui Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henchion_M/0/1/0/all/0/1"&gt;Maeve Henchion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacher_I/0/1/0/all/0/1"&gt;Ivan Bacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1"&gt;Brian Mac Namee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentiment Analysis of Covid-19 Tweets using Evolutionary Classification-Based LSTM Model. (arXiv:2106.06910v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06910</id>
        <link href="http://arxiv.org/abs/2106.06910"/>
        <updated>2021-06-15T01:45:13.169Z</updated>
        <summary type="html"><![CDATA[As the Covid-19 outbreaks rapidly all over the world day by day and also
affects the lives of million, a number of countries declared complete lock-down
to check its intensity. During this lockdown period, social media plat-forms
have played an important role to spread information about this pandemic across
the world, as people used to express their feelings through the social
networks. Considering this catastrophic situation, we developed an experimental
approach to analyze the reactions of people on Twitter taking into ac-count the
popular words either directly or indirectly based on this pandemic. This paper
represents the sentiment analysis on collected large number of tweets on
Coronavirus or Covid-19. At first, we analyze the trend of public sentiment on
the topics related to Covid-19 epidemic using an evolutionary classification
followed by the n-gram analysis. Then we calculated the sentiment ratings on
collected tweet based on their class. Finally, we trained the long-short term
network using two types of rated tweets to predict sentiment on Covid-19 data
and obtained an overall accuracy of 84.46%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1"&gt;Arunava Kumar Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Sourav Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolya_A/0/1/0/all/0/1"&gt;Anup Kumar Kolya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CHECKED: Chinese COVID-19 Fake News Dataset. (arXiv:2010.09029v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09029</id>
        <link href="http://arxiv.org/abs/2010.09029"/>
        <updated>2021-06-15T01:45:13.134Z</updated>
        <summary type="html"><![CDATA[COVID-19 has impacted all lives. To maintain social distancing and avoiding
exposure, works and lives have gradually moved online. Under this trend, social
media usage to obtain COVID-19 news has increased. Also, misinformation on
COVID-19 is frequently spread on social media. In this work, we develop
CHECKED, the first Chinese dataset on COVID-19 misinformation. CHECKED provides
a total 2,104 verified microblogs related to COVID-19 from December 2019 to
August 2020, identified by using a specific list of keywords. Correspondingly,
CHECKED includes 1,868,175 reposts, 1,185,702 comments, and 56,852,736 likes
that reveal how these verified microblogs are spread and reacted on Weibo. The
dataset contains a rich set of multimedia information for each microblog
including ground-truth label, textual, visual, temporal, and network
information. Extensive experiments have been conducted to analyze CHECKED data
and to provide benchmark results for well-established methods when predicting
fake news using CHECKED. We hope that CHECKED can facilitate studies that
target misinformation on coronavirus. The dataset is available at
https://github.com/cyang03/CHECKED.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xinyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafarani_R/0/1/0/all/0/1"&gt;Reza Zafarani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Engineering Knowledge Graph from Patent Database. (arXiv:2106.06739v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.06739</id>
        <link href="http://arxiv.org/abs/2106.06739"/>
        <updated>2021-06-15T01:45:13.110Z</updated>
        <summary type="html"><![CDATA[We propose a large, scalable engineering knowledge graph, comprising sets of
(entity, relationship, entity) triples that are real-world engineering facts
found in the patent database. We apply a set of rules based on the syntactic
and lexical properties of claims in a patent document to extract facts. We
aggregate these facts within each patent document and integrate the aggregated
sets of facts across the patent database to obtain the engineering knowledge
graph. Such a knowledge graph is expected to support inference, reasoning, and
recalling in various engineering tasks. The knowledge graph has a greater size
and coverage in comparison with the previously used knowledge graphs and
semantic networks in the engineering literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siddharth_L/0/1/0/all/0/1"&gt;L Siddharth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1"&gt;Lucienne T.M. Blessing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wood_K/0/1/0/all/0/1"&gt;Kristin L. Wood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jianxi Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curriculum Pre-Training Heterogeneous Subgraph Transformer for Top-$N$ Recommendation. (arXiv:2106.06722v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.06722</id>
        <link href="http://arxiv.org/abs/2106.06722"/>
        <updated>2021-06-15T01:45:12.267Z</updated>
        <summary type="html"><![CDATA[Due to the flexibility in modelling data heterogeneity, heterogeneous
information network (HIN) has been adopted to characterize complex and
heterogeneous auxiliary data in top-$N$ recommender systems, called
\emph{HIN-based recommendation}. HIN characterizes complex, heterogeneous data
relations, containing a variety of information that may not be related to the
recommendation task. Therefore, it is challenging to effectively leverage
useful information from HINs for improving the recommendation performance. To
address the above issue, we propose a Curriculum pre-training based
HEterogeneous Subgraph Transformer (called \emph{CHEST}) with new \emph{data
characterization}, \emph{representation model} and \emph{learning algorithm}.

Specifically, we consider extracting useful information from HIN to compose
the interaction-specific heterogeneous subgraph, containing both sufficient and
relevant context information for recommendation. Then we capture the rich
semantics (\eg graph structure and path semantics) within the subgraph via a
heterogeneous subgraph Transformer, where we encode the subgraph with
multi-slot sequence representations. Besides, we design a curriculum
pre-training strategy to provide an elementary-to-advanced learning process, by
which we smoothly transfer basic semantics in HIN for modeling user-item
interaction relation.

Extensive experiments conducted on three real-world datasets demonstrate the
superiority of our proposed method over a number of competitive baselines,
especially when only limited training data is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoLoss: Automated Loss Function Search in Recommendations. (arXiv:2106.06713v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.06713</id>
        <link href="http://arxiv.org/abs/2106.06713"/>
        <updated>2021-06-15T01:45:12.220Z</updated>
        <summary type="html"><![CDATA[Designing an effective loss function plays a crucial role in training deep
recommender systems. Most existing works often leverage a predefined and fixed
loss function that could lead to suboptimal recommendation quality and training
efficiency. Some recent efforts rely on exhaustively or manually searched
weights to fuse a group of candidate loss functions, which is exceptionally
costly in computation and time. They also neglect the various convergence
behaviors of different data examples. In this work, we propose an AutoLoss
framework that can automatically and adaptively search for the appropriate loss
function from a set of candidates. To be specific, we develop a novel
controller network, which can dynamically adjust the loss probabilities in a
differentiable manner. Unlike existing algorithms, the proposed controller can
adaptively generate the loss probabilities for different data examples
according to their varied convergence behaviors. Such design improves the
model's generalizability and transferability between deep recommender systems
and datasets. We evaluate the proposed framework on two benchmark datasets. The
results show that AutoLoss outperforms representative baselines. Further
experiments have been conducted to deepen our understandings of AutoLoss,
including its transferability, components and training efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haochen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1"&gt;Wenqi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Adversarial Attacks. (arXiv:2103.02014v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02014</id>
        <link href="http://arxiv.org/abs/2103.02014"/>
        <updated>2021-06-14T22:41:41.478Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks expose important vulnerabilities of deep learning models,
yet little attention has been paid to settings where data arrives as a stream.
In this paper, we formalize the online adversarial attack problem, emphasizing
two key elements found in real-world use-cases: attackers must operate under
partial knowledge of the target model, and the decisions made by the attacker
are irrevocable since they operate on a transient data stream. We first
rigorously analyze a deterministic variant of the online threat model by
drawing parallels to the well-studied $k$-secretary problem in theoretical
computer science and propose Virtual+, a simple yet practical online algorithm.
Our main theoretical result show Virtual+ yields provably the best competitive
ratio over all single-threshold algorithms for $k<5$ -- extending previous
analysis of the $k$-secretary problem. We also introduce the \textit{stochastic
$k$-secretary} -- effectively reducing online blackbox transfer attacks to a
$k$-secretary problem under noise -- and prove theoretical bounds on the
performance of \textit{any} online algorithms adapted to this setting. Finally,
we complement our theoretical results by conducting experiments on both MNIST
and CIFAR-10 with both vanilla and robust classifiers, revealing not only the
necessity of online algorithms in achieving near-optimal performance but also
the rich interplay of a given attack strategy towards online attack selection,
enabling simple strategies like FGSM to outperform classically strong whitebox
adversaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mladenovic_A/0/1/0/all/0/1"&gt;Andjela Mladenovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bose_A/0/1/0/all/0/1"&gt;Avishek Joey Bose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berard_H/0/1/0/all/0/1"&gt;Hugo Berard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L. Hamilton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1"&gt;Simon Lacoste-Julien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1"&gt;Pascal Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1"&gt;Gauthier Gidel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths. (arXiv:2101.09612v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09612</id>
        <link href="http://arxiv.org/abs/2101.09612"/>
        <updated>2021-06-14T22:41:41.457Z</updated>
        <summary type="html"><![CDATA[We give a simple proof for the global convergence of gradient descent in
training deep ReLU networks with the standard square loss, and show some of its
improvements over the state-of-the-art. In particular, while prior works
require all the hidden layers to be wide with width at least $\Omega(N^8)$ ($N$
being the number of training samples), we require a single wide layer of
linear, quadratic or cubic width depending on the type of initialization.
Unlike many recent proofs based on the Neural Tangent Kernel (NTK), our proof
need not track the evolution of the entire NTK matrix, or more generally, any
quantities related to the changes of activation patterns during training.
Instead, we only need to track the evolution of the output at the last hidden
layer, which can be done much more easily thanks to the Lipschitz property of
ReLU. Some highlights of our setting: (i) all the layers are trained with
standard gradient descent, (ii) the network has standard parameterization as
opposed to the NTK one, and (iii) the network has a single wide layer as
opposed to having all wide hidden layers as in most of NTK-related results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1"&gt;Quynh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Stochastic Simulation with Interactive Neural Processes. (arXiv:2106.02770v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02770</id>
        <link href="http://arxiv.org/abs/2106.02770"/>
        <updated>2021-06-14T22:41:41.445Z</updated>
        <summary type="html"><![CDATA[Stochastic simulations such as large-scale, spatiotemporal, age-structured
epidemic models are computationally expensive at fine-grained resolution. We
propose Interactive Neural Process (INP), an interactive framework to
continuously learn a deep learning surrogate model and accelerate simulation.
Our framework is based on the novel integration of Bayesian active learning,
stochastic simulation and deep sequence modeling. In particular, we develop a
novel spatiotemporal neural process model to mimic the underlying process
dynamics. Our model automatically infers the latent process which describes the
intrinsic uncertainty of the simulator. This also gives rise to a new
acquisition function that can quantify the uncertainty of deep learning
predictions. We design Bayesian active learning algorithms to iteratively query
the simulator, gather more data, and continuously improve the model. We perform
theoretical analysis and demonstrate that our approach reduces sample
complexity compared with random sampling in high dimension. Empirically, we
demonstrate our framework can faithfully imitate the behavior of a complex
infectious disease simulator with a small number of examples, enabling rapid
simulation and scenario exploration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongxia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chinazzi_M/0/1/0/all/0/1"&gt;Matteo Chinazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vespignani_A/0/1/0/all/0/1"&gt;Alessandro Vespignani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi-An Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Rose Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional and Adversarial Euler-based Generators For Time Series. (arXiv:2102.05313v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05313</id>
        <link href="http://arxiv.org/abs/2102.05313"/>
        <updated>2021-06-14T22:41:41.415Z</updated>
        <summary type="html"><![CDATA[We introduce three new generative models for time series. Based on Euler
discretization and Wasserstein metrics, they are able to capture time marginal
distributions and temporal dynamics. Two of these methods rely on the
adaptation of generative adversarial networks (GANs) to time series. Both of
them outperform state-of-the-art benchmarks by capturing the underlying
temporal structure on synthetic time series. The third algorithm, called
Conditional Euler Generator (CEGEN), minimizes a dedicated distance between the
transition probability distributions over all time steps. In the context of Ito
processes, we provide theoretical guarantees that minimizing this criterion
implies accurate estimations of the drift and volatility parameters. We
demonstrate empirically that CEGEN outperforms state-of-the-art and GAN
generators on both marginal and temporal dynamics metrics. Besides, it
identifies accurate correlation structures in high dimension. When few data
points are available, we verify the effectiveness of CEGEN, when combined with
transfer learning methods on Monte Carlo simulations. Finally, we illustrate
the robustness of our method on various real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Remlinger_C/0/1/0/all/0/1"&gt;Carl Remlinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mikael_J/0/1/0/all/0/1"&gt;Joseph Mikael&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Elie_R/0/1/0/all/0/1"&gt;Romuald Elie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Logic of Machine Learning. (arXiv:2006.09500v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09500</id>
        <link href="http://arxiv.org/abs/2006.09500"/>
        <updated>2021-06-14T22:41:41.392Z</updated>
        <summary type="html"><![CDATA[I propose a new, logical, foundation for ML. ML is approached as a problem of
maximizing consistency of a hypothesis in a context of a given training set.
Nonjudgmental logic (NjL) with modalities ``It appears that'', ``Assume that''
is introduced to formalize and quantify the inconsistency. Many popular ML
algorithms (from hierarchical clustering to k-NN and SVM) are shown to
corroborate the conjecture. In addition, it is demonstrated that NjL allows to
formalize and solve several general learning problems which are not considered
as ML usually.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sapir_M/0/1/0/all/0/1"&gt;Marina Sapir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Modular Analysis of Provable Acceleration via Polyak's Momentum: Training a Wide ReLU Network and a Deep Linear Network. (arXiv:2010.01618v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01618</id>
        <link href="http://arxiv.org/abs/2010.01618"/>
        <updated>2021-06-14T22:41:41.359Z</updated>
        <summary type="html"><![CDATA[Incorporating a so-called "momentum" dynamic in gradient descent methods is
widely used in neural net training as it has been broadly observed that, at
least empirically, it often leads to significantly faster convergence. At the
same time, there are very few theoretical guarantees in the literature to
explain this apparent acceleration effect. Even for the classical strongly
convex quadratic problems, several existing results only show Polyak's momentum
has an accelerated linear rate asymptotically. In this paper, we first revisit
the quadratic problems and show a non-asymptotic accelerated linear rate of
Polyak's momentum. Then, we provably show that Polyak's momentum achieves
acceleration for training a one-layer wide ReLU network and a deep linear
network, which are perhaps the two most popular canonical models for studying
optimization and deep learning in the literature. Prior work Du at al. 2019 and
Wu et al. 2019 showed that using vanilla gradient descent, and with an use of
over-parameterization, the error decays as $(1- \Theta(\frac{1}{ \kappa'}))^t$
after $t$ iterations, where $\kappa'$ is the condition number of a Gram Matrix.
Our result shows that with the appropriate choice of parameters Polyak's
momentum has a rate of $(1-\Theta(\frac{1}{\sqrt{\kappa'}}))^t$. For the deep
linear network, prior work Hu et al. 2020 showed that vanilla gradient descent
has a rate of $(1-\Theta(\frac{1}{\kappa}))^t$, where $\kappa$ is the condition
number of a data matrix. Our result shows an acceleration rate $(1-
\Theta(\frac{1}{\sqrt{\kappa}}))^t$ is achievable by Polyak's momentum. All the
results in this work are obtained from a modular analysis, which can be of
independent interest. This work establishes that momentum does indeed speed up
neural net training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun-Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chi-Heng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1"&gt;Jacob Abernethy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymmetric Heavy Tails and Implicit Bias in Gaussian Noise Injections. (arXiv:2102.07006v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07006</id>
        <link href="http://arxiv.org/abs/2102.07006"/>
        <updated>2021-06-14T01:38:56.498Z</updated>
        <summary type="html"><![CDATA[Gaussian noise injections (GNIs) are a family of simple and widely-used
regularisation methods for training neural networks, where one injects additive
or multiplicative Gaussian noise to the network activations at every iteration
of the optimisation algorithm, which is typically chosen as stochastic gradient
descent (SGD). In this paper we focus on the so-called `implicit effect' of
GNIs, which is the effect of the injected noise on the dynamics of SGD. We show
that this effect induces an asymmetric heavy-tailed noise on SGD gradient
updates. In order to model this modified dynamics, we first develop a
Langevin-like stochastic differential equation that is driven by a general
family of asymmetric heavy-tailed noise. Using this model we then formally
prove that GNIs induce an `implicit bias', which varies depending on the
heaviness of the tails and the level of asymmetry. Our empirical results
confirm that different types of neural networks trained with GNIs are
well-modelled by the proposed dynamics and that the implicit effect of these
injections induces a bias that degrades the performance of networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Camuto_A/0/1/0/all/0/1"&gt;Alexander Camuto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingjiong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1"&gt;Chris Holmes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1"&gt;Mert G&amp;#xfc;rb&amp;#xfc;zbalaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1"&gt;Umut &amp;#x15e;im&amp;#x15f;ekli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrate Before Use: Improving Few-Shot Performance of Language Models. (arXiv:2102.09690v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09690</id>
        <link href="http://arxiv.org/abs/2102.09690"/>
        <updated>2021-06-14T01:38:56.490Z</updated>
        <summary type="html"><![CDATA[GPT-3 can perform numerous tasks when provided a natural language prompt that
contains a few training examples. We show that this type of few-shot learning
can be unstable: the choice of prompt format, training examples, and even the
order of the training examples can cause accuracy to vary from near chance to
near state-of-the-art. We demonstrate that this instability arises from the
bias of language models towards predicting certain answers, e.g., those that
are placed near the end of the prompt or are common in the pre-training data.
To mitigate this, we first estimate the model's bias towards each answer by
asking for its prediction when given the training prompt and a content-free
test input such as "N/A". We then fit calibration parameters that cause the
prediction for this input to be uniform across answers. On a diverse set of
tasks, this contextual calibration procedure substantially improves GPT-3 and
GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across
different choices of the prompt.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tony Z. Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1"&gt;Eric Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1"&gt;Dan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[View Generalization for Single Image Textured 3D Models. (arXiv:2106.06533v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06533</id>
        <link href="http://arxiv.org/abs/2106.06533"/>
        <updated>2021-06-14T01:38:56.482Z</updated>
        <summary type="html"><![CDATA[Humans can easily infer the underlying 3D geometry and texture of an object
only from a single 2D image. Current computer vision methods can do this, too,
but suffer from view generalization problems - the models inferred tend to make
poor predictions of appearance in novel views. As for generalization problems
in machine learning, the difficulty is balancing single-view accuracy (cf.
training error; bias) with novel view accuracy (cf. test error; variance). We
describe a class of models whose geometric rigidity is easily controlled to
manage this tradeoff. We describe a cycle consistency loss that improves view
generalization (roughly, a model from a generated view should predict the
original view well). View generalization of textures requires that models share
texture information, so a car seen from the back still has headlights because
other cars have headlights. We describe a cycle consistency loss that
encourages model textures to be aligned, so as to encourage sharing. We compare
our method against the state-of-the-art method and show both qualitative and
quantitative improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1"&gt;Anand Bhattad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dundar_A/0/1/0/all/0/1"&gt;Aysegul Dundar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guilin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1"&gt;Andrew Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1"&gt;Bryan Catanzaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Contrastive Divergence Training of Energy Based Models. (arXiv:2012.01316v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01316</id>
        <link href="http://arxiv.org/abs/2012.01316"/>
        <updated>2021-06-14T01:38:56.462Z</updated>
        <summary type="html"><![CDATA[Contrastive divergence is a popular method of training energy-based models,
but is known to have difficulties with training stability. We propose an
adaptation to improve contrastive divergence training by scrutinizing a
gradient term that is difficult to calculate and is often left out for
convenience. We show that this gradient term is numerically significant and in
practice is important to avoid training instabilities, while being tractable to
estimate. We further highlight how data augmentation and multi-scale processing
can be used to improve model robustness and generation quality. Finally, we
empirically evaluate stability of model architectures and show improved
performance on a host of benchmarks and use cases,such as image generation, OOD
detection, and compositional generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yilun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1"&gt;Igor Mordatch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic bounds on neuron death in deep rectifier networks. (arXiv:2007.06192v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06192</id>
        <link href="http://arxiv.org/abs/2007.06192"/>
        <updated>2021-06-14T01:38:56.451Z</updated>
        <summary type="html"><![CDATA[Neuron death is a complex phenomenon with implications for model
trainability: the deeper the network, the lower the probability of finding a
valid initialization. In this work, we derive both upper and lower bounds on
the probability that a ReLU network is initialized to a trainable point, as a
function of model hyperparameters. We show that it is possible to increase the
depth of a network indefinitely, so long as the width increases as well.
Furthermore, our bounds are asymptotically tight under reasonable assumptions:
first, the upper bound coincides with the true probability for a single-layer
network with the largest possible input set. Second, the true probability
converges to our lower bound as the input set shrinks to a single point, or as
the network complexity grows under an assumption about the output variance. We
confirm these results by numerical simulation, showing rapid convergence to the
lower bound with increasing network depth. Then, motivated by the theory, we
propose a practical sign flipping scheme which guarantees that the ratio of
living data points in a $k$-layer network is at least $2^{-k}$. Finally, we
show how these issues are mitigated by network design features currently seen
in practice, such as batch normalization, residual connections, dense networks
and skip connections. This suggests that neuron death may provide insight into
the efficacy of various model architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rister_B/0/1/0/all/0/1"&gt;Blaine Rister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L. Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantile Bandits for Best Arms Identification. (arXiv:2010.11568v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11568</id>
        <link href="http://arxiv.org/abs/2010.11568"/>
        <updated>2021-06-14T01:38:56.444Z</updated>
        <summary type="html"><![CDATA[We consider a variant of the best arm identification task in stochastic
multi-armed bandits. Motivated by risk-averse decision-making problems, our
goal is to identify a set of $m$ arms with the highest $\tau$-quantile values
within a fixed budget. We prove asymmetric two-sided concentration inequalities
for order statistics and quantiles of random variables that have non-decreasing
hazard rate, which may be of independent interest. With these inequalities, we
analyse a quantile version of Successive Accepts and Rejects (Q-SAR). We derive
an upper bound for the probability of arm misidentification, the first
justification of a quantile based algorithm for fixed budget multiple best arms
identification. We show illustrative experiments for best arm identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_C/0/1/0/all/0/1"&gt;Cheng Soon Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best Arm Identification in Graphical Bilinear Bandits. (arXiv:2012.07641v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07641</id>
        <link href="http://arxiv.org/abs/2012.07641"/>
        <updated>2021-06-14T01:38:56.437Z</updated>
        <summary type="html"><![CDATA[We introduce a new graphical bilinear bandit problem where a learner (or a
\emph{central entity}) allocates arms to the nodes of a graph and observes for
each edge a noisy bilinear reward representing the interaction between the two
end nodes. We study the best arm identification problem in which the learner
wants to find the graph allocation maximizing the sum of the bilinear rewards.
By efficiently exploiting the geometry of this bandit problem, we propose a
\emph{decentralized} allocation strategy based on random sampling with
theoretical guarantees. In particular, we characterize the influence of the
graph structure (e.g. star, complete or circle) on the convergence rate and
propose empirical experiments that confirm this dependency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rizk_G/0/1/0/all/0/1"&gt;Geovani Rizk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1"&gt;Albert Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colin_I/0/1/0/all/0/1"&gt;Igor Colin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laraki_R/0/1/0/all/0/1"&gt;Rida Laraki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chevaleyre_Y/0/1/0/all/0/1"&gt;Yann Chevaleyre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous-Time Model-Based Reinforcement Learning. (arXiv:2102.04764v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04764</id>
        <link href="http://arxiv.org/abs/2102.04764"/>
        <updated>2021-06-14T01:38:56.430Z</updated>
        <summary type="html"><![CDATA[Model-based reinforcement learning (MBRL) approaches rely on discrete-time
state transition models whereas physical systems and the vast majority of
control tasks operate in continuous-time. To avoid time-discretization
approximation of the underlying process, we propose a continuous-time MBRL
framework based on a novel actor-critic method. Our approach also infers the
unknown state evolution differentials with Bayesian neural ordinary
differential equations (ODE) to account for epistemic uncertainty. We implement
and test our method on a new ODE-RL suite that explicitly solves
continuous-time control systems. Our experiments illustrate that the model is
robust against irregular and noisy data, is sample-efficient, and can solve
control problems which pose challenges to discrete-time MBRL methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yildiz_C/0/1/0/all/0/1"&gt;&amp;#xc7;a&amp;#x11f;atay Y&amp;#x131;ld&amp;#x131;z&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heinonen_M/0/1/0/all/0/1"&gt;Markus Heinonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahdesmaki_H/0/1/0/all/0/1"&gt;Harri L&amp;#xe4;hdesm&amp;#xe4;ki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Learning and its Application for Time-Series Prediction. (arXiv:2106.03211v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03211</id>
        <link href="http://arxiv.org/abs/2106.03211"/>
        <updated>2021-06-14T01:38:56.408Z</updated>
        <summary type="html"><![CDATA[Extreme events are occurrences whose magnitude and potential cause extensive
damage on people, infrastructure, and the environment. Motivated by the extreme
nature of the current global health landscape, which is plagued by the
coronavirus pandemic, we seek to better understand and model extreme events.
Modeling extreme events is common in practice and plays an important role in
time-series prediction applications. Our goal is to (i) compare and investigate
the effect of some common extreme events modeling methods to explore which
method can be practical in reality and (ii) accelerate the deep learning
training process, which commonly uses deep recurrent neural network (RNN), by
implementing the asynchronous local Stochastic Gradient Descent (SGD) framework
among multiple compute nodes. In order to verify our distributed extreme events
modeling, we evaluate our proposed framework on a stock data set S\&P500, with
a standard recurrent neural network. Our intuition is to explore the (best)
extreme events modeling method which could work well under the distributed deep
learning setting. Moreover, by using asynchronous distributed learning, we aim
to significantly reduce the communication cost among the compute nodes and
central server, which is the main bottleneck of almost all distributed learning
frameworks.

We implement our proposed work and evaluate its performance on representative
data sets, such as S&P500 stock in $5$-year period. The experimental results
validate the correctness of the design principle and show a significant
training duration reduction upto $8$x, compared to the baseline single compute
node. Our results also show that our proposed work can achieve the same level
of test accuracy, compared to the baseline setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Nhuong V. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Legitime_S/0/1/0/all/0/1"&gt;Sybille Legitime&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing the Travel and Charging Behavior of Electric Vehicles -- A Data-driven Approach. (arXiv:2106.06475v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06475</id>
        <link href="http://arxiv.org/abs/2106.06475"/>
        <updated>2021-06-14T01:38:56.398Z</updated>
        <summary type="html"><![CDATA[The increasing market penetration of electric vehicles (EVs) may pose
significant electricity demand on power systems. This electricity demand is
affected by the inherent uncertainties of EVs' travel behavior that makes
forecasting the daily charging demand (CD) very challenging. In this project,
we use the National House Hold Survey (NHTS) data to form sequences of trips,
and develop machine learning models to predict the parameters of the next trip
of the drivers, including trip start time, end time, and distance. These
parameters are later used to model the temporal charging behavior of EVs. The
simulation results show that the proposed modeling can effectively estimate the
daily CD pattern based on travel behavior of EVs, and simple machine learning
techniques can forecast the travel parameters with acceptable accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baghali_S/0/1/0/all/0/1"&gt;Sina Baghali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1"&gt;Samiul Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zhaomiao Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Selection Tutorial with Python Examples. (arXiv:2106.06437v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06437</id>
        <link href="http://arxiv.org/abs/2106.06437"/>
        <updated>2021-06-14T01:38:56.391Z</updated>
        <summary type="html"><![CDATA[In Machine Learning, feature selection entails selecting a subset of the
available features in a dataset to use for model development. There are many
motivations for feature selection, it may result in better models, it may
provide insight into the data and it may deliver economies in data gathering or
data processing. For these reasons feature selection has received a lot of
attention in data analytics research. In this paper we provide an overview of
the main methods and present practical examples with Python implementations.
While the main focus is on supervised feature selection techniques, we also
cover some feature transformation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cunningham_P/0/1/0/all/0/1"&gt;Padraig Cunningham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kathirgamanathan_B/0/1/0/all/0/1"&gt;Bahavathy Kathirgamanathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delany_S/0/1/0/all/0/1"&gt;Sarah Jane Delany&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making EfficientNet More Efficient: Exploring Batch-Independent Normalization, Group Convolutions and Reduced Resolution Training. (arXiv:2106.03640v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03640</id>
        <link href="http://arxiv.org/abs/2106.03640"/>
        <updated>2021-06-14T01:38:56.380Z</updated>
        <summary type="html"><![CDATA[Much recent research has been dedicated to improving the efficiency of
training and inference for image classification. This effort has commonly
focused on explicitly improving theoretical efficiency, often measured as
ImageNet validation accuracy per FLOP. These theoretical savings have, however,
proven challenging to achieve in practice, particularly on high-performance
training accelerators.

In this work, we focus on improving the practical efficiency of the
state-of-the-art EfficientNet models on a new class of accelerator, the
Graphcore IPU. We do this by extending this family of models in the following
ways: (i) generalising depthwise convolutions to group convolutions; (ii)
adding proxy-normalized activations to match batch normalization performance
with batch-independent statistics; (iii) reducing compute by lowering the
training resolution and inexpensively fine-tuning at higher resolution. We find
that these three methods improve the practical efficiency for both training and
inference. Our code will be made available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Masters_D/0/1/0/all/0/1"&gt;Dominic Masters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labatie_A/0/1/0/all/0/1"&gt;Antoine Labatie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1"&gt;Zach Eaton-Rosen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1"&gt;Carlo Luschi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise and Fluctuation of Finite Learning Rate Stochastic Gradient Descent. (arXiv:2012.03636v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03636</id>
        <link href="http://arxiv.org/abs/2012.03636"/>
        <updated>2021-06-14T01:38:56.373Z</updated>
        <summary type="html"><![CDATA[In the vanishing learning rate regime, stochastic gradient descent (SGD) is
now relatively well understood. In this work, we propose to study the basic
properties of SGD and its variants in the non-vanishing learning rate regime.
The focus is on deriving exactly solvable results and discussing their
implications. The main contributions of this work are to derive the stationary
distribution for discrete-time SGD in a quadratic loss function with and
without momentum; in particular, one implication of our result is that the
fluctuation caused by discrete-time dynamics takes a distorted shape and is
dramatically larger than a continuous-time theory could predict. Examples of
applications of the proposed theory considered in this work include the
approximation error of variants of SGD, the effect of minibatch noise, the
optimal Bayesian inference, the escape rate from a sharp minimum, and the
stationary covariance of a few second-order methods including damped Newton's
method, natural gradient descent, and Adam.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kangqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ziyin_L/0/1/0/all/0/1"&gt;Liu Ziyin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ueda_M/0/1/0/all/0/1"&gt;Masahito Ueda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Collaboration. (arXiv:2105.02569v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02569</id>
        <link href="http://arxiv.org/abs/2105.02569"/>
        <updated>2021-06-14T01:38:56.355Z</updated>
        <summary type="html"><![CDATA[We propose a new ensemble framework for supervised learning, called machine
collaboration (MaC), using a collection of base machines for prediction tasks.
Unlike bagging/stacking (a parallel & independent framework) and boosting (a
sequential & top-down framework), MaC is a type of circular & interactive
learning framework. The circular & interactive feature helps the base machines
to transfer information circularly and update their structures and parameters
accordingly. The theoretical result on the risk bound of the estimator from MaC
reveals that the circular & interactive feature can help MaC reduce risk via a
parsimonious ensemble. We conduct extensive experiments on MaC using both
simulated data and 119 benchmark real datasets. The results demonstrate that in
most cases, MaC performs significantly better than several other
state-of-the-art methods, including classification and regression trees, neural
networks, stacking, and boosting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qingfeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Autoencoders: A Harmonic Perspective. (arXiv:2105.14866v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14866</id>
        <link href="http://arxiv.org/abs/2105.14866"/>
        <updated>2021-06-14T01:38:56.348Z</updated>
        <summary type="html"><![CDATA[In this work we study Variational Autoencoders (VAEs) from the perspective of
harmonic analysis. By viewing a VAE's latent space as a Gaussian Space, a
variety of measure space, we derive a series of results that show that the
encoder variance of a VAE controls the frequency content of the functions
parameterised by the VAE encoder and decoder neural networks. In particular we
demonstrate that larger encoder variances reduce the high frequency content of
these functions. Our analysis allows us to show that increasing this variance
effectively induces a soft Lipschitz constraint on the decoder network of a
VAE, which is a core contributor to the adversarial robustness of VAEs. We
further demonstrate that adding Gaussian noise to the input of a VAE allows us
to more finely control the frequency content and the Lipschitz constant of the
VAE encoder networks. To support our theoretical analysis we run experiments
with VAEs with small fully-connected neural networks and with larger
convolutional networks, demonstrating empirically that our theory holds for a
variety of neural network architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Camuto_A/0/1/0/all/0/1"&gt;Alexander Camuto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Willetts_M/0/1/0/all/0/1"&gt;Matthew Willetts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions. (arXiv:2104.02710v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02710</id>
        <link href="http://arxiv.org/abs/2104.02710"/>
        <updated>2021-06-14T01:38:56.339Z</updated>
        <summary type="html"><![CDATA[Multi-agent behavior modeling aims to understand the interactions that occur
between agents. We present a multi-agent dataset from behavioral neuroscience,
the Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists
of trajectory data of social interactions, recorded from videos of freely
behaving mice in a standard resident-intruder assay. To help accelerate
behavioral studies, the CalMS21 dataset provides benchmarks to evaluate the
performance of automated behavior classification methods in three settings: (1)
for training on large behavioral datasets all annotated by a single annotator,
(2) for style transfer to learn inter-annotator differences in behavior
definitions, and (3) for learning of new behaviors of interest given limited
training data. The dataset consists of 6 million frames of unlabeled tracked
poses of interacting mice, as well as over 1 million frames with tracked poses
and corresponding frame-level behavior annotations. The challenge of our
dataset is to be able to classify behaviors accurately using both labeled and
unlabeled tracking data, as well as being able to generalize to new settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jennifer J. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karigo_T/0/1/0/all/0/1"&gt;Tomomi Karigo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1"&gt;Dipam Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1"&gt;Sharada P. Mohanty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wild_B/0/1/0/all/0/1"&gt;Benjamin Wild&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Quan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1"&gt;David J. Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1"&gt;Pietro Perona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yisong Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1"&gt;Ann Kennedy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Integer Linear Programming Framework for Mining Constraints from Data. (arXiv:2006.10836v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10836</id>
        <link href="http://arxiv.org/abs/2006.10836"/>
        <updated>2021-06-14T01:38:56.331Z</updated>
        <summary type="html"><![CDATA[Structured output prediction problems (e.g., sequential tagging, hierarchical
multi-class classification) often involve constraints over the output label
space. These constraints interact with the learned models to filter infeasible
solutions and facilitate in building an accountable system. However, although
constraints are useful, they are often based on hand-crafted rules. This raises
a question -- \emph{can we mine constraints and rules from data based on a
learning algorithm?}

In this paper, we present a general framework for mining constraints from
data. In particular, we consider the inference in structured output prediction
as an integer linear programming (ILP) problem. Then, given the coefficients of
the objective function and the corresponding solution, we mine the underlying
constraints by estimating the outer and inner polytopes of the feasible set. We
verify the proposed constraint mining algorithm in various synthetic and
real-world applications and demonstrate that the proposed approach successfully
identifies the feasible set at scale.

In particular, we show that our approach can learn to solve 9x9 Sudoku
puzzles and minimal spanning tree problems from examples without providing the
underlying rules. Our algorithm can also integrate with a neural network model
to learn the hierarchical label structure of a multi-label classification task.
Besides, we provide a theoretical analysis about the tightness of the polytopes
and the reliability of the mined constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1"&gt;Tao Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks. (arXiv:2006.07869v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07869</id>
        <link href="http://arxiv.org/abs/2006.07869"/>
        <updated>2021-06-14T01:38:56.323Z</updated>
        <summary type="html"><![CDATA[Multi-agent deep reinforcement learning (MARL) suffers from a lack of
commonly-used evaluation tasks and criteria, making comparisons between
approaches difficult. In this work, we consistently evaluate and compare three
different classes of MARL algorithms (independent learning, centralised
multi-agent policy gradient, value decomposition) in a diverse range of
cooperative multi-agent learning tasks. Our experiments serve as a reference
for the expected performance of algorithms across different learning tasks, and
we provide insights regarding the effectiveness of different learning
approaches. We open-source EPyMARL, which extends the PyMARL
codebase~\citep{samvelyan19smac} to include additional algorithms and allow for
flexible configuration of algorithm implementation details such as parameter
sharing. Finally, we open-source two environments for multi-agent research
which focus on coordination under sparse rewards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papoudakis_G/0/1/0/all/0/1"&gt;Georgios Papoudakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1"&gt;Filippos Christianos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schafer_L/0/1/0/all/0/1"&gt;Lukas Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1"&gt;Stefano V. Albrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Reinforcement Learning for Air-to-Air Combat. (arXiv:2105.00990v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00990</id>
        <link href="http://arxiv.org/abs/2105.00990"/>
        <updated>2021-06-14T01:38:56.302Z</updated>
        <summary type="html"><![CDATA[Artificial Intelligence (AI) is becoming a critical component in the defense
industry, as recently demonstrated by DARPA`s AlphaDogfight Trials (ADT). ADT
sought to vet the feasibility of AI algorithms capable of piloting an F-16 in
simulated air-to-air combat. As a participant in ADT, Lockheed Martin`s (LM)
approach combines a hierarchical architecture with maximum-entropy
reinforcement learning (RL), integrates expert knowledge through reward
shaping, and supports modularity of policies. This approach achieved a $2^{nd}$
place finish in the final ADT event (among eight total competitors) and
defeated a graduate of the US Air Force's (USAF) F-16 Weapons Instructor Course
in match play.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pope_A/0/1/0/all/0/1"&gt;Adrian P. Pope&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ide_J/0/1/0/all/0/1"&gt;Jaime S. Ide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Micovic_D/0/1/0/all/0/1"&gt;Daria Micovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_H/0/1/0/all/0/1"&gt;Henry Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenbluth_D/0/1/0/all/0/1"&gt;David Rosenbluth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritholtz_L/0/1/0/all/0/1"&gt;Lee Ritholtz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Twedt_J/0/1/0/all/0/1"&gt;Jason C. Twedt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walker_T/0/1/0/all/0/1"&gt;Thayne T. Walker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alcedo_K/0/1/0/all/0/1"&gt;Kevin Alcedo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javorsek_D/0/1/0/all/0/1"&gt;Daniel Javorsek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills. (arXiv:2104.07749v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07749</id>
        <link href="http://arxiv.org/abs/2104.07749"/>
        <updated>2021-06-14T01:38:56.294Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning useful robotic skills from previously
collected offline data without access to manually specified rewards or
additional online exploration, a setting that is becoming increasingly
important for scaling robot learning by reusing past robotic data. In
particular, we propose the objective of learning a functional understanding of
the environment by learning to reach any goal state in a given dataset. We
employ goal-conditioned Q-learning with hindsight relabeling and develop
several techniques that enable training in a particularly challenging offline
setting. We find that our method can operate on high-dimensional camera images
and learn a variety of skills on real robots that generalize to previously
unseen scenes and objects. We also show that our method can learn to reach
long-horizon goals across multiple episodes through goal chaining, and learn
rich representations that can help with downstream tasks through pre-training
or auxiliary objectives. The videos of our experiments can be found at
https://actionable-models.github.io]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chebotar_Y/0/1/0/all/0/1"&gt;Yevgen Chebotar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1"&gt;Karol Hausman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1"&gt;Ted Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalashnikov_D/0/1/0/all/0/1"&gt;Dmitry Kalashnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varley_J/0/1/0/all/0/1"&gt;Jake Varley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irpan_A/0/1/0/all/0/1"&gt;Alex Irpan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eysenbach_B/0/1/0/all/0/1"&gt;Benjamin Eysenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1"&gt;Ryan Julian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs. (arXiv:2106.06150v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06150</id>
        <link href="http://arxiv.org/abs/2106.06150"/>
        <updated>2021-06-14T01:38:56.287Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are powerful tools for learning from graph data
and are widely used in various applications such as social network
recommendation, fraud detection, and graph search. The graphs in these
applications are typically large, usually containing hundreds of millions of
nodes. Training GNN models on such large graphs efficiently remains a big
challenge. Despite a number of sampling-based methods have been proposed to
enable mini-batch training on large graphs, these methods have not been proved
to work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU
training. The state-of-the-art sampling-based methods are usually not optimized
for these real-world hardware setups, in which data movement between CPUs and
GPUs is a bottleneck. To address this issue, we propose Global Neighborhood
Sampling that aims at training GNNs on giant graphs specifically for
mixed-CPU-GPU training. The algorithm samples a global cache of nodes
periodically for all mini-batches and stores them in GPUs. This global cache
allows in-GPU importance sampling of mini-batches, which drastically reduces
the number of nodes in a mini-batch, especially in the input layer, to reduce
data copy between CPU and GPU and mini-batch computation without compromising
the training convergence rate or model accuracy. We provide a highly efficient
implementation of this method and show that our implementation outperforms an
efficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant
graphs. It outperforms an efficient implementation of LADIES with small layers
by a factor of 2X-14X while achieving much higher accuracy than LADIES.We also
theoretically analyze the proposed algorithm and show that with cached node
data of a proper size, it enjoys a comparable convergence rate as the
underlying node-wise sampling method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jialin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1"&gt;Da Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lin F. Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1"&gt;Geroge Karypis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Recovery of Semantic Attributes. (arXiv:2103.11888v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11888</id>
        <link href="http://arxiv.org/abs/2103.11888"/>
        <updated>2021-06-14T01:38:56.278Z</updated>
        <summary type="html"><![CDATA[We consider the problem of the extraction of semantic attributes, supervised
only with classification labels. For example, when learning to classify images
of birds into species, we would like to observe the emergence of features that
zoologists use to classify birds. To tackle this problem, we propose training a
neural network with discrete features in the last layer, which is followed by
two heads: a multi-layered perceptron (MLP) and a decision tree. Since decision
trees utilize simple binary decision stumps we expect those discrete features
to obtain semantic meaning. We present a theoretical analysis as well as a
practical method for learning in the intersection of two hypothesis classes.
Our results on multiple benchmarks show an improved ability to extract a set of
features that are highly correlated with the set of unseen attributes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1"&gt;Ameen Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galanti_T/0/1/0/all/0/1"&gt;Tomer Galanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheltonozhskiy_E/0/1/0/all/0/1"&gt;Evgeniy Zheltonozhskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baskin_C/0/1/0/all/0/1"&gt;Chaim Baskin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved, Deterministic Smoothing for L_1 Certified Robustness. (arXiv:2103.10834v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10834</id>
        <link href="http://arxiv.org/abs/2103.10834"/>
        <updated>2021-06-14T01:38:56.270Z</updated>
        <summary type="html"><![CDATA[Randomized smoothing is a general technique for computing sample-dependent
robustness guarantees against adversarial attacks for deep classifiers. Prior
works on randomized smoothing against L_1 adversarial attacks use additive
smoothing noise and provide probabilistic robustness guarantees. In this work,
we propose a non-additive and deterministic smoothing method, Deterministic
Smoothing with Splitting Noise (DSSN). To develop DSSN, we first develop SSN, a
randomized method which involves generating each noisy smoothing sample by
first randomly splitting the input space and then returning a representation of
the center of the subdivision occupied by the input sample. In contrast to
uniform additive smoothing, the SSN certification does not require the random
noise components used to be independent. Thus, smoothing can be done
effectively in just one dimension and can therefore be efficiently derandomized
for quantized data (e.g., images). To the best of our knowledge, this is the
first work to provide deterministic "randomized smoothing" for a norm-based
adversarial threat model while allowing for an arbitrary classifier (i.e., a
deep model) to be used as a base classifier and without requiring an
exponential number of smoothing samples. On CIFAR-10 and ImageNet datasets, we
provide substantially larger L_1 robustness certificates compared to prior
works, establishing a new state-of-the-art. The determinism of our method also
leads to significantly faster certificate computation. Code is available at:
https://github.com/alevine0/smoothingSplittingNoise]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levine_A/0/1/0/all/0/1"&gt;Alexander Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1"&gt;Soheil Feizi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning of Continuous-time Bayesian Networks through Interventions. (arXiv:2105.14742v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14742</id>
        <link href="http://arxiv.org/abs/2105.14742"/>
        <updated>2021-06-14T01:38:56.248Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning structures and parameters of
Continuous-time Bayesian Networks (CTBNs) from time-course data under minimal
experimental resources. In practice, the cost of generating experimental data
poses a bottleneck, especially in the natural and social sciences. A popular
approach to overcome this is Bayesian optimal experimental design (BOED).
However, BOED becomes infeasible in high-dimensional settings, as it involves
integration over all possible experimental outcomes. We propose a novel
criterion for experimental design based on a variational approximation of the
expected information gain. We show that for CTBNs, a semi-analytical expression
for this criterion can be calculated for structure and parameter learning. By
doing so, we can replace sampling over experimental outcomes by solving the
CTBNs master-equation, for which scalable approximations exist. This alleviates
the computational burden of sampling possible experimental outcomes in
high-dimensions. We employ this framework in order to recommend interventional
sequences. In this context, we extend the CTBN model to conditional CTBNs in
order to incorporate interventions. We demonstrate the performance of our
criterion on synthetic and real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Linzner_D/0/1/0/all/0/1"&gt;Dominik Linzner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Koeppl_H/0/1/0/all/0/1"&gt;Heinz Koeppl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses. (arXiv:2106.06519v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06519</id>
        <link href="http://arxiv.org/abs/2106.06519"/>
        <updated>2021-06-14T01:38:56.241Z</updated>
        <summary type="html"><![CDATA[Spoken Language Understanding (SLU) systems parse speech into semantic
structures like dialog acts and slots. This involves the use of an Automatic
Speech Recognizer (ASR) to transcribe speech into multiple text alternatives
(hypotheses). Transcription errors, common in ASRs, impact downstream SLU
performance negatively. Approaches to mitigate such errors involve using richer
information from the ASR, either in form of N-best hypotheses or word-lattices.
We hypothesize that transformer models learn better with a simpler utterance
representation using the concatenation of the N-best ASR alternatives, where
each alternative is separated by a special delimiter [SEP]. In our work, we
test our hypothesis by using concatenated N-best ASR alternatives as the input
to transformer encoder models, namely BERT and XLM-RoBERTa, and achieve
performance equivalent to the prior state-of-the-art model on DSTC2 dataset. We
also show that our approach significantly outperforms the prior
state-of-the-art when subjected to the low data regime. Additionally, this
methodology is accessible to users of third-party ASR APIs which do not provide
word-lattice information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ganesan_K/0/1/0/all/0/1"&gt;Karthik Ganesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bamdev_P/0/1/0/all/0/1"&gt;Pakhi Bamdev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+B_J/0/1/0/all/0/1"&gt;Jaivarsan B&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venugopal_A/0/1/0/all/0/1"&gt;Amresh Venugopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tushar_A/0/1/0/all/0/1"&gt;Abhinav Tushar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Continual Adaptation with Active Self-Training. (arXiv:2106.06526v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06526</id>
        <link href="http://arxiv.org/abs/2106.06526"/>
        <updated>2021-06-14T01:38:56.234Z</updated>
        <summary type="html"><![CDATA[Models trained with offline data often suffer from continual distribution
shifts and expensive labeling in changing environments. This calls for a new
online learning paradigm where the learner can continually adapt to changing
environments with limited labels. In this paper, we propose a new online
setting -- Online Active Continual Adaptation, where the learner aims to
continually adapt to changing distributions using both unlabeled samples and
active queries of limited labels. To this end, we propose Online Self-Adaptive
Mirror Descent (OSAMD), which adopts an online teacher-student structure to
enable online self-training from unlabeled data, and a margin-based criterion
that decides whether to query the labels to track changing distributions.
Theoretically, we show that, in the separable case, OSAMD has an $O({T}^{1/2})$
dynamic regret bound under mild assumptions, which is even tighter than the
lower bound $\Omega(T^{2/3})$ of traditional online learning with full labels.
In the general case, we show a regret bound of $O({\alpha^*}^{1/3} {T}^{2/3} +
\alpha^* T)$, where $\alpha^*$ denotes the separability of domains and is
usually small. Our theoretical results show that OSAMD can fast adapt to
changing environments with active queries. Empirically, we demonstrate that
OSAMD achieves favorable regrets under changing environments with limited
labels on both simulated and real-world data, which corroborates our
theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shiji Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shanghang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lianzhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Heng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wenwu Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Knowledge Graph Alignment by Probabilistic Reasoning and Semantic Embedding. (arXiv:2105.05596v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05596</id>
        <link href="http://arxiv.org/abs/2105.05596"/>
        <updated>2021-06-14T01:38:56.227Z</updated>
        <summary type="html"><![CDATA[Knowledge Graph (KG) alignment is to discover the mappings (i.e., equivalent
entities, relations, and others) between two KGs. The existing methods can be
divided into the embedding-based models, and the conventional reasoning and
lexical matching based systems. The former compute the similarity of entities
via their cross-KG embeddings, but they usually rely on an ideal supervised
learning setting for good performance and lack appropriate reasoning to avoid
logically wrong mappings; while the latter address the reasoning issue but are
poor at utilizing the KG graph structures and the entity contexts. In this
study, we aim at combining the above two solutions and thus propose an
iterative framework named PRASE which is based on probabilistic reasoning and
semantic embedding. It learns the KG embeddings via entity mappings from a
probabilistic reasoning system named PARIS, and feeds the resultant entity
mappings and embeddings back into PARIS for augmentation. The PRASE framework
is compatible with different embedding-based models, and our experiments on
multiple datasets have demonstrated its state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhiyuan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yuejia Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Trained One-class Classification for Unsupervised Anomaly Detection. (arXiv:2106.06115v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06115</id>
        <link href="http://arxiv.org/abs/2106.06115"/>
        <updated>2021-06-14T01:38:56.220Z</updated>
        <summary type="html"><![CDATA[Anomaly detection (AD), separating anomalies from normal data, has various
applications across domains, from manufacturing to healthcare. While most
previous works have shown to be effective for cases with fully or partially
labeled data, they are less practical for AD applications due to tedious data
labeling processes. In this work, we focus on unsupervised AD problems whose
entire training data are unlabeled and may contain both normal and anomalous
samples. To tackle this problem, we build a robust one-class classification
framework via data refinement. To refine the data accurately, we propose an
ensemble of one-class classifiers, each of which is trained on a disjoint
subset of training data. Moreover, we propose a self-training of deep
representation one-class classifiers (STOC) that iteratively refines the data
and deep representations. In experiments, we show the efficacy of our method
for unsupervised anomaly detection on benchmarks from image and tabular data
domains. For example, with a 10% anomaly ratio on CIFAR-10 data, the proposed
method outperforms state-of-the-art one-class classification method by 6.3 AUC
and 12.5 average precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jinsung Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kihyuk Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1"&gt;Sercan O. Arik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chen-Yu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1"&gt;Tomas Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inter-domain Multi-relational Link Prediction. (arXiv:2106.06171v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06171</id>
        <link href="http://arxiv.org/abs/2106.06171"/>
        <updated>2021-06-14T01:38:56.202Z</updated>
        <summary type="html"><![CDATA[Multi-relational graph is a ubiquitous and important data structure, allowing
flexible representation of multiple types of interactions and relations between
entities. Similar to other graph-structured data, link prediction is one of the
most important tasks on multi-relational graphs and is often used for knowledge
completion. When related graphs coexist, it is of great benefit to build a
larger graph via integrating the smaller ones. The integration requires
predicting hidden relational connections between entities belonged to different
graphs (inter-domain link prediction). However, this poses a real challenge to
existing methods that are exclusively designed for link prediction between
entities of the same graph only (intra-domain link prediction). In this study,
we propose a new approach to tackle the inter-domain link prediction problem by
softly aligning the entity distributions between different domains with optimal
transport and maximum mean discrepancy regularizers. Experiments on real-world
datasets show that optimal transport regularizer is beneficial and considerably
improves the performance of baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Phuc_L/0/1/0/all/0/1"&gt;Luu Huu Phuc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1"&gt;Koh Takeuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okajima_S/0/1/0/all/0/1"&gt;Seiji Okajima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolmachev_A/0/1/0/all/0/1"&gt;Arseny Tolmachev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takebayashi_T/0/1/0/all/0/1"&gt;Tomoyoshi Takebayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maruhashi_K/0/1/0/all/0/1"&gt;Koji Maruhashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1"&gt;Hisashi Kashima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Free Learning for Two-Player Zero-Sum Partially Observable Markov Games with Perfect Recall. (arXiv:2106.06279v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06279</id>
        <link href="http://arxiv.org/abs/2106.06279"/>
        <updated>2021-06-14T01:38:56.196Z</updated>
        <summary type="html"><![CDATA[We study the problem of learning a Nash equilibrium (NE) in an imperfect
information game (IIG) through self-play. Precisely, we focus on two-player,
zero-sum, episodic, tabular IIG under the perfect-recall assumption where the
only feedback is realizations of the game (bandit feedback). In particular, the
dynamic of the IIG is not known -- we can only access it by sampling or
interacting with a game simulator. For this learning setting, we provide the
Implicit Exploration Online Mirror Descent (IXOMD) algorithm. It is a
model-free algorithm with a high-probability bound on the convergence rate to
the NE of order $1/\sqrt{T}$ where $T$ is the number of played games. Moreover,
IXOMD is computationally efficient as it needs to perform the updates only
along the sampled trajectory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kozuno_T/0/1/0/all/0/1"&gt;Tadashi Kozuno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Menard_P/0/1/0/all/0/1"&gt;Pierre M&amp;#xe9;nard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Munos_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Munos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Valko_M/0/1/0/all/0/1"&gt;Michal Valko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning via Persistency of Excitation. (arXiv:2106.02078v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02078</id>
        <link href="http://arxiv.org/abs/2106.02078"/>
        <updated>2021-06-14T01:38:56.189Z</updated>
        <summary type="html"><![CDATA[Improving adversarial robustness of neural networks remains a major
challenge. Fundamentally, training a network is a parameter estimation problem.
In adaptive control theory, maintaining persistency of excitation (PoE) is
integral to ensuring convergence of parameter estimates in dynamical systems to
their robust optima. In this work, we show that network training using gradient
descent is equivalent to a dynamical system parameter estimation problem.
Leveraging this relationship, we prove a sufficient condition for PoE of
gradient descent is achieved when the learning rate is less than the inverse of
the Lipschitz constant of the gradient of loss function. We provide an
efficient technique for estimating the corresponding Lipschitz constant using
extreme value theory and demonstrate that by only scaling the learning rate
schedule we can increase adversarial accuracy by up to 15% points on benchmark
datasets. Our approach also universally increases the adversarial accuracy by
0.1% to 0.3% points in various state-of-the-art adversarially trained models on
the AutoAttack benchmark, where every small margin of improvement is
significant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sridhar_K/0/1/0/all/0/1"&gt;Kaustubh Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sokolsky_O/0/1/0/all/0/1"&gt;Oleg Sokolsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_I/0/1/0/all/0/1"&gt;Insup Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Weimer_J/0/1/0/all/0/1"&gt;James Weimer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymptotics of representation learning in finite Bayesian neural networks. (arXiv:2106.00651v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00651</id>
        <link href="http://arxiv.org/abs/2106.00651"/>
        <updated>2021-06-14T01:38:56.183Z</updated>
        <summary type="html"><![CDATA[Recent works have suggested that finite Bayesian neural networks may
outperform their infinite cousins because finite networks can flexibly adapt
their internal representations. However, our theoretical understanding of how
the learned hidden layer representations of finite networks differ from the
fixed representations of infinite networks remains incomplete. Perturbative
finite-width corrections to the network prior and posterior have been studied,
but the asymptotics of learned features have not been fully characterized.
Here, we argue that the leading finite-width corrections to the average feature
kernels for any Bayesian network with linear readout and quadratic cost have a
largely universal form. We illustrate this explicitly for two classes of fully
connected networks: deep linear networks and networks with a single nonlinear
hidden layer. Our results begin to elucidate which features of data wide
Bayesian neural networks learn to represent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zavatone_Veth_J/0/1/0/all/0/1"&gt;Jacob A. Zavatone-Veth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canatar_A/0/1/0/all/0/1"&gt;Abdulkadir Canatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pehlevan_C/0/1/0/all/0/1"&gt;Cengiz Pehlevan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Price graphs: Utilizing the structural information of financial time series for stock prediction. (arXiv:2106.02522v2 [q-fin.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02522</id>
        <link href="http://arxiv.org/abs/2106.02522"/>
        <updated>2021-06-14T01:38:56.176Z</updated>
        <summary type="html"><![CDATA[Stock prediction, with the purpose of forecasting the future price trends of
stocks, is crucial for maximizing profits from stock investments. While great
research efforts have been devoted to exploiting deep neural networks for
improved stock prediction, two major issues still exist in recent studies.
First, the capture of long-range dependencies in time series is not
sufficiently addressed. Second, the chaotic property of financial time series
fundamentally lowers prediction performance. In this study, we propose a novel
framework to address both issues regarding stock prediction. Specifically, in
terms of transforming time series into complex networks, we convert market
price series into graphs. Then, structural information, referring to
associations among temporal points and the node weights, is extracted from the
mapped graphs to resolve the problems regarding long-range dependencies and the
chaotic property. We take graph embeddings to represent the associations among
temporal points as the prediction model inputs. Node weights are used as a
priori knowledge to enhance the learning of temporal attention. The
effectiveness of our proposed framework is validated using real-world stock
data, and our approach obtains the best performance among several
state-of-the-art benchmarks. Moreover, in the conducted trading simulations,
our framework further obtains the highest cumulative profits. Our results
supplement the existing applications of complex network methods in the
financial realm and provide insightful implications for investment applications
regarding decision support in financial markets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Wu_J/0/1/0/all/0/1"&gt;Junran Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xueyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Li_S/0/1/0/all/0/1"&gt;Shangzhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jichang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction. (arXiv:2105.10446v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10446</id>
        <link href="http://arxiv.org/abs/2105.10446"/>
        <updated>2021-06-14T01:38:56.157Z</updated>
        <summary type="html"><![CDATA[This work attempts to provide a plausible theoretical framework that aims to
interpret modern deep (convolutional) networks from the principles of data
compression and discriminative representation. We argue that for
high-dimensional multi-class data, the optimal linear discriminative
representation maximizes the coding rate difference between the whole dataset
and the average of all the subsets. We show that the basic iterative gradient
ascent scheme for optimizing the rate reduction objective naturally leads to a
multi-layer deep network, named ReduNet, which shares common characteristics of
modern deep networks. The deep layered architectures, linear and nonlinear
operators, and even parameters of the network are all explicitly constructed
layer-by-layer via forward propagation, although they are amenable to
fine-tuning via back propagation. All components of so-obtained ``white-box''
network have precise optimization, statistical, and geometric interpretation.
Moreover, all linear operators of the so-derived network naturally become
multi-channel convolutions when we enforce classification to be rigorously
shift-invariant. The derivation in the invariant setting suggests a trade-off
between sparsity and invariance, and also indicates that such a deep
convolution network is significantly more efficient to construct and learn in
the spectral domain. Our preliminary simulations and experiments clearly verify
the effectiveness of both the rate reduction objective and the associated
ReduNet. All code and data are available at https://github.com/Ma-Lab-Berkeley.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1"&gt;Kwan Ho Ryan Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chong You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haozhi Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1"&gt;John Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scale-invariant scale-channel networks: Deep networks that generalise to previously unseen scales. (arXiv:2106.06418v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06418</id>
        <link href="http://arxiv.org/abs/2106.06418"/>
        <updated>2021-06-14T01:38:56.150Z</updated>
        <summary type="html"><![CDATA[The ability to handle large scale variations is crucial for many real world
visual tasks. A straightforward approach for handling scale in a deep network
is to process an image at several scales simultaneously in a set of scale
channels. Scale invariance can then, in principle, be achieved by using weight
sharing between the scale channels together with max or average pooling over
the outputs from the scale channels. The ability of such scale channel networks
to generalise to scales not present in the training set over significant scale
ranges has, however, not previously been explored.

In this paper, we present a systematic study of this methodology by
implementing different types of scale channel networks and evaluating their
ability to generalise to previously unseen scales. We develop a formalism for
analysing the covariance and invariance properties of scale channel networks,
and explore how different design choices, unique to scaling transformations,
affect the overall performance of scale channel networks. We first show that
two previously proposed scale channel network designs do not generalise well to
scales not present in the training set. We explain theoretically and
demonstrate experimentally why generalisation fails in these cases.

We then propose a new type of foveated scale channel architecture}, where the
scale channels process increasingly larger parts of the image with decreasing
resolution. This new type of scale channel network is shown to generalise
extremely well, provided sufficient image resolution and the absence of
boundary effects. Our proposed FovMax and FovAvg networks perform almost
identically over a scale range of 8, also when training on single scale
training data, and do also give improved performance when learning from
datasets with large scale variations in the small sample regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jansson_Y/0/1/0/all/0/1"&gt;Ylva Jansson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindeberg_T/0/1/0/all/0/1"&gt;Tony Lindeberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRLD-SP: A Deep Reinforcement Learning-based Dynamic Service Placement in Edge-Enabled Internet of Vehicles. (arXiv:2106.06291v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.06291</id>
        <link href="http://arxiv.org/abs/2106.06291"/>
        <updated>2021-06-14T01:38:56.142Z</updated>
        <summary type="html"><![CDATA[The growth of 5G and edge computing has enabled the emergence of Internet of
Vehicles. It supports different types of services with different resource and
service requirements. However, limited resources at the edge, high mobility of
vehicles, increasing demand, and dynamicity in service request-types have made
service placement a challenging task. A typical static placement solution is
not effective as it does not consider the traffic mobility and service
dynamics. Handling dynamics in IoV for service placement is an important and
challenging problem which is the primary focus of our work in this paper. We
propose a Deep Reinforcement Learning-based Dynamic Service Placement (DRLD-SP)
framework with the objective of minimizing the maximum edge resource usage and
service delay while considering the vehicle's mobility, varying demand, and
dynamics in the requests for different types of services. We use SUMO and
MATLAB to carry out simulation experiments. The experimental results show that
the proposed DRLD-SP approach is effective and outperforms other static and
dynamic placement approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Talpur_A/0/1/0/all/0/1"&gt;Anum Talpur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurusamy_M/0/1/0/all/0/1"&gt;Mohan Gurusamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Beyond Linear Transformers with Recurrent Fast Weight Programmers. (arXiv:2106.06295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06295</id>
        <link href="http://arxiv.org/abs/2106.06295"/>
        <updated>2021-06-14T01:38:56.135Z</updated>
        <summary type="html"><![CDATA[Transformers with linearised attention ("linear Transformers") have
demonstrated the practical scalability and effectiveness of outer product-based
Fast Weight Programmers (FWPs) from the '90s. However, the original FWP
formulation is more general than the one of linear Transformers: a slow neural
network (NN) continually reprograms the weights of a fast NN with arbitrary NN
architectures. In existing linear Transformers, both NNs are feedforward and
consist of a single layer. Here we explore new variations by adding recurrence
to the slow and fast nets. We evaluate our novel recurrent FWPs (RFWPs) on two
synthetic algorithmic tasks (code execution and sequential ListOps),
Wikitext-103 language models, and on the Atari 2600 2D game environment. Our
models exhibit properties of Transformers and RNNs. In the reinforcement
learning setting, we report large improvements over LSTM in several Atari
games. Our code is public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1"&gt;Kazuki Irie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schlag_I/0/1/0/all/0/1"&gt;Imanol Schlag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Csordas_R/0/1/0/all/0/1"&gt;R&amp;#xf3;bert Csord&amp;#xe1;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Catch-A-Waveform: Learning to Generate Audio from a Single Short Example. (arXiv:2106.06426v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.06426</id>
        <link href="http://arxiv.org/abs/2106.06426"/>
        <updated>2021-06-14T01:38:56.127Z</updated>
        <summary type="html"><![CDATA[Models for audio generation are typically trained on hours of recordings.
Here, we illustrate that capturing the essence of an audio source is typically
possible from as little as a few tens of seconds from a single training signal.
Specifically, we present a GAN-based generative model that can be trained on
one short audio signal from any domain (e.g. speech, music, etc.) and does not
require pre-training or any other form of external supervision. Once trained,
our model can generate random samples of arbitrary duration that maintain
semantic similarity to the training waveform, yet exhibit new compositions of
its audio primitives. This enables a long line of interesting applications,
including generating new jazz improvisations or new a-cappella rap variants
based on a single short example, producing coherent modifications to famous
songs (e.g. adding a new verse to a Beatles song based solely on the original
recording), filling-in of missing parts (inpainting), extending the bandwidth
of a speech signal (super-resolution), and enhancing old recordings without
access to any clean training example. We show that in all cases, no more than
20 seconds of training audio commonly suffice for our model to achieve
state-of-the-art results. This is despite its complete lack of prior knowledge
about the nature of audio signals in general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Greshler_G/0/1/0/all/0/1"&gt;Gal Greshler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaham_T/0/1/0/all/0/1"&gt;Tamar Rott Shaham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michaeli_T/0/1/0/all/0/1"&gt;Tomer Michaeli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Robustness through the Lens of Causality. (arXiv:2106.06196v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06196</id>
        <link href="http://arxiv.org/abs/2106.06196"/>
        <updated>2021-06-14T01:38:56.111Z</updated>
        <summary type="html"><![CDATA[The adversarial vulnerability of deep neural networks has attracted
significant attention in machine learning. From a causal viewpoint, adversarial
attacks can be considered as a specific type of distribution change on natural
data. As causal reasoning has an instinct for modeling distribution change, we
propose to incorporate causality into mitigating adversarial vulnerability.
However, causal formulations of the intuition of adversarial attack and the
development of robust DNNs are still lacking in the literature. To bridge this
gap, we construct a causal graph to model the generation process of adversarial
examples and define the adversarial distribution to formalize the intuition of
adversarial attacks. From a causal perspective, we find that the label is
spuriously correlated with the style (content-independent) information when an
instance is given. The spurious correlation implies that the adversarial
distribution is constructed via making the statistical conditional association
between style information and labels drastically different from that in natural
distribution. Thus, DNNs that fit the spurious correlation are vulnerable to
the adversarial distribution. Inspired by the observation, we propose the
adversarial distribution alignment method to eliminate the difference between
the natural distribution and the adversarial distribution. Extensive
experiments demonstrate the efficacy of the proposed method. Our method can be
seen as the first attempt to leverage causality for mitigating adversarial
vulnerability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yonggang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Mingming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1"&gt;Xinmei Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guarantees for Tuning the Step Size using a Learning-to-Learn Approach. (arXiv:2006.16495v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16495</id>
        <link href="http://arxiv.org/abs/2006.16495"/>
        <updated>2021-06-14T01:38:56.104Z</updated>
        <summary type="html"><![CDATA[Choosing the right parameters for optimization algorithms is often the key to
their success in practice. Solving this problem using a learning-to-learn
approach -- using meta-gradient descent on a meta-objective based on the
trajectory that the optimizer generates -- was recently shown to be effective.
However, the meta-optimization problem is difficult. In particular, the
meta-gradient can often explode/vanish, and the learned optimizer may not have
good generalization performance if the meta-objective is not chosen carefully.
In this paper we give meta-optimization guarantees for the learning-to-learn
approach on a simple problem of tuning the step size for quadratic loss. Our
results show that the na\"ive objective suffers from meta-gradient
explosion/vanishing problem. Although there is a way to design the
meta-objective so that the meta-gradient remains polynomially bounded,
computing the meta-gradient directly using backpropagation leads to numerical
issues. We also characterize when it is necessary to compute the meta-objective
on a separate validation set to ensure the generalization performance of the
learned optimizer. Finally, we verify our results empirically and show that a
similar phenomenon appears even for more complicated learned optimizers
parametrized by neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yuan_S/0/1/0/all/0/1"&gt;Shuai Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ge_R/0/1/0/all/0/1"&gt;Rong Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Game Theoretic Neural Optimizer. (arXiv:2105.03788v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03788</id>
        <link href="http://arxiv.org/abs/2105.03788"/>
        <updated>2021-06-14T01:38:56.097Z</updated>
        <summary type="html"><![CDATA[The connection between training deep neural networks (DNNs) and optimal
control theory (OCT) has attracted considerable attention as a principled tool
of algorithmic design. Despite few attempts being made, they have been limited
to architectures where the layer propagation resembles a Markovian dynamical
system. This casts doubts on their flexibility to modern networks that heavily
rely on non-Markovian dependencies between layers (e.g. skip connections in
residual networks). In this work, we propose a novel dynamic game perspective
by viewing each layer as a player in a dynamic game characterized by the DNN
itself. Through this lens, different classes of optimizers can be seen as
matching different types of Nash equilibria, depending on the implicit
information structure of each (p)layer. The resulting method, called Dynamic
Game Theoretic Neural Optimizer (DGNOpt), not only generalizes OCT-inspired
optimizers to richer network class; it also motivates a new training principle
by solving a multi-player cooperative game. DGNOpt shows convergence
improvements over existing methods on image classification datasets with
residual and inception networks. Our work marries strengths from both OCT and
game theory, paving ways to new algorithmic opportunities from robust optimal
control and bandit-based optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guan-Horng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianrong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1"&gt;Evangelos A. Theodorou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Anomaly Detection Ensembles using Item Response Theory. (arXiv:2106.06243v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06243</id>
        <link href="http://arxiv.org/abs/2106.06243"/>
        <updated>2021-06-14T01:38:56.090Z</updated>
        <summary type="html"><![CDATA[Constructing an ensemble from a heterogeneous set of unsupervised anomaly
detection methods is challenging because the class labels or the ground truth
is unknown. Thus, traditional ensemble techniques that use the response
variable or the class labels cannot be used to construct an ensemble for
unsupervised anomaly detection.

We use Item Response Theory (IRT) -- a class of models used in educational
psychometrics to assess student and test question characteristics -- to
construct an unsupervised anomaly detection ensemble. IRT's latent trait
computation lends itself to anomaly detection because the latent trait can be
used to uncover the hidden ground truth. Using a novel IRT mapping to the
anomaly detection problem, we construct an ensemble that can downplay noisy,
non-discriminatory methods and accentuate sharper methods. We demonstrate the
effectiveness of the IRT ensemble on an extensive data repository, by comparing
its performance to other ensemble techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kandanaarachchi_S/0/1/0/all/0/1"&gt;Sevvandi Kandanaarachchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Average Losses for Partial-Label Learning. (arXiv:2106.06152v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06152</id>
        <link href="http://arxiv.org/abs/2106.06152"/>
        <updated>2021-06-14T01:38:56.083Z</updated>
        <summary type="html"><![CDATA[Partial-label (PL) learning is a typical weakly supervised classification
problem, where a PL of an instance is a set of candidate labels such that a
fixed but unknown candidate is the true label. For PL learning, there are two
lines of research: (a) the identification-based strategy (IBS) purifies each
label set and extracts the true label; (b) the average-based strategy (ABS)
treats all candidates equally for training. In the past two decades, IBS was a
much hotter topic than ABS, since it was believed that IBS is more promising.
In this paper, we theoretically analyze ABS and find it also promising in the
sense of the robustness of its loss functions. Specifically, we consider five
problem settings for the generation of clean or noisy PLs, and we prove that
average PL losses with bounded multi-class losses are always robust under mild
assumptions on the domination of true labels, while average PL losses with
unbounded multi-class losses (e.g., the cross-entropy loss) may not be robust.
We also conduct experiments to validate our theoretical findings. Note that IBS
is heuristic, and we cannot prove its robustness by a similar proof technique;
hence, ABS is more advantageous from a theoretical point of view, and it is
worth paying attention to the design of more advanced PL learning methods
following ABS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1"&gt;Jiaqi Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Miao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1"&gt;Bo An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1"&gt;Xin Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03743</id>
        <link href="http://arxiv.org/abs/2106.03743"/>
        <updated>2021-06-14T01:38:56.048Z</updated>
        <summary type="html"><![CDATA[We investigate the reasons for the performance degradation incurred with
batch-independent normalization. We find that the prototypical techniques of
layer normalization and instance normalization both induce the appearance of
failure modes in the neural network's pre-activations: (i) layer normalization
induces a collapse towards channel-wise constant functions; (ii) instance
normalization induces a lack of variability in instance statistics, symptomatic
of an alteration of the expressivity. To alleviate failure mode (i) without
aggravating failure mode (ii), we introduce the technique "Proxy Normalization"
that normalizes post-activations using a proxy distribution. When combined with
layer normalization or group normalization, this batch-independent
normalization emulates batch normalization's behavior and consistently matches
or exceeds its performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Labatie_A/0/1/0/all/0/1"&gt;Antoine Labatie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masters_D/0/1/0/all/0/1"&gt;Dominic Masters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1"&gt;Zach Eaton-Rosen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1"&gt;Carlo Luschi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Twins: Revisiting the Design of Spatial Attention in Vision Transformers. (arXiv:2104.13840v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13840</id>
        <link href="http://arxiv.org/abs/2104.13840"/>
        <updated>2021-06-14T01:38:55.853Z</updated>
        <summary type="html"><![CDATA[Very recently, a variety of vision transformer architectures for dense
prediction tasks have been proposed and they show that the design of spatial
attention is critical to their success in these tasks. In this work, we revisit
the design of the spatial attention and demonstrate that a carefully-devised
yet simple spatial attention mechanism performs favourably against the
state-of-the-art schemes. As a result, we propose two vision transformer
architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures
are highly-efficient and easy to implement, only involving matrix
multiplications that are highly optimized in modern deep learning frameworks.
More importantly, the proposed architectures achieve excellent performance on a
wide range of visual tasks including imagelevel classification as well as dense
detection and segmentation. The simplicity and strong performance suggest that
our proposed architectures may serve as stronger backbones for many vision
tasks. Our code will be released soon at
https://github.com/Meituan-AutoML/Twins .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1"&gt;Xiangxiang Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zhi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Haibing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiaolin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1"&gt;Huaxia Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Moreau-Yosida $f$-divergences. (arXiv:2102.13416v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13416</id>
        <link href="http://arxiv.org/abs/2102.13416"/>
        <updated>2021-06-14T01:38:55.846Z</updated>
        <summary type="html"><![CDATA[Variational representations of $f$-divergences are central to many machine
learning algorithms, with Lipschitz constrained variants recently gaining
attention. Inspired by this, we define the Moreau-Yosida approximation of
$f$-divergences with respect to the Wasserstein-$1$ metric. The corresponding
variational formulas provide a generalization of a number of recent results,
novel special cases of interest and a relaxation of the hard Lipschitz
constraint. Additionally, we prove that the so-called tight variational
representation of $f$-divergences can be to be taken over the quotient space of
Lipschitz functions, and give a characterization of functions achieving the
supremum in the variational representation. On the practical side, we propose
an algorithm to calculate the tight convex conjugate of $f$-divergences
compatible with automatic differentiation frameworks. As an application of our
results, we propose the Moreau-Yosida $f$-GAN, providing an implementation of
the variational formulas for the Kullback-Leibler, reverse Kullback-Leibler,
$\chi^2$, reverse $\chi^2$, squared Hellinger, Jensen-Shannon, Jeffreys,
triangular discrimination and total variation divergences as GANs trained on
CIFAR-10, leading to competitive results and a simple solution to the problem
of uniqueness of the optimal critic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Terjek_D/0/1/0/all/0/1"&gt;D&amp;#xe1;vid Terj&amp;#xe9;k&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Performance FPGA-based Accelerator for Bayesian Neural Networks. (arXiv:2105.09163v2 [cs.AR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09163</id>
        <link href="http://arxiv.org/abs/2105.09163"/>
        <updated>2021-06-14T01:38:55.837Z</updated>
        <summary type="html"><![CDATA[Neural networks (NNs) have demonstrated their potential in a wide range of
applications such as image recognition, decision making or recommendation
systems. However, standard NNs are unable to capture their model uncertainty
which is crucial for many safety-critical applications including healthcare and
autonomous vehicles. In comparison, Bayesian neural networks (BNNs) are able to
express uncertainty in their prediction via a mathematical grounding.
Nevertheless, BNNs have not been as widely used in industrial practice, mainly
because of their expensive computational cost and limited hardware performance.
This work proposes a novel FPGA-based hardware architecture to accelerate BNNs
inferred through Monte Carlo Dropout. Compared with other state-of-the-art BNN
accelerators, the proposed accelerator can achieve up to 4 times higher energy
efficiency and 9 times better compute efficiency. Considering partial Bayesian
inference, an automatic framework is proposed, which explores the trade-off
between hardware and algorithmic performance. Extensive experiments are
conducted to demonstrate that our proposed framework can effectively find the
optimal points in the design space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Hongxiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferianc_M/0/1/0/all/0/1"&gt;Martin Ferianc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1"&gt;Miguel Rodrigues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hongyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1"&gt;Xinyu Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luk_W/0/1/0/all/0/1"&gt;Wayne Luk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Extend Molecular Scaffolds with Structural Motifs. (arXiv:2103.03864v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03864</id>
        <link href="http://arxiv.org/abs/2103.03864"/>
        <updated>2021-06-14T01:38:55.830Z</updated>
        <summary type="html"><![CDATA[Recent advancements in deep learning-based modeling of molecules promise to
accelerate in silico drug discovery. A plethora of generative models is
available, building molecules either atom-by-atom and bond-by-bond or
fragment-by-fragment. However, many drug discovery projects require a fixed
scaffold to be present in the generated molecule, and incorporating that
constraint has only recently been explored. In this work, we propose a new
graph-based model that naturally supports scaffolds as initial seed of the
generative procedure, which is possible because our model is not conditioned on
the generation history. At the same time, our generation procedure can flexibly
choose between adding individual atoms and entire fragments. We show that
training using a randomized generation order is necessary for good performance
when extending scaffolds, and that the results are further improved by
increasing the fragment vocabulary size. Our model pushes the state-of-the-art
of graph-based molecule generation, while being an order of magnitude faster to
train and sample from than existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maziarz_K/0/1/0/all/0/1"&gt;Krzysztof Maziarz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jackson_Flux_H/0/1/0/all/0/1"&gt;Henry Jackson-Flux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cameron_P/0/1/0/all/0/1"&gt;Pashmina Cameron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sirockin_F/0/1/0/all/0/1"&gt;Finton Sirockin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1"&gt;Nadine Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stiefl_N/0/1/0/all/0/1"&gt;Nikolaus Stiefl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segler_M/0/1/0/all/0/1"&gt;Marwin Segler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1"&gt;Marc Brockschmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Shapley Value of Classifiers in Ensemble Games. (arXiv:2101.02153v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02153</id>
        <link href="http://arxiv.org/abs/2101.02153"/>
        <updated>2021-06-14T01:38:55.810Z</updated>
        <summary type="html"><![CDATA[What is the value of an individual model in an ensemble of binary
classifiers? We answer this question by introducing a class of transferable
utility cooperative games called \textit{ensemble games}. In machine learning
ensembles, pre-trained models cooperate to make classification decisions. To
quantify the importance of models in these ensemble games, we define
\textit{Troupe} -- an efficient algorithm which allocates payoffs based on
approximate Shapley values of the classifiers. We argue that the Shapley value
of models in these games is an effective decision metric for choosing a high
performing subset of models from the ensemble. Our analytical findings prove
that our Shapley value estimation scheme is precise and scalable; its
performance increases with size of the dataset and ensemble. Empirical results
on real world graph classification tasks demonstrate that our algorithm
produces high quality estimates of the Shapley value. We find that Shapley
values can be utilized for ensemble pruning, and that adversarial models
receive a low valuation. Complex classifiers are frequently found to be
responsible for both correct and incorrect classification decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rozemberczki_B/0/1/0/all/0/1"&gt;Benedek Rozemberczki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_R/0/1/0/all/0/1"&gt;Rik Sarkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RNN with Particle Flow for Probabilistic Spatio-temporal Forecasting. (arXiv:2106.06064v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06064</id>
        <link href="http://arxiv.org/abs/2106.06064"/>
        <updated>2021-06-14T01:38:55.803Z</updated>
        <summary type="html"><![CDATA[Spatio-temporal forecasting has numerous applications in analyzing wireless,
traffic, and financial networks. Many classical statistical models often fall
short in handling the complexity and high non-linearity present in time-series
data. Recent advances in deep learning allow for better modelling of spatial
and temporal dependencies. While most of these models focus on obtaining
accurate point forecasts, they do not characterize the prediction uncertainty.
In this work, we consider the time-series data as a random realization from a
nonlinear state-space model and target Bayesian inference of the hidden states
for probabilistic forecasting. We use particle flow as the tool for
approximating the posterior distribution of the states, as it is shown to be
highly effective in complex, high-dimensional settings. Thorough
experimentation on several real world time-series datasets demonstrates that
our approach provides better characterization of uncertainty while maintaining
comparable accuracy to the state-of-the art point forecasting methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pal_S/0/1/0/all/0/1"&gt;Soumyasundar Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ma_L/0/1/0/all/0/1"&gt;Liheng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yingxue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Coates_M/0/1/0/all/0/1"&gt;Mark Coates&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Binary Classification from Multiple Unlabeled Datasets via Surrogate Set Classification. (arXiv:2102.00678v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00678</id>
        <link href="http://arxiv.org/abs/2102.00678"/>
        <updated>2021-06-14T01:38:55.796Z</updated>
        <summary type="html"><![CDATA[To cope with high annotation costs, training a classifier only from weakly
supervised data has attracted a great deal of attention these days. Among
various approaches, strengthening supervision from completely unsupervised
classification is a promising direction, which typically employs class priors
as the only supervision and trains a binary classifier from unlabeled (U)
datasets. While existing risk-consistent methods are theoretically grounded
with high flexibility, they can learn only from two U sets. In this paper, we
propose a new approach for binary classification from $m$ U-sets for $m\ge2$.
Our key idea is to consider an auxiliary classification task called surrogate
set classification (SSC), which is aimed at predicting from which U set each
observed data is drawn. SSC can be solved by a standard (multi-class)
classification method, and we use the SSC solution to obtain the final binary
classifier through a certain linear-fractional transformation. We built our
method in a flexible and efficient end-to-end deep learning framework and prove
it to be classifier-consistent. Through experiments, we demonstrate the
superiority of our proposed method over state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1"&gt;Nan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1"&gt;Shida Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1"&gt;Issei Sato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surface Warping Incorporating Machine Learning Assisted Domain Likelihood Estimation: A New Paradigm in Mine Geology Modelling and Automation. (arXiv:2103.03923v2 [physics.geo-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03923</id>
        <link href="http://arxiv.org/abs/2103.03923"/>
        <updated>2021-06-14T01:38:55.790Z</updated>
        <summary type="html"><![CDATA[This paper illustrates an application of machine learning (ML) within a
complex system that performs grade estimation. In surface mining, assay
measurements taken from production drilling often provide useful information
that allows initially inaccurate surfaces created using sparse exploration data
to be revised and subsequently improved. Recently, a Bayesian warping technique
has been proposed to reshape modeled surfaces using geochemical and spatial
constraints imposed by newly acquired blasthole data. This paper focuses on
incorporating machine learning into this warping framework to make the
likelihood computation generalizable. The technique works by adjusting the
position of vertices on the surface to maximize the integrity of modeled
geological boundaries with respect to sparse geochemical observations. Its
foundation is laid by a Bayesian derivation in which the geological domain
likelihood given the chemistry, p(g|c), plays a similar role to p(y(c)|g). This
observation allows a manually calibrated process centered around the latter to
be automated since ML techniques may be used to estimate the former in a
data-driven way. Machine learning performance is evaluated for gradient
boosting, neural network, random forest and other classifiers in a binary and
multi-class context using precision and recall rates. Once ML likelihood
estimators are integrated in the surface warping framework, surface shaping
performance is evaluated using unseen data by examining the categorical
distribution of test samples located above and below the warped surface.
Large-scale validation experiments are performed to assess the overall efficacy
of ML assisted surface warping as a fully integrated component within an ore
grade estimation system where the posterior mean is obtained via Gaussian
Process inference with a Matern 3/2 kernel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Leung_R/0/1/0/all/0/1"&gt;Raymond Leung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Balamurali_M/0/1/0/all/0/1"&gt;Mehala Balamurali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lowe_A/0/1/0/all/0/1"&gt;Alexander Lowe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Archimedean Copulas. (arXiv:2102.11351v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11351</id>
        <link href="http://arxiv.org/abs/2102.11351"/>
        <updated>2021-06-14T01:38:55.782Z</updated>
        <summary type="html"><![CDATA[We propose a new generative modeling technique for learning multidimensional
cumulative distribution functions (CDFs) in the form of copulas. Specifically,
we consider certain classes of copulas known as Archimedean and hierarchical
Archimedean copulas, popular for their parsimonious representation and ability
to model different tail dependencies. We consider their representation as
mixture models with Laplace transforms of latent random variables from
generative neural networks. This alternative representation allows for
computational efficiencies and easy sampling, especially in high dimensions. We
describe multiple methods for optimizing the network parameters. Finally, we
present empirical results that demonstrate the efficacy of our proposed method
in learning multidimensional CDFs and its computational efficiency compared to
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1"&gt;Yuting Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1"&gt;Ali Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elkhalil_K/0/1/0/all/0/1"&gt;Khalil Elkhalil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1"&gt;Vahid Tarokh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Infinite-dimensional Folded-in-time Deep Neural Networks. (arXiv:2101.02966v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02966</id>
        <link href="http://arxiv.org/abs/2101.02966"/>
        <updated>2021-06-14T01:38:55.776Z</updated>
        <summary type="html"><![CDATA[The method recently introduced in arXiv:2011.10115 realizes a deep neural
network with just a single nonlinear element and delayed feedback. It is
applicable for the description of physically implemented neural networks. In
this work, we present an infinite-dimensional generalization, which allows for
a more rigorous mathematical analysis and a higher flexibility in choosing the
weight functions. Precisely speaking, the weights are described by Lebesgue
integrable functions instead of step functions. We also provide a functional
back-propagation algorithm, which enables gradient descent training of the
weights. In addition, with a slight modification, our concept realizes
recurrent neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stelzer_F/0/1/0/all/0/1"&gt;Florian Stelzer&lt;/a&gt; (1, 2 and 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Yanchuk_S/0/1/0/all/0/1"&gt;Serhiy Yanchuk&lt;/a&gt; (1) ((1) Institute of Mathematics, Technische Universit&amp;#xe4;t Berlin, Germany, (2) Department of Mathematics, Humboldt-Universit&amp;#xe4;t zu Berlin, Germany, (3) Institute of Computer Science, University of Tartu, Estonia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Of Moments and Matching: A Game-Theoretic Framework for Closing the Imitation Gap. (arXiv:2103.03236v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03236</id>
        <link href="http://arxiv.org/abs/2103.03236"/>
        <updated>2021-06-14T01:38:55.757Z</updated>
        <summary type="html"><![CDATA[We provide a unifying view of a large family of previous imitation learning
algorithms through the lens of moment matching. At its core, our classification
scheme is based on whether the learner attempts to match (1) reward or (2)
action-value moments of the expert's behavior, with each option leading to
differing algorithmic approaches. By considering adversarially chosen
divergences between learner and expert behavior, we are able to derive bounds
on policy performance that apply for all algorithms in each of these classes,
the first to our knowledge. We also introduce the notion of moment
recoverability, implicit in many previous analyses of imitation learning, which
allows us to cleanly delineate how well each algorithmic family is able to
mitigate compounding errors. We derive three novel algorithm templates (AdVIL,
AdRIL, and DAeQuIL) with strong guarantees, simple implementation, and
competitive empirical performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Swamy_G/0/1/0/all/0/1"&gt;Gokul Swamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1"&gt;Sanjiban Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagnell_J/0/1/0/all/0/1"&gt;J. Andrew Bagnell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiwei Steven Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signal Processing on Higher-Order Networks: Livin' on the Edge ... and Beyond. (arXiv:2101.05510v3 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05510</id>
        <link href="http://arxiv.org/abs/2101.05510"/>
        <updated>2021-06-14T01:38:55.749Z</updated>
        <summary type="html"><![CDATA[In this tutorial, we provide a didactic treatment of the emerging topic of
signal processing on higher-order networks. Drawing analogies from discrete and
graph signal processing, we introduce the building blocks for processing data
on simplicial complexes and hypergraphs, two common higher-order network
abstractions that can incorporate polyadic relationships. We provide brief
introductions to simplicial complexes and hypergraphs, with a special emphasis
on the concepts needed for the processing of signals supported on these
structures. Specifically, we discuss Fourier analysis, signal denoising, signal
interpolation, node embeddings, and nonlinear processing through neural
networks, using these two higher-order network models. In the context of
simplicial complexes, we specifically focus on signal processing using the
Hodge Laplacian matrix, a multi-relational operator that leverages the special
structure of simplicial complexes and generalizes desirable properties of the
Laplacian matrix in graph signal processing. For hypergraphs, we present both
matrix and tensor representations, and discuss the trade-offs in adopting one
or the other. We also highlight limitations and potential research avenues,
both to inform practitioners and to motivate the contribution of new
researchers to the area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schaub_M/0/1/0/all/0/1"&gt;Michael T. Schaub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seby_J/0/1/0/all/0/1"&gt;Jean-Baptiste Seby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roddenberry_T/0/1/0/all/0/1"&gt;T. Mitchell Roddenberry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1"&gt;Santiago Segarra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. (arXiv:2102.05918v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05918</id>
        <link href="http://arxiv.org/abs/2102.05918"/>
        <updated>2021-06-14T01:38:55.742Z</updated>
        <summary type="html"><![CDATA[Pre-trained representations are becoming crucial for many NLP and perception
tasks. While representation learning in NLP has transitioned to training on raw
text without human annotations, visual and vision-language representations
still rely heavily on curated training datasets that are expensive or require
expert knowledge. For vision applications, representations are mostly learned
using datasets with explicit class labels such as ImageNet or OpenImages. For
vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all
involve a non-trivial data collection (and cleaning) process. This costly
curation process limits the size of datasets and hence hinders the scaling of
trained models. In this paper, we leverage a noisy dataset of over one billion
image alt-text pairs, obtained without expensive filtering or post-processing
steps in the Conceptual Captions dataset. A simple dual-encoder architecture
learns to align visual and language representations of the image and text pairs
using a contrastive loss. We show that the scale of our corpus can make up for
its noise and leads to state-of-the-art representations even with such a simple
learning scheme. Our visual representation achieves strong performance when
transferred to classification tasks such as ImageNet and VTAB. The aligned
visual and language representations enables zero-shot image classification and
also set new state-of-the-art results on Flickr30K and MSCOCO image-text
retrieval benchmarks, even when compared with more sophisticated
cross-attention models. The representations also enable cross-modality search
with complex text and text + image queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1"&gt;Chao Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Ye Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1"&gt;Zarana Parekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1"&gt;Yunhsuan Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duerig_T/0/1/0/all/0/1"&gt;Tom Duerig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Theory of Label Propagation for Subpopulation Shift. (arXiv:2102.11203v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11203</id>
        <link href="http://arxiv.org/abs/2102.11203"/>
        <updated>2021-06-14T01:38:55.735Z</updated>
        <summary type="html"><![CDATA[One of the central problems in machine learning is domain adaptation. Unlike
past theoretical work, we consider a new model for subpopulation shift in the
input or representation space. In this work, we propose a provably effective
framework for domain adaptation based on label propagation. In our analysis, we
use a simple but realistic expansion assumption, proposed in
\citet{wei2021theoretical}. Using a teacher classifier trained on the source
domain, our algorithm not only propagates to the target domain but also
improves upon the teacher. By leveraging existing generalization bounds, we
also obtain end-to-end finite-sample guarantees on the entire algorithm. In
addition, we extend our theoretical framework to a more general setting of
source-to-target transfer based on a third unlabeled dataset, which can be
easily applied in various learning scenarios. Inspired by our theory, we adapt
consistency-based semi-supervised learning methods to domain adaptation
settings and gain significant improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruiqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1"&gt;Qi Lei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box Optimization. (arXiv:2102.10707v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10707</id>
        <link href="http://arxiv.org/abs/2102.10707"/>
        <updated>2021-06-14T01:38:55.721Z</updated>
        <summary type="html"><![CDATA[We consider the zeroth-order optimization problem in the huge-scale setting,
where the dimension of the problem is so large that performing even basic
vector operations on the decision variables is infeasible. In this paper, we
propose a novel algorithm, coined ZO-BCD, that exhibits favorable overall query
complexity and has a much smaller per-iteration computational complexity. In
addition, we discuss how the memory footprint of ZO-BCD can be reduced even
further by the clever use of circulant measurement matrices. As an application
of our new method, we propose the idea of crafting adversarial attacks on
neural network based classifiers in a wavelet domain, which can result in
problem dimensions of over 1.7 million. In particular, we show that crafting
adversarial examples to audio classifiers in a wavelet domain can achieve the
state-of-the-art attack success rate of 97.9%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cai_H/0/1/0/all/0/1"&gt;HanQin Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lou_Y/0/1/0/all/0/1"&gt;Yuchen Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+McKenzie_D/0/1/0/all/0/1"&gt;Daniel McKenzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wotao Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Abstract Representations through Lossy Compression of Multi-Modal Signals. (arXiv:2101.11376v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11376</id>
        <link href="http://arxiv.org/abs/2101.11376"/>
        <updated>2021-06-14T01:38:55.714Z</updated>
        <summary type="html"><![CDATA[A key competence for open-ended learning is the formation of increasingly
abstract representations useful for driving complex behavior. Abstract
representations ignore specific details and facilitate generalization. Here we
consider the learning of abstract representations in a multi-modal setting with
two or more input modalities. We treat the problem as a lossy compression
problem and show that generic lossy compression of multimodal sensory input
naturally extracts abstract representations that tend to strip away modalitiy
specific details and preferentially retain information that is shared across
the different modalities. Furthermore, we propose an architecture to learn
abstract representations by identifying and retaining only the information that
is shared across multiple modalities while discarding any modality specific
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilmot_C/0/1/0/all/0/1"&gt;Charles Wilmot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Triesch_J/0/1/0/all/0/1"&gt;Jochen Triesch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Two-Way Matrix Reordering for Relational Data Analysis. (arXiv:2103.14203v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14203</id>
        <link href="http://arxiv.org/abs/2103.14203"/>
        <updated>2021-06-14T01:38:55.708Z</updated>
        <summary type="html"><![CDATA[Matrix reordering is a task to permute the rows and columns of a given
observed matrix such that the resulting reordered matrix shows meaningful or
interpretable structural patterns. Most existing matrix reordering techniques
share the common processes of extracting some feature representations from an
observed matrix in a predefined manner, and applying matrix reordering based on
it. However, in some practical cases, we do not always have prior knowledge
about the structural pattern of an observed matrix. To address this problem, we
propose a new matrix reordering method, called deep two-way matrix reordering
(DeepTMR), using a neural network model. The trained network can automatically
extract nonlinear row/column features from an observed matrix, which can then
be used for matrix reordering. Moreover, the proposed DeepTMR provides the
denoised mean matrix of a given observed matrix as an output of the trained
network. This denoised mean matrix can be used to visualize the global
structure of the reordered observed matrix. We demonstrate the effectiveness of
the proposed DeepTMR by applying it to both synthetic and practical datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1"&gt;Chihiro Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Certifying L-infinity Robustness using Neural Networks with L-inf-dist Neurons. (arXiv:2102.05363v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05363</id>
        <link href="http://arxiv.org/abs/2102.05363"/>
        <updated>2021-06-14T01:38:55.702Z</updated>
        <summary type="html"><![CDATA[It is well-known that standard neural networks, even with a high
classification accuracy, are vulnerable to small $\ell_\infty$-norm bounded
adversarial perturbations. Although many attempts have been made, most previous
works either can only provide empirical verification of the defense to a
particular attack method, or can only develop a certified guarantee of the
model robustness in limited scenarios. In this paper, we seek for a new
approach to develop a theoretically principled neural network that inherently
resists $\ell_\infty$ perturbations. In particular, we design a novel neuron
that uses $\ell_\infty$-distance as its basic operation (which we call
$\ell_\infty$-dist neuron), and show that any neural network constructed with
$\ell_\infty$-dist neurons (called $\ell_{\infty}$-dist net) is naturally a
1-Lipschitz function with respect to $\ell_\infty$-norm. This directly provides
a rigorous guarantee of the certified robustness based on the margin of
prediction outputs. We then prove that such networks have enough expressive
power to approximate any 1-Lipschitz function with robust generalization
guarantee. We further provide a holistic training strategy that can greatly
alleviate optimization difficulties. Experimental results show that using
$\ell_{\infty}$-dist nets as basic building blocks, we consistently achieve
state-of-the-art performance on commonly used datasets: 93.09% certified
accuracy on MNIST ($\epsilon=0.3$), 35.42% on CIFAR-10 ($\epsilon=8/255$) and
16.31% on TinyImageNet ($\epsilon=1/255$).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bohang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhou Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Profiling for Adversarial Training: On the Ruin of Problematic Data. (arXiv:2102.07437v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07437</id>
        <link href="http://arxiv.org/abs/2102.07437"/>
        <updated>2021-06-14T01:38:55.695Z</updated>
        <summary type="html"><![CDATA[There are multiple intriguing problems hovering in adversarial training,
including robustness-accuracy trade-off, robust overfitting, and robustness
overestimation. These problems pose great challenges to both reliable
evaluation and practical deployment. Here, we show that these problems share
one common cause -- low quality samples in the dataset. We first identify an
intrinsic property of the data called \emph{problematic score} and then design
controlled experiments to investigate its connections with these problems.
Specifically, we find that when problematic data is removed, robust overfitting
and robustness overestimation can be largely alleviated; and
robustness-accuracy trade-off becomes less significant. These observations not
only verify our intuition about data quality but also open new opportunities to
advance adversarial training. Interestingly, simply removing problematic data
from adversarial training, while making the training set smaller, yields better
robustness for leading adversarial training strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chengyu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1"&gt;Jingbo Shang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SparseBERT: Rethinking the Importance Analysis in Self-attention. (arXiv:2102.12871v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12871</id>
        <link href="http://arxiv.org/abs/2102.12871"/>
        <updated>2021-06-14T01:38:55.687Z</updated>
        <summary type="html"><![CDATA[Transformer-based models are popularly used in natural language processing
(NLP). Its core component, self-attention, has aroused widespread interest. To
understand the self-attention mechanism, a direct method is to visualize the
attention map of a pre-trained model. Based on the patterns observed, a series
of efficient Transformers with different sparse attention masks have been
proposed. From a theoretical perspective, universal approximability of
Transformer-based models is also recently proved. However, the above
understanding and analysis of self-attention is based on a pre-trained model.
To rethink the importance analysis in self-attention, we study the significance
of different positions in attention matrix during pre-training. A surprising
result is that diagonal elements in the attention map are the least important
compared with other attention positions. We provide a proof showing that these
diagonal elements can indeed be removed without deteriorating model
performance. Furthermore, we propose a Differentiable Attention Mask (DAM)
algorithm, which further guides the design of the SparseBERT. Extensive
experiments verify our interesting findings and illustrate the effect of the
proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Han Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jiahui Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1"&gt;James T. Kwok&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demystifying Assumptions in Learning to Discover Novel Classes. (arXiv:2102.04002v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04002</id>
        <link href="http://arxiv.org/abs/2102.04002"/>
        <updated>2021-06-14T01:38:55.681Z</updated>
        <summary type="html"><![CDATA[In learning to discover novel classes (L2DNC), we are given labeled data from
seen classes and unlabeled data from unseen classes, and we train clustering
models for the unseen classes. However, the rigorous definition of L2DNC is
unexplored, which results in that its implicit assumptions are still unclear.
In this paper, we demystify assumptions behind L2DNC and find that high-level
semantic features should be shared among the seen and unseen classes. This
naturally motivates us to link L2DNC to meta-learning that has exactly the same
assumption as L2DNC. Based on this finding, L2DNC is not only theoretically
solvable, but can also be empirically solved by meta-learning algorithms after
slight modifications. This L2DNC methodology significantly reduces the amount
of unlabeled data needed for training and makes it more practical, as
demonstrated in experiments. The use of very limited data is also justified by
the application scenario of L2DNC: since it is unnatural to label only
seen-class data, L2DNC is sampling instead of labeling in causality. Therefore,
unseen-class data should be collected on the way of collecting seen-class data,
which is why they are novel and first need to be clustered.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1"&gt;Haoang Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenjing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1"&gt;Long Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MagNet: A Neural Network for Directed Graphs. (arXiv:2102.11391v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11391</id>
        <link href="http://arxiv.org/abs/2102.11391"/>
        <updated>2021-06-14T01:38:55.674Z</updated>
        <summary type="html"><![CDATA[The prevalence of graph-based data has spurred the rapid development of graph
neural networks (GNNs) and related machine learning algorithms. Yet, despite
the many datasets naturally modeled as directed graphs, including citation,
website, and traffic networks, the vast majority of this research focuses on
undirected graphs. In this paper, we propose MagNet, a spectral GNN for
directed graphs based on a complex Hermitian matrix known as the magnetic
Laplacian. This matrix encodes undirected geometric structure in the magnitude
of its entries and directional information in their phase. A "charge" parameter
attunes spectral information to variation among directed cycles. We apply our
network to a variety of directed graph node classification and link prediction
tasks showing that MagNet performs well on all tasks and that its performance
exceeds all other methods on a majority of such tasks. The underlying
principles of MagNet are such that it can be adapted to other spectral GNN
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xitong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yixuan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brugnone_N/0/1/0/all/0/1"&gt;Nathan Brugnone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perlmutter_M/0/1/0/all/0/1"&gt;Michael Perlmutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirn_M/0/1/0/all/0/1"&gt;Matthew Hirn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variance Reduced Training with Stratified Sampling for Forecasting Models. (arXiv:2103.02062v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02062</id>
        <link href="http://arxiv.org/abs/2103.02062"/>
        <updated>2021-06-14T01:38:55.640Z</updated>
        <summary type="html"><![CDATA[In large-scale time series forecasting, one often encounters the situation
where the temporal patterns of time series, while drifting over time, differ
from one another in the same dataset. In this paper, we provably show under
such heterogeneity, training a forecasting model with commonly used stochastic
optimizers (e.g. SGD) potentially suffers large variance on gradient
estimation, and thus incurs long-time training. We show that this issue can be
efficiently alleviated via stratification, which allows the optimizer to sample
from pre-grouped time series strata. For better trading-off gradient variance
and computation complexity, we further propose SCott (Stochastic Stratified
Control Variate Gradient Descent), a variance reduced SGD-style optimizer that
utilizes stratified sampling via control variate. In theory, we provide the
convergence guarantee of SCott on smooth non-convex objectives. Empirically, we
evaluate SCott and other baseline optimizers on both synthetic and real-world
time series forecasting problems, and demonstrate SCott converges faster with
respect to both iterations and wall clock time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yucheng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1"&gt;Youngsuk Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lifan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1"&gt;Dean Foster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Asymptotics for Sequential Experiments. (arXiv:2101.09855v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09855</id>
        <link href="http://arxiv.org/abs/2101.09855"/>
        <updated>2021-06-14T01:38:55.630Z</updated>
        <summary type="html"><![CDATA[We propose a new diffusion-asymptotic analysis for sequentially randomized
experiments, including those that arise in solving multi-armed bandit problems.
In an experiment with $ n $ time steps, we let the mean reward gaps between
actions scale to the order $1/\sqrt{n}$ so as to preserve the difficulty of the
learning task as $n$ grows. In this regime, we show that the behavior of a
class of sequentially randomized Markov experiments converges to a diffusion
limit, given as the solution of a stochastic differential equation. The
diffusion limit thus enables us to derive refined, instance-specific
characterization of the stochastic dynamics of adaptive experiments. As an
application of this framework, we use the diffusion limit to obtain several new
insights on the regret and belief evolution of Thompson sampling. We show that
a version of Thompson sampling with an asymptotically uninformative prior
variance achieves nearly-optimal instance-specific regret scaling when the
reward gaps are relatively large. We also demonstrate that, in this regime, the
posterior beliefs underlying Thompson sampling are highly unstable over time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Wager_S/0/1/0/all/0/1"&gt;Stefan Wager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kuang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting to Misspecification in Contextual Bandits with Offline Regression Oracles. (arXiv:2102.13240v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13240</id>
        <link href="http://arxiv.org/abs/2102.13240"/>
        <updated>2021-06-14T01:38:55.619Z</updated>
        <summary type="html"><![CDATA[Computationally efficient contextual bandits are often based on estimating a
predictive model of rewards given contexts and arms using past data. However,
when the reward model is not well-specified, the bandit algorithm may incur
unexpected regret, so recent work has focused on algorithms that are robust to
misspecification. We propose a simple family of contextual bandit algorithms
that adapt to misspecification error by reverting to a good safe policy when
there is evidence that misspecification is causing a regret increase. Our
algorithm requires only an offline regression oracle to ensure regret
guarantees that gracefully degrade in terms of a measure of the average
misspecification level. Compared to prior work, we attain similar regret
guarantees, but we do no rely on a master algorithm, and do not require more
robust oracles like online or constrained regression oracles (e.g., Foster et
al. (2020a); Krishnamurthy et al. (2020)). This allows us to design algorithms
for more general function approximation classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1"&gt;Sanath Kumar Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadad_V/0/1/0/all/0/1"&gt;Vitor Hadad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1"&gt;Susan Athey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design. (arXiv:2103.02438v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02438</id>
        <link href="http://arxiv.org/abs/2103.02438"/>
        <updated>2021-06-14T01:38:55.609Z</updated>
        <summary type="html"><![CDATA[We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of
adaptive Bayesian experimental design that allows experiments to be run in
real-time. Traditional sequential Bayesian optimal experimental design
approaches require substantial computation at each stage of the experiment.
This makes them unsuitable for most real-world applications, where decisions
must typically be made quickly. DAD addresses this restriction by learning an
amortized design network upfront and then using this to rapidly run (multiple)
adaptive experiments at deployment time. This network represents a design
policy which takes as input the data from previous steps, and outputs the next
design using a single forward pass; these design decisions can be made in
milliseconds during the live experiment. To train the network, we introduce
contrastive information bounds that are suitable objectives for the sequential
setting, and propose a customized network architecture that exploits key
symmetries. We demonstrate that DAD successfully amortizes the process of
experimental design, outperforming alternative strategies on a number of
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1"&gt;Adam Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ivanova_D/0/1/0/all/0/1"&gt;Desi R. Ivanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Malik_I/0/1/0/all/0/1"&gt;Ilyas Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Competitions and Online Learning with Strategic Forecasters. (arXiv:2102.08358v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08358</id>
        <link href="http://arxiv.org/abs/2102.08358"/>
        <updated>2021-06-14T01:38:55.592Z</updated>
        <summary type="html"><![CDATA[Winner-take-all competitions in forecasting and machine-learning suffer from
distorted incentives. Witkowski et al. 2018 identified this problem and
proposed ELF, a truthful mechanism to select a winner. We show that, from a
pool of $n$ forecasters, ELF requires $\Theta(n\log n)$ events or test data
points to select a near-optimal forecaster with high probability. We then show
that standard online learning algorithms select an $\epsilon$-optimal
forecaster using only $O(\log(n) / \epsilon^2)$ events, by way of a strong
approximate-truthfulness guarantee. This bound matches the best possible even
in the nonstrategic setting. We then apply these mechanisms to obtain the first
no-regret guarantee for non-myopic strategic experts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frongillo_R/0/1/0/all/0/1"&gt;Rafael Frongillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_R/0/1/0/all/0/1"&gt;Robert Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thilagar_A/0/1/0/all/0/1"&gt;Anish Thilagar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waggoner_B/0/1/0/all/0/1"&gt;Bo Waggoner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lower-Bounded Proper Losses for Weakly Supervised Classification. (arXiv:2103.02893v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02893</id>
        <link href="http://arxiv.org/abs/2103.02893"/>
        <updated>2021-06-14T01:38:55.562Z</updated>
        <summary type="html"><![CDATA[This paper discusses the problem of weakly supervised classification, in
which instances are given weak labels that are produced by some
label-corruption process. The goal is to derive conditions under which loss
functions for weak-label learning are proper and lower-bounded -- two essential
requirements for the losses used in class-probability estimation. To this end,
we derive a representation theorem for proper losses in supervised learning,
which dualizes the Savage representation. We use this theorem to characterize
proper weak-label losses and find a condition for them to be lower-bounded.
From these theoretical findings, we derive a novel regularization scheme called
generalized logit squeezing, which makes any proper weak-label loss bounded
from below, without losing properness. Furthermore, we experimentally
demonstrate the effectiveness of our proposed approach, as compared to improper
or unbounded losses. The results highlight the importance of properness and
lower-boundedness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yoshida_S/0/1/0/all/0/1"&gt;Shuhei M. Yoshida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Takenouchi_T/0/1/0/all/0/1"&gt;Takashi Takenouchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards an efficient approach for the nonconvex $\ell_p$ ball projection: algorithm and analysis. (arXiv:2101.01350v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01350</id>
        <link href="http://arxiv.org/abs/2101.01350"/>
        <updated>2021-06-14T01:38:55.555Z</updated>
        <summary type="html"><![CDATA[This paper primarily focuses on computing the Euclidean projection of a
vector onto the $\ell_{p}$ ball in which $p\in(0,1)$. Such a problem emerges as
the core building block in statistical machine learning and signal processing
tasks because of its ability to promote sparsity. However, efficient numerical
algorithms for finding the projections are still not available, particularly in
large-scale optimization. To meet this challenge, we first derive the
first-order necessary optimality conditions of this problem using Fr\'echet
normal cone. Based on this characterization, we develop a novel numerical
approach for computing the stationary point through solving a sequence of
projections onto the reweighted $\ell_{1}$-balls. This method is practically
simple to implement and computationally efficient. Moreover, the proposed
algorithm is shown to converge uniquely under mild conditions and has a
worst-case $O(1/\sqrt{k})$ convergence rate. Numerical experiments demonstrate
the efficiency of our proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiangyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiashan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Anytime Prediction with Parallel Cascaded Networks and a Temporal-Difference Loss. (arXiv:2102.09808v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09808</id>
        <link href="http://arxiv.org/abs/2102.09808"/>
        <updated>2021-06-14T01:38:55.548Z</updated>
        <summary type="html"><![CDATA[Although deep feedforward neural networks share some characteristics with the
primate visual system, a key distinction is their dynamics. Deep nets typically
operate in serial stages wherein each layer completes its computation before
processing begins in subsequent layers. In contrast, biological systems have
cascaded dynamics: information propagates from neurons at all layers in
parallel but transmission occurs gradually over time, leading to speed-accuracy
trade offs even in feedforward architectures. We explore the consequences of
biologically inspired parallel hardware by constructing cascaded ResNets in
which each residual block has propagation delays but all blocks update in
parallel in a stateful manner. Because information transmitted through skip
connections avoids delays, the functional depth of the architecture increases
over time, yielding anytime predictions that improve with internal-processing
time. We introduce a temporal-difference training loss that achieves a strictly
superior speed-accuracy profile over standard losses and enables the cascaded
architecture to outperform state-of-the-art anytime-prediction methods. The
cascaded architecture has intriguing properties, including: it classifies
typical instances more rapidly than atypical instances; it is more robust to
both persistent and transient noise than is a conventional ResNet; and its
time-varying output trace provides a signal that can be exploited to improve
information processing and inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iuzzolino_M/0/1/0/all/0/1"&gt;Michael L. Iuzzolino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1"&gt;Michael C. Mozer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSPM: A Modularized and Scalable Multi-Agent Reinforcement Learning-based System for Financial Portfolio Management. (arXiv:2102.03502v3 [q-fin.PM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03502</id>
        <link href="http://arxiv.org/abs/2102.03502"/>
        <updated>2021-06-14T01:38:55.539Z</updated>
        <summary type="html"><![CDATA[Financial portfolio management is one of the most applicable problems in
reinforcement learning (RL) owing to its sequential decision-making nature.
Existing RL-based approaches, while inspiring, often lack scalability,
reusability, or profundity of intake information to accommodate the
ever-changing capital markets. In this paper, we propose MSPM, a modularized
and scalable, multi-agent RL-based system for financial portfolio management.
MSPM involves two asynchronously updated units: an Evolving Agent Module (EAM)
and Strategic Agent Module (SAM). A self-sustained EAM produces
signal-comprised information for a specific asset using heterogeneous data
inputs, and each EAM employs its reusability to have connections to multiple
SAMs. An SAM is responsible for asset reallocation in a portfolio using
profound information from the connected EAMs. With the elaborate architecture
and the multi-step condensation of volatile market information, MSPM aims to
provide a customizable, stable, and dedicated solution to portfolio management,
unlike existing approaches. We also tackle the data-shortage issue of
newly-listed stocks by transfer learning, and validate the indispensability of
EAM with four different portfolios. Experiments on 8-year U.S. stock market
data prove the effectiveness of MSPM in profit accumulation, by its
outperformance over existing benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhenhan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Tanaka_F/0/1/0/all/0/1"&gt;Fumihide Tanaka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UnNatural Language Inference. (arXiv:2101.00010v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00010</id>
        <link href="http://arxiv.org/abs/2101.00010"/>
        <updated>2021-06-14T01:38:55.532Z</updated>
        <summary type="html"><![CDATA[Recent investigations into the inner-workings of state-of-the-art large-scale
pre-trained Transformer-based Natural Language Understanding (NLU) models
indicate that they appear to know humanlike syntax, at least to some extent. We
provide novel evidence that complicates this claim: we find that
state-of-the-art Natural Language Inference (NLI) models assign the same labels
to permuted examples as they do to the original, i.e. they are largely
invariant to random word-order permutations. This behavior notably differs from
that of humans; we struggle with ungrammatical sentences. To measure the
severity of this issue, we propose a suite of metrics and investigate which
properties of particular permutations lead models to be word-order invariant.
In the MNLI dataset, for example, we find almost all (98.7%) examples contain
at least one permutation which elicits the gold label. Models are sometimes
even able to assign gold labels to permutations that they originally failed to
predict correctly. We provide a comprehensive empirical evaluation of this
phenomenon, and further show that this issue exists for both Transformers and
pre-Transformer RNN / ConvNet based encoders, as well as across multiple
languages (English and Mandarin Chinese). Our code and data are available at
https://github.com/facebookresearch/unlu.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1"&gt;Koustuv Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1"&gt;Prasanna Parthasarathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1"&gt;Adina Williams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Public Data for Practical Private Query Release. (arXiv:2102.08598v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08598</id>
        <link href="http://arxiv.org/abs/2102.08598"/>
        <updated>2021-06-14T01:38:55.507Z</updated>
        <summary type="html"><![CDATA[In many statistical problems, incorporating priors can significantly improve
performance. However, the use of prior knowledge in differentially private
query release has remained underexplored, despite such priors commonly being
available in the form of public datasets, such as previous US Census releases.
With the goal of releasing statistics about a private dataset, we present
PMW^Pub, which -- unlike existing baselines -- leverages public data drawn from
a related distribution as prior information. We provide a theoretical analysis
and an empirical evaluation on the American Community Survey (ACS) and ADULT
datasets, which shows that our method outperforms state-of-the-art methods.
Furthermore, PMW^Pub scales well to high-dimensional data domains, where
running many existing methods would be computationally infeasible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Terrance Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vietri_G/0/1/0/all/0/1"&gt;Giuseppe Vietri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1"&gt;Thomas Steinke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullman_J/0/1/0/all/0/1"&gt;Jonathan Ullman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiwei Steven Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Task Reinforcement Learning with Context-based Representations. (arXiv:2102.06177v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06177</id>
        <link href="http://arxiv.org/abs/2102.06177"/>
        <updated>2021-06-14T01:38:55.501Z</updated>
        <summary type="html"><![CDATA[The benefit of multi-task learning over single-task learning relies on the
ability to use relations across tasks to improve performance on any single
task. While sharing representations is an important mechanism to share
information across tasks, its success depends on how well the structure
underlying the tasks is captured. In some real-world situations, we have access
to metadata, or additional information about a task, that may not provide any
new insight in the context of a single task setup alone but inform relations
across multiple tasks. While this metadata can be useful for improving
multi-task learning performance, effectively incorporating it can be an
additional challenge. We posit that an efficient approach to knowledge transfer
is through the use of multiple context-dependent, composable representations
shared across a family of tasks. In this framework, metadata can help to learn
interpretable representations and provide the context to inform which
representations to compose and how to compose them. We use the proposed
approach to obtain state-of-the-art results in Meta-World, a challenging
multi-task benchmark consisting of 50 distinct robotic manipulation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sodhani_S/0/1/0/all/0/1"&gt;Shagun Sodhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Amy Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite-Sample Analysis of Off-Policy Natural Actor-Critic Algorithm. (arXiv:2102.09318v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09318</id>
        <link href="http://arxiv.org/abs/2102.09318"/>
        <updated>2021-06-14T01:38:55.494Z</updated>
        <summary type="html"><![CDATA[In this paper, we provide finite-sample convergence guarantees for an
off-policy variant of the natural actor-critic (NAC) algorithm based on
Importance Sampling. In particular, we show that the algorithm converges to a
global optimal policy with a sample complexity of
$\mathcal{O}(\epsilon^{-3}\log^2(1/\epsilon))$ under an appropriate choice of
stepsizes. In order to overcome the issue of large variance due to Importance
Sampling, we propose the $Q$-trace algorithm for the critic, which is inspired
by the V-trace algorithm \cite{espeholt2018impala}. This enables us to
explicitly control the bias and variance, and characterize the trade-off
between them. As an advantage of off-policy sampling, a major feature of our
result is that we do not need any additional assumptions, beyond the ergodicity
of the Markov chain induced by the behavior policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khodadadian_S/0/1/0/all/0/1"&gt;Sajad Khodadadian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zaiwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1"&gt;Siva Theja Maguluri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts Generalization. (arXiv:2012.14193v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14193</id>
        <link href="http://arxiv.org/abs/2012.14193"/>
        <updated>2021-06-14T01:38:55.434Z</updated>
        <summary type="html"><![CDATA[The early phase of training a deep neural network has a dramatic effect on
the local curvature of the loss function. For instance, using a small learning
rate does not guarantee stable optimization because the optimization trajectory
has a tendency to steer towards regions of the loss surface with increasing
local curvature. We ask whether this tendency is connected to the widely
observed phenomenon that the choice of the learning rate strongly influences
generalization. We first show that stochastic gradient descent (SGD) implicitly
penalizes the trace of the Fisher Information Matrix (FIM), a measure of the
local curvature, from the start of training. We argue it is an implicit
regularizer in SGD by showing that explicitly penalizing the trace of the FIM
can significantly improve generalization. We highlight that poor final
generalization coincides with the trace of the FIM attaining a large value
early in training, to which we refer as catastrophic Fisher explosion. Finally,
to gain insight into the regularization effect of penalizing the trace of the
FIM, we show that it limits memorization by reducing the learning speed of
examples with noisy labels more than that of the examples with clean labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jastrzebski_S/0/1/0/all/0/1"&gt;Stanislaw Jastrzebski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1"&gt;Devansh Arpit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astrand_O/0/1/0/all/0/1"&gt;Oliver Astrand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerg_G/0/1/0/all/0/1"&gt;Giancarlo Kerg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1"&gt;Richard Socher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1"&gt;Krzysztof Geras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Particle Dual Averaging: Optimization of Mean Field Neural Networks with Global Convergence Rate Analysis. (arXiv:2012.15477v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15477</id>
        <link href="http://arxiv.org/abs/2012.15477"/>
        <updated>2021-06-14T01:38:55.368Z</updated>
        <summary type="html"><![CDATA[We propose the particle dual averaging (PDA) method, which generalizes the
dual averaging method in convex optimization to the optimization over
probability distributions with quantitative runtime guarantee. The algorithm
consists of an inner loop and outer loop: the inner loop utilizes the Langevin
algorithm to approximately solve for a stationary distribution, which is then
optimized in the outer loop. The method can thus be interpreted as an extension
of the Langevin algorithm to naturally handle nonlinear functional on the
probability space. An important application of the proposed method is the
optimization of neural network in the mean field regime, which is theoretically
attractive due to the presence of nonlinear feature learning, but quantitative
convergence rate can be challenging to obtain. By adapting finite-dimensional
convex optimization theory into the space of distributions, we analyze PDA in
regularized empirical / expected risk minimization, and establish quantitative
global convergence in learning two-layer mean field neural networks under more
general settings. Our theoretical results are supported by numerical
simulations on neural networks with reasonable size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nitanda_A/0/1/0/all/0/1"&gt;Atsushi Nitanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_D/0/1/0/all/0/1"&gt;Denny Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Divergence Regulated Encoder Network for Joint Dimensionality Reduction and Classification. (arXiv:2012.15764v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15764</id>
        <link href="http://arxiv.org/abs/2012.15764"/>
        <updated>2021-06-14T01:38:55.308Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate performing joint dimensionality reduction and
classification using a novel histogram neural network. Motivated by a popular
dimensionality reduction approach, t-Distributed Stochastic Neighbor Embedding
(t-SNE), our proposed method incorporates a classification loss computed on
samples in a low-dimensional embedding space. We compare the learned sample
embeddings against coordinates found by t-SNE in terms of classification
accuracy and qualitative assessment. We also explore use of various divergence
measures in the t-SNE objective. The proposed method has several advantages
such as readily embedding out-of-sample points and reducing feature
dimensionality while retaining class discriminability. Our results show that
the proposed approach maintains and/or improves classification performance and
reveals characteristics of features produced by neural networks that may be
helpful for other applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peeples_J/0/1/0/all/0/1"&gt;Joshua Peeples&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walker_S/0/1/0/all/0/1"&gt;Sarah Walker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCurley_C/0/1/0/all/0/1"&gt;Connor McCurley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zare_A/0/1/0/all/0/1"&gt;Alina Zare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_J/0/1/0/all/0/1"&gt;James Keller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scene-Agnostic Multi-Microphone Speech Dereverberation. (arXiv:2010.11875v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11875</id>
        <link href="http://arxiv.org/abs/2010.11875"/>
        <updated>2021-06-14T01:38:55.301Z</updated>
        <summary type="html"><![CDATA[Neural networks (NNs) have been widely applied in speech processing tasks,
and, in particular, those employing microphone arrays. Nevertheless, most
existing NN architectures can only deal with fixed and position-specific
microphone arrays. In this paper, we present an NN architecture that can cope
with microphone arrays whose number and positions of the microphones are
unknown, and demonstrate its applicability in the speech dereverberation task.
To this end, our approach harnesses recent advances in deep learning on
set-structured data to design an architecture that enhances the reverberant
log-spectrum. We use noisy and noiseless versions of a simulated reverberant
dataset to test the proposed architecture. Our experiments on the noisy data
show that the proposed scene-agnostic setup outperforms a powerful scene-aware
framework, sometimes even with fewer microphones. With the noiseless dataset we
show that, in most cases, our method outperforms the position-aware network as
well as the state-of-the-art weighted linear prediction error (WPE) algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yemini_Y/0/1/0/all/0/1"&gt;Yochai Yemini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fetaya_E/0/1/0/all/0/1"&gt;Ethan Fetaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maron_H/0/1/0/all/0/1"&gt;Haggai Maron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gannot_S/0/1/0/all/0/1"&gt;Sharon Gannot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonparametric Learning of Two-Layer ReLU Residual Units. (arXiv:2008.07648v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07648</id>
        <link href="http://arxiv.org/abs/2008.07648"/>
        <updated>2021-06-14T01:38:55.295Z</updated>
        <summary type="html"><![CDATA[We describe an algorithm that learns two-layer residual units with rectified
linear unit (ReLU) activation: suppose the input $\mathbf{x}$ is from a
distribution with support space $\mathbb{R}^d$ and the ground-truth generative
model is such a residual unit, given by \[\mathbf{y}=
\boldsymbol{B}^\ast\left[\left(\boldsymbol{A}^\ast\mathbf{x}\right)^+ +
\mathbf{x}\right]\text{,}\] where ground-truth network parameters
$\boldsymbol{A}^\ast \in \mathbb{R}^{d\times d}$ is a nonnegative full-rank
matrix and $\boldsymbol{B}^\ast \in \mathbb{R}^{m\times d}$ is full-rank with
$m \geq d$ and for $\mathbf{c} \in \mathbb{R}^d$, $[\mathbf{c}^{+}]_i =
\max\{0, c_i\}$. We design layer-wise objectives as functionals whose analytic
minimizers express the exact ground-truth network in terms of its parameters
and nonlinearities. Following this objective landscape, learning residual units
from finite samples can be formulated using convex optimization of a
nonparametric function: for each layer, we first formulate the corresponding
empirical risk minimization (ERM) as a positive semi-definite quadratic program
(QP), then we show the solution space of the QP can be equivalently determined
by a set of linear inequalities, which can then be efficiently solved by linear
programming (LP). We further prove the statistical strong consistency of our
algorithm, and demonstrate the robustness and sample efficiency of our
algorithm by experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhunxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Linyun He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1"&gt;Chunchuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1"&gt;Shay B. Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple and Efficient Hard Label Black-box Adversarial Attacks in Low Query Budget Regimes. (arXiv:2007.07210v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.07210</id>
        <link href="http://arxiv.org/abs/2007.07210"/>
        <updated>2021-06-14T01:38:55.288Z</updated>
        <summary type="html"><![CDATA[We focus on the problem of black-box adversarial attacks, where the aim is to
generate adversarial examples for deep learning models solely based on
information limited to output label~(hard label) to a queried data input. We
propose a simple and efficient Bayesian Optimization~(BO) based approach for
developing black-box adversarial attacks. Issues with BO's performance in high
dimensions are avoided by searching for adversarial examples in a structured
low-dimensional subspace. We demonstrate the efficacy of our proposed attack
method by evaluating both $\ell_\infty$ and $\ell_2$ norm constrained
untargeted and targeted hard label black-box attacks on three standard datasets
- MNIST, CIFAR-10 and ImageNet. Our proposed approach consistently achieves 2x
to 10x higher attack success rate while requiring 10x to 20x fewer queries
compared to the current state-of-the-art black-box adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1"&gt;Satya Narayan Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahu_A/0/1/0/all/0/1"&gt;Anit Kumar Sahu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willmott_D/0/1/0/all/0/1"&gt;Devin Willmott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1"&gt;J. Zico Kolter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributional Soft Actor-Critic: Off-Policy Reinforcement Learning for Addressing Value Estimation Errors. (arXiv:2001.02811v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.02811</id>
        <link href="http://arxiv.org/abs/2001.02811"/>
        <updated>2021-06-14T01:38:55.271Z</updated>
        <summary type="html"><![CDATA[In reinforcement learning (RL), function approximation errors are known to
easily lead to the Q-value overestimations, thus greatly reducing policy
performance. This paper presents a distributional soft actor-critic (DSAC)
algorithm, which is an off-policy RL method for continuous control setting, to
improve the policy performance by mitigating Q-value overestimations. We first
discover in theory that learning a distribution function of state-action
returns can effectively mitigate Q-value overestimations because it is capable
of adaptively adjusting the update stepsize of the Q-value function. Then, a
distributional soft policy iteration (DSPI) framework is developed by embedding
the return distribution function into maximum entropy RL. Finally, we present a
deep off-policy actor-critic variant of DSPI, called DSAC, which directly
learns a continuous return distribution by keeping the variance of the
state-action returns within a reasonable range to address exploding and
vanishing gradient problems. We evaluate DSAC on the suite of MuJoCo continuous
control tasks, achieving the state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jingliang Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yang Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shengbo Eben Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yangang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1"&gt;Bo Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can we have it all? On the Trade-off between Spatial and Adversarial Robustness of Neural Networks. (arXiv:2002.11318v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.11318</id>
        <link href="http://arxiv.org/abs/2002.11318"/>
        <updated>2021-06-14T01:38:55.256Z</updated>
        <summary type="html"><![CDATA[(Non-)robustness of neural networks to small, adversarial pixel-wise
perturbations, and as more recently shown, to even random spatial
transformations (e.g., translations, rotations) entreats both theoretical and
empirical understanding. Spatial robustness to random translations and
rotations is commonly attained via equivariant models (e.g., StdCNNs, GCNNs)
and training augmentation, whereas adversarial robustness is typically achieved
by adversarial training. In this paper, we prove a quantitative trade-off
between spatial and adversarial robustness in a simple statistical setting. We
complement this empirically by showing that: (a) as the spatial robustness of
equivariant models improves by training augmentation with progressively larger
transformations, their adversarial robustness worsens progressively, and (b) as
the state-of-the-art robust models are adversarially trained with progressively
larger pixel-wise perturbations, their spatial robustness drops progressively.
Towards achieving pareto-optimality in this trade-off, we propose a method
based on curriculum learning that trains gradually on more difficult
perturbations (both spatial and adversarial) to improve spatial and adversarial
robustness simultaneously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamath_S/0/1/0/all/0/1"&gt;Sandesh Kamath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1"&gt;Amit Deshpande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subrahmanyam_K/0/1/0/all/0/1"&gt;K V Subrahmanyam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inference of Causal Effects when Control Variables are Unknown. (arXiv:2012.08154v3 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08154</id>
        <link href="http://arxiv.org/abs/2012.08154"/>
        <updated>2021-06-14T01:38:55.250Z</updated>
        <summary type="html"><![CDATA[Conventional methods in causal effect inferencetypically rely on specifying a
valid set of control variables. When this set is unknown or misspecified,
inferences will be erroneous. We propose a method for inferring average causal
effects when all potential confounders are observed, but thecontrol variables
are unknown. When the data-generating process belongs to the class of acyclical
linear structural causal models, we prove that themethod yields asymptotically
valid confidence intervals. Our results build upon a smooth characterization of
linear directed acyclic graphs. We verify the capability of the method to
produce valid confidence intervals for average causal effects using synthetic
data, even when the appropriate specification of control variables is unknown.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hult_L/0/1/0/all/0/1"&gt;Ludvig Hult&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zachariah_D/0/1/0/all/0/1"&gt;Dave Zachariah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hutch++: Optimal Stochastic Trace Estimation. (arXiv:2010.09649v5 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09649</id>
        <link href="http://arxiv.org/abs/2010.09649"/>
        <updated>2021-06-14T01:38:55.242Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating the trace of a matrix $A$ that can only be
accessed through matrix-vector multiplication. We introduce a new randomized
algorithm, Hutch++, which computes a $(1 \pm \epsilon)$ approximation to
$tr(A)$ for any positive semidefinite (PSD) $A$ using just $O(1/\epsilon)$
matrix-vector products. This improves on the ubiquitous Hutchinson's estimator,
which requires $O(1/\epsilon^2)$ matrix-vector products. Our approach is based
on a simple technique for reducing the variance of Hutchinson's estimator using
a low-rank approximation step, and is easy to implement and analyze. Moreover,
we prove that, up to a logarithmic factor, the complexity of Hutch++ is optimal
amongst all matrix-vector query algorithms, even when queries can be chosen
adaptively. We show that it significantly outperforms Hutchinson's method in
experiments. While our theory mainly requires $A$ to be positive semidefinite,
we provide generalized guarantees for general square matrices, and show
empirical gains in such applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_R/0/1/0/all/0/1"&gt;Raphael A. Meyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Cameron Musco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Christopher Musco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Network Approximation for Smooth Functions. (arXiv:2001.03040v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.03040</id>
        <link href="http://arxiv.org/abs/2001.03040"/>
        <updated>2021-06-14T01:38:55.233Z</updated>
        <summary type="html"><![CDATA[This paper establishes optimal approximation error characterization of deep
ReLU networks for smooth functions in terms of both width and depth
simultaneously. To that end, we first prove that multivariate polynomials can
be approximated by deep ReLU networks of width $\mathcal{O}(N)$ and depth
$\mathcal{O}(L)$ with an approximation error $\mathcal{O}(N^{-L})$. Through
local Taylor expansions and their deep ReLU network approximations, we show
that deep ReLU networks of width $\mathcal{O}(N\ln N)$ and depth
$\mathcal{O}(L\ln L)$ can approximate $f\in C^s([0,1]^d)$ with a nearly optimal
approximation rate $\mathcal{O}(\|f\|_{C^s([0,1]^d)}N^{-2s/d}L^{-2s/d})$. Our
estimate is non-asymptotic in the sense that it is valid for arbitrary width
and depth specified by $N\in\mathbb{N}^+$ and $L\in\mathbb{N}^+$, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jianfeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zuowei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Value Alignment Verification. (arXiv:2012.01557v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01557</id>
        <link href="http://arxiv.org/abs/2012.01557"/>
        <updated>2021-06-14T01:38:55.213Z</updated>
        <summary type="html"><![CDATA[As humans interact with autonomous agents to perform increasingly
complicated, potentially risky tasks, it is important to be able to efficiently
evaluate an agent's performance and correctness. In this paper we formalize and
theoretically analyze the problem of efficient value alignment verification:
how to efficiently test whether the behavior of another agent is aligned with a
human's values. The goal is to construct a kind of "driver's test" that a human
can give to any agent which will verify value alignment via a minimal number of
queries. We study alignment verification problems with both idealized humans
that have an explicit reward function as well as problems where they have
implicit values. We analyze verification of exact value alignment for rational
agents and propose and analyze heuristic and approximate value alignment
verification tests in a wide range of gridworlds and a continuous autonomous
driving domain. Finally, we prove that there exist sufficient conditions such
that we can verify exact and approximate alignment across an infinite set of
test environments via a constant-query-complexity alignment test.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1"&gt;Daniel S. Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1"&gt;Jordan Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1"&gt;Anca D. Dragan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1"&gt;Scott Niekum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Knowledge Distillation Ensemble Framework for Predicting Short and Long-term Hospitalisation Outcomes from Electronic Health Records Data. (arXiv:2011.09361v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09361</id>
        <link href="http://arxiv.org/abs/2011.09361"/>
        <updated>2021-06-14T01:38:55.206Z</updated>
        <summary type="html"><![CDATA[The ability to perform accurate prognosis of patients is crucial for
proactive clinical decision making, informed resource management and
personalised care. Existing outcome prediction models suffer from a low recall
of infrequent positive outcomes. We present a highly-scalable and robust
machine learning framework to automatically predict adversity represented by
mortality and ICU admission from time-series vital signs and laboratory results
obtained within the first 24 hours of hospital admission. The stacked platform
comprises two components: a) an unsupervised LSTM Autoencoder that learns an
optimal representation of the time-series, using it to differentiate the less
frequent patterns which conclude with an adverse event from the majority
patterns that do not, and b) a gradient boosting model, which relies on the
constructed representation to refine prediction, incorporating static features
of demographics, admission details and clinical summaries. The model is used to
assess a patient's risk of adversity over time and provides visual
justifications of its prediction based on the patient's static features and
dynamic signals. Results of three case studies for predicting mortality and ICU
admission show that the model outperforms all existing outcome prediction
models, achieving PR-AUC of 0.891 (95$%$ CI: 0.878 - 0.969) in predicting
mortality in ICU and general ward settings and 0.908 (95$%$ CI: 0.870-0.935) in
predicting ICU admission.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1"&gt;Zina M Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bean_D/0/1/0/all/0/1"&gt;Daniel Bean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Searle_T/0/1/0/all/0/1"&gt;Thomas Searle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Honghan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shek_A/0/1/0/all/0/1"&gt;Anthony Shek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kraljevic_Z/0/1/0/all/0/1"&gt;Zeljko Kraljevic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galloway_J/0/1/0/all/0/1"&gt;James Galloway&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norton_S/0/1/0/all/0/1"&gt;Sam Norton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teo_J/0/1/0/all/0/1"&gt;James T Teo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1"&gt;Richard JB Dobson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Possibility results for graph clustering: A novel consistency axiom. (arXiv:1806.06142v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1806.06142</id>
        <link href="http://arxiv.org/abs/1806.06142"/>
        <updated>2021-06-14T01:38:55.197Z</updated>
        <summary type="html"><![CDATA[Kleinberg introduced three natural clustering properties, or axioms, and
showed they cannot be simultaneously satisfied by any clustering algorithm. We
present a new clustering property, Monotonic Consistency, which avoids the
well-known problematic behaviour of Kleinberg's Consistency axiom, and the
impossibility result. Namely, we describe a clustering algorithm, Morse
Clustering, inspired by Morse Theory in Differential Topology, which satisfies
Kleinberg's original axioms with Consistency replaced by Monotonic Consistency.
Morse clustering uncovers the underlying flow structure on a set or graph and
returns a partition into trees representing basins of attraction of critical
vertices. We also generalise Kleinberg's axiomatic approach to sparse graphs,
showing an impossibility result for Consistency, and a possibility result for
Monotonic Consistency and Morse clustering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Strazzeri_F/0/1/0/all/0/1"&gt;Fabio Strazzeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Garcia_R/0/1/0/all/0/1"&gt;Rub&amp;#xe9;n J. S&amp;#xe1;nchez-Garc&amp;#xed;a&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Distribution-Dependent Analysis of Meta-Learning. (arXiv:2011.00344v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00344</id>
        <link href="http://arxiv.org/abs/2011.00344"/>
        <updated>2021-06-14T01:38:55.190Z</updated>
        <summary type="html"><![CDATA[A key problem in the theory of meta-learning is to understand how the task
distributions influence transfer risk, the expected error of a meta-learner on
a new task drawn from the unknown task distribution. In this paper, focusing on
fixed design linear regression with Gaussian noise and a Gaussian task (or
parameter) distribution, we give distribution-dependent lower bounds on the
transfer risk of any algorithm, while we also show that a novel, weighted
version of the so-called biased regularized regression method is able to match
these lower bounds up to a fixed constant factor. Notably, the weighting is
derived from the covariance of the Gaussian task distribution. Altogether, our
results provide a precise characterization of the difficulty of meta-learning
in this Gaussian setting. While this problem setting may appear simple, we show
that it is rich enough to unify the "parameter sharing" and "representation
learning" streams of meta-learning; in particular, representation learning is
obtained as the special case when the covariance matrix of the task
distribution is unknown. For this case we propose to adopt the EM method, which
is shown to enjoy efficient updates in our case. The paper is completed by an
empirical study of EM. In particular, our experimental results show that the EM
algorithm can attain the lower bound as the number of tasks grows, while the
algorithm is also successful in competing with its alternatives when used in a
representation learning context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Konobeev_M/0/1/0/all/0/1"&gt;Mikhail Konobeev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kuzborskij_I/0/1/0/all/0/1"&gt;Ilja Kuzborskij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesv&amp;#xe1;ri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Represent Your Own Policies: Reinforcement Learning with Policy-extended Value Function Approximator. (arXiv:2010.09536v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09536</id>
        <link href="http://arxiv.org/abs/2010.09536"/>
        <updated>2021-06-14T01:38:55.182Z</updated>
        <summary type="html"><![CDATA[We study Policy-extended Value Function Approximator (PeVFA) in Reinforcement
Learning (RL), which extends conventional value function approximator (VFA) to
take as input not only the state (and action) but also an explicit policy
representation. Such an extension enables PeVFA to preserve values of multiple
policies at the same time and brings an appealing characteristic, i.e.,
\emph{value generalization among policies}. We formally analyze the value
generalization under Generalized Policy Iteration (GPI). From theoretical and
empirical lens, we show that generalized value estimates offered by PeVFA may
have lower initial approximation error to true values of successive policies,
which is expected to improve consecutive value approximation during GPI. Based
on above clues, we introduce a new form of GPI with PeVFA which leverages the
value generalization along policy improvement path. Moreover, we propose a
representation learning framework for RL policy, providing several approaches
to learn effective policy embeddings from policy network parameters or
state-action pairs. In our experiments, we evaluate the efficacy of value
generalization offered by PeVFA and policy representation learning in several
OpenAI Gym continuous control tasks. For a representative instance of algorithm
implementation, Proximal Policy Optimization (PPO) re-implemented under the
paradigm of GPI with PeVFA achieves about 40\% performance improvement on its
vanilla counterpart in most environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hongyao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zhaopeng Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1"&gt;Jianye Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graves_D/0/1/0/all/0/1"&gt;Daniel Graves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1"&gt;Hangyu Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wulong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yaodong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Changmin Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[K-shot NAS: Learnable Weight-Sharing for NAS with K-shot Supernets. (arXiv:2106.06442v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06442</id>
        <link href="http://arxiv.org/abs/2106.06442"/>
        <updated>2021-06-14T01:38:55.163Z</updated>
        <summary type="html"><![CDATA[In one-shot weight sharing for NAS, the weights of each operation (at each
layer) are supposed to be identical for all architectures (paths) in the
supernet. However, this rules out the possibility of adjusting operation
weights to cater for different paths, which limits the reliability of the
evaluation results. In this paper, instead of counting on a single supernet, we
introduce $K$-shot supernets and take their weights for each operation as a
dictionary. The operation weight for each path is represented as a convex
combination of items in a dictionary with a simplex code. This enables a matrix
approximation of the stand-alone weight matrix with a higher rank ($K>1$). A
\textit{simplex-net} is introduced to produce architecture-customized code for
each path. As a result, all paths can adaptively learn how to share weights in
the $K$-shot supernets and acquire corresponding weights for better evaluation.
$K$-shot supernets and simplex-net can be iteratively trained, and we further
extend the search to the channel dimension. Extensive experiments on benchmark
datasets validate that K-shot NAS significantly improves the evaluation
accuracy of paths and thus brings in impressive performance improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xiu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Black-box Explanation of Object Detectors via Saliency Maps. (arXiv:2006.03204v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03204</id>
        <link href="http://arxiv.org/abs/2006.03204"/>
        <updated>2021-06-14T01:38:55.155Z</updated>
        <summary type="html"><![CDATA[We propose D-RISE, a method for generating visual explanations for the
predictions of object detectors. Utilizing the proposed similarity metric that
accounts for both localization and categorization aspects of object detection
allows our method to produce saliency maps that show image areas that most
affect the prediction. D-RISE can be considered "black-box" in the software
testing sense, as it only needs access to the inputs and outputs of an object
detector. Compared to gradient-based methods, D-RISE is more general and
agnostic to the particular type of object detector being tested, and does not
need knowledge of the inner workings of the model. We show that D-RISE can be
easily applied to different object detectors including one-stage detectors such
as YOLOv3 and two-stage detectors such as Faster-RCNN. We present a detailed
analysis of the generated visual explanations to highlight the utilization of
context and possible biases learned by object detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petsiuk_V/0/1/0/all/0/1"&gt;Vitali Petsiuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1"&gt;Rajiv Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manjunatha_V/0/1/0/all/0/1"&gt;Varun Manjunatha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morariu_V/0/1/0/all/0/1"&gt;Vlad I. Morariu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_A/0/1/0/all/0/1"&gt;Ashutosh Mehra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1"&gt;Vicente Ordonez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the optimal regularizer for inverse problems. (arXiv:2106.06513v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06513</id>
        <link href="http://arxiv.org/abs/2106.06513"/>
        <updated>2021-06-14T01:38:55.148Z</updated>
        <summary type="html"><![CDATA[In this work, we consider the linear inverse problem $y=Ax+\epsilon$, where
$A\colon X\to Y$ is a known linear operator between the separable Hilbert
spaces $X$ and $Y$, $x$ is a random variable in $X$ and $\epsilon$ is a
zero-mean random process in $Y$. This setting covers several inverse problems
in imaging including denoising, deblurring, and X-ray tomography. Within the
classical framework of regularization, we focus on the case where the
regularization functional is not given a priori but learned from data. Our
first result is a characterization of the optimal generalized Tikhonov
regularizer, with respect to the mean squared error. We find that it is
completely independent of the forward operator $A$ and depends only on the mean
and covariance of $x$. Then, we consider the problem of learning the
regularizer from a finite training set in two different frameworks: one
supervised, based on samples of both $x$ and $y$, and one unsupervised, based
only on samples of $x$. In both cases, we prove generalization bounds, under
some weak assumptions on the distribution of $x$ and $\epsilon$, including the
case of sub-Gaussian variables. Our bounds hold in infinite-dimensional spaces,
thereby showing that finer and finer discretizations do not make this learning
problem harder. The results are validated through numerical simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Alberti_G/0/1/0/all/0/1"&gt;Giovanni S. Alberti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vito_E/0/1/0/all/0/1"&gt;Ernesto De Vito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lassas_M/0/1/0/all/0/1"&gt;Matti Lassas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ratti_L/0/1/0/all/0/1"&gt;Luca Ratti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Santacesaria_M/0/1/0/all/0/1"&gt;Matteo Santacesaria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis. (arXiv:2010.05690v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05690</id>
        <link href="http://arxiv.org/abs/2010.05690"/>
        <updated>2021-06-14T01:38:55.141Z</updated>
        <summary type="html"><![CDATA[The issue of COVID-19, increasing with a massive mortality rate. This led to
the WHO declaring it as a pandemic. In this situation, it is crucial to perform
efficient and fast diagnosis. The reverse transcript polymerase chain reaction
(RTPCR) test is conducted to detect the presence of SARS-CoV-2. This test is
time-consuming and instead chest CT (or Chest X-ray) can be used for a fast and
accurate diagnosis. Automated diagnosis is considered to be important as it
reduces human effort and provides accurate and low-cost tests. The
contributions of our research are three-fold. First, it is aimed to analyse the
behaviour and performance of variant vision models ranging from Inception to
NAS networks with the appropriate fine-tuning procedure. Second, the behaviour
of these models is visually analysed by plotting CAMs for individual networks
and determining classification performance with AUCROC curves. Thirdly, stacked
ensembles techniques are imparted to provide higher generalisation on combining
the fine-tuned models, in which six ensemble neural networks are designed by
combining the existing fine-tuned networks. Implying these stacked ensembles
provides a great generalization to the models. The ensemble model designed by
combining all the fine-tuned networks obtained a state-of-the-art accuracy
score of 99.17%. The precision and recall for the COVID-19 class are 99.99% and
89.79% respectively, which resembles the robustness of the stacked ensembles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+B_L/0/1/0/all/0/1"&gt;Lalith Bharadwaj B&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boddeda_R/0/1/0/all/0/1"&gt;Rohit Boddeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1"&gt;Sai Vardhan K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+G_M/0/1/0/all/0/1"&gt;Madhu G&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Unified Quadrature Framework for Large-Scale Kernel Machines. (arXiv:2011.01668v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.01668</id>
        <link href="http://arxiv.org/abs/2011.01668"/>
        <updated>2021-06-14T01:38:55.132Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop a quadrature framework for large-scale kernel
machines via a numerical integration representation. Considering that the
integration domain and measure of typical kernels, e.g., Gaussian kernels,
arc-cosine kernels, are fully symmetric, we leverage deterministic fully
symmetric interpolatory rules to efficiently compute quadrature nodes and
associated weights for kernel approximation. The developed interpolatory rules
are able to reduce the number of needed nodes while retaining a high
approximation accuracy. Further, we randomize the above deterministic rules by
the classical Monte-Carlo sampling and control variates techniques with two
merits: 1) The proposed stochastic rules make the dimension of the feature
mapping flexibly varying, such that we can control the discrepancy between the
original and approximate kernels by tuning the dimnension. 2) Our stochastic
rules have nice statistical properties of unbiasedness and variance reduction
with fast convergence rate. In addition, we elucidate the relationship between
our deterministic/stochastic interpolatory rules and current quadrature rules
for kernel approximation, including the sparse grids quadrature and stochastic
spherical-radial rules, thereby unifying these methods under our framework.
Experimental results on several benchmark datasets show that our methods
compare favorably with other representative kernel approximation based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fanghui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yudong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1"&gt;Johan A.K. Suykens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Compositional Shape Priors for Few-Shot 3D Reconstruction. (arXiv:2106.06440v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06440</id>
        <link href="http://arxiv.org/abs/2106.06440"/>
        <updated>2021-06-14T01:38:55.110Z</updated>
        <summary type="html"><![CDATA[The impressive performance of deep convolutional neural networks in
single-view 3D reconstruction suggests that these models perform non-trivial
reasoning about the 3D structure of the output space. Recent work has
challenged this belief, showing that, on standard benchmarks, complex
encoder-decoder architectures perform similarly to nearest-neighbor baselines
or simple linear decoder models that exploit large amounts of per-category
data. However, building large collections of 3D shapes for supervised training
is a laborious process; a more realistic and less constraining task is
inferring 3D shapes for categories with few available training examples,
calling for a model that can successfully generalize to novel object classes.
In this work we experimentally demonstrate that naive baselines fail in this
few-shot learning setting, in which the network must learn informative shape
priors for inference of new categories. We propose three ways to learn a
class-specific global shape prior, directly from data. Using these techniques,
we are able to capture multi-scale information about the 3D shape, and account
for intra-class variability by virtue of an implicit compositional structure.
Experiments on the popular ShapeNet dataset show that our method outperforms a
zero-shot baseline by over 40%, and the current state-of-the-art by over 10%,
in terms of relative performance, in the few-shot setting.12]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michalkiewicz_M/0/1/0/all/0/1"&gt;Mateusz Michalkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsogkas_S/0/1/0/all/0/1"&gt;Stavros Tsogkas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parisot_S/0/1/0/all/0/1"&gt;Sarah Parisot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1"&gt;Mahsa Baktashmotlagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eriksson_A/0/1/0/all/0/1"&gt;Anders Eriksson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1"&gt;Eugene Belilovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPPL: Probabilistic Programming with Fast Exact Symbolic Inference. (arXiv:2010.03485v3 [cs.PL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03485</id>
        <link href="http://arxiv.org/abs/2010.03485"/>
        <updated>2021-06-14T01:38:55.101Z</updated>
        <summary type="html"><![CDATA[We present the Sum-Product Probabilistic Language (SPPL), a new probabilistic
programming language that automatically delivers exact solutions to a broad
range of probabilistic inference queries. SPPL translates probabilistic
programs into sum-product expressions, a new symbolic representation and
associated semantic domain that extends standard sum-product networks to
support mixed-type distributions, numeric transformations, logical formulas,
and pointwise and set-valued constraints. We formalize SPPL via a novel
translation strategy from probabilistic programs to sum-product expressions and
give sound exact algorithms for conditioning on and computing probabilities of
events. SPPL imposes a collection of restrictions on probabilistic programs to
ensure they can be translated into sum-product expressions, which allow the
system to leverage new techniques for improving the scalability of translation
and inference by automatically exploiting probabilistic structure. We implement
a prototype of SPPL with a modular architecture and evaluate it on benchmarks
the system targets, showing that it obtains up to 3500x speedups over
state-of-the-art symbolic systems on tasks such as verifying the fairness of
decision tree classifiers, smoothing hidden Markov models, conditioning
transformed random variables, and computing rare event probabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saad_F/0/1/0/all/0/1"&gt;Feras A. Saad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rinard_M/0/1/0/all/0/1"&gt;Martin C. Rinard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansinghka_V/0/1/0/all/0/1"&gt;Vikash K. Mansinghka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Robustness of Predictive Uncertainty Estimation: Are Dirichlet-based Models Reliable?. (arXiv:2010.14986v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14986</id>
        <link href="http://arxiv.org/abs/2010.14986"/>
        <updated>2021-06-14T01:38:55.092Z</updated>
        <summary type="html"><![CDATA[Dirichlet-based uncertainty (DBU) models are a recent and promising class of
uncertainty-aware models. DBU models predict the parameters of a Dirichlet
distribution to provide fast, high-quality uncertainty estimates alongside with
class predictions. In this work, we present the first large-scale, in-depth
study of the robustness of DBU models under adversarial attacks. Our results
suggest that uncertainty estimates of DBU models are not robust w.r.t. three
important tasks: (1) indicating correctly and wrongly classified samples; (2)
detecting adversarial examples; and (3) distinguishing between in-distribution
(ID) and out-of-distribution (OOD) data. Additionally, we explore the first
approaches to make DBU models more robust. While adversarial training has a
minor effect, our median smoothing based approach significantly increases
robustness of DBU models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kopetzki_A/0/1/0/all/0/1"&gt;Anna-Kathrin Kopetzki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charpentier_B/0/1/0/all/0/1"&gt;Bertrand Charpentier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zugner_D/0/1/0/all/0/1"&gt;Daniel Z&amp;#xfc;gner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giri_S/0/1/0/all/0/1"&gt;Sandhya Giri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training. (arXiv:2009.03294v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.03294</id>
        <link href="http://arxiv.org/abs/2009.03294"/>
        <updated>2021-06-14T01:38:55.085Z</updated>
        <summary type="html"><![CDATA[Normalization is known to help the optimization of deep neural networks.
Curiously, different architectures require specialized normalization methods.
In this paper, we study what normalization is effective for Graph Neural
Networks (GNNs). First, we adapt and evaluate the existing methods from other
domains to GNNs. Faster convergence is achieved with InstanceNorm compared to
BatchNorm and LayerNorm. We provide an explanation by showing that InstanceNorm
serves as a preconditioner for GNNs, but such preconditioning effect is weaker
with BatchNorm due to the heavy batch noise in graph datasets. Second, we show
that the shift operation in InstanceNorm results in an expressiveness
degradation of GNNs for highly regular graphs. We address this issue by
proposing GraphNorm with a learnable shift. Empirically, GNNs with GraphNorm
converge faster compared to GNNs using other normalization. GraphNorm also
improves the generalization of GNNs, achieving better performance on graph
classification benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shengjie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Keyulu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Model Selection in Contextual Bandits with Many Classes via Offline Oracles. (arXiv:2106.06483v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06483</id>
        <link href="http://arxiv.org/abs/2106.06483"/>
        <updated>2021-06-14T01:38:55.078Z</updated>
        <summary type="html"><![CDATA[We study the problem of model selection for contextual bandits, in which the
algorithm must balance the bias-variance trade-off for model estimation while
also balancing the exploration-exploitation trade-off. In this paper, we
propose the first reduction of model selection in contextual bandits to offline
model selection oracles, allowing for flexible general purpose algorithms with
computational requirements no worse than those for model selection for
regression. Our main result is a new model selection guarantee for stochastic
contextual bandits. When one of the classes in our set is realizable, up to a
logarithmic dependency on the number of classes, our algorithm attains optimal
realizability-based regret bounds for that class under one of two conditions:
if the time-horizon is large enough, or if an assumption that helps with
detecting misspecification holds. Hence our algorithm adapts to the complexity
of this unknown class. Even when this realizable class is known, we prove
improved regret guarantees in early rounds by relying on simpler model classes
for those rounds and hence further establish the importance of model selection
in contextual bandits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1"&gt;Sanath Kumar Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1"&gt;Susan Athey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Architecture Search without Training. (arXiv:2006.04647v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04647</id>
        <link href="http://arxiv.org/abs/2006.04647"/>
        <updated>2021-06-14T01:38:55.057Z</updated>
        <summary type="html"><![CDATA[The time and effort involved in hand-designing deep neural networks is
immense. This has prompted the development of Neural Architecture Search (NAS)
techniques to automate this design. However, NAS algorithms tend to be slow and
expensive; they need to train vast numbers of candidate networks to inform the
search process. This could be alleviated if we could partially predict a
network's trained accuracy from its initial state. In this work, we examine the
overlap of activations between datapoints in untrained networks and motivate
how this can give a measure which is usefully indicative of a network's trained
performance. We incorporate this measure into a simple algorithm that allows us
to search for powerful networks without any training in a matter of seconds on
a single GPU, and verify its effectiveness on NAS-Bench-101, NAS-Bench-201,
NATS-Bench, and Network Design Spaces. Our approach can be readily combined
with more expensive search methods; we examine a simple adaptation of
regularised evolutionary search. Code for reproducing our experiments is
available at https://github.com/BayesWatch/nas-without-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mellor_J/0/1/0/all/0/1"&gt;Joseph Mellor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_J/0/1/0/all/0/1"&gt;Jack Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1"&gt;Amos Storkey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crowley_E/0/1/0/all/0/1"&gt;Elliot J. Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC-Learning for Strategic Classification. (arXiv:2012.03310v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03310</id>
        <link href="http://arxiv.org/abs/2012.03310"/>
        <updated>2021-06-14T01:38:55.050Z</updated>
        <summary type="html"><![CDATA[The study of strategic or adversarial manipulation of testing data to fool a
classifier has attracted much recent attention. Most previous works have
focused on two extreme situations where any testing data point either is
completely adversarial or always equally prefers the positive label. In this
paper, we generalize both of these through a unified framework for strategic
classification, and introduce the notion of strategic VC-dimension (SVC) to
capture the PAC-learnability in our general strategic setup. SVC provably
generalizes the recent concept of adversarial VC-dimension (AVC) introduced by
Cullina et al. arXiv:1806.01471. We instantiate our framework for the
fundamental strategic linear classification problem. We fully characterize: (1)
the statistical learnability of linear classifiers by pinning down its SVC; (2)
its computational tractability by pinning down the complexity of the empirical
risk minimization problem. Interestingly, the SVC of linear classifiers is
always upper bounded by its standard VC-dimension. This characterization also
strictly generalizes the AVC bound for linear classifiers in arXiv:1806.01471.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sundaram_R/0/1/0/all/0/1"&gt;Ravi Sundaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vullikanti_A/0/1/0/all/0/1"&gt;Anil Vullikanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haifeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_F/0/1/0/all/0/1"&gt;Fan Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Limitations of Large Width in Neural Networks: A Deep Gaussian Process Perspective. (arXiv:2106.06529v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06529</id>
        <link href="http://arxiv.org/abs/2106.06529"/>
        <updated>2021-06-14T01:38:55.044Z</updated>
        <summary type="html"><![CDATA[Large width limits have been a recent focus of deep learning research: modulo
computational practicalities, do wider networks outperform narrower ones?
Answering this question has been challenging, as conventional networks gain
representational power with width, potentially masking any negative effects.
Our analysis in this paper decouples capacity and width via the generalization
of neural networks to Deep Gaussian Processes (Deep GP), a class of
hierarchical models that subsume neural nets. In doing so, we aim to understand
how width affects standard neural networks once they have sufficient capacity
for a given modeling task. Our theoretical and empirical results on Deep GP
suggest that large width is generally detrimental to hierarchical models.
Surprisingly, we prove that even nonparametric Deep GP converge to Gaussian
processes, effectively becoming shallower without any increase in
representational power. The posterior, which corresponds to a mixture of
data-adaptable basis functions, becomes less data-dependent with width. Our
tail analysis demonstrates that width and depth have opposite effects: depth
accentuates a model's non-Gaussianity, while width makes models increasingly
Gaussian. We find there is a "sweet spot" that maximizes test set performance
before the limiting GP behavior prevents adaptability, occurring at width = 1
or width = 2 for nonparametric Deep GP. These results make strong predictions
about the same phenomenon in conventional neural networks: we show empirically
that many neural network architectures need 10 - 500 hidden units for
sufficient capacity - depending on the dataset - but further width degrades
test performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1"&gt;Geoff Pleiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cunningham_J/0/1/0/all/0/1"&gt;John P. Cunningham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hurricane Forecasting: A Novel Multimodal Machine Learning Framework. (arXiv:2011.06125v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.06125</id>
        <link href="http://arxiv.org/abs/2011.06125"/>
        <updated>2021-06-14T01:38:55.036Z</updated>
        <summary type="html"><![CDATA[This paper describes a machine learning (ML) framework for tropical cyclone
intensity and track forecasting, combining multiple distinct ML techniques and
utilizing diverse data sources. Our framework, which we refer to as Hurricast
(HURR), is built upon the combination of distinct data processing techniques
using gradient-boosted trees and novel encoder-decoder architectures, including
CNN, GRU and Transformers components. We propose a deep-feature extractor
methodology to mix spatial-temporal data with statistical data efficiently. Our
multimodal framework unleashes the potential of making forecasts based on a
wide range of data sources, including historical storm data, and visual data
such as reanalysis atmospheric images. We evaluate our models with current
operational forecasts in North Atlantic and Eastern Pacific basins on 2016-2019
for 24-hour lead time, and show our models consistently outperform
statistical-dynamical models and compete with the best dynamical models, while
computing forecasts in seconds. Furthermore, the inclusion of Hurricast into an
operational forecast consensus model leads to a significant improvement of 5% -
15% over NHC's official forecast, thus highlighting the complementary
properties with existing approaches. In summary, our work demonstrates that
combining different data sources and distinct machine learning methodologies
can lead to superior tropical cyclone forecasting. We hope that this work opens
the door for further use of machine learning in meteorological forecasting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boussioux_L/0/1/0/all/0/1"&gt;L&amp;#xe9;onard Boussioux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1"&gt;Cynthia Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guenais_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Gu&amp;#xe9;nais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertsimas_D/0/1/0/all/0/1"&gt;Dimitris Bertsimas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing Classifier Adjacency Relations: A Case Study in Speaker Verification and Voice Anti-Spoofing. (arXiv:2106.06362v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.06362</id>
        <link href="http://arxiv.org/abs/2106.06362"/>
        <updated>2021-06-14T01:38:55.030Z</updated>
        <summary type="html"><![CDATA[Whether it be for results summarization, or the analysis of classifier
fusion, some means to compare different classifiers can often provide
illuminating insight into their behaviour, (dis)similarity or complementarity.
We propose a simple method to derive 2D representation from detection scores
produced by an arbitrary set of binary classifiers in response to a common
dataset. Based upon rank correlations, our method facilitates a visual
comparison of classifiers with arbitrary scores and with close relation to
receiver operating characteristic (ROC) and detection error trade-off (DET)
analyses. While the approach is fully versatile and can be applied to any
detection task, we demonstrate the method using scores produced by automatic
speaker verification and voice anti-spoofing systems. The former are produced
by a Gaussian mixture model system trained with VoxCeleb data whereas the
latter stem from submissions to the ASVspoof 2019 challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kinnunen_T/0/1/0/all/0/1"&gt;Tomi Kinnunen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nautsch_A/0/1/0/all/0/1"&gt;Andreas Nautsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahidullah_M/0/1/0/all/0/1"&gt;Md Sahidullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evans_N/0/1/0/all/0/1"&gt;Nicholas Evans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Todisco_M/0/1/0/all/0/1"&gt;Massimiliano Todisco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delgado_H/0/1/0/all/0/1"&gt;H&amp;#xe9;ctor Delgado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1"&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kong Aik Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nystr\"om landmark sampling and regularized Christoffel functions. (arXiv:1905.12346v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.12346</id>
        <link href="http://arxiv.org/abs/1905.12346"/>
        <updated>2021-06-14T01:38:55.012Z</updated>
        <summary type="html"><![CDATA[Selecting diverse and important items, called landmarks, from a large set is
a problem of interest in machine learning. As a specific example, in order to
deal with large training sets, kernel methods often rely on low rank matrix
Nystr\"om approximations based on the selection or sampling of landmarks. In
this context, we propose a deterministic and a randomized adaptive algorithm
for selecting landmark points within a training data set, which are related to
the minima of a sequence of kernelized Christoffel functions. Beyond the known
connection between Christoffel functions and leverage scores, a connection of
our method with determinantal point processes (DPPs) is also explained. Namely,
our construction promotes diversity among important landmark points in a way
similar to DPPs. Also, we explain how our randomized adaptive algorithm can
influence the accuracy of Kernel Ridge Regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fanuel_M/0/1/0/all/0/1"&gt;Micha&amp;#xeb;l Fanuel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schreurs_J/0/1/0/all/0/1"&gt;Joachim Schreurs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1"&gt;Johan A.K. Suykens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Environment Inference for Invariant Learning. (arXiv:2010.07249v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07249</id>
        <link href="http://arxiv.org/abs/2010.07249"/>
        <updated>2021-06-14T01:38:55.006Z</updated>
        <summary type="html"><![CDATA[Learning models that gracefully handle distribution shifts is central to
research on domain generalization, robust optimization, and fairness. A
promising formulation is domain-invariant learning, which identifies the key
issue of learning which features are domain-specific versus domain-invariant.
An important assumption in this area is that the training examples are
partitioned into "domains" or "environments". Our focus is on the more common
setting where such partitions are not provided. We propose EIIL, a general
framework for domain-invariant learning that incorporates Environment Inference
to directly infer partitions that are maximally informative for downstream
Invariant Learning. We show that EIIL outperforms invariant learning methods on
the CMNIST benchmark without using environment labels, and significantly
outperforms ERM on worst-group performance in the Waterbirds and CivilComments
datasets. Finally, we establish connections between EIIL and algorithmic
fairness, which enables EIIL to improve accuracy and calibration in a fair
prediction problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1"&gt;Elliot Creager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobsen_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rn-Henrik Jacobsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1"&gt;Richard Zemel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revealing the Structure of Deep Neural Networks via Convex Duality. (arXiv:2002.09773v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.09773</id>
        <link href="http://arxiv.org/abs/2002.09773"/>
        <updated>2021-06-14T01:38:54.999Z</updated>
        <summary type="html"><![CDATA[We study regularized deep neural networks (DNNs) and introduce a convex
analytic framework to characterize the structure of the hidden layers. We show
that a set of optimal hidden layer weights for a norm regularized DNN training
problem can be explicitly found as the extreme points of a convex set. For the
special case of deep linear networks, we prove that each optimal weight matrix
aligns with the previous layers via duality. More importantly, we apply the
same characterization to deep ReLU networks with whitened data and prove the
same weight alignment holds. As a corollary, we also prove that norm
regularized deep ReLU networks yield spline interpolation for one-dimensional
datasets which was previously known only for two-layer networks. Furthermore,
we provide closed-form solutions for the optimal layer weights when data is
rank-one or whitened. The same analysis also applies to architectures with
batch normalization even for arbitrary data. Therefore, we obtain a complete
explanation for a recent empirical observation termed Neural Collapse where
class means collapse to the vertices of a simplex equiangular tight frame.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ergen_T/0/1/0/all/0/1"&gt;Tolga Ergen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1"&gt;Mert Pilanci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Physical Adversarial Examples for Autonomous Driving with Fast and Differentiable Image Compositing. (arXiv:2010.08844v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08844</id>
        <link href="http://arxiv.org/abs/2010.08844"/>
        <updated>2021-06-14T01:38:54.988Z</updated>
        <summary type="html"><![CDATA[There is considerable evidence that deep neural networks are vulnerable to
adversarial perturbations applied directly to their digital inputs. However, it
remains an open question whether this translates to vulnerabilities in real
systems. For example, an attack on self-driving cars would in practice entail
modifying the driving environment, which then impacts the video inputs to the
car's controller, thereby indirectly leading to incorrect driving decisions.
Such attacks require accounting for system dynamics and tracking viewpoint
changes. We propose a scalable approach for finding adversarial modifications
of a simulated autonomous driving environment using a differentiable
approximation for the mapping from environmental modifications (rectangles on
the road) to the corresponding video inputs to the controller neural network.
Given the parameters of the rectangles, our proposed differentiable mapping
composites them onto pre-recorded video streams of the original environment,
accounting for geometric and color variations. Moreover, we propose a multiple
trajectory sampling approach that enables our attacks to be robust to a car's
self-correcting behavior. When combined with a neural network-based controller,
our approach allows the design of adversarial modifications through end-to-end
gradient-based optimization. Using the Carla autonomous driving simulator, we
show that our approach is significantly more scalable and far more effective at
identifying autonomous vehicle vulnerabilities in simulation experiments than a
state-of-the-art approach based on Bayesian Optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinghan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boloor_A/0/1/0/all/0/1"&gt;Adith Boloor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1"&gt;Ayan Chakrabarti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1"&gt;Yevgeniy Vorobeychik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Mechanical Analysis of Neural Network Pruning. (arXiv:2006.16617v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16617</id>
        <link href="http://arxiv.org/abs/2006.16617"/>
        <updated>2021-06-14T01:38:54.980Z</updated>
        <summary type="html"><![CDATA[Deep learning architectures with a huge number of parameters are often
compressed using pruning techniques to ensure computational efficiency of
inference during deployment. Despite multitude of empirical advances, there is
a lack of theoretical understanding of the effectiveness of different pruning
methods. We inspect different pruning techniques under the statistical
mechanics formulation of a teacher-student framework and derive their
generalization error (GE) bounds. It has been shown that Determinantal Point
Process (DPP) based node pruning method is notably superior to competing
approaches when tested on real datasets. Using GE bounds in the aforementioned
setup we provide theoretical guarantees for their empirical observations.
Another consistent finding in literature is that sparse neural networks (edge
pruned) generalize better than dense neural networks (node pruned) for a fixed
number of parameters. We use our theoretical setup to prove this finding and
show that even the baseline random edge pruning method performs better than the
DPP node pruning method. We also validate this empirically on real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acharyya_R/0/1/0/all/0/1"&gt;Rupam Acharyya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chattoraj_A/0/1/0/all/0/1"&gt;Ankani Chattoraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Boyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Shouman Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefankovic_D/0/1/0/all/0/1"&gt;Daniel Stefankovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asynchronous \epsilon-Greedy Bayesian Optimisation. (arXiv:2010.07615v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07615</id>
        <link href="http://arxiv.org/abs/2010.07615"/>
        <updated>2021-06-14T01:38:54.962Z</updated>
        <summary type="html"><![CDATA[Batch Bayesian optimisation (BO) is a successful technique for the
optimisation of expensive black-box functions. Asynchronous BO can reduce
wallclock time by starting a new evaluation as soon as another finishes, thus
maximising resource utilisation. To maximise resource allocation, we develop a
novel asynchronous BO method, AEGiS (Asynchronous $\epsilon$-Greedy Global
Search) that combines greedy search, exploiting the surrogate's mean
prediction, with Thompson sampling and random selection from the approximate
Pareto set describing the trade-off between exploitation (surrogate mean
prediction) and exploration (surrogate posterior variance). We demonstrate
empirically the efficacy of AEGiS on synthetic benchmark problems,
meta-surrogate hyperparameter tuning problems and real-world problems, showing
that AEGiS generally outperforms existing methods for asynchronous BO. When a
single worker is available performance is no worse than BO using expected
improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ath_G/0/1/0/all/0/1"&gt;George De Ath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Everson_R/0/1/0/all/0/1"&gt;Richard M. Everson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fieldsend_J/0/1/0/all/0/1"&gt;Jonathan E. Fieldsend&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Driven Multiscale Design of Cellular Composites with Multiclass Microstructures for Natural Frequency Maximization. (arXiv:2106.06478v1 [cs.CE])]]></title>
        <id>http://arxiv.org/abs/2106.06478</id>
        <link href="http://arxiv.org/abs/2106.06478"/>
        <updated>2021-06-14T01:38:54.954Z</updated>
        <summary type="html"><![CDATA[For natural frequency optimization of engineering structures, cellular
composites have been shown to possess an edge over solid. However, existing
multiscale design methods for cellular composites are either computationally
exhaustive or confined to a single class of microstructures. In this paper, we
propose a data-driven topology optimization (TO) approach to enable the
multiscale design of cellular structures with various choices of microstructure
classes. The key component is a newly proposed latent-variable Gaussian process
(LVGP) model through which different classes of microstructures are mapped into
a low-dimensional continuous latent space. It provides an interpretable
distance metric between classes and captures their effects on the homogenized
stiffness tensors. By introducing latent vectors as design variables, a
differentiable transition of stiffness matrix between classes can be easily
achieved with an analytical gradient. After integrating LVGP with the
density-based TO, an efficient data-driven cellular composite optimization
process is developed to enable concurrent exploration of microstructure
concepts and the associated volume fractions for natural frequency
optimization. Examples reveal that the proposed cellular designs with
multiclass microstructures achieve higher natural frequencies than both
single-scale and single-class designs. This framework can be easily extended to
other multi-scale TO problems, such as thermal compliance and dynamic response
optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beek_A/0/1/0/all/0/1"&gt;Anton van Beek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Da_D/0/1/0/all/0/1"&gt;Daicong Da&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_Y/0/1/0/all/0/1"&gt;Yu-Chin Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1"&gt;Ping Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signed Graph Metric Learning via Gershgorin Disc Perfect Alignment. (arXiv:2006.08816v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08816</id>
        <link href="http://arxiv.org/abs/2006.08816"/>
        <updated>2021-06-14T01:38:54.527Z</updated>
        <summary type="html"><![CDATA[Given a convex and differentiable objective $Q(\M)$ for a real symmetric
matrix $\M$ in the positive definite (PD) cone -- used to compute Mahalanobis
distances -- we propose a fast general metric learning framework that is
entirely projection-free. We first assume that $\M$ resides in a space $\cS$ of
generalized graph Laplacian matrices corresponding to balanced signed graphs.
$\M \in \cS$ that is also PD is called a graph metric matrix. Unlike low-rank
metric matrices common in the literature, $\cS$ includes the important
diagonal-only matrices as a special case. The key theorem to circumvent full
eigen-decomposition and enable fast metric matrix optimization is Gershgorin
disc perfect alignment (GDPA): given $\M \in \cS$ and diagonal matrix $\S$,
where $S_{ii} = 1/v_i$ and $\v$ is $\M$'s first eigenvector, we prove that
Gershgorin disc left-ends of similarity transform $\B = \S \M \S^{-1}$ are
perfectly aligned at the smallest eigenvalue $\lambda_{\min}$. Using this
theorem, we replace the PD cone constraint in the metric learning problem with
tightest possible linear constraints per iteration, so that the alternating
optimization of the diagonal / off-diagonal terms in $\M$ can be solved
efficiently as linear programs via the Frank-Wolfe method. We update $\v$ using
Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) with warm
start as entries in $\M$ are optimized successively. Experiments show that our
graph metric optimization is significantly faster than cone-projection schemes,
and produces competitive binary classification performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_G/0/1/0/all/0/1"&gt;Gene Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overfitting in Bayesian Optimization: an empirical study and early-stopping solution. (arXiv:2104.08166v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08166</id>
        <link href="http://arxiv.org/abs/2104.08166"/>
        <updated>2021-06-14T01:38:54.519Z</updated>
        <summary type="html"><![CDATA[Tuning machine learning models with Bayesian optimization (BO) is a
successful strategy to find good hyperparameters. BO defines an iterative
procedure where a cross-validated metric is evaluated on promising
hyperparameters. In practice, however, an improvement of the validation metric
may not translate in better predictive performance on a test set, especially
when tuning models trained on small datasets. In other words, unlike
conventional wisdom dictates, BO can overfit. In this paper, we carry out the
first systematic investigation of overfitting in BO and demonstrate that this
issue is serious, yet often overlooked in practice. We propose a novel
criterion to early stop BO, which aims to maintain the solution quality while
saving the unnecessary iterations that can lead to overfitting. Experiments on
real-world hyperparameter optimization problems show that our approach
effectively meets these goals and is more adaptive comparing to baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makarova_A/0/1/0/all/0/1"&gt;Anastasia Makarova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Huibin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perrone_V/0/1/0/all/0/1"&gt;Valerio Perrone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_A/0/1/0/all/0/1"&gt;Aaron Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faddoul_J/0/1/0/all/0/1"&gt;Jean Baptiste Faddoul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeger_M/0/1/0/all/0/1"&gt;Matthias Seeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1"&gt;Cedric Archambeau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularized Softmax Deep Multi-Agent $Q$-Learning. (arXiv:2103.11883v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11883</id>
        <link href="http://arxiv.org/abs/2103.11883"/>
        <updated>2021-06-14T01:38:54.513Z</updated>
        <summary type="html"><![CDATA[Tackling overestimation in $Q$-learning is an important problem that has been
extensively studied in single-agent reinforcement learning, but has received
comparatively little attention in the multi-agent setting. In this work, we
empirically demonstrate that QMIX, a popular $Q$-learning algorithm for
cooperative multi-agent reinforcement learning (MARL), suffers from a more
severe overestimation in practice than previously acknowledged, and is not
mitigated by existing approaches. We rectify this with a novel
regularization-based update scheme that penalizes large joint action-values
that deviate from a baseline and demonstrate its effectiveness in stabilizing
learning. Furthermore, we propose to employ a softmax operator, which we
efficiently approximate in a novel way in the multi-agent setting, to further
reduce the potential overestimation bias. Our approach, Regularized Softmax
(RES) Deep Multi-Agent $Q$-Learning, is general and can be applied to any
$Q$-learning based MARL algorithm. We demonstrate that, when applied to QMIX,
RES avoids severe overestimation and significantly improves performance,
yielding state-of-the-art results in a variety of cooperative multi-agent
tasks, including the challenging StarCraft II micromanagement benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1"&gt;Ling Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1"&gt;Tabish Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1"&gt;Bei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longbo Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1"&gt;Shimon Whiteson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models. (arXiv:2104.07788v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07788</id>
        <link href="http://arxiv.org/abs/2104.07788"/>
        <updated>2021-06-14T01:38:54.496Z</updated>
        <summary type="html"><![CDATA[We present PyTorch Geometric Temporal a deep learning framework combining
state-of-the-art machine learning algorithms for neural spatiotemporal signal
processing. The main goal of the library is to make temporal geometric deep
learning available for researchers and machine learning practitioners in a
unified easy-to-use framework. PyTorch Geometric Temporal was created with
foundations on existing libraries in the PyTorch eco-system, streamlined neural
network layer definitions, temporal snapshot generators for batching, and
integrated benchmark datasets. These features are illustrated with a
tutorial-like case study. Experiments demonstrate the predictive performance of
the models implemented in the library on real world problems such as
epidemiological forecasting, ridehail demand prediction and web-traffic
management. Our sensitivity analysis of runtime shows that the framework can
potentially operate on web-scale datasets with rich temporal features and
spatial structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rozemberczki_B/0/1/0/all/0/1"&gt;Benedek Rozemberczki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scherer_P/0/1/0/all/0/1"&gt;Paul Scherer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yixuan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panagopoulos_G/0/1/0/all/0/1"&gt;George Panagopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedel_A/0/1/0/all/0/1"&gt;Alexander Riedel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astefanoaei_M/0/1/0/all/0/1"&gt;Maria Astefanoaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiss_O/0/1/0/all/0/1"&gt;Oliver Kiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beres_F/0/1/0/all/0/1"&gt;Ferenc Beres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_G/0/1/0/all/0/1"&gt;Guzm&amp;#xe1;n L&amp;#xf3;pez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collignon_N/0/1/0/all/0/1"&gt;Nicolas Collignon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_R/0/1/0/all/0/1"&gt;Rik Sarkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalizable Episodic Memory for Deep Reinforcement Learning. (arXiv:2103.06469v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06469</id>
        <link href="http://arxiv.org/abs/2103.06469"/>
        <updated>2021-06-14T01:38:54.489Z</updated>
        <summary type="html"><![CDATA[Episodic memory-based methods can rapidly latch onto past successful
strategies by a non-parametric memory and improve sample efficiency of
traditional reinforcement learning. However, little effort is put into the
continuous domain, where a state is never visited twice, and previous episodic
methods fail to efficiently aggregate experience across trajectories. To
address this problem, we propose Generalizable Episodic Memory (GEM), which
effectively organizes the state-action values of episodic memory in a
generalizable manner and supports implicit planning on memorized trajectories.
GEM utilizes a double estimator to reduce the overestimation bias induced by
value propagation in the planning process. Empirical evaluation shows that our
method significantly outperforms existing trajectory-based methods on various
MuJoCo continuous control tasks. To further show the general applicability, we
evaluate our method on Atari games with discrete action space, which also shows
a significant improvement over baseline algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jianing Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1"&gt;Guangxiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhizhou Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying and Reducing Bias in Maximum Likelihood Estimation of Structured Anomalies. (arXiv:2007.07878v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.07878</id>
        <link href="http://arxiv.org/abs/2007.07878"/>
        <updated>2021-06-14T01:38:54.483Z</updated>
        <summary type="html"><![CDATA[Anomaly estimation, or the problem of finding a subset of a dataset that
differs from the rest of the dataset, is a classic problem in machine learning
and data mining. In both theoretical work and in applications, the anomaly is
assumed to have a specific structure defined by membership in an
$\textit{anomaly family}$. For example, in temporal data the anomaly family may
be time intervals, while in network data the anomaly family may be connected
subgraphs. The most prominent approach for anomaly estimation is to compute the
Maximum Likelihood Estimator (MLE) of the anomaly; however, it was recently
observed that for normally distributed data, the MLE is a $\textit{biased}$
estimator for some anomaly families. In this work, we demonstrate that in the
normal means setting, the bias of the MLE depends on the size of the anomaly
family. We prove that if the number of sets in the anomaly family that contain
the anomaly is sub-exponential, then the MLE is asymptotically unbiased. We
also provide empirical evidence that the converse is true: if the number of
such sets is exponential, then the MLE is asymptotically biased. Our analysis
unifies a number of earlier results on the bias of the MLE for specific anomaly
families. Next, we derive a new anomaly estimator using a mixture model, and we
prove that our anomaly estimator is asymptotically unbiased regardless of the
size of the anomaly family. We illustrate the advantages of our estimator
versus the MLE on disease outbreak and highway traffic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chitra_U/0/1/0/all/0/1"&gt;Uthsav Chitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1"&gt;Kimberly Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jasper C.H. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raphael_B/0/1/0/all/0/1"&gt;Benjamin J. Raphael&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Multi-Object Tracking and Segmentation with GMPHD Filter and Mask-based Affinity Fusion. (arXiv:2009.00100v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00100</id>
        <link href="http://arxiv.org/abs/2009.00100"/>
        <updated>2021-06-14T01:38:54.476Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a highly practical fully online multi-object
tracking and segmentation (MOTS) method that uses instance segmentation results
as an input. The proposed method is based on the Gaussian mixture probability
hypothesis density (GMPHD) filter, a hierarchical data association (HDA), and a
mask-based affinity fusion (MAF) model to achieve high-performance online
tracking. The HDA consists of two associations: segment-to-track and
track-to-track associations. One affinity, for position and motion, is computed
by using the GMPHD filter, and the other affinity, for appearance is computed
by using the responses from a single object tracker such as a kernalized
correlation filter. These two affinities are simply fused by using a
score-level fusion method such as min-max normalization referred to as MAF. In
addition, to reduce the number of false positive segments, we adopt mask
IoU-based merging (mask merging). The proposed MOTS framework with the key
modules: HDA, MAF, and mask merging, is easily extensible to simultaneously
track multiple types of objects with CPU only execution in parallel processing.
In addition, the developed framework only requires simple parameter tuning
unlike many existing MOTS methods that need intensive hyperparameter
optimization. In the experiments on the two popular MOTS datasets, the key
modules show some improvements. For instance, ID-switch decreases by more than
half compared to a baseline method in the training sets. In conclusion, our
tracker achieves state-of-the-art MOTS performance in the test sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Young-min Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1"&gt;Young-chul Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1"&gt;Kwangjin Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1"&gt;Moongu Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1"&gt;Witold Pedrycz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Symbolic Regression that Scales. (arXiv:2106.06427v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06427</id>
        <link href="http://arxiv.org/abs/2106.06427"/>
        <updated>2021-06-14T01:38:54.470Z</updated>
        <summary type="html"><![CDATA[Symbolic equations are at the core of scientific discovery. The task of
discovering the underlying equation from a set of input-output pairs is called
symbolic regression. Traditionally, symbolic regression methods use
hand-designed strategies that do not improve with experience. In this paper, we
introduce the first symbolic regression method that leverages large scale
pre-training. We procedurally generate an unbounded set of equations, and
simultaneously pre-train a Transformer to predict the symbolic equation from a
corresponding set of input-output-pairs. At test time, we query the model on a
new set of points and use its output to guide the search for the equation. We
show empirically that this approach can re-discover a set of well-known
physical equations, and that it improves over time with more data and compute.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biggio_L/0/1/0/all/0/1"&gt;Luca Biggio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bendinelli_T/0/1/0/all/0/1"&gt;Tommaso Bendinelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neitz_A/0/1/0/all/0/1"&gt;Alexander Neitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1"&gt;Aurelien Lucchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parascandolo_G/0/1/0/all/0/1"&gt;Giambattista Parascandolo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online and Distribution-Free Robustness: Regression and Contextual Bandits with Huber Contamination. (arXiv:2010.04157v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04157</id>
        <link href="http://arxiv.org/abs/2010.04157"/>
        <updated>2021-06-14T01:38:54.453Z</updated>
        <summary type="html"><![CDATA[In this work we revisit two classic high-dimensional online learning
problems, namely linear regression and contextual bandits, from the perspective
of adversarial robustness. Existing works in algorithmic robust statistics make
strong distributional assumptions that ensure that the input data is evenly
spread out or comes from a nice generative model. Is it possible to achieve
strong robustness guarantees even without distributional assumptions
altogether, where the sequence of tasks we are asked to solve is adaptively and
adversarially chosen?

We answer this question in the affirmative for both linear regression and
contextual bandits. In fact our algorithms succeed where conventional methods
fail. In particular we show strong lower bounds against Huber regression and
more generally any convex M-estimator. Our approach is based on a novel
alternating minimization scheme that interleaves ordinary least-squares with a
simple convex program that finds the optimal reweighting of the distribution
under a spectral constraint. Our results obtain essentially optimal dependence
on the contamination level $\eta$, reach the optimal breakdown point, and
naturally apply to infinite dimensional settings where the feature vectors are
represented implicitly via a kernel map.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sitan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1"&gt;Frederic Koehler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1"&gt;Ankur Moitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yau_M/0/1/0/all/0/1"&gt;Morris Yau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coded-InvNet for Resilient Prediction Serving Systems. (arXiv:2106.06445v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06445</id>
        <link href="http://arxiv.org/abs/2106.06445"/>
        <updated>2021-06-14T01:38:54.446Z</updated>
        <summary type="html"><![CDATA[Inspired by a new coded computation algorithm for invertible functions, we
propose Coded-InvNet a new approach to design resilient prediction serving
systems that can gracefully handle stragglers or node failures. Coded-InvNet
leverages recent findings in the deep learning literature such as invertible
neural networks, Manifold Mixup, and domain translation algorithms, identifying
interesting research directions that span across machine learning and systems.
Our experimental results show that Coded-InvNet can outperform existing
approaches, especially when the compute resource overhead is as low as 10%. For
instance, without knowing which of the ten workers is going to fail, our
algorithm can design a backup task so that it can correctly recover the missing
prediction result with an accuracy of 85.9%, significantly outperforming the
previous SOTA by 32.5%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1"&gt;Tuan Dinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kangwook Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Video Synthesis with Action Graphs. (arXiv:2006.15327v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15327</id>
        <link href="http://arxiv.org/abs/2006.15327"/>
        <updated>2021-06-14T01:38:54.436Z</updated>
        <summary type="html"><![CDATA[Videos of actions are complex signals containing rich compositional structure
in space and time. Current video generation methods lack the ability to
condition the generation on multiple coordinated and potentially simultaneous
timed actions. To address this challenge, we propose to represent the actions
in a graph structure called Action Graph and present the new ``Action Graph To
Video'' synthesis task. Our generative model for this task (AG2Vid)
disentangles motion and appearance features, and by incorporating a scheduling
mechanism for actions facilitates a timely and coordinated video generation. We
train and evaluate AG2Vid on the CATER and Something-Something V2 datasets, and
show that the resulting videos have better visual quality and semantic
consistency compared to baselines. Finally, our model demonstrates zero-shot
abilities by synthesizing novel compositions of the learned actions. For code
and pretrained models, see the project page https://roeiherz.github.io/AG2Video]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1"&gt;Amir Bar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1"&gt;Roei Herzig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1"&gt;Anna Rohrbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1"&gt;Amir Globerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards the Unification and Robustness of Perturbation and Gradient Based Explanations. (arXiv:2102.10618v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10618</id>
        <link href="http://arxiv.org/abs/2102.10618"/>
        <updated>2021-06-14T01:38:54.429Z</updated>
        <summary type="html"><![CDATA[As machine learning black boxes are increasingly being deployed in critical
domains such as healthcare and criminal justice, there has been a growing
emphasis on developing techniques for explaining these black boxes in a post
hoc manner. In this work, we analyze two popular post hoc interpretation
techniques: SmoothGrad which is a gradient based method, and a variant of LIME
which is a perturbation based method. More specifically, we derive explicit
closed form expressions for the explanations output by these two methods and
show that they both converge to the same explanation in expectation, i.e., when
the number of perturbed samples used by these methods is large. We then
leverage this connection to establish other desirable properties, such as
robustness, for these techniques. We also derive finite sample complexity
bounds for the number of perturbations required for these methods to converge
to their expected explanation. Finally, we empirically validate our theory
using extensive experimentation on both synthetic and real world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sushant Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jabbari_S/0/1/0/all/0/1"&gt;Shahin Jabbari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1"&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Sohini Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiwei Steven Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Complexity in Decentralized Training. (arXiv:2006.08085v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08085</id>
        <link href="http://arxiv.org/abs/2006.08085"/>
        <updated>2021-06-14T01:38:54.408Z</updated>
        <summary type="html"><![CDATA[Decentralization is a promising method of scaling up parallel machine
learning systems. In this paper, we provide a tight lower bound on the
iteration complexity for such methods in a stochastic non-convex setting. Our
lower bound reveals a theoretical gap in known convergence rates of many
existing decentralized training algorithms, such as D-PSGD. We prove by
construction this lower bound is tight and achievable. Motivated by our
insights, we further propose DeTAG, a practical gossip-style decentralized
algorithm that achieves the lower bound with only a logarithm gap. Empirically,
we compare DeTAG with other decentralized algorithms on image classification
tasks, and we show DeTAG enjoys faster convergence compared to baselines,
especially on unshuffled data and in sparse networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yucheng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WAX-ML: A Python library for machine learning and feedback loops on streaming data. (arXiv:2106.06524v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06524</id>
        <link href="http://arxiv.org/abs/2106.06524"/>
        <updated>2021-06-14T01:38:54.386Z</updated>
        <summary type="html"><![CDATA[Wax is what you put on a surfboard to avoid slipping. It is an essential tool
to go surfing... We introduce WAX-ML a research-oriented Python library
providing tools to design powerful machine learning algorithms and feedback
loops working on streaming data. It strives to complement JAX with tools
dedicated to time series. WAX-ML makes JAX-based programs easy to use for
end-users working with pandas and xarray for data manipulation. It provides a
simple mechanism for implementing feedback loops, allows the implementation of
online learning and reinforcement learning algorithms with functions, and makes
them easy to integrate by end-users working with the object-oriented
reinforcement learning framework from the Gym library. It is released with an
Apache open-source license on GitHub at https://github.com/eserie/wax-ml.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Serie_E/0/1/0/all/0/1"&gt;Emmanuel S&amp;#xe9;ri&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Robust Mean Estimation under Coordinate-level Corruption. (arXiv:2002.04137v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.04137</id>
        <link href="http://arxiv.org/abs/2002.04137"/>
        <updated>2021-06-14T01:38:54.378Z</updated>
        <summary type="html"><![CDATA[We study the problem of robust mean estimation and introduce a novel Hamming
distance-based measure of distribution shift for coordinate-level corruptions.
We show that this measure yields adversary models that capture more realistic
corruptions than those used in prior works, and present an
information-theoretic analysis of robust mean estimation in these settings. We
show that for structured distributions, methods that leverage the structure
yield information theoretically more accurate mean estimation. We also focus on
practical algorithms for robust mean estimation and study when data
cleaning-inspired approaches that first fix corruptions in the input data and
then perform robust mean estimation can match the information theoretic bounds
of our analysis. We finally demonstrate experimentally that this two-step
approach outperforms structure-agnostic robust estimation and provides accurate
mean estimation even for high-magnitude corruption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zifan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jongho Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1"&gt;Theodoros Rekatsinas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1"&gt;Christos Tzamos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoupled Greedy Learning of CNNs for Synchronous and Asynchronous Distributed Learning. (arXiv:2106.06401v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06401</id>
        <link href="http://arxiv.org/abs/2106.06401"/>
        <updated>2021-06-14T01:38:54.371Z</updated>
        <summary type="html"><![CDATA[A commonly cited inefficiency of neural network training using
back-propagation is the update locking problem: each layer must wait for the
signal to propagate through the full network before updating. Several
alternatives that can alleviate this issue have been proposed. In this context,
we consider a simple alternative based on minimal feedback, which we call
Decoupled Greedy Learning (DGL). It is based on a classic greedy relaxation of
the joint training objective, recently shown to be effective in the context of
Convolutional Neural Networks (CNNs) on large-scale image classification. We
consider an optimization of this objective that permits us to decouple the
layer training, allowing for layers or modules in networks to be trained with a
potentially linear parallelization. With the use of a replay buffer we show
that this approach can be extended to asynchronous settings, where modules can
operate and continue to update with possibly large communication delays. To
address bandwidth and memory issues we propose an approach based on online
vector quantization. This allows to drastically reduce the communication
bandwidth between modules and required memory for replay buffers. We show
theoretically and empirically that this approach converges and compare it to
the sequential solvers. We demonstrate the effectiveness of DGL against
alternative approaches on the CIFAR-10 dataset and on the large-scale ImageNet
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1"&gt;Eugene Belilovsky&lt;/a&gt; (MILA), &lt;a href="http://arxiv.org/find/cs/1/au:+Leconte_L/0/1/0/all/0/1"&gt;Louis Leconte&lt;/a&gt; (MLIA, CMAP), &lt;a href="http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1"&gt;Lucas Caccia&lt;/a&gt; (MILA), &lt;a href="http://arxiv.org/find/cs/1/au:+Eickenberg_M/0/1/0/all/0/1"&gt;Michael Eickenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1"&gt;Edouard Oyallon&lt;/a&gt; (MLIA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring the sensitivity of Gaussian processes to kernel choice. (arXiv:2106.06510v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06510</id>
        <link href="http://arxiv.org/abs/2106.06510"/>
        <updated>2021-06-14T01:38:54.364Z</updated>
        <summary type="html"><![CDATA[Gaussian processes (GPs) are used to make medical and scientific decisions,
including in cardiac care and monitoring of carbon dioxide emissions. But the
choice of GP kernel is often somewhat arbitrary. In particular, uncountably
many kernels typically align with qualitative prior knowledge (e.g. function
smoothness or stationarity). But in practice, data analysts choose among a
handful of convenient standard kernels (e.g. squared exponential). In the
present work, we ask: Would decisions made with a GP differ under other,
qualitatively interchangeable kernels? We show how to formulate this
sensitivity analysis as a constrained optimization problem over a
finite-dimensional space. We can then use standard optimizers to identify
substantive changes in relevant decisions made with a GP. We demonstrate in
both synthetic and real-world examples that decisions made with a GP can
exhibit substantial sensitivity to kernel choice, even when prior draws are
qualitatively interchangeable to a user.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Stephenson_W/0/1/0/all/0/1"&gt;William T. Stephenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Soumya Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tin D. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yurochkin_M/0/1/0/all/0/1"&gt;Mikhail Yurochkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deshpande_S/0/1/0/all/0/1"&gt;Sameer K. Deshpande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Broderick_T/0/1/0/all/0/1"&gt;Tamara Broderick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime. (arXiv:2006.12297v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12297</id>
        <link href="http://arxiv.org/abs/2006.12297"/>
        <updated>2021-06-14T01:38:54.357Z</updated>
        <summary type="html"><![CDATA[We analyze the convergence of the averaged stochastic gradient descent for
overparameterized two-layer neural networks for regression problems. It was
recently found that a neural tangent kernel (NTK) plays an important role in
showing the global convergence of gradient-based methods under the NTK regime,
where the learning dynamics for overparameterized neural networks can be almost
characterized by that for the associated reproducing kernel Hilbert space
(RKHS). However, there is still room for a convergence rate analysis in the NTK
regime. In this study, we show that the averaged stochastic gradient descent
can achieve the minimax optimal convergence rate, with the global convergence
guarantee, by exploiting the complexities of the target function and the RKHS
associated with the NTK. Moreover, we show that the target function specified
by the NTK of a ReLU network can be learned at the optimal convergence rate
through a smooth approximation of a ReLU network under certain conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nitanda_A/0/1/0/all/0/1"&gt;Atsushi Nitanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Weakly Supervised Action Segmentation Using Mutual Consistency. (arXiv:1904.03116v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.03116</id>
        <link href="http://arxiv.org/abs/1904.03116"/>
        <updated>2021-06-14T01:38:54.351Z</updated>
        <summary type="html"><![CDATA[Action segmentation is the task of predicting the actions for each frame of a
video. As obtaining the full annotation of videos for action segmentation is
expensive, weakly supervised approaches that can learn only from transcripts
are appealing. In this paper, we propose a novel end-to-end approach for weakly
supervised action segmentation based on a two-branch neural network. The two
branches of our network predict two redundant but different representations for
action segmentation and we propose a novel mutual consistency (MuCon) loss that
enforces the consistency of the two redundant representations. Using the MuCon
loss together with a loss for transcript prediction, our proposed approach
achieves the accuracy of state-of-the-art approaches while being $14$ times
faster to train and $20$ times faster during inference. The MuCon loss proves
beneficial even in the fully supervised setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Souri_Y/0/1/0/all/0/1"&gt;Yaser Souri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1"&gt;Mohsen Fayyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minciullo_L/0/1/0/all/0/1"&gt;Luca Minciullo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1"&gt;Gianpiero Francesca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1"&gt;Juergen Gall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploration-Exploitation Motivated Variational Auto-Encoder for Recommender Systems. (arXiv:2006.03573v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03573</id>
        <link href="http://arxiv.org/abs/2006.03573"/>
        <updated>2021-06-14T01:38:54.335Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed rapid developments on collaborative filtering
techniques for improving the performance of recommender systems due to the
growing need of companies to help users discover new and relevant items.
However, the majority of existing literature focuses on delivering items which
match the user model learned from users' past preferences. A good
recommendation model is expected to recommend items that are known to enjoy and
items that are novel to try. In this work, we introduce an
exploitation-exploration motivated variational auto-encoder (XploVAE) to
collaborative filtering. To facilitate personalized recommendations, we
construct user-specific subgraphs, which contain the first-order proximity
capturing observed user-item interactions for exploitation and the high-order
proximity for exploration. A hierarchical latent space model is utilized to
learn the personalized item embedding for a given user, along with the
population distribution of all user subgraphs. Finally, experimental results on
various real-world datasets clearly demonstrate the effectiveness of our
proposed model on leveraging the exploitation and exploration recommendation
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yizi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_M/0/1/0/all/0/1"&gt;Meimei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Neural Hidden Markov Models with a Continuous latent state space. (arXiv:2106.06536v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06536</id>
        <link href="http://arxiv.org/abs/2106.06536"/>
        <updated>2021-06-14T01:38:54.329Z</updated>
        <summary type="html"><![CDATA[We introduce a new procedure to neuralize unsupervised Hidden Markov Models
in the continuous case. This provides higher flexibility to solve problems with
underlying latent variables. This approach is evaluated on both synthetic and
real data. On top of generating likely model parameters with comparable
performances to off-the-shelf neural architecture (LSTMs, GRUs,..), the
obtained results are easily interpretable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jarboui_F/0/1/0/all/0/1"&gt;Firas Jarboui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1"&gt;Vianney Perchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probability Paths and the Structure of Predictions over Time. (arXiv:2106.06515v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06515</id>
        <link href="http://arxiv.org/abs/2106.06515"/>
        <updated>2021-06-14T01:38:54.322Z</updated>
        <summary type="html"><![CDATA[In settings ranging from weather forecasts to political prognostications to
financial projections, probability estimates of future binary outcomes often
evolve over time. For example, the estimated likelihood of rain on a specific
day changes by the hour as new information becomes available. Given a
collection of such probability paths, we introduce a Bayesian framework --
which we call the Gaussian latent information martingale, or GLIM -- for
modeling the structure of dynamic predictions over time. Suppose, for example,
that the likelihood of rain in a week is 50%, and consider two hypothetical
scenarios. In the first, one expects the forecast is equally likely to become
either 25% or 75% tomorrow; in the second, one expects the forecast to stay
constant for the next several days. A time-sensitive decision-maker might
select a course of action immediately in the latter scenario, but may postpone
their decision in the former, knowing that new information is imminent. We
model these trajectories by assuming predictions update according to a latent
process of information flow, which is inferred from historical data. In
contrast to general methods for time series analysis, this approach preserves
the martingale structure of probability paths and better quantifies future
uncertainties around probability paths. We show that GLIM outperforms three
popular baseline methods, producing better estimated posterior probability path
distributions measured by three different metrics. By elucidating the dynamic
structure of predictions over time, we hope to help individuals make more
informed choices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhiyuan/0/1/0/all/0/1"&gt;Zhiyuan&lt;/a&gt; (Jerry)Lin, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_H/0/1/0/all/0/1"&gt;Hao Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1"&gt;Sharad Goel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An adaptive cognitive sensor node for ECG monitoring in the Internet of Medical Things. (arXiv:2106.06498v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06498</id>
        <link href="http://arxiv.org/abs/2106.06498"/>
        <updated>2021-06-14T01:38:54.306Z</updated>
        <summary type="html"><![CDATA[The Internet of Medical Things (IoMT) paradigm is becoming mainstream in
multiple clinical trials and healthcare procedures. It relies on novel very
accurate and compact sensing devices and communication infrastructures, opening
previously unmatched possibilities of implementing data collection and
continuous patient monitoring. Nevertheless, to fully exploit the potential of
this technology, some steps forwards are needed. First, the edge-computing
paradigm must be added to the picture. A certain level of near-sensor
processing has to be enabled, to improve the scalability, portability,
reliability, responsiveness of the IoMT nodes. Second, novel, increasingly
accurate, data analysis algorithms, such as those based on artificial
intelligence and Deep Learning, must be exploited. To reach these objectives,
designers, programmers of IoMT nodes, have to face challenging optimization
tasks, in order to execute fairly complex computing tasks on low-power wearable
and portable processing systems, with tight power and battery lifetime budgets.
In this work, we explore the implementation of cognitive data analysis
algorithm on resource-constrained computing platforms. To minimize power
consumption, we add an adaptivity layer that dynamically manages the hardware
and software configuration of the device to adapt it at runtime to the required
operating mode. We have assessed our approach on a use-case using a
convolutional neural network to classify electrocardiogram (ECG) traces on a
low-power microcontroller. Our experimental results show that adapting the node
setup to the workload at runtime can save up to 50% power consumption and a
quantized neural network reaches an accuracy value higher than 98% for
arrhythmia disorders detection on MIT-BIH Arrhythmia dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scrugli_M/0/1/0/all/0/1"&gt;Matteo Antonio Scrugli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loi_D/0/1/0/all/0/1"&gt;Daniela Loi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raffo_L/0/1/0/all/0/1"&gt;Luigi Raffo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meloni_P/0/1/0/all/0/1"&gt;Paolo Meloni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Segmentation Without Labels with Large-Scale Generative Models. (arXiv:2006.04988v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04988</id>
        <link href="http://arxiv.org/abs/2006.04988"/>
        <updated>2021-06-14T01:38:54.298Z</updated>
        <summary type="html"><![CDATA[The recent rise of unsupervised and self-supervised learning has dramatically
reduced the dependency on labeled data, providing effective image
representations for transfer to downstream vision tasks. Furthermore, recent
works employed these representations in a fully unsupervised setup for image
classification, reducing the need for human labels on the fine-tuning stage as
well. This work demonstrates that large-scale unsupervised models can also
perform a more challenging object segmentation task, requiring neither
pixel-level nor image-level labeling. Namely, we show that recent unsupervised
GANs allow to differentiate between foreground/background pixels, providing
high-quality saliency masks. By extensive comparison on standard benchmarks, we
outperform existing unsupervised alternatives for object segmentation,
achieving new state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1"&gt;Andrey Voynov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morozov_S/0/1/0/all/0/1"&gt;Stanislav Morozov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1"&gt;Artem Babenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recovery of Meteorites Using an Autonomous Drone and Machine Learning. (arXiv:2106.06523v1 [astro-ph.EP])]]></title>
        <id>http://arxiv.org/abs/2106.06523</id>
        <link href="http://arxiv.org/abs/2106.06523"/>
        <updated>2021-06-14T01:38:54.290Z</updated>
        <summary type="html"><![CDATA[The recovery of freshly fallen meteorites from tracked and triangulated
meteors is critical to determining their source asteroid families. However,
locating meteorite fragments in strewn fields remains a challenge with very few
meteorites being recovered from the meteors triangulated in past and ongoing
meteor camera networks. We examined if locating meteorites can be automated
using machine learning and an autonomous drone. Drones can be programmed to fly
a grid search pattern and take systematic pictures of the ground over a large
survey area. Those images can be analyzed using a machine learning classifier
to identify meteorites in the field among many other features. Here, we
describe a proof-of-concept meteorite classifier that deploys off-line a
combination of different convolution neural networks to recognize meteorites
from images taken by drones in the field. The system was implemented in a
conceptual drone setup and tested in the suspected strewn field of a recent
meteorite fall near Walker Lake, Nevada.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Citron_R/0/1/0/all/0/1"&gt;Robert I. Citron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Jenniskens_P/0/1/0/all/0/1"&gt;Peter Jenniskens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Watkins_C/0/1/0/all/0/1"&gt;Christopher Watkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sinha_S/0/1/0/all/0/1"&gt;Sravanthi Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Shah_A/0/1/0/all/0/1"&gt;Amar Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Raissi_C/0/1/0/all/0/1"&gt;Chedy Raissi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Devillepoix_H/0/1/0/all/0/1"&gt;Hadrien Devillepoix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Albers_J/0/1/0/all/0/1"&gt;Jim Albers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Probabilistic Model for Blind Source Separation via Legendre Transformation. (arXiv:1909.11294v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.11294</id>
        <link href="http://arxiv.org/abs/1909.11294"/>
        <updated>2021-06-14T01:38:54.282Z</updated>
        <summary type="html"><![CDATA[We present a novel blind source separation (BSS) method, called information
geometric blind source separation (IGBSS). Our formulation is based on the
log-linear model equipped with a hierarchically structured sample space, which
has theoretical guarantees to uniquely recover a set of source signals by
minimizing the KL divergence from a set of mixed signals. Source signals,
received signals, and mixing matrices are realized as different layers in our
hierarchical sample space. Our empirical results have demonstrated on images
and time series data that our approach is superior to well established
techniques and is able to separate signals with complex interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Luo_S/0/1/0/all/0/1"&gt;Simon Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Azizi_L/0/1/0/all/0/1"&gt;Lamiae Azizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Mahito Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Polyhedral Verification of Recurrent Neural Networks. (arXiv:2005.13300v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.13300</id>
        <link href="http://arxiv.org/abs/2005.13300"/>
        <updated>2021-06-14T01:38:54.275Z</updated>
        <summary type="html"><![CDATA[We present a scalable and precise verifier for recurrent neural networks,
called Prover based on two novel ideas: (i) a method to compute a set of
polyhedral abstractions for the non-convex and nonlinear recurrent update
functions by combining sampling, optimization, and Fermat's theorem, and (ii) a
gradient descent based algorithm for abstraction refinement guided by the
certification problem that combines multiple abstractions for each neuron.
Using Prover, we present the first study of certifying a non-trivial use case
of recurrent neural networks, namely speech classification. To achieve this, we
additionally develop custom abstractions for the non-linear speech
preprocessing pipeline. Our evaluation shows that Prover successfully verifies
several challenging recurrent models in computer vision, speech, and motion
sensor data classification beyond the reach of prior work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ryou_W/0/1/0/all/0/1"&gt;Wonryong Ryou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiayu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1"&gt;Mislav Balunovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1"&gt;Gagandeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dan_A/0/1/0/all/0/1"&gt;Andrei Dan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1"&gt;Martin Vechev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Gradient Bayesian Robust Optimization for Imitation Learning. (arXiv:2106.06499v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06499</id>
        <link href="http://arxiv.org/abs/2106.06499"/>
        <updated>2021-06-14T01:38:54.267Z</updated>
        <summary type="html"><![CDATA[The difficulty in specifying rewards for many real-world problems has led to
an increased focus on learning rewards from human feedback, such as
demonstrations. However, there are often many different reward functions that
explain the human feedback, leaving agents with uncertainty over what the true
reward function is. While most policy optimization approaches handle this
uncertainty by optimizing for expected performance, many applications demand
risk-averse behavior. We derive a novel policy gradient-style robust
optimization approach, PG-BROIL, that optimizes a soft-robust objective that
balances expected performance and risk. To the best of our knowledge, PG-BROIL
is the first policy optimization algorithm robust to a distribution of reward
hypotheses which can scale to continuous MDPs. Results suggest that PG-BROIL
can produce a family of behaviors ranging from risk-neutral to risk-averse and
outperforms state-of-the-art imitation learning algorithms when learning from
ambiguous demonstrations by hedging against uncertainty, rather than seeking to
uniquely identify the demonstrator's reward function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Javed_Z/0/1/0/all/0/1"&gt;Zaynah Javed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1"&gt;Daniel S. Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Satvik Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jerry Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1"&gt;Ashwin Balakrishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrik_M/0/1/0/all/0/1"&gt;Marek Petrik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1"&gt;Anca D. Dragan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Sparse Networks for Interpretable Predictions. (arXiv:2106.06468v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06468</id>
        <link href="http://arxiv.org/abs/2106.06468"/>
        <updated>2021-06-14T01:38:54.260Z</updated>
        <summary type="html"><![CDATA[Despite the enormous success of neural networks, they are still hard to
interpret and often overfit when applied to low-sample-size (LSS) datasets. To
tackle these obstacles, we propose a framework for training locally sparse
neural networks where the local sparsity is learned via a sample-specific
gating mechanism that identifies the subset of most relevant features for each
measurement. The sample-specific sparsity is predicted via a \textit{gating}
network, which is trained in tandem with the \textit{prediction} network. By
learning these subsets and weights of a prediction model, we obtain an
interpretable neural network that can handle LSS data and can remove nuisance
variables, which are irrelevant for the supervised learning task. Using both
synthetic and real-world datasets, we demonstrate that our method outperforms
state-of-the-art models when predicting the target function with far fewer
features per instance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Junchen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindenbaum_O/0/1/0/all/0/1"&gt;Ofir Lindenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kluger_Y/0/1/0/all/0/1"&gt;Yuval Kluger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invariant Information Bottleneck for Domain Generalization. (arXiv:2106.06333v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06333</id>
        <link href="http://arxiv.org/abs/2106.06333"/>
        <updated>2021-06-14T01:38:54.253Z</updated>
        <summary type="html"><![CDATA[The main challenge for domain generalization (DG) is to overcome the
potential distributional shift between multiple training domains and unseen
test domains. One popular class of DG algorithms aims to learn representations
that have an invariant causal relation across the training domains. However,
certain features, called \emph{pseudo-invariant features}, may be invariant in
the training domain but not the test domain and can substantially decreases the
performance of existing algorithms. To address this issue, we propose a novel
algorithm, called Invariant Information Bottleneck (IIB), that learns a
minimally sufficient representation that is invariant across training and
testing domains. By minimizing the mutual information between the
representation and inputs, IIB alleviates its reliance on pseudo-invariant
features, which is desirable for DG. To verify the effectiveness of the IIB
principle, we conduct extensive experiments on large-scale DG benchmarks. The
results show that IIB outperforms invariant learning baseline (e.g. IRM) by an
average of 2.8\% and 3.8\% accuracy over two evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yifei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yezhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wenzhen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1"&gt;Colorado J. Reed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1"&gt;Tong Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dictionary and prior learning with unrolled algorithms for unsupervised inverse problems. (arXiv:2106.06338v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06338</id>
        <link href="http://arxiv.org/abs/2106.06338"/>
        <updated>2021-06-14T01:38:54.244Z</updated>
        <summary type="html"><![CDATA[Inverse problems consist in recovering a signal given noisy observations. One
classical resolution approach is to leverage sparsity and integrate prior
knowledge of the signal to the reconstruction algorithm to get a plausible
solution. Still, this prior might not be sufficiently adapted to the data. In
this work, we study Dictionary and Prior learning from degraded measurements as
a bi-level problem, and we take advantage of unrolled algorithms to solve
approximate formulations of Synthesis and Analysis. We provide an empirical and
theoretical analysis of automatic differentiation for Dictionary Learning to
understand better the pros and cons of unrolling in this context. We find that
unrolled algorithms speed up the recovery process for a small number of
iterations by improving the gradient estimation. Then we compare Analysis and
Synthesis by evaluating the performance of unrolled algorithms for inverse
problems, without access to any ground truth data for several classes of
dictionaries and priors. While Analysis can achieve good results,Synthesis is
more robust and performs better. Finally, we illustrate our method on pattern
and structure learning tasks from degraded measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malezieux_B/0/1/0/all/0/1"&gt;Beno&amp;#xee;t Mal&amp;#xe9;zieux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1"&gt;Thomas Moreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowalski_M/0/1/0/all/0/1"&gt;Matthieu Kowalski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Herded Gibbs Sampling. (arXiv:2106.06430v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06430</id>
        <link href="http://arxiv.org/abs/2106.06430"/>
        <updated>2021-06-14T01:38:54.214Z</updated>
        <summary type="html"><![CDATA[Herding is a technique to sequentially generate deterministic samples from a
probability distribution. In this work, we propose a continuous herded Gibbs
sampler, that combines kernel herding on continuous densities with Gibbs
sampling. Our algorithm allows for deterministically sampling from
high-dimensional multivariate probability densities, without directly sampling
from the joint density. Experiments with Gaussian mixture densities indicate
that the L2 error decreases similarly to kernel herding, while the computation
time is significantly lower, i.e., linear in the number of dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Laura M. Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Baum_M/0/1/0/all/0/1"&gt;Marcus Baum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Detection of Trojaned Neural Networks. (arXiv:2106.06469v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06469</id>
        <link href="http://arxiv.org/abs/2106.06469"/>
        <updated>2021-06-14T01:38:54.194Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are known to have security issues. One particular threat
is the Trojan attack. It occurs when the attackers stealthily manipulate the
model's behavior through Trojaned training samples, which can later be
exploited.

Guided by basic neuroscientific principles we discover subtle -- yet critical
-- structural deviation characterizing Trojaned models. In our analysis we use
topological tools. They allow us to model high-order dependencies in the
networks, robustly compare different networks, and localize structural
abnormalities. One interesting observation is that Trojaned models develop
short-cuts from input to output layers.

Inspired by these observations, we devise a strategy for robust detection of
Trojaned models. Compared to standard baselines it displays better performance
on multiple benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Songzhu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yikai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_H/0/1/0/all/0/1"&gt;Hubert Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goswami_M/0/1/0/all/0/1"&gt;Mayank Goswami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Driven Adaptive Prior. (arXiv:2106.06406v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06406</id>
        <link href="http://arxiv.org/abs/2106.06406"/>
        <updated>2021-06-14T01:38:54.165Z</updated>
        <summary type="html"><![CDATA[Denoising diffusion probabilistic models have been recently proposed to
generate high-quality samples by estimating the gradient of the data density.
The framework assumes the prior noise as a standard Gaussian distribution,
whereas the corresponding data distribution may be more complicated than the
standard Gaussian distribution, which potentially introduces inefficiency in
denoising the prior noise into the data sample because of the discrepancy
between the data and the prior. In this paper, we propose PriorGrad to improve
the efficiency of the conditional diffusion model (for example, a vocoder using
a mel-spectrogram as the condition) by applying an adaptive prior derived from
the data statistics based on the conditional information. We formulate the
training and sampling procedures of PriorGrad and demonstrate the advantages of
an adaptive prior through a theoretical analysis. Focusing on the audio domain,
we consider the recently proposed diffusion-based audio generative models based
on both the spectral and time domains and show that PriorGrad achieves a faster
convergence leading to data and parameter efficiency and improved quality, and
thereby demonstrating the efficiency of a data-driven adaptive prior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sang-gil Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kim_H/0/1/0/all/0/1"&gt;Heeseung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shin_C/0/1/0/all/0/1"&gt;Chaehun Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Qi Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Reinforcement Learning as Anti-Exploration. (arXiv:2106.06431v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06431</id>
        <link href="http://arxiv.org/abs/2106.06431"/>
        <updated>2021-06-14T01:38:54.158Z</updated>
        <summary type="html"><![CDATA[Offline Reinforcement Learning (RL) aims at learning an optimal control from
a fixed dataset, without interactions with the system. An agent in this setting
should avoid selecting actions whose consequences cannot be predicted from the
data. This is the converse of exploration in RL, which favors such actions. We
thus take inspiration from the literature on bonus-based exploration to design
a new offline RL agent. The core idea is to subtract a prediction-based
exploration bonus from the reward, instead of adding it for exploration. This
allows the policy to stay close to the support of the dataset. We connect this
approach to a more common regularization of the learned policy towards the
data. Instantiated with a bonus based on the prediction error of a variational
autoencoder, we show that our agent is competitive with the state of the art on
a set of continuous control locomotion and manipulation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rezaeifar_S/0/1/0/all/0/1"&gt;Shideh Rezaeifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dadashi_R/0/1/0/all/0/1"&gt;Robert Dadashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vieillard_N/0/1/0/all/0/1"&gt;Nino Vieillard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussenot_L/0/1/0/all/0/1"&gt;L&amp;#xe9;onard Hussenot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1"&gt;Olivier Bachem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1"&gt;Olivier Pietquin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1"&gt;Matthieu Geist&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JKOnet: Proximal Optimal Transport Modeling of Population Dynamics. (arXiv:2106.06345v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06345</id>
        <link href="http://arxiv.org/abs/2106.06345"/>
        <updated>2021-06-14T01:38:54.151Z</updated>
        <summary type="html"><![CDATA[Consider a heterogeneous population of points evolving with time. While the
population evolves, both in size and nature, we can observe it periodically,
through snapshots taken at different timestamps. Each of these snapshots is
formed by sampling points from the population at that time, and then creating
features to recover point clouds. While these snapshots describe the
population's evolution on aggregate, they do not provide directly insights on
individual trajectories. This scenario is encountered in several applications,
notably single-cell genomics experiments, tracking of particles, or when
studying crowd motion. In this paper, we propose to model that dynamic as
resulting from the celebrated Jordan-Kinderlehrer-Otto (JKO) proximal scheme.
The JKO scheme posits that the configuration taken by a population at time $t$
is one that trades off a decrease w.r.t. an energy (the model we seek to learn)
penalized by an optimal transport distance w.r.t. the previous configuration.
To that end, we propose JKOnet, a neural architecture that combines an energy
model on measures, with (small) optimal displacements solved with input convex
neural networks (ICNN). We demonstrate the applicability of our model to
explain and predict population dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bunne_C/0/1/0/all/0/1"&gt;Charlotte Bunne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Papaxanthos_L/0/1/0/all/0/1"&gt;Laetitia Meng-Papaxanthos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1"&gt;Marco Cuturi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding Generalization via Decomposing Excess Risk Dynamics. (arXiv:2106.06153v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06153</id>
        <link href="http://arxiv.org/abs/2106.06153"/>
        <updated>2021-06-14T01:38:54.144Z</updated>
        <summary type="html"><![CDATA[Generalization is one of the critical issues in machine learning. However,
traditional methods like uniform convergence are not powerful enough to fully
explain generalization because they may yield vacuous bounds even in
overparameterized linear regression regimes. An alternative solution is to
analyze the generalization dynamics to derive algorithm-dependent bounds, e.g.,
stability. Unfortunately, the stability-based bound is still far from
explaining the remarkable generalization ability of neural networks due to the
coarse-grained analysis of the signal and noise. Inspired by the observation
that neural networks show a slow convergence rate when fitting noise, we
propose decomposing the excess risk dynamics and applying stability-based bound
only on the variance part (which measures how the model performs on pure
noise). We provide two applications for the framework, including a linear case
(overparameterized linear regression with gradient descent) and a non-linear
case (matrix recovery with gradient flow). Under the decomposition framework,
the new bound accords better with the theoretical and empirical evidence
compared to the stability-based bound and uniform convergence bound.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teng_J/0/1/0/all/0/1"&gt;Jiaye Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianhao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yang Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preferential Temporal Difference Learning. (arXiv:2106.06508v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06508</id>
        <link href="http://arxiv.org/abs/2106.06508"/>
        <updated>2021-06-14T01:38:54.137Z</updated>
        <summary type="html"><![CDATA[Temporal-Difference (TD) learning is a general and very useful tool for
estimating the value function of a given policy, which in turn is required to
find good policies. Generally speaking, TD learning updates states whenever
they are visited. When the agent lands in a state, its value can be used to
compute the TD-error, which is then propagated to other states. However, it may
be interesting, when computing updates, to take into account other information
than whether a state is visited or not. For example, some states might be more
important than others (such as states which are frequently seen in a successful
trajectory). Or, some states might have unreliable value estimates (for
example, due to partial observability or lack of data), making their values
less desirable as targets. We propose an approach to re-weighting states used
in TD updates, both when they are the input and when they provide the target
for the update. We prove that our approach converges with linear function
approximation and illustrate its desirable empirical behaviour compared to
other TD-style methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anand_N/0/1/0/all/0/1"&gt;Nishanth Anand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label Noise SGD Provably Prefers Flat Global Minimizers. (arXiv:2106.06530v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06530</id>
        <link href="http://arxiv.org/abs/2106.06530"/>
        <updated>2021-06-14T01:38:54.117Z</updated>
        <summary type="html"><![CDATA[In overparametrized models, the noise in stochastic gradient descent (SGD)
implicitly regularizes the optimization trajectory and determines which local
minimum SGD converges to. Motivated by empirical studies that demonstrate that
training with noisy labels improves generalization, we study the implicit
regularization effect of SGD with label noise. We show that SGD with label
noise converges to a stationary point of a regularized loss $L(\theta) +\lambda
R(\theta)$, where $L(\theta)$ is the training loss, $\lambda$ is an effective
regularization parameter depending on the step size, strength of the label
noise, and the batch size, and $R(\theta)$ is an explicit regularizer that
penalizes sharp minimizers. Our analysis uncovers an additional regularization
effect of large learning rates beyond the linear scaling rule that penalizes
large eigenvalues of the Hessian more than small ones. We also prove extensions
to classification with general loss functions, SGD with momentum, and SGD with
general noise covariance, significantly strengthening the prior work of Blanc
et al. to global convergence and large learning rates and of HaoChen et al. to
general models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Damian_A/0/1/0/all/0/1"&gt;Alex Damian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Survey of Image Based Graph Neural Networks. (arXiv:2106.06307v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06307</id>
        <link href="http://arxiv.org/abs/2106.06307"/>
        <updated>2021-06-14T01:38:54.108Z</updated>
        <summary type="html"><![CDATA[In this survey paper, we analyze image based graph neural networks and
propose a three-step classification approach. We first convert the image into
superpixels using the Quickshift algorithm so as to reduce 30% of the input
data. The superpixels are subsequently used to generate a region adjacency
graph. Finally, the graph is passed through a state-of-art graph convolutional
neural network to get classification scores. We also analyze the spatial and
spectral convolution filtering techniques in graph neural networks.
Spectral-based models perform better than spatial-based models and classical
CNN with lesser compute cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nazir_U/0/1/0/all/0/1"&gt;Usman Nazir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;He Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taj_M/0/1/0/all/0/1"&gt;Murtaza Taj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Can Knowledge Bring to Machine Learning? -- A Survey of Low-shot Learning for Structured Data. (arXiv:2106.06410v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06410</id>
        <link href="http://arxiv.org/abs/2106.06410"/>
        <updated>2021-06-14T01:38:54.102Z</updated>
        <summary type="html"><![CDATA[Supervised machine learning has several drawbacks that make it difficult to
use in many situations. Drawbacks include: heavy reliance on massive training
data, limited generalizability and poor expressiveness of high-level semantics.
Low-shot Learning attempts to address these drawbacks. Low-shot learning allows
the model to obtain good predictive power with very little or no training data,
where structured knowledge plays a key role as a high-level semantic
representation of human. This article will review the fundamental factors of
low-shot learning technologies, with a focus on the operation of structured
knowledge under different low-shot conditions. We also introduce other
techniques relevant to low-shot learning. Finally, we point out the limitations
of low-shot learning, the prospects and gaps of industrial applications, and
future research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chapman_A/0/1/0/all/0/1"&gt;Adriane Chapman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_G/0/1/0/all/0/1"&gt;Guihua Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hall_D/0/1/0/all/0/1"&gt;Dame Wendy Hall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonmyopic Multifidelity Active Search. (arXiv:2106.06356v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06356</id>
        <link href="http://arxiv.org/abs/2106.06356"/>
        <updated>2021-06-14T01:38:54.095Z</updated>
        <summary type="html"><![CDATA[Active search is a learning paradigm where we seek to identify as many
members of a rare, valuable class as possible given a labeling budget. Previous
work on active search has assumed access to a faithful (and expensive) oracle
reporting experimental results. However, some settings offer access to cheaper
surrogates such as computational simulation that may aid in the search. We
propose a model of multifidelity active search, as well as a novel,
computationally efficient policy for this setting that is motivated by
state-of-the-art classical policies. Our policy is nonmyopic and budget aware,
allowing for a dynamic tradeoff between exploration and exploitation. We
evaluate the performance of our solution on real-world datasets and demonstrate
significantly better performance than natural benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1"&gt;Quan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modiri_A/0/1/0/all/0/1"&gt;Arghavan Modiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garnett_R/0/1/0/all/0/1"&gt;Roman Garnett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Order Matters: Probabilistic Modeling of Node Sequence for Graph Generation. (arXiv:2106.06189v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06189</id>
        <link href="http://arxiv.org/abs/2106.06189"/>
        <updated>2021-06-14T01:38:54.088Z</updated>
        <summary type="html"><![CDATA[A graph generative model defines a distribution over graphs. One type of
generative model is constructed by autoregressive neural networks, which
sequentially add nodes and edges to generate a graph. However, the likelihood
of a graph under the autoregressive model is intractable, as there are numerous
sequences leading to the given graph; this makes maximum likelihood estimation
challenging. Instead, in this work we derive the exact joint probability over
the graph and the node ordering of the sequential process. From the joint, we
approximately marginalize out the node orderings and compute a lower bound on
the log-likelihood using variational inference. We train graph generative
models by maximizing this bound, without using the ad-hoc node orderings of
previous methods. Our experiments show that the log-likelihood bound is
significantly tighter than the bound of previous schemes. Moreover, the models
fitted with the proposed algorithm can generate high-quality graphs that match
the structures of target graphs not seen during training. We have made our code
publicly available at
\hyperref[https://github.com/tufts-ml/graph-generation-vi]{https://github.com/tufts-ml/graph-generation-vi}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaohui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jiajing Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ruiz_F/0/1/0/all/0/1"&gt;Francisco J. R. Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liping Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Record Similarity for Practical Vertical Federated Learning. (arXiv:2106.06312v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06312</id>
        <link href="http://arxiv.org/abs/2106.06312"/>
        <updated>2021-06-14T01:38:54.068Z</updated>
        <summary type="html"><![CDATA[As the privacy of machine learning has drawn increasing attention, federated
learning is introduced to enable collaborative learning without revealing raw
data. Notably, \textit{vertical federated learning} (VFL), where parties share
the same set of samples but only hold partial features, has a wide range of
real-world applications. However, existing studies in VFL rarely study the
``record linkage'' process. They either design algorithms assuming the data
from different parties have been linked or use simple linkage methods like
exact-linkage or top1-linkage. These approaches are unsuitable for many
applications, such as the GPS location and noisy titles requiring fuzzy
matching. In this paper, we design a novel similarity-based VFL framework,
FedSim, which is suitable for more real-world applications and achieves higher
performance on traditional VFL tasks. Moreover, we theoretically analyze the
privacy risk caused by sharing similarities. Our experiments on three synthetic
datasets and five real-world datasets with various similarity metrics show that
FedSim consistently outperforms other state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhaomin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qinbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1"&gt;Bingsheng He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keyframe-Focused Visual Imitation Learning. (arXiv:2106.06452v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06452</id>
        <link href="http://arxiv.org/abs/2106.06452"/>
        <updated>2021-06-14T01:38:54.061Z</updated>
        <summary type="html"><![CDATA[Imitation learning trains control policies by mimicking pre-recorded expert
demonstrations. In partially observable settings, imitation policies must rely
on observation histories, but many seemingly paradoxical results show better
performance for policies that only access the most recent observation. Recent
solutions ranging from causal graph learning to deep information bottlenecks
have shown promising results, but failed to scale to realistic settings such as
visual imitation. We propose a solution that outperforms these prior approaches
by upweighting demonstration keyframes corresponding to expert action
changepoints. This simple approach easily scales to complex visual imitation
settings. Our experimental results demonstrate consistent performance
improvements over all baselines on image-based Gym MuJoCo continuous control
tasks. Finally, on the CARLA photorealistic vision-based urban driving
simulator, we resolve a long-standing issue in behavioral cloning for driving
by demonstrating effective imitation from observation histories. Supplementary
materials and code at: \url{https://tinyurl.com/imitation-keyframes}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1"&gt;Chuan Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jierui Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jianing Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1"&gt;Dinesh Jayaraman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Within-layer Diversity Reduces Generalization Gap. (arXiv:2106.06012v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06012</id>
        <link href="http://arxiv.org/abs/2106.06012"/>
        <updated>2021-06-14T01:38:54.054Z</updated>
        <summary type="html"><![CDATA[Neural networks are composed of multiple layers arranged in a hierarchical
structure jointly trained with a gradient-based optimization, where the errors
are back-propagated from the last layer back to the first one. At each
optimization step, neurons at a given layer receive feedback from neurons
belonging to higher layers of the hierarchy. In this paper, we propose to
complement this traditional 'between-layer' feedback with additional
'within-layer' feedback to encourage diversity of the activations within the
same layer. To this end, we measure the pairwise similarity between the outputs
of the neurons and use it to model the layer's overall diversity. By penalizing
similarities and promoting diversity, we encourage each neuron to learn a
distinctive representation and, thus, to enrich the data representation learned
within the layer and to increase the total capacity of the model. We
theoretically study how the within-layer activation diversity affects the
generalization performance of a neural network and prove that increasing the
diversity of hidden activations reduces the estimation error. In addition to
the theoretical guarantees, we present an empirical study on three datasets
confirming that the proposed approach enhances the performance of
state-of-the-art neural network models and decreases the generalization gap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laakom_F/0/1/0/all/0/1"&gt;Firas Laakom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1"&gt;Jenni Raitoharju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1"&gt;Moncef Gabbouj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Language Models for Continuously Evolving Content. (arXiv:2106.06297v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06297</id>
        <link href="http://arxiv.org/abs/2106.06297"/>
        <updated>2021-06-14T01:38:54.047Z</updated>
        <summary type="html"><![CDATA[The content on the web is in a constant state of flux. New entities, issues,
and ideas continuously emerge, while the semantics of the existing conversation
topics gradually shift. In recent years, pre-trained language models like BERT
greatly improved the state-of-the-art for a large spectrum of content
understanding tasks. Therefore, in this paper, we aim to study how these
language models can be adapted to better handle continuously evolving web
content. In our study, we first analyze the evolution of 2013 - 2019 Twitter
data, and unequivocally confirm that a BERT model trained on past tweets would
heavily deteriorate when directly applied to data from later years. Then, we
investigate two possible sources of the deterioration: the semantic shift of
existing tokens and the sub-optimal or failed understanding of new tokens. To
this end, we both explore two different vocabulary composition methods, as well
as propose three sampling methods which help in efficient incremental training
for BERT-like models. Compared to a new model trained from scratch offline, our
incremental training (a) reduces the training costs, (b) achieves better
performance on evolving content, and (c) is suitable for online deployment. The
superiority of our methods is validated using two downstream tasks. We
demonstrate significant improvements when incrementally evolving the model from
a particular base year, on the task of Country Hashtag Prediction, as well as
on the OffensEval 2019 task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hombaiah_S/0/1/0/all/0/1"&gt;Spurthi Amba Hombaiah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mingyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1"&gt;Michael Bendersky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1"&gt;Marc Najork&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViT-Inception-GAN for Image Colourising. (arXiv:2106.06321v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06321</id>
        <link href="http://arxiv.org/abs/2106.06321"/>
        <updated>2021-06-14T01:38:54.040Z</updated>
        <summary type="html"><![CDATA[Studies involving colourising images has been garnering researchers' keen
attention over time, assisted by significant advances in various Machine
Learning techniques and compute power availability. Traditionally, colourising
images have been an intricate task that gave a substantial degree of freedom
during the assignment of chromatic information. In our proposed method, we
attempt to colourise images using Vision Transformer - Inception - Generative
Adversarial Network (ViT-I-GAN), which has an Inception-v3 fusion embedding in
the generator. For a stable and robust network, we have used Vision Transformer
(ViT) as the discriminator. We trained the model on the Unsplash and the COCO
dataset for demonstrating the improvement made by the Inception-v3 embedding.
We have compared the results between ViT-GANs with and without Inception-v3
embedding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bana_T/0/1/0/all/0/1"&gt;Tejas Bana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loya_J/0/1/0/all/0/1"&gt;Jatan Loya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1"&gt;Siddhant Kulkarni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Transformer Networks: Learning Meta-path Graphs to Improve GNNs. (arXiv:2106.06218v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06218</id>
        <link href="http://arxiv.org/abs/2106.06218"/>
        <updated>2021-06-14T01:38:54.033Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have been widely applied to various fields due
to their powerful representations of graph-structured data. Despite the success
of GNNs, most existing GNNs are designed to learn node representations on the
fixed and homogeneous graphs. The limitations especially become problematic
when learning representations on a misspecified graph or a heterogeneous graph
that consists of various types of nodes and edges. To address this limitations,
we propose Graph Transformer Networks (GTNs) that are capable of generating new
graph structures, which preclude noisy connections and include useful
connections (e.g., meta-paths) for tasks, while learning effective node
representations on the new graphs in an end-to-end fashion. We further propose
enhanced version of GTNs, Fast Graph Transformer Networks (FastGTNs), that
improve scalability of graph transformations. Compared to GTNs, FastGTNs are
230x faster and use 100x less memory while allowing the identical graph
transformations as GTNs. In addition, we extend graph transformations to the
semantic proximity of nodes allowing non-local operations beyond meta-paths.
Extensive experiments on both homogeneous graphs and heterogeneous graphs show
that GTNs and FastGTNs with non-local operations achieve the state-of-the-art
performance for node classification tasks. The code is available:
https://github.com/seongjunyun/Graph_Transformer_Networks]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Seongjun Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1"&gt;Minbyul Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1"&gt;Sungdong Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seunghun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1"&gt;Sean S. Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_R/0/1/0/all/0/1"&gt;Raehyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1"&gt;Jaewoo Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunwoo J. Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[States of confusion: Eye and Head tracking reveal surgeons' confusion during arthroscopic surgery. (arXiv:2106.06261v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.06261</id>
        <link href="http://arxiv.org/abs/2106.06261"/>
        <updated>2021-06-14T01:38:54.010Z</updated>
        <summary type="html"><![CDATA[During arthroscopic surgeries, surgeons are faced with challenges like
cognitive re-projection of the 2D screen output into the 3D operating site or
navigation through highly similar tissue. Training of these cognitive processes
takes much time and effort for young surgeons, but is necessary and crucial for
their education. In this study we want to show how to recognize states of
confusion of young surgeons during an arthroscopic surgery, by looking at their
eye and head movements and feeding them to a machine learning model. With an
accuracy of over 94\% and detection speed of 0.039 seconds, our model is a step
towards online diagnostic and training systems for the perceptual-cognitive
processes of surgeons during arthroscopic surgeries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hosp_B/0/1/0/all/0/1"&gt;Benedikt Hosp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1"&gt;Myat Su Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haddawy_p/0/1/0/all/0/1"&gt;peter Haddawy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watcharporas_R/0/1/0/all/0/1"&gt;Ratthapoom Watcharporas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_ngasoonsong_p/0/1/0/all/0/1"&gt;paphon Sa-ngasoonsong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1"&gt;Enkelejda Kasneci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Conditional Gaussian Mixture Model for Constrained Clustering. (arXiv:2106.06385v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06385</id>
        <link href="http://arxiv.org/abs/2106.06385"/>
        <updated>2021-06-14T01:38:54.003Z</updated>
        <summary type="html"><![CDATA[Constrained clustering has gained significant attention in the field of
machine learning as it can leverage prior information on a growing amount of
only partially labeled data. Following recent advances in deep generative
models, we propose a novel framework for constrained clustering that is
intuitive, interpretable, and can be trained efficiently in the framework of
stochastic gradient variational inference. By explicitly integrating domain
knowledge in the form of probabilistic relations, our proposed model (DC-GMM)
uncovers the underlying distribution of data conditioned on prior clustering
preferences, expressed as pairwise constraints. These constraints guide the
clustering process towards a desirable partition of the data by indicating
which samples should or should not belong to the same cluster. We provide
extensive experiments to demonstrate that DC-GMM shows superior clustering
performances and robustness compared to state-of-the-art deep constrained
clustering methods on a wide range of data sets. We further demonstrate the
usefulness of our approach on two challenging real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manduchi_L/0/1/0/all/0/1"&gt;Laura Manduchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chin_Cheong_K/0/1/0/all/0/1"&gt;Kieran Chin-Cheong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michel_H/0/1/0/all/0/1"&gt;Holger Michel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wellmann_S/0/1/0/all/0/1"&gt;Sven Wellmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1"&gt;Julia E. Vogt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HPO-B: A Large-Scale Reproducible Benchmark for Black-Box HPO based on OpenML. (arXiv:2106.06257v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06257</id>
        <link href="http://arxiv.org/abs/2106.06257"/>
        <updated>2021-06-14T01:38:53.995Z</updated>
        <summary type="html"><![CDATA[Hyperparameter optimization (HPO) is a core problem for the machine learning
community and remains largely unsolved due to the significant computational
resources required to evaluate hyperparameter configurations. As a result, a
series of recent related works have focused on the direction of transfer
learning for quickly fine-tuning hyperparameters on a dataset. Unfortunately,
the community does not have a common large-scale benchmark for comparing HPO
algorithms. Instead, the de facto practice consists of empirical protocols on
arbitrary small-scale meta-datasets that vary inconsistently across
publications, making reproducibility a challenge. To resolve this major
bottleneck and enable a fair and fast comparison of black-box HPO methods on a
level playing field, we propose HPO-B, a new large-scale benchmark in the form
of a collection of meta-datasets. Our benchmark is assembled and preprocessed
from the OpenML repository and consists of 176 search spaces (algorithms)
evaluated sparsely on 196 datasets with a total of 6.4 million hyperparameter
evaluations. For ensuring reproducibility on our benchmark, we detail explicit
experimental protocols, splits, and evaluation measures for comparing methods
for both non-transfer, as well as, transfer learning HPO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arango_S/0/1/0/all/0/1"&gt;Sebastian Pineda Arango&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jomaa_H/0/1/0/all/0/1"&gt;Hadi S. Jomaa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wistuba_M/0/1/0/all/0/1"&gt;Martin Wistuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grabocka_J/0/1/0/all/0/1"&gt;Josif Grabocka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Learnability via Gradient Method for Two-Layer ReLU Neural Networks in Teacher-Student Setting. (arXiv:2106.06251v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06251</id>
        <link href="http://arxiv.org/abs/2106.06251"/>
        <updated>2021-06-14T01:38:53.985Z</updated>
        <summary type="html"><![CDATA[Deep learning empirically achieves high performance in many applications, but
its training dynamics has not been fully understood theoretically. In this
paper, we explore theoretical analysis on training two-layer ReLU neural
networks in a teacher-student regression model, in which a student network
learns an unknown teacher network through its outputs. We show that with a
specific regularization and sufficient over-parameterization, the student
network can identify the parameters of the teacher network with high
probability via gradient descent with a norm dependent stepsize even though the
objective function is highly non-convex. The key theoretical tool is the
measure representation of the neural networks and a novel application of a dual
certificate argument for sparse estimation on a measure space. We analyze the
global minima and global convergence property in the measure space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Akiyama_S/0/1/0/all/0/1"&gt;Shunta Akiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Enhanced Machine Learning Pipeline against Diverse Adversarial Attacks. (arXiv:2106.06235v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06235</id>
        <link href="http://arxiv.org/abs/2106.06235"/>
        <updated>2021-06-14T01:38:53.979Z</updated>
        <summary type="html"><![CDATA[Despite the great successes achieved by deep neural networks (DNNs), recent
studies show that they are vulnerable against adversarial examples, which aim
to mislead DNNs by adding small adversarial perturbations. Several defenses
have been proposed against such attacks, while many of them have been
adaptively attacked. In this work, we aim to enhance the ML robustness from a
different perspective by leveraging domain knowledge: We propose a Knowledge
Enhanced Machine Learning Pipeline (KEMLP) to integrate domain knowledge (i.e.,
logic relationships among different predictions) into a probabilistic graphical
model via first-order logic rules. In particular, we develop KEMLP by
integrating a diverse set of weak auxiliary models based on their logical
relationships to the main DNN model that performs the target task.
Theoretically, we provide convergence results and prove that, under mild
conditions, the prediction of KEMLP is more robust than that of the main DNN
model. Empirically, we take road sign recognition as an example and leverage
the relationships between road signs and their shapes and contents as domain
knowledge. We show that compared with adversarial training and other baselines,
KEMLP achieves higher robustness against physical attacks, $\mathcal{L}_p$
bounded attacks, unforeseen attacks, and natural corruptions under both
whitebox and blackbox settings, while still maintaining high clean accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gurel_N/0/1/0/all/0/1"&gt;Nezihe Merve G&amp;#xfc;rel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1"&gt;Xiangyu Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rimanic_L/0/1/0/all/0/1"&gt;Luka Rimanic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nested and Balanced Entity Recognition using Multi-Task Learning. (arXiv:2106.06216v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06216</id>
        <link href="http://arxiv.org/abs/2106.06216"/>
        <updated>2021-06-14T01:38:53.958Z</updated>
        <summary type="html"><![CDATA[Entity Recognition (ER) within a text is a fundamental exercise in Natural
Language Processing, enabling further depending tasks such as Knowledge
Extraction, Text Summarisation, or Keyphrase Extraction. An entity consists of
single words or of a consecutive sequence of terms, constituting the basic
building blocks for communication. Mainstream ER approaches are mainly limited
to flat structures, concentrating on the outermost entities while ignoring the
inner ones. This paper introduces a partly-layered network architecture that
deals with the complexity of overlapping and nested cases. The proposed
architecture consists of two parts: (1) a shared Sequence Layer and (2) a
stacked component with multiple Tagging Layers. The adoption of such an
architecture has the advantage of preventing overfit to a specific word-length,
thus maintaining performance for longer entities despite their lower frequency.
To verify the proposed architecture's effectiveness, we train and evaluate this
architecture to recognise two kinds of entities - Concepts (CR) and Named
Entities (NER). Our approach achieves state-of-the-art NER performances, while
it outperforms previous CR approaches. Considering these promising results, we
see the possibility to evolve the architecture for other cases such as the
extraction of events or the detection of argumentative components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Waldis_A/0/1/0/all/0/1"&gt;Andreas Waldis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazzola_L/0/1/0/all/0/1"&gt;Luca Mazzola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Complexity of Sparse Tensor PCA. (arXiv:2106.06308v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06308</id>
        <link href="http://arxiv.org/abs/2106.06308"/>
        <updated>2021-06-14T01:38:53.951Z</updated>
        <summary type="html"><![CDATA[We study the problem of sparse tensor principal component analysis: given a
tensor $\pmb Y = \pmb W + \lambda x^{\otimes p}$ with $\pmb W \in
\otimes^p\mathbb{R}^n$ having i.i.d. Gaussian entries, the goal is to recover
the $k$-sparse unit vector $x \in \mathbb{R}^n$. The model captures both sparse
PCA (in its Wigner form) and tensor PCA.

For the highly sparse regime of $k \leq \sqrt{n}$, we present a family of
algorithms that smoothly interpolates between a simple polynomial-time
algorithm and the exponential-time exhaustive search algorithm. For any $1 \leq
t \leq k$, our algorithms recovers the sparse vector for signal-to-noise ratio
$\lambda \geq \tilde{\mathcal{O}} (\sqrt{t} \cdot (k/t)^{p/2})$ in time
$\tilde{\mathcal{O}}(n^{p+t})$, capturing the state-of-the-art guarantees for
the matrix settings (in both the polynomial-time and sub-exponential time
regimes).

Our results naturally extend to the case of $r$ distinct $k$-sparse signals
with disjoint supports, with guarantees that are independent of the number of
spikes. Even in the restricted case of sparse PCA, known algorithms only
recover the sparse vectors for $\lambda \geq \tilde{\mathcal{O}}(k \cdot r)$
while our algorithms require $\lambda \geq \tilde{\mathcal{O}}(k)$.

Finally, by analyzing the low-degree likelihood ratio, we complement these
algorithmic results with rigorous evidence illustrating the trade-offs between
signal-to-noise ratio and running time. This lower bound captures the known
lower bounds for both sparse PCA and tensor PCA. In this general model, we
observe a more intricate three-way trade-off between the number of samples $n$,
the sparsity $k$, and the tensor power $p$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choo_D/0/1/0/all/0/1"&gt;Davin Choo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+dOrsi_T/0/1/0/all/0/1"&gt;Tommaso d&amp;#x27;Orsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KRADA: Known-region-aware Domain Alignment for Open World Semantic Segmentation. (arXiv:2106.06237v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.06237</id>
        <link href="http://arxiv.org/abs/2106.06237"/>
        <updated>2021-06-14T01:38:53.944Z</updated>
        <summary type="html"><![CDATA[In semantic segmentation, we aim to train a pixel-level classifier to assign
category labels to all pixels in an image, where labeled training images and
unlabeled test images are from the same distribution and share the same label
set. However, in an open world, the unlabeled test images probably contain
unknown categories and have different distributions from the labeled images.
Hence, in this paper, we consider a new, more realistic, and more challenging
problem setting where the pixel-level classifier has to be trained with labeled
images and unlabeled open-world images -- we name it open world semantic
segmentation (OSS). In OSS, the trained classifier is expected to identify
unknown-class pixels and classify known-class pixels well. To solve OSS, we
first investigate which distribution that unknown-class pixels obey. Then,
motivated by the goodness-of-fit test, we use statistical measurements to show
how a pixel fits the distribution of an unknown class and select highly-fitted
pixels to form the unknown region in each image. Eventually, we propose an
end-to-end learning framework, known-region-aware domain alignment (KRADA), to
distinguish unknown classes while aligning distributions of known classes in
labeled and unlabeled open-world images. The effectiveness of KRADA has been
verified on two synthetic tasks and one COVID-19 segmentation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chenhong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chen Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheung_W/0/1/0/all/0/1"&gt;William Cheung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Approach to Lifelong Learning: The Plastic Support Structure. (arXiv:2106.06298v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06298</id>
        <link href="http://arxiv.org/abs/2106.06298"/>
        <updated>2021-06-14T01:38:53.937Z</updated>
        <summary type="html"><![CDATA[We propose a novel approach to lifelong learning, introducing a compact
encapsulated support structure which endows a network with the capability to
expand its capacity as needed to learn new tasks while preventing the loss of
learned tasks. This is achieved by splitting neurons with high semantic drift
and constructing an adjacent network to encode the new tasks at hand. We call
this the Plastic Support Structure (PSS), it is a compact structure to learn
new tasks that cannot be efficiently encoded in the existing structure of the
network. We validate the PSS on public datasets against existing lifelong
learning architectures, showing it performs similarly to them but without prior
knowledge of the task and in some cases with fewer parameters and in a more
understandable fashion where the PSS is an encapsulated container for specific
features related to specific tasks, thus making it an ideal "add-on" solution
for endowing a network to learn more tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kanaan_G/0/1/0/all/0/1"&gt;Georges Kanaan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kai Wen Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fenaux_L/0/1/0/all/0/1"&gt;Lucas Fenaux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Courteous Behavior of Automated Vehicles at Unsignalized Intersections via Reinforcement Learning. (arXiv:2106.06369v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06369</id>
        <link href="http://arxiv.org/abs/2106.06369"/>
        <updated>2021-06-14T01:38:53.931Z</updated>
        <summary type="html"><![CDATA[The transition from today's mostly human-driven traffic to a purely automated
one will be a gradual evolution, with the effect that we will likely experience
mixed traffic in the near future. Connected and automated vehicles can benefit
human-driven ones and the whole traffic system in different ways, for example
by improving collision avoidance and reducing traffic waves. Many studies have
been carried out to improve intersection management, a significant bottleneck
in traffic, with intelligent traffic signals or exclusively automated vehicles.
However, the problem of how to improve mixed traffic at unsignalized
intersections has received less attention. In this paper, we propose a novel
approach to optimizing traffic flow at intersections in mixed traffic
situations using deep reinforcement learning. Our reinforcement learning agent
learns a policy for a centralized controller to let connected autonomous
vehicles at unsignalized intersections give up their right of way and yield to
other vehicles to optimize traffic flow. We implemented our approach and tested
it in the traffic simulator SUMO based on simulated and real traffic data. The
experimental evaluation demonstrates that our method significantly improves
traffic flow through unsignalized intersections in mixed traffic settings and
also provides better performance on a wide range of traffic situations compared
to the state-of-the-art traffic signal controller for the corresponding
signalized intersection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Shengchao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welschehold_T/0/1/0/all/0/1"&gt;Tim Welschehold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buscher_D/0/1/0/all/0/1"&gt;Daniel B&amp;#xfc;scher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1"&gt;Wolfram Burgard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework to Enhance Generalization of Deep Metric Learning methods using General Discriminative Feature Learning and Class Adversarial Neural Networks. (arXiv:2106.06420v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06420</id>
        <link href="http://arxiv.org/abs/2106.06420"/>
        <updated>2021-06-14T01:38:53.913Z</updated>
        <summary type="html"><![CDATA[Metric learning algorithms aim to learn a distance function that brings the
semantically similar data items together and keeps dissimilar ones at a
distance. The traditional Mahalanobis distance learning is equivalent to find a
linear projection. In contrast, Deep Metric Learning (DML) methods are proposed
that automatically extract features from data and learn a non-linear
transformation from input space to a semantically embedding space. Recently,
many DML methods are proposed focused to enhance the discrimination power of
the learned metric by providing novel sampling strategies or loss functions.
This approach is very helpful when both the training and test examples are
coming from the same set of categories. However, it is less effective in many
applications of DML such as image retrieval and person-reidentification. Here,
the DML should learn general semantic concepts from observed classes and employ
them to rank or identify objects from unseen categories. Neglecting the
generalization ability of the learned representation and just emphasizing to
learn a more discriminative embedding on the observed classes may lead to the
overfitting problem. To address this limitation, we propose a framework to
enhance the generalization power of existing DML methods in a Zero-Shot
Learning (ZSL) setting by general yet discriminative representation learning
and employing a class adversarial neural network. To learn a more general
representation, we propose to employ feature maps of intermediate layers in a
deep neural network and enhance their discrimination power through an attention
mechanism. Besides, a class adversarial network is utilized to enforce the deep
model to seek class invariant features for the DML task. We evaluate our work
on widely used machine vision datasets in a ZSL setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Kaabi_K/0/1/0/all/0/1"&gt;Karrar Al-Kaabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monsefi_R/0/1/0/all/0/1"&gt;Reza Monsefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zabihzadeh_D/0/1/0/all/0/1"&gt;Davood Zabihzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TOHAN: A One-step Approach towards Few-shot Hypothesis Adaptation. (arXiv:2106.06326v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06326</id>
        <link href="http://arxiv.org/abs/2106.06326"/>
        <updated>2021-06-14T01:38:53.904Z</updated>
        <summary type="html"><![CDATA[In few-shot domain adaptation (FDA), classifiers for the target domain are
trained with accessible labeled data in the source domain (SD) and few labeled
data in the target domain (TD). However, data usually contain private
information in the current era, e.g., data distributed on personal phones.
Thus, the private information will be leaked if we directly access data in SD
to train a target-domain classifier (required by FDA methods). In this paper,
to thoroughly prevent the privacy leakage in SD, we consider a very challenging
problem setting, where the classifier for the TD has to be trained using few
labeled target data and a well-trained SD classifier, named few-shot hypothesis
adaptation (FHA). In FHA, we cannot access data in SD, as a result, the private
information in SD will be protected well. To this end, we propose a target
orientated hypothesis adaptation network (TOHAN) to solve the FHA problem,
where we generate highly-compatible unlabeled data (i.e., an intermediate
domain) to help train a target-domain classifier. TOHAN maintains two deep
networks simultaneously, where one focuses on learning an intermediate domain
and the other takes care of the intermediate-to-target distributional
adaptation and the target-risk minimization. Experimental results show that
TOHAN outperforms competitive baselines significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1"&gt;Haoang Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenjing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1"&gt;Long Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_W/0/1/0/all/0/1"&gt;William K. Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1"&gt;James T. Kwok&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Risk Adaptation in Distributional Reinforcement Learning. (arXiv:2106.06317v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06317</id>
        <link href="http://arxiv.org/abs/2106.06317"/>
        <updated>2021-06-14T01:38:53.898Z</updated>
        <summary type="html"><![CDATA[The use of Reinforcement Learning (RL) agents in practical applications
requires the consideration of suboptimal outcomes, depending on the familiarity
of the agent with its environment. This is especially important in
safety-critical environments, where errors can lead to high costs or damage. In
distributional RL, the risk-sensitivity can be controlled via different
distortion measures of the estimated return distribution. However, these
distortion functions require an estimate of the risk level, which is difficult
to obtain and depends on the current state. In this work, we demonstrate the
suboptimality of a static risk level estimation and propose a method to
dynamically select risk levels at each environment step. Our method ARA
(Automatic Risk Adaptation) estimates the appropriate risk level in both known
and unknown environments using a Random Network Distillation error. We show
reduced failure rates by up to a factor of 7 and improved generalization
performance by up to 14% compared to both risk-aware and risk-agnostic agents
in several locomotion environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_F/0/1/0/all/0/1"&gt;Frederik Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eimer_T/0/1/0/all/0/1"&gt;Theresa Eimer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1"&gt;Bodo Rosenhahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1"&gt;Marius Lindauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning. (arXiv:2106.06232v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06232</id>
        <link href="http://arxiv.org/abs/2106.06232"/>
        <updated>2021-06-14T01:38:53.890Z</updated>
        <summary type="html"><![CDATA[Deep Q Network (DQN) firstly kicked the door of deep reinforcement learning
(DRL) via combining deep learning (DL) with reinforcement learning (RL), which
has noticed that the distribution of the acquired data would change during the
training process. DQN found this property might cause instability for training,
so it proposed effective methods to handle the downside of the property.
Instead of focusing on the unfavourable aspects, we find it critical for RL to
ease the gap between the estimated data distribution and the ground truth data
distribution while supervised learning (SL) fails to do so. From this new
perspective, we extend the basic paradigm of RL called the Generalized Policy
Iteration (GPI) into a more generalized version, which is called the
Generalized Data Distribution Iteration (GDI). We see massive RL algorithms and
techniques can be unified into the GDI paradigm, which can be considered as one
of the special cases of GDI. We provide theoretical proof of why GDI is better
than GPI and how it works. Several practical algorithms based on GDI have been
proposed to verify the effectiveness and extensiveness of it. Empirical
experiments prove our state-of-the-art (SOTA) performance on Arcade Learning
Environment (ALE), wherein our algorithm has achieved 9620.98% mean human
normalized score (HNS), 1146.39% median HNS and 22 human world record
breakthroughs (HWRB) using only 200 training frames. Our work aims to lead the
RL research to step into the journey of conquering the human world records and
seek real superhuman agents on both performance and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jiajun Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Changnan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yue Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Selection for Bayesian Autoencoders. (arXiv:2106.06245v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06245</id>
        <link href="http://arxiv.org/abs/2106.06245"/>
        <updated>2021-06-14T01:38:53.884Z</updated>
        <summary type="html"><![CDATA[We develop a novel method for carrying out model selection for Bayesian
autoencoders (BAEs) by means of prior hyper-parameter optimization. Inspired by
the common practice of type-II maximum likelihood optimization and its
equivalence to Kullback-Leibler divergence minimization, we propose to optimize
the distributional sliced-Wasserstein distance (DSWD) between the output of the
autoencoder and the empirical data distribution. The advantages of this
formulation are that we can estimate the DSWD based on samples and handle
high-dimensional problems. We carry out posterior estimation of the BAE
parameters via stochastic gradient Hamiltonian Monte Carlo and turn our BAE
into a generative model by fitting a flexible Dirichlet mixture model in the
latent space. Consequently, we obtain a powerful alternative to variational
autoencoders, which are the preferred choice in modern applications of
autoencoders for representation learning with uncertainty. We evaluate our
approach qualitatively and quantitatively using a vast experimental campaign on
a number of unsupervised learning tasks and show that, in small-data regimes
where priors matter, our approach provides state-of-the-art results,
outperforming multiple competitive baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tran_B/0/1/0/all/0/1"&gt;Ba-Hien Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rossi_S/0/1/0/all/0/1"&gt;Simone Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Milios_D/0/1/0/all/0/1"&gt;Dimitrios Milios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Michiardi_P/0/1/0/all/0/1"&gt;Pietro Michiardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bonilla_E/0/1/0/all/0/1"&gt;Edwin V. Bonilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Filippone_M/0/1/0/all/0/1"&gt;Maurizio Filippone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DG-LMC: A Turn-key and Scalable Synchronous Distributed MCMC Algorithm. (arXiv:2106.06300v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.06300</id>
        <link href="http://arxiv.org/abs/2106.06300"/>
        <updated>2021-06-14T01:38:53.866Z</updated>
        <summary type="html"><![CDATA[Performing reliable Bayesian inference on a big data scale is becoming a
keystone in the modern era of machine learning. A workhorse class of methods to
achieve this task are Markov chain Monte Carlo (MCMC) algorithms and their
design to handle distributed datasets has been the subject of many works.
However, existing methods are not completely either reliable or computationally
efficient. In this paper, we propose to fill this gap in the case where the
dataset is partitioned and stored on computing nodes within a cluster under a
master/slaves architecture. We derive a user-friendly centralised distributed
MCMC algorithm with provable scaling in high-dimensional settings. We
illustrate the relevance of the proposed methodology on both synthetic and real
data experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Plassier_V/0/1/0/all/0/1"&gt;Vincent Plassier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vono_M/0/1/0/all/0/1"&gt;Maxime Vono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1"&gt;Alain Durmus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Moulines_E/0/1/0/all/0/1"&gt;Eric Moulines&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Reinforcement Learning with Linear Function Approximation. (arXiv:2106.06239v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06239</id>
        <link href="http://arxiv.org/abs/2106.06239"/>
        <updated>2021-06-14T01:38:53.859Z</updated>
        <summary type="html"><![CDATA[Safety in reinforcement learning has become increasingly important in recent
years. Yet, existing solutions either fail to strictly avoid choosing unsafe
actions, which may lead to catastrophic results in safety-critical systems, or
fail to provide regret guarantees for settings where safety constraints need to
be learned. In this paper, we address both problems by first modeling safety as
an unknown linear cost function of states and actions, which must always fall
below a certain threshold. We then present algorithms, termed SLUCB-QVI and
RSLUCB-QVI, for episodic Markov decision processes (MDPs) with linear function
approximation. We show that SLUCB-QVI and RSLUCB-QVI, while with \emph{no
safety violation}, achieve a
$\tilde{\mathcal{O}}\left(\kappa\sqrt{d^3H^3T}\right)$ regret, nearly matching
that of state-of-the-art unsafe algorithms, where $H$ is the duration of each
episode, $d$ is the dimension of the feature mapping, $\kappa$ is a constant
characterizing the safety constraints, and $T$ is the total number of action
plays. We further present numerical simulations that corroborate our
theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amani_S/0/1/0/all/0/1"&gt;Sanae Amani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1"&gt;Christos Thrampoulidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lin F. Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TrafficStream: A Streaming Traffic Flow Forecasting Framework Based on Graph Neural Networks and Continual Learning. (arXiv:2106.06273v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06273</id>
        <link href="http://arxiv.org/abs/2106.06273"/>
        <updated>2021-06-14T01:38:53.853Z</updated>
        <summary type="html"><![CDATA[With the rapid growth of traffic sensors deployed, a massive amount of
traffic flow data are collected, revealing the long-term evolution of traffic
flows and the gradual expansion of traffic networks. How to accurately
forecasting these traffic flow attracts the attention of researchers as it is
of great significance for improving the efficiency of transportation systems.
However, existing methods mainly focus on the spatial-temporal correlation of
static networks, leaving the problem of efficiently learning models on networks
with expansion and evolving patterns less studied. To tackle this problem, we
propose a Streaming Traffic Flow Forecasting Framework, TrafficStream, based on
Graph Neural Networks (GNNs) and Continual Learning (CL), achieving accurate
predictions and high efficiency. Firstly, we design a traffic pattern fusion
method, cleverly integrating the new patterns that emerged during the long-term
period into the model. A JS-divergence-based algorithm is proposed to mine new
traffic patterns. Secondly, we introduce CL to consolidate the knowledge
learned previously and transfer them to the current model. Specifically, we
adopt two strategies: historical data replay and parameter smoothing. We
construct a streaming traffic dataset to verify the efficiency and
effectiveness of our model. Extensive experiments demonstrate its excellent
potential to extract traffic patterns with high efficiency on long-term
streaming network scene. The source code is available at
https://github.com/AprLie/TrafficStream.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junshan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1"&gt;Kunqing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LocoProp: Enhancing BackProp via Local Loss Optimization. (arXiv:2106.06199v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06199</id>
        <link href="http://arxiv.org/abs/2106.06199"/>
        <updated>2021-06-14T01:38:53.845Z</updated>
        <summary type="html"><![CDATA[We study a local loss construction approach for optimizing neural networks.
We start by motivating the problem as minimizing a squared loss between the
pre-activations of each layer and a local target, plus a regularizer term on
the weights. The targets are chosen so that the first gradient descent step on
the local objectives recovers vanilla BackProp, while the exact solution to
each problem results in a preconditioned gradient update. We improve the local
loss construction by forming a Bregman divergence in each layer tailored to the
transfer function which keeps the local problem convex w.r.t. the weights. The
generalized local problem is again solved iteratively by taking small gradient
descent steps on the weights, for which the first step recovers BackProp. We
run several ablations and show that our construction consistently improves
convergence, reducing the gap between first-order and second-order methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amid_E/0/1/0/all/0/1"&gt;Ehsan Amid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1"&gt;Rohan Anil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warmuth_M/0/1/0/all/0/1"&gt;Manfred K. Warmuth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Probabilistic Koopman: Long-term time-series forecasting under periodic uncertainties. (arXiv:2106.06033v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06033</id>
        <link href="http://arxiv.org/abs/2106.06033"/>
        <updated>2021-06-14T01:38:53.839Z</updated>
        <summary type="html"><![CDATA[Probabilistic forecasting of complex phenomena is paramount to various
scientific disciplines and applications. Despite the generality and importance
of the problem, general mathematical techniques that allow for stable long-term
forecasts with calibrated uncertainty measures are lacking. For most time
series models, the difficulty of obtaining accurate probabilistic future time
step predictions increases with the prediction horizon. In this paper, we
introduce a surprisingly simple approach that characterizes time-varying
distributions and enables reasonably accurate predictions thousands of
timesteps into the future. This technique, which we call Deep Probabilistic
Koopman (DPK), is based on recent advances in linear Koopman operator theory,
and does not require time stepping for future time predictions. Koopman models
also tend to have a small parameter footprint (often less than 10,000
parameters). We demonstrate the long-term forecasting performance of these
models on a diversity of domains, including electricity demand forecasting,
atmospheric chemistry, and neuroscience. For electricity demand modeling, our
domain-agnostic technique outperforms all of 177 domain-specific competitors in
the most recent Global Energy Forecasting Competition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mallen_A/0/1/0/all/0/1"&gt;Alex Mallen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lange_H/0/1/0/all/0/1"&gt;Henning Lange&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1"&gt;J. Nathan Kutz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Pool in Graph Neural Networks for Extrapolation. (arXiv:2106.06210v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06210</id>
        <link href="http://arxiv.org/abs/2106.06210"/>
        <updated>2021-06-14T01:38:53.832Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are one of the most popular approaches to using
deep learning on graph-structured data, and they have shown state-of-the-art
performances on a variety of tasks. However, according to a recent study, a
careful choice of pooling functions, which are used for the aggregation or
readout operation in GNNs, is crucial for enabling GNNs to extrapolate. Without
the ideal combination of pooling functions, which varies across tasks, GNNs
completely fail to generalize to out-of-distribution data, while the number of
possible combinations grows exponentially with the number of layers. In this
paper, we present GNP, a $L^p$ norm-like pooling function that is trainable
end-to-end for any given task. Notably, GNP generalizes most of the widely-used
pooling functions. We verify experimentally that simply replacing all pooling
functions with GNP enables GNNs to extrapolate well on many node-level,
graph-level, and set-related tasks; and GNP sometimes performs even better than
optimal combinations of existing pooling functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1"&gt;Jihoon Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1"&gt;Taehyung Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1"&gt;Kijung Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Juho Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Taylor Expansion of Discount Factors. (arXiv:2106.06170v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06170</id>
        <link href="http://arxiv.org/abs/2106.06170"/>
        <updated>2021-06-14T01:38:53.813Z</updated>
        <summary type="html"><![CDATA[In practical reinforcement learning (RL), the discount factor used for
estimating value functions often differs from that used for defining the
evaluation objective. In this work, we study the effect that this discrepancy
of discount factors has during learning, and discover a family of objectives
that interpolate value functions of two distinct discount factors. Our analysis
suggests new ways for estimating value functions and performing policy
optimization updates, which demonstrate empirical performance gains. This
framework also leads to new insights on commonly-used deep RL heuristic
modifications to policy optimization algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yunhao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rowland_M/0/1/0/all/0/1"&gt;Mark Rowland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Munos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1"&gt;Michal Valko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial purification with Score-based generative models. (arXiv:2106.06041v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06041</id>
        <link href="http://arxiv.org/abs/2106.06041"/>
        <updated>2021-06-14T01:38:53.806Z</updated>
        <summary type="html"><![CDATA[While adversarial training is considered as a standard defense method against
adversarial attacks for image classifiers, adversarial purification, which
purifies attacked images into clean images with a standalone purification
model, has shown promises as an alternative defense method. Recently, an
Energy-Based Model (EBM) trained with Markov-Chain Monte-Carlo (MCMC) has been
highlighted as a purification model, where an attacked image is purified by
running a long Markov-chain using the gradients of the EBM. Yet, the
practicality of the adversarial purification using an EBM remains questionable
because the number of MCMC steps required for such purification is too large.
In this paper, we propose a novel adversarial purification method based on an
EBM trained with Denoising Score-Matching (DSM). We show that an EBM trained
with DSM can quickly purify attacked images within a few steps. We further
introduce a simple yet effective randomized purification scheme that injects
random noises into images before purification. This process screens the
adversarial perturbations imposed on images by the random noises and brings the
images to the regime where the EBM can denoise well. We show that our
purification method is robust against various attacks and demonstrate its
state-of-the-art performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jongmin Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Juho Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing the Effectiveness of Syntactic Structure to Learn Code Edit Representations. (arXiv:2106.06110v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06110</id>
        <link href="http://arxiv.org/abs/2106.06110"/>
        <updated>2021-06-14T01:38:53.684Z</updated>
        <summary type="html"><![CDATA[In recent times, it has been shown that one can use code as data to aid
various applications such as automatic commit message generation, automatic
generation of pull request descriptions and automatic program repair. Take for
instance the problem of commit message generation. Treating source code as a
sequence of tokens, state of the art techniques generate commit messages using
neural machine translation models. However, they tend to ignore the syntactic
structure of programming languages.

Previous work, i.e., code2seq has used structural information from Abstract
Syntax Tree (AST) to represent source code and they use it to automatically
generate method names. In this paper, we elaborate upon this state of the art
approach and modify it to represent source code edits. We determine the effect
of using such syntactic structure for the problem of classifying code edits.
Inspired by the code2seq approach, we evaluate how using structural information
from AST, i.e., paths between AST leaf nodes can help with the task of code
edit classification on two datasets of fine-grained syntactic edits.

Our experiments shows that attempts of adding syntactic structure does not
result in any improvements over less sophisticated methods. The results suggest
that techniques such as code2seq, while promising, have a long way to go before
they can be generically applied to learning code edit representations. We hope
that these results will benefit other researchers and inspire them to work
further on this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qureshi_S/0/1/0/all/0/1"&gt;Syed Arbaaz Qureshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1"&gt;Sonu Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhagwan_R/0/1/0/all/0/1"&gt;Ranjita Bhagwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Rahul Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Optimization Kernel: Towards Robust Deep Learning. (arXiv:2106.06097v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06097</id>
        <link href="http://arxiv.org/abs/2106.06097"/>
        <updated>2021-06-14T01:38:53.677Z</updated>
        <summary type="html"><![CDATA[Recent studies show a close connection between neural networks (NN) and
kernel methods. However, most of these analyses (e.g., NTK) focus on the
influence of (infinite) width instead of the depth of NN models. There remains
a gap between theory and practical network designs that benefit from the depth.
This paper first proposes a novel kernel family named Neural Optimization
Kernel (NOK). Our kernel is defined as the inner product between two $T$-step
updated functionals in RKHS w.r.t. a regularized optimization problem.
Theoretically, we proved the monotonic descent property of our update rule for
both convex and non-convex problems, and a $O(1/T)$ convergence rate of our
updates for convex problems. Moreover, we propose a data-dependent structured
approximation of our NOK, which builds the connection between training deep NNs
and kernel methods associated with NOK. The resultant computational graph is a
ResNet-type finite width NN. Our structured approximation preserved the
monotonic descent property and $O(1/T)$ convergence rate. Namely, a $T$-layer
NN performs $T$-step monotonic descent updates. Notably, we show our
$T$-layered structured NN with ReLU maintains a $O(1/T)$ convergence rate
w.r.t. a convex regularized problem, which explains the success of ReLU on
training deep NN from a NN architecture optimization perspective. For the
unsupervised learning and the shared parameter case, we show the equivalence of
training structured NN with GD and performing functional gradient descent in
RKHS associated with a fixed (data-dependent) NOK at an infinity-width regime.
For finite NOKs, we prove generalization bounds. Remarkably, we show that
overparameterized deep NN (NOK) can increase the expressive power to reduce
empirical risk and reduce the generalization bound at the same time. Extensive
experiments verify the robustness of our structured NOK blocks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yueming Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor Tsang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HIFI: Anomaly Detection for Multivariate Time Series with High-order Feature Interactions. (arXiv:2106.06167v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06167</id>
        <link href="http://arxiv.org/abs/2106.06167"/>
        <updated>2021-06-14T01:38:53.651Z</updated>
        <summary type="html"><![CDATA[Monitoring complex systems results in massive multivariate time series data,
and anomaly detection of these data is very important to maintain the normal
operation of the systems. Despite the recent emergence of a large number of
anomaly detection algorithms for multivariate time series, most of them ignore
the correlation modeling among multivariate, which can often lead to poor
anomaly detection results. In this work, we propose a novel anomaly detection
model for multivariate time series with \underline{HI}gh-order
\underline{F}eature \underline{I}nteractions (HIFI). More specifically, HIFI
builds multivariate feature interaction graph automatically and uses the graph
convolutional neural network to achieve high-order feature interactions, in
which the long-term temporal dependencies are modeled by attention mechanisms
and a variational encoding technique is utilized to improve the model
performance and robustness. Extensive experiments on three publicly available
datasets demonstrate the superiority of our framework compared with
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1"&gt;Liwei Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuanhao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kai Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the Precise Feature for Cluster Assignment. (arXiv:2106.06159v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06159</id>
        <link href="http://arxiv.org/abs/2106.06159"/>
        <updated>2021-06-14T01:38:53.629Z</updated>
        <summary type="html"><![CDATA[Clustering is one of the fundamental tasks in computer vision and pattern
recognition. Recently, deep clustering methods (algorithms based on deep
learning) have attracted wide attention with their impressive performance. Most
of these algorithms combine deep unsupervised representation learning and
standard clustering together. However, the separation of representation
learning and clustering will lead to suboptimal solutions because the two-stage
strategy prevents representation learning from adapting to subsequent tasks
(e.g., clustering according to specific cues). To overcome this issue, efforts
have been made in the dynamic adaption of representation and cluster
assignment, whereas current state-of-the-art methods suffer from heuristically
constructed objectives with representation and cluster assignment alternatively
optimized. To further standardize the clustering problem, we audaciously
formulate the objective of clustering as finding a precise feature as the cue
for cluster assignment. Based on this, we propose a general-purpose deep
clustering framework which radically integrates representation learning and
clustering into a single pipeline for the first time. The proposed framework
exploits the powerful ability of recently developed generative models for
learning intrinsic features, and imposes an entropy minimization on the
distribution of the cluster assignment by a dedicated variational algorithm.
Experimental results show that the performance of the proposed method is
superior, or at least comparable to, the state-of-the-art methods on the
handwritten digit recognition, fashion recognition, face recognition and object
recognition benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1"&gt;Yanhai Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xinghui Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1"&gt;Feng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Junyu Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Generative-Contrastive Representation Learning. (arXiv:2106.06162v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06162</id>
        <link href="http://arxiv.org/abs/2106.06162"/>
        <updated>2021-06-14T01:38:53.620Z</updated>
        <summary type="html"><![CDATA[Unsupervised representation learning has recently received lots of interest
due to its powerful generalizability through effectively leveraging large-scale
unlabeled data. There are two prevalent approaches for this, contrastive
learning and generative pre-training, where the former learns representations
from instance-wise discrimination tasks and the latter learns them from
estimating the likelihood. These seemingly orthogonal approaches have their own
strengths and weaknesses. Contrastive learning tends to extract semantic
information and discards details irrelevant for classifying objects, making the
representations effective for discriminative tasks while degrading robustness
to out-of-distribution data. On the other hand, the generative pre-training
directly estimates the data distribution, so the representations tend to be
robust but not optimal for discriminative tasks. In this paper, we show that we
could achieve the best of both worlds by a hybrid training scheme.
Specifically, we demonstrated that a transformer-based encoder-decoder
architecture trained with both contrastive and generative losses can learn
highly discriminative and robust representations without hurting the generative
performance. We extensively validate our approach on various tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Saehoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungwoong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Juho Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Homophily a Necessity for Graph Neural Networks?. (arXiv:2106.06134v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06134</id>
        <link href="http://arxiv.org/abs/2106.06134"/>
        <updated>2021-06-14T01:38:53.614Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have shown great prowess in learning
representations suitable for numerous graph-based machine learning tasks. When
applied to semi-supervised node classification, GNNs are widely believed to
work well due to the homophily assumption (``like attracts like''), and fail to
generalize to heterophilous graphs where dissimilar nodes connect. Recent works
design new architectures to overcome such heterophily-related limitations,
citing poor baseline performance and new architecture improvements on a few
heterophilous graph benchmark datasets as evidence for this notion. In our
experiments, we empirically find that standard graph convolutional networks
(GCNs) can actually achieve better performance than such carefully designed
methods on some commonly used heterophilous graphs. This motivates us to
reconsider whether homophily is truly necessary for good GNN performance. We
find that this claim is not quite true, and in fact, GCNs can achieve strong
performance on heterophilous graphs under certain conditions. Our work
carefully characterizes these conditions, and provides supporting theoretical
understanding and empirical observations. Finally, we examine existing
heterophilous graphs benchmarks and reconcile how the GCN (under)performs on
them based on this understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaorui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1"&gt;Neil Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiliang Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NAAQA: A Neural Architecture for Acoustic Question Answering. (arXiv:2106.06147v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06147</id>
        <link href="http://arxiv.org/abs/2106.06147"/>
        <updated>2021-06-14T01:38:53.607Z</updated>
        <summary type="html"><![CDATA[The goal of the Acoustic Question Answering (AQA) task is to answer a
free-form text question about the content of an acoustic scene. It was inspired
by the Visual Question Answering (VQA) task. In this paper, based on the
previously introduced CLEAR dataset, we propose a new benchmark for AQA that
emphasizes the specific challenges of acoustic inputs, e.g. variable duration
scenes. We also introduce NAAQA, a neural architecture that leverages specific
properties of acoustic inputs. The usage of time and frequency 1D convolutions
to process 2D spectro-temporal representations of acoustic content shows
promising results and enables reductions in model complexity. NAAQA achieves
91.6% of accuracy on the AQA task with about 7 times fewer parameters than the
previously explored VQA model. We provide a detailed analysis of the results
for the different question types. The effectiveness of coordinate maps in this
acoustic context was also studied and we show that time coordinate maps augment
temporal localization capabilities which enhance performance of the network by
about 17 percentage points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdelnour_J/0/1/0/all/0/1"&gt;Jerome Abdelnour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1"&gt;Jean Rouat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salvi_G/0/1/0/all/0/1"&gt;Giampiero Salvi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generate, Annotate, and Learn: Generative Models Advance Self-Training and Knowledge Distillation. (arXiv:2106.06168v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06168</id>
        <link href="http://arxiv.org/abs/2106.06168"/>
        <updated>2021-06-14T01:38:53.601Z</updated>
        <summary type="html"><![CDATA[Semi-Supervised Learning (SSL) has seen success in many application domains,
but this success often hinges on the availability of task-specific unlabeled
data. Knowledge distillation (KD) has enabled compressing deep networks and
ensembles, achieving the best results when distilling knowledge on fresh
task-specific unlabeled examples. However, task-specific unlabeled data can be
challenging to find. We present a general framework called "generate, annotate,
and learn (GAL)" that uses unconditional generative models to synthesize
in-domain unlabeled data, helping advance SSL and KD on different tasks. To
obtain strong task-specific generative models, we adopt generic generative
models, pretrained on open-domain data, and fine-tune them on inputs from
specific tasks. Then, we use existing classifiers to annotate generated
unlabeled examples with soft pseudo labels, which are used for additional
training. When self-training is combined with samples generated from
GPT2-large, fine-tuned on the inputs of each GLUE task, we outperform a strong
RoBERTa-large baseline on the GLUE benchmark. Moreover, KD on GPT-2 samples
yields a new state-of-the-art for 6-layer transformers on the GLUE leaderboard.
Finally, self-training with GAL offers significant gains on image
classification on CIFAR-10 and four tabular tasks from the UCI repository]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xuanli He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nassar_I/0/1/0/all/0/1"&gt;Islam Nassar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiros_J/0/1/0/all/0/1"&gt;Jamie Kiros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1"&gt;Gholamreza Haffari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1"&gt;Mohammad Norouzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning. (arXiv:2106.06135v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.06135</id>
        <link href="http://arxiv.org/abs/2106.06135"/>
        <updated>2021-06-14T01:38:53.582Z</updated>
        <summary type="html"><![CDATA[Games are abstractions of the real world, where artificial agents learn to
compete and cooperate with other agents. While significant achievements have
been made in various perfect- and imperfect-information games, DouDizhu (a.k.a.
Fighting the Landlord), a three-player card game, is still unsolved. DouDizhu
is a very challenging domain with competition, collaboration, imperfect
information, large state space, and particularly a massive set of possible
actions where the legal actions vary significantly from turn to turn.
Unfortunately, modern reinforcement learning algorithms mainly focus on simple
and small action spaces, and not surprisingly, are shown not to make
satisfactory progress in DouDizhu. In this work, we propose a conceptually
simple yet effective DouDizhu AI system, namely DouZero, which enhances
traditional Monte-Carlo methods with deep neural networks, action encoding, and
parallel actors. Starting from scratch in a single server with four GPUs,
DouZero outperformed all the existing DouDizhu AI programs in days of training
and was ranked the first in the Botzone leaderboard among 344 AI agents.
Through building DouZero, we show that classic Monte-Carlo methods can be made
to deliver strong results in a hard domain with a complex action space. The
code and an online demo are released at https://github.com/kwai/DouZero with
the hope that this insight could motivate future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1"&gt;Daochen Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jingru Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wenye Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1"&gt;Xiangru Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xia Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomalous Sound Detection Using a Binary Classification Model and Class Centroids. (arXiv:2106.06151v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.06151</id>
        <link href="http://arxiv.org/abs/2106.06151"/>
        <updated>2021-06-14T01:38:53.576Z</updated>
        <summary type="html"><![CDATA[An anomalous sound detection system to detect unknown anomalous sounds
usually needs to be built using only normal sound data. Moreover, it is
desirable to improve the system by effectively using a small amount of
anomalous sound data, which will be accumulated through the system's operation.
As one of the methods to meet these requirements, we focus on a binary
classification model that is developed by using not only normal data but also
outlier data in the other domains as pseudo-anomalous sound data, which can be
easily updated by using anomalous data. In this paper, we implement a new loss
function based on metric learning to learn the distance relationship from each
class centroid in feature space for the binary classification model. The
proposed multi-task learning of the binary classification and the metric
learning makes it possible to build the feature space where the within-class
variance is minimized and the between-class variance is maximized while keeping
normal and anomalous classes linearly separable. We also investigate the
effectiveness of additionally using anomalous sound data for further improving
the binary classification model. Our results showed that multi-task learning
using binary classification and metric learning to consider the distance from
each class centroid in the feature space is effective, and performance can be
significantly improved by using even a small amount of anomalous data during
training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuroyanagi_I/0/1/0/all/0/1"&gt;Ibuki Kuroyanagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1"&gt;Tomoki Hayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1"&gt;Kazuya Takeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyGAD: An Intuitive Genetic Algorithm Python Library. (arXiv:2106.06158v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.06158</id>
        <link href="http://arxiv.org/abs/2106.06158"/>
        <updated>2021-06-14T01:38:53.569Z</updated>
        <summary type="html"><![CDATA[This paper introduces PyGAD, an open-source easy-to-use Python library for
building the genetic algorithm. PyGAD supports a wide range of parameters to
give the user control over everything in its life cycle. This includes, but is
not limited to, population, gene value range, gene data type, parent selection,
crossover, and mutation. PyGAD is designed as a general-purpose optimization
library that allows the user to customize the fitness function. Its usage
consists of 3 main steps: build the fitness function, create an instance of the
pygad.GA class, and calling the pygad.GA.run() method. The library supports
training deep learning models created either with PyGAD itself or with
frameworks like Keras and PyTorch. Given its stable state, PyGAD is also in
active development to respond to the user's requested features and enhancement
received on GitHub https://github.com/ahmedfgad/GeneticAlgorithmPython. PyGAD
comes with documentation https://pygad.readthedocs.io for further details and
examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gad_A/0/1/0/all/0/1"&gt;Ahmed Fawzy Gad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comprehensive solution to retrieval-based chatbot construction. (arXiv:2106.06139v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06139</id>
        <link href="http://arxiv.org/abs/2106.06139"/>
        <updated>2021-06-14T01:38:53.562Z</updated>
        <summary type="html"><![CDATA[In this paper we present the results of our experiments in training and
deploying a self-supervised retrieval-based chatbot trained with contrastive
learning for assisting customer support agents. In contrast to most existing
research papers in this area where the focus is on solving just one component
of a deployable chatbot, we present an end-to-end set of solutions to take the
reader from an unlabelled chatlogs to a deployed chatbot. This set of solutions
includes creating a self-supervised dataset and a weakly labelled dataset from
chatlogs, as well as a systematic approach to selecting a fixed list of canned
responses. We present a hierarchical-based RNN architecture for the response
selection model, chosen for its ability to cache intermediate utterance
embeddings, which helped to meet deployment inference speed requirements. We
compare the performance of this architecture across 3 different learning
objectives: self-supervised contrastive learning, binary classification, and
multi-class classification. We find that using a self-supervised contrastive
learning model outperforms training the binary and multi-class classification
models on a weakly labelled dataset. Our results validate that the
self-supervised contrastive learning approach can be effectively used for a
real-world chatbot scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moore_K/0/1/0/all/0/1"&gt;Kristen Moore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1"&gt;Shenjun Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhen He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudolf_T/0/1/0/all/0/1"&gt;Torsten Rudolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fisher_N/0/1/0/all/0/1"&gt;Nils Fisher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Victor_B/0/1/0/all/0/1"&gt;Brandon Victor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jindal_N/0/1/0/all/0/1"&gt;Neha Jindal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DORO: Distributional and Outlier Robust Optimization. (arXiv:2106.06142v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06142</id>
        <link href="http://arxiv.org/abs/2106.06142"/>
        <updated>2021-06-14T01:38:53.554Z</updated>
        <summary type="html"><![CDATA[Many machine learning tasks involve subpopulation shift where the testing
data distribution is a subpopulation of the training distribution. For such
settings, a line of recent work has proposed the use of a variant of empirical
risk minimization(ERM) known as distributionally robust optimization (DRO). In
this work, we apply DRO to real, large-scale tasks with subpopulation shift,
and observe that DRO performs relatively poorly, and moreover has severe
instability. We identify one direct cause of this phenomenon: sensitivity of
DRO to outliers in the datasets. To resolve this issue, we propose the
framework of DORO, for Distributional and Outlier Robust Optimization. At the
core of this approach is a refined risk function which prevents DRO from
overfitting to potential outliers. We instantiate DORO for the Cressie-Read
family of R\'enyi divergence, and delve into two specific instances of this
family: CVaR and $\chi^2$-DRO. We theoretically prove the effectiveness of the
proposed method, and empirically show that DORO improves the performance and
stability of DRO with experiments on large modern datasets, thereby positively
addressing the open question raised by Hashimoto et al., 2018.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_R/0/1/0/all/0/1"&gt;Runtian Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dan_C/0/1/0/all/0/1"&gt;Chen Dan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1"&gt;J. Zico Kolter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1"&gt;Pradeep Ravikumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Bayesian Learning via Stepwise Regression. (arXiv:2106.06095v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06095</id>
        <link href="http://arxiv.org/abs/2106.06095"/>
        <updated>2021-06-14T01:38:53.536Z</updated>
        <summary type="html"><![CDATA[Sparse Bayesian Learning (SBL) is a powerful framework for attaining sparsity
in probabilistic models. Herein, we propose a coordinate ascent algorithm for
SBL termed Relevance Matching Pursuit (RMP) and show that, as its noise
variance parameter goes to zero, RMP exhibits a surprising connection to
Stepwise Regression. Further, we derive novel guarantees for Stepwise
Regression algorithms, which also shed light on RMP. Our guarantees for Forward
Regression improve on deterministic and probabilistic results for Orthogonal
Matching Pursuit with noise. Our analysis of Backward Regression on determined
systems culminates in a bound on the residual of the optimal solution to the
subset selection problem that, if satisfied, guarantees the optimality of the
result. To our knowledge, this bound is the first that can be computed in
polynomial time and depends chiefly on the smallest singular value of the
matrix. We report numerical experiments using a variety of feature selection
algorithms. Notably, RMP and its limiting variant are both efficient and
maintain strong performance with correlated features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ament_S/0/1/0/all/0/1"&gt;Sebastian Ament&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1"&gt;Carla Gomes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Performance FPGA-based Accelerator for Bayesian Recurrent Neural Networks. (arXiv:2106.06048v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06048</id>
        <link href="http://arxiv.org/abs/2106.06048"/>
        <updated>2021-06-14T01:38:53.529Z</updated>
        <summary type="html"><![CDATA[Neural networks have demonstrated their great performance in a wide range of
tasks. Especially in time-series analysis, recurrent architectures based on
long-short term memory (LSTM) cells have manifested excellent capability to
model time dependencies in real-world data. However, standard recurrent
architectures cannot estimate their uncertainty which is essential for
safety-critical applications such as in medicine. In contrast, Bayesian
recurrent neural networks (RNNs) are able to provide uncertainty estimation
with improved accuracy. Nonetheless, Bayesian RNNs are computationally and
memory demanding, which limits their practicality despite their advantages. To
address this issue, we propose an FPGA-based hardware design to accelerate
Bayesian LSTM-based RNNs. To further improve the overall algorithmic-hardware
performance, a co-design framework is proposed to explore the most optimal
algorithmic-hardware configurations for Bayesian RNNs. We conduct extensive
experiments on health-related tasks to demonstrate the improvement of our
design and the effectiveness of our framework. Compared with GPU
implementation, our FPGA-based design can achieve up to 10 times speedup with
nearly 106 times higher energy efficiency. To the best of our knowledge, this
is the first work targeting the acceleration of Bayesian RNNs on FPGAs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferianc_M/0/1/0/all/0/1"&gt;Martin Ferianc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Que_Z/0/1/0/all/0/1"&gt;Zhiqiang Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Hongxiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luk_W/0/1/0/all/0/1"&gt;Wayne Luk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1"&gt;Miguel Rodrigues&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-Level Task Parameters: A Robust Multi-task Weighting Framework. (arXiv:2106.06129v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06129</id>
        <link href="http://arxiv.org/abs/2106.06129"/>
        <updated>2021-06-14T01:38:53.522Z</updated>
        <summary type="html"><![CDATA[Recent works have shown that deep neural networks benefit from multi-task
learning by learning a shared representation across several related tasks.
However, performance of such systems depend on relative weighting between
various losses involved during training. Prior works on loss weighting schemes
assume that instances are equally easy or hard for all tasks. In order to break
this assumption, we let the training process dictate the optimal weighting of
tasks for every instance in the dataset. More specifically, we equip every
instance in the dataset with a set of learnable parameters (instance-level task
parameters) where the cardinality is equal to the number of tasks learned by
the model. These parameters model the weighting of each task for an instance.
They are updated by gradient descent and do not require hand-crafted rules. We
conduct extensive experiments on SURREAL and CityScapes datasets, for human
shape and pose estimation, depth estimation and semantic segmentation tasks. In
these tasks, our approach outperforms recent dynamic loss weighting approaches,
e.g. reducing surface estimation errors by 8.97% on SURREAL. When applied to
datasets where one or more tasks can have noisy annotations, the proposed
method learns to prioritize learning from clean labels for a given task, e.g.
reducing surface estimation errors by up to 60%. We also show that we can
reliably detect corrupt labels for a given task as a by-product from learned
instance-level task parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasu_P/0/1/0/all/0/1"&gt;Pavan Kumar Anasosalu Vasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1"&gt;Shreyas Saxena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1"&gt;Oncel Tuzel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Adaptive Nonlinear Control: Theory and Algorithms. (arXiv:2106.06098v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06098</id>
        <link href="http://arxiv.org/abs/2106.06098"/>
        <updated>2021-06-14T01:38:53.516Z</updated>
        <summary type="html"><![CDATA[We present an online multi-task learning approach for adaptive nonlinear
control, which we call Online Meta-Adaptive Control (OMAC). The goal is to
control a nonlinear system subject to adversarial disturbance and unknown
$\textit{environment-dependent}$ nonlinear dynamics, under the assumption that
the environment-dependent dynamics can be well captured with some shared
representation. Our approach is motivated by robot control, where a robotic
system encounters a sequence of new environmental conditions that it must
quickly adapt to. A key emphasis is to integrate online representation learning
with established methods from control theory, in order to arrive at a unified
framework that yields both control-theoretic and learning-theoretic guarantees.
We provide instantiations of our approach under varying conditions, leading to
the first non-asymptotic end-to-end convergence guarantee for multi-task
adaptive nonlinear control. OMAC can also be integrated with deep
representation learning. Experiments show that OMAC significantly outperforms
conventional adaptive control approaches which do not learn the shared
representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1"&gt;Guanya Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1"&gt;Kamyar Azizzadenesheli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1"&gt;Soon-Jo Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yisong Yue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Federated Learning via Inexact ADMM. (arXiv:2106.06127v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06127</id>
        <link href="http://arxiv.org/abs/2106.06127"/>
        <updated>2021-06-14T01:38:53.509Z</updated>
        <summary type="html"><![CDATA[Differential privacy (DP) techniques can be applied to the federated learning
model to protect data privacy against inference attacks to communication among
the learning agents. The DP techniques, however, hinder achieving a greater
learning performance while ensuring strong data privacy. In this paper we
develop a DP inexact alternating direction method of multipliers algorithm that
solves a sequence of trust-region subproblems with the objective perturbation
by random noises generated from a Laplace distribution. We show that our
algorithm provides $\bar{\epsilon}$-DP for every iteration and
$\mathcal{O}(1/T)$ rate of convergence in expectation, where $T$ is the number
of iterations. Using MNIST and FEMNIST datasets for the image classification,
we demonstrate that our algorithm reduces the testing error by at most $22\%$
compared with the existing DP algorithm, while achieving the same level of data
privacy. The numerical experiment also shows that our algorithm converges
faster than the existing algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ryu_M/0/1/0/all/0/1"&gt;Minseok Ryu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kibaek Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monotonic Neural Network: combining Deep Learning with Domain Knowledge for Chiller Plants Energy Optimization. (arXiv:2106.06143v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.06143</id>
        <link href="http://arxiv.org/abs/2106.06143"/>
        <updated>2021-06-14T01:38:53.487Z</updated>
        <summary type="html"><![CDATA[In this paper, we are interested in building a domain knowledge based deep
learning framework to solve the chiller plants energy optimization problems.
Compared to the hotspot applications of deep learning (e.g. image
classification and NLP), it is difficult to collect enormous data for deep
network training in real-world physical systems. Most existing methods reduce
the complex systems into linear model to facilitate the training on small
samples. To tackle the small sample size problem, this paper considers domain
knowledge in the structure and loss design of deep network to build a nonlinear
model with lower redundancy function space. Specifically, the energy
consumption estimation of most chillers can be physically viewed as an
input-output monotonic problem. Thus, we can design a Neural Network with
monotonic constraints to mimic the physical behavior of the system. We verify
the proposed method in a cooling system of a data center, experimental results
show the superiority of our framework in energy optimization compared to the
existing ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ma_F/0/1/0/all/0/1"&gt;Fanhe Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Faen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ben_S/0/1/0/all/0/1"&gt;Shenglan Ben&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qin_S/0/1/0/all/0/1"&gt;Shuxin Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pengcheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Changsheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_F/0/1/0/all/0/1"&gt;Fengyi Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Large-scale Teacher-Student Training for On-device Acoustic Models. (arXiv:2106.06126v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.06126</id>
        <link href="http://arxiv.org/abs/2106.06126"/>
        <updated>2021-06-14T01:38:53.478Z</updated>
        <summary type="html"><![CDATA[We present results from Alexa speech teams on semi-supervised learning (SSL)
of acoustic models (AM) with experiments spanning over 3000 hours of GPU time,
making our study one of the largest of its kind. We discuss SSL for AMs in a
small footprint setting, showing that a smaller capacity model trained with 1
million hours of unsupervised data can outperform a baseline supervised system
by 14.3% word error rate reduction (WERR). When increasing the supervised data
to seven-fold, our gains diminish to 7.1% WERR; to improve SSL efficiency at
larger supervised data regimes, we employ a step-wise distillation into a
smaller model, obtaining a WERR of 14.4%. We then switch to SSL using larger
student models in low data regimes; while learning efficiency with unsupervised
data is higher, student models may outperform teacher models in such a setting.
We develop a theoretical sketch to explain this behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swaminathan_R/0/1/0/all/0/1"&gt;Rupak Vignesh Swaminathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parthasarathi_S/0/1/0/all/0/1"&gt;Sree Hari Krishnan Parthasarathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1"&gt;Chunchuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1"&gt;Athanasios Mouchtaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunzmann_S/0/1/0/all/0/1"&gt;Siegfried Kunzmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive-Scale Boundary Blackbox Attack via Projective Gradient Estimation. (arXiv:2106.06056v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06056</id>
        <link href="http://arxiv.org/abs/2106.06056"/>
        <updated>2021-06-14T01:38:53.470Z</updated>
        <summary type="html"><![CDATA[Boundary based blackbox attack has been recognized as practical and
effective, given that an attacker only needs to access the final model
prediction. However, the query efficiency of it is in general high especially
for high dimensional image data. In this paper, we show that such efficiency
highly depends on the scale at which the attack is applied, and attacking at
the optimal scale significantly improves the efficiency. In particular, we
propose a theoretical framework to analyze and show three key characteristics
to improve the query efficiency. We prove that there exists an optimal scale
for projective gradient estimation. Our framework also explains the
satisfactory performance achieved by existing boundary black-box attacks. Based
on our theoretical framework, we propose Progressive-Scale enabled projective
Boundary Attack (PSBA) to improve the query efficiency via progressive scaling
techniques. In particular, we employ Progressive-GAN to optimize the scale of
projections, which we call PSBA-PGAN. We evaluate our approach on both spatial
and frequency scales. Extensive experiments on MNIST, CIFAR-10, CelebA, and
ImageNet against different models including a real-world face recognition API
show that PSBA-PGAN significantly outperforms existing baseline attacks in
terms of query efficiency and attack success rate. We also observe relatively
stable optimal scales for different models and datasets. The code is publicly
available at https://github.com/AI-secure/PSBA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiawei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Huichen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaolu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradual Domain Adaptation in the Wild:When Intermediate Distributions are Absent. (arXiv:2106.06080v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06080</id>
        <link href="http://arxiv.org/abs/2106.06080"/>
        <updated>2021-06-14T01:38:53.460Z</updated>
        <summary type="html"><![CDATA[We focus on the problem of domain adaptation when the goal is shifting the
model towards the target distribution, rather than learning domain invariant
representations. It has been shown that under the following two assumptions:
(a) access to samples from intermediate distributions, and (b) samples being
annotated with the amount of change from the source distribution, self-training
can be successfully applied on gradually shifted samples to adapt the model
toward the target distribution. We hypothesize having (a) is enough to enable
iterative self-training to slowly adapt the model to the target distribution,
by making use of an implicit curriculum. In the case where (a) does not hold,
we observe that iterative self-training falls short. We propose GIFT, a method
that creates virtual samples from intermediate distributions by interpolating
representations of examples from source and target domains. We evaluate an
iterative-self-training method on datasets with natural distribution shifts,
and show that when applied on top of other domain adaptation methods, it
improves the performance of the model on the target dataset. We run an analysis
on a synthetic dataset to show that in the presence of (a)
iterative-self-training naturally forms a curriculum of samples. Furthermore,
we show that when (a) does not hold, GIFT performs better than iterative
self-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abnar_S/0/1/0/all/0/1"&gt;Samira Abnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1"&gt;Rianne van den Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1"&gt;Golnaz Ghiasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalchbrenner_N/0/1/0/all/0/1"&gt;Nal Kalchbrenner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1"&gt;Hanie Sedghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence and Alignment of Gradient Descentwith Random Back propagation Weights. (arXiv:2106.06044v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06044</id>
        <link href="http://arxiv.org/abs/2106.06044"/>
        <updated>2021-06-14T01:38:53.451Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent with backpropagation is the workhorse of
artificial neural networks. It has long been recognized that backpropagation
fails to be a biologically plausible algorithm. Fundamentally, it is a
non-local procedure -- updating one neuron's synaptic weights requires
knowledge of synaptic weights or receptive fields of downstream neurons. This
limits the use of artificial neural networks as a tool for understanding the
biological principles of information processing in the brain. Lillicrap et al.
(2016) propose a more biologically plausible "feedback alignment" algorithm
that uses random and fixed backpropagation weights, and show promising
simulations. In this paper we study the mathematical properties of the feedback
alignment procedure by analyzing convergence and alignment for two-layer
networks under squared error loss. In the overparameterized setting, we prove
that the error converges to zero exponentially fast, and also that
regularization is necessary in order for the parameters to become aligned with
the random backpropagation weights. Simulations are given that are consistent
with this analysis and suggest further generalizations. These results
contribute to our understanding of how biologically plausible algorithms might
carry out weight learning in a manner different from Hebbian learning, with
performance that is comparable with the full non-local backpropagation
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Song_G/0/1/0/all/0/1"&gt;Ganlin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruitu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lafferty_J/0/1/0/all/0/1"&gt;John Lafferty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Optimisation with Formal Guarantees. (arXiv:2106.06067v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06067</id>
        <link href="http://arxiv.org/abs/2106.06067"/>
        <updated>2021-06-14T01:38:53.444Z</updated>
        <summary type="html"><![CDATA[Application domains of Bayesian optimization include optimizing black-box

functions or very complex functions. The functions we are interested in
describe

complex real-world systems applied in industrial settings. Even though

they do have explicit representations, standard optimization

techniques fail to provide validated solutions and correctness

guarantees for them.

In this paper we present a combination of Bayesian optimisation and SMT-based
constraint solving to achieve safe and stable solutions with optimality
guarantees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brausse_F/0/1/0/all/0/1"&gt;Franz Brau&amp;#xdf;e&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khasidashvili_Z/0/1/0/all/0/1"&gt;Zurab Khasidashvili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korovin_K/0/1/0/all/0/1"&gt;Konstantin Korovin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Verifying Quantized Neural Networks using SMT-Based Model Checking. (arXiv:2106.05997v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05997</id>
        <link href="http://arxiv.org/abs/2106.05997"/>
        <updated>2021-06-14T01:38:53.425Z</updated>
        <summary type="html"><![CDATA[Artificial Neural Networks (ANNs) are being deployed on an increasing number
of safety-critical applications, including autonomous cars and medical
diagnosis. However, concerns about their reliability have been raised due to
their black-box nature and apparent fragility to adversarial attacks. Here, we
develop and evaluate a symbolic verification framework using incremental model
checking (IMC) and satisfiability modulo theories (SMT) to check for
vulnerabilities in ANNs. More specifically, we propose several ANN-related
optimizations for IMC, including invariant inference via interval analysis and
the discretization of non-linear activation functions. With this, we can
provide guarantees on the safe behavior of ANNs implemented both in
floating-point and fixed-point (quantized) arithmetic. In this regard, our
verification approach was able to verify and produce adversarial examples for
52 test cases spanning image classification and general machine learning
applications. For small- to medium-sized ANN, our approach completes most of
its verification runs in minutes. Moreover, in contrast to most
state-of-the-art methods, our approach is not restricted to specific choices of
activation functions or non-quantized representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sena_L/0/1/0/all/0/1"&gt;Luiz Sena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xidan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alves_E/0/1/0/all/0/1"&gt;Erickson Alves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bessa_I/0/1/0/all/0/1"&gt;Iury Bessa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manino_E/0/1/0/all/0/1"&gt;Edoardo Manino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cordeiro_L/0/1/0/all/0/1"&gt;Lucas Cordeiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Framework for Constructing Nonconvex Regularizations. (arXiv:2106.06123v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.06123</id>
        <link href="http://arxiv.org/abs/2106.06123"/>
        <updated>2021-06-14T01:38:53.417Z</updated>
        <summary type="html"><![CDATA[Over the past decades, many individual nonconvex methods have been proposed
to achieve better sparse recovery performance in various scenarios. However,
how to construct a valid nonconvex regularization function remains open in
practice. In this paper, we fill in this gap by presenting a unified framework
for constructing the nonconvex regularization based on the probability density
function. Meanwhile, a new nonconvex sparse recovery method constructed via the
Weibull distribution is studied.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhiyong Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedBABU: Towards Enhanced Representation for Federated Image Classification. (arXiv:2106.06042v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06042</id>
        <link href="http://arxiv.org/abs/2106.06042"/>
        <updated>2021-06-14T01:38:53.411Z</updated>
        <summary type="html"><![CDATA[Federated learning has evolved to improve a single global model under data
heterogeneity (as a curse) or to develop multiple personalized models using
data heterogeneity (as a blessing). However, there has been little research
considering both directions simultaneously. In this paper, we first investigate
the relationship between them by analyzing Federated Averaging at the client
level and determine that a better federated global model performance does not
constantly improve personalization. To elucidate the cause of this
personalization performance degradation problem, we decompose the entire
network into the body (i.e., extractor), related to universality, and the head
(i.e., classifier), related to personalization. We then point out that this
problem stems from training the head. Based on this observation, we propose a
novel federated learning algorithm, coined as FedBABU, which updates only the
body of the model during federated training (i.e., the head is randomly
initialized and never updated), and the head is fine-tuned for personalization
during the evaluation process. Extensive experiments show consistent
performance improvements and an efficient personalization of FedBABU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jaehoon Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sangmook Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Se-Young Yun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Multidisciplinary Design Optimization with Neural Networks. (arXiv:2106.06092v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06092</id>
        <link href="http://arxiv.org/abs/2106.06092"/>
        <updated>2021-06-14T01:38:53.405Z</updated>
        <summary type="html"><![CDATA[The design of complex engineering systems leads to solving very large
optimization problems involving different disciplines. Strategies allowing
disciplines to optimize in parallel by providing sub-objectives and splitting
the problem into smaller parts, such as Collaborative Optimization, are
promising solutions.However, most of them have slow convergence which reduces
their practical use. Earlier efforts to fasten convergence by learning
surrogate models have not yet succeeded at sufficiently improving the
competitiveness of these strategies.This paper shows that, in the case of
Collaborative Optimization, faster and more reliable convergence can be
obtained by solving an interesting instance of binary classification: on top of
the target label, the training data of one of the two classes contains the
distance to the decision boundary and its derivative. Leveraging this
information, we propose to train a neural network with an asymmetric loss
function, a structure that guarantees Lipshitz continuity, and a regularization
towards respecting basic distance function properties. The approach is
demonstrated on a toy learning example, and then applied to a multidisciplinary
aircraft design problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Becdelievre_J/0/1/0/all/0/1"&gt;Jean de Becdelievre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kroo_I/0/1/0/all/0/1"&gt;Ilan Kroo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Twin Neural Network Regression is a Semi-Supervised Regression Algorithm. (arXiv:2106.06124v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06124</id>
        <link href="http://arxiv.org/abs/2106.06124"/>
        <updated>2021-06-14T01:38:53.397Z</updated>
        <summary type="html"><![CDATA[Twin neural network regression (TNNR) is a semi-supervised regression
algorithm, it can be trained on unlabelled data points as long as other,
labelled anchor data points, are present. TNNR is trained to predict
differences between the target values of two different data points rather than
the targets themselves. By ensembling predicted differences between the targets
of an unseen data point and all training data points, it is possible to obtain
a very accurate prediction for the original regression problem. Since any loop
of predicted differences should sum to zero, loops can be supplied to the
training data, even if the data points themselves within loops are unlabelled.
Semi-supervised training improves TNNR performance, which is already state of
the art, significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wetzel_S/0/1/0/all/0/1"&gt;Sebastian J. Wetzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melko_R/0/1/0/all/0/1"&gt;Roger G. Melko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamblyn_I/0/1/0/all/0/1"&gt;Isaac Tamblyn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. (arXiv:2106.06054v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06054</id>
        <link href="http://arxiv.org/abs/2106.06054"/>
        <updated>2021-06-14T01:38:53.376Z</updated>
        <summary type="html"><![CDATA[In recent years, many incidents have been reported where machine learning
models exhibited discrimination among people based on race, sex, age, etc.
Research has been conducted to measure and mitigate unfairness in machine
learning models. For a machine learning task, it is a common practice to build
a pipeline that includes an ordered set of data preprocessing stages followed
by a classifier. However, most of the research on fairness has considered a
single classifier based prediction task. What are the fairness impacts of the
preprocessing stages in machine learning pipeline? Furthermore, studies showed
that often the root cause of unfairness is ingrained in the data itself, rather
than the model. But no research has been conducted to measure the unfairness
caused by a specific transformation made in the data preprocessing stage. In
this paper, we introduced the causal method of fairness to reason about the
fairness impact of data preprocessing stages in ML pipeline. We leveraged
existing metrics to define the fairness measures of the stages. Then we
conducted a detailed fairness evaluation of the preprocessing stages in 37
pipelines collected from three different sources. Our results show that certain
data transformers are causing the model to exhibit unfairness. We identified a
number of fairness patterns in several categories of data transformers.
Finally, we showed how the local fairness of a preprocessing stage composes in
the global fairness of the pipeline. We used the fairness composition to choose
appropriate downstream transformer that mitigates unfairness in the machine
learning pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1"&gt;Sumon Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1"&gt;Hridesh Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Transformer: Predicting Samples of Unseen, Future Domains. (arXiv:2106.06057v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06057</id>
        <link href="http://arxiv.org/abs/2106.06057"/>
        <updated>2021-06-14T01:38:53.369Z</updated>
        <summary type="html"><![CDATA[The data distribution commonly evolves over time leading to problems such as
concept drift that often decrease classifier performance. We seek to predict
unseen data (and their labels) allowing us to tackle challenges due to a
non-constant data distribution in a \emph{proactive} manner rather than
detecting and reacting to already existing changes that might already have led
to errors. To this end, we learn a domain transformer in an unsupervised manner
that allows generating data of unseen domains. Our approach first matches
independently learned latent representations of two given domains obtained from
an auto-encoder using a Cycle-GAN. In turn, a transformation of the original
samples can be learned that can be applied iteratively to extrapolate to unseen
domains. Our evaluation on CNNs on image data confirms the usefulness of the
approach. It also achieves very good results on the well-known problem of
unsupervised domain adaption, where labels but not samples have to be
predicted.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1"&gt;Johannes Schneider&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Nonmyopic Approach to Cost-Constrained Bayesian Optimization. (arXiv:2106.06079v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06079</id>
        <link href="http://arxiv.org/abs/2106.06079"/>
        <updated>2021-06-14T01:38:53.361Z</updated>
        <summary type="html"><![CDATA[Bayesian optimization (BO) is a popular method for optimizing
expensive-to-evaluate black-box functions. BO budgets are typically given in
iterations, which implicitly assumes each evaluation has the same cost. In
fact, in many BO applications, evaluation costs vary significantly in different
regions of the search space. In hyperparameter optimization, the time spent on
neural network training increases with layer size; in clinical trials, the
monetary cost of drug compounds vary; and in optimal control, control actions
have differing complexities. Cost-constrained BO measures convergence with
alternative cost metrics such as time, money, or energy, for which the sample
efficiency of standard BO methods is ill-suited. For cost-constrained BO, cost
efficiency is far more important than sample efficiency. In this paper, we
formulate cost-constrained BO as a constrained Markov decision process (CMDP),
and develop an efficient rollout approximation to the optimal CMDP policy that
takes both the cost and future iterations into account. We validate our method
on a collection of hyperparameter optimization problems as well as a sensor set
selection application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1"&gt;Eric Hans Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eriksson_D/0/1/0/all/0/1"&gt;David Eriksson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perrone_V/0/1/0/all/0/1"&gt;Valerio Perrone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeger_M/0/1/0/all/0/1"&gt;Matthias Seeger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Expert Annotation Differences in Animal Behavior. (arXiv:2106.06114v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06114</id>
        <link href="http://arxiv.org/abs/2106.06114"/>
        <updated>2021-06-14T01:38:53.354Z</updated>
        <summary type="html"><![CDATA[Hand-annotated data can vary due to factors such as subjective differences,
intra-rater variability, and differing annotator expertise. We study
annotations from different experts who labelled the same behavior classes on a
set of animal behavior videos, and observe a variation in annotation styles. We
propose a new method using program synthesis to help interpret annotation
differences for behavior analysis. Our model selects relevant trajectory
features and learns a temporal filter as part of a program, which corresponds
to estimated importance an annotator places on that feature at each timestamp.
Our experiments on a dataset from behavioral neuroscience demonstrate that
compared to baseline approaches, our method is more accurate at capturing
annotator labels and learns interpretable temporal filters. We believe that our
method can lead to greater reproducibility of behavior annotations used in
scientific studies. We plan to release our code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tjandrasuwita_M/0/1/0/all/0/1"&gt;Megan Tjandrasuwita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jennifer J. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1"&gt;Ann Kennedy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Swarat Chaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yisong Yue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FiSH: Fair Spatial Hotspots. (arXiv:2106.06049v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06049</id>
        <link href="http://arxiv.org/abs/2106.06049"/>
        <updated>2021-06-14T01:38:53.347Z</updated>
        <summary type="html"><![CDATA[Pervasiveness of tracking devices and enhanced availability of spatially
located data has deepened interest in using them for various policy
interventions, through computational data analysis tasks such as spatial hot
spot detection. In this paper, we consider, for the first time to our best
knowledge, fairness in detecting spatial hot spots. We motivate the need for
ensuring fairness through statistical parity over the collective population
covered across chosen hot spots. We then characterize the task of identifying a
diverse set of solutions in the noteworthiness-fairness trade-off spectrum, to
empower the user to choose a trade-off justified by the policy domain. Being a
novel task formulation, we also develop a suite of evaluation metrics for fair
hot spots, motivated by the need to evaluate pertinent aspects of the task. We
illustrate the computational infeasibility of identifying fair hot spots using
naive and/or direct approaches and devise a method, codenamed {\it FiSH}, for
efficiently identifying high-quality, fair and diverse sets of spatial hot
spots. FiSH traverses the tree-structured search space using heuristics that
guide it towards identifying effective and fair sets of spatial hot spots.
Through an extensive empirical analysis over a real-world dataset from the
domain of human development, we illustrate that FiSH generates high-quality
solutions at fast response times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+P_D/0/1/0/all/0/1"&gt;Deepak P&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Sowmya S Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven battery operation for energy arbitrage using rainbow deep reinforcement learning. (arXiv:2106.06061v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06061</id>
        <link href="http://arxiv.org/abs/2106.06061"/>
        <updated>2021-06-14T01:38:53.339Z</updated>
        <summary type="html"><![CDATA[As the world seeks to become more sustainable, intelligent solutions are
needed to increase the penetration of renewable energy. In this paper, the
model-free deep reinforcement learning algorithm Rainbow Deep Q-Networks is
used to control a battery in a small microgrid to perform energy arbitrage and
more efficiently utilise solar and wind energy sources. The grid operates with
its own demand and renewable generation based on a dataset collected at Keele
University, as well as using dynamic energy pricing from a real wholesale
energy market. Four scenarios are tested including using demand and price
forecasting produced with local weather data. The algorithm and its
subcomponents are evaluated against two continuous control benchmarks with
Rainbow able to outperform all other method. This research shows the importance
of using the distributional approach for reinforcement learning when working
with complex environments and reward functions, as well as how it can be used
to visualise and contextualise the agent's behaviour for real-world
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Harrold_D/0/1/0/all/0/1"&gt;Daniel J. B. Harrold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jun Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Zhong Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Decentralized Adaptive Momentum Method for Solving a Class of Min-Max Optimization Problems. (arXiv:2106.06075v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.06075</id>
        <link href="http://arxiv.org/abs/2106.06075"/>
        <updated>2021-06-14T01:38:53.331Z</updated>
        <summary type="html"><![CDATA[Min-max saddle point games have recently been intensely studied, due to their
wide range of applications, including training Generative Adversarial
Networks~(GANs). However, most of the recent efforts for solving them are
limited to special regimes such as convex-concave games. Further, it is
customarily assumed that the underlying optimization problem is solved either
by a single machine or in the case of multiple machines connected in
centralized fashion, wherein each one communicates with a central node. The
latter approach becomes challenging, when the underlying communications network
has low bandwidth. In addition, privacy considerations may dictate that certain
nodes can communicate with a subset of other nodes. Hence, it is of interest to
develop methods that solve min-max games in a decentralized manner. To that
end, we develop a decentralized adaptive momentum (ADAM)-type algorithm for
solving min-max optimization problem under the condition that the objective
function satisfies a Minty Variational Inequality condition, which is a
generalization to convex-concave case. The proposed method overcomes
shortcomings of recent non-adaptive gradient-based decentralized algorithms for
min-max optimization problems that do not perform well in practice and require
careful tuning. In this paper, we obtain non-asymptotic rates of convergence of
the proposed algorithm (coined DADAM$^3$) for finding a (stochastic)
first-order Nash equilibrium point and subsequently evaluate its performance on
training GANs. The extensive empirical evaluation shows that DADAM$^3$
outperforms recently developed methods, including decentralized optimistic
stochastic gradient for solving such min-max problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Barazandeh_B/0/1/0/all/0/1"&gt;Babak Barazandeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tianjian Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Michailidis_G/0/1/0/all/0/1"&gt;George Michailidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Ensemble Approach Towards Adversarial Robustness. (arXiv:2106.05996v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05996</id>
        <link href="http://arxiv.org/abs/2106.05996"/>
        <updated>2021-06-14T01:38:53.306Z</updated>
        <summary type="html"><![CDATA[It is a known phenomenon that adversarial robustness comes at a cost to
natural accuracy. To improve this trade-off, this paper proposes an ensemble
approach that divides a complex robust-classification task into simpler
subtasks. Specifically, fractal divide derives multiple training sets from the
training data, and fractal aggregation combines inference outputs from multiple
classifiers that are trained on those sets. The resulting ensemble classifiers
have a unique property that ensures robustness for an input if certain
don't-care conditions are met. The new techniques are evaluated on MNIST and
Fashion-MNIST, with no adversarial training. The MNIST classifier has 99%
natural accuracy, 70% measured robustness and 36.9% provable robustness, within
L2 distance of 2. The Fashion-MNIST classifier has 90% natural accuracy, 54.5%
measured robustness and 28.2% provable robustness, within L2 distance of 1.5.
Both results are new state of the art, and we also present new state-of-the-art
binary results on challenging label-pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1"&gt;Haifeng Qian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Hierarchical Structures with Continuous Recursive Neural Networks. (arXiv:2106.06038v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06038</id>
        <link href="http://arxiv.org/abs/2106.06038"/>
        <updated>2021-06-14T01:38:53.299Z</updated>
        <summary type="html"><![CDATA[Recursive Neural Networks (RvNNs), which compose sequences according to their
underlying hierarchical syntactic structure, have performed well in several
natural language processing tasks compared to similar models without structural
biases. However, traditional RvNNs are incapable of inducing the latent
structure in a plain text sequence on their own. Several extensions have been
proposed to overcome this limitation. Nevertheless, these extensions tend to
rely on surrogate gradients or reinforcement learning at the cost of higher
bias or variance. In this work, we propose Continuous Recursive Neural Network
(CRvNN) as a backpropagation-friendly alternative to address the aforementioned
limitations. This is done by incorporating a continuous relaxation to the
induced structure. We demonstrate that CRvNN achieves strong performance in
challenging synthetic tasks such as logical inference and ListOps. We also show
that CRvNN performs comparably or better than prior latent structure models on
real-world tasks such as sentiment analysis and natural language inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1"&gt;Jishnu Ray Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1"&gt;Cornelia Caragea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning. (arXiv:2106.06047v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06047</id>
        <link href="http://arxiv.org/abs/2106.06047"/>
        <updated>2021-06-14T01:38:53.293Z</updated>
        <summary type="html"><![CDATA[Federated learning is an emerging research paradigm enabling collaborative
training of machine learning models among different organizations while keeping
data private at each institution. Despite recent progress, there remain
fundamental challenges such as lack of convergence and potential for
catastrophic forgetting in federated learning across real-world heterogeneous
devices. In this paper, we demonstrate that attention-based architectures
(e.g., Transformers) are fairly robust to distribution shifts and hence improve
federated learning over heterogeneous data. Concretely, we conduct the first
rigorous empirical investigation of different neural architectures across a
range of federated algorithms, real-world benchmarks, and heterogeneous data
splits. Our experiments show that simply replacing convolutional networks with
Transformers can greatly reduce catastrophic forgetting of previous devices,
accelerate convergence, and reach a better global model, especially when
dealing with heterogeneous data. We will release our code and pretrained models
at https://github.com/Liangqiong/ViT-FL-main to encourage future exploration in
robust architectures as an alternative to current research efforts on the
optimization front.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Liangqiong Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuyin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yingda Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Feifei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Variational Gaussian Processes via Harmonic Kernel Decomposition. (arXiv:2106.05992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05992</id>
        <link href="http://arxiv.org/abs/2106.05992"/>
        <updated>2021-06-14T01:38:53.275Z</updated>
        <summary type="html"><![CDATA[We introduce a new scalable variational Gaussian process approximation which
provides a high fidelity approximation while retaining general applicability.
We propose the harmonic kernel decomposition (HKD), which uses Fourier series
to decompose a kernel as a sum of orthogonal kernels. Our variational
approximation exploits this orthogonality to enable a large number of inducing
points at a low computational cost. We demonstrate that, on a range of
regression and classification problems, our approach can exploit input space
symmetries such as translations and reflections, and it significantly
outperforms standard variational methods in scalability and accuracy. Notably,
our approach achieves state-of-the-art results on CIFAR-10 among pure GP
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shengyang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jiaxin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"&gt;Andrew Gordon Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1"&gt;Roger Grosse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesising Reinforcement Learning Policies through Set-Valued Inductive Rule Learning. (arXiv:2106.06009v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.06009</id>
        <link href="http://arxiv.org/abs/2106.06009"/>
        <updated>2021-06-14T01:38:53.269Z</updated>
        <summary type="html"><![CDATA[Today's advanced Reinforcement Learning algorithms produce black-box
policies, that are often difficult to interpret and trust for a person. We
introduce a policy distilling algorithm, building on the CN2 rule mining
algorithm, that distills the policy into a rule-based decision system. At the
core of our approach is the fact that an RL process does not just learn a
policy, a mapping from states to actions, but also produces extra
meta-information, such as action values indicating the quality of alternative
actions. This meta-information can indicate whether more than one action is
near-optimal for a certain state. We extend CN2 to make it able to leverage
knowledge about equally-good actions to distill the policy into fewer rules,
increasing its interpretability by a person. Then, to ensure that the rules
explain a valid, non-degenerate policy, we introduce a refinement algorithm
that fine-tunes the rules to obtain good performance when executed in the
environment. We demonstrate the applicability of our algorithm on the Mario AI
benchmark, a complex task that requires modern reinforcement learning
algorithms including neural networks. The explanations we produce capture the
learned policy in only a few rules, that allow a person to understand what the
black-box agent learned. Source code:
https://gitlab.ai.vub.ac.be/yocoppen/svcn2]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Coppens_Y/0/1/0/all/0/1"&gt;Youri Coppens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steckelmacher_D/0/1/0/all/0/1"&gt;Denis Steckelmacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jonker_C/0/1/0/all/0/1"&gt;Catholijn M. Jonker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowe_A/0/1/0/all/0/1"&gt;Ann Now&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Theoretic Evaluation of Privacy-Leakage, Interpretability, and Transferability for a Novel Trustworthy AI Framework. (arXiv:2106.06046v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06046</id>
        <link href="http://arxiv.org/abs/2106.06046"/>
        <updated>2021-06-14T01:38:53.262Z</updated>
        <summary type="html"><![CDATA[Guidelines and principles of trustworthy AI should be adhered to in practice
during the development of AI systems. This work suggests a novel information
theoretic trustworthy AI framework based on the hypothesis that information
theory enables taking into account the ethical AI principles during the
development of machine learning and deep learning models via providing a way to
study and optimize the inherent tradeoffs between trustworthy AI principles. A
unified approach to "privacy-preserving interpretable and transferable
learning" is presented via introducing the information theoretic measures for
privacy-leakage, interpretability, and transferability. A technique based on
variational optimization, employing conditionally deep autoencoders, is
developed for practically calculating the defined information theoretic
measures for privacy-leakage, interpretability, and transferability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;Mohit Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1"&gt;Bernhard A. Moser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1"&gt;Lukas Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freudenthaler_B/0/1/0/all/0/1"&gt;Bernhard Freudenthaler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Framework for Sensing and Modeling Interference in IoT Frequency Bands. (arXiv:2106.06010v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06010</id>
        <link href="http://arxiv.org/abs/2106.06010"/>
        <updated>2021-06-14T01:38:53.255Z</updated>
        <summary type="html"><![CDATA[Spectrum scarcity has surfaced as a prominent concern in wireless radio
communications with the emergence of new technologies over the past few years.
As a result, there is growing need for better understanding of the spectrum
occupancy with newly emerging access technologies supporting the Internet of
Things. In this paper, we present a framework to capture and model the traffic
behavior of short-time spectrum occupancy for IoT applications in the shared
bands to determine the existing interference. The proposed capturing method
utilizes a software defined radio to monitor the short bursts of IoT
transmissions by capturing the time series data which is converted to power
spectral density to extract the observed occupancy. Furthermore, we propose the
use of an unsupervised machine learning technique to enhance conventionally
implemented energy detection methods. Our experimental results show that the
temporal and frequency behavior of the spectrum can be well-captured using the
combination of two models, namely, semi-Markov chains and a
Poisson-distribution arrival rate. We conduct an extensive measurement campaign
in different urban environments and incorporate the spatial effect on the IoT
shared spectrum.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Homssi_B/0/1/0/all/0/1"&gt;Bassel Al Homssi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Hourani_A/0/1/0/all/0/1"&gt;Akram Al-Hourani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krusevac_Z/0/1/0/all/0/1"&gt;Zarko Krusevac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rowe_W/0/1/0/all/0/1"&gt;Wayne S T Rowe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm. (arXiv:2106.06027v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06027</id>
        <link href="http://arxiv.org/abs/2106.06027"/>
        <updated>2021-06-14T01:38:53.248Z</updated>
        <summary type="html"><![CDATA[Sparse adversarial attacks can fool deep neural networks (DNNs) by only
perturbing a few pixels (regularized by l_0 norm). Recent efforts combine it
with another l_infty imperceptible on the perturbation magnitudes. The
resultant sparse and imperceptible attacks are practically relevant, and
indicate an even higher vulnerability of DNNs that we usually imagined.
However, such attacks are more challenging to generate due to the optimization
difficulty by coupling the l_0 regularizer and box constraints with a
non-convex objective. In this paper, we address this challenge by proposing a
homotopy algorithm, to jointly tackle the sparsity and the perturbation bound
in one unified framework. Each iteration, the main step of our algorithm is to
optimize an l_0-regularized adversarial loss, by leveraging the nonmonotone
Accelerated Proximal Gradient Method (nmAPG) for nonconvex programming; it is
followed by an l_0 change control step, and an optional post-attack step
designed to escape bad local minima. We also extend the algorithm to handling
the structural sparsity regularizer. We extensively examine the effectiveness
of our proposed homotopy attack for both targeted and non-targeted attack
scenarios, on CIFAR-10 and ImageNet datasets. Compared to state-of-the-art
methods, our homotopy attack leads to significantly fewer perturbations, e.g.,
reducing 42.91% on CIFAR-10 and 75.03% on ImageNet (average case, targeted
attack), at similar maximal perturbation magnitudes, when still achieving 100%
attack success rates. Our codes are available at:
https://github.com/VITA-Group/SparseADV_Homotopy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Mingkang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A self-adapting super-resolution structures framework for automatic design of GAN. (arXiv:2106.06011v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06011</id>
        <link href="http://arxiv.org/abs/2106.06011"/>
        <updated>2021-06-14T01:38:53.238Z</updated>
        <summary type="html"><![CDATA[With the development of deep learning, the single super-resolution image
reconstruction network models are becoming more and more complex. Small changes
in hyperparameters of the models have a greater impact on model performance. In
the existing works, experts have gradually explored a set of optimal model
parameters based on empirical values or performing brute-force search. In this
paper, we introduce a new super-resolution image reconstruction generative
adversarial network framework, and a Bayesian optimization method used to
optimizing the hyperparameters of the generator and discriminator. The
generator is made by self-calibrated convolution, and discriminator is made by
convolution lays. We have defined the hyperparameters such as the number of
network layers and the number of neurons. Our method adopts Bayesian
optimization as a optimization policy of GAN in our model. Not only can find
the optimal hyperparameter solution automatically, but also can construct a
super-resolution image reconstruction network, reducing the manual workload.
Experiments show that Bayesian optimization can search the optimal solution
earlier than the other two optimization algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yibo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haidi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yiming Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shunyao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingliang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coordinate Independent Convolutional Networks -- Isometry and Gauge Equivariant Convolutions on Riemannian Manifolds. (arXiv:2106.06020v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06020</id>
        <link href="http://arxiv.org/abs/2106.06020"/>
        <updated>2021-06-14T01:38:53.228Z</updated>
        <summary type="html"><![CDATA[Motivated by the vast success of deep convolutional networks, there is a
great interest in generalizing convolutions to non-Euclidean manifolds. A major
complication in comparison to flat spaces is that it is unclear in which
alignment a convolution kernel should be applied on a manifold. The underlying
reason for this ambiguity is that general manifolds do not come with a
canonical choice of reference frames (gauge). Kernels and features therefore
have to be expressed relative to arbitrary coordinates. We argue that the
particular choice of coordinatization should not affect a network's inference
-- it should be coordinate independent. A simultaneous demand for coordinate
independence and weight sharing is shown to result in a requirement on the
network to be equivariant under local gauge transformations (changes of local
reference frames). The ambiguity of reference frames depends thereby on the
G-structure of the manifold, such that the necessary level of gauge
equivariance is prescribed by the corresponding structure group G. Coordinate
independent convolutions are proven to be equivariant w.r.t. those isometries
that are symmetries of the G-structure. The resulting theory is formulated in a
coordinate free fashion in terms of fiber bundles. To exemplify the design of
coordinate independent convolutions, we implement a convolutional network on
the M\"obius strip. The generality of our differential geometric formulation of
convolutional networks is demonstrated by an extensive literature review which
explains a large number of Euclidean CNNs, spherical CNNs and CNNs on general
surfaces as specific instances of coordinate independent convolutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1"&gt;Maurice Weiler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1"&gt;Patrick Forr&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verlinde_E/0/1/0/all/0/1"&gt;Erik Verlinde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DECORE: Deep Compression with Reinforcement Learning. (arXiv:2106.06091v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.06091</id>
        <link href="http://arxiv.org/abs/2106.06091"/>
        <updated>2021-06-14T01:38:53.217Z</updated>
        <summary type="html"><![CDATA[Deep learning has become an increasingly popular and powerful option for
modern pattern recognition systems. However, many deep neural networks have
millions to billions of parameters, making them untenable for real-world
applications with constraints on memory or latency. As a result, powerful
network compression techniques are a must for the widespread adoption of deep
learning. We present DECORE, a reinforcement learning approach to automate the
network compression process. Using a simple policy gradient method to learn
which neurons or channels to keep or remove, we are able to achieve compression
rates 3x to 5x greater than contemporary approaches. In contrast with other
architecture search methods, DECORE is simple and quick to train, requiring
only a few hours of training on 1 GPU. When applied to standard network
architectures on different datasets, our approach achieves 11x to 103x
compression on different architectures while maintaining accuracies similar to
those of the original, large networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alwani_M/0/1/0/all/0/1"&gt;Manoj Alwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madhavan_V/0/1/0/all/0/1"&gt;Vashisht Madhavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Vision with Sparse Mixture of Experts. (arXiv:2106.05974v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05974</id>
        <link href="http://arxiv.org/abs/2106.05974"/>
        <updated>2021-06-14T01:38:53.189Z</updated>
        <summary type="html"><![CDATA[Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent
scalability in Natural Language Processing. In Computer Vision, however, almost
all performant networks are "dense", that is, every input is processed by every
parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision
Transformer, that is scalable and competitive with the largest dense networks.
When applied to image recognition, V-MoE matches the performance of
state-of-the-art networks, while requiring as little as half of the compute at
inference time. Further, we propose an extension to the routing algorithm that
can prioritize subsets of each input across the entire batch, leading to
adaptive per-image compute. This allows V-MoE to trade-off performance and
compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE
to scale vision models, and train a 15B parameter model that attains 90.35% on
ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riquelme_C/0/1/0/all/0/1"&gt;Carlos Riquelme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1"&gt;Joan Puigcerver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1"&gt;Basil Mustafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_M/0/1/0/all/0/1"&gt;Maxim Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1"&gt;Rodolphe Jenatton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Susano Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1"&gt;Daniel Keysers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChemRL-GEM: Geometry Enhanced Molecular Representation Learning for Property Prediction. (arXiv:2106.06130v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06130</id>
        <link href="http://arxiv.org/abs/2106.06130"/>
        <updated>2021-06-14T01:38:53.181Z</updated>
        <summary type="html"><![CDATA[Effective molecular representation learning is of great importance to
facilitate molecular property prediction, which is a fundamental task for the
drug and material industry. Recent advances in graph neural networks (GNNs)
have shown great promise in applying GNNs for molecular representation
learning. Moreover, a few recent studies have also demonstrated successful
applications of self-supervised learning methods to pre-train the GNNs to
overcome the problem of insufficient labeled molecules. However, existing GNNs
and pre-training strategies usually treat molecules as topological graph data
without fully utilizing the molecular geometry information. Whereas, the
three-dimensional (3D) spatial structure of a molecule, a.k.a molecular
geometry, is one of the most critical factors for determining molecular
physical, chemical, and biological properties. To this end, we propose a novel
Geometry Enhanced Molecular representation learning method (GEM) for Chemical
Representation Learning (ChemRL). At first, we design a geometry-based GNN
architecture that simultaneously models atoms, bonds, and bond angles in a
molecule. To be specific, we devised double graphs for a molecule: The first
one encodes the atom-bond relations; The second one encodes bond-angle
relations. Moreover, on top of the devised GNN architecture, we propose several
novel geometry-level self-supervised learning strategies to learn spatial
knowledge by utilizing the local and global molecular 3D structures. We compare
ChemRL-GEM with various state-of-the-art (SOTA) baselines on different
molecular benchmarks and exhibit that ChemRL-GEM can significantly outperform
all baselines in both regression and classification tasks. For example, the
experimental results show an overall improvement of $8.8\%$ on average compared
to SOTA baselines on the regression tasks, demonstrating the superiority of the
proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xiaomin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lihang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jieqiong Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Donglong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shanzhuo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingbo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haifeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks for Natural Language Processing: A Survey. (arXiv:2106.06090v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06090</id>
        <link href="http://arxiv.org/abs/2106.06090"/>
        <updated>2021-06-14T01:38:53.155Z</updated>
        <summary type="html"><![CDATA[Deep learning has become the dominant approach in coping with various tasks
in Natural LanguageProcessing (NLP). Although text inputs are typically
represented as a sequence of tokens, there isa rich variety of NLP problems
that can be best expressed with a graph structure. As a result, thereis a surge
of interests in developing new deep learning techniques on graphs for a large
numberof NLP tasks. In this survey, we present a comprehensive overview onGraph
Neural Networks(GNNs) for Natural Language Processing. We propose a new
taxonomy of GNNs for NLP, whichsystematically organizes existing research of
GNNs for NLP along three axes: graph construction,graph representation
learning, and graph based encoder-decoder models. We further introducea large
number of NLP applications that are exploiting the power of GNNs and summarize
thecorresponding benchmark datasets, evaluation metrics, and open-source codes.
Finally, we discussvarious outstanding challenges for making the full use of
GNNs for NLP as well as future researchdirections. To the best of our
knowledge, this is the first comprehensive overview of Graph NeuralNetworks for
Natural Language Processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1"&gt;Kai Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaojie Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1"&gt;Hanning Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shucheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1"&gt;Jian Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1"&gt;Bo Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis. (arXiv:2010.05690v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05690</id>
        <link href="http://arxiv.org/abs/2010.05690"/>
        <updated>2021-06-14T01:38:53.136Z</updated>
        <summary type="html"><![CDATA[The issue of COVID-19, increasing with a massive mortality rate. This led to
the WHO declaring it as a pandemic. In this situation, it is crucial to perform
efficient and fast diagnosis. The reverse transcript polymerase chain reaction
(RTPCR) test is conducted to detect the presence of SARS-CoV-2. This test is
time-consuming and instead chest CT (or Chest X-ray) can be used for a fast and
accurate diagnosis. Automated diagnosis is considered to be important as it
reduces human effort and provides accurate and low-cost tests. The
contributions of our research are three-fold. First, it is aimed to analyse the
behaviour and performance of variant vision models ranging from Inception to
NAS networks with the appropriate fine-tuning procedure. Second, the behaviour
of these models is visually analysed by plotting CAMs for individual networks
and determining classification performance with AUCROC curves. Thirdly, stacked
ensembles techniques are imparted to provide higher generalisation on combining
the fine-tuned models, in which six ensemble neural networks are designed by
combining the existing fine-tuned networks. Implying these stacked ensembles
provides a great generalization to the models. The ensemble model designed by
combining all the fine-tuned networks obtained a state-of-the-art accuracy
score of 99.17%. The precision and recall for the COVID-19 class are 99.99% and
89.79% respectively, which resembles the robustness of the stacked ensembles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+B_L/0/1/0/all/0/1"&gt;Lalith Bharadwaj B&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boddeda_R/0/1/0/all/0/1"&gt;Rohit Boddeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1"&gt;Sai Vardhan K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+G_M/0/1/0/all/0/1"&gt;Madhu G&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Sequences as Distributions with Uncertainty for Sequential Recommendation. (arXiv:2106.06165v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.06165</id>
        <link href="http://arxiv.org/abs/2106.06165"/>
        <updated>2021-06-14T01:38:53.118Z</updated>
        <summary type="html"><![CDATA[The sequential patterns within the user interactions are pivotal for
representing the user's preference and capturing latent relationships among
items. The recent advancements of sequence modeling by Transformers advocate
the community to devise more effective encoders for the sequential
recommendation. Most existing sequential methods assume users are
deterministic. However, item-item transitions might fluctuate significantly in
several item aspects and exhibit randomness of user interests. This
\textit{stochastic characteristics} brings up a solid demand to include
uncertainties in representing sequences and items. Additionally, modeling
sequences and items with uncertainties expands users' and items' interaction
spaces, thus further alleviating cold-start problems.

In this work, we propose a Distribution-based Transformer for Sequential
Recommendation (DT4SR), which injects uncertainties into sequential modeling.
We use Elliptical Gaussian distributions to describe items and sequences with
uncertainty. We describe the uncertainty in items and sequences as Elliptical
Gaussian distribution. And we adopt Wasserstein distance to measure the
similarity between distributions. We devise two novel Trans-formers for
modeling mean and covariance, which guarantees the positive-definite property
of distributions. The proposed method significantly outperforms the
state-of-the-art methods. The experiments on three benchmark datasets also
demonstrate its effectiveness in alleviating cold-start issues. The code is
available inhttps://github.com/DyGRec/DT4SR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Ziwei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lei Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Weakly Supervised Action Segmentation Using Mutual Consistency. (arXiv:1904.03116v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.03116</id>
        <link href="http://arxiv.org/abs/1904.03116"/>
        <updated>2021-06-14T01:38:53.101Z</updated>
        <summary type="html"><![CDATA[Action segmentation is the task of predicting the actions for each frame of a
video. As obtaining the full annotation of videos for action segmentation is
expensive, weakly supervised approaches that can learn only from transcripts
are appealing. In this paper, we propose a novel end-to-end approach for weakly
supervised action segmentation based on a two-branch neural network. The two
branches of our network predict two redundant but different representations for
action segmentation and we propose a novel mutual consistency (MuCon) loss that
enforces the consistency of the two redundant representations. Using the MuCon
loss together with a loss for transcript prediction, our proposed approach
achieves the accuracy of state-of-the-art approaches while being $14$ times
faster to train and $20$ times faster during inference. The MuCon loss proves
beneficial even in the fully supervised setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Souri_Y/0/1/0/all/0/1"&gt;Yaser Souri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1"&gt;Mohsen Fayyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minciullo_L/0/1/0/all/0/1"&gt;Luca Minciullo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1"&gt;Gianpiero Francesca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1"&gt;Juergen Gall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Knowledge Graph Alignment by Probabilistic Reasoning and Semantic Embedding. (arXiv:2105.05596v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05596</id>
        <link href="http://arxiv.org/abs/2105.05596"/>
        <updated>2021-06-14T01:38:53.094Z</updated>
        <summary type="html"><![CDATA[Knowledge Graph (KG) alignment is to discover the mappings (i.e., equivalent
entities, relations, and others) between two KGs. The existing methods can be
divided into the embedding-based models, and the conventional reasoning and
lexical matching based systems. The former compute the similarity of entities
via their cross-KG embeddings, but they usually rely on an ideal supervised
learning setting for good performance and lack appropriate reasoning to avoid
logically wrong mappings; while the latter address the reasoning issue but are
poor at utilizing the KG graph structures and the entity contexts. In this
study, we aim at combining the above two solutions and thus propose an
iterative framework named PRASE which is based on probabilistic reasoning and
semantic embedding. It learns the KG embeddings via entity mappings from a
probabilistic reasoning system named PARIS, and feeds the resultant entity
mappings and embeddings back into PARIS for augmentation. The PRASE framework
is compatible with different embedding-based models, and our experiments on
multiple datasets have demonstrated its state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhiyuan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yuejia Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pedestrian Attribute Recognition in Video Surveillance Scenarios Based on View-attribute Attention Localization. (arXiv:2106.06485v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06485</id>
        <link href="http://arxiv.org/abs/2106.06485"/>
        <updated>2021-06-14T01:38:53.070Z</updated>
        <summary type="html"><![CDATA[Pedestrian attribute recognition in surveillance scenarios is still a
challenging task due to inaccurate localization of specific attributes. In this
paper, we propose a novel view-attribute localization method based on attention
(VALA), which relies on the strong relevance between attributes and views to
capture specific view-attributes and to localize attribute-corresponding areas
by attention mechanism. A specific view-attribute is composed by the extracted
attribute feature and four view scores which are predicted by view predictor as
the confidences for attribute from different views. View-attribute is then
delivered back to shallow network layers for supervising deep feature
extraction. To explore the location of a view-attribute, regional attention is
introduced to aggregate spatial information of the input attribute feature in
height and width direction for constraining the image into a narrow range.
Moreover, the inter-channel dependency of view-feature is embedded in the above
two spatial directions. An attention attribute-specific region is gained after
fining the narrow range by balancing the ratio of channel dependencies between
height and width branches. The final view-attribute recognition outcome is
obtained by combining the output of regional attention with the view scores
from view predictor. Experiments on three wide datasets (RAP, RAPv2, PETA, and
PA-100K) demonstrate the effectiveness of our approach compared with
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weichen Chen&lt;/a&gt; (1) &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xinyi Yu&lt;/a&gt; (1) &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1"&gt;Linlin Ou&lt;/a&gt; (1) ((1) Collage of Information Engineering, Zhejiang University of Technology, Hangzhou, China)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Neural Networks: A Survey. (arXiv:2102.04906v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04906</id>
        <link href="http://arxiv.org/abs/2102.04906"/>
        <updated>2021-06-14T01:38:53.062Z</updated>
        <summary type="html"><![CDATA[Dynamic neural network is an emerging research topic in deep learning.
Compared to static models which have fixed computational graphs and parameters
at the inference stage, dynamic networks can adapt their structures or
parameters to different inputs, leading to notable advantages in terms of
accuracy, computational efficiency, adaptiveness, etc. In this survey, we
comprehensively review this rapidly developing area by dividing dynamic
networks into three main categories: 1) instance-wise dynamic models that
process each instance with data-dependent architectures or parameters; 2)
spatial-wise dynamic networks that conduct adaptive computation with respect to
different spatial locations of image data and 3) temporal-wise dynamic models
that perform adaptive inference along the temporal dimension for sequential
data such as videos and texts. The important research problems of dynamic
networks, e.g., architecture design, decision making scheme, optimization
technique and applications, are reviewed systematically. Finally, we discuss
the open problems in this field together with interesting future research
directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yizeng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shiji Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Le Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Honghui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yulin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViT-Inception-GAN for Image Colourising. (arXiv:2106.06321v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06321</id>
        <link href="http://arxiv.org/abs/2106.06321"/>
        <updated>2021-06-14T01:38:53.052Z</updated>
        <summary type="html"><![CDATA[Studies involving colourising images has been garnering researchers' keen
attention over time, assisted by significant advances in various Machine
Learning techniques and compute power availability. Traditionally, colourising
images have been an intricate task that gave a substantial degree of freedom
during the assignment of chromatic information. In our proposed method, we
attempt to colourise images using Vision Transformer - Inception - Generative
Adversarial Network (ViT-I-GAN), which has an Inception-v3 fusion embedding in
the generator. For a stable and robust network, we have used Vision Transformer
(ViT) as the discriminator. We trained the model on the Unsplash and the COCO
dataset for demonstrating the improvement made by the Inception-v3 embedding.
We have compared the results between ViT-GANs with and without Inception-v3
embedding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bana_T/0/1/0/all/0/1"&gt;Tejas Bana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loya_J/0/1/0/all/0/1"&gt;Jatan Loya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1"&gt;Siddhant Kulkarni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Genetic U-Net: Automatically Designed Deep Networks for Retinal Vessel Segmentation Using a Genetic Algorithm. (arXiv:2010.15560v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15560</id>
        <link href="http://arxiv.org/abs/2010.15560"/>
        <updated>2021-06-14T01:38:53.045Z</updated>
        <summary type="html"><![CDATA[Recently, many methods based on hand-designed convolutional neural networks
(CNNs) have achieved promising results in automatic retinal vessel
segmentation. However, these CNNs remain constrained in capturing retinal
vessels in complex fundus images. To improve their segmentation performance,
these CNNs tend to have many parameters, which may lead to overfitting and high
computational complexity. Moreover, the manual design of competitive CNNs is
time-consuming and requires extensive empirical knowledge. Herein, a novel
automated design method, called Genetic U-Net, is proposed to generate a
U-shaped CNN that can achieve better retinal vessel segmentation but with fewer
architecture-based parameters, thereby addressing the above issues. First, we
devised a condensed but flexible search space based on a U-shaped
encoder-decoder. Then, we used an improved genetic algorithm to identify
better-performing architectures in the search space and investigated the
possibility of finding a superior network architecture with fewer parameters.
The experimental results show that the architecture obtained using the proposed
method offered a superior performance with less than 1% of the number of the
original U-Net parameters in particular and with significantly fewer parameters
than other state-of-the-art models. Furthermore, through in-depth investigation
of the experimental results, several effective operations and patterns of
networks to generate superior retinal vessel segmentations were identified.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiahong Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Zhun Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning via Persistency of Excitation. (arXiv:2106.02078v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02078</id>
        <link href="http://arxiv.org/abs/2106.02078"/>
        <updated>2021-06-14T01:38:53.036Z</updated>
        <summary type="html"><![CDATA[Improving adversarial robustness of neural networks remains a major
challenge. Fundamentally, training a network is a parameter estimation problem.
In adaptive control theory, maintaining persistency of excitation (PoE) is
integral to ensuring convergence of parameter estimates in dynamical systems to
their robust optima. In this work, we show that network training using gradient
descent is equivalent to a dynamical system parameter estimation problem.
Leveraging this relationship, we prove a sufficient condition for PoE of
gradient descent is achieved when the learning rate is less than the inverse of
the Lipschitz constant of the gradient of loss function. We provide an
efficient technique for estimating the corresponding Lipschitz constant using
extreme value theory and demonstrate that by only scaling the learning rate
schedule we can increase adversarial accuracy by up to 15% points on benchmark
datasets. Our approach also universally increases the adversarial accuracy by
0.1% to 0.3% points in various state-of-the-art adversarially trained models on
the AutoAttack benchmark, where every small margin of improvement is
significant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sridhar_K/0/1/0/all/0/1"&gt;Kaustubh Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sokolsky_O/0/1/0/all/0/1"&gt;Oleg Sokolsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_I/0/1/0/all/0/1"&gt;Insup Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Weimer_J/0/1/0/all/0/1"&gt;James Weimer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Refining Pseudo Labels with Clustering Consensus over Generations for Unsupervised Object Re-identification. (arXiv:2106.06133v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06133</id>
        <link href="http://arxiv.org/abs/2106.06133"/>
        <updated>2021-06-14T01:38:53.018Z</updated>
        <summary type="html"><![CDATA[Unsupervised object re-identification targets at learning discriminative
representations for object retrieval without any annotations. Clustering-based
methods conduct training with the generated pseudo labels and currently
dominate this research direction. However, they still suffer from the issue of
pseudo label noise. To tackle the challenge, we propose to properly estimate
pseudo label similarities between consecutive training generations with
clustering consensus and refine pseudo labels with temporally propagated and
ensembled pseudo labels. To the best of our knowledge, this is the first
attempt to leverage the spirit of temporal ensembling to improve classification
with dynamically changing classes over generations. The proposed pseudo label
refinery strategy is simple yet effective and can be seamlessly integrated into
existing clustering-based unsupervised re-identification methods. With our
proposed approach, state-of-the-art method can be further boosted with up to
8.8% mAP improvements on the challenging MSMT17 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yixiao Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Segmentation Without Labels with Large-Scale Generative Models. (arXiv:2006.04988v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04988</id>
        <link href="http://arxiv.org/abs/2006.04988"/>
        <updated>2021-06-14T01:38:53.010Z</updated>
        <summary type="html"><![CDATA[The recent rise of unsupervised and self-supervised learning has dramatically
reduced the dependency on labeled data, providing effective image
representations for transfer to downstream vision tasks. Furthermore, recent
works employed these representations in a fully unsupervised setup for image
classification, reducing the need for human labels on the fine-tuning stage as
well. This work demonstrates that large-scale unsupervised models can also
perform a more challenging object segmentation task, requiring neither
pixel-level nor image-level labeling. Namely, we show that recent unsupervised
GANs allow to differentiate between foreground/background pixels, providing
high-quality saliency masks. By extensive comparison on standard benchmarks, we
outperform existing unsupervised alternatives for object segmentation,
achieving new state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1"&gt;Andrey Voynov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morozov_S/0/1/0/all/0/1"&gt;Stanislav Morozov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1"&gt;Artem Babenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can we have it all? On the Trade-off between Spatial and Adversarial Robustness of Neural Networks. (arXiv:2002.11318v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.11318</id>
        <link href="http://arxiv.org/abs/2002.11318"/>
        <updated>2021-06-14T01:38:53.003Z</updated>
        <summary type="html"><![CDATA[(Non-)robustness of neural networks to small, adversarial pixel-wise
perturbations, and as more recently shown, to even random spatial
transformations (e.g., translations, rotations) entreats both theoretical and
empirical understanding. Spatial robustness to random translations and
rotations is commonly attained via equivariant models (e.g., StdCNNs, GCNNs)
and training augmentation, whereas adversarial robustness is typically achieved
by adversarial training. In this paper, we prove a quantitative trade-off
between spatial and adversarial robustness in a simple statistical setting. We
complement this empirically by showing that: (a) as the spatial robustness of
equivariant models improves by training augmentation with progressively larger
transformations, their adversarial robustness worsens progressively, and (b) as
the state-of-the-art robust models are adversarially trained with progressively
larger pixel-wise perturbations, their spatial robustness drops progressively.
Towards achieving pareto-optimality in this trade-off, we propose a method
based on curriculum learning that trains gradually on more difficult
perturbations (both spatial and adversarial) to improve spatial and adversarial
robustness simultaneously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamath_S/0/1/0/all/0/1"&gt;Sandesh Kamath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1"&gt;Amit Deshpande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subrahmanyam_K/0/1/0/all/0/1"&gt;K V Subrahmanyam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[K-shot NAS: Learnable Weight-Sharing for NAS with K-shot Supernets. (arXiv:2106.06442v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06442</id>
        <link href="http://arxiv.org/abs/2106.06442"/>
        <updated>2021-06-14T01:38:52.995Z</updated>
        <summary type="html"><![CDATA[In one-shot weight sharing for NAS, the weights of each operation (at each
layer) are supposed to be identical for all architectures (paths) in the
supernet. However, this rules out the possibility of adjusting operation
weights to cater for different paths, which limits the reliability of the
evaluation results. In this paper, instead of counting on a single supernet, we
introduce $K$-shot supernets and take their weights for each operation as a
dictionary. The operation weight for each path is represented as a convex
combination of items in a dictionary with a simplex code. This enables a matrix
approximation of the stand-alone weight matrix with a higher rank ($K>1$). A
\textit{simplex-net} is introduced to produce architecture-customized code for
each path. As a result, all paths can adaptively learn how to share weights in
the $K$-shot supernets and acquire corresponding weights for better evaluation.
$K$-shot supernets and simplex-net can be iteratively trained, and we further
extend the search to the channel dimension. Extensive experiments on benchmark
datasets validate that K-shot NAS significantly improves the evaluation
accuracy of paths and thus brings in impressive performance improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xiu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making EfficientNet More Efficient: Exploring Batch-Independent Normalization, Group Convolutions and Reduced Resolution Training. (arXiv:2106.03640v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03640</id>
        <link href="http://arxiv.org/abs/2106.03640"/>
        <updated>2021-06-14T01:38:52.986Z</updated>
        <summary type="html"><![CDATA[Much recent research has been dedicated to improving the efficiency of
training and inference for image classification. This effort has commonly
focused on explicitly improving theoretical efficiency, often measured as
ImageNet validation accuracy per FLOP. These theoretical savings have, however,
proven challenging to achieve in practice, particularly on high-performance
training accelerators.

In this work, we focus on improving the practical efficiency of the
state-of-the-art EfficientNet models on a new class of accelerator, the
Graphcore IPU. We do this by extending this family of models in the following
ways: (i) generalising depthwise convolutions to group convolutions; (ii)
adding proxy-normalized activations to match batch normalization performance
with batch-independent statistics; (iii) reducing compute by lowering the
training resolution and inexpensively fine-tuning at higher resolution. We find
that these three methods improve the practical efficiency for both training and
inference. Our code will be made available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Masters_D/0/1/0/all/0/1"&gt;Dominic Masters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labatie_A/0/1/0/all/0/1"&gt;Antoine Labatie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1"&gt;Zach Eaton-Rosen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1"&gt;Carlo Luschi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibration and Auto-Refinement for Light Field Cameras. (arXiv:2106.06181v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06181</id>
        <link href="http://arxiv.org/abs/2106.06181"/>
        <updated>2021-06-14T01:38:52.979Z</updated>
        <summary type="html"><![CDATA[The ability to create an accurate three-dimensional reconstruction of a
captured scene draws attention to the principles of light fields. This paper
presents an approach for light field camera calibration and rectification,
based on pairwise pattern-based parameters extraction. It is followed by a
correspondence-based algorithm for camera parameters refinement from arbitrary
scenes using the triangulation filter and nonlinear optimization. The
effectiveness of our approach is validated on both real and synthetic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anisimov_Y/0/1/0/all/0/1"&gt;Yuriy Anisimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reis_G/0/1/0/all/0/1"&gt;Gerd Reis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1"&gt;Didier Stricker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Multi-Object Tracking and Segmentation with GMPHD Filter and Mask-based Affinity Fusion. (arXiv:2009.00100v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00100</id>
        <link href="http://arxiv.org/abs/2009.00100"/>
        <updated>2021-06-14T01:38:52.960Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a highly practical fully online multi-object
tracking and segmentation (MOTS) method that uses instance segmentation results
as an input. The proposed method is based on the Gaussian mixture probability
hypothesis density (GMPHD) filter, a hierarchical data association (HDA), and a
mask-based affinity fusion (MAF) model to achieve high-performance online
tracking. The HDA consists of two associations: segment-to-track and
track-to-track associations. One affinity, for position and motion, is computed
by using the GMPHD filter, and the other affinity, for appearance is computed
by using the responses from a single object tracker such as a kernalized
correlation filter. These two affinities are simply fused by using a
score-level fusion method such as min-max normalization referred to as MAF. In
addition, to reduce the number of false positive segments, we adopt mask
IoU-based merging (mask merging). The proposed MOTS framework with the key
modules: HDA, MAF, and mask merging, is easily extensible to simultaneously
track multiple types of objects with CPU only execution in parallel processing.
In addition, the developed framework only requires simple parameter tuning
unlike many existing MOTS methods that need intensive hyperparameter
optimization. In the experiments on the two popular MOTS datasets, the key
modules show some improvements. For instance, ID-switch decreases by more than
half compared to a baseline method in the training sets. In conclusion, our
tracker achieves state-of-the-art MOTS performance in the test sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Young-min Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1"&gt;Young-chul Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1"&gt;Kwangjin Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1"&gt;Moongu Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1"&gt;Witold Pedrycz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AugNet: End-to-End Unsupervised Visual Representation Learning with Image Augmentation. (arXiv:2106.06250v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06250</id>
        <link href="http://arxiv.org/abs/2106.06250"/>
        <updated>2021-06-14T01:38:52.951Z</updated>
        <summary type="html"><![CDATA[Most of the achievements in artificial intelligence so far were accomplished
by supervised learning which requires numerous annotated training data and thus
costs innumerable manpower for labeling. Unsupervised learning is one of the
effective solutions to overcome such difficulties. In our work, we propose
AugNet, a new deep learning training paradigm to learn image features from a
collection of unlabeled pictures. We develop a method to construct the
similarities between pictures as distance metrics in the embedding space by
leveraging the inter-correlation between augmented versions of samples. Our
experiments demonstrate that the method is able to represent the image in low
dimensional space and performs competitively in downstream tasks such as image
classification and image similarity comparison. Specifically, we achieved over
60% and 27% accuracy on the STL10 and CIFAR100 datasets with unsupervised
clustering, respectively. Moreover, unlike many deep-learning-based image
retrieval algorithms, our approach does not require access to external
annotated datasets to train the feature extractor, but still shows comparable
or even better feature representation ability and easy-to-use characteristics.
In our evaluations, the method outperforms all the state-of-the-art image
retrieval algorithms on some out-of-domain image datasets. The code for the
model implementation is available at
https://github.com/chenmingxiang110/AugNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingxiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1"&gt;Zhanguo Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haonan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bitao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Liufang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhecheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Image Forensic Technique Based on JPEG Ghosts. (arXiv:2106.06439v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06439</id>
        <link href="http://arxiv.org/abs/2106.06439"/>
        <updated>2021-06-14T01:38:52.944Z</updated>
        <summary type="html"><![CDATA[The unprecedented growth in the easy availability of photo-editing tools has
endangered the power of digital images.An image was supposed to be worth more
than a thousand words,but now this can be said only if it can be authenticated
orthe integrity of the image can be proved to be intact. In thispaper, we
propose a digital image forensic technique for JPEG images. It can detect any
forgery in the image if the forged portion called a ghost image is having a
compression quality different from that of the cover image. It is based on
resaving the JPEG image at different JPEG qualities, and the detection of the
forged portion is maximum when it is saved at the same JPEG quality as the
cover image. Also, we can precisely predictthe JPEG quality of the cover image
by analyzing the similarity using Structural Similarity Index Measure (SSIM) or
the energyof the images. The first maxima in SSIM or the first minima inenergy
correspond to the cover image JPEG quality. We created adataset for varying
JPEG compression qualities of the ghost and the cover images and validated the
scalability of the experimental results.We also, experimented with varied
attack scenarios, e.g. high-quality ghost image embedded in low quality of
cover image,low-quality ghost image embedded in high-quality of cover image,and
ghost image and cover image both at the same quality.The proposed method is
able to localize the tampered portions accurately even for forgeries as small
as 10x10 sized pixel blocks.Our technique is also robust against other attack
scenarios like copy-move forgery, inserting text into image, rescaling
(zoom-out/zoom-in) ghost image and then pasting on cover image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1"&gt;Divakar Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KLUE: Korean Language Understanding Evaluation. (arXiv:2105.09680v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09680</id>
        <link href="http://arxiv.org/abs/2105.09680"/>
        <updated>2021-06-14T01:38:52.938Z</updated>
        <summary type="html"><![CDATA[We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE
is a collection of 8 Korean natural language understanding (NLU) tasks,
including Topic Classification, SemanticTextual Similarity, Natural Language
Inference, Named Entity Recognition, Relation Extraction, Dependency Parsing,
Machine Reading Comprehension, and Dialogue State Tracking. We build all of the
tasks from scratch from diverse source corpora while respecting copyrights, to
ensure accessibility for anyone without any restrictions. With ethical
considerations in mind, we carefully design annotation protocols. Along with
the benchmark tasks and data, we provide suitable evaluation metrics and
fine-tuning recipes for pretrained language models for each task. We
furthermore release the pretrained language models (PLM), KLUE-BERT and
KLUE-RoBERTa, to help reproducing baseline models on KLUE and thereby
facilitate future research. We make a few interesting observations from the
preliminary experiments using the proposed KLUE benchmark suite, already
demonstrating the usefulness of this new benchmark suite. First, we find
KLUE-RoBERTa-large outperforms other baselines, including multilingual PLMs and
existing open-source Korean PLMs. Second, we see minimal degradation in
performance even when we replace personally identifiable information from the
pretraining corpus, suggesting that privacy and NLU capability are not at odds
with each other. Lastly, we find that using BPE tokenization in combination
with morpheme-level pre-tokenization is effective in tasks involving
morpheme-level tagging, detection and generation. In addition to accelerating
Korean NLP research, our comprehensive documentation on creating KLUE will
facilitate creating similar resources for other languages in the future. KLUE
is available at https://klue-benchmark.com.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sungjoon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1"&gt;Jihyung Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungdong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1"&gt;Won Ik Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiyoon Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jangwon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chisung Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junseong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yongsook Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1"&gt;Taehwan Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joohong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Juhyun Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Sungwon Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1"&gt;Younghoon Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1"&gt;Inkwon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1"&gt;Sangwoo Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dongjun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Myeonghwa Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1"&gt;Seongbo Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Do_S/0/1/0/all/0/1"&gt;Seungwon Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sunkyoung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1"&gt;Kyungtae Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jongwon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyumin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jamin Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seonghyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1"&gt;Lucy Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1"&gt;Alice Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1"&gt;Jung-Woo Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shallow Optical Flow Three-Stream CNN for Macro- and Micro-Expression Spotting from Long Videos. (arXiv:2106.06489v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06489</id>
        <link href="http://arxiv.org/abs/2106.06489"/>
        <updated>2021-06-14T01:38:52.930Z</updated>
        <summary type="html"><![CDATA[Facial expressions vary from the visible to the subtle. In recent years, the
analysis of micro-expressions $-$ a natural occurrence resulting from the
suppression of one's true emotions, has drawn the attention of researchers with
a broad range of potential applications. However, spotting microexpressions in
long videos becomes increasingly challenging when intertwined with normal or
macro-expressions. In this paper, we propose a shallow optical flow
three-stream CNN (SOFTNet) model to predict a score that captures the
likelihood of a frame being in an expression interval. By fashioning the
spotting task as a regression problem, we introduce pseudo-labeling to
facilitate the learning process. We demonstrate the efficacy and efficiency of
the proposed approach on the recent MEGC 2020 benchmark, where state-of-the-art
performance is achieved on CAS(ME)$^{2}$ with equally promising results on SAMM
Long Videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liong_G/0/1/0/all/0/1"&gt;Gen-Bing Liong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1"&gt;John See&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1"&gt;Lai-Kuan Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-Level Task Parameters: A Robust Multi-task Weighting Framework. (arXiv:2106.06129v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06129</id>
        <link href="http://arxiv.org/abs/2106.06129"/>
        <updated>2021-06-14T01:38:52.911Z</updated>
        <summary type="html"><![CDATA[Recent works have shown that deep neural networks benefit from multi-task
learning by learning a shared representation across several related tasks.
However, performance of such systems depend on relative weighting between
various losses involved during training. Prior works on loss weighting schemes
assume that instances are equally easy or hard for all tasks. In order to break
this assumption, we let the training process dictate the optimal weighting of
tasks for every instance in the dataset. More specifically, we equip every
instance in the dataset with a set of learnable parameters (instance-level task
parameters) where the cardinality is equal to the number of tasks learned by
the model. These parameters model the weighting of each task for an instance.
They are updated by gradient descent and do not require hand-crafted rules. We
conduct extensive experiments on SURREAL and CityScapes datasets, for human
shape and pose estimation, depth estimation and semantic segmentation tasks. In
these tasks, our approach outperforms recent dynamic loss weighting approaches,
e.g. reducing surface estimation errors by 8.97% on SURREAL. When applied to
datasets where one or more tasks can have noisy annotations, the proposed
method learns to prioritize learning from clean labels for a given task, e.g.
reducing surface estimation errors by up to 60%. We also show that we can
reliably detect corrupt labels for a given task as a by-product from learned
instance-level task parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasu_P/0/1/0/all/0/1"&gt;Pavan Kumar Anasosalu Vasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1"&gt;Shreyas Saxena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1"&gt;Oncel Tuzel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Anytime Prediction with Parallel Cascaded Networks and a Temporal-Difference Loss. (arXiv:2102.09808v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09808</id>
        <link href="http://arxiv.org/abs/2102.09808"/>
        <updated>2021-06-14T01:38:52.905Z</updated>
        <summary type="html"><![CDATA[Although deep feedforward neural networks share some characteristics with the
primate visual system, a key distinction is their dynamics. Deep nets typically
operate in serial stages wherein each layer completes its computation before
processing begins in subsequent layers. In contrast, biological systems have
cascaded dynamics: information propagates from neurons at all layers in
parallel but transmission occurs gradually over time, leading to speed-accuracy
trade offs even in feedforward architectures. We explore the consequences of
biologically inspired parallel hardware by constructing cascaded ResNets in
which each residual block has propagation delays but all blocks update in
parallel in a stateful manner. Because information transmitted through skip
connections avoids delays, the functional depth of the architecture increases
over time, yielding anytime predictions that improve with internal-processing
time. We introduce a temporal-difference training loss that achieves a strictly
superior speed-accuracy profile over standard losses and enables the cascaded
architecture to outperform state-of-the-art anytime-prediction methods. The
cascaded architecture has intriguing properties, including: it classifies
typical instances more rapidly than atypical instances; it is more robust to
both persistent and transient noise than is a conventional ResNet; and its
time-varying output trace provides a signal that can be exploited to improve
information processing and inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iuzzolino_M/0/1/0/all/0/1"&gt;Michael L. Iuzzolino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1"&gt;Michael C. Mozer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Compositional Shape Priors for Few-Shot 3D Reconstruction. (arXiv:2106.06440v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06440</id>
        <link href="http://arxiv.org/abs/2106.06440"/>
        <updated>2021-06-14T01:38:52.898Z</updated>
        <summary type="html"><![CDATA[The impressive performance of deep convolutional neural networks in
single-view 3D reconstruction suggests that these models perform non-trivial
reasoning about the 3D structure of the output space. Recent work has
challenged this belief, showing that, on standard benchmarks, complex
encoder-decoder architectures perform similarly to nearest-neighbor baselines
or simple linear decoder models that exploit large amounts of per-category
data. However, building large collections of 3D shapes for supervised training
is a laborious process; a more realistic and less constraining task is
inferring 3D shapes for categories with few available training examples,
calling for a model that can successfully generalize to novel object classes.
In this work we experimentally demonstrate that naive baselines fail in this
few-shot learning setting, in which the network must learn informative shape
priors for inference of new categories. We propose three ways to learn a
class-specific global shape prior, directly from data. Using these techniques,
we are able to capture multi-scale information about the 3D shape, and account
for intra-class variability by virtue of an implicit compositional structure.
Experiments on the popular ShapeNet dataset show that our method outperforms a
zero-shot baseline by over 40%, and the current state-of-the-art by over 10%,
in terms of relative performance, in the few-shot setting.12]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michalkiewicz_M/0/1/0/all/0/1"&gt;Mateusz Michalkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsogkas_S/0/1/0/all/0/1"&gt;Stavros Tsogkas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parisot_S/0/1/0/all/0/1"&gt;Sarah Parisot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1"&gt;Mahsa Baktashmotlagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eriksson_A/0/1/0/all/0/1"&gt;Anders Eriksson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1"&gt;Eugene Belilovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation. (arXiv:2106.06471v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06471</id>
        <link href="http://arxiv.org/abs/2106.06471"/>
        <updated>2021-06-14T01:38:52.891Z</updated>
        <summary type="html"><![CDATA[Medical report generation is one of the most challenging tasks in medical
image analysis. Although existing approaches have achieved promising results,
they either require a predefined template database in order to retrieve
sentences or ignore the hierarchical nature of medical report generation. To
address these issues, we propose MedWriter that incorporates a novel
hierarchical retrieval mechanism to automatically extract both report and
sentence-level templates for clinically accurate report generation. MedWriter
first employs the Visual-Language Retrieval~(VLR) module to retrieve the most
relevant reports for the given images. To guarantee the logical coherence
between sentences, the Language-Language Retrieval~(LLR) module is introduced
to retrieve relevant sentences based on the previous generated description. At
last, a language decoder fuses image features and features from retrieved
reports and sentences to generate meaningful medical reports. We verified the
effectiveness of our model by automatic evaluation and human evaluation on two
datasets, i.e., Open-I and MIMIC-CXR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xingyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1"&gt;Muchao Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1"&gt;Quanzeng You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Fenglong Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is Multimodality?. (arXiv:2103.06304v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06304</id>
        <link href="http://arxiv.org/abs/2103.06304"/>
        <updated>2021-06-14T01:38:52.885Z</updated>
        <summary type="html"><![CDATA[The last years have shown rapid developments in the field of multimodal
machine learning, combining e.g., vision, text or speech. In this position
paper we explain how the field uses outdated definitions of multimodality that
prove unfit for the machine learning era. We propose a new task-relative
definition of (multi)modality in the context of multimodal machine learning
that focuses on representations and information that are relevant for a given
machine learning task. With our new definition of multimodality we aim to
provide a missing foundation for multimodal research, an important component of
language grounding and a crucial milestone towards NLU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1"&gt;Letitia Parcalabescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trost_N/0/1/0/all/0/1"&gt;Nils Trost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Anette Frank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. (arXiv:2102.05918v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05918</id>
        <link href="http://arxiv.org/abs/2102.05918"/>
        <updated>2021-06-14T01:38:52.868Z</updated>
        <summary type="html"><![CDATA[Pre-trained representations are becoming crucial for many NLP and perception
tasks. While representation learning in NLP has transitioned to training on raw
text without human annotations, visual and vision-language representations
still rely heavily on curated training datasets that are expensive or require
expert knowledge. For vision applications, representations are mostly learned
using datasets with explicit class labels such as ImageNet or OpenImages. For
vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all
involve a non-trivial data collection (and cleaning) process. This costly
curation process limits the size of datasets and hence hinders the scaling of
trained models. In this paper, we leverage a noisy dataset of over one billion
image alt-text pairs, obtained without expensive filtering or post-processing
steps in the Conceptual Captions dataset. A simple dual-encoder architecture
learns to align visual and language representations of the image and text pairs
using a contrastive loss. We show that the scale of our corpus can make up for
its noise and leads to state-of-the-art representations even with such a simple
learning scheme. Our visual representation achieves strong performance when
transferred to classification tasks such as ImageNet and VTAB. The aligned
visual and language representations enables zero-shot image classification and
also set new state-of-the-art results on Flickr30K and MSCOCO image-text
retrieval benchmarks, even when compared with more sophisticated
cross-attention models. The representations also enable cross-modality search
with complex text and text + image queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1"&gt;Chao Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Ye Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1"&gt;Zarana Parekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1"&gt;Yunhsuan Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duerig_T/0/1/0/all/0/1"&gt;Tom Duerig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework to Enhance Generalization of Deep Metric Learning methods using General Discriminative Feature Learning and Class Adversarial Neural Networks. (arXiv:2106.06420v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06420</id>
        <link href="http://arxiv.org/abs/2106.06420"/>
        <updated>2021-06-14T01:38:52.861Z</updated>
        <summary type="html"><![CDATA[Metric learning algorithms aim to learn a distance function that brings the
semantically similar data items together and keeps dissimilar ones at a
distance. The traditional Mahalanobis distance learning is equivalent to find a
linear projection. In contrast, Deep Metric Learning (DML) methods are proposed
that automatically extract features from data and learn a non-linear
transformation from input space to a semantically embedding space. Recently,
many DML methods are proposed focused to enhance the discrimination power of
the learned metric by providing novel sampling strategies or loss functions.
This approach is very helpful when both the training and test examples are
coming from the same set of categories. However, it is less effective in many
applications of DML such as image retrieval and person-reidentification. Here,
the DML should learn general semantic concepts from observed classes and employ
them to rank or identify objects from unseen categories. Neglecting the
generalization ability of the learned representation and just emphasizing to
learn a more discriminative embedding on the observed classes may lead to the
overfitting problem. To address this limitation, we propose a framework to
enhance the generalization power of existing DML methods in a Zero-Shot
Learning (ZSL) setting by general yet discriminative representation learning
and employing a class adversarial neural network. To learn a more general
representation, we propose to employ feature maps of intermediate layers in a
deep neural network and enhance their discrimination power through an attention
mechanism. Besides, a class adversarial network is utilized to enforce the deep
model to seek class invariant features for the DML task. We evaluate our work
on widely used machine vision datasets in a ZSL setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Kaabi_K/0/1/0/all/0/1"&gt;Karrar Al-Kaabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monsefi_R/0/1/0/all/0/1"&gt;Reza Monsefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zabihzadeh_D/0/1/0/all/0/1"&gt;Davood Zabihzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning. (arXiv:2106.06047v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06047</id>
        <link href="http://arxiv.org/abs/2106.06047"/>
        <updated>2021-06-14T01:38:52.854Z</updated>
        <summary type="html"><![CDATA[Federated learning is an emerging research paradigm enabling collaborative
training of machine learning models among different organizations while keeping
data private at each institution. Despite recent progress, there remain
fundamental challenges such as lack of convergence and potential for
catastrophic forgetting in federated learning across real-world heterogeneous
devices. In this paper, we demonstrate that attention-based architectures
(e.g., Transformers) are fairly robust to distribution shifts and hence improve
federated learning over heterogeneous data. Concretely, we conduct the first
rigorous empirical investigation of different neural architectures across a
range of federated algorithms, real-world benchmarks, and heterogeneous data
splits. Our experiments show that simply replacing convolutional networks with
Transformers can greatly reduce catastrophic forgetting of previous devices,
accelerate convergence, and reach a better global model, especially when
dealing with heterogeneous data. We will release our code and pretrained models
at https://github.com/Liangqiong/ViT-FL-main to encourage future exploration in
robust architectures as an alternative to current research efforts on the
optimization front.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Liangqiong Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuyin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yingda Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Feifei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Black-box Explanation of Object Detectors via Saliency Maps. (arXiv:2006.03204v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03204</id>
        <link href="http://arxiv.org/abs/2006.03204"/>
        <updated>2021-06-14T01:38:52.847Z</updated>
        <summary type="html"><![CDATA[We propose D-RISE, a method for generating visual explanations for the
predictions of object detectors. Utilizing the proposed similarity metric that
accounts for both localization and categorization aspects of object detection
allows our method to produce saliency maps that show image areas that most
affect the prediction. D-RISE can be considered "black-box" in the software
testing sense, as it only needs access to the inputs and outputs of an object
detector. Compared to gradient-based methods, D-RISE is more general and
agnostic to the particular type of object detector being tested, and does not
need knowledge of the inner workings of the model. We show that D-RISE can be
easily applied to different object detectors including one-stage detectors such
as YOLOv3 and two-stage detectors such as Faster-RCNN. We present a detailed
analysis of the generated visual explanations to highlight the utilization of
context and possible biases learned by object detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petsiuk_V/0/1/0/all/0/1"&gt;Vitali Petsiuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1"&gt;Rajiv Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manjunatha_V/0/1/0/all/0/1"&gt;Varun Manjunatha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morariu_V/0/1/0/all/0/1"&gt;Vlad I. Morariu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_A/0/1/0/all/0/1"&gt;Ashutosh Mehra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1"&gt;Vicente Ordonez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-based Partial Face Recognition. (arXiv:2106.06415v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06415</id>
        <link href="http://arxiv.org/abs/2106.06415"/>
        <updated>2021-06-14T01:38:52.839Z</updated>
        <summary type="html"><![CDATA[Photos of faces captured in unconstrained environments, such as large crowds,
still constitute challenges for current face recognition approaches as often
faces are occluded by objects or people in the foreground. However, few studies
have addressed the task of recognizing partial faces. In this paper, we propose
a novel approach to partial face recognition capable of recognizing faces with
different occluded areas. We achieve this by combining attentional pooling of a
ResNet's intermediate feature maps with a separate aggregation module. We
further adapt common losses to partial faces in order to ensure that the
attention maps are diverse and handle occluded parts. Our thorough analysis
demonstrates that we outperform all baselines under multiple benchmark
protocols, including naturally and synthetically occluded partial faces. This
suggests that our method successfully focuses on the relevant parts of the
occluded face.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1"&gt;Stefan H&amp;#xf6;rmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zeyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1"&gt;Martin Knoche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teepe_T/0/1/0/all/0/1"&gt;Torben Teepe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1"&gt;Gerhard Rigoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Twins: Revisiting the Design of Spatial Attention in Vision Transformers. (arXiv:2104.13840v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13840</id>
        <link href="http://arxiv.org/abs/2104.13840"/>
        <updated>2021-06-14T01:38:52.820Z</updated>
        <summary type="html"><![CDATA[Very recently, a variety of vision transformer architectures for dense
prediction tasks have been proposed and they show that the design of spatial
attention is critical to their success in these tasks. In this work, we revisit
the design of the spatial attention and demonstrate that a carefully-devised
yet simple spatial attention mechanism performs favourably against the
state-of-the-art schemes. As a result, we propose two vision transformer
architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures
are highly-efficient and easy to implement, only involving matrix
multiplications that are highly optimized in modern deep learning frameworks.
More importantly, the proposed architectures achieve excellent performance on a
wide range of visual tasks including imagelevel classification as well as dense
detection and segmentation. The simplicity and strong performance suggest that
our proposed architectures may serve as stronger backbones for many vision
tasks. Our code will be released soon at
https://github.com/Meituan-AutoML/Twins .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1"&gt;Xiangxiang Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zhi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Haibing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiaolin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1"&gt;Huaxia Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive-Scale Boundary Blackbox Attack via Projective Gradient Estimation. (arXiv:2106.06056v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06056</id>
        <link href="http://arxiv.org/abs/2106.06056"/>
        <updated>2021-06-14T01:38:52.811Z</updated>
        <summary type="html"><![CDATA[Boundary based blackbox attack has been recognized as practical and
effective, given that an attacker only needs to access the final model
prediction. However, the query efficiency of it is in general high especially
for high dimensional image data. In this paper, we show that such efficiency
highly depends on the scale at which the attack is applied, and attacking at
the optimal scale significantly improves the efficiency. In particular, we
propose a theoretical framework to analyze and show three key characteristics
to improve the query efficiency. We prove that there exists an optimal scale
for projective gradient estimation. Our framework also explains the
satisfactory performance achieved by existing boundary black-box attacks. Based
on our theoretical framework, we propose Progressive-Scale enabled projective
Boundary Attack (PSBA) to improve the query efficiency via progressive scaling
techniques. In particular, we employ Progressive-GAN to optimize the scale of
projections, which we call PSBA-PGAN. We evaluate our approach on both spatial
and frequency scales. Extensive experiments on MNIST, CIFAR-10, CelebA, and
ImageNet against different models including a real-world face recognition API
show that PSBA-PGAN significantly outperforms existing baseline attacks in
terms of query efficiency and attack success rate. We also observe relatively
stable optimal scales for different models and datasets. The code is publicly
available at https://github.com/AI-secure/PSBA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiawei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Huichen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaolu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Online Monitoring and Data-driven Control: A Study of Segmentation Algorithms for Laser Powder Bed Fusion Processes. (arXiv:2011.09065v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09065</id>
        <link href="http://arxiv.org/abs/2011.09065"/>
        <updated>2021-06-14T01:38:52.804Z</updated>
        <summary type="html"><![CDATA[An increasing number of laser powder bed fusion machines use off-axis
infrared cameras to improve online monitoring and data-driven control
capabilities. However, there is still a severe lack of algorithmic solutions to
properly process the infrared images from these cameras that has led to several
key limitations: a lack of online monitoring capabilities for the laser tracks,
insufficient pre-processing of the infrared images for data-driven methods, and
large memory requirements for storing the infrared images. To address these
limitations, we study over 30 segmentation algorithms that segment each
infrared image into a foreground and background. By evaluating each algorithm
based on its segmentation accuracy, computational speed, and spatter detection
characteristics, we identify promising algorithmic solutions. The identified
algorithms can be readily applied to the laser powder bed fusion machines to
address each of the above limitations and thus, significantly improve process
control.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nettekoven_A/0/1/0/all/0/1"&gt;Alexander Nettekoven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fish_S/0/1/0/all/0/1"&gt;Scott Fish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beaman_J/0/1/0/all/0/1"&gt;Joseph Beaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction. (arXiv:2105.10446v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10446</id>
        <link href="http://arxiv.org/abs/2105.10446"/>
        <updated>2021-06-14T01:38:52.796Z</updated>
        <summary type="html"><![CDATA[This work attempts to provide a plausible theoretical framework that aims to
interpret modern deep (convolutional) networks from the principles of data
compression and discriminative representation. We argue that for
high-dimensional multi-class data, the optimal linear discriminative
representation maximizes the coding rate difference between the whole dataset
and the average of all the subsets. We show that the basic iterative gradient
ascent scheme for optimizing the rate reduction objective naturally leads to a
multi-layer deep network, named ReduNet, which shares common characteristics of
modern deep networks. The deep layered architectures, linear and nonlinear
operators, and even parameters of the network are all explicitly constructed
layer-by-layer via forward propagation, although they are amenable to
fine-tuning via back propagation. All components of so-obtained ``white-box''
network have precise optimization, statistical, and geometric interpretation.
Moreover, all linear operators of the so-derived network naturally become
multi-channel convolutions when we enforce classification to be rigorously
shift-invariant. The derivation in the invariant setting suggests a trade-off
between sparsity and invariance, and also indicates that such a deep
convolution network is significantly more efficient to construct and learn in
the spectral domain. Our preliminary simulations and experiments clearly verify
the effectiveness of both the rate reduction objective and the associated
ReduNet. All code and data are available at https://github.com/Ma-Lab-Berkeley.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1"&gt;Kwan Ho Ryan Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chong You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haozhi Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1"&gt;John Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Modeling of Probabilities for Coding the Octree Representation of Point Clouds. (arXiv:2106.06482v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06482</id>
        <link href="http://arxiv.org/abs/2106.06482"/>
        <updated>2021-06-14T01:38:52.789Z</updated>
        <summary type="html"><![CDATA[This paper describes a novel lossless point cloud compression algorithm that
uses a neural network for estimating the coding probabilities for the occupancy
status of voxels, depending on wide three dimensional contexts around the voxel
to be encoded. The point cloud is represented as an octree, with each
resolution layer being sequentially encoded and decoded using arithmetic
coding, starting from the lowest resolution, until the final resolution is
reached. The occupancy probability of each voxel of the splitting pattern at
each node of the octree is modeled by a neural network, having at its input the
already encoded occupancy status of several octree nodes (belonging to the past
and current resolutions), corresponding to a 3D context surrounding the node to
be encoded. The algorithm has a fast and a slow version, the fast version
selecting differently several voxels of the context, which allows an increased
parallelization by sending larger batches of templates to be estimated by the
neural network, at both encoder and decoder. The proposed algorithms yield
state-of-the-art results on benchmark datasets. The implementation will be made
available at https://github.com/marmus12/nnctx]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaya_E/0/1/0/all/0/1"&gt;Emre Can Kaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabus_I/0/1/0/all/0/1"&gt;Ioan Tabus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimSwap: An Efficient Framework For High Fidelity Face Swapping. (arXiv:2106.06340v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06340</id>
        <link href="http://arxiv.org/abs/2106.06340"/>
        <updated>2021-06-14T01:38:52.769Z</updated>
        <summary type="html"><![CDATA[We propose an efficient framework, called Simple Swap (SimSwap), aiming for
generalized and high fidelity face swapping. In contrast to previous approaches
that either lack the ability to generalize to arbitrary identity or fail to
preserve attributes like facial expression and gaze direction, our framework is
capable of transferring the identity of an arbitrary source face into an
arbitrary target face while preserving the attributes of the target face. We
overcome the above defects in the following two ways. First, we present the ID
Injection Module (IIM) which transfers the identity information of the source
face into the target face at feature level. By using this module, we extend the
architecture of an identity-specific face swapping algorithm to a framework for
arbitrary face swapping. Second, we propose the Weak Feature Matching Loss
which efficiently helps our framework to preserve the facial attributes in an
implicit way. Extensive experiments on wild faces demonstrate that our SimSwap
is able to achieve competitive identity performance while preserving attributes
better than previous state-of-the-art methods. The code is already available on
github: https://github.com/neuralchen/SimSwap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Renwang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuanhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yanhao Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Architecture Search without Training. (arXiv:2006.04647v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04647</id>
        <link href="http://arxiv.org/abs/2006.04647"/>
        <updated>2021-06-14T01:38:52.761Z</updated>
        <summary type="html"><![CDATA[The time and effort involved in hand-designing deep neural networks is
immense. This has prompted the development of Neural Architecture Search (NAS)
techniques to automate this design. However, NAS algorithms tend to be slow and
expensive; they need to train vast numbers of candidate networks to inform the
search process. This could be alleviated if we could partially predict a
network's trained accuracy from its initial state. In this work, we examine the
overlap of activations between datapoints in untrained networks and motivate
how this can give a measure which is usefully indicative of a network's trained
performance. We incorporate this measure into a simple algorithm that allows us
to search for powerful networks without any training in a matter of seconds on
a single GPU, and verify its effectiveness on NAS-Bench-101, NAS-Bench-201,
NATS-Bench, and Network Design Spaces. Our approach can be readily combined
with more expensive search methods; we examine a simple adaptation of
regularised evolutionary search. Code for reproducing our experiments is
available at https://github.com/BayesWatch/nas-without-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mellor_J/0/1/0/all/0/1"&gt;Joseph Mellor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_J/0/1/0/all/0/1"&gt;Jack Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1"&gt;Amos Storkey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crowley_E/0/1/0/all/0/1"&gt;Elliot J. Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KRADA: Known-region-aware Domain Alignment for Open World Semantic Segmentation. (arXiv:2106.06237v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.06237</id>
        <link href="http://arxiv.org/abs/2106.06237"/>
        <updated>2021-06-14T01:38:52.734Z</updated>
        <summary type="html"><![CDATA[In semantic segmentation, we aim to train a pixel-level classifier to assign
category labels to all pixels in an image, where labeled training images and
unlabeled test images are from the same distribution and share the same label
set. However, in an open world, the unlabeled test images probably contain
unknown categories and have different distributions from the labeled images.
Hence, in this paper, we consider a new, more realistic, and more challenging
problem setting where the pixel-level classifier has to be trained with labeled
images and unlabeled open-world images -- we name it open world semantic
segmentation (OSS). In OSS, the trained classifier is expected to identify
unknown-class pixels and classify known-class pixels well. To solve OSS, we
first investigate which distribution that unknown-class pixels obey. Then,
motivated by the goodness-of-fit test, we use statistical measurements to show
how a pixel fits the distribution of an unknown class and select highly-fitted
pixels to form the unknown region in each image. Eventually, we propose an
end-to-end learning framework, known-region-aware domain alignment (KRADA), to
distinguish unknown classes while aligning distributions of known classes in
labeled and unlabeled open-world images. The effectiveness of KRADA has been
verified on two synthetic tasks and one COVID-19 segmentation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chenhong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chen Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheung_W/0/1/0/all/0/1"&gt;William Cheung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recovery of Meteorites Using an Autonomous Drone and Machine Learning. (arXiv:2106.06523v1 [astro-ph.EP])]]></title>
        <id>http://arxiv.org/abs/2106.06523</id>
        <link href="http://arxiv.org/abs/2106.06523"/>
        <updated>2021-06-14T01:38:52.579Z</updated>
        <summary type="html"><![CDATA[The recovery of freshly fallen meteorites from tracked and triangulated
meteors is critical to determining their source asteroid families. However,
locating meteorite fragments in strewn fields remains a challenge with very few
meteorites being recovered from the meteors triangulated in past and ongoing
meteor camera networks. We examined if locating meteorites can be automated
using machine learning and an autonomous drone. Drones can be programmed to fly
a grid search pattern and take systematic pictures of the ground over a large
survey area. Those images can be analyzed using a machine learning classifier
to identify meteorites in the field among many other features. Here, we
describe a proof-of-concept meteorite classifier that deploys off-line a
combination of different convolution neural networks to recognize meteorites
from images taken by drones in the field. The system was implemented in a
conceptual drone setup and tested in the suspected strewn field of a recent
meteorite fall near Walker Lake, Nevada.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Citron_R/0/1/0/all/0/1"&gt;Robert I. Citron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Jenniskens_P/0/1/0/all/0/1"&gt;Peter Jenniskens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Watkins_C/0/1/0/all/0/1"&gt;Christopher Watkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sinha_S/0/1/0/all/0/1"&gt;Sravanthi Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Shah_A/0/1/0/all/0/1"&gt;Amar Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Raissi_C/0/1/0/all/0/1"&gt;Chedy Raissi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Devillepoix_H/0/1/0/all/0/1"&gt;Hadrien Devillepoix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Albers_J/0/1/0/all/0/1"&gt;Jim Albers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised and Unsupervised Sense Annotation via Translations. (arXiv:2106.06462v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06462</id>
        <link href="http://arxiv.org/abs/2106.06462"/>
        <updated>2021-06-14T01:38:52.571Z</updated>
        <summary type="html"><![CDATA[Acquisition of multilingual training data continues to be a challenge in word
sense disambiguation (WSD). To address this problem, unsupervised approaches
have been developed in recent years that automatically generate sense
annotations suitable for training supervised WSD systems. We present three new
methods to creating sense-annotated corpora, which leverage translations,
parallel corpora, lexical resources, and contextual and synset embeddings. Our
semi-supervised method applies machine translation to transfer existing sense
annotations to other languages. Our two unsupervised methods use a
knowledge-based WSD system to annotate a parallel corpus, and refine the
resulting sense annotations by identifying lexical translations. We obtain
state-of-the-art results on standard WSD benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1"&gt;Bradley Hauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1"&gt;Grzegorz Kondrak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1"&gt;Yixing Luan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mallik_A/0/1/0/all/0/1"&gt;Arnob Mallik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1"&gt;Lili Mou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A modular framework for object-based saccadic decisions in dynamic scenes. (arXiv:2106.06073v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06073</id>
        <link href="http://arxiv.org/abs/2106.06073"/>
        <updated>2021-06-14T01:38:52.552Z</updated>
        <summary type="html"><![CDATA[Visually exploring the world around us is not a passive process. Instead, we
actively explore the world and acquire visual information over time. Here, we
present a new model for simulating human eye-movement behavior in dynamic
real-world scenes. We model this active scene exploration as a sequential
decision making process. We adapt the popular drift-diffusion model (DDM) for
perceptual decision making and extend it towards multiple options, defined by
objects present in the scene. For each possible choice, the model integrates
evidence over time and a decision (saccadic eye movement) is triggered as soon
as evidence crosses a decision threshold. Drawing this explicit connection
between decision making and object-based scene perception is highly relevant in
the context of active viewing, where decisions are made continuously while
interacting with an external environment. We validate our model with a
carefully designed ablation study and explore influences of our model
parameters. A comparison on the VidCom dataset supports the plausibility of the
proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roth_N/0/1/0/all/0/1"&gt;Nicolas Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bideau_P/0/1/0/all/0/1"&gt;Pia Bideau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellwich_O/0/1/0/all/0/1"&gt;Olaf Hellwich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rolfs_M/0/1/0/all/0/1"&gt;Martin Rolfs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Obermayer_K/0/1/0/all/0/1"&gt;Klaus Obermayer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Bounding Boxes and Probabilistic Intersection-over-Union for Object Detection. (arXiv:2106.06072v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06072</id>
        <link href="http://arxiv.org/abs/2106.06072"/>
        <updated>2021-06-14T01:38:52.544Z</updated>
        <summary type="html"><![CDATA[Most object detection methods use bounding boxes to encode and represent the
object shape and location. In this work, we explore a fuzzy representation of
object regions using Gaussian distributions, which provides an implicit binary
representation as (potentially rotated) ellipses. We also present a similarity
measure for the Gaussian distributions based on the Hellinger Distance, which
can be viewed as a Probabilistic Intersection-over-Union (ProbIoU). Our
experimental results show that the proposed Gaussian representations are closer
to annotated segmentation masks in publicly available datasets, and that loss
functions based on ProbIoU can be successfully used to regress the parameters
of the Gaussian representation. Furthermore, we present a simple mapping scheme
from traditional (or rotated) bounding boxes to Gaussian representations,
allowing the proposed ProbIoU-based losses to be seamlessly integrated into any
object detector.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Llerena_J/0/1/0/all/0/1"&gt;Jeffri M. Llerena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeni_L/0/1/0/all/0/1"&gt;Luis Felipe Zeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kristen_L/0/1/0/all/0/1"&gt;Lucas N. Kristen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1"&gt;Claudio Jung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WAX-ML: A Python library for machine learning and feedback loops on streaming data. (arXiv:2106.06524v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06524</id>
        <link href="http://arxiv.org/abs/2106.06524"/>
        <updated>2021-06-14T01:38:52.530Z</updated>
        <summary type="html"><![CDATA[Wax is what you put on a surfboard to avoid slipping. It is an essential tool
to go surfing... We introduce WAX-ML a research-oriented Python library
providing tools to design powerful machine learning algorithms and feedback
loops working on streaming data. It strives to complement JAX with tools
dedicated to time series. WAX-ML makes JAX-based programs easy to use for
end-users working with pandas and xarray for data manipulation. It provides a
simple mechanism for implementing feedback loops, allows the implementation of
online learning and reinforcement learning algorithms with functions, and makes
them easy to integrate by end-users working with the object-oriented
reinforcement learning framework from the Gym library. It is released with an
Apache open-source license on GitHub at https://github.com/eserie/wax-ml.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Serie_E/0/1/0/all/0/1"&gt;Emmanuel S&amp;#xe9;ri&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses. (arXiv:2106.06519v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06519</id>
        <link href="http://arxiv.org/abs/2106.06519"/>
        <updated>2021-06-14T01:38:52.520Z</updated>
        <summary type="html"><![CDATA[Spoken Language Understanding (SLU) systems parse speech into semantic
structures like dialog acts and slots. This involves the use of an Automatic
Speech Recognizer (ASR) to transcribe speech into multiple text alternatives
(hypotheses). Transcription errors, common in ASRs, impact downstream SLU
performance negatively. Approaches to mitigate such errors involve using richer
information from the ASR, either in form of N-best hypotheses or word-lattices.
We hypothesize that transformer models learn better with a simpler utterance
representation using the concatenation of the N-best ASR alternatives, where
each alternative is separated by a special delimiter [SEP]. In our work, we
test our hypothesis by using concatenated N-best ASR alternatives as the input
to transformer encoder models, namely BERT and XLM-RoBERTa, and achieve
performance equivalent to the prior state-of-the-art model on DSTC2 dataset. We
also show that our approach significantly outperforms the prior
state-of-the-art when subjected to the low data regime. Additionally, this
methodology is accessible to users of third-party ASR APIs which do not provide
word-lattice information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ganesan_K/0/1/0/all/0/1"&gt;Karthik Ganesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bamdev_P/0/1/0/all/0/1"&gt;Pakhi Bamdev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+B_J/0/1/0/all/0/1"&gt;Jaivarsan B&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venugopal_A/0/1/0/all/0/1"&gt;Amresh Venugopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tushar_A/0/1/0/all/0/1"&gt;Abhinav Tushar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcoming Difficulty in Obtaining Dark-skinned Subjects for Remote-PPG by Synthetic Augmentation. (arXiv:2106.06007v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06007</id>
        <link href="http://arxiv.org/abs/2106.06007"/>
        <updated>2021-06-14T01:38:52.510Z</updated>
        <summary type="html"><![CDATA[Camera-based remote photoplethysmography (rPPG) provides a non-contact way to
measure physiological signals (e.g., heart rate) using facial videos. Recent
deep learning architectures have improved the accuracy of such physiological
measurement significantly, yet they are restricted by the diversity of the
annotated videos. The existing datasets MMSE-HR, AFRL, and UBFC-RPPG contain
roughly 10%, 0%, and 5% of dark-skinned subjects respectively. The unbalanced
training sets result in a poor generalization capability to unseen subjects and
lead to unwanted bias toward different demographic groups. In Western academia,
it is regrettably difficult in a university setting to collect data on these
dark-skinned subjects. Here we show a first attempt to overcome the lack of
dark-skinned subjects by synthetic augmentation. A joint optimization framework
is utilized to translate real videos from light-skinned subjects to dark skin
tones while retaining their pulsatile signals. In the experiment, our method
exhibits around 31% reduction in mean absolute error for the dark-skinned group
and 46% improvement on bias mitigation for all the groups, as compared with the
previous work trained with just real samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ba_Y/0/1/0/all/0/1"&gt;Yunhao Ba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karinca_K/0/1/0/all/0/1"&gt;Kerim Doruk Karinca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bozkurt_O/0/1/0/all/0/1"&gt;Oyku Deniz Bozkurt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadambi_A/0/1/0/all/0/1"&gt;Achuta Kadambi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Unsupervised Domain Adaptation for Visual Recognition. (arXiv:2106.06112v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06112</id>
        <link href="http://arxiv.org/abs/2106.06112"/>
        <updated>2021-06-14T01:38:52.493Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) aims to learn a well-performed model in
an unlabeled target domain by leveraging labeled data from one or multiple
related source domains. It remains a great challenge due to 1) the lack of
annotations in the target domain and 2) the rich discrepancy between the
distributions of source and target data. We propose Spectral UDA (SUDA), an
efficient yet effective UDA technique that works in the spectral space and is
generic across different visual recognition tasks in detection, classification
and segmentation. SUDA addresses UDA challenges from two perspectives. First,
it mitigates inter-domain discrepancies by a spectrum transformer (ST) that
maps source and target images into spectral space and learns to enhance
domain-invariant spectra while suppressing domain-variant spectra
simultaneously. To this end, we design novel adversarial multi-head spectrum
attention that leverages contextual information to identify domain-variant and
domain-invariant spectra effectively. Second, it mitigates the lack of
annotations in target domain by introducing multi-view spectral learning which
aims to learn comprehensive yet confident target representations by maximizing
the mutual information among multiple ST augmentations capturing different
spectral views of each target sample. Extensive experiments over different
visual tasks (e.g., detection, classification and segmentation) show that SUDA
achieves superior accuracy and it is also complementary with state-of-the-art
UDA methods with consistent performance boosts but little extra computation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Part-aware Panoptic Segmentation. (arXiv:2106.06351v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06351</id>
        <link href="http://arxiv.org/abs/2106.06351"/>
        <updated>2021-06-14T01:38:52.486Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce the new scene understanding task of Part-aware
Panoptic Segmentation (PPS), which aims to understand a scene at multiple
levels of abstraction, and unifies the tasks of scene parsing and part parsing.
For this novel task, we provide consistent annotations on two commonly used
datasets: Cityscapes and Pascal VOC. Moreover, we present a single metric to
evaluate PPS, called Part-aware Panoptic Quality (PartPQ). For this new task,
using the metric and annotations, we set multiple baselines by merging results
of existing state-of-the-art methods for panoptic segmentation and part
segmentation. Finally, we conduct several experiments that evaluate the
importance of the different levels of abstraction in this single task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geus_D/0/1/0/all/0/1"&gt;Daan de Geus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meletis_P/0/1/0/all/0/1"&gt;Panagiotis Meletis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chenyang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1"&gt;Xiaoxiao Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubbelman_G/0/1/0/all/0/1"&gt;Gijs Dubbelman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyGAD: An Intuitive Genetic Algorithm Python Library. (arXiv:2106.06158v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.06158</id>
        <link href="http://arxiv.org/abs/2106.06158"/>
        <updated>2021-06-14T01:38:52.479Z</updated>
        <summary type="html"><![CDATA[This paper introduces PyGAD, an open-source easy-to-use Python library for
building the genetic algorithm. PyGAD supports a wide range of parameters to
give the user control over everything in its life cycle. This includes, but is
not limited to, population, gene value range, gene data type, parent selection,
crossover, and mutation. PyGAD is designed as a general-purpose optimization
library that allows the user to customize the fitness function. Its usage
consists of 3 main steps: build the fitness function, create an instance of the
pygad.GA class, and calling the pygad.GA.run() method. The library supports
training deep learning models created either with PyGAD itself or with
frameworks like Keras and PyTorch. Given its stable state, PyGAD is also in
active development to respond to the user's requested features and enhancement
received on GitHub https://github.com/ahmedfgad/GeneticAlgorithmPython. PyGAD
comes with documentation https://pygad.readthedocs.io for further details and
examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gad_A/0/1/0/all/0/1"&gt;Ahmed Fawzy Gad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Deep Learning Architectures for Fast Identification of Bacterial Strains in Resource-Constrained Devices. (arXiv:2106.06505v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06505</id>
        <link href="http://arxiv.org/abs/2106.06505"/>
        <updated>2021-06-14T01:38:52.472Z</updated>
        <summary type="html"><![CDATA[This work presents twelve fine-tuned deep learning architectures to solve the
bacterial classification problem over the Digital Image of Bacterial Species
Dataset. The base architectures were mainly published as mobile or efficient
solutions to the ImageNet challenge, and all experiments presented in this work
consisted of making several modifications to the original designs, in order to
make them able to solve the bacterial classification problem by using
fine-tuning and transfer learning techniques. This work also proposes a novel
data augmentation technique for this dataset, which is based on the idea of
artificial zooming, strongly increasing the performance of every tested
architecture, even doubling it in some cases. In order to get robust and
complete evaluations, all experiments were performed with 10-fold
cross-validation and evaluated with five different metrics: top-1 and top-5
accuracy, precision, recall, and F1 score. This paper presents a complete
comparison of the twelve different architectures, cross-validated with the
original and the augmented version of the dataset, the results are also
compared with several literature methods. Overall, eight of the eleven
architectures surpassed the 0.95 scores in top-1 accuracy with our data
augmentation method, being 0.9738 the highest top-1 accuracy. The impact of the
data augmentation technique is reported with relative improvement scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_R/0/1/0/all/0/1"&gt;R. Gallardo Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1"&gt;S. Jarqu&amp;#xed;n Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1"&gt;B. Beltr&amp;#xe1;n Mart&amp;#xed;nez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gracidas_C/0/1/0/all/0/1"&gt;C. Hern&amp;#xe1;ndez Gracidas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torres_R/0/1/0/all/0/1"&gt;R. Mart&amp;#xed;nez Torres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UnNatural Language Inference. (arXiv:2101.00010v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00010</id>
        <link href="http://arxiv.org/abs/2101.00010"/>
        <updated>2021-06-14T01:38:52.464Z</updated>
        <summary type="html"><![CDATA[Recent investigations into the inner-workings of state-of-the-art large-scale
pre-trained Transformer-based Natural Language Understanding (NLU) models
indicate that they appear to know humanlike syntax, at least to some extent. We
provide novel evidence that complicates this claim: we find that
state-of-the-art Natural Language Inference (NLI) models assign the same labels
to permuted examples as they do to the original, i.e. they are largely
invariant to random word-order permutations. This behavior notably differs from
that of humans; we struggle with ungrammatical sentences. To measure the
severity of this issue, we propose a suite of metrics and investigate which
properties of particular permutations lead models to be word-order invariant.
In the MNLI dataset, for example, we find almost all (98.7%) examples contain
at least one permutation which elicits the gold label. Models are sometimes
even able to assign gold labels to permutations that they originally failed to
predict correctly. We provide a comprehensive empirical evaluation of this
phenomenon, and further show that this issue exists for both Transformers and
pre-Transformer RNN / ConvNet based encoders, as well as across multiple
languages (English and Mandarin Chinese). Our code and data are available at
https://github.com/facebookresearch/unlu.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1"&gt;Koustuv Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1"&gt;Prasanna Parthasarathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1"&gt;Adina Williams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Video Synthesis with Action Graphs. (arXiv:2006.15327v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15327</id>
        <link href="http://arxiv.org/abs/2006.15327"/>
        <updated>2021-06-14T01:38:52.447Z</updated>
        <summary type="html"><![CDATA[Videos of actions are complex signals containing rich compositional structure
in space and time. Current video generation methods lack the ability to
condition the generation on multiple coordinated and potentially simultaneous
timed actions. To address this challenge, we propose to represent the actions
in a graph structure called Action Graph and present the new ``Action Graph To
Video'' synthesis task. Our generative model for this task (AG2Vid)
disentangles motion and appearance features, and by incorporating a scheduling
mechanism for actions facilitates a timely and coordinated video generation. We
train and evaluate AG2Vid on the CATER and Something-Something V2 datasets, and
show that the resulting videos have better visual quality and semantic
consistency compared to baselines. Finally, our model demonstrates zero-shot
abilities by synthesizing novel compositions of the learned actions. For code
and pretrained models, see the project page https://roeiherz.github.io/AG2Video]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1"&gt;Amir Bar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1"&gt;Roei Herzig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1"&gt;Anna Rohrbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1"&gt;Amir Globerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Semantic Scene Completion: a Survey. (arXiv:2103.07466v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07466</id>
        <link href="http://arxiv.org/abs/2103.07466"/>
        <updated>2021-06-14T01:38:52.440Z</updated>
        <summary type="html"><![CDATA[Semantic Scene Completion (SSC) aims to jointly estimate the complete
geometry and semantics of a scene, assuming partial sparse input. In the last
years following the multiplication of large-scale 3D datasets, SSC has gained
significant momentum in the research community because it holds unresolved
challenges. Specifically, SSC lies in the ambiguous completion of large
unobserved areas and the weak supervision signal of the ground truth. This led
to a substantially increasing number of papers on the matter. This survey aims
to identify, compare and analyze the techniques providing a critical analysis
of the SSC literature on both methods and datasets. Throughout the paper, we
provide an in-depth analysis of the existing works covering all choices made by
the authors while highlighting the remaining avenues of research. SSC
performance of the SoA on the most popular datasets is also evaluated and
analyzed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roldao_L/0/1/0/all/0/1"&gt;Luis Roldao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1"&gt;Raoul de Charette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verroust_Blondet_A/0/1/0/all/0/1"&gt;Anne Verroust-Blondet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Physical Adversarial Examples for Autonomous Driving with Fast and Differentiable Image Compositing. (arXiv:2010.08844v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08844</id>
        <link href="http://arxiv.org/abs/2010.08844"/>
        <updated>2021-06-14T01:38:52.433Z</updated>
        <summary type="html"><![CDATA[There is considerable evidence that deep neural networks are vulnerable to
adversarial perturbations applied directly to their digital inputs. However, it
remains an open question whether this translates to vulnerabilities in real
systems. For example, an attack on self-driving cars would in practice entail
modifying the driving environment, which then impacts the video inputs to the
car's controller, thereby indirectly leading to incorrect driving decisions.
Such attacks require accounting for system dynamics and tracking viewpoint
changes. We propose a scalable approach for finding adversarial modifications
of a simulated autonomous driving environment using a differentiable
approximation for the mapping from environmental modifications (rectangles on
the road) to the corresponding video inputs to the controller neural network.
Given the parameters of the rectangles, our proposed differentiable mapping
composites them onto pre-recorded video streams of the original environment,
accounting for geometric and color variations. Moreover, we propose a multiple
trajectory sampling approach that enables our attacks to be robust to a car's
self-correcting behavior. When combined with a neural network-based controller,
our approach allows the design of adversarial modifications through end-to-end
gradient-based optimization. Using the Carla autonomous driving simulator, we
show that our approach is significantly more scalable and far more effective at
identifying autonomous vehicle vulnerabilities in simulation experiments than a
state-of-the-art approach based on Bayesian Optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinghan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boloor_A/0/1/0/all/0/1"&gt;Adith Boloor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1"&gt;Ayan Chakrabarti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1"&gt;Yevgeniy Vorobeychik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is Multimodality?. (arXiv:2103.06304v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06304</id>
        <link href="http://arxiv.org/abs/2103.06304"/>
        <updated>2021-06-14T01:38:52.425Z</updated>
        <summary type="html"><![CDATA[The last years have shown rapid developments in the field of multimodal
machine learning, combining e.g., vision, text or speech. In this position
paper we explain how the field uses outdated definitions of multimodality that
prove unfit for the machine learning era. We propose a new task-relative
definition of (multi)modality in the context of multimodal machine learning
that focuses on representations and information that are relevant for a given
machine learning task. With our new definition of multimodality we aim to
provide a missing foundation for multimodal research, an important component of
language grounding and a crucial milestone towards NLU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1"&gt;Letitia Parcalabescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trost_N/0/1/0/all/0/1"&gt;Nils Trost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Anette Frank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast, Accurate Barcode Detection in Ultra High-Resolution Images. (arXiv:2102.06868v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06868</id>
        <link href="http://arxiv.org/abs/2102.06868"/>
        <updated>2021-06-14T01:38:52.419Z</updated>
        <summary type="html"><![CDATA[Object detection in Ultra High-Resolution (UHR) images has long been a
challenging problem in computer vision due to the varying scales of the
targeted objects. When it comes to barcode detection, resizing UHR input images
to smaller sizes often leads to the loss of pertinent information, while
processing them directly is highly inefficient and computationally expensive.
In this paper, we propose using semantic segmentation to achieve a fast and
accurate detection of barcodes of various scales in UHR images. Our pipeline
involves a modified Region Proposal Network (RPN) on images of size greater
than 10k$\times$10k and a newly proposed Y-Net segmentation network, followed
by a post-processing workflow for fitting a bounding box around each segmented
barcode mask. The end-to-end system has a latency of 16 milliseconds, which is
$2.5\times$ faster than YOLOv4 and $5.9\times$ faster than Mask R-CNN. In terms
of accuracy, our method outperforms YOLOv4 and Mask R-CNN by a $mAP$ of 5.5%
and 47.1% respectively, on a synthetic dataset. We have made available the
generated synthetic barcode dataset and its code at
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quenum_J/0/1/0/all/0/1"&gt;Jerome Quenum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kehan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zakhor_A/0/1/0/all/0/1"&gt;Avideh Zakhor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Real-World Blind Face Restoration with Generative Facial Prior. (arXiv:2101.04061v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04061</id>
        <link href="http://arxiv.org/abs/2101.04061"/>
        <updated>2021-06-14T01:38:52.402Z</updated>
        <summary type="html"><![CDATA[Blind face restoration usually relies on facial priors, such as facial
geometry prior or reference prior, to restore realistic and faithful details.
However, very low-quality inputs cannot offer accurate geometric prior while
high-quality references are inaccessible, limiting the applicability in
real-world scenarios. In this work, we propose GFP-GAN that leverages rich and
diverse priors encapsulated in a pretrained face GAN for blind face
restoration. This Generative Facial Prior (GFP) is incorporated into the face
restoration process via novel channel-split spatial feature transform layers,
which allow our method to achieve a good balance of realness and fidelity.
Thanks to the powerful generative facial prior and delicate designs, our
GFP-GAN could jointly restore facial details and enhance colors with just a
single forward pass, while GAN inversion methods require expensive
image-specific optimization at inference. Extensive experiments show that our
method achieves superior performance to prior art on both synthetic and
real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xintao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Honglun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Ying Shan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridge the Gap Between Model-based and Model-free Human Reconstruction. (arXiv:2106.06313v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06313</id>
        <link href="http://arxiv.org/abs/2106.06313"/>
        <updated>2021-06-14T01:38:52.395Z</updated>
        <summary type="html"><![CDATA[It is challenging to directly estimate the geometry of human from a single
image due to the high diversity and complexity of body shapes with the various
clothing styles. Most of model-based approaches are limited to predict the
shape and pose of a minimally clothed body with over-smoothing surface.
Although capturing the fine detailed geometries, the model-free methods are
lack of the fixed mesh topology. To address these issues, we propose a novel
topology-preserved human reconstruction approach by bridging the gap between
model-based and model-free human reconstruction. We present an end-to-end
neural network that simultaneously predicts the pixel-aligned implicit surface
and the explicit mesh model built by graph convolutional neural network.
Moreover, an extra graph convolutional neural network is employed to estimate
the vertex offsets between the implicit surface and parametric mesh model.
Finally, we suggest an efficient implicit registration method to refine the
neural network output in implicit space. Experiments on DeepHuman dataset
showed that our approach is effective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Lixiang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianke Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions. (arXiv:2104.02710v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02710</id>
        <link href="http://arxiv.org/abs/2104.02710"/>
        <updated>2021-06-14T01:38:52.388Z</updated>
        <summary type="html"><![CDATA[Multi-agent behavior modeling aims to understand the interactions that occur
between agents. We present a multi-agent dataset from behavioral neuroscience,
the Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists
of trajectory data of social interactions, recorded from videos of freely
behaving mice in a standard resident-intruder assay. To help accelerate
behavioral studies, the CalMS21 dataset provides benchmarks to evaluate the
performance of automated behavior classification methods in three settings: (1)
for training on large behavioral datasets all annotated by a single annotator,
(2) for style transfer to learn inter-annotator differences in behavior
definitions, and (3) for learning of new behaviors of interest given limited
training data. The dataset consists of 6 million frames of unlabeled tracked
poses of interacting mice, as well as over 1 million frames with tracked poses
and corresponding frame-level behavior annotations. The challenge of our
dataset is to be able to classify behaviors accurately using both labeled and
unlabeled tracking data, as well as being able to generalize to new settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jennifer J. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karigo_T/0/1/0/all/0/1"&gt;Tomomi Karigo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1"&gt;Dipam Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1"&gt;Sharada P. Mohanty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wild_B/0/1/0/all/0/1"&gt;Benjamin Wild&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Quan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1"&gt;David J. Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1"&gt;Pietro Perona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yisong Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1"&gt;Ann Kennedy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Survey of Image Based Graph Neural Networks. (arXiv:2106.06307v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06307</id>
        <link href="http://arxiv.org/abs/2106.06307"/>
        <updated>2021-06-14T01:38:52.381Z</updated>
        <summary type="html"><![CDATA[In this survey paper, we analyze image based graph neural networks and
propose a three-step classification approach. We first convert the image into
superpixels using the Quickshift algorithm so as to reduce 30% of the input
data. The superpixels are subsequently used to generate a region adjacency
graph. Finally, the graph is passed through a state-of-art graph convolutional
neural network to get classification scores. We also analyze the spatial and
spectral convolution filtering techniques in graph neural networks.
Spectral-based models perform better than spatial-based models and classical
CNN with lesser compute cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nazir_U/0/1/0/all/0/1"&gt;Usman Nazir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;He Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taj_M/0/1/0/all/0/1"&gt;Murtaza Taj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[View Generalization for Single Image Textured 3D Models. (arXiv:2106.06533v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06533</id>
        <link href="http://arxiv.org/abs/2106.06533"/>
        <updated>2021-06-14T01:38:52.374Z</updated>
        <summary type="html"><![CDATA[Humans can easily infer the underlying 3D geometry and texture of an object
only from a single 2D image. Current computer vision methods can do this, too,
but suffer from view generalization problems - the models inferred tend to make
poor predictions of appearance in novel views. As for generalization problems
in machine learning, the difficulty is balancing single-view accuracy (cf.
training error; bias) with novel view accuracy (cf. test error; variance). We
describe a class of models whose geometric rigidity is easily controlled to
manage this tradeoff. We describe a cycle consistency loss that improves view
generalization (roughly, a model from a generated view should predict the
original view well). View generalization of textures requires that models share
texture information, so a car seen from the back still has headlights because
other cars have headlights. We describe a cycle consistency loss that
encourages model textures to be aligned, so as to encourage sharing. We compare
our method against the state-of-the-art method and show both qualitative and
quantitative improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1"&gt;Anand Bhattad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dundar_A/0/1/0/all/0/1"&gt;Aysegul Dundar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guilin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1"&gt;Andrew Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1"&gt;Bryan Catanzaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scale-invariant scale-channel networks: Deep networks that generalise to previously unseen scales. (arXiv:2106.06418v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06418</id>
        <link href="http://arxiv.org/abs/2106.06418"/>
        <updated>2021-06-14T01:38:52.368Z</updated>
        <summary type="html"><![CDATA[The ability to handle large scale variations is crucial for many real world
visual tasks. A straightforward approach for handling scale in a deep network
is to process an image at several scales simultaneously in a set of scale
channels. Scale invariance can then, in principle, be achieved by using weight
sharing between the scale channels together with max or average pooling over
the outputs from the scale channels. The ability of such scale channel networks
to generalise to scales not present in the training set over significant scale
ranges has, however, not previously been explored.

In this paper, we present a systematic study of this methodology by
implementing different types of scale channel networks and evaluating their
ability to generalise to previously unseen scales. We develop a formalism for
analysing the covariance and invariance properties of scale channel networks,
and explore how different design choices, unique to scaling transformations,
affect the overall performance of scale channel networks. We first show that
two previously proposed scale channel network designs do not generalise well to
scales not present in the training set. We explain theoretically and
demonstrate experimentally why generalisation fails in these cases.

We then propose a new type of foveated scale channel architecture}, where the
scale channels process increasingly larger parts of the image with decreasing
resolution. This new type of scale channel network is shown to generalise
extremely well, provided sufficient image resolution and the absence of
boundary effects. Our proposed FovMax and FovAvg networks perform almost
identically over a scale range of 8, also when training on single scale
training data, and do also give improved performance when learning from
datasets with large scale variations in the small sample regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jansson_Y/0/1/0/all/0/1"&gt;Ylva Jansson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindeberg_T/0/1/0/all/0/1"&gt;Tony Lindeberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HMM-Free Encoder Pre-Training for Streaming RNN Transducer. (arXiv:2104.10764v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10764</id>
        <link href="http://arxiv.org/abs/2104.10764"/>
        <updated>2021-06-14T01:38:52.361Z</updated>
        <summary type="html"><![CDATA[This work describes an encoder pre-training procedure using frame-wise label
to improve the training of streaming recurrent neural network transducer
(RNN-T) model. Streaming RNN-T trained from scratch usually performs worse than
non-streaming RNN-T. Although it is common to address this issue through
pre-training components of RNN-T with other criteria or frame-wise alignment
guidance, the alignment is not easily available in end-to-end manner. In this
work, frame-wise alignment, used to pre-train streaming RNN-T's encoder, is
generated without using a HMM-based system. Therefore an all-neural framework
equipping HMM-free encoder pre-training is constructed. This is achieved by
expanding the spikes of CTC model to their left/right blank frames, and two
expanding strategies are proposed. To our best knowledge, this is the first
work to simulate HMM-based frame-wise label using CTC model for pre-training.
Experiments conducted on LibriSpeech and MLS English tasks show the proposed
pre-training procedure, compared with random initialization, reduces the WER by
relatively 5%~11% and the emission latency by 60 ms. Besides, the method is
lexicon-free, so it is friendly to new languages without manually designed
lexicon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jingyu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yufeng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1"&gt;Junfeng Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jinkun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zejun Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Intra-Batch Connections for Deep Metric Learning. (arXiv:2102.07753v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07753</id>
        <link href="http://arxiv.org/abs/2102.07753"/>
        <updated>2021-06-14T01:38:52.339Z</updated>
        <summary type="html"><![CDATA[The goal of metric learning is to learn a function that maps samples to a
lower-dimensional space where similar samples lie closer than dissimilar ones.
Particularly, deep metric learning utilizes neural networks to learn such a
mapping. Most approaches rely on losses that only take the relations between
pairs or triplets of samples into account, which either belong to the same
class or two different classes. However, these methods do not explore the
embedding space in its entirety. To this end, we propose an approach based on
message passing networks that takes all the relations in a mini-batch into
account. We refine embedding vectors by exchanging messages among all samples
in a given batch allowing the training process to be aware of its overall
structure. Since not all samples are equally important to predict a decision
boundary, we use an attention mechanism during message passing to allow samples
to weigh the importance of each neighbor accordingly. We achieve
state-of-the-art results on clustering and image retrieval on the CUB-200-2011,
Cars196, Stanford Online Products, and In-Shop Clothes datasets. To facilitate
further research, we make available the code and the models at
https://github.com/dvl-tum/intra_batch_connections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seidenschwarz_J/0/1/0/all/0/1"&gt;Jenny Seidenschwarz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1"&gt;Ismail Elezi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1"&gt;Laura Leal-Taix&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A deep learning approach to clustering visual arts. (arXiv:2106.06234v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06234</id>
        <link href="http://arxiv.org/abs/2106.06234"/>
        <updated>2021-06-14T01:38:52.333Z</updated>
        <summary type="html"><![CDATA[Clustering artworks is difficult for several reasons. On the one hand,
recognizing meaningful patterns based on domain knowledge and visual perception
is extremely hard. On the other hand, applying traditional clustering and
feature reduction techniques to the highly dimensional pixel space can be
ineffective. To address these issues, in this paper we propose DELIUS: a DEep
learning approach to cLustering vIsUal artS. The method uses a pre-trained
convolutional network to extract features and then feeds these features into a
deep embedded clustering model, where the task of mapping the raw input data to
a latent space is jointly optimized with the task of finding a set of cluster
centroids in this latent space. Quantitative and qualitative experimental
results show the effectiveness of the proposed method. DELIUS can be useful for
several tasks related to art analysis, in particular visual link retrieval and
historical knowledge discovery in painting datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castellano_G/0/1/0/all/0/1"&gt;Giovanna Castellano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vessio_G/0/1/0/all/0/1"&gt;Gennaro Vessio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A self-adapting super-resolution structures framework for automatic design of GAN. (arXiv:2106.06011v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06011</id>
        <link href="http://arxiv.org/abs/2106.06011"/>
        <updated>2021-06-14T01:38:52.326Z</updated>
        <summary type="html"><![CDATA[With the development of deep learning, the single super-resolution image
reconstruction network models are becoming more and more complex. Small changes
in hyperparameters of the models have a greater impact on model performance. In
the existing works, experts have gradually explored a set of optimal model
parameters based on empirical values or performing brute-force search. In this
paper, we introduce a new super-resolution image reconstruction generative
adversarial network framework, and a Bayesian optimization method used to
optimizing the hyperparameters of the generator and discriminator. The
generator is made by self-calibrated convolution, and discriminator is made by
convolution lays. We have defined the hyperparameters such as the number of
network layers and the number of neurons. Our method adopts Bayesian
optimization as a optimization policy of GAN in our model. Not only can find
the optimal hyperparameter solution automatically, but also can construct a
super-resolution image reconstruction network, reducing the manual workload.
Experiments show that Bayesian optimization can search the optimal solution
earlier than the other two optimization algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yibo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haidi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yiming Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shunyao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingliang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Antipodal Robotic Grasping using Generative Residual Convolutional Neural Network. (arXiv:1909.04810v4 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.04810</id>
        <link href="http://arxiv.org/abs/1909.04810"/>
        <updated>2021-06-14T01:38:52.310Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a modular robotic system to tackle the problem of
generating and performing antipodal robotic grasps for unknown objects from
n-channel image of the scene. We propose a novel Generative Residual
Convolutional Neural Network (GR-ConvNet) model that can generate robust
antipodal grasps from n-channel input at real-time speeds (~20ms). We evaluate
the proposed model architecture on standard datasets and a diverse set of
household objects. We achieved state-of-the-art accuracy of 97.7% and 94.6% on
Cornell and Jacquard grasping datasets respectively. We also demonstrate a
grasp success rate of 95.4% and 93% on household and adversarial objects
respectively using a 7 DoF robotic arm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumra_S/0/1/0/all/0/1"&gt;Sulabh Kumra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1"&gt;Shirin Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahin_F/0/1/0/all/0/1"&gt;Ferat Sahin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiPointNet: Binary Neural Network for Point Clouds. (arXiv:2010.05501v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05501</id>
        <link href="http://arxiv.org/abs/2010.05501"/>
        <updated>2021-06-14T01:38:52.303Z</updated>
        <summary type="html"><![CDATA[To alleviate the resource constraint for real-time point cloud applications
that run on edge devices, in this paper we present BiPointNet, the first model
binarization approach for efficient deep learning on point clouds. We discover
that the immense performance drop of binarized models for point clouds mainly
stems from two challenges: aggregation-induced feature homogenization that
leads to a degradation of information entropy, and scale distortion that
hinders optimization and invalidates scale-sensitive structures. With
theoretical justifications and in-depth analysis, our BiPointNet introduces
Entropy-Maximizing Aggregation (EMA) to modulate the distribution before
aggregation for the maximum information entropy, and Layer-wise Scale Recovery
(LSR) to efficiently restore feature representation capacity. Extensive
experiments show that BiPointNet outperforms existing binarization methods by
convincing margins, at the level even comparable with the full precision
counterpart. We highlight that our techniques are generic, guaranteeing
significant improvements on various fundamental tasks and mainstream backbones.
Moreover, BiPointNet gives an impressive 14.7x speedup and 18.9x storage saving
on real-world resource-constrained devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1"&gt;Haotong Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhongang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mingyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yifu Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Haiyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1"&gt;Shuai Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianglong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Global Illumination Decomposition of Videos. (arXiv:1908.01961v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.01961</id>
        <link href="http://arxiv.org/abs/1908.01961"/>
        <updated>2021-06-14T01:38:52.296Z</updated>
        <summary type="html"><![CDATA[We propose the first approach for the decomposition of a monocular color
video into direct and indirect illumination components in real time. We
retrieve, in separate layers, the contribution made to the scene appearance by
the scene reflectance, the light sources and the reflections from various
coherent scene regions to one another. Existing techniques that invert global
light transport require image capture under multiplexed controlled lighting, or
only enable the decomposition of a single image at slow off-line frame rates.
In contrast, our approach works for regular videos and produces temporally
coherent decomposition layers at real-time frame rates. At the core of our
approach are several sparsity priors that enable the estimation of the
per-pixel direct and indirect illumination layers based on a small set of
jointly estimated base reflectance colors. The resulting variational
decomposition problem uses a new formulation based on sparse and dense sets of
non-linear equations that we solve efficiently using a novel alternating
data-parallel optimization strategy. We evaluate our approach qualitatively and
quantitatively, and show improvements over the state of the art in this field,
in both quality and runtime. In addition, we demonstrate various real-time
appearance editing applications for videos with consistent illumination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1"&gt;Abhimitra Meka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiei_M/0/1/0/all/0/1"&gt;Mohammad Shafiei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1"&gt;Michael Zollhoefer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1"&gt;Christian Richardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conterfactual Generative Zero-Shot Semantic Segmentation. (arXiv:2106.06360v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06360</id>
        <link href="http://arxiv.org/abs/2106.06360"/>
        <updated>2021-06-14T01:38:52.289Z</updated>
        <summary type="html"><![CDATA[zero-shot learning is an essential part of computer vision. As a classical
downstream task, zero-shot semantic segmentation has been studied because of
its applicant value. One of the popular zero-shot semantic segmentation methods
is based on the generative model Most new proposed works added structures on
the same architecture to enhance this model. However, we found that, from the
view of causal inference, the result of the original model has been influenced
by spurious statistical relationships. Thus the performance of the prediction
shows severe bias. In this work, we consider counterfactual methods to avoid
the confounder in the original model. Based on this method, we proposed a new
framework for zero-shot semantic segmentation. Our model is compared with
baseline models on two real-world datasets, Pascal-VOC and Pascal-Context. The
experiment results show proposed models can surpass previous confounded models
and can still make use of additional structures to improve the performance. We
also design a simple structure based on Graph Convolutional Networks (GCN) in
this work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1"&gt;Feihong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1"&gt;Ping Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Generalising Neural Implicit Representations. (arXiv:2101.12690v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12690</id>
        <link href="http://arxiv.org/abs/2101.12690"/>
        <updated>2021-06-14T01:38:52.283Z</updated>
        <summary type="html"><![CDATA[Neural implicit representations have shown substantial improvements in
efficiently storing 3D data, when compared to conventional formats. However,
the focus of existing work has mainly been on storage and subsequent
reconstruction. In this work, we show that training neural representations for
reconstruction tasks alongside conventional tasks can produce more general
encodings that admit equal quality reconstructions to single task training,
whilst improving results on conventional tasks when compared to single task
encodings. We reformulate the semantic segmentation task, creating a more
representative task for implicit representation contexts, and through
multi-task experiments on reconstruction, classification, and segmentation,
show our approach learns feature rich encodings that admit equal performance
for each task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Costain_T/0/1/0/all/0/1"&gt;Theo W. Costain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1"&gt;Victor Adrian Prisacariu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation Disentanglement for Multi-modal brain MR Analysis. (arXiv:2102.11456v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11456</id>
        <link href="http://arxiv.org/abs/2102.11456"/>
        <updated>2021-06-14T01:38:52.242Z</updated>
        <summary type="html"><![CDATA[Multi-modal MRIs are widely used in neuroimaging applications since different
MR sequences provide complementary information about brain structures. Recent
works have suggested that multi-modal deep learning analysis can benefit from
explicitly disentangling anatomical (shape) and modality (appearance)
information into separate image presentations. In this work, we challenge
mainstream strategies by showing that they do not naturally lead to
representation disentanglement both in theory and in practice. To address this
issue, we propose a margin loss that regularizes the similarity in
relationships of the representations across subjects and modalities. To enable
robust training, we further use a conditional convolution to design a single
model for encoding images of all modalities. Lastly, we propose a fusion
function to combine the disentangled anatomical representations as a set of
modality-invariant features for downstream tasks. We evaluate the proposed
method on three multi-modal neuroimaging datasets. Experiments show that our
proposed method can achieve superior disentangled representations compared to
existing disentanglement strategies. Results also indicate that the fused
anatomical representation has potential in the downstream task of zero-dose PET
reconstruction and brain tumor segmentation. The code is available at
\url{https://github.com/ouyangjiahong/representation-disentanglement}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1"&gt;Jiahong Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M. Pohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaharchuk_G/0/1/0/all/0/1"&gt;Greg Zaharchuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Next Local Appearance for Video Anomaly Detection. (arXiv:2106.06059v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06059</id>
        <link href="http://arxiv.org/abs/2106.06059"/>
        <updated>2021-06-14T01:38:52.219Z</updated>
        <summary type="html"><![CDATA[We present a local anomaly detection method in videos. As opposed to most
existing methods that are computationally expensive and are not very
generalizable across different video scenes, we propose an adversarial
framework that learns the temporal local appearance variations by predicting
the appearance of a normally behaving object in the next frame of a scene by
only relying on its current and past appearances. In the presence of an
abnormally behaving object, the reconstruction error between the real and the
predicted next appearance of that object indicates the likelihood of an
anomaly. Our method is competitive with the existing state-of-the-art while
being significantly faster for both training and inference and being better at
generalizing to unseen video scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1"&gt;Pankaj Raj Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1"&gt;Guillaume-Alexandre Bilodeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seoud_L/0/1/0/all/0/1"&gt;Lama Seoud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Step-Wise Hierarchical Alignment Network for Image-Text Matching. (arXiv:2106.06509v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06509</id>
        <link href="http://arxiv.org/abs/2106.06509"/>
        <updated>2021-06-14T01:38:52.171Z</updated>
        <summary type="html"><![CDATA[Image-text matching plays a central role in bridging the semantic gap between
vision and language. The key point to achieve precise visual-semantic alignment
lies in capturing the fine-grained cross-modal correspondence between image and
text. Most previous methods rely on single-step reasoning to discover the
visual-semantic interactions, which lacks the ability of exploiting the
multi-level information to locate the hierarchical fine-grained relevance.
Different from them, in this work, we propose a step-wise hierarchical
alignment network (SHAN) that decomposes image-text matching into multi-step
cross-modal reasoning process. Specifically, we first achieve local-to-local
alignment at fragment level, following by performing global-to-local and
global-to-global alignment at context level sequentially. This progressive
alignment strategy supplies our model with more complementary and sufficient
semantic clues to understand the hierarchical correlations between image and
text. The experimental results on two benchmark datasets demonstrate the
superiority of our proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1"&gt;Zhong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kexin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoran Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Small Object Detection for Near Real-Time Egocentric Perception in a Manual Assembly Scenario. (arXiv:2106.06403v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06403</id>
        <link href="http://arxiv.org/abs/2106.06403"/>
        <updated>2021-06-14T01:38:52.139Z</updated>
        <summary type="html"><![CDATA[Detecting small objects in video streams of head-worn augmented reality
devices in near real-time is a huge challenge: training data is typically
scarce, the input video stream can be of limited quality, and small objects are
notoriously hard to detect. In industrial scenarios, however, it is often
possible to leverage contextual knowledge for the detection of small objects.
Furthermore, CAD data of objects are typically available and can be used to
generate synthetic training data. We describe a near real-time small object
detection pipeline for egocentric perception in a manual assembly scenario: We
generate a training data set based on CAD data and realistic backgrounds in
Unity. We then train a YOLOv4 model for a two-stage detection process: First,
the context is recognized, then the small object of interest is detected. We
evaluate our pipeline on the augmented reality device Microsoft Hololens 2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tavakoli_H/0/1/0/all/0/1"&gt;Hooman Tavakoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walunj_S/0/1/0/all/0/1"&gt;Snehal Walunj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pahlevannejad_P/0/1/0/all/0/1"&gt;Parsha Pahlevannejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plociennik_C/0/1/0/all/0/1"&gt;Christiane Plociennik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruskowski_M/0/1/0/all/0/1"&gt;Martin Ruskowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalizing Cross-Document Event Coreference Resolution Across Multiple Corpora. (arXiv:2011.12249v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12249</id>
        <link href="http://arxiv.org/abs/2011.12249"/>
        <updated>2021-06-14T01:38:52.119Z</updated>
        <summary type="html"><![CDATA[Cross-document event coreference resolution (CDCR) is an NLP task in which
mentions of events need to be identified and clustered throughout a collection
of documents. CDCR aims to benefit downstream multi-document applications, but
despite recent progress on corpora and system development, downstream
improvements from applying CDCR have not been shown yet. We make the
observation that every CDCR system to date was developed, trained, and tested
only on a single respective corpus. This raises strong concerns on their
generalizability -- a must-have for downstream applications where the magnitude
of domains or event mentions is likely to exceed those found in a curated
corpus. To investigate this assumption, we define a uniform evaluation setup
involving three CDCR corpora: ECB+, the Gun Violence Corpus and the Football
Coreference Corpus (which we reannotate on token level to make our analysis
possible). We compare a corpus-independent, feature-based system against a
recent neural system developed for ECB+. Whilst being inferior in absolute
numbers, the feature-based system shows more consistent performance across all
corpora whereas the neural system is hit-and-miss. Via model introspection, we
find that the importance of event actions, event time, etc. for resolving
coreference in practice varies greatly between the corpora. Additional analysis
shows that several systems overfit on the structure of the ECB+ corpus. We
conclude with recommendations on how to achieve generally applicable CDCR
systems in the future -- the most important being that evaluation on multiple
CDCR corpora is strongly necessary. To facilitate future research, we release
our dataset, annotation guidelines, and system implementation to the public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bugert_M/0/1/0/all/0/1"&gt;Michael Bugert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1"&gt;Nils Reimers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Discussion on Building Practical NLP Leaderboards: The Case of Machine Translation. (arXiv:2106.06292v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06292</id>
        <link href="http://arxiv.org/abs/2106.06292"/>
        <updated>2021-06-14T01:38:52.100Z</updated>
        <summary type="html"><![CDATA[Recent advances in AI and ML applications have benefited from rapid progress
in NLP research. Leaderboards have emerged as a popular mechanism to track and
accelerate progress in NLP through competitive model development. While this
has increased interest and participation, the over-reliance on single, and
accuracy-based metrics have shifted focus from other important metrics that
might be equally pertinent to consider in real-world contexts. In this paper,
we offer a preliminary discussion of the risks associated with focusing
exclusively on accuracy metrics and draw on recent discussions to highlight
prescriptive suggestions on how to develop more practical and effective
leaderboards that can better reflect the real-world utility of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Santy_S/0/1/0/all/0/1"&gt;Sebastin Santy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1"&gt;Prasanta Bhattacharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MlTr: Multi-label Classification with Transformer. (arXiv:2106.06195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06195</id>
        <link href="http://arxiv.org/abs/2106.06195"/>
        <updated>2021-06-14T01:38:52.084Z</updated>
        <summary type="html"><![CDATA[The task of multi-label image classification is to recognize all the object
labels presented in an image. Though advancing for years, small objects,
similar objects and objects with high conditional probability are still the
main bottlenecks of previous convolutional neural network(CNN) based models,
limited by convolutional kernels' representational capacity. Recent vision
transformer networks utilize the self-attention mechanism to extract the
feature of pixel granularity, which expresses richer local semantic
information, while is insufficient for mining global spatial dependence. In
this paper, we point out the three crucial problems that CNN-based methods
encounter and explore the possibility of conducting specific transformer
modules to settle them. We put forward a Multi-label Transformer
architecture(MlTr) constructed with windows partitioning, in-window pixel
attention, cross-window attention, particularly improving the performance of
multi-label image classification tasks. The proposed MlTr shows
state-of-the-art results on various prevalent multi-label datasets such as
MS-COCO, Pascal-VOC, and NUS-WIDE with 88.5%, 95.8%, and 65.5% respectively.
The code will be available soon at https://github.com/starmemda/MlTr/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xing Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hezheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiangyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1"&gt;Dong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1"&gt;Nian Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Honglin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Team RUC_AIM3 Technical Report at ActivityNet 2021: Entities Object Localization. (arXiv:2106.06138v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06138</id>
        <link href="http://arxiv.org/abs/2106.06138"/>
        <updated>2021-06-14T01:38:52.076Z</updated>
        <summary type="html"><![CDATA[Entities Object Localization (EOL) aims to evaluate how grounded or faithful
a description is, which consists of caption generation and object grounding.
Previous works tackle this problem by jointly training the two modules in a
framework, which limits the complexity of each module. Therefore, in this work,
we propose to divide these two modules into two stages and improve them
respectively to boost the whole system performance. For the caption generation,
we propose a Unified Multi-modal Pre-training Model (UMPM) to generate event
descriptions with rich objects for better localization. For the object
grounding, we fine-tune the state-of-the-art detection model MDETR and design a
post processing method to make the grounding results more faithful. Our overall
system achieves the state-of-the-art performances on both sub-tasks in Entities
Object Localization challenge at Activitynet 2021, with 72.57 localization
accuracy on the testing set of sub-task I and 0.2477 F1_all_per_sent on the
hidden testing set of sub-task II.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_L/0/1/0/all/0/1"&gt;Ludan Ruan&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jieting Chen&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yuqing Song&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt; (1) ((1) Renmin University of China, (2) INRIA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Language Models for Continuously Evolving Content. (arXiv:2106.06297v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06297</id>
        <link href="http://arxiv.org/abs/2106.06297"/>
        <updated>2021-06-14T01:38:52.068Z</updated>
        <summary type="html"><![CDATA[The content on the web is in a constant state of flux. New entities, issues,
and ideas continuously emerge, while the semantics of the existing conversation
topics gradually shift. In recent years, pre-trained language models like BERT
greatly improved the state-of-the-art for a large spectrum of content
understanding tasks. Therefore, in this paper, we aim to study how these
language models can be adapted to better handle continuously evolving web
content. In our study, we first analyze the evolution of 2013 - 2019 Twitter
data, and unequivocally confirm that a BERT model trained on past tweets would
heavily deteriorate when directly applied to data from later years. Then, we
investigate two possible sources of the deterioration: the semantic shift of
existing tokens and the sub-optimal or failed understanding of new tokens. To
this end, we both explore two different vocabulary composition methods, as well
as propose three sampling methods which help in efficient incremental training
for BERT-like models. Compared to a new model trained from scratch offline, our
incremental training (a) reduces the training costs, (b) achieves better
performance on evolving content, and (c) is suitable for online deployment. The
superiority of our methods is validated using two downstream tasks. We
demonstrate significant improvements when incrementally evolving the model from
a particular base year, on the task of Country Hashtag Prediction, as well as
on the OffensEval 2019 task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hombaiah_S/0/1/0/all/0/1"&gt;Spurthi Amba Hombaiah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mingyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1"&gt;Michael Bendersky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1"&gt;Marc Najork&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards unsupervised phone and word segmentation using self-supervised vector-quantized neural networks. (arXiv:2012.07551v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07551</id>
        <link href="http://arxiv.org/abs/2012.07551"/>
        <updated>2021-06-14T01:38:52.061Z</updated>
        <summary type="html"><![CDATA[We investigate segmenting and clustering speech into low-bitrate phone-like
sequences without supervision. We specifically constrain pretrained
self-supervised vector-quantized (VQ) neural networks so that blocks of
contiguous feature vectors are assigned to the same code, thereby giving a
variable-rate segmentation of the speech into discrete units. Two segmentation
methods are considered. In the first, features are greedily merged until a
prespecified number of segments are reached. The second uses dynamic
programming to optimize a squared error with a penalty term to encourage fewer
but longer segments. We show that these VQ segmentation methods can be used
without alteration across a wide range of tasks: unsupervised phone
segmentation, ABX phone discrimination, same-different word discrimination, and
as inputs to a symbolic word segmentation algorithm. The penalized dynamic
programming method generally performs best. While performance on individual
tasks is only comparable to the state-of-the-art in some cases, in all tasks a
reasonable competing approach is outperformed at a substantially lower bitrate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1"&gt;Herman Kamper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekerk_B/0/1/0/all/0/1"&gt;Benjamin van Niekerk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm. (arXiv:2106.06027v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06027</id>
        <link href="http://arxiv.org/abs/2106.06027"/>
        <updated>2021-06-14T01:38:52.042Z</updated>
        <summary type="html"><![CDATA[Sparse adversarial attacks can fool deep neural networks (DNNs) by only
perturbing a few pixels (regularized by l_0 norm). Recent efforts combine it
with another l_infty imperceptible on the perturbation magnitudes. The
resultant sparse and imperceptible attacks are practically relevant, and
indicate an even higher vulnerability of DNNs that we usually imagined.
However, such attacks are more challenging to generate due to the optimization
difficulty by coupling the l_0 regularizer and box constraints with a
non-convex objective. In this paper, we address this challenge by proposing a
homotopy algorithm, to jointly tackle the sparsity and the perturbation bound
in one unified framework. Each iteration, the main step of our algorithm is to
optimize an l_0-regularized adversarial loss, by leveraging the nonmonotone
Accelerated Proximal Gradient Method (nmAPG) for nonconvex programming; it is
followed by an l_0 change control step, and an optional post-attack step
designed to escape bad local minima. We also extend the algorithm to handling
the structural sparsity regularizer. We extensively examine the effectiveness
of our proposed homotopy attack for both targeted and non-targeted attack
scenarios, on CIFAR-10 and ImageNet datasets. Compared to state-of-the-art
methods, our homotopy attack leads to significantly fewer perturbations, e.g.,
reducing 42.91% on CIFAR-10 and 75.03% on ImageNet (average case, targeted
attack), at similar maximal perturbation magnitudes, when still achieving 100%
attack success rates. Our codes are available at:
https://github.com/VITA-Group/SparseADV_Homotopy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Mingkang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Within-layer Diversity Reduces Generalization Gap. (arXiv:2106.06012v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06012</id>
        <link href="http://arxiv.org/abs/2106.06012"/>
        <updated>2021-06-14T01:38:52.035Z</updated>
        <summary type="html"><![CDATA[Neural networks are composed of multiple layers arranged in a hierarchical
structure jointly trained with a gradient-based optimization, where the errors
are back-propagated from the last layer back to the first one. At each
optimization step, neurons at a given layer receive feedback from neurons
belonging to higher layers of the hierarchy. In this paper, we propose to
complement this traditional 'between-layer' feedback with additional
'within-layer' feedback to encourage diversity of the activations within the
same layer. To this end, we measure the pairwise similarity between the outputs
of the neurons and use it to model the layer's overall diversity. By penalizing
similarities and promoting diversity, we encourage each neuron to learn a
distinctive representation and, thus, to enrich the data representation learned
within the layer and to increase the total capacity of the model. We
theoretically study how the within-layer activation diversity affects the
generalization performance of a neural network and prove that increasing the
diversity of hidden activations reduces the estimation error. In addition to
the theoretical guarantees, we present an empirical study on three datasets
confirming that the proposed approach enhances the performance of
state-of-the-art neural network models and decreases the generalization gap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laakom_F/0/1/0/all/0/1"&gt;Firas Laakom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1"&gt;Jenni Raitoharju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1"&gt;Moncef Gabbouj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning for Text Classification with Information Disentanglement Based Regularization. (arXiv:2104.05489v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05489</id>
        <link href="http://arxiv.org/abs/2104.05489"/>
        <updated>2021-06-14T01:38:52.028Z</updated>
        <summary type="html"><![CDATA[Continual learning has become increasingly important as it enables NLP models
to constantly learn and gain knowledge over time. Previous continual learning
methods are mainly designed to preserve knowledge from previous tasks, without
much emphasis on how to well generalize models to new tasks. In this work, we
propose an information disentanglement based regularization method for
continual learning on text classification. Our proposed method first
disentangles text hidden spaces into representations that are generic to all
tasks and representations specific to each individual task, and further
regularizes these representations differently to better constrain the knowledge
required to generalize. We also introduce two simple auxiliary tasks: next
sentence prediction and task-id prediction, for learning better generic and
specific representation spaces. Experiments conducted on large-scale benchmarks
demonstrate the effectiveness of our method in continual text classification
tasks with various sequences and lengths over state-of-the-art baselines. We
have publicly released our code at https://github.com/GT-SALT/IDBR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yufan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanzhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuezhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Diyi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To Beam Or Not To Beam: That is a Question of Cooperation for Language GANs. (arXiv:2106.06363v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06363</id>
        <link href="http://arxiv.org/abs/2106.06363"/>
        <updated>2021-06-14T01:38:52.020Z</updated>
        <summary type="html"><![CDATA[Due to the discrete nature of words, language GANs require to be optimized
from rewards provided by discriminator networks, via reinforcement learning
methods. This is a much harder setting than for continuous tasks, which enjoy
gradient flows from discriminators to generators, usually leading to dramatic
learning instabilities. However, we claim that this can be solved by making
discriminator and generator networks cooperate to produce output sequences
during training. These cooperative outputs, inherently built to obtain higher
discrimination scores, not only provide denser rewards for training, but also
form a more compact artificial set for discriminator training, hence improving
its accuracy and stability. In this paper, we show that our SelfGAN framework,
built on this cooperative principle, outperforms Teacher Forcing and obtains
state-of-the-art results on two challenging tasks, Summarization and Question
Generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1"&gt;Thomas Scialom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dray_P/0/1/0/all/0/1"&gt;Paul-Alexis Dray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1"&gt;Sylvain Lamprier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1"&gt;Benjamin Piwowarski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1"&gt;Jacopo Staiano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the Precise Feature for Cluster Assignment. (arXiv:2106.06159v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06159</id>
        <link href="http://arxiv.org/abs/2106.06159"/>
        <updated>2021-06-14T01:38:52.014Z</updated>
        <summary type="html"><![CDATA[Clustering is one of the fundamental tasks in computer vision and pattern
recognition. Recently, deep clustering methods (algorithms based on deep
learning) have attracted wide attention with their impressive performance. Most
of these algorithms combine deep unsupervised representation learning and
standard clustering together. However, the separation of representation
learning and clustering will lead to suboptimal solutions because the two-stage
strategy prevents representation learning from adapting to subsequent tasks
(e.g., clustering according to specific cues). To overcome this issue, efforts
have been made in the dynamic adaption of representation and cluster
assignment, whereas current state-of-the-art methods suffer from heuristically
constructed objectives with representation and cluster assignment alternatively
optimized. To further standardize the clustering problem, we audaciously
formulate the objective of clustering as finding a precise feature as the cue
for cluster assignment. Based on this, we propose a general-purpose deep
clustering framework which radically integrates representation learning and
clustering into a single pipeline for the first time. The proposed framework
exploits the powerful ability of recently developed generative models for
learning intrinsic features, and imposes an entropy minimization on the
distribution of the cluster assignment by a dedicated variational algorithm.
Experimental results show that the performance of the proposed method is
superior, or at least comparable to, the state-of-the-art methods on the
handwritten digit recognition, fashion recognition, face recognition and object
recognition benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1"&gt;Yanhai Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xinghui Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1"&gt;Feng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Junyu Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment. (arXiv:2106.06381v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06381</id>
        <link href="http://arxiv.org/abs/2106.06381"/>
        <updated>2021-06-14T01:38:51.995Z</updated>
        <summary type="html"><![CDATA[The cross-lingual language models are typically pretrained with masked
language modeling on multilingual text or parallel sentences. In this paper, we
introduce denoising word alignment as a new cross-lingual pre-training task.
Specifically, the model first self-labels word alignments for parallel
sentences. Then we randomly mask tokens in a bitext pair. Given a masked token,
the model uses a pointer network to predict the aligned token in the other
language. We alternately perform the above two steps in an
expectation-maximization manner. Experimental results show that our method
improves cross-lingual transferability on various datasets, especially on the
token-level tasks, such as question answering, and structured prediction.
Moreover, the model can serve as a pretrained word aligner, which achieves
reasonably low error rates on the alignment benchmarks. The code and pretrained
parameters are available at https://github.com/CZWin32768/XLM-Align.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1"&gt;Zewen Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Bo Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shaohan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1"&gt;Xian-Ling Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heyan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Vision with Sparse Mixture of Experts. (arXiv:2106.05974v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05974</id>
        <link href="http://arxiv.org/abs/2106.05974"/>
        <updated>2021-06-14T01:38:51.987Z</updated>
        <summary type="html"><![CDATA[Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent
scalability in Natural Language Processing. In Computer Vision, however, almost
all performant networks are "dense", that is, every input is processed by every
parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision
Transformer, that is scalable and competitive with the largest dense networks.
When applied to image recognition, V-MoE matches the performance of
state-of-the-art networks, while requiring as little as half of the compute at
inference time. Further, we propose an extension to the routing algorithm that
can prioritize subsets of each input across the entire batch, leading to
adaptive per-image compute. This allows V-MoE to trade-off performance and
compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE
to scale vision models, and train a 15B parameter model that attains 90.35% on
ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riquelme_C/0/1/0/all/0/1"&gt;Carlos Riquelme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1"&gt;Joan Puigcerver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1"&gt;Basil Mustafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_M/0/1/0/all/0/1"&gt;Maxim Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1"&gt;Rodolphe Jenatton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Susano Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1"&gt;Daniel Keysers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation. (arXiv:2106.06471v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06471</id>
        <link href="http://arxiv.org/abs/2106.06471"/>
        <updated>2021-06-14T01:38:51.979Z</updated>
        <summary type="html"><![CDATA[Medical report generation is one of the most challenging tasks in medical
image analysis. Although existing approaches have achieved promising results,
they either require a predefined template database in order to retrieve
sentences or ignore the hierarchical nature of medical report generation. To
address these issues, we propose MedWriter that incorporates a novel
hierarchical retrieval mechanism to automatically extract both report and
sentence-level templates for clinically accurate report generation. MedWriter
first employs the Visual-Language Retrieval~(VLR) module to retrieve the most
relevant reports for the given images. To guarantee the logical coherence
between sentences, the Language-Language Retrieval~(LLR) module is introduced
to retrieve relevant sentences based on the previous generated description. At
last, a language decoder fuses image features and features from retrieved
reports and sentences to generate meaningful medical reports. We verified the
effectiveness of our model by automatic evaluation and human evaluation on two
datasets, i.e., Open-I and MIMIC-CXR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xingyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1"&gt;Muchao Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1"&gt;Quanzeng You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Fenglong Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coordinate Independent Convolutional Networks -- Isometry and Gauge Equivariant Convolutions on Riemannian Manifolds. (arXiv:2106.06020v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.06020</id>
        <link href="http://arxiv.org/abs/2106.06020"/>
        <updated>2021-06-14T01:38:51.971Z</updated>
        <summary type="html"><![CDATA[Motivated by the vast success of deep convolutional networks, there is a
great interest in generalizing convolutions to non-Euclidean manifolds. A major
complication in comparison to flat spaces is that it is unclear in which
alignment a convolution kernel should be applied on a manifold. The underlying
reason for this ambiguity is that general manifolds do not come with a
canonical choice of reference frames (gauge). Kernels and features therefore
have to be expressed relative to arbitrary coordinates. We argue that the
particular choice of coordinatization should not affect a network's inference
-- it should be coordinate independent. A simultaneous demand for coordinate
independence and weight sharing is shown to result in a requirement on the
network to be equivariant under local gauge transformations (changes of local
reference frames). The ambiguity of reference frames depends thereby on the
G-structure of the manifold, such that the necessary level of gauge
equivariance is prescribed by the corresponding structure group G. Coordinate
independent convolutions are proven to be equivariant w.r.t. those isometries
that are symmetries of the G-structure. The resulting theory is formulated in a
coordinate free fashion in terms of fiber bundles. To exemplify the design of
coordinate independent convolutions, we implement a convolutional network on
the M\"obius strip. The generality of our differential geometric formulation of
convolutional networks is demonstrated by an extensive literature review which
explains a large number of Euclidean CNNs, spherical CNNs and CNNs on general
surfaces as specific instances of coordinate independent convolutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1"&gt;Maurice Weiler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1"&gt;Patrick Forr&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verlinde_E/0/1/0/all/0/1"&gt;Erik Verlinde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Controlled Generation with Encoder-Decoder Transformers. (arXiv:2106.06411v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06411</id>
        <link href="http://arxiv.org/abs/2106.06411"/>
        <updated>2021-06-14T01:38:51.963Z</updated>
        <summary type="html"><![CDATA[Controlling neural network-based models for natural language generation (NLG)
has broad applications in numerous areas such as machine translation, document
summarization, and dialog systems. Approaches that enable such control in a
zero-shot manner would be of great importance as, among other reasons, they
remove the need for additional annotated data and training. In this work, we
propose novel approaches for controlling encoder-decoder transformer-based NLG
models in a zero-shot manner. This is done by introducing three control knobs;
namely, attention biasing, decoder mixing, and context augmentation, that are
applied to these models at generation time. These knobs control the generation
process by directly manipulating trained NLG models (e.g., biasing
cross-attention layers) to realize the desired attributes in the generated
outputs. We show that not only are these NLG models robust to such
manipulations, but also their behavior could be controlled without an impact on
their generation performance. These results, to the best of our knowledge, are
the first of their kind. Through these control knobs, we also investigate the
role of transformer decoder's self-attention module and show strong evidence
that its primary role is maintaining fluency of sentences generated by these
models. Based on this hypothesis, we show that alternative architectures for
transformer decoders could be viable options. We also study how this hypothesis
could lead to more efficient ways for training encoder-decoder transformer
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1"&gt;Devamanyu Hazarika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namazifar_M/0/1/0/all/0/1"&gt;Mahdi Namazifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1"&gt;Dilek Hakkani-T&amp;#xfc;r&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Explanation of Dialogue Response Generation. (arXiv:2106.06528v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06528</id>
        <link href="http://arxiv.org/abs/2106.06528"/>
        <updated>2021-06-14T01:38:51.956Z</updated>
        <summary type="html"><![CDATA[In comparison to the interpretation of classification models, the explanation
of sequence generation models is also an important problem, however it has seen
little attention. In this work, we study model-agnostic explanations of a
representative text generation task -- dialogue response generation. Dialog
response generation is challenging with its open-ended sentences and multiple
acceptable responses. To gain insights into the reasoning process of a
generation model, we propose anew method, local explanation of response
generation (LERG) that regards the explanations as the mutual interaction of
segments in input and output sentences. LERG views the sequence prediction as
uncertainty estimation of a human response and then creates explanations by
perturbing the input and calculating the certainty change over the human
response. We show that LERG adheres to desired properties of explanations for
text generation including unbiased approximation, consistency and cause
identification. Empirically, our results show that our method consistently
improves other widely used methods on proposed automatic- and human- evaluation
metrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can
extract both explicit and implicit relations between input and output segments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1"&gt;Yi-Lin Tuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pryor_C/0/1/0/all/0/1"&gt;Connor Pryor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenhu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1"&gt;Lise Getoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;William Yang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Should Agents Ask Questions For Situated Learning? An Annotated Dialogue Corpus. (arXiv:2106.06504v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06504</id>
        <link href="http://arxiv.org/abs/2106.06504"/>
        <updated>2021-06-14T01:38:51.937Z</updated>
        <summary type="html"><![CDATA[Intelligent agents that are confronted with novel concepts in situated
environments will need to ask their human teammates questions to learn about
the physical world. To better understand this problem, we need data about
asking questions in situated task-based interactions. To this end, we present
the Human-Robot Dialogue Learning (HuRDL) Corpus - a novel dialogue corpus
collected in an online interactive virtual environment in which human
participants play the role of a robot performing a collaborative
tool-organization task. We describe the corpus data and a corresponding
annotation scheme to offer insight into the form and content of questions that
humans ask to facilitate learning in a situated environment. We provide the
corpus as an empirically-grounded resource for improving question generation in
situated intelligent agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gervits_F/0/1/0/all/0/1"&gt;Felix Gervits&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roque_A/0/1/0/all/0/1"&gt;Antonio Roque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Briggs_G/0/1/0/all/0/1"&gt;Gordon Briggs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheutz_M/0/1/0/all/0/1"&gt;Matthias Scheutz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marge_M/0/1/0/all/0/1"&gt;Matthew Marge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. (arXiv:2102.05918v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05918</id>
        <link href="http://arxiv.org/abs/2102.05918"/>
        <updated>2021-06-14T01:38:51.931Z</updated>
        <summary type="html"><![CDATA[Pre-trained representations are becoming crucial for many NLP and perception
tasks. While representation learning in NLP has transitioned to training on raw
text without human annotations, visual and vision-language representations
still rely heavily on curated training datasets that are expensive or require
expert knowledge. For vision applications, representations are mostly learned
using datasets with explicit class labels such as ImageNet or OpenImages. For
vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all
involve a non-trivial data collection (and cleaning) process. This costly
curation process limits the size of datasets and hence hinders the scaling of
trained models. In this paper, we leverage a noisy dataset of over one billion
image alt-text pairs, obtained without expensive filtering or post-processing
steps in the Conceptual Captions dataset. A simple dual-encoder architecture
learns to align visual and language representations of the image and text pairs
using a contrastive loss. We show that the scale of our corpus can make up for
its noise and leads to state-of-the-art representations even with such a simple
learning scheme. Our visual representation achieves strong performance when
transferred to classification tasks such as ImageNet and VTAB. The aligned
visual and language representations enables zero-shot image classification and
also set new state-of-the-art results on Flickr30K and MSCOCO image-text
retrieval benchmarks, even when compared with more sophisticated
cross-attention models. The representations also enable cross-modality search
with complex text and text + image queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1"&gt;Chao Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Ye Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1"&gt;Zarana Parekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1"&gt;Yunhsuan Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duerig_T/0/1/0/all/0/1"&gt;Tom Duerig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spoken Style Learning with Multi-modal Hierarchical Context Encoding for Conversational Text-to-Speech Synthesis. (arXiv:2106.06233v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.06233</id>
        <link href="http://arxiv.org/abs/2106.06233"/>
        <updated>2021-06-14T01:38:51.923Z</updated>
        <summary type="html"><![CDATA[For conversational text-to-speech (TTS) systems, it is vital that the systems
can adjust the spoken styles of synthesized speech according to different
content and spoken styles in historical conversations. However, the study about
learning spoken styles from historical conversations is still in its infancy.
Only the transcripts of the historical conversations are considered, which
neglects the spoken styles in historical speeches. Moreover, only the
interactions of the global aspect between speakers are modeled, missing the
party aspect self interactions inside each speaker. In this paper, to achieve
better spoken style learning for conversational TTS, we propose a spoken style
learning approach with multi-modal hierarchical context encoding. The textual
information and spoken styles in the historical conversations are processed
through multiple hierarchical recurrent neural networks to learn the spoken
style related features in global and party aspects. The attention mechanism is
further employed to summarize these features into a conversational context
encoding. Experimental results demonstrate the effectiveness of our proposed
approach, which outperform a baseline method using context encoding learnt only
from the transcripts in global aspects, with MOS score on the naturalness of
synthesized speech increasing from 3.138 to 3.408 and ABX preference rate
exceeding the baseline method by 36.45%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingbei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1"&gt;Yi Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1"&gt;Helen Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1"&gt;Chao Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1"&gt;Dan Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrate Before Use: Improving Few-Shot Performance of Language Models. (arXiv:2102.09690v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09690</id>
        <link href="http://arxiv.org/abs/2102.09690"/>
        <updated>2021-06-14T01:38:51.916Z</updated>
        <summary type="html"><![CDATA[GPT-3 can perform numerous tasks when provided a natural language prompt that
contains a few training examples. We show that this type of few-shot learning
can be unstable: the choice of prompt format, training examples, and even the
order of the training examples can cause accuracy to vary from near chance to
near state-of-the-art. We demonstrate that this instability arises from the
bias of language models towards predicting certain answers, e.g., those that
are placed near the end of the prompt or are common in the pre-training data.
To mitigate this, we first estimate the model's bias towards each answer by
asking for its prediction when given the training prompt and a content-free
test input such as "N/A". We then fit calibration parameters that cause the
prediction for this input to be uniform across answers. On a diverse set of
tasks, this contextual calibration procedure substantially improves GPT-3 and
GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across
different choices of the prompt.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tony Z. Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1"&gt;Eric Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1"&gt;Dan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sprachsynthese -- State-of-the-Art in englischer und deutscher Sprache. (arXiv:2106.06230v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06230</id>
        <link href="http://arxiv.org/abs/2106.06230"/>
        <updated>2021-06-14T01:38:51.907Z</updated>
        <summary type="html"><![CDATA[Reading text aloud is an important feature for modern computer applications.
It not only facilitates access to information for visually impaired people, but
is also a pleasant convenience for non-impaired users. In this article, the
state of the art of speech synthesis is presented separately for
mel-spectrogram generation and vocoders. It concludes with an overview of
available data sets for English and German with a discussion of the
transferability of the good speech synthesis results from English to German
language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peinl_R/0/1/0/all/0/1"&gt;Ren&amp;#xe9; Peinl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution. (arXiv:2106.06361v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06361</id>
        <link href="http://arxiv.org/abs/2106.06361"/>
        <updated>2021-06-14T01:38:51.881Z</updated>
        <summary type="html"><![CDATA[Recent studies show that neural natural language processing (NLP) models are
vulnerable to backdoor attacks. Injected with backdoors, models perform
normally on benign examples but produce attacker-specified predictions when the
backdoor is activated, presenting serious security threats to real-world
applications. Since existing textual backdoor attacks pay little attention to
the invisibility of backdoors, they can be easily detected and blocked. In this
work, we present invisible backdoors that are activated by a learnable
combination of word substitution. We show that NLP models can be injected with
backdoors that lead to a nearly 100% attack success rate, whereas being highly
invisible to existing defense strategies and even human inspections. The
results raise a serious alarm to the security of NLP models, which requires
further research to be resolved. All the data and code of this paper are
released at https://github.com/thunlp/BkdAtk-LWS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Fanchao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Sophia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentence Extraction-Based Machine Reading Comprehension for Vietnamese. (arXiv:2105.09043v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09043</id>
        <link href="http://arxiv.org/abs/2105.09043"/>
        <updated>2021-06-14T01:38:51.870Z</updated>
        <summary type="html"><![CDATA[The development of natural language processing (NLP) in general and machine
reading comprehension in particular has attracted the great attention of the
research community. In recent years, there are a few datasets for machine
reading comprehension tasks in Vietnamese with large sizes, such as UIT-ViQuAD
and UIT-ViNewsQA. However, the datasets are not diverse in answers to serve the
research. In this paper, we introduce UIT-ViWikiQA, the first dataset for
evaluating sentence extraction-based machine reading comprehension in the
Vietnamese language. The UIT-ViWikiQA dataset is converted from the UIT-ViQuAD
dataset, consisting of comprises 23.074 question-answers based on 5.109
passages of 174 Wikipedia Vietnamese articles. We propose a conversion
algorithm to create the dataset for sentence extraction-based machine reading
comprehension and three types of approaches for sentence extraction-based
machine reading comprehension in Vietnamese. Our experiments show that the best
machine model is XLM-R_Large, which achieves an exact match (EM) of 85.97% and
an F1-score of 88.77% on our dataset. Besides, we analyze experimental results
in terms of the question type in Vietnamese and the effect of context on the
performance of the MRC models, thereby showing the challenges from the
UIT-ViWikiQA dataset that we propose to the language processing community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Do_P/0/1/0/all/0/1"&gt;Phong Nguyen-Thuan Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Nhat Duy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1"&gt;Tin Van Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Kiet Van Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Gia-Tuan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Ngan Luu-Thuy Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedNLP: An interpretable NLP System to Decode Federal Reserve Communications. (arXiv:2106.06247v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06247</id>
        <link href="http://arxiv.org/abs/2106.06247"/>
        <updated>2021-06-14T01:38:51.846Z</updated>
        <summary type="html"><![CDATA[The Federal Reserve System (the Fed) plays a significant role in affecting
monetary policy and financial conditions worldwide. Although it is important to
analyse the Fed's communications to extract useful information, it is generally
long-form and complex due to the ambiguous and esoteric nature of content. In
this paper, we present FedNLP, an interpretable multi-component Natural
Language Processing system to decode Federal Reserve communications. This
system is designed for end-users to explore how NLP techniques can assist their
holistic understanding of the Fed's communications with NO coding. Behind the
scenes, FedNLP uses multiple NLP models from traditional machine learning
algorithms to deep neural network architectures in each downstream task. The
demonstration shows multiple results at once including sentiment analysis,
summary of the document, prediction of the Federal Funds Rate movement and
visualization for interpreting the prediction model's result.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jean Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Youn_H/0/1/0/all/0/1"&gt;Hoyoul Luis Youn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stevens_N/0/1/0/all/0/1"&gt;Nicholas Stevens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1"&gt;Josiah Poon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Soyeon Caren Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HUI-Audio-Corpus-German: A high quality TTS dataset. (arXiv:2106.06309v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.06309</id>
        <link href="http://arxiv.org/abs/2106.06309"/>
        <updated>2021-06-14T01:38:51.825Z</updated>
        <summary type="html"><![CDATA[The increasing availability of audio data on the internet lead to a multitude
of datasets for development and training of text to speech applications, based
on neural networks. Highly differing quality of voice, low sampling rates, lack
of text normalization and disadvantageous alignment of audio samples to
corresponding transcript sentences still limit the performance of deep neural
networks trained on this task. Additionally, data resources in languages like
German are still very limited. We introduce the "HUI-Audio-Corpus-German", a
large, open-source dataset for TTS engines, created with a processing pipeline,
which produces high quality audio to transcription alignments and decreases
manual effort needed for creation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Puchtler_P/0/1/0/all/0/1"&gt;Pascal Puchtler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wirth_J/0/1/0/all/0/1"&gt;Johannes Wirth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peinl_R/0/1/0/all/0/1"&gt;Ren&amp;#xe9; Peinl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving RNN-T ASR Performance with Date-Time and Location Awareness. (arXiv:2106.06183v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.06183</id>
        <link href="http://arxiv.org/abs/2106.06183"/>
        <updated>2021-06-14T01:38:51.757Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore the benefits of incorporating context into a
Recurrent Neural Network (RNN-T) based Automatic Speech Recognition (ASR) model
to improve the speech recognition for virtual assistants. Specifically, we use
meta information extracted from the time at which the utterance is spoken and
the approximate location information to make ASR context aware. We show that
these contextual information, when used individually, improves overall
performance by as much as 3.48% relative to the baseline and when the contexts
are combined, the model learns complementary features and the recognition
improves by 4.62%. On specific domains, these contextual signals show
improvements as high as 11.5%, without any significant degradation on others.
We ran experiments with models trained on data of sizes 30K hours and 10K
hours. We show that the scale of improvement with the 10K hours dataset is much
higher than the one obtained with 30K hours dataset. Our results indicate that
with limited data to train the ASR model, contextual signals can improve the
performance significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ray_S/0/1/0/all/0/1"&gt;Swayambhu Nath Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mitra_S/0/1/0/all/0/1"&gt;Soumyajit Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilgi_R/0/1/0/all/0/1"&gt;Raghavendra Bilgi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garimella_S/0/1/0/all/0/1"&gt;Sri Garimella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nested and Balanced Entity Recognition using Multi-Task Learning. (arXiv:2106.06216v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06216</id>
        <link href="http://arxiv.org/abs/2106.06216"/>
        <updated>2021-06-14T01:38:51.705Z</updated>
        <summary type="html"><![CDATA[Entity Recognition (ER) within a text is a fundamental exercise in Natural
Language Processing, enabling further depending tasks such as Knowledge
Extraction, Text Summarisation, or Keyphrase Extraction. An entity consists of
single words or of a consecutive sequence of terms, constituting the basic
building blocks for communication. Mainstream ER approaches are mainly limited
to flat structures, concentrating on the outermost entities while ignoring the
inner ones. This paper introduces a partly-layered network architecture that
deals with the complexity of overlapping and nested cases. The proposed
architecture consists of two parts: (1) a shared Sequence Layer and (2) a
stacked component with multiple Tagging Layers. The adoption of such an
architecture has the advantage of preventing overfit to a specific word-length,
thus maintaining performance for longer entities despite their lower frequency.
To verify the proposed architecture's effectiveness, we train and evaluate this
architecture to recognise two kinds of entities - Concepts (CR) and Named
Entities (NER). Our approach achieves state-of-the-art NER performances, while
it outperforms previous CR approaches. Considering these promising results, we
see the possibility to evolve the architecture for other cases such as the
extraction of events or the detection of argumentative components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Waldis_A/0/1/0/all/0/1"&gt;Andreas Waldis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazzola_L/0/1/0/all/0/1"&gt;Luca Mazzola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards User-Driven Neural Machine Translation. (arXiv:2106.06200v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06200</id>
        <link href="http://arxiv.org/abs/2106.06200"/>
        <updated>2021-06-14T01:38:51.697Z</updated>
        <summary type="html"><![CDATA[A good translation should not only translate the original content
semantically, but also incarnate personal traits of the original text. For a
real-world neural machine translation (NMT) system, these user traits (e.g.,
topic preference, stylistic characteristics and expression habits) can be
preserved in user behavior (e.g., historical inputs). However, current NMT
systems marginally consider the user behavior due to: 1) the difficulty of
modeling user portraits in zero-shot scenarios, and 2) the lack of
user-behavior annotated parallel dataset. To fill this gap, we introduce a
novel framework called user-driven NMT. Specifically, a cache-based module and
a user-driven contrastive learning method are proposed to offer NMT the ability
to capture potential user traits from their historical inputs under a zero-shot
learning fashion. Furthermore, we contribute the first Chinese-English parallel
corpus annotated with user behavior called UDT-Corpus. Experimental results
confirm that the proposed user-driven NMT can generate user-specific
translations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Huan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1"&gt;Liang Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Baosong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dayiheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haibo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1"&gt;Weihua Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Degen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jinsong Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection. (arXiv:2106.06213v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06213</id>
        <link href="http://arxiv.org/abs/2106.06213"/>
        <updated>2021-06-14T01:38:51.652Z</updated>
        <summary type="html"><![CDATA[Traditional toxicity detection models have focused on the single utterance
level without deeper understanding of context. We introduce CONDA, a new
dataset for in-game toxic language detection enabling joint intent
classification and slot filling analysis, which is the core task of Natural
Language Understanding (NLU). The dataset consists of 45K utterances from 12K
conversations from the chat logs of 1.9K completed Dota 2 matches. We propose a
robust dual semantic-level toxicity framework, which handles utterance and
token-level patterns, and rich contextual chatting history. Accompanying the
dataset is a thorough in-game toxicity analysis, which provides comprehensive
understanding of context at utterance, token, and dual levels. Inspired by NLU,
we also apply its metrics to the toxicity detection tasks for assessing
toxicity and game-specific aspects. We evaluate strong NLU models on CONDA,
providing fine-grained results for different intent classes and slot classes.
Furthermore, we examine the coverage of toxicity nature in our dataset by
comparing it with other toxicity datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weld_H/0/1/0/all/0/1"&gt;Henry Weld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Guanghao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jean Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tongshu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kunze Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xinghong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1"&gt;Siqu Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1"&gt;Josiah Poon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Soyeon Caren Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via Synchronous Semantic Decoding. (arXiv:2106.06228v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06228</id>
        <link href="http://arxiv.org/abs/2106.06228"/>
        <updated>2021-06-14T01:38:51.627Z</updated>
        <summary type="html"><![CDATA[Semantic parsing is challenging due to the structure gap and the semantic gap
between utterances and logical forms. In this paper, we propose an unsupervised
semantic parsing method - Synchronous Semantic Decoding (SSD), which can
simultaneously resolve the semantic gap and the structure gap by jointly
leveraging paraphrasing and grammar constrained decoding. Specifically, we
reformulate semantic parsing as a constrained paraphrasing problem: given an
utterance, our model synchronously generates its canonical utterance and
meaning representation. During synchronous decoding: the utterance paraphrasing
is constrained by the structure of the logical form, therefore the canonical
utterance can be paraphrased controlledly; the semantic decoding is guided by
the semantics of the canonical utterance, therefore its logical form can be
generated unsupervisedly. Experimental results show that SSD is a promising
approach and can achieve competitive unsupervised semantic parsing performance
on multiple datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_C/0/1/0/all/0/1"&gt;Chunlei Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xianpei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Le Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weipeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiansong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1"&gt;Xunliang Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data. (arXiv:2106.06169v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06169</id>
        <link href="http://arxiv.org/abs/2106.06169"/>
        <updated>2021-06-14T01:38:51.568Z</updated>
        <summary type="html"><![CDATA[Maintaining consistent personas is essential for dialogue agents. Although
tremendous advancements have been brought, the limited-scale of annotated
persona-dense data are still barriers towards training robust and consistent
persona-based dialogue models. In this work, we show how the challenges can be
addressed by disentangling persona-based dialogue generation into two sub-tasks
with a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a
BERT-based encoder and two BERT-based decoders, where one decoder is for
response generation, and another is for consistency understanding. In
particular, to learn the ability of consistency understanding from large-scale
non-dialogue inference data, we train the second decoder in an unlikelihood
manner. Under different limited data settings, both automatic and human
evaluations demonstrate that the proposed model outperforms strong baselines in
response quality and persona consistency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Haoyu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaiyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei-Nan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TellMeWhy: A Dataset for Answering Why-Questions in Narratives. (arXiv:2106.06132v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06132</id>
        <link href="http://arxiv.org/abs/2106.06132"/>
        <updated>2021-06-14T01:38:51.559Z</updated>
        <summary type="html"><![CDATA[Answering questions about why characters perform certain actions is central
to understanding and reasoning about narratives. Despite recent progress in QA,
it is not clear if existing models have the ability to answer "why" questions
that may require commonsense knowledge external to the input narrative. In this
work, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more
than 30k questions and free-form answers concerning why characters in short
narratives perform the actions described. For a third of this dataset, the
answers are not present within the narrative. Given the limitations of
automated evaluation for this task, we also present a systematized human
evaluation interface for this dataset. Our evaluation of state-of-the-art
models show that they are far below human performance on answering such
questions. They are especially worse on questions whose answers are external to
the narrative, thus providing a challenge for future QA and narrative
understanding research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lal_Y/0/1/0/all/0/1"&gt;Yash Kumar Lal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chambers_N/0/1/0/all/0/1"&gt;Nathanael Chambers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1"&gt;Raymond Mooney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1"&gt;Niranjan Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anytime Ranking on Document-Ordered Indexes. (arXiv:2104.08976v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08976</id>
        <link href="http://arxiv.org/abs/2104.08976"/>
        <updated>2021-06-14T01:38:51.546Z</updated>
        <summary type="html"><![CDATA[Inverted indexes continue to be a mainstay of text search engines, allowing
efficient querying of large document collections. While there are a number of
possible organizations, document-ordered indexes are the most common, since
they are amenable to various query types, support index updates, and allow for
efficient dynamic pruning operations. One disadvantage with document-ordered
indexes is that high-scoring documents can be distributed across the document
identifier space, meaning that index traversal algorithms that terminate early
might put search effectiveness at risk. The alternative is impact-ordered
indexes, which primarily support top-k disjunctions, but also allow for anytime
query processing, where the search can be terminated at any time, with search
quality improving as processing latency increases. Anytime query processing can
be used to effectively reduce high-percentile tail latency which is essential
for operational scenarios in which a service level agreement (SLA) imposes
response time requirements. In this work, we show how document-ordered indexes
can be organized such that they can be queried in an anytime fashion, enabling
strict latency control with effective early termination. Our experiments show
that processing document-ordered topical segments selected by a simple score
estimator outperforms existing anytime algorithms, and allows query runtimes to
be accurately limited in order to comply with SLA requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mackenzie_J/0/1/0/all/0/1"&gt;Joel Mackenzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petri_M/0/1/0/all/0/1"&gt;Matthias Petri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moffat_A/0/1/0/all/0/1"&gt;Alistair Moffat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spoken Term Detection Methods for Sparse Transcription in Very Low-resource Settings. (arXiv:2106.06160v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06160</id>
        <link href="http://arxiv.org/abs/2106.06160"/>
        <updated>2021-06-14T01:38:51.537Z</updated>
        <summary type="html"><![CDATA[We investigate the efficiency of two very different spoken term detection
approaches for transcription when the available data is insufficient to train a
robust ASR system. This work is grounded in very low-resource language
documentation scenario where only few minutes of recording have been
transcribed for a given language so far.Experiments on two oral languages show
that a pretrained universal phone recognizer, fine-tuned with only a few
minutes of target language speech, can be used for spoken term detection with a
better overall performance than a dynamic time warping approach. In addition,
we show that representing phoneme recognition ambiguity in a graph structure
can further boost the recall while maintaining high precision in the low
resource spoken term detection task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferrand_E/0/1/0/all/0/1"&gt;&amp;#xc9;ric Le Ferrand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bird_S/0/1/0/all/0/1"&gt;Steven Bird&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1"&gt;Laurent Besacier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CodemixedNLP: An Extensible and Open NLP Toolkit for Code-Mixing. (arXiv:2106.06004v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06004</id>
        <link href="http://arxiv.org/abs/2106.06004"/>
        <updated>2021-06-14T01:38:51.510Z</updated>
        <summary type="html"><![CDATA[The NLP community has witnessed steep progress in a variety of tasks across
the realms of monolingual and multilingual language processing recently. These
successes, in conjunction with the proliferating mixed language interactions on
social media have boosted interest in modeling code-mixed texts. In this work,
we present CodemixedNLP, an open-source library with the goals of bringing
together the advances in code-mixed NLP and opening it up to a wider machine
learning community. The library consists of tools to develop and benchmark
versatile model architectures that are tailored for mixed texts, methods to
expand training sets, techniques to quantify mixing styles, and fine-tuned
state-of-the-art models for 7 tasks in Hinglish. We believe this work has a
potential to foster a distributed yet collaborative and sustainable ecosystem
in an otherwise dispersed space of code-mixing research. The toolkit is
designed to be simple, easily extensible, and resourceful to both researchers
as well as practitioners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jayanthi_S/0/1/0/all/0/1"&gt;Sai Muralidhar Jayanthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nerella_K/0/1/0/all/0/1"&gt;Kavya Nerella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1"&gt;Khyathi Raghavi Chandu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1"&gt;Alan W Black&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Knowledge Gain during Web Search based on Multimedia Resource Consumption. (arXiv:2106.06244v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.06244</id>
        <link href="http://arxiv.org/abs/2106.06244"/>
        <updated>2021-06-14T01:38:51.500Z</updated>
        <summary type="html"><![CDATA[In informal learning scenarios the popularity of multimedia content, such as
video tutorials or lectures, has significantly increased. Yet, the users'
interactions, navigation behavior, and consequently learning outcome, have not
been researched extensively. Related work in this field, also called search as
learning, has focused on behavioral or text resource features to predict
learning outcome and knowledge gain. In this paper, we investigate whether we
can exploit features representing multimedia resource consumption to predict of
knowledge gain (KG) during Web search from in-session data, that is without
prior knowledge about the learner. For this purpose, we suggest a set of
multimedia features related to image and video consumption. Our feature
extraction is evaluated in a lab study with 113 participants where we collected
data for a given search as learning task on the formation of thunderstorms and
lightning. We automatically analyze the monitored log data and utilize
state-of-the-art computer vision methods to extract features about the seen
multimedia resources. Experimental results demonstrate that multimedia features
can improve KG prediction. Finally, we provide an analysis on feature
importance (text and multimedia) for KG prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Otto_C/0/1/0/all/0/1"&gt;Christian Otto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Ran Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pardi_G/0/1/0/all/0/1"&gt;Georg Pardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoyer_J/0/1/0/all/0/1"&gt;Johannes von Hoyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rokicki_M/0/1/0/all/0/1"&gt;Markus Rokicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoppe_A/0/1/0/all/0/1"&gt;Anett Hoppe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holtz_P/0/1/0/all/0/1"&gt;Peter Holtz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kammerer_Y/0/1/0/all/0/1"&gt;Yvonne Kammerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dietze_S/0/1/0/all/0/1"&gt;Stefan Dietze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1"&gt;Ralph Ewerth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation. (arXiv:2106.06471v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06471</id>
        <link href="http://arxiv.org/abs/2106.06471"/>
        <updated>2021-06-14T01:38:51.477Z</updated>
        <summary type="html"><![CDATA[Medical report generation is one of the most challenging tasks in medical
image analysis. Although existing approaches have achieved promising results,
they either require a predefined template database in order to retrieve
sentences or ignore the hierarchical nature of medical report generation. To
address these issues, we propose MedWriter that incorporates a novel
hierarchical retrieval mechanism to automatically extract both report and
sentence-level templates for clinically accurate report generation. MedWriter
first employs the Visual-Language Retrieval~(VLR) module to retrieve the most
relevant reports for the given images. To guarantee the logical coherence
between sentences, the Language-Language Retrieval~(LLR) module is introduced
to retrieve relevant sentences based on the previous generated description. At
last, a language decoder fuses image features and features from retrieved
reports and sentences to generate meaningful medical reports. We verified the
effectiveness of our model by automatic evaluation and human evaluation on two
datasets, i.e., Open-I and MIMIC-CXR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xingyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1"&gt;Muchao Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1"&gt;Quanzeng You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Fenglong Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing Political Prudence of Open-domain Chatbots. (arXiv:2106.06157v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06157</id>
        <link href="http://arxiv.org/abs/2106.06157"/>
        <updated>2021-06-14T01:38:51.469Z</updated>
        <summary type="html"><![CDATA[Politically sensitive topics are still a challenge for open-domain chatbots.
However, dealing with politically sensitive content in a responsible,
non-partisan, and safe behavior way is integral for these chatbots. Currently,
the main approach to handling political sensitivity is by simply changing such
a topic when it is detected. This is safe but evasive and results in a chatbot
that is less engaging. In this work, as a first step towards a politically safe
chatbot, we propose a group of metrics for assessing their political prudence.
We then conduct political prudence analysis of various chatbots and discuss
their behavior from multiple angles through our automatic metric and human
evaluation metrics. The testsets and codebase are released to promote research
in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1"&gt;Yejin Bang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1"&gt;Nayeon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1"&gt;Etsuko Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1"&gt;Andrea Madotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1"&gt;Pascale Fung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking. (arXiv:2106.06052v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06052</id>
        <link href="http://arxiv.org/abs/2106.06052"/>
        <updated>2021-06-14T01:38:51.458Z</updated>
        <summary type="html"><![CDATA[We introduce Dynaboard, an evaluation-as-a-service framework for hosting
benchmarks and conducting holistic model comparison, integrated with the
Dynabench platform. Our platform evaluates NLP models directly instead of
relying on self-reported metrics or predictions on a single dataset. Under this
paradigm, models are submitted to be evaluated in the cloud, circumventing the
issues of reproducibility, accessibility, and backwards compatibility that
often hinder benchmarking in NLP. This allows users to interact with uploaded
models in real time to assess their quality, and permits the collection of
additional metrics such as memory use, throughput, and robustness, which --
despite their importance to practitioners -- have traditionally been absent
from leaderboards. On each task, models are ranked according to the Dynascore,
a novel utility-based aggregation of these statistics, which users can
customize to better reflect their preferences, placing more/less weight on a
particular axis of evaluation or dataset. As state-of-the-art NLP models push
the limits of traditional benchmarks, Dynaboard offers a standardized solution
for a more diverse and comprehensive evaluation of model quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhiyi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1"&gt;Kawin Ethayarajh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1"&gt;Tristan Thrush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Somya Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Ledell Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Robin Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1"&gt;Christopher Potts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1"&gt;Adina Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comprehensive solution to retrieval-based chatbot construction. (arXiv:2106.06139v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06139</id>
        <link href="http://arxiv.org/abs/2106.06139"/>
        <updated>2021-06-14T01:38:51.444Z</updated>
        <summary type="html"><![CDATA[In this paper we present the results of our experiments in training and
deploying a self-supervised retrieval-based chatbot trained with contrastive
learning for assisting customer support agents. In contrast to most existing
research papers in this area where the focus is on solving just one component
of a deployable chatbot, we present an end-to-end set of solutions to take the
reader from an unlabelled chatlogs to a deployed chatbot. This set of solutions
includes creating a self-supervised dataset and a weakly labelled dataset from
chatlogs, as well as a systematic approach to selecting a fixed list of canned
responses. We present a hierarchical-based RNN architecture for the response
selection model, chosen for its ability to cache intermediate utterance
embeddings, which helped to meet deployment inference speed requirements. We
compare the performance of this architecture across 3 different learning
objectives: self-supervised contrastive learning, binary classification, and
multi-class classification. We find that using a self-supervised contrastive
learning model outperforms training the binary and multi-class classification
models on a weakly labelled dataset. Our results validate that the
self-supervised contrastive learning approach can be effectively used for a
real-world chatbot scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moore_K/0/1/0/all/0/1"&gt;Kristen Moore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1"&gt;Shenjun Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhen He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudolf_T/0/1/0/all/0/1"&gt;Torsten Rudolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fisher_N/0/1/0/all/0/1"&gt;Nils Fisher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Victor_B/0/1/0/all/0/1"&gt;Brandon Victor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jindal_N/0/1/0/all/0/1"&gt;Neha Jindal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NAAQA: A Neural Architecture for Acoustic Question Answering. (arXiv:2106.06147v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06147</id>
        <link href="http://arxiv.org/abs/2106.06147"/>
        <updated>2021-06-14T01:38:51.433Z</updated>
        <summary type="html"><![CDATA[The goal of the Acoustic Question Answering (AQA) task is to answer a
free-form text question about the content of an acoustic scene. It was inspired
by the Visual Question Answering (VQA) task. In this paper, based on the
previously introduced CLEAR dataset, we propose a new benchmark for AQA that
emphasizes the specific challenges of acoustic inputs, e.g. variable duration
scenes. We also introduce NAAQA, a neural architecture that leverages specific
properties of acoustic inputs. The usage of time and frequency 1D convolutions
to process 2D spectro-temporal representations of acoustic content shows
promising results and enables reductions in model complexity. NAAQA achieves
91.6% of accuracy on the AQA task with about 7 times fewer parameters than the
previously explored VQA model. We provide a detailed analysis of the results
for the different question types. The effectiveness of coordinate maps in this
acoustic context was also studied and we show that time coordinate maps augment
temporal localization capabilities which enhance performance of the network by
about 17 percentage points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdelnour_J/0/1/0/all/0/1"&gt;Jerome Abdelnour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1"&gt;Jean Rouat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salvi_G/0/1/0/all/0/1"&gt;Giampiero Salvi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation. (arXiv:2106.06125v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06125</id>
        <link href="http://arxiv.org/abs/2106.06125"/>
        <updated>2021-06-14T01:38:51.408Z</updated>
        <summary type="html"><![CDATA[A well-known limitation in pretrain-finetune paradigm lies in its
inflexibility caused by the one-size-fits-all vocabulary. This potentially
weakens the effect when applying pretrained models into natural language
generation (NLG) tasks, especially for the subword distributions between
upstream and downstream tasks with significant discrepancy. Towards approaching
this problem, we extend the vanilla pretrain-finetune pipeline with an extra
embedding transfer step. Specifically, a plug-and-play embedding generator is
introduced to produce the representation of any input token, according to
pre-trained embeddings of its morphologically similar ones. Thus, embeddings of
mismatch tokens in downstream tasks can also be efficiently initialized. We
conduct experiments on a variety of NLG tasks under the pretrain-finetune
fashion. Experimental results and extensive analyses show that the proposed
strategy offers us opportunities to feel free to transfer the vocabulary,
leading to more efficient and better performed downstream NLG models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Baosong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dayiheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haibo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1"&gt;Weihua Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Min Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haiying Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jinsong Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks for Natural Language Processing: A Survey. (arXiv:2106.06090v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06090</id>
        <link href="http://arxiv.org/abs/2106.06090"/>
        <updated>2021-06-14T01:38:51.397Z</updated>
        <summary type="html"><![CDATA[Deep learning has become the dominant approach in coping with various tasks
in Natural LanguageProcessing (NLP). Although text inputs are typically
represented as a sequence of tokens, there isa rich variety of NLP problems
that can be best expressed with a graph structure. As a result, thereis a surge
of interests in developing new deep learning techniques on graphs for a large
numberof NLP tasks. In this survey, we present a comprehensive overview onGraph
Neural Networks(GNNs) for Natural Language Processing. We propose a new
taxonomy of GNNs for NLP, whichsystematically organizes existing research of
GNNs for NLP along three axes: graph construction,graph representation
learning, and graph based encoder-decoder models. We further introducea large
number of NLP applications that are exploiting the power of GNNs and summarize
thecorresponding benchmark datasets, evaluation metrics, and open-source codes.
Finally, we discussvarious outstanding challenges for making the full use of
GNNs for NLP as well as future researchdirections. To the best of our
knowledge, this is the first comprehensive overview of Graph NeuralNetworks for
Natural Language Processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1"&gt;Kai Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaojie Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1"&gt;Hanning Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shucheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1"&gt;Jian Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1"&gt;Bo Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models. (arXiv:2106.06087v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06087</id>
        <link href="http://arxiv.org/abs/2106.06087"/>
        <updated>2021-06-14T01:38:51.386Z</updated>
        <summary type="html"><![CDATA[Targeted syntactic evaluations have demonstrated the ability of language
models to perform subject-verb agreement given difficult contexts. To elucidate
the mechanisms by which the models accomplish this behavior, this study applies
causal mediation analysis to pre-trained neural language models. We investigate
the magnitude of models' preferences for grammatical inflections, as well as
whether neurons process subject-verb agreement similarly across sentences with
different syntactic structures. We uncover similarities and differences across
architectures and model sizes -- notably, that larger models do not necessarily
learn stronger preferences. We also observe two distinct mechanisms for
producing subject-verb agreement depending on the syntactic structure of the
input sentence. Finally, we find that language models rely on similar sets of
neurons when given sentences with similar syntactic structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1"&gt;Matthew Finlayson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1"&gt;Aaron Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shieber_S/0/1/0/all/0/1"&gt;Stuart Shieber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1"&gt;Sebastian Gehrmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1"&gt;Tal Linzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1"&gt;Yonatan Belinkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-lingual Emotion Detection. (arXiv:2106.06017v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06017</id>
        <link href="http://arxiv.org/abs/2106.06017"/>
        <updated>2021-06-14T01:38:51.364Z</updated>
        <summary type="html"><![CDATA[Emotion detection is of great importance for understanding humans.
Constructing annotated datasets to train automated models can be expensive. We
explore the efficacy of cross-lingual approaches that would use data from a
source language to build models for emotion detection in a target language. We
compare three approaches, namely: i) using inherently multilingual models; ii)
translating training data into the target language; and iii) using an
automatically tagged parallel corpus. In our study, we consider English as the
source language with Arabic and Spanish as target languages. We study the
effectiveness of different classification models such as BERT and SVMs trained
with different features. Our BERT-based monolingual models that are trained on
target language data surpass state-of-the-art (SOTA) by 4% and 5% absolute
Jaccard score for Arabic and Spanish respectively. Next, we show that using
cross-lingual approaches with English data alone, we can achieve more than 90%
and 80% relative effectiveness of the Arabic and Spanish BERT models
respectively. Lastly, we use LIME to interpret the differences between models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1"&gt;Sabit Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1"&gt;Shaden Shaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1"&gt;Kareem Darwish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Hierarchical Structures with Continuous Recursive Neural Networks. (arXiv:2106.06038v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06038</id>
        <link href="http://arxiv.org/abs/2106.06038"/>
        <updated>2021-06-14T01:38:51.346Z</updated>
        <summary type="html"><![CDATA[Recursive Neural Networks (RvNNs), which compose sequences according to their
underlying hierarchical syntactic structure, have performed well in several
natural language processing tasks compared to similar models without structural
biases. However, traditional RvNNs are incapable of inducing the latent
structure in a plain text sequence on their own. Several extensions have been
proposed to overcome this limitation. Nevertheless, these extensions tend to
rely on surrogate gradients or reinforcement learning at the cost of higher
bias or variance. In this work, we propose Continuous Recursive Neural Network
(CRvNN) as a backpropagation-friendly alternative to address the aforementioned
limitations. This is done by incorporating a continuous relaxation to the
induced structure. We demonstrate that CRvNN achieves strong performance in
challenging synthetic tasks such as logical inference and ListOps. We also show
that CRvNN performs comparably or better than prior latent structure models on
real-world tasks such as sentiment analysis and natural language inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1"&gt;Jishnu Ray Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1"&gt;Cornelia Caragea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Large-Scale Rich Context Query and Recommendation Dataset in Online Knowledge-Sharing. (arXiv:2106.06467v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.06467</id>
        <link href="http://arxiv.org/abs/2106.06467"/>
        <updated>2021-06-14T01:38:51.319Z</updated>
        <summary type="html"><![CDATA[Data plays a vital role in machine learning studies. In the research of
recommendation, both user behaviors and side information are helpful to model
users. So, large-scale real scenario datasets with abundant user behaviors will
contribute a lot. However, it is not easy to get such datasets as most of them
are only hold and protected by companies. In this paper, a new large-scale
dataset collected from a knowledge-sharing platform is presented, which is
composed of around 100M interactions collected within 10 days, 798K users, 165K
questions, 554K answers, 240K authors, 70K topics, and more than 501K user
query keywords. There are also descriptions of users, answers, questions,
authors, and topics, which are anonymous. Note that each user's latest query
keywords have not been included in previous open datasets, which reveal users'
explicit information needs.

We characterize the dataset and demonstrate its potential applications for
recommendation study. Multiple experiments show the dataset can be used to
evaluate algorithms in general top-N recommendation, sequential recommendation,
and context-aware recommendation. This dataset can also be used to integrate
search and recommendation and recommendation with negative feedback. Besides,
tasks beyond recommendation, such as user gender prediction, most valuable
answerer identification, and high-quality answer recognition, can also use this
dataset. To the best of our knowledge, this is the largest real-world
interaction dataset for personalized recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_B/0/1/0/all/0/1"&gt;Bin Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Min Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Weizhi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shaoyun Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xinxing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1"&gt;Houzhi Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yiqun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shaoping Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Sense Per Translation. (arXiv:2106.06082v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06082</id>
        <link href="http://arxiv.org/abs/2106.06082"/>
        <updated>2021-06-14T01:38:51.306Z</updated>
        <summary type="html"><![CDATA[The idea of using lexical translations to define sense inventories has a long
history in lexical semantics. We propose a theoretical framework which allows
us to answer the question of why this apparently reasonable idea failed to
produce useful results. We formally prove several propositions on how the
translations of a word relate to its senses, as well as on the relationship
between synonymy and polysemy. We empirically validate our theoretical findings
on BabelNet, and demonstrate how they could be used to perform unsupervised
word sense disambiguation of a substantial fraction of the lexicon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1"&gt;Bradley Hauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1"&gt;Grzegorz Kondrak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic, Structure-Aware Algorithms for Improved Variety, Accuracy, and Coverage of AMR Alignments. (arXiv:2106.06002v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06002</id>
        <link href="http://arxiv.org/abs/2106.06002"/>
        <updated>2021-06-14T01:38:51.293Z</updated>
        <summary type="html"><![CDATA[We present algorithms for aligning components of Abstract Meaning
Representation (AMR) graphs to spans in English sentences. We leverage
unsupervised learning in combination with heuristics, taking the best of both
worlds from previous AMR aligners. Our unsupervised models, however, are more
sensitive to graph substructures, without requiring a separate syntactic parse.
Our approach covers a wider variety of AMR substructures than previously
considered, achieves higher coverage of nodes and edges, and does so with
higher accuracy. We will release our LEAMR datasets and aligner for use in
research on AMR parsing, generation, and evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blodgett_A/0/1/0/all/0/1"&gt;Austin Blodgett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1"&gt;Nathan Schneider&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework to Enhance Generalization of Deep Metric Learning methods using General Discriminative Feature Learning and Class Adversarial Neural Networks. (arXiv:2106.06420v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06420</id>
        <link href="http://arxiv.org/abs/2106.06420"/>
        <updated>2021-06-14T01:38:51.279Z</updated>
        <summary type="html"><![CDATA[Metric learning algorithms aim to learn a distance function that brings the
semantically similar data items together and keeps dissimilar ones at a
distance. The traditional Mahalanobis distance learning is equivalent to find a
linear projection. In contrast, Deep Metric Learning (DML) methods are proposed
that automatically extract features from data and learn a non-linear
transformation from input space to a semantically embedding space. Recently,
many DML methods are proposed focused to enhance the discrimination power of
the learned metric by providing novel sampling strategies or loss functions.
This approach is very helpful when both the training and test examples are
coming from the same set of categories. However, it is less effective in many
applications of DML such as image retrieval and person-reidentification. Here,
the DML should learn general semantic concepts from observed classes and employ
them to rank or identify objects from unseen categories. Neglecting the
generalization ability of the learned representation and just emphasizing to
learn a more discriminative embedding on the observed classes may lead to the
overfitting problem. To address this limitation, we propose a framework to
enhance the generalization power of existing DML methods in a Zero-Shot
Learning (ZSL) setting by general yet discriminative representation learning
and employing a class adversarial neural network. To learn a more general
representation, we propose to employ feature maps of intermediate layers in a
deep neural network and enhance their discrimination power through an attention
mechanism. Besides, a class adversarial network is utilized to enforce the deep
model to seek class invariant features for the DML task. We evaluate our work
on widely used machine vision datasets in a ZSL setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Kaabi_K/0/1/0/all/0/1"&gt;Karrar Al-Kaabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monsefi_R/0/1/0/all/0/1"&gt;Reza Monsefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zabihzadeh_D/0/1/0/all/0/1"&gt;Davood Zabihzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Sequences as Distributions with Uncertainty for Sequential Recommendation. (arXiv:2106.06165v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.06165</id>
        <link href="http://arxiv.org/abs/2106.06165"/>
        <updated>2021-06-14T01:38:51.217Z</updated>
        <summary type="html"><![CDATA[The sequential patterns within the user interactions are pivotal for
representing the user's preference and capturing latent relationships among
items. The recent advancements of sequence modeling by Transformers advocate
the community to devise more effective encoders for the sequential
recommendation. Most existing sequential methods assume users are
deterministic. However, item-item transitions might fluctuate significantly in
several item aspects and exhibit randomness of user interests. This
\textit{stochastic characteristics} brings up a solid demand to include
uncertainties in representing sequences and items. Additionally, modeling
sequences and items with uncertainties expands users' and items' interaction
spaces, thus further alleviating cold-start problems.

In this work, we propose a Distribution-based Transformer for Sequential
Recommendation (DT4SR), which injects uncertainties into sequential modeling.
We use Elliptical Gaussian distributions to describe items and sequences with
uncertainty. We describe the uncertainty in items and sequences as Elliptical
Gaussian distribution. And we adopt Wasserstein distance to measure the
similarity between distributions. We devise two novel Trans-formers for
modeling mean and covariance, which guarantees the positive-definite property
of distributions. The proposed method significantly outperforms the
state-of-the-art methods. The experiments on three benchmark datasets also
demonstrate its effectiveness in alleviating cold-start issues. The code is
available inhttps://github.com/DyGRec/DT4SR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Ziwei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lei Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nested and Balanced Entity Recognition using Multi-Task Learning. (arXiv:2106.06216v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.06216</id>
        <link href="http://arxiv.org/abs/2106.06216"/>
        <updated>2021-06-14T01:38:51.205Z</updated>
        <summary type="html"><![CDATA[Entity Recognition (ER) within a text is a fundamental exercise in Natural
Language Processing, enabling further depending tasks such as Knowledge
Extraction, Text Summarisation, or Keyphrase Extraction. An entity consists of
single words or of a consecutive sequence of terms, constituting the basic
building blocks for communication. Mainstream ER approaches are mainly limited
to flat structures, concentrating on the outermost entities while ignoring the
inner ones. This paper introduces a partly-layered network architecture that
deals with the complexity of overlapping and nested cases. The proposed
architecture consists of two parts: (1) a shared Sequence Layer and (2) a
stacked component with multiple Tagging Layers. The adoption of such an
architecture has the advantage of preventing overfit to a specific word-length,
thus maintaining performance for longer entities despite their lower frequency.
To verify the proposed architecture's effectiveness, we train and evaluate this
architecture to recognise two kinds of entities - Concepts (CR) and Named
Entities (NER). Our approach achieves state-of-the-art NER performances, while
it outperforms previous CR approaches. Considering these promising results, we
see the possibility to evolve the architecture for other cases such as the
extraction of events or the detection of argumentative components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Waldis_A/0/1/0/all/0/1"&gt;Andreas Waldis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazzola_L/0/1/0/all/0/1"&gt;Luca Mazzola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DebiasGAN: Eliminating Position Bias in News Recommendation with Adversarial Learning. (arXiv:2106.06258v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.06258</id>
        <link href="http://arxiv.org/abs/2106.06258"/>
        <updated>2021-06-14T01:38:51.192Z</updated>
        <summary type="html"><![CDATA[News recommendation is important for improving news reading experience of
users. Users' news click behaviors are widely used for inferring user interests
and predicting future clicks. However, click behaviors are heavily affected by
the biases brought by the positions of news displayed on the webpage. It is
important to eliminate the effect of position biases on the recommendation
model to accurately target user interests. In this paper, we propose a news
recommendation method named DebiasGAN that can effectively eliminate the effect
of position biases via adversarial learning. We use a bias-aware click model to
capture the influence of position bias on click behaviors, and we use a
bias-invariant click model with random candidate news positions to estimate the
ideally unbiased click scores. We apply adversarial learning techniques to the
hidden representations learned by the two models to help the bias-invariant
click model capture the bias-independent interest of users on news.
Experimental results on two real-world datasets show that DebiasGAN can
effectively improve the accuracy of news recommendation by eliminating position
biases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Multi-Object Tracking and Segmentation with GMPHD Filter and Mask-based Affinity Fusion. (arXiv:2009.00100v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00100</id>
        <link href="http://arxiv.org/abs/2009.00100"/>
        <updated>2021-06-14T01:38:50.799Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a highly practical fully online multi-object
tracking and segmentation (MOTS) method that uses instance segmentation results
as an input. The proposed method is based on the Gaussian mixture probability
hypothesis density (GMPHD) filter, a hierarchical data association (HDA), and a
mask-based affinity fusion (MAF) model to achieve high-performance online
tracking. The HDA consists of two associations: segment-to-track and
track-to-track associations. One affinity, for position and motion, is computed
by using the GMPHD filter, and the other affinity, for appearance is computed
by using the responses from a single object tracker such as a kernalized
correlation filter. These two affinities are simply fused by using a
score-level fusion method such as min-max normalization referred to as MAF. In
addition, to reduce the number of false positive segments, we adopt mask
IoU-based merging (mask merging). The proposed MOTS framework with the key
modules: HDA, MAF, and mask merging, is easily extensible to simultaneously
track multiple types of objects with CPU only execution in parallel processing.
In addition, the developed framework only requires simple parameter tuning
unlike many existing MOTS methods that need intensive hyperparameter
optimization. In the experiments on the two popular MOTS datasets, the key
modules show some improvements. For instance, ID-switch decreases by more than
half compared to a baseline method in the training sets. In conclusion, our
tracker achieves state-of-the-art MOTS performance in the test sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Young-min Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1"&gt;Young-chul Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1"&gt;Kwangjin Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1"&gt;Moongu Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1"&gt;Witold Pedrycz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IoT Virtualization with ML-based Information Extraction. (arXiv:2106.06022v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.06022</id>
        <link href="http://arxiv.org/abs/2106.06022"/>
        <updated>2021-06-14T01:38:50.756Z</updated>
        <summary type="html"><![CDATA[For IoT to reach its full potential, the sharing and reuse of information in
different applications and across verticals is of paramount importance.
However, there are a plethora of IoT platforms using different representations,
protocols and interaction patterns. To address this issue, the Fed4IoT project
has developed an IoT virtualization platform that, on the one hand, integrates
information from many different source platforms and, on the other hand, makes
the information required by the respective users available in the target
platform of choice. To enable this, information is translated into a common,
neutral exchange format. The format of choice is NGSI-LD, which is being
standardized by the ETSI Industry Specification Group on Context Information
Management (ETSI ISG CIM). Thing Visors are the components that translate the
source information to NGSI-LD, which is then delivered to the target platform
and translated into the target format. ThingVisors can be implemented by hand,
but this requires significant human effort, especially considering the
heterogeneity of low level information produced by a multitude of sensors.
Thus, supporting the human developer and, ideally, fully automating the process
of extracting and enriching data and translating it to NGSI-LD is a crucial
step. Machine learning is a promising approach for this, but it typically
requires large amounts of hand-labelled data for training, an effort that makes
it unrealistic in many IoT scenarios. A programmatic labelling approach called
knowledge infusion that encodes expert knowledge is used for matching a schema
or ontology extracted from the data with a target schema or ontology, providing
the basis for annotating the data and facilitating the translation to NGSI-LD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1"&gt;Martin Bauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Abstract Representations through Lossy Compression of Multi-Modal Signals. (arXiv:2101.11376v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11376</id>
        <link href="http://arxiv.org/abs/2101.11376"/>
        <updated>2021-06-14T01:38:50.480Z</updated>
        <summary type="html"><![CDATA[A key competence for open-ended learning is the formation of increasingly
abstract representations useful for driving complex behavior. Abstract
representations ignore specific details and facilitate generalization. Here we
consider the learning of abstract representations in a multi-modal setting with
two or more input modalities. We treat the problem as a lossy compression
problem and show that generic lossy compression of multimodal sensory input
naturally extracts abstract representations that tend to strip away modalitiy
specific details and preferentially retain information that is shared across
the different modalities. Furthermore, we propose an architecture to learn
abstract representations by identifying and retaining only the information that
is shared across multiple modalities while discarding any modality specific
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilmot_C/0/1/0/all/0/1"&gt;Charles Wilmot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Triesch_J/0/1/0/all/0/1"&gt;Jochen Triesch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shallow Optical Flow Three-Stream CNN for Macro- and Micro-Expression Spotting from Long Videos. (arXiv:2106.06489v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.06489</id>
        <link href="http://arxiv.org/abs/2106.06489"/>
        <updated>2021-06-14T01:38:50.449Z</updated>
        <summary type="html"><![CDATA[Facial expressions vary from the visible to the subtle. In recent years, the
analysis of micro-expressions $-$ a natural occurrence resulting from the
suppression of one's true emotions, has drawn the attention of researchers with
a broad range of potential applications. However, spotting microexpressions in
long videos becomes increasingly challenging when intertwined with normal or
macro-expressions. In this paper, we propose a shallow optical flow
three-stream CNN (SOFTNet) model to predict a score that captures the
likelihood of a frame being in an expression interval. By fashioning the
spotting task as a regression problem, we introduce pseudo-labeling to
facilitate the learning process. We demonstrate the efficacy and efficiency of
the proposed approach on the recent MEGC 2020 benchmark, where state-of-the-art
performance is achieved on CAS(ME)$^{2}$ with equally promising results on SAMM
Long Videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liong_G/0/1/0/all/0/1"&gt;Gen-Bing Liong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1"&gt;John See&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1"&gt;Lai-Kuan Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super-Resolution Image Reconstruction Based on Self-Calibrated Convolutional GAN. (arXiv:2106.05545v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.05545</id>
        <link href="http://arxiv.org/abs/2106.05545"/>
        <updated>2021-06-11T22:07:42.030Z</updated>
        <summary type="html"><![CDATA[With the effective application of deep learning in computer vision,
breakthroughs have been made in the research of super-resolution images
reconstruction. However, many researches have pointed out that the
insufficiency of the neural network extraction on image features may bring the
deteriorating of newly reconstructed image. On the other hand, the generated
pictures are sometimes too artificial because of over-smoothing. In order to
solve the above problems, we propose a novel self-calibrated convolutional
generative adversarial networks. The generator consists of feature extraction
and image reconstruction. Feature extraction uses self-calibrated convolutions,
which contains four portions, and each portion has specific functions. It can
not only expand the range of receptive fields, but also obtain long-range
spatial and inter-channel dependencies. Then image reconstruction is performed,
and finally a super-resolution image is reconstructed. We have conducted
thorough experiments on different datasets including set5, set14 and BSD100
under the SSIM evaluation method. The experimental results prove the
effectiveness of the proposed network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yibo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haidi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yiming Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shunyao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingliang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anatomy X-Net: A Semi-Supervised Anatomy Aware Convolutional Neural Network for Thoracic Disease Classification. (arXiv:2106.05915v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.05915</id>
        <link href="http://arxiv.org/abs/2106.05915"/>
        <updated>2021-06-11T22:07:42.006Z</updated>
        <summary type="html"><![CDATA[Thoracic disease detection from chest radiographs using deep learning methods
has been an active area of research in the last decade. Most previous methods
attempt to focus on the diseased organs of the image by identifying spatial
regions responsible for significant contributions to the model's prediction. In
contrast, expert radiologists first locate the prominent anatomical structures
before determining if those regions are anomalous. Therefore, integrating
anatomical knowledge within deep learning models could bring substantial
improvement in automatic disease classification. This work proposes an
anatomy-aware attention-based architecture named Anatomy X-Net, that
prioritizes the spatial features guided by the pre-identified anatomy regions.
We leverage a semi-supervised learning method using the JSRT dataset containing
organ-level annotation to obtain the anatomical segmentation masks (for lungs
and heart) for the NIH and CheXpert datasets. The proposed Anatomy X-Net uses
the pre-trained DenseNet-121 as the backbone network with two corresponding
structured modules, the Anatomy Aware Attention (AAA) and Probabilistic
Weighted Average Pooling (PWAP), in a cohesive framework for anatomical
attention learning. Our proposed method sets new state-of-the-art performance
on the official NIH test set with an AUC score of 0.8439, proving the efficacy
of utilizing the anatomy segmentation knowledge to improve the thoracic disease
classification. Furthermore, the Anatomy X-Net yields an averaged AUC of 0.9020
on the Stanford CheXpert dataset, improving on existing methods that
demonstrate the generalizability of the proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kamal_U/0/1/0/all/0/1"&gt;Uday Kamal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zunaed_M/0/1/0/all/0/1"&gt;Mohammad Zunaed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nizam_N/0/1/0/all/0/1"&gt;Nusrat Binta Nizam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hasan_T/0/1/0/all/0/1"&gt;Taufiq Hasan&lt;/a&gt;</name>
        </author>
    </entry>
</feed>